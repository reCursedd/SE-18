{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384714169", "html_url": "https://github.com/pytorch/pytorch/issues/6996#issuecomment-384714169", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6996", "id": 384714169, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDcxNDE2OQ==", "user": {"login": "sytrus-in-github", "id": 12224616, "node_id": "MDQ6VXNlcjEyMjI0NjE2", "avatar_url": "https://avatars0.githubusercontent.com/u/12224616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytrus-in-github", "html_url": "https://github.com/sytrus-in-github", "followers_url": "https://api.github.com/users/sytrus-in-github/followers", "following_url": "https://api.github.com/users/sytrus-in-github/following{/other_user}", "gists_url": "https://api.github.com/users/sytrus-in-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytrus-in-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytrus-in-github/subscriptions", "organizations_url": "https://api.github.com/users/sytrus-in-github/orgs", "repos_url": "https://api.github.com/users/sytrus-in-github/repos", "events_url": "https://api.github.com/users/sytrus-in-github/events{/privacy}", "received_events_url": "https://api.github.com/users/sytrus-in-github/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T16:57:56Z", "updated_at": "2018-04-26T16:57:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>by the way, same tests with nightly build 2018.04.20:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.<span class=\"pl-c1\">__version__</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'</span>2018.04.20<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>).fill_(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nan<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> a.cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.max(a)\ntensor(<span class=\"pl-c1\">1.0000</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.min(a)\ntensor(<span class=\"pl-c1\">1.0000</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>).fill_(<span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nan<span class=\"pl-pds\">'</span></span>))\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> a.cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.max(a)\ntensor(<span class=\"pl-c1\">1.00000e+38</span> <span class=\"pl-k\">*</span>\n       <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.4028</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.min(a)\ntensor(<span class=\"pl-c1\">1.00000e+38</span> <span class=\"pl-k\">*</span>\n       <span class=\"pl-c1\">3.4028</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>This might shed some light as to why it crashes saying \"overflow\".</p>", "body_text": "by the way, same tests with nightly build 2018.04.20:\n>>> import torch\n>>> torch.__version__\n'2018.04.20'\n>>> a = torch.Tensor(4, 3).fill_(1)\n>>> a[1, 2] = float('nan')\n>>> a = a.cuda()\n>>> torch.max(a)\ntensor(1.0000, device='cuda:0')\n>>> torch.min(a)\ntensor(1.0000, device='cuda:0')\n>>> a = torch.Tensor(4, 3).fill_(float('nan'))\n>>> a = a.cuda()\n>>> torch.max(a)\ntensor(1.00000e+38 *\n       -3.4028, device='cuda:0')\n>>> torch.min(a)\ntensor(1.00000e+38 *\n       3.4028, device='cuda:0')\nThis might shed some light as to why it crashes saying \"overflow\".", "body": "by the way, same tests with nightly build 2018.04.20:\r\n```python\r\n>>> import torch\r\n>>> torch.__version__\r\n'2018.04.20'\r\n>>> a = torch.Tensor(4, 3).fill_(1)\r\n>>> a[1, 2] = float('nan')\r\n>>> a = a.cuda()\r\n>>> torch.max(a)\r\ntensor(1.0000, device='cuda:0')\r\n>>> torch.min(a)\r\ntensor(1.0000, device='cuda:0')\r\n>>> a = torch.Tensor(4, 3).fill_(float('nan'))\r\n>>> a = a.cuda()\r\n>>> torch.max(a)\r\ntensor(1.00000e+38 *\r\n       -3.4028, device='cuda:0')\r\n>>> torch.min(a)\r\ntensor(1.00000e+38 *\r\n       3.4028, device='cuda:0')\r\n```\r\nThis might shed some light as to why it crashes saying \"overflow\"."}