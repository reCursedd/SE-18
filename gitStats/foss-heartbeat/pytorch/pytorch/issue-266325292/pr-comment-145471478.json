{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145471478", "pull_request_review_id": 70277135, "id": 145471478, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTQ3MTQ3OA==", "diff_hunk": "@@ -1,92 +1,38 @@\n-from ..stochastic_function import StochasticFunction\n+import torch\n+from ..function import Function\n \n-# Gradient formulas are based on Simple Statistical Gradient-Following\n-# Algorithms for Connectionist Reinforcement Learning, available at\n-# http://incompleteideas.net/sutton/williams-92.pdf\n \n-\n-class Multinomial(StochasticFunction):\n-\n-    def __init__(self, num_samples, with_replacement):\n-        super(Multinomial, self).__init__()\n-        self.num_samples = num_samples\n-        self.with_replacement = with_replacement\n-\n-    def forward(self, probs):\n-        samples = probs.multinomial(self.num_samples, self.with_replacement)\n-        self.save_for_backward(probs, samples)\n-        self.mark_non_differentiable(samples)\n+class Multinomial(Function):\n+    @staticmethod\n+    def forward(ctx, probs, num_samples, with_replacement):\n+        samples = probs.multinomial(num_samples, with_replacement)\n+        ctx.mark_non_differentiable(samples)\n         return samples\n \n-    def backward(self, reward):\n-        probs, samples = self.saved_tensors\n-        if probs.dim() == 1:\n-            probs = probs.unsqueeze(0)\n-            samples = samples.unsqueeze(0)\n-            reward = reward.unsqueeze(0)\n-        # normalize probs (multinomial accepts weights)\n-        probs /= probs.sum(1, True).expand_as(probs)\n-        grad_probs = probs.new().resize_as_(probs).zero_()\n-        output_probs = probs.gather(1, samples)\n-        output_probs.add_(1e-6).reciprocal_()\n-        output_probs.neg_().mul_(reward)\n-        # TODO: add batched index_add\n-        for i in range(probs.size(0)):\n-            grad_probs[i].index_add_(0, samples[i], output_probs[i])\n-        return grad_probs\n-\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return None, None, None", "path": "torch/autograd/_functions/stochastic.py", "position": 47, "original_position": 47, "commit_id": "62f13a8139a6c201c87bfa0d86dad66d2da9886e", "original_commit_id": "62f13a8139a6c201c87bfa0d86dad66d2da9886e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I think it's because for `z = f(x, y)` if:\r\n\r\n  x.requires_grad=False\r\n  y.requires_grad=True\r\n\r\nand both have `grad_fn`s defined, then z.grad_fn.next_function will include both.\r\n\r\nI'm not particularly concerned right now for two reasons:\r\n\r\n1. These particular functions won't last very long. Since they're simple wrappers around the torch functions they can easily be moved to C++\r\n2. A big part of this change is to make it so that functions which don't require_grad don't have grad_fn", "created_at": "2017-10-18T16:35:50Z", "updated_at": "2018-11-23T15:35:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/3157#discussion_r145471478", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3157", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145471478"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3157#discussion_r145471478"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3157"}}, "body_html": "<p>I think it's because for <code>z = f(x, y)</code> if:</p>\n<p>x.requires_grad=False<br>\ny.requires_grad=True</p>\n<p>and both have <code>grad_fn</code>s defined, then z.grad_fn.next_function will include both.</p>\n<p>I'm not particularly concerned right now for two reasons:</p>\n<ol>\n<li>These particular functions won't last very long. Since they're simple wrappers around the torch functions they can easily be moved to C++</li>\n<li>A big part of this change is to make it so that functions which don't require_grad don't have grad_fn</li>\n</ol>", "body_text": "I think it's because for z = f(x, y) if:\nx.requires_grad=False\ny.requires_grad=True\nand both have grad_fns defined, then z.grad_fn.next_function will include both.\nI'm not particularly concerned right now for two reasons:\n\nThese particular functions won't last very long. Since they're simple wrappers around the torch functions they can easily be moved to C++\nA big part of this change is to make it so that functions which don't require_grad don't have grad_fn", "in_reply_to_id": 145378480}