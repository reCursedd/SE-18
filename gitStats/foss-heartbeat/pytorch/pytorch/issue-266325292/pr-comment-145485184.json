{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145485184", "pull_request_review_id": 70292863, "id": 145485184, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTQ4NTE4NA==", "diff_hunk": "@@ -0,0 +1,104 @@\n+\"\"\"\n+The ``reinforce`` package contains sampling functions that return both the\n+random samples and the log of the probability density function (pdf) or\n+probability mass function (pmf) evaluated at each sample. These\n+log-probabilities can be backpropagated through for policy gradient methods.\n+\n+Example::\n+\n+    probs = network(input)\n+    action, log_prob = torch.reinforce.multinomial(probs)\n+    loss = -log_prob * get_reward(env, action)\n+    loss.backward()\n+\"\"\"\n+import math\n+import torch\n+\n+\n+__all__ = ['multinomial', 'bernoulli', 'normal']\n+\n+\n+def multinomial(input, num_samples=1):\n+    r\"\"\"\n+    Samples from the multinomial distribution characterized by `input`. Returns\n+    the samples and the log-probabilities of each returned sample.\n+\n+    See also: :func:`torch.multinomial`\n+\n+    .. note::\n+\n+        When `num_samples` is greater than one, this function always samples\n+        with replacement.", "path": "torch/reinforce/__init__.py", "position": 31, "original_position": 31, "commit_id": "62f13a8139a6c201c87bfa0d86dad66d2da9886e", "original_commit_id": "62f13a8139a6c201c87bfa0d86dad66d2da9886e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "The probabilities are then conditional on the previously drawn samples. I'm not sure if that would affect policy-gradient methods.", "created_at": "2017-10-18T17:28:15Z", "updated_at": "2018-11-23T15:35:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/3157#discussion_r145485184", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3157", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145485184"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3157#discussion_r145485184"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3157"}}, "body_html": "<p>The probabilities are then conditional on the previously drawn samples. I'm not sure if that would affect policy-gradient methods.</p>", "body_text": "The probabilities are then conditional on the previously drawn samples. I'm not sure if that would affect policy-gradient methods.", "in_reply_to_id": 145378743}