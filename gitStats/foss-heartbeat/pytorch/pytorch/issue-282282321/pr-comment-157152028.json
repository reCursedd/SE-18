{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157152028", "pull_request_review_id": 83749506, "id": 157152028, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzE1MjAyOA==", "diff_hunk": "@@ -695,13 +695,13 @@\n   self: hardshrink_backward(grad, self, lambd)\n \n - name: hardtanh(Tensor self, Scalar min_val, Scalar max_val)\n-  self: hardtanh_backward(grad, self, min_val, max_val)\n+  self: hardtanh_backward(grad, output, min_val, max_val)\n \n - name: hardtanh_(Tensor self, Scalar min_val, Scalar max_val)\n   self: hardtanh_backward(grad, output, min_val, max_val)\n \n - name: leaky_relu(Tensor self, Scalar negative_slope)\n-  self: leaky_relu_backward(grad, self, negative_slope)\n+  self: leaky_relu_backward(grad, output, negative_slope)", "path": "tools/autograd/derivatives.yaml", "position": 12, "original_position": 12, "commit_id": "6f78845ed598d15e936525cbad33c94b1a9730bd", "original_commit_id": "6f78845ed598d15e936525cbad33c94b1a9730bd", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "My concerns are not so much about fixing the memory usage issues, but more about correctness. We can't be passing outputs to backwards that expect inputs and vice versa.\r\n\r\nBTW this is not true for leaky relu if `negative_slope` is positive - output will be `>= 0`.", "created_at": "2017-12-15T09:11:27Z", "updated_at": "2018-11-23T15:37:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/4184#discussion_r157152028", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4184", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157152028"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4184#discussion_r157152028"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4184"}}, "body_html": "<p>My concerns are not so much about fixing the memory usage issues, but more about correctness. We can't be passing outputs to backwards that expect inputs and vice versa.</p>\n<p>BTW this is not true for leaky relu if <code>negative_slope</code> is positive - output will be <code>&gt;= 0</code>.</p>", "body_text": "My concerns are not so much about fixing the memory usage issues, but more about correctness. We can't be passing outputs to backwards that expect inputs and vice versa.\nBTW this is not true for leaky relu if negative_slope is positive - output will be >= 0.", "in_reply_to_id": 157139885}