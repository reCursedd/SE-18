{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12461", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12461/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12461/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12461/events", "html_url": "https://github.com/pytorch/pytorch/issues/12461", "id": 367908921, "node_id": "MDU6SXNzdWUzNjc5MDg5MjE=", "number": 12461, "title": "Certain operations cause implicity sync-points", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-08T18:44:14Z", "updated_at": "2018-10-09T15:08:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>When profiling networks I've found that certain functions seem to cause the CUDA backend to synchronize with the host.</p>\n<p>This is similar to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"240870749\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1989\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1989/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1989\">#1989</a> but is applicable to a wider variety of functions, which I believe shouldn't cause syncs, but they seem to.</p>\n<p>Even if these operations do implicitly cause syncs it would be nice to have a list of which functions do this in the docs (or perhaps someone can point me to this if this already exists).</p>\n<h2>To Reproduce</h2>\n<p>I have a MWE showing several functions that do and do not cause implicit sync points. The basic idea is I create some data, and run it through some busy work operations that do not cause syncing (i.e. convolutions), then I run it through an operation to test if it syncs. Finally, I run it through a few more busy work ops and then force a synchronization at the end. I run a line profile to check how long each call takes. If the operation causes a sync, then it will show up as taking time in the profiler, otherwise if there is no implicit syncing, then the bulk of time should be taken up by the final explicit sync.</p>\n<p>The operation that I test first is uint8 masking (i.e. masked_select / <strong>getitem</strong>) (which is how I first uncovered the issue)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">profile_onthefly</span>(<span class=\"pl-smi\">func</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">_wrapper</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kw</span>):\n            <span class=\"pl-k\">import</span> line_profiler\n            <span class=\"pl-k\">from</span> six.moves <span class=\"pl-k\">import</span> cStringIO\n            profile <span class=\"pl-k\">=</span> line_profiler.LineProfiler()\n\n            result <span class=\"pl-k\">=</span> profile(func)(<span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kw)\n\n            file_ <span class=\"pl-k\">=</span> cStringIO()\n            profile.print_stats(<span class=\"pl-v\">stream</span><span class=\"pl-k\">=</span>file_, <span class=\"pl-v\">stripzeros</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n            file_.seek(<span class=\"pl-c1\">0</span>)\n            text <span class=\"pl-k\">=</span> file_.read()\n            <span class=\"pl-c1\">print</span>(text)\n            <span class=\"pl-k\">return</span> result\n        <span class=\"pl-k\">return</span> _wrapper\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_mask</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>, <span class=\"pl-smi\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_mask = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_mask))\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_sync = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_sync))\n\n        torch.cuda.synchronize()\n\n        x <span class=\"pl-k\">=</span> data\n        N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do some busy work</span>\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n            x <span class=\"pl-k\">=</span> conv2d(x)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a mask</span>\n        mask <span class=\"pl-k\">=</span> (data <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">.5</span>).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">if</span> do_mask:\n            x <span class=\"pl-k\">=</span> x[mask]\n\n        <span class=\"pl-k\">if</span> do_sync:\n            torch.cuda.synchronize()\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n            x <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> x\n            x <span class=\"pl-k\">=</span> torch.sqrt(x)\n\n        torch.cuda.synchronize()\n\n    xpu <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-c1\">0</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Setup dummy data</span>\n    bsize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n    data <span class=\"pl-k\">=</span> torch.rand(bsize, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">512</span>).to(xpu)\n    conv2d <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).to(xpu)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Profile to show how masking causes an implicity sync-point</span>\n\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, <span class=\"pl-v\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, <span class=\"pl-v\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, <span class=\"pl-v\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>The final three lines run the test function in 3 different ways.</p>\n<p>The first line (<code>profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=True, do_sync=False)</code>) shows that if we do a mask, but we don't do an internal sync, then there is a bulk of time taken up by the masking operation (suggesting that an implicit sync is occurring).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------</span><span class=\"pl-k\">-</span>\n <span class=\"pl-k\">*</span> do_mask <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n <span class=\"pl-k\">*</span> do_sync <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.261704</span> s\nFile: <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>joncrall<span class=\"pl-k\">/</span>_torch_sync_mwe.py\nFunction: test_syncpoint_mask at line <span class=\"pl-c1\">22</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">22</span>                                               <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_mask</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>, <span class=\"pl-smi\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-c1\">23</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">27.0</span>     <span class=\"pl-c1\">27.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">24</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_mask = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_mask))\n    <span class=\"pl-c1\">25</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">4.0</span>      <span class=\"pl-c1\">4.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_sync = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_sync))\n    <span class=\"pl-c1\">26</span>                                           \n    <span class=\"pl-c1\">27</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">88.0</span>     <span class=\"pl-c1\">88.0</span>      <span class=\"pl-c1\">0.0</span>          torch.cuda.synchronize()\n    <span class=\"pl-c1\">28</span>                                           \n    <span class=\"pl-c1\">29</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          x <span class=\"pl-k\">=</span> data\n    <span class=\"pl-c1\">30</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">31</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do some busy work</span>\n    <span class=\"pl-c1\">32</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">15.0</span>      <span class=\"pl-c1\">1.4</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">33</span>        <span class=\"pl-c1\">10</span>      <span class=\"pl-c1\">10231.0</span>   <span class=\"pl-c1\">1023.1</span>      <span class=\"pl-c1\">3.9</span>              x <span class=\"pl-k\">=</span> conv2d(x)\n    <span class=\"pl-c1\">34</span>                                           \n    <span class=\"pl-c1\">35</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a mask</span>\n    <span class=\"pl-c1\">36</span>         <span class=\"pl-c1\">1</span>        <span class=\"pl-c1\">455.0</span>    <span class=\"pl-c1\">455.0</span>      <span class=\"pl-c1\">0.2</span>          mask <span class=\"pl-k\">=</span> (data <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">.5</span>).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">37</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">16.0</span>     <span class=\"pl-c1\">16.0</span>      <span class=\"pl-c1\">0.0</span>          x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">38</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">if</span> do_mask:\n    <span class=\"pl-c1\">39</span>         <span class=\"pl-c1\">1</span>     <span class=\"pl-c1\">222128.0</span> <span class=\"pl-c1\">222128.0</span>     <span class=\"pl-c1\">84.9</span>              x <span class=\"pl-k\">=</span> x[mask]\n    <span class=\"pl-c1\">40</span>                                           \n    <span class=\"pl-c1\">41</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">if</span> do_sync:\n    <span class=\"pl-c1\">42</span>                                                       torch.cuda.synchronize()\n    <span class=\"pl-c1\">43</span>                                           \n    <span class=\"pl-c1\">44</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n    <span class=\"pl-c1\">45</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">13.0</span>      <span class=\"pl-c1\">1.2</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">46</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">297.0</span>     <span class=\"pl-c1\">29.7</span>      <span class=\"pl-c1\">0.1</span>              x <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> x\n    <span class=\"pl-c1\">47</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">2895.0</span>    <span class=\"pl-c1\">289.5</span>      <span class=\"pl-c1\">1.1</span>              x <span class=\"pl-k\">=</span> torch.sqrt(x)\n    <span class=\"pl-c1\">48</span>                                           \n    <span class=\"pl-c1\">49</span>         <span class=\"pl-c1\">1</span>      <span class=\"pl-c1\">25523.0</span>  <span class=\"pl-c1\">25523.0</span>      <span class=\"pl-c1\">9.8</span>          torch.cuda.synchronize()</pre></div>\n<p>We can see that in this case 84.9% of the time is spent on the mask operation because it is actually waiting for all those convolutions to finish.</p>\n<p>The next line (<code>profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=False)</code>), verifies that if we do neither a mask or a sync, then the majority of time is indeed taken up by the final explicit sync call.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------</span><span class=\"pl-k\">-</span>\n <span class=\"pl-k\">*</span> do_mask <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n <span class=\"pl-k\">*</span> do_sync <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.233294</span> s\nFile: <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>joncrall<span class=\"pl-k\">/</span>_torch_sync_mwe.py\nFunction: test_syncpoint_mask at line <span class=\"pl-c1\">22</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">22</span>                                               <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_mask</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>, <span class=\"pl-smi\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-c1\">23</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">24</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">7.0</span>      <span class=\"pl-c1\">7.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_mask = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_mask))\n    <span class=\"pl-c1\">25</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">5.0</span>      <span class=\"pl-c1\">5.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_sync = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_sync))\n    <span class=\"pl-c1\">26</span>                                           \n    <span class=\"pl-c1\">27</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">14.0</span>     <span class=\"pl-c1\">14.0</span>      <span class=\"pl-c1\">0.0</span>          torch.cuda.synchronize()\n    <span class=\"pl-c1\">28</span>                                           \n    <span class=\"pl-c1\">29</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          x <span class=\"pl-k\">=</span> data\n    <span class=\"pl-c1\">30</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">31</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do some busy work</span>\n    <span class=\"pl-c1\">32</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">12.0</span>      <span class=\"pl-c1\">1.1</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">33</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">1091.0</span>    <span class=\"pl-c1\">109.1</span>      <span class=\"pl-c1\">0.5</span>              x <span class=\"pl-k\">=</span> conv2d(x)\n    <span class=\"pl-c1\">34</span>                                           \n    <span class=\"pl-c1\">35</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a mask</span>\n    <span class=\"pl-c1\">36</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">32.0</span>     <span class=\"pl-c1\">32.0</span>      <span class=\"pl-c1\">0.0</span>          mask <span class=\"pl-k\">=</span> (data <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">.5</span>).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">37</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>          x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">38</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">if</span> do_mask:\n    <span class=\"pl-c1\">39</span>                                                       x <span class=\"pl-k\">=</span> x[mask]\n    <span class=\"pl-c1\">40</span>                                           \n    <span class=\"pl-c1\">41</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">if</span> do_sync:\n    <span class=\"pl-c1\">42</span>                                                       torch.cuda.synchronize()\n    <span class=\"pl-c1\">43</span>                                           \n    <span class=\"pl-c1\">44</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n    <span class=\"pl-c1\">45</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">16.0</span>      <span class=\"pl-c1\">1.5</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">46</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">254.0</span>     <span class=\"pl-c1\">25.4</span>      <span class=\"pl-c1\">0.1</span>              x <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> x\n    <span class=\"pl-c1\">47</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">5359.0</span>    <span class=\"pl-c1\">535.9</span>      <span class=\"pl-c1\">2.3</span>              x <span class=\"pl-k\">=</span> torch.sqrt(x)\n    <span class=\"pl-c1\">48</span>                                           \n    <span class=\"pl-c1\">49</span>         <span class=\"pl-c1\">1</span>     <span class=\"pl-c1\">226486.0</span> <span class=\"pl-c1\">226486.0</span>     <span class=\"pl-c1\">97.1</span>          torch.cuda.synchronize()</pre></div>\n<p>In this case 97% of the time is taken up by the final synchronize operation, which means that no intermediate syncing has taken place, all the work is done in a lazy fashion, so the final sync eats up all the time.</p>\n<p>Finally the final line <code>profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=True)</code>, shows that if we force an intermediate sync (without masking), we get a similar performance time as if we did the mask.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------</span><span class=\"pl-k\">-</span>\n <span class=\"pl-k\">*</span> do_mask <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n <span class=\"pl-k\">*</span> do_sync <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.233516</span> s\nFile: <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>joncrall<span class=\"pl-k\">/</span>_torch_sync_mwe.py\nFunction: test_syncpoint_mask at line <span class=\"pl-c1\">22</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">22</span>                                               <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_mask</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>, <span class=\"pl-smi\">do_mask</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">do_sync</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-c1\">23</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">24</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">7.0</span>      <span class=\"pl-c1\">7.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_mask = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_mask))\n    <span class=\"pl-c1\">25</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">5.0</span>      <span class=\"pl-c1\">5.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> * do_sync = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(do_sync))\n    <span class=\"pl-c1\">26</span>                                           \n    <span class=\"pl-c1\">27</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">13.0</span>     <span class=\"pl-c1\">13.0</span>      <span class=\"pl-c1\">0.0</span>          torch.cuda.synchronize()\n    <span class=\"pl-c1\">28</span>                                           \n    <span class=\"pl-c1\">29</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          x <span class=\"pl-k\">=</span> data\n    <span class=\"pl-c1\">30</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>          N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">31</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do some busy work</span>\n    <span class=\"pl-c1\">32</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">14.0</span>      <span class=\"pl-c1\">1.3</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">33</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">1093.0</span>    <span class=\"pl-c1\">109.3</span>      <span class=\"pl-c1\">0.5</span>              x <span class=\"pl-k\">=</span> conv2d(x)\n    <span class=\"pl-c1\">34</span>                                           \n    <span class=\"pl-c1\">35</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a mask</span>\n    <span class=\"pl-c1\">36</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">34.0</span>     <span class=\"pl-c1\">34.0</span>      <span class=\"pl-c1\">0.0</span>          mask <span class=\"pl-k\">=</span> (data <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">.5</span>).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">37</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>          x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">38</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">if</span> do_mask:\n    <span class=\"pl-c1\">39</span>                                                       x <span class=\"pl-k\">=</span> x[mask]\n    <span class=\"pl-c1\">40</span>                                           \n    <span class=\"pl-c1\">41</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">if</span> do_sync:\n    <span class=\"pl-c1\">42</span>         <span class=\"pl-c1\">1</span>     <span class=\"pl-c1\">183895.0</span> <span class=\"pl-c1\">183895.0</span>     <span class=\"pl-c1\">78.8</span>              torch.cuda.synchronize()\n    <span class=\"pl-c1\">43</span>                                           \n    <span class=\"pl-c1\">44</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n    <span class=\"pl-c1\">45</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">10.0</span>      <span class=\"pl-c1\">0.9</span>      <span class=\"pl-c1\">0.0</span>          <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">46</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">220.0</span>     <span class=\"pl-c1\">22.0</span>      <span class=\"pl-c1\">0.1</span>              x <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> x\n    <span class=\"pl-c1\">47</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">232.0</span>     <span class=\"pl-c1\">23.2</span>      <span class=\"pl-c1\">0.1</span>              x <span class=\"pl-k\">=</span> torch.sqrt(x)\n    <span class=\"pl-c1\">48</span>                                           \n    <span class=\"pl-c1\">49</span>         <span class=\"pl-c1\">1</span>      <span class=\"pl-c1\">47975.0</span>  <span class=\"pl-c1\">47975.0</span>     <span class=\"pl-c1\">20.5</span>          torch.cuda.synchronize()</pre></div>\n<p>In this case we see if we replace the mask with a explicit sync, that takes up 78% of the time, which is similar to the 84% of the time we saw with the masking operation.</p>\n<hr>\n<p>I test this for a few other operations as well</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> --- There seem to be a few operations that cause implicity syncs</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_synpoint_other</span>(<span class=\"pl-smi\">opname</span>, <span class=\"pl-smi\">op</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>):\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>opname = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(opname))\n            torch.cuda.synchronize()\n            x <span class=\"pl-k\">=</span> data\n            N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n                x <span class=\"pl-k\">=</span> conv2d(x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n            op(x)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n                x <span class=\"pl-k\">=</span> torch.sqrt(x <span class=\"pl-k\">*</span> x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n            torch.cuda.synchronize()\n        profile_onthefly(test_syncpoint_)(data, conv2d)\n\n    test_synpoint_other(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x.sum())\n    test_synpoint_other(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum(dim=0)<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x.sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>))\n    test_synpoint_other(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x.sigmoid())</pre></div>\n<p>A simple sum will cause an  implicit sync</p>\n<div class=\"highlight highlight-source-python\"><pre>opname <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.221028</span> s\nFile: <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>joncrall<span class=\"pl-k\">/</span>_torch_sync_mwe.py\nFunction: test_syncpoint_ at line <span class=\"pl-c1\">68</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">68</span>                                                   <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>):\n    <span class=\"pl-c1\">69</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">9.0</span>      <span class=\"pl-c1\">9.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">70</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>opname = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(opname))\n    <span class=\"pl-c1\">71</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">13.0</span>     <span class=\"pl-c1\">13.0</span>      <span class=\"pl-c1\">0.0</span>              torch.cuda.synchronize()\n    <span class=\"pl-c1\">72</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              x <span class=\"pl-k\">=</span> data\n    <span class=\"pl-c1\">73</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">74</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">12.0</span>      <span class=\"pl-c1\">1.1</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">75</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">1148.0</span>    <span class=\"pl-c1\">114.8</span>      <span class=\"pl-c1\">0.5</span>                  x <span class=\"pl-k\">=</span> conv2d(x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n    <span class=\"pl-c1\">76</span>         <span class=\"pl-c1\">1</span>     <span class=\"pl-c1\">174351.0</span> <span class=\"pl-c1\">174351.0</span>     <span class=\"pl-c1\">78.9</span>              op(x)\n    <span class=\"pl-c1\">77</span>                                                       <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n    <span class=\"pl-c1\">78</span>        <span class=\"pl-c1\">11</span>          <span class=\"pl-c1\">9.0</span>      <span class=\"pl-c1\">0.8</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">79</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">455.0</span>     <span class=\"pl-c1\">45.5</span>      <span class=\"pl-c1\">0.2</span>                  x <span class=\"pl-k\">=</span> torch.sqrt(x <span class=\"pl-k\">*</span> x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n    <span class=\"pl-c1\">80</span>         <span class=\"pl-c1\">1</span>      <span class=\"pl-c1\">45021.0</span>  <span class=\"pl-c1\">45021.0</span>     <span class=\"pl-c1\">20.4</span>              torch.cuda.synchronize()</pre></div>\n<p>However, a <code>sum(dim=0)</code> and a <code>sigmoid</code> call will not.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------</span><span class=\"pl-k\">-</span>\nopname <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum(dim=0)<span class=\"pl-pds\">'</span></span>\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.221</span> s\nFile: <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>joncrall<span class=\"pl-k\">/</span>_torch_sync_mwe.py\nFunction: test_syncpoint_ at line <span class=\"pl-c1\">68</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">68</span>                                                   <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>):\n    <span class=\"pl-c1\">69</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">8.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">70</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">7.0</span>      <span class=\"pl-c1\">7.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>opname = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(opname))\n    <span class=\"pl-c1\">71</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">13.0</span>     <span class=\"pl-c1\">13.0</span>      <span class=\"pl-c1\">0.0</span>              torch.cuda.synchronize()\n    <span class=\"pl-c1\">72</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              x <span class=\"pl-k\">=</span> data\n    <span class=\"pl-c1\">73</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">74</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">11.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">75</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">1099.0</span>    <span class=\"pl-c1\">109.9</span>      <span class=\"pl-c1\">0.5</span>                  x <span class=\"pl-k\">=</span> conv2d(x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n    <span class=\"pl-c1\">76</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">92.0</span>     <span class=\"pl-c1\">92.0</span>      <span class=\"pl-c1\">0.0</span>              op(x)\n    <span class=\"pl-c1\">77</span>                                                       <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n    <span class=\"pl-c1\">78</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">10.0</span>      <span class=\"pl-c1\">0.9</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">79</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">421.0</span>     <span class=\"pl-c1\">42.1</span>      <span class=\"pl-c1\">0.2</span>                  x <span class=\"pl-k\">=</span> torch.sqrt(x <span class=\"pl-k\">*</span> x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n    <span class=\"pl-c1\">80</span>         <span class=\"pl-c1\">1</span>     <span class=\"pl-c1\">219337.0</span> <span class=\"pl-c1\">219337.0</span>     <span class=\"pl-c1\">99.2</span>              torch.cuda.synchronize()\n\n\n<span class=\"pl-ii\">--------</span><span class=\"pl-k\">-</span>\nopname <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.222286</span> s\nFile: <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>joncrall<span class=\"pl-k\">/</span>_torch_sync_mwe.py\nFunction: test_syncpoint_ at line <span class=\"pl-c1\">68</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">68</span>                                                   <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_syncpoint_</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">conv2d</span>):\n    <span class=\"pl-c1\">69</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">24.0</span>     <span class=\"pl-c1\">24.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>---------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">70</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">24.0</span>     <span class=\"pl-c1\">24.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>opname = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(opname))\n    <span class=\"pl-c1\">71</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">13.0</span>     <span class=\"pl-c1\">13.0</span>      <span class=\"pl-c1\">0.0</span>              torch.cuda.synchronize()\n    <span class=\"pl-c1\">72</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>      <span class=\"pl-c1\">0.0</span>              x <span class=\"pl-k\">=</span> data\n    <span class=\"pl-c1\">73</span>         <span class=\"pl-c1\">1</span>          <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">74</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">12.0</span>      <span class=\"pl-c1\">1.1</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">75</span>        <span class=\"pl-c1\">10</span>       <span class=\"pl-c1\">1088.0</span>    <span class=\"pl-c1\">108.8</span>      <span class=\"pl-c1\">0.5</span>                  x <span class=\"pl-k\">=</span> conv2d(x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n    <span class=\"pl-c1\">76</span>         <span class=\"pl-c1\">1</span>         <span class=\"pl-c1\">83.0</span>     <span class=\"pl-c1\">83.0</span>      <span class=\"pl-c1\">0.0</span>              op(x)\n    <span class=\"pl-c1\">77</span>                                                       <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do more busy work</span>\n    <span class=\"pl-c1\">78</span>        <span class=\"pl-c1\">11</span>         <span class=\"pl-c1\">11.0</span>      <span class=\"pl-c1\">1.0</span>      <span class=\"pl-c1\">0.0</span>              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    <span class=\"pl-c1\">79</span>        <span class=\"pl-c1\">10</span>        <span class=\"pl-c1\">419.0</span>     <span class=\"pl-c1\">41.9</span>      <span class=\"pl-c1\">0.2</span>                  x <span class=\"pl-k\">=</span> torch.sqrt(x <span class=\"pl-k\">*</span> x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> non-syncing busy work</span>\n    <span class=\"pl-c1\">80</span>         <span class=\"pl-c1\">1</span>     <span class=\"pl-c1\">220611.0</span> <span class=\"pl-c1\">220611.0</span>     <span class=\"pl-c1\">99.2</span>              torch.cuda.synchronize()</pre></div>\n<p>Instead of increasing the length of this report, I'll simply summarize the rest.</p>\n<p>Ops that did cause implicit sync:</p>\n<ul>\n<li>masked_select</li>\n<li>nonzero</li>\n<li>sum(dim=None)</li>\n</ul>\n<p>Ops that did not cause implicit sync:</p>\n<ul>\n<li>sum(dim=)</li>\n<li>conv</li>\n<li>reshape</li>\n<li>squeeze</li>\n<li>sigmoid</li>\n<li>arithmetic operations <code>+ - / * </code></li>\n</ul>\n<h2>Expected behavior</h2>\n<p>Obviously there are lots of operations I didn't test, so it would be nice to have an understanding of when I should expect an operation to cause an implicit sync and when I shouldn't.</p>\n<p>I expect its something to do with operations which have clearly defined input and output shapes, but having something more than my hunches would be nice. Also, perhaps this is a bug, and masked_select should actually not force a sync.</p>\n<h2>Environment</h2>\n<pre><code>PyTorch version: 1.0.0.dev20181008\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\n\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.148\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\n\nNvidia driver version: 396.54\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n</code></pre>", "body_text": "\ud83d\udc1b Bug\n\nWhen profiling networks I've found that certain functions seem to cause the CUDA backend to synchronize with the host.\nThis is similar to #1989 but is applicable to a wider variety of functions, which I believe shouldn't cause syncs, but they seem to.\nEven if these operations do implicitly cause syncs it would be nice to have a list of which functions do this in the docs (or perhaps someone can point me to this if this already exists).\nTo Reproduce\nI have a MWE showing several functions that do and do not cause implicit sync points. The basic idea is I create some data, and run it through some busy work operations that do not cause syncing (i.e. convolutions), then I run it through an operation to test if it syncs. Finally, I run it through a few more busy work ops and then force a synchronization at the end. I run a line profile to check how long each call takes. If the operation causes a sync, then it will show up as taking time in the profiler, otherwise if there is no implicit syncing, then the bulk of time should be taken up by the final explicit sync.\nThe operation that I test first is uint8 masking (i.e. masked_select / getitem) (which is how I first uncovered the issue)\nimport torch\n\n\ndef main():\n\n    def profile_onthefly(func):\n        def _wrapper(*args, **kw):\n            import line_profiler\n            from six.moves import cStringIO\n            profile = line_profiler.LineProfiler()\n\n            result = profile(func)(*args, **kw)\n\n            file_ = cStringIO()\n            profile.print_stats(stream=file_, stripzeros=True)\n            file_.seek(0)\n            text = file_.read()\n            print(text)\n            return result\n        return _wrapper\n\n    def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\n        print('---------')\n        print(' * do_mask = {!r}'.format(do_mask))\n        print(' * do_sync = {!r}'.format(do_sync))\n\n        torch.cuda.synchronize()\n\n        x = data\n        N = 10\n        # Do some busy work\n        for i in range(N):\n            x = conv2d(x)\n\n        # Create a mask\n        mask = (data > .5).view(-1)\n        x = x.view(-1)\n        if do_mask:\n            x = x[mask]\n\n        if do_sync:\n            torch.cuda.synchronize()\n\n        # Do more busy work\n        for i in range(N):\n            x = x * x\n            x = torch.sqrt(x)\n\n        torch.cuda.synchronize()\n\n    xpu = torch.device(0)\n    # Setup dummy data\n    bsize = 128\n    data = torch.rand(bsize, 3, 512, 512).to(xpu)\n    conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1).to(xpu)\n\n    # Profile to show how masking causes an implicity sync-point\n\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=True, do_sync=False)\n\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=False)\n\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=True)\nThe final three lines run the test function in 3 different ways.\nThe first line (profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=True, do_sync=False)) shows that if we do a mask, but we don't do an internal sync, then there is a bulk of time taken up by the masking operation (suggesting that an implicit sync is occurring).\n---------\n * do_mask = True\n * do_sync = False\nTimer unit: 1e-06 s\n\nTotal time: 0.261704 s\nFile: /home/joncrall/_torch_sync_mwe.py\nFunction: test_syncpoint_mask at line 22\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    22                                               def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\n    23         1         27.0     27.0      0.0          print('---------')\n    24         1          8.0      8.0      0.0          print(' * do_mask = {!r}'.format(do_mask))\n    25         1          4.0      4.0      0.0          print(' * do_sync = {!r}'.format(do_sync))\n    26                                           \n    27         1         88.0     88.0      0.0          torch.cuda.synchronize()\n    28                                           \n    29         1          1.0      1.0      0.0          x = data\n    30         1          1.0      1.0      0.0          N = 10\n    31                                                   # Do some busy work\n    32        11         15.0      1.4      0.0          for i in range(N):\n    33        10      10231.0   1023.1      3.9              x = conv2d(x)\n    34                                           \n    35                                                   # Create a mask\n    36         1        455.0    455.0      0.2          mask = (data > .5).view(-1)\n    37         1         16.0     16.0      0.0          x = x.view(-1)\n    38         1          1.0      1.0      0.0          if do_mask:\n    39         1     222128.0 222128.0     84.9              x = x[mask]\n    40                                           \n    41         1          1.0      1.0      0.0          if do_sync:\n    42                                                       torch.cuda.synchronize()\n    43                                           \n    44                                                   # Do more busy work\n    45        11         13.0      1.2      0.0          for i in range(N):\n    46        10        297.0     29.7      0.1              x = x * x\n    47        10       2895.0    289.5      1.1              x = torch.sqrt(x)\n    48                                           \n    49         1      25523.0  25523.0      9.8          torch.cuda.synchronize()\nWe can see that in this case 84.9% of the time is spent on the mask operation because it is actually waiting for all those convolutions to finish.\nThe next line (profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=False)), verifies that if we do neither a mask or a sync, then the majority of time is indeed taken up by the final explicit sync call.\n---------\n * do_mask = False\n * do_sync = False\nTimer unit: 1e-06 s\n\nTotal time: 0.233294 s\nFile: /home/joncrall/_torch_sync_mwe.py\nFunction: test_syncpoint_mask at line 22\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    22                                               def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\n    23         1          8.0      8.0      0.0          print('---------')\n    24         1          7.0      7.0      0.0          print(' * do_mask = {!r}'.format(do_mask))\n    25         1          5.0      5.0      0.0          print(' * do_sync = {!r}'.format(do_sync))\n    26                                           \n    27         1         14.0     14.0      0.0          torch.cuda.synchronize()\n    28                                           \n    29         1          1.0      1.0      0.0          x = data\n    30         1          1.0      1.0      0.0          N = 10\n    31                                                   # Do some busy work\n    32        11         12.0      1.1      0.0          for i in range(N):\n    33        10       1091.0    109.1      0.5              x = conv2d(x)\n    34                                           \n    35                                                   # Create a mask\n    36         1         32.0     32.0      0.0          mask = (data > .5).view(-1)\n    37         1          8.0      8.0      0.0          x = x.view(-1)\n    38         1          0.0      0.0      0.0          if do_mask:\n    39                                                       x = x[mask]\n    40                                           \n    41         1          0.0      0.0      0.0          if do_sync:\n    42                                                       torch.cuda.synchronize()\n    43                                           \n    44                                                   # Do more busy work\n    45        11         16.0      1.5      0.0          for i in range(N):\n    46        10        254.0     25.4      0.1              x = x * x\n    47        10       5359.0    535.9      2.3              x = torch.sqrt(x)\n    48                                           \n    49         1     226486.0 226486.0     97.1          torch.cuda.synchronize()\nIn this case 97% of the time is taken up by the final synchronize operation, which means that no intermediate syncing has taken place, all the work is done in a lazy fashion, so the final sync eats up all the time.\nFinally the final line profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=True), shows that if we force an intermediate sync (without masking), we get a similar performance time as if we did the mask.\n---------\n * do_mask = False\n * do_sync = True\nTimer unit: 1e-06 s\n\nTotal time: 0.233516 s\nFile: /home/joncrall/_torch_sync_mwe.py\nFunction: test_syncpoint_mask at line 22\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    22                                               def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\n    23         1          8.0      8.0      0.0          print('---------')\n    24         1          7.0      7.0      0.0          print(' * do_mask = {!r}'.format(do_mask))\n    25         1          5.0      5.0      0.0          print(' * do_sync = {!r}'.format(do_sync))\n    26                                           \n    27         1         13.0     13.0      0.0          torch.cuda.synchronize()\n    28                                           \n    29         1          1.0      1.0      0.0          x = data\n    30         1          0.0      0.0      0.0          N = 10\n    31                                                   # Do some busy work\n    32        11         14.0      1.3      0.0          for i in range(N):\n    33        10       1093.0    109.3      0.5              x = conv2d(x)\n    34                                           \n    35                                                   # Create a mask\n    36         1         34.0     34.0      0.0          mask = (data > .5).view(-1)\n    37         1          8.0      8.0      0.0          x = x.view(-1)\n    38         1          0.0      0.0      0.0          if do_mask:\n    39                                                       x = x[mask]\n    40                                           \n    41         1          1.0      1.0      0.0          if do_sync:\n    42         1     183895.0 183895.0     78.8              torch.cuda.synchronize()\n    43                                           \n    44                                                   # Do more busy work\n    45        11         10.0      0.9      0.0          for i in range(N):\n    46        10        220.0     22.0      0.1              x = x * x\n    47        10        232.0     23.2      0.1              x = torch.sqrt(x)\n    48                                           \n    49         1      47975.0  47975.0     20.5          torch.cuda.synchronize()\nIn this case we see if we replace the mask with a explicit sync, that takes up 78% of the time, which is similar to the 84% of the time we saw with the masking operation.\n\nI test this for a few other operations as well\n    # --- There seem to be a few operations that cause implicity syncs\n\n    def test_synpoint_other(opname, op):\n        def test_syncpoint_(data, conv2d):\n            print('---------')\n            print('opname = {!r}'.format(opname))\n            torch.cuda.synchronize()\n            x = data\n            N = 10\n            for i in range(N):\n                x = conv2d(x)  # non-syncing busy work\n            op(x)\n            # Do more busy work\n            for i in range(N):\n                x = torch.sqrt(x * x)  # non-syncing busy work\n            torch.cuda.synchronize()\n        profile_onthefly(test_syncpoint_)(data, conv2d)\n\n    test_synpoint_other('sum', lambda x: x.sum())\n    test_synpoint_other('sum(dim=0)', lambda x: x.sum(dim=0))\n    test_synpoint_other('sigmoid', lambda x: x.sigmoid())\nA simple sum will cause an  implicit sync\nopname = 'sum'\nTimer unit: 1e-06 s\n\nTotal time: 0.221028 s\nFile: /home/joncrall/_torch_sync_mwe.py\nFunction: test_syncpoint_ at line 68\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    68                                                   def test_syncpoint_(data, conv2d):\n    69         1          9.0      9.0      0.0              print('---------')\n    70         1          8.0      8.0      0.0              print('opname = {!r}'.format(opname))\n    71         1         13.0     13.0      0.0              torch.cuda.synchronize()\n    72         1          1.0      1.0      0.0              x = data\n    73         1          1.0      1.0      0.0              N = 10\n    74        11         12.0      1.1      0.0              for i in range(N):\n    75        10       1148.0    114.8      0.5                  x = conv2d(x)  # non-syncing busy work\n    76         1     174351.0 174351.0     78.9              op(x)\n    77                                                       # Do more busy work\n    78        11          9.0      0.8      0.0              for i in range(N):\n    79        10        455.0     45.5      0.2                  x = torch.sqrt(x * x)  # non-syncing busy work\n    80         1      45021.0  45021.0     20.4              torch.cuda.synchronize()\nHowever, a sum(dim=0) and a sigmoid call will not.\n---------\nopname = 'sum(dim=0)'\nTimer unit: 1e-06 s\n\nTotal time: 0.221 s\nFile: /home/joncrall/_torch_sync_mwe.py\nFunction: test_syncpoint_ at line 68\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    68                                                   def test_syncpoint_(data, conv2d):\n    69         1          8.0      8.0      0.0              print('---------')\n    70         1          7.0      7.0      0.0              print('opname = {!r}'.format(opname))\n    71         1         13.0     13.0      0.0              torch.cuda.synchronize()\n    72         1          1.0      1.0      0.0              x = data\n    73         1          1.0      1.0      0.0              N = 10\n    74        11         11.0      1.0      0.0              for i in range(N):\n    75        10       1099.0    109.9      0.5                  x = conv2d(x)  # non-syncing busy work\n    76         1         92.0     92.0      0.0              op(x)\n    77                                                       # Do more busy work\n    78        11         10.0      0.9      0.0              for i in range(N):\n    79        10        421.0     42.1      0.2                  x = torch.sqrt(x * x)  # non-syncing busy work\n    80         1     219337.0 219337.0     99.2              torch.cuda.synchronize()\n\n\n---------\nopname = 'sigmoid'\nTimer unit: 1e-06 s\n\nTotal time: 0.222286 s\nFile: /home/joncrall/_torch_sync_mwe.py\nFunction: test_syncpoint_ at line 68\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    68                                                   def test_syncpoint_(data, conv2d):\n    69         1         24.0     24.0      0.0              print('---------')\n    70         1         24.0     24.0      0.0              print('opname = {!r}'.format(opname))\n    71         1         13.0     13.0      0.0              torch.cuda.synchronize()\n    72         1          0.0      0.0      0.0              x = data\n    73         1          1.0      1.0      0.0              N = 10\n    74        11         12.0      1.1      0.0              for i in range(N):\n    75        10       1088.0    108.8      0.5                  x = conv2d(x)  # non-syncing busy work\n    76         1         83.0     83.0      0.0              op(x)\n    77                                                       # Do more busy work\n    78        11         11.0      1.0      0.0              for i in range(N):\n    79        10        419.0     41.9      0.2                  x = torch.sqrt(x * x)  # non-syncing busy work\n    80         1     220611.0 220611.0     99.2              torch.cuda.synchronize()\nInstead of increasing the length of this report, I'll simply summarize the rest.\nOps that did cause implicit sync:\n\nmasked_select\nnonzero\nsum(dim=None)\n\nOps that did not cause implicit sync:\n\nsum(dim=)\nconv\nreshape\nsqueeze\nsigmoid\narithmetic operations + - / * \n\nExpected behavior\nObviously there are lots of operations I didn't test, so it would be nice to have an understanding of when I should expect an operation to cause an implicit sync and when I shouldn't.\nI expect its something to do with operations which have clearly defined input and output shapes, but having something more than my hunches would be nice. Also, perhaps this is a bug, and masked_select should actually not force a sync.\nEnvironment\nPyTorch version: 1.0.0.dev20181008\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\n\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.148\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\n\nNvidia driver version: 396.54\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen profiling networks I've found that certain functions seem to cause the CUDA backend to synchronize with the host. \r\n\r\nThis is similar to #1989 but is applicable to a wider variety of functions, which I believe shouldn't cause syncs, but they seem to. \r\n\r\nEven if these operations do implicitly cause syncs it would be nice to have a list of which functions do this in the docs (or perhaps someone can point me to this if this already exists). \r\n\r\n## To Reproduce\r\n\r\nI have a MWE showing several functions that do and do not cause implicit sync points. The basic idea is I create some data, and run it through some busy work operations that do not cause syncing (i.e. convolutions), then I run it through an operation to test if it syncs. Finally, I run it through a few more busy work ops and then force a synchronization at the end. I run a line profile to check how long each call takes. If the operation causes a sync, then it will show up as taking time in the profiler, otherwise if there is no implicit syncing, then the bulk of time should be taken up by the final explicit sync. \r\n\r\nThe operation that I test first is uint8 masking (i.e. masked_select / __getitem__) (which is how I first uncovered the issue)\r\n\r\n```python\r\nimport torch\r\n\r\n\r\ndef main():\r\n\r\n    def profile_onthefly(func):\r\n        def _wrapper(*args, **kw):\r\n            import line_profiler\r\n            from six.moves import cStringIO\r\n            profile = line_profiler.LineProfiler()\r\n\r\n            result = profile(func)(*args, **kw)\r\n\r\n            file_ = cStringIO()\r\n            profile.print_stats(stream=file_, stripzeros=True)\r\n            file_.seek(0)\r\n            text = file_.read()\r\n            print(text)\r\n            return result\r\n        return _wrapper\r\n\r\n    def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\r\n        print('---------')\r\n        print(' * do_mask = {!r}'.format(do_mask))\r\n        print(' * do_sync = {!r}'.format(do_sync))\r\n\r\n        torch.cuda.synchronize()\r\n\r\n        x = data\r\n        N = 10\r\n        # Do some busy work\r\n        for i in range(N):\r\n            x = conv2d(x)\r\n\r\n        # Create a mask\r\n        mask = (data > .5).view(-1)\r\n        x = x.view(-1)\r\n        if do_mask:\r\n            x = x[mask]\r\n\r\n        if do_sync:\r\n            torch.cuda.synchronize()\r\n\r\n        # Do more busy work\r\n        for i in range(N):\r\n            x = x * x\r\n            x = torch.sqrt(x)\r\n\r\n        torch.cuda.synchronize()\r\n\r\n    xpu = torch.device(0)\r\n    # Setup dummy data\r\n    bsize = 128\r\n    data = torch.rand(bsize, 3, 512, 512).to(xpu)\r\n    conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1).to(xpu)\r\n\r\n    # Profile to show how masking causes an implicity sync-point\r\n\r\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=True, do_sync=False)\r\n\r\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=False)\r\n\r\n    profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=True)\r\n```\r\n\r\nThe final three lines run the test function in 3 different ways. \r\n\r\nThe first line (`profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=True, do_sync=False)`) shows that if we do a mask, but we don't do an internal sync, then there is a bulk of time taken up by the masking operation (suggesting that an implicit sync is occurring). \r\n\r\n```python\r\n---------\r\n * do_mask = True\r\n * do_sync = False\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.261704 s\r\nFile: /home/joncrall/_torch_sync_mwe.py\r\nFunction: test_syncpoint_mask at line 22\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    22                                               def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\r\n    23         1         27.0     27.0      0.0          print('---------')\r\n    24         1          8.0      8.0      0.0          print(' * do_mask = {!r}'.format(do_mask))\r\n    25         1          4.0      4.0      0.0          print(' * do_sync = {!r}'.format(do_sync))\r\n    26                                           \r\n    27         1         88.0     88.0      0.0          torch.cuda.synchronize()\r\n    28                                           \r\n    29         1          1.0      1.0      0.0          x = data\r\n    30         1          1.0      1.0      0.0          N = 10\r\n    31                                                   # Do some busy work\r\n    32        11         15.0      1.4      0.0          for i in range(N):\r\n    33        10      10231.0   1023.1      3.9              x = conv2d(x)\r\n    34                                           \r\n    35                                                   # Create a mask\r\n    36         1        455.0    455.0      0.2          mask = (data > .5).view(-1)\r\n    37         1         16.0     16.0      0.0          x = x.view(-1)\r\n    38         1          1.0      1.0      0.0          if do_mask:\r\n    39         1     222128.0 222128.0     84.9              x = x[mask]\r\n    40                                           \r\n    41         1          1.0      1.0      0.0          if do_sync:\r\n    42                                                       torch.cuda.synchronize()\r\n    43                                           \r\n    44                                                   # Do more busy work\r\n    45        11         13.0      1.2      0.0          for i in range(N):\r\n    46        10        297.0     29.7      0.1              x = x * x\r\n    47        10       2895.0    289.5      1.1              x = torch.sqrt(x)\r\n    48                                           \r\n    49         1      25523.0  25523.0      9.8          torch.cuda.synchronize()\r\n```\r\nWe can see that in this case 84.9% of the time is spent on the mask operation because it is actually waiting for all those convolutions to finish.\r\n\r\n\r\nThe next line (`profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=False)`), verifies that if we do neither a mask or a sync, then the majority of time is indeed taken up by the final explicit sync call. \r\n\r\n```python\r\n---------\r\n * do_mask = False\r\n * do_sync = False\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.233294 s\r\nFile: /home/joncrall/_torch_sync_mwe.py\r\nFunction: test_syncpoint_mask at line 22\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    22                                               def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\r\n    23         1          8.0      8.0      0.0          print('---------')\r\n    24         1          7.0      7.0      0.0          print(' * do_mask = {!r}'.format(do_mask))\r\n    25         1          5.0      5.0      0.0          print(' * do_sync = {!r}'.format(do_sync))\r\n    26                                           \r\n    27         1         14.0     14.0      0.0          torch.cuda.synchronize()\r\n    28                                           \r\n    29         1          1.0      1.0      0.0          x = data\r\n    30         1          1.0      1.0      0.0          N = 10\r\n    31                                                   # Do some busy work\r\n    32        11         12.0      1.1      0.0          for i in range(N):\r\n    33        10       1091.0    109.1      0.5              x = conv2d(x)\r\n    34                                           \r\n    35                                                   # Create a mask\r\n    36         1         32.0     32.0      0.0          mask = (data > .5).view(-1)\r\n    37         1          8.0      8.0      0.0          x = x.view(-1)\r\n    38         1          0.0      0.0      0.0          if do_mask:\r\n    39                                                       x = x[mask]\r\n    40                                           \r\n    41         1          0.0      0.0      0.0          if do_sync:\r\n    42                                                       torch.cuda.synchronize()\r\n    43                                           \r\n    44                                                   # Do more busy work\r\n    45        11         16.0      1.5      0.0          for i in range(N):\r\n    46        10        254.0     25.4      0.1              x = x * x\r\n    47        10       5359.0    535.9      2.3              x = torch.sqrt(x)\r\n    48                                           \r\n    49         1     226486.0 226486.0     97.1          torch.cuda.synchronize()\r\n```\r\nIn this case 97% of the time is taken up by the final synchronize operation, which means that no intermediate syncing has taken place, all the work is done in a lazy fashion, so the final sync eats up all the time. \r\n\r\n\r\nFinally the final line `profile_onthefly(test_syncpoint_mask)(data, conv2d, do_mask=False, do_sync=True)`, shows that if we force an intermediate sync (without masking), we get a similar performance time as if we did the mask. \r\n\r\n```python\r\n---------\r\n * do_mask = False\r\n * do_sync = True\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.233516 s\r\nFile: /home/joncrall/_torch_sync_mwe.py\r\nFunction: test_syncpoint_mask at line 22\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    22                                               def test_syncpoint_mask(data, conv2d, do_mask=False, do_sync=False):\r\n    23         1          8.0      8.0      0.0          print('---------')\r\n    24         1          7.0      7.0      0.0          print(' * do_mask = {!r}'.format(do_mask))\r\n    25         1          5.0      5.0      0.0          print(' * do_sync = {!r}'.format(do_sync))\r\n    26                                           \r\n    27         1         13.0     13.0      0.0          torch.cuda.synchronize()\r\n    28                                           \r\n    29         1          1.0      1.0      0.0          x = data\r\n    30         1          0.0      0.0      0.0          N = 10\r\n    31                                                   # Do some busy work\r\n    32        11         14.0      1.3      0.0          for i in range(N):\r\n    33        10       1093.0    109.3      0.5              x = conv2d(x)\r\n    34                                           \r\n    35                                                   # Create a mask\r\n    36         1         34.0     34.0      0.0          mask = (data > .5).view(-1)\r\n    37         1          8.0      8.0      0.0          x = x.view(-1)\r\n    38         1          0.0      0.0      0.0          if do_mask:\r\n    39                                                       x = x[mask]\r\n    40                                           \r\n    41         1          1.0      1.0      0.0          if do_sync:\r\n    42         1     183895.0 183895.0     78.8              torch.cuda.synchronize()\r\n    43                                           \r\n    44                                                   # Do more busy work\r\n    45        11         10.0      0.9      0.0          for i in range(N):\r\n    46        10        220.0     22.0      0.1              x = x * x\r\n    47        10        232.0     23.2      0.1              x = torch.sqrt(x)\r\n    48                                           \r\n    49         1      47975.0  47975.0     20.5          torch.cuda.synchronize()\r\n```\r\nIn this case we see if we replace the mask with a explicit sync, that takes up 78% of the time, which is similar to the 84% of the time we saw with the masking operation. \r\n\r\n------------------------------\r\n\r\nI test this for a few other operations as well\r\n\r\n```python\r\n    # --- There seem to be a few operations that cause implicity syncs\r\n\r\n    def test_synpoint_other(opname, op):\r\n        def test_syncpoint_(data, conv2d):\r\n            print('---------')\r\n            print('opname = {!r}'.format(opname))\r\n            torch.cuda.synchronize()\r\n            x = data\r\n            N = 10\r\n            for i in range(N):\r\n                x = conv2d(x)  # non-syncing busy work\r\n            op(x)\r\n            # Do more busy work\r\n            for i in range(N):\r\n                x = torch.sqrt(x * x)  # non-syncing busy work\r\n            torch.cuda.synchronize()\r\n        profile_onthefly(test_syncpoint_)(data, conv2d)\r\n\r\n    test_synpoint_other('sum', lambda x: x.sum())\r\n    test_synpoint_other('sum(dim=0)', lambda x: x.sum(dim=0))\r\n    test_synpoint_other('sigmoid', lambda x: x.sigmoid())\r\n```\r\n\r\nA simple sum will cause an  implicit sync\r\n```python\r\nopname = 'sum'\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.221028 s\r\nFile: /home/joncrall/_torch_sync_mwe.py\r\nFunction: test_syncpoint_ at line 68\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    68                                                   def test_syncpoint_(data, conv2d):\r\n    69         1          9.0      9.0      0.0              print('---------')\r\n    70         1          8.0      8.0      0.0              print('opname = {!r}'.format(opname))\r\n    71         1         13.0     13.0      0.0              torch.cuda.synchronize()\r\n    72         1          1.0      1.0      0.0              x = data\r\n    73         1          1.0      1.0      0.0              N = 10\r\n    74        11         12.0      1.1      0.0              for i in range(N):\r\n    75        10       1148.0    114.8      0.5                  x = conv2d(x)  # non-syncing busy work\r\n    76         1     174351.0 174351.0     78.9              op(x)\r\n    77                                                       # Do more busy work\r\n    78        11          9.0      0.8      0.0              for i in range(N):\r\n    79        10        455.0     45.5      0.2                  x = torch.sqrt(x * x)  # non-syncing busy work\r\n    80         1      45021.0  45021.0     20.4              torch.cuda.synchronize()\r\n```\r\n\r\nHowever, a `sum(dim=0)` and a `sigmoid` call will not. \r\n\r\n```python\r\n---------\r\nopname = 'sum(dim=0)'\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.221 s\r\nFile: /home/joncrall/_torch_sync_mwe.py\r\nFunction: test_syncpoint_ at line 68\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    68                                                   def test_syncpoint_(data, conv2d):\r\n    69         1          8.0      8.0      0.0              print('---------')\r\n    70         1          7.0      7.0      0.0              print('opname = {!r}'.format(opname))\r\n    71         1         13.0     13.0      0.0              torch.cuda.synchronize()\r\n    72         1          1.0      1.0      0.0              x = data\r\n    73         1          1.0      1.0      0.0              N = 10\r\n    74        11         11.0      1.0      0.0              for i in range(N):\r\n    75        10       1099.0    109.9      0.5                  x = conv2d(x)  # non-syncing busy work\r\n    76         1         92.0     92.0      0.0              op(x)\r\n    77                                                       # Do more busy work\r\n    78        11         10.0      0.9      0.0              for i in range(N):\r\n    79        10        421.0     42.1      0.2                  x = torch.sqrt(x * x)  # non-syncing busy work\r\n    80         1     219337.0 219337.0     99.2              torch.cuda.synchronize()\r\n\r\n\r\n---------\r\nopname = 'sigmoid'\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.222286 s\r\nFile: /home/joncrall/_torch_sync_mwe.py\r\nFunction: test_syncpoint_ at line 68\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    68                                                   def test_syncpoint_(data, conv2d):\r\n    69         1         24.0     24.0      0.0              print('---------')\r\n    70         1         24.0     24.0      0.0              print('opname = {!r}'.format(opname))\r\n    71         1         13.0     13.0      0.0              torch.cuda.synchronize()\r\n    72         1          0.0      0.0      0.0              x = data\r\n    73         1          1.0      1.0      0.0              N = 10\r\n    74        11         12.0      1.1      0.0              for i in range(N):\r\n    75        10       1088.0    108.8      0.5                  x = conv2d(x)  # non-syncing busy work\r\n    76         1         83.0     83.0      0.0              op(x)\r\n    77                                                       # Do more busy work\r\n    78        11         11.0      1.0      0.0              for i in range(N):\r\n    79        10        419.0     41.9      0.2                  x = torch.sqrt(x * x)  # non-syncing busy work\r\n    80         1     220611.0 220611.0     99.2              torch.cuda.synchronize()\r\n```\r\n\r\nInstead of increasing the length of this report, I'll simply summarize the rest. \r\n\r\nOps that did cause implicit sync:\r\n* masked_select\r\n* nonzero\r\n* sum(dim=None)\r\n\r\nOps that did not cause implicit sync:\r\n* sum(dim=<Int>)\r\n* conv\r\n* reshape\r\n* squeeze\r\n* sigmoid\r\n* arithmetic operations `+ - / * `\r\n\r\n## Expected behavior\r\n\r\nObviously there are lots of operations I didn't test, so it would be nice to have an understanding of when I should expect an operation to cause an implicit sync and when I shouldn't.\r\n\r\nI expect its something to do with operations which have clearly defined input and output shapes, but having something more than my hunches would be nice. Also, perhaps this is a bug, and masked_select should actually not force a sync. \r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0.dev20181008\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 396.54\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```"}