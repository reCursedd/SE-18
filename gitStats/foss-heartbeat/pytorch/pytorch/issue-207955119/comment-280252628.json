{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/280252628", "html_url": "https://github.com/pytorch/pytorch/pull/753#issuecomment-280252628", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/753", "id": 280252628, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDI1MjYyOA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T07:24:13Z", "updated_at": "2017-02-16T07:24:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>,  replicate replicates the module and does not care about inputs. Scatter already handles tuple input:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L5-L13\">https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L5-L13</a><br>\nso tuple input already works on multiple GPUs. Issue arises when you wrap a model in DataParallel and try to run it on a single GPU. Or did I misunderstand you?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>, I'll add unit test.</p>", "body_text": "@apaszke,  replicate replicates the module and does not care about inputs. Scatter already handles tuple input:\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L5-L13\nso tuple input already works on multiple GPUs. Issue arises when you wrap a model in DataParallel and try to run it on a single GPU. Or did I misunderstand you?\n@soumith, I'll add unit test.", "body": "@apaszke,  replicate replicates the module and does not care about inputs. Scatter already handles tuple input: \r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L5-L13\r\nso tuple input already works on multiple GPUs. Issue arises when you wrap a model in DataParallel and try to run it on a single GPU. Or did I misunderstand you?\r\n@soumith, I'll add unit test. "}