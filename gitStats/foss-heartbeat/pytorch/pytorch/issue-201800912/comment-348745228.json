{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348745228", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-348745228", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 348745228, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODc0NTIyOA==", "user": {"login": "peterjc123", "id": 9998726, "node_id": "MDQ6VXNlcjk5OTg3MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9998726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterjc123", "html_url": "https://github.com/peterjc123", "followers_url": "https://api.github.com/users/peterjc123/followers", "following_url": "https://api.github.com/users/peterjc123/following{/other_user}", "gists_url": "https://api.github.com/users/peterjc123/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterjc123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterjc123/subscriptions", "organizations_url": "https://api.github.com/users/peterjc123/orgs", "repos_url": "https://api.github.com/users/peterjc123/repos", "events_url": "https://api.github.com/users/peterjc123/events{/privacy}", "received_events_url": "https://api.github.com/users/peterjc123/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-03T07:08:32Z", "updated_at": "2017-12-03T07:08:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8689423\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/CodeJjang\">@CodeJjang</a> It's not broken. It's just slow. However, I'll copy it here for you to see.<br>\nThe document of CUDA says that</p>\n<blockquote>\n<p>IPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality is not supported on Tegra platforms.</p>\n</blockquote>\n<p>So your code is not possible to run in Windows since you tend to share data on GPU from one process to another process with the usage of cudaIpcOpenMemHandle. There're two ways to solve it.</p>\n<ol>\n<li>Don't share Tensors. Set num_workers to 0.</li>\n<li>Share CPU Tensors instead. Don't return CUDA tensors in your data loader.</li>\n</ol>", "body_text": "@CodeJjang It's not broken. It's just slow. However, I'll copy it here for you to see.\nThe document of CUDA says that\n\nIPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality is not supported on Tegra platforms.\n\nSo your code is not possible to run in Windows since you tend to share data on GPU from one process to another process with the usage of cudaIpcOpenMemHandle. There're two ways to solve it.\n\nDon't share Tensors. Set num_workers to 0.\nShare CPU Tensors instead. Don't return CUDA tensors in your data loader.", "body": "@CodeJjang It's not broken. It's just slow. However, I'll copy it here for you to see.\r\nThe document of CUDA says that\r\n> IPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality is not supported on Tegra platforms.\r\n\r\nSo your code is not possible to run in Windows since you tend to share data on GPU from one process to another process with the usage of cudaIpcOpenMemHandle. There're two ways to solve it.\r\n1. Don't share Tensors. Set num_workers to 0.\r\n2. Share CPU Tensors instead. Don't return CUDA tensors in your data loader."}