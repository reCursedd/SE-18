{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342346284", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-342346284", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 342346284, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjM0NjI4NA==", "user": {"login": "dkc2117", "id": 5635743, "node_id": "MDQ6VXNlcjU2MzU3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5635743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dkc2117", "html_url": "https://github.com/dkc2117", "followers_url": "https://api.github.com/users/dkc2117/followers", "following_url": "https://api.github.com/users/dkc2117/following{/other_user}", "gists_url": "https://api.github.com/users/dkc2117/gists{/gist_id}", "starred_url": "https://api.github.com/users/dkc2117/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dkc2117/subscriptions", "organizations_url": "https://api.github.com/users/dkc2117/orgs", "repos_url": "https://api.github.com/users/dkc2117/repos", "events_url": "https://api.github.com/users/dkc2117/events{/privacy}", "received_events_url": "https://api.github.com/users/dkc2117/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-07T01:32:43Z", "updated_at": "2017-11-07T01:39:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a></p>\n<p>I am running the main portion of my code (e.g. in my example code, everything below \"# various code omitted for brevity\") within a __main__ and still get the CUDA runtime error. I will include exact code of a single py file with no omissions that can reproduce the error on my system.</p>\n<p>I believe I installed via conda without having to specify a whl file (e.g. I did \"conda install -c peterjc123 pytorch\"). Windows 10 pro with latest patches. Python 3.6.1. Is there other information I can provide that would be helpful?</p>\n<p>Per \"pip freeze\":<br>\ntorch==0.2.1+a4fc05a</p>\n<p>Per \"conda list\"<br>\npytorch                   0.2.1           py36he6bf560_0.2.1cu80    peterjc123</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> Dataset\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> With USE_CUDA = True, I receive \"cuda runtime error (63) : OS call failed...\" and never reach \"hello world\"</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If I instead set USE_CUDA = False, then the console will print \"hello world\" instead of an error</span>\n<span class=\"pl-c1\">USE_CUDA</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">USE_CUDA</span>:\n\ttorch_fType <span class=\"pl-k\">=</span> torch.cuda.FloatTensor\n\ttorch_iType <span class=\"pl-k\">=</span> torch.cuda.LongTensor\n<span class=\"pl-k\">else</span>:\n\ttorch_fType <span class=\"pl-k\">=</span> torch.FloatTensor\n\ttorch_iType <span class=\"pl-k\">=</span> torch.LongTensor\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CBOWWindowedTextDataset</span>(<span class=\"pl-e\">Dataset</span>):\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>orig_vocabulary_as_ids should be a list of int</span>\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">orig_vocabulary_as_ids</span>, <span class=\"pl-smi\">max_context_distance</span>):\n\t\t<span class=\"pl-c1\">super</span>()\n\t\t<span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids <span class=\"pl-k\">=</span> orig_vocabulary_as_ids\n\t\t<span class=\"pl-c1\">self</span>.max_context_distance <span class=\"pl-k\">=</span> max_context_distance\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.max_context_distance, <span class=\"pl-c1\">0</span>)\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n\t\twindow <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx:idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>:idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>]\n\t\ttarget <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance]\n\t\t<span class=\"pl-k\">return</span> torch_iType(window), target\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n\t<span class=\"pl-c1\">MAX_CONTEXT_DISTANCE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\t<span class=\"pl-c1\">MINIBATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\n\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> any old ids will do just to reproduce error...</span>\n\torig_vocabulary_as_ids <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10000</span>))\n\n\tdataset <span class=\"pl-k\">=</span> CBOWWindowedTextDataset(\n\t\t<span class=\"pl-v\">orig_vocabulary_as_ids</span><span class=\"pl-k\">=</span>orig_vocabulary_as_ids,\n\t\t<span class=\"pl-v\">max_context_distance</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MAX_CONTEXT_DISTANCE</span>,\n\t)\n\n\t<span class=\"pl-k\">if</span> <span class=\"pl-c1\">USE_CUDA</span>:\n\t\tnum_workers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n\t<span class=\"pl-k\">else</span>:\n\t\tnum_workers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\n\tdata_loader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MINIBATCH_SIZE</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>num_workers)\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>##code gets here without error</span>\n\t<span class=\"pl-k\">for</span> batch_idx, (windows, targets) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(data_loader):\n\t\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>## error occurs before this line executes, so when starting enumeration...</span>\n\t\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>hello world<span class=\"pl-pds\">'</span></span>)\n</pre></div>", "body_text": "@peterjc123\nI am running the main portion of my code (e.g. in my example code, everything below \"# various code omitted for brevity\") within a __main__ and still get the CUDA runtime error. I will include exact code of a single py file with no omissions that can reproduce the error on my system.\nI believe I installed via conda without having to specify a whl file (e.g. I did \"conda install -c peterjc123 pytorch\"). Windows 10 pro with latest patches. Python 3.6.1. Is there other information I can provide that would be helpful?\nPer \"pip freeze\":\ntorch==0.2.1+a4fc05a\nPer \"conda list\"\npytorch                   0.2.1           py36he6bf560_0.2.1cu80    peterjc123\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\n# With USE_CUDA = True, I receive \"cuda runtime error (63) : OS call failed...\" and never reach \"hello world\"\n# If I instead set USE_CUDA = False, then the console will print \"hello world\" instead of an error\nUSE_CUDA = True\n\nif USE_CUDA:\n\ttorch_fType = torch.cuda.FloatTensor\n\ttorch_iType = torch.cuda.LongTensor\nelse:\n\ttorch_fType = torch.FloatTensor\n\ttorch_iType = torch.LongTensor\n\nclass CBOWWindowedTextDataset(Dataset):\n\t#orig_vocabulary_as_ids should be a list of int\n\tdef __init__(self, orig_vocabulary_as_ids, max_context_distance):\n\t\tsuper()\n\t\tself.orig_vocabulary_as_ids = orig_vocabulary_as_ids\n\t\tself.max_context_distance = max_context_distance\n\n\tdef __len__(self):\n\t\treturn max(len(self.orig_vocabulary_as_ids) - 2*self.max_context_distance, 0)\n\n\tdef __getitem__(self, idx):\n\t\twindow = self.orig_vocabulary_as_ids[idx:idx+self.max_context_distance] + self.orig_vocabulary_as_ids[idx+self.max_context_distance+1:idx+self.max_context_distance*2+1]\n\t\ttarget = self.orig_vocabulary_as_ids[idx+self.max_context_distance]\n\t\treturn torch_iType(window), target\n\nif __name__ == '__main__':\n\tMAX_CONTEXT_DISTANCE = 4\n\tMINIBATCH_SIZE = 64\n\n\t# any old ids will do just to reproduce error...\n\torig_vocabulary_as_ids = list(range(1,10000))\n\n\tdataset = CBOWWindowedTextDataset(\n\t\torig_vocabulary_as_ids=orig_vocabulary_as_ids,\n\t\tmax_context_distance=MAX_CONTEXT_DISTANCE,\n\t)\n\n\tif USE_CUDA:\n\t\tnum_workers = 1\n\telse:\n\t\tnum_workers = 4\n\n\tdata_loader = DataLoader(dataset, batch_size=MINIBATCH_SIZE, num_workers=num_workers)\n\t###code gets here without error\n\tfor batch_idx, (windows, targets) in enumerate(data_loader):\n\t\t### error occurs before this line executes, so when starting enumeration...\n\t\tprint('hello world')", "body": "@peterjc123 \r\n\r\nI am running the main portion of my code (e.g. in my example code, everything below \"# various code omitted for brevity\") within a \\_\\_main\\_\\_ and still get the CUDA runtime error. I will include exact code of a single py file with no omissions that can reproduce the error on my system.\r\n\r\nI believe I installed via conda without having to specify a whl file (e.g. I did \"conda install -c peterjc123 pytorch\"). Windows 10 pro with latest patches. Python 3.6.1. Is there other information I can provide that would be helpful?\r\n\r\nPer \"pip freeze\":\r\ntorch==0.2.1+a4fc05a\r\n\r\nPer \"conda list\"\r\npytorch                   0.2.1           py36he6bf560_0.2.1cu80    peterjc123\r\n\r\n```python\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import Dataset\r\n\r\n# With USE_CUDA = True, I receive \"cuda runtime error (63) : OS call failed...\" and never reach \"hello world\"\r\n# If I instead set USE_CUDA = False, then the console will print \"hello world\" instead of an error\r\nUSE_CUDA = True\r\n\r\nif USE_CUDA:\r\n\ttorch_fType = torch.cuda.FloatTensor\r\n\ttorch_iType = torch.cuda.LongTensor\r\nelse:\r\n\ttorch_fType = torch.FloatTensor\r\n\ttorch_iType = torch.LongTensor\r\n\r\nclass CBOWWindowedTextDataset(Dataset):\r\n\t#orig_vocabulary_as_ids should be a list of int\r\n\tdef __init__(self, orig_vocabulary_as_ids, max_context_distance):\r\n\t\tsuper()\r\n\t\tself.orig_vocabulary_as_ids = orig_vocabulary_as_ids\r\n\t\tself.max_context_distance = max_context_distance\r\n\r\n\tdef __len__(self):\r\n\t\treturn max(len(self.orig_vocabulary_as_ids) - 2*self.max_context_distance, 0)\r\n\r\n\tdef __getitem__(self, idx):\r\n\t\twindow = self.orig_vocabulary_as_ids[idx:idx+self.max_context_distance] + self.orig_vocabulary_as_ids[idx+self.max_context_distance+1:idx+self.max_context_distance*2+1]\r\n\t\ttarget = self.orig_vocabulary_as_ids[idx+self.max_context_distance]\r\n\t\treturn torch_iType(window), target\r\n\r\nif __name__ == '__main__':\r\n\tMAX_CONTEXT_DISTANCE = 4\r\n\tMINIBATCH_SIZE = 64\r\n\r\n\t# any old ids will do just to reproduce error...\r\n\torig_vocabulary_as_ids = list(range(1,10000))\r\n\r\n\tdataset = CBOWWindowedTextDataset(\r\n\t\torig_vocabulary_as_ids=orig_vocabulary_as_ids,\r\n\t\tmax_context_distance=MAX_CONTEXT_DISTANCE,\r\n\t)\r\n\r\n\tif USE_CUDA:\r\n\t\tnum_workers = 1\r\n\telse:\r\n\t\tnum_workers = 4\r\n\r\n\tdata_loader = DataLoader(dataset, batch_size=MINIBATCH_SIZE, num_workers=num_workers)\r\n\t###code gets here without error\r\n\tfor batch_idx, (windows, targets) in enumerate(data_loader):\r\n\t\t### error occurs before this line executes, so when starting enumeration...\r\n\t\tprint('hello world')\r\n\r\n```\r\n\r\n"}