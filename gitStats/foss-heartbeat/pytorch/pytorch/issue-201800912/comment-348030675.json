{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348030675", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-348030675", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 348030675, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODAzMDY3NQ==", "user": {"login": "SpaceCowboy850", "id": 12373859, "node_id": "MDQ6VXNlcjEyMzczODU5", "avatar_url": "https://avatars2.githubusercontent.com/u/12373859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SpaceCowboy850", "html_url": "https://github.com/SpaceCowboy850", "followers_url": "https://api.github.com/users/SpaceCowboy850/followers", "following_url": "https://api.github.com/users/SpaceCowboy850/following{/other_user}", "gists_url": "https://api.github.com/users/SpaceCowboy850/gists{/gist_id}", "starred_url": "https://api.github.com/users/SpaceCowboy850/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SpaceCowboy850/subscriptions", "organizations_url": "https://api.github.com/users/SpaceCowboy850/orgs", "repos_url": "https://api.github.com/users/SpaceCowboy850/repos", "events_url": "https://api.github.com/users/SpaceCowboy850/events{/privacy}", "received_events_url": "https://api.github.com/users/SpaceCowboy850/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T23:23:48Z", "updated_at": "2017-11-29T23:30:58Z", "author_association": "NONE", "body_html": "<p>Okay, so here is cut down code.  It seems contigent on how many files I have in the .\\validation_data folder.  It currently has about 1010 files totalling some 130MB, and with that, and number workers set to<br>\n1 or more, it crashes.</p>\n<p>If I reduce the number of files down to say, 350, it will complete, and not generate the error above.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a> - I have emailed you the full file if you need further details.</p>\n<pre><code>if __name__ == \"__main__\":\n    class CustomDataSet(data.Dataset):\n        def __init__(self, args, is_cropped, root, replicates=1):\n            print( \"DatasetFromFrameOnlyFolder::__init__ ENTERED\")\n            ...\n            return\n\n        def __getitem__(self, index):\n            print( \"DatasetFromFrameOnlyFolder::__getitem__ ENTERED\")\n            ...\n            return [input_images, torch.zeros(1)], [target_image, torch.zeros(1)]\n\n        def __len__(self):\n            return self.size * self.replicates\n\n    # Dynamically load the dataset class with parameters passed in via \"--training_dataset_[param]=[value]\" arguments\n    args.effective_batch_size = args.batch_size\n\n    gpuargs = {'num_workers': args.number_workers, 'pin_memory': True} if args.cuda else {}\n\n    validation_dataset = CustomDataSet(args, True, \".\\\\validation_data\")\n    validation_loader = DataLoader(validation_dataset, batch_size=args.effective_batch_size, shuffle=False, **gpuargs)\n\n    # BROKEN LINE\n    IBREAKYOU = validation_loader.__iter__()\n\nprint(\"\\n\")\nsys.stdout.flush()\nos._exit(0)\n</code></pre>", "body_text": "Okay, so here is cut down code.  It seems contigent on how many files I have in the .\\validation_data folder.  It currently has about 1010 files totalling some 130MB, and with that, and number workers set to\n1 or more, it crashes.\nIf I reduce the number of files down to say, 350, it will complete, and not generate the error above.\n@peterjc123 - I have emailed you the full file if you need further details.\nif __name__ == \"__main__\":\n    class CustomDataSet(data.Dataset):\n        def __init__(self, args, is_cropped, root, replicates=1):\n            print( \"DatasetFromFrameOnlyFolder::__init__ ENTERED\")\n            ...\n            return\n\n        def __getitem__(self, index):\n            print( \"DatasetFromFrameOnlyFolder::__getitem__ ENTERED\")\n            ...\n            return [input_images, torch.zeros(1)], [target_image, torch.zeros(1)]\n\n        def __len__(self):\n            return self.size * self.replicates\n\n    # Dynamically load the dataset class with parameters passed in via \"--training_dataset_[param]=[value]\" arguments\n    args.effective_batch_size = args.batch_size\n\n    gpuargs = {'num_workers': args.number_workers, 'pin_memory': True} if args.cuda else {}\n\n    validation_dataset = CustomDataSet(args, True, \".\\\\validation_data\")\n    validation_loader = DataLoader(validation_dataset, batch_size=args.effective_batch_size, shuffle=False, **gpuargs)\n\n    # BROKEN LINE\n    IBREAKYOU = validation_loader.__iter__()\n\nprint(\"\\n\")\nsys.stdout.flush()\nos._exit(0)", "body": "Okay, so here is cut down code.  It seems contigent on how many files I have in the .\\validation_data folder.  It currently has about 1010 files totalling some 130MB, and with that, and number workers set to \r\n1 or more, it crashes.\r\n\r\nIf I reduce the number of files down to say, 350, it will complete, and not generate the error above.\r\n\r\n@peterjc123 - I have emailed you the full file if you need further details.\r\n```\r\nif __name__ == \"__main__\":\r\n    class CustomDataSet(data.Dataset):\r\n        def __init__(self, args, is_cropped, root, replicates=1):\r\n            print( \"DatasetFromFrameOnlyFolder::__init__ ENTERED\")\r\n            ...\r\n            return\r\n\r\n        def __getitem__(self, index):\r\n            print( \"DatasetFromFrameOnlyFolder::__getitem__ ENTERED\")\r\n            ...\r\n            return [input_images, torch.zeros(1)], [target_image, torch.zeros(1)]\r\n\r\n        def __len__(self):\r\n            return self.size * self.replicates\r\n\r\n    # Dynamically load the dataset class with parameters passed in via \"--training_dataset_[param]=[value]\" arguments\r\n    args.effective_batch_size = args.batch_size\r\n\r\n    gpuargs = {'num_workers': args.number_workers, 'pin_memory': True} if args.cuda else {}\r\n\r\n    validation_dataset = CustomDataSet(args, True, \".\\\\validation_data\")\r\n    validation_loader = DataLoader(validation_dataset, batch_size=args.effective_batch_size, shuffle=False, **gpuargs)\r\n\r\n    # BROKEN LINE\r\n    IBREAKYOU = validation_loader.__iter__()\r\n\r\nprint(\"\\n\")\r\nsys.stdout.flush()\r\nos._exit(0)\r\n```"}