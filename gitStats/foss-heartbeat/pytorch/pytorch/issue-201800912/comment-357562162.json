{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357562162", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-357562162", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 357562162, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzU2MjE2Mg==", "user": {"login": "peterjc123", "id": 9998726, "node_id": "MDQ6VXNlcjk5OTg3MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9998726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterjc123", "html_url": "https://github.com/peterjc123", "followers_url": "https://api.github.com/users/peterjc123/followers", "following_url": "https://api.github.com/users/peterjc123/following{/other_user}", "gists_url": "https://api.github.com/users/peterjc123/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterjc123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterjc123/subscriptions", "organizations_url": "https://api.github.com/users/peterjc123/orgs", "repos_url": "https://api.github.com/users/peterjc123/repos", "events_url": "https://api.github.com/users/peterjc123/events{/privacy}", "received_events_url": "https://api.github.com/users/peterjc123/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T01:38:16Z", "updated_at": "2018-01-15T01:49:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5242016\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dreness\">@dreness</a> By setting <code>num_worker</code> to 0, you just turned multiprocessing off. That's why it doesn't throw exceptions. However, it is not a final solution, because it will slow down the data loading like <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> said. Multiprocessing of DataLoader in Windows should be fixed. But it's a quite hard work, since the current style is not the best for Windows. In Windows, processes should be preserved rather than forked, because it is so slow to start a new process in Windows. Furthermore, the current DataLoader are facing problems when using with multiprocessing, especially when you set <code>num_worker</code> to a big number. It will be slow to release the memory because the child processes process things too fast. The main process  just cannot process so much data. I'm thinking for a solution, but cannot figure out a good one now.</p>", "body_text": "@dreness By setting num_worker to 0, you just turned multiprocessing off. That's why it doesn't throw exceptions. However, it is not a final solution, because it will slow down the data loading like @SsnL said. Multiprocessing of DataLoader in Windows should be fixed. But it's a quite hard work, since the current style is not the best for Windows. In Windows, processes should be preserved rather than forked, because it is so slow to start a new process in Windows. Furthermore, the current DataLoader are facing problems when using with multiprocessing, especially when you set num_worker to a big number. It will be slow to release the memory because the child processes process things too fast. The main process  just cannot process so much data. I'm thinking for a solution, but cannot figure out a good one now.", "body": "@dreness By setting `num_worker` to 0, you just turned multiprocessing off. That's why it doesn't throw exceptions. However, it is not a final solution, because it will slow down the data loading like @SsnL said. Multiprocessing of DataLoader in Windows should be fixed. But it's a quite hard work, since the current style is not the best for Windows. In Windows, processes should be preserved rather than forked, because it is so slow to start a new process in Windows. Furthermore, the current DataLoader are facing problems when using with multiprocessing, especially when you set `num_worker` to a big number. It will be slow to release the memory because the child processes process things too fast. The main process  just cannot process so much data. I'm thinking for a solution, but cannot figure out a good one now."}