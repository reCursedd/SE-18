{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342829112", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-342829112", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 342829112, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjgyOTExMg==", "user": {"login": "LaceyChen17", "id": 11754923, "node_id": "MDQ6VXNlcjExNzU0OTIz", "avatar_url": "https://avatars3.githubusercontent.com/u/11754923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LaceyChen17", "html_url": "https://github.com/LaceyChen17", "followers_url": "https://api.github.com/users/LaceyChen17/followers", "following_url": "https://api.github.com/users/LaceyChen17/following{/other_user}", "gists_url": "https://api.github.com/users/LaceyChen17/gists{/gist_id}", "starred_url": "https://api.github.com/users/LaceyChen17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LaceyChen17/subscriptions", "organizations_url": "https://api.github.com/users/LaceyChen17/orgs", "repos_url": "https://api.github.com/users/LaceyChen17/repos", "events_url": "https://api.github.com/users/LaceyChen17/events{/privacy}", "received_events_url": "https://api.github.com/users/LaceyChen17/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-08T14:13:15Z", "updated_at": "2017-11-08T14:13:44Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a> hi, I'm trying to use DataParallel for multi-gpu acceleration.  But it crashed on windows. I think there may be something wrong with nccl but it's beyond my ability. Could you plz give me a hand ?</p>\n<pre><code>e:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    222         for hook in self._forward_pre_hooks.values():\n    223             hook(self, input)\n--&gt; 224         result = self.forward(*input, **kwargs)\n    225         for hook in self._forward_hooks.values():\n    226             hook_result = hook(self, input, result)\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py in forward(self, *inputs, **kwargs)\n     57         if len(self.device_ids) == 1:\n     58             return self.module(*inputs[0], **kwargs[0])\n---&gt; 59         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n     60         outputs = self.parallel_apply(replicas, inputs, kwargs)\n     61         return self.gather(outputs, self.output_device)\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py in replicate(self, module, device_ids)\n     62 \n     63     def replicate(self, module, device_ids):\n---&gt; 64         return replicate(module, device_ids)\n     65 \n     66     def scatter(self, inputs, kwargs, device_ids):\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\replicate.py in replicate(network, devices)\n     10     params = list(network.parameters())\n     11     param_indices = {param: idx for idx, param in enumerate(params)}\n---&gt; 12     param_copies = Broadcast(devices)(*params)\n     13     if len(params) &gt; 0:\n     14         param_copies = [param_copies[i:i + len(params)]\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py in forward(self, *inputs)\n     17         self.num_inputs = len(inputs)\n     18         self.input_device = inputs[0].get_device()\n---&gt; 19         outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\n     20         return tuple([t for tensors in outputs for t in tensors])\n     21 \n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\comm.py in broadcast_coalesced(tensors, devices, buffer_size)\n     52     outputs[0].extend(tensors)\n     53     for chunk in _take_tensors(tensors, buffer_size):\n---&gt; 54         results = broadcast(_flatten_tensors(chunk), devices)\n     55         # use the broadcasted tensors for the remaining devices\n     56         for dst, res in zip(outputs[1:], results[1:]):\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\comm.py in broadcast(tensor, devices)\n     17         corresponding to indices from ``devices``.\n     18     \"\"\"\n---&gt; 19     if nccl.is_available([tensor]) and len(set(devices)) == len(devices):\n     20         tensors = [tensor]\n     21         for device in devices[1:]:\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\nccl.py in is_available(tensors)\n     36         devices.add(device)\n     37 \n---&gt; 38     if _libnccl() is None:\n     39         warnings.warn('NCCL library not found. Check your LD_LIBRARY_PATH')\n     40         return False\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\nccl.py in _libnccl()\n     15     global lib\n     16     if lib is None:\n---&gt; 17         lib = ctypes.pydll.LoadLibrary(find_library('nccl'))\n     18         # lib = ctypes.pydll.LoadLibrary(None)\n     19         if hasattr(lib, 'ncclCommDestroy'):\n\ne:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py in LoadLibrary(self, name)\n    424 \n    425     def LoadLibrary(self, name):\n--&gt; 426         return self._dlltype(name)\n    427 \n    428 cdll = LibraryLoader(CDLL)\n\ne:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error)\n    346 \n    347         if handle is None:\n--&gt; 348             self._handle = _dlopen(self._name, mode)\n    349         else:\n    350             self._handle = handle\n\nTypeError: bad argument type for built-in operation\n\n&gt; e:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py(348)__init__()\n    346 \n    347         if handle is None:\n--&gt; 348             self._handle = _dlopen(self._name, mode)\n    349         else:\n    350             self._handle = handle\n</code></pre>", "body_text": "@peterjc123 hi, I'm trying to use DataParallel for multi-gpu acceleration.  But it crashed on windows. I think there may be something wrong with nccl but it's beyond my ability. Could you plz give me a hand ?\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    222         for hook in self._forward_pre_hooks.values():\n    223             hook(self, input)\n--> 224         result = self.forward(*input, **kwargs)\n    225         for hook in self._forward_hooks.values():\n    226             hook_result = hook(self, input, result)\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py in forward(self, *inputs, **kwargs)\n     57         if len(self.device_ids) == 1:\n     58             return self.module(*inputs[0], **kwargs[0])\n---> 59         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n     60         outputs = self.parallel_apply(replicas, inputs, kwargs)\n     61         return self.gather(outputs, self.output_device)\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py in replicate(self, module, device_ids)\n     62 \n     63     def replicate(self, module, device_ids):\n---> 64         return replicate(module, device_ids)\n     65 \n     66     def scatter(self, inputs, kwargs, device_ids):\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\replicate.py in replicate(network, devices)\n     10     params = list(network.parameters())\n     11     param_indices = {param: idx for idx, param in enumerate(params)}\n---> 12     param_copies = Broadcast(devices)(*params)\n     13     if len(params) > 0:\n     14         param_copies = [param_copies[i:i + len(params)]\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py in forward(self, *inputs)\n     17         self.num_inputs = len(inputs)\n     18         self.input_device = inputs[0].get_device()\n---> 19         outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\n     20         return tuple([t for tensors in outputs for t in tensors])\n     21 \n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\comm.py in broadcast_coalesced(tensors, devices, buffer_size)\n     52     outputs[0].extend(tensors)\n     53     for chunk in _take_tensors(tensors, buffer_size):\n---> 54         results = broadcast(_flatten_tensors(chunk), devices)\n     55         # use the broadcasted tensors for the remaining devices\n     56         for dst, res in zip(outputs[1:], results[1:]):\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\comm.py in broadcast(tensor, devices)\n     17         corresponding to indices from ``devices``.\n     18     \"\"\"\n---> 19     if nccl.is_available([tensor]) and len(set(devices)) == len(devices):\n     20         tensors = [tensor]\n     21         for device in devices[1:]:\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\nccl.py in is_available(tensors)\n     36         devices.add(device)\n     37 \n---> 38     if _libnccl() is None:\n     39         warnings.warn('NCCL library not found. Check your LD_LIBRARY_PATH')\n     40         return False\n\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\nccl.py in _libnccl()\n     15     global lib\n     16     if lib is None:\n---> 17         lib = ctypes.pydll.LoadLibrary(find_library('nccl'))\n     18         # lib = ctypes.pydll.LoadLibrary(None)\n     19         if hasattr(lib, 'ncclCommDestroy'):\n\ne:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py in LoadLibrary(self, name)\n    424 \n    425     def LoadLibrary(self, name):\n--> 426         return self._dlltype(name)\n    427 \n    428 cdll = LibraryLoader(CDLL)\n\ne:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error)\n    346 \n    347         if handle is None:\n--> 348             self._handle = _dlopen(self._name, mode)\n    349         else:\n    350             self._handle = handle\n\nTypeError: bad argument type for built-in operation\n\n> e:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py(348)__init__()\n    346 \n    347         if handle is None:\n--> 348             self._handle = _dlopen(self._name, mode)\n    349         else:\n    350             self._handle = handle", "body": "@peterjc123 hi, I'm trying to use DataParallel for multi-gpu acceleration.  But it crashed on windows. I think there may be something wrong with nccl but it's beyond my ability. Could you plz give me a hand ? \r\n```\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\r\n    222         for hook in self._forward_pre_hooks.values():\r\n    223             hook(self, input)\r\n--> 224         result = self.forward(*input, **kwargs)\r\n    225         for hook in self._forward_hooks.values():\r\n    226             hook_result = hook(self, input, result)\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py in forward(self, *inputs, **kwargs)\r\n     57         if len(self.device_ids) == 1:\r\n     58             return self.module(*inputs[0], **kwargs[0])\r\n---> 59         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n     60         outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n     61         return self.gather(outputs, self.output_device)\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py in replicate(self, module, device_ids)\r\n     62 \r\n     63     def replicate(self, module, device_ids):\r\n---> 64         return replicate(module, device_ids)\r\n     65 \r\n     66     def scatter(self, inputs, kwargs, device_ids):\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\replicate.py in replicate(network, devices)\r\n     10     params = list(network.parameters())\r\n     11     param_indices = {param: idx for idx, param in enumerate(params)}\r\n---> 12     param_copies = Broadcast(devices)(*params)\r\n     13     if len(params) > 0:\r\n     14         param_copies = [param_copies[i:i + len(params)]\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py in forward(self, *inputs)\r\n     17         self.num_inputs = len(inputs)\r\n     18         self.input_device = inputs[0].get_device()\r\n---> 19         outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\r\n     20         return tuple([t for tensors in outputs for t in tensors])\r\n     21 \r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\comm.py in broadcast_coalesced(tensors, devices, buffer_size)\r\n     52     outputs[0].extend(tensors)\r\n     53     for chunk in _take_tensors(tensors, buffer_size):\r\n---> 54         results = broadcast(_flatten_tensors(chunk), devices)\r\n     55         # use the broadcasted tensors for the remaining devices\r\n     56         for dst, res in zip(outputs[1:], results[1:]):\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\comm.py in broadcast(tensor, devices)\r\n     17         corresponding to indices from ``devices``.\r\n     18     \"\"\"\r\n---> 19     if nccl.is_available([tensor]) and len(set(devices)) == len(devices):\r\n     20         tensors = [tensor]\r\n     21         for device in devices[1:]:\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\nccl.py in is_available(tensors)\r\n     36         devices.add(device)\r\n     37 \r\n---> 38     if _libnccl() is None:\r\n     39         warnings.warn('NCCL library not found. Check your LD_LIBRARY_PATH')\r\n     40         return False\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\site-packages\\torch\\cuda\\nccl.py in _libnccl()\r\n     15     global lib\r\n     16     if lib is None:\r\n---> 17         lib = ctypes.pydll.LoadLibrary(find_library('nccl'))\r\n     18         # lib = ctypes.pydll.LoadLibrary(None)\r\n     19         if hasattr(lib, 'ncclCommDestroy'):\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py in LoadLibrary(self, name)\r\n    424 \r\n    425     def LoadLibrary(self, name):\r\n--> 426         return self._dlltype(name)\r\n    427 \r\n    428 cdll = LibraryLoader(CDLL)\r\n\r\ne:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error)\r\n    346 \r\n    347         if handle is None:\r\n--> 348             self._handle = _dlopen(self._name, mode)\r\n    349         else:\r\n    350             self._handle = handle\r\n\r\nTypeError: bad argument type for built-in operation\r\n\r\n> e:\\users\\v-yihoch\\anaconda\\lib\\ctypes\\__init__.py(348)__init__()\r\n    346 \r\n    347         if handle is None:\r\n--> 348             self._handle = _dlopen(self._name, mode)\r\n    349         else:\r\n    350             self._handle = handle\r\n```\r\n "}