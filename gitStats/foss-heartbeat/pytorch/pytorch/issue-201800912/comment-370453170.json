{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/370453170", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-370453170", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 370453170, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDQ1MzE3MA==", "user": {"login": "shekhovt", "id": 2486893, "node_id": "MDQ6VXNlcjI0ODY4OTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2486893?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shekhovt", "html_url": "https://github.com/shekhovt", "followers_url": "https://api.github.com/users/shekhovt/followers", "following_url": "https://api.github.com/users/shekhovt/following{/other_user}", "gists_url": "https://api.github.com/users/shekhovt/gists{/gist_id}", "starred_url": "https://api.github.com/users/shekhovt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shekhovt/subscriptions", "organizations_url": "https://api.github.com/users/shekhovt/orgs", "repos_url": "https://api.github.com/users/shekhovt/repos", "events_url": "https://api.github.com/users/shekhovt/events{/privacy}", "received_events_url": "https://api.github.com/users/shekhovt/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-05T15:21:21Z", "updated_at": "2018-03-06T10:09:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a> Thanks a lot for the windows build! Everything works fine with cuda! But it is leaking memory like hell :(<br>\nI am speaking about pytorch-0.3.1.<br>\nWhen I run the training, each epoch eats up memory until the virtual memory is over (pagefile size). Then it throws the error:</p>\n<p>Traceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main<br>\nexitcode = _main(fd)<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 115, in _main<br>\nself = reduction.pickle.load(from_parent)<br>\nMemoryError</p>\n<p>... my stuff...</p>\n<p>File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 417, in <strong>iter</strong><br>\nreturn DataLoaderIter(self)<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 234, in <strong>init</strong><br>\nw.start()<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\", line 105, in start<br>\nself._popen = self._Popen(self)<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 223, in _Popen<br>\nreturn _default_context.get_context().Process._Popen(process_obj)<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 322, in _Popen<br>\nreturn Popen(process_obj)<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in <strong>init</strong><br>\nreduction.dump(process_obj, to_child)<br>\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump<br>\nForkingPickler(file, protocol).dump(obj)<br>\nBrokenPipeError: [Errno 32] Broken pipe</p>\n<p>The error is raised from the the data loader, but I am not sure it is the one to blame, maybe it just happens the one to be needing memory when there is none available.<br>\nI am using num_workers=1, as recommended above in this forum.<br>\nDo you see whether there could be some workaround or a way to track the issue down?<br>\nHas anybody else encountered this problem, or it may be because of some strange setup / drivers?</p>\n<p>Created a separate issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"302637847\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5590\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5590/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5590\">#5590</a> for tracking this problem</p>", "body_text": "@peterjc123 Thanks a lot for the windows build! Everything works fine with cuda! But it is leaking memory like hell :(\nI am speaking about pytorch-0.3.1.\nWhen I run the training, each epoch eats up memory until the virtual memory is over (pagefile size). Then it throws the error:\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main\nexitcode = _main(fd)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 115, in _main\nself = reduction.pickle.load(from_parent)\nMemoryError\n... my stuff...\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 417, in iter\nreturn DataLoaderIter(self)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 234, in init\nw.start()\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\", line 105, in start\nself._popen = self._Popen(self)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 223, in _Popen\nreturn _default_context.get_context().Process._Popen(process_obj)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 322, in _Popen\nreturn Popen(process_obj)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in init\nreduction.dump(process_obj, to_child)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\nForkingPickler(file, protocol).dump(obj)\nBrokenPipeError: [Errno 32] Broken pipe\nThe error is raised from the the data loader, but I am not sure it is the one to blame, maybe it just happens the one to be needing memory when there is none available.\nI am using num_workers=1, as recommended above in this forum.\nDo you see whether there could be some workaround or a way to track the issue down?\nHas anybody else encountered this problem, or it may be because of some strange setup / drivers?\nCreated a separate issue #5590 for tracking this problem", "body": "@peterjc123 Thanks a lot for the windows build! Everything works fine with cuda! But it is leaking memory like hell :(\r\nI am speaking about pytorch-0.3.1.\r\nWhen I run the training, each epoch eats up memory until the virtual memory is over (pagefile size). Then it throws the error:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 115, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nMemoryError\r\n\r\n... my stuff...\r\n\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 417, in __iter__\r\n    return DataLoaderIter(self)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 234, in __init__\r\n    w.start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 322, in _Popen\r\n    return Popen(process_obj)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\r\n    reduction.dump(process_obj, to_child)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nBrokenPipeError: [Errno 32] Broken pipe\r\n\r\nThe error is raised from the the data loader, but I am not sure it is the one to blame, maybe it just happens the one to be needing memory when there is none available.\r\nI am using num_workers=1, as recommended above in this forum.\r\nDo you see whether there could be some workaround or a way to track the issue down?\r\nHas anybody else encountered this problem, or it may be because of some strange setup / drivers?\r\n\r\nCreated a separate issue #5590 for tracking this problem\r\n"}