{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357561007", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-357561007", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 357561007, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzU2MTAwNw==", "user": {"login": "dreness", "id": 5242016, "node_id": "MDQ6VXNlcjUyNDIwMTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/5242016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dreness", "html_url": "https://github.com/dreness", "followers_url": "https://api.github.com/users/dreness/followers", "following_url": "https://api.github.com/users/dreness/following{/other_user}", "gists_url": "https://api.github.com/users/dreness/gists{/gist_id}", "starred_url": "https://api.github.com/users/dreness/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dreness/subscriptions", "organizations_url": "https://api.github.com/users/dreness/orgs", "repos_url": "https://api.github.com/users/dreness/repos", "events_url": "https://api.github.com/users/dreness/events{/privacy}", "received_events_url": "https://api.github.com/users/dreness/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T01:26:51Z", "updated_at": "2018-01-15T01:26:51Z", "author_association": "NONE", "body_html": "<p>Amazing, hats off to you, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a>! I've got this all working in py36 using your pytorch package and CUDA 9.</p>\n<p>I did want to mention two things:</p>\n<ol>\n<li>Instead of wrapping stuff in <code>if __name__ == \"__main__\":</code> to work around the subprocessing problem that might happen in GPU mode, it seems like you can simply add \"--nThreads 0\" to the cli arguments (e.g. when calling train.py). I tried this due to <a href=\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/169#issuecomment-354841108\" data-hovercard-type=\"issue\" data-hovercard-url=\"/junyanz/pytorch-CycleGAN-and-pix2pix/issues/169/hovercard\">this helpful comment</a> from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> - I might have figured it out eventually, but because the tracebacks were all about subprocessing / forking and whatnot, I didn't immediately make the connection to nThreads (in python, you need one process per worker thread due to the GIL). I'm not exactly sure what the scope of the subprocessing problem is, but maybe torch.utils.data.DataLoader could be taught to clamp num_workers to zero when that is needed? It also doesn't feel like this carries any performance penalty, as I'm still pretty much totally GPU-bound (see screenshots below).</li>\n<li>visdom was stuck trying to download first-run stuff after running <code>python -m visdom.server</code>. When I found the list of URLs (via <a href=\"https://github.com/facebookresearch/visdom/issues/185\" data-hovercard-type=\"issue\" data-hovercard-url=\"/facebookresearch/visdom/issues/185/hovercard\">this github issue</a>) and tried to download one manually using wget, I noticed immediately that wget was using an IPv6 address, leading me to suspect that this machine's IPv6 configuration is incomplete (in this case, probably something like 'the stack is up, so I get AAAA DNS results from my router, but I have no external route) - which doesn't surprise me, after all it's just my gaming + computer vision box :P I turned off IPv6 and visdom.server bootstrapped almost instantly. Now I'm doing a test training session and my fans are SPINNING as my loss function error creeps downward...</li>\n</ol>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/5242016/34923114-ab064d26-f94d-11e7-8dad-c083d8e7fea2.png\"><img src=\"https://user-images.githubusercontent.com/5242016/34923114-ab064d26-f94d-11e7-8dad-c083d8e7fea2.png\" alt=\"newplot\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/5242016/34923119-b0ac8024-f94d-11e7-9917-d52eba3c74ba.png\"><img src=\"https://user-images.githubusercontent.com/5242016/34923119-b0ac8024-f94d-11e7-9917-d52eba3c74ba.png\" alt=\"afterburner\" style=\"max-width:100%;\"></a></p>", "body_text": "Amazing, hats off to you, @peterjc123! I've got this all working in py36 using your pytorch package and CUDA 9.\nI did want to mention two things:\n\nInstead of wrapping stuff in if __name__ == \"__main__\": to work around the subprocessing problem that might happen in GPU mode, it seems like you can simply add \"--nThreads 0\" to the cli arguments (e.g. when calling train.py). I tried this due to this helpful comment from @SsnL - I might have figured it out eventually, but because the tracebacks were all about subprocessing / forking and whatnot, I didn't immediately make the connection to nThreads (in python, you need one process per worker thread due to the GIL). I'm not exactly sure what the scope of the subprocessing problem is, but maybe torch.utils.data.DataLoader could be taught to clamp num_workers to zero when that is needed? It also doesn't feel like this carries any performance penalty, as I'm still pretty much totally GPU-bound (see screenshots below).\nvisdom was stuck trying to download first-run stuff after running python -m visdom.server. When I found the list of URLs (via this github issue) and tried to download one manually using wget, I noticed immediately that wget was using an IPv6 address, leading me to suspect that this machine's IPv6 configuration is incomplete (in this case, probably something like 'the stack is up, so I get AAAA DNS results from my router, but I have no external route) - which doesn't surprise me, after all it's just my gaming + computer vision box :P I turned off IPv6 and visdom.server bootstrapped almost instantly. Now I'm doing a test training session and my fans are SPINNING as my loss function error creeps downward...", "body": "Amazing, hats off to you, @peterjc123! I've got this all working in py36 using your pytorch package and CUDA 9.\r\n\r\nI did want to mention two things:\r\n\r\n1. Instead of wrapping stuff in ```if __name__ == \"__main__\":``` to work around the subprocessing problem that might happen in GPU mode, it seems like you can simply add \"--nThreads 0\" to the cli arguments (e.g. when calling train.py). I tried this due to [this helpful comment](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/169#issuecomment-354841108) from @SsnL - I might have figured it out eventually, but because the tracebacks were all about subprocessing / forking and whatnot, I didn't immediately make the connection to nThreads (in python, you need one process per worker thread due to the GIL). I'm not exactly sure what the scope of the subprocessing problem is, but maybe torch.utils.data.DataLoader could be taught to clamp num_workers to zero when that is needed? It also doesn't feel like this carries any performance penalty, as I'm still pretty much totally GPU-bound (see screenshots below).\r\n2. visdom was stuck trying to download first-run stuff after running ```python -m visdom.server```. When I found the list of URLs (via [this github issue](https://github.com/facebookresearch/visdom/issues/185)) and tried to download one manually using wget, I noticed immediately that wget was using an IPv6 address, leading me to suspect that this machine's IPv6 configuration is incomplete (in this case, probably something like 'the stack is up, so I get AAAA DNS results from my router, but I have no external route) - which doesn't surprise me, after all it's just my gaming + computer vision box :P I turned off IPv6 and visdom.server bootstrapped almost instantly. Now I'm doing a test training session and my fans are SPINNING as my loss function error creeps downward...\r\n\r\n![newplot](https://user-images.githubusercontent.com/5242016/34923114-ab064d26-f94d-11e7-8dad-c083d8e7fea2.png)\r\n![afterburner](https://user-images.githubusercontent.com/5242016/34923119-b0ac8024-f94d-11e7-9917-d52eba3c74ba.png)\r\n"}