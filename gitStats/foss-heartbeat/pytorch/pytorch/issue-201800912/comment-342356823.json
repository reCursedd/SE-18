{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342356823", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-342356823", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 342356823, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjM1NjgyMw==", "user": {"login": "peterjc123", "id": 9998726, "node_id": "MDQ6VXNlcjk5OTg3MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9998726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterjc123", "html_url": "https://github.com/peterjc123", "followers_url": "https://api.github.com/users/peterjc123/followers", "following_url": "https://api.github.com/users/peterjc123/following{/other_user}", "gists_url": "https://api.github.com/users/peterjc123/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterjc123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterjc123/subscriptions", "organizations_url": "https://api.github.com/users/peterjc123/orgs", "repos_url": "https://api.github.com/users/peterjc123/repos", "events_url": "https://api.github.com/users/peterjc123/events{/privacy}", "received_events_url": "https://api.github.com/users/peterjc123/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-07T02:34:41Z", "updated_at": "2017-11-07T02:37:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5635743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dkc2117\">@dkc2117</a> The document of CUDA says that</p>\n<blockquote>\n<p>IPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality is not supported on Tegra platforms.</p>\n</blockquote>\n<p>So your code is not possible to run in Windows since you tend to share data on GPU from one process to another process with the usage of <code>cudaIpcOpenMemHandle</code>. There're two ways to solve it.</p>\n<ol>\n<li>Don't share CUDA Tensors. Set <code>num_workers</code> to 0.</li>\n<li>Share CPU Tensors instead. Change the code into the following.</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> Dataset\n\n<span class=\"pl-c1\">USE_CUDA</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CBOWWindowedTextDataset</span>(<span class=\"pl-e\">Dataset</span>):\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>orig_vocabulary_as_ids should be a list of int</span>\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">orig_vocabulary_as_ids</span>, <span class=\"pl-smi\">max_context_distance</span>):\n\t\t<span class=\"pl-c1\">super</span>()\n\t\t<span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids <span class=\"pl-k\">=</span> orig_vocabulary_as_ids\n\t\t<span class=\"pl-c1\">self</span>.max_context_distance <span class=\"pl-k\">=</span> max_context_distance\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.max_context_distance, <span class=\"pl-c1\">0</span>)\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n\t\twindow <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx:idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>:idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>]\n\t\ttarget <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance]\n\t\t<span class=\"pl-k\">return</span> torch.LongTensor(window), target\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n\t<span class=\"pl-c1\">MAX_CONTEXT_DISTANCE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\t<span class=\"pl-c1\">MINIBATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\n\n\torig_vocabulary_as_ids <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10000</span>))\n\n\tdataset <span class=\"pl-k\">=</span> CBOWWindowedTextDataset(\n\t\t<span class=\"pl-v\">orig_vocabulary_as_ids</span><span class=\"pl-k\">=</span>orig_vocabulary_as_ids,\n\t\t<span class=\"pl-v\">max_context_distance</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MAX_CONTEXT_DISTANCE</span>,\n\t)\n\n\t<span class=\"pl-k\">if</span> <span class=\"pl-c1\">USE_CUDA</span>:\n\t\tnum_workers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n\t<span class=\"pl-k\">else</span>:\n\t\tnum_workers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\n\tdata_loader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MINIBATCH_SIZE</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>num_workers)\n\n\t<span class=\"pl-k\">for</span> batch_idx, (windows, targets) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(data_loader):\n\t\t<span class=\"pl-k\">if</span> <span class=\"pl-c1\">USE_CUDA</span>:\n\t\t\twindows <span class=\"pl-k\">=</span> windows.cuda()\n\t\t\ttargets <span class=\"pl-k\">=</span> targets.cuda()\n\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>hello world<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "@dkc2117 The document of CUDA says that\n\nIPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality is not supported on Tegra platforms.\n\nSo your code is not possible to run in Windows since you tend to share data on GPU from one process to another process with the usage of cudaIpcOpenMemHandle. There're two ways to solve it.\n\nDon't share CUDA Tensors. Set num_workers to 0.\nShare CPU Tensors instead. Change the code into the following.\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nUSE_CUDA = True\n\nclass CBOWWindowedTextDataset(Dataset):\n\t#orig_vocabulary_as_ids should be a list of int\n\tdef __init__(self, orig_vocabulary_as_ids, max_context_distance):\n\t\tsuper()\n\t\tself.orig_vocabulary_as_ids = orig_vocabulary_as_ids\n\t\tself.max_context_distance = max_context_distance\n\n\tdef __len__(self):\n\t\treturn max(len(self.orig_vocabulary_as_ids) - 2*self.max_context_distance, 0)\n\n\tdef __getitem__(self, idx):\n\t\twindow = self.orig_vocabulary_as_ids[idx:idx+self.max_context_distance] + self.orig_vocabulary_as_ids[idx+self.max_context_distance+1:idx+self.max_context_distance*2+1]\n\t\ttarget = self.orig_vocabulary_as_ids[idx+self.max_context_distance]\n\t\treturn torch.LongTensor(window), target\n\nif __name__ == '__main__':\n\tMAX_CONTEXT_DISTANCE = 4\n\tMINIBATCH_SIZE = 64\n\n\torig_vocabulary_as_ids = list(range(1,10000))\n\n\tdataset = CBOWWindowedTextDataset(\n\t\torig_vocabulary_as_ids=orig_vocabulary_as_ids,\n\t\tmax_context_distance=MAX_CONTEXT_DISTANCE,\n\t)\n\n\tif USE_CUDA:\n\t\tnum_workers = 1\n\telse:\n\t\tnum_workers = 4\n\n\tdata_loader = DataLoader(dataset, batch_size=MINIBATCH_SIZE, num_workers=num_workers)\n\n\tfor batch_idx, (windows, targets) in enumerate(data_loader):\n\t\tif USE_CUDA:\n\t\t\twindows = windows.cuda()\n\t\t\ttargets = targets.cuda()\n\tprint('hello world')", "body": "@dkc2117 The document of CUDA says that \r\n> IPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality is not supported on Tegra platforms.\r\n\r\nSo your code is not possible to run in Windows since you tend to share data on GPU from one process to another process with the usage of `cudaIpcOpenMemHandle`. There're two ways to solve it.\r\n1. Don't share CUDA Tensors. Set `num_workers` to 0.\r\n2. Share CPU Tensors instead. Change the code into the following.\r\n```Python\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import Dataset\r\n\r\nUSE_CUDA = True\r\n\r\nclass CBOWWindowedTextDataset(Dataset):\r\n\t#orig_vocabulary_as_ids should be a list of int\r\n\tdef __init__(self, orig_vocabulary_as_ids, max_context_distance):\r\n\t\tsuper()\r\n\t\tself.orig_vocabulary_as_ids = orig_vocabulary_as_ids\r\n\t\tself.max_context_distance = max_context_distance\r\n\r\n\tdef __len__(self):\r\n\t\treturn max(len(self.orig_vocabulary_as_ids) - 2*self.max_context_distance, 0)\r\n\r\n\tdef __getitem__(self, idx):\r\n\t\twindow = self.orig_vocabulary_as_ids[idx:idx+self.max_context_distance] + self.orig_vocabulary_as_ids[idx+self.max_context_distance+1:idx+self.max_context_distance*2+1]\r\n\t\ttarget = self.orig_vocabulary_as_ids[idx+self.max_context_distance]\r\n\t\treturn torch.LongTensor(window), target\r\n\r\nif __name__ == '__main__':\r\n\tMAX_CONTEXT_DISTANCE = 4\r\n\tMINIBATCH_SIZE = 64\r\n\r\n\torig_vocabulary_as_ids = list(range(1,10000))\r\n\r\n\tdataset = CBOWWindowedTextDataset(\r\n\t\torig_vocabulary_as_ids=orig_vocabulary_as_ids,\r\n\t\tmax_context_distance=MAX_CONTEXT_DISTANCE,\r\n\t)\r\n\r\n\tif USE_CUDA:\r\n\t\tnum_workers = 1\r\n\telse:\r\n\t\tnum_workers = 4\r\n\r\n\tdata_loader = DataLoader(dataset, batch_size=MINIBATCH_SIZE, num_workers=num_workers)\r\n\r\n\tfor batch_idx, (windows, targets) in enumerate(data_loader):\r\n\t\tif USE_CUDA:\r\n\t\t\twindows = windows.cuda()\r\n\t\t\ttargets = targets.cuda()\r\n\tprint('hello world')\r\n```"}