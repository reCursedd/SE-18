{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340950113", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-340950113", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 340950113, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDk1MDExMw==", "user": {"login": "dkc2117", "id": 5635743, "node_id": "MDQ6VXNlcjU2MzU3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5635743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dkc2117", "html_url": "https://github.com/dkc2117", "followers_url": "https://api.github.com/users/dkc2117/followers", "following_url": "https://api.github.com/users/dkc2117/following{/other_user}", "gists_url": "https://api.github.com/users/dkc2117/gists{/gist_id}", "starred_url": "https://api.github.com/users/dkc2117/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dkc2117/subscriptions", "organizations_url": "https://api.github.com/users/dkc2117/orgs", "repos_url": "https://api.github.com/users/dkc2117/repos", "events_url": "https://api.github.com/users/dkc2117/events{/privacy}", "received_events_url": "https://api.github.com/users/dkc2117/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T01:08:31Z", "updated_at": "2017-11-01T01:13:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a> I got the same error when trying to use a dataloader (even with num_threads=1). The error only occurs when I enable CUDA (Works fine in cpu mode).</p>\n<p>I will excerpt relevant code here, please reply if you want me to post the the full code somewhere. CBOWWindowedTextDataset is a fairly simple Dataset wrapper around an in memory array of integer ids that returns a window of IDs (in the form of a tensor) and a target ID (just an integer) for each sample.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">USE_CUDA</span>:\n\ttorch_fType <span class=\"pl-k\">=</span> torch.cuda.FloatTensor\n\ttorch_iType <span class=\"pl-k\">=</span> torch.cuda.LongTensor\n<span class=\"pl-k\">else</span>:\n\ttorch_fType <span class=\"pl-k\">=</span> torch.FloatTensor\n\ttorch_iType <span class=\"pl-k\">=</span> torch.LongTensor\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CBOWWindowedTextDataset</span>(<span class=\"pl-e\">Dataset</span>):\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>orig_vocabulary_as_ids should be a list of int</span>\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">orig_vocabulary_as_ids</span>, <span class=\"pl-smi\">max_context_distance</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MAX_CONTEXT_DISTANCE</span>):\n\t\t<span class=\"pl-c1\">super</span>()\n\t\t<span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids <span class=\"pl-k\">=</span> orig_vocabulary_as_ids\n\t\t<span class=\"pl-c1\">self</span>.max_context_distance <span class=\"pl-k\">=</span> max_context_distance\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.max_context_distance, <span class=\"pl-c1\">0</span>)\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n\t\twindow <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx:idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>:idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>]\n\t\ttarget <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.orig_vocabulary_as_ids[idx<span class=\"pl-k\">+</span><span class=\"pl-c1\">self</span>.max_context_distance]\n\t\t<span class=\"pl-k\">return</span> torch_iType(window), target\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>######</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> various code omitted for brevity</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>######</span>\ndataset <span class=\"pl-k\">=</span> CBOWWindowedTextDataset(\n\t<span class=\"pl-v\">orig_vocabulary_as_ids</span><span class=\"pl-k\">=</span>orig_vocabulary_as_ids,\n\t<span class=\"pl-v\">max_context_distance</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MAX_CONTEXT_DISTANCE</span>,\n)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">USE_CUDA</span>:\n\tnum_workers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-k\">else</span>:\n\tnum_workers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\ndata_loader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MINIBATCH_SIZE</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>num_workers)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##code gets here without error</span>\n<span class=\"pl-k\">for</span> batch_idx, (windows, targets) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(data_loader):\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>## error occurs before this line executes, so when starting enumeration...</span>\n\tprobs_var <span class=\"pl-k\">=</span> net(autograd.Variable(windows))</pre></div>", "body_text": "@peterjc123 I got the same error when trying to use a dataloader (even with num_threads=1). The error only occurs when I enable CUDA (Works fine in cpu mode).\nI will excerpt relevant code here, please reply if you want me to post the the full code somewhere. CBOWWindowedTextDataset is a fairly simple Dataset wrapper around an in memory array of integer ids that returns a window of IDs (in the form of a tensor) and a target ID (just an integer) for each sample.\nif USE_CUDA:\n\ttorch_fType = torch.cuda.FloatTensor\n\ttorch_iType = torch.cuda.LongTensor\nelse:\n\ttorch_fType = torch.FloatTensor\n\ttorch_iType = torch.LongTensor\n\nclass CBOWWindowedTextDataset(Dataset):\n\t#orig_vocabulary_as_ids should be a list of int\n\tdef __init__(self, orig_vocabulary_as_ids, max_context_distance=MAX_CONTEXT_DISTANCE):\n\t\tsuper()\n\t\tself.orig_vocabulary_as_ids = orig_vocabulary_as_ids\n\t\tself.max_context_distance = max_context_distance\n\n\tdef __len__(self):\n\t\treturn max(len(self.orig_vocabulary_as_ids) - 2*self.max_context_distance, 0)\n\n\tdef __getitem__(self, idx):\n\t\twindow = self.orig_vocabulary_as_ids[idx:idx+self.max_context_distance] + self.orig_vocabulary_as_ids[idx+self.max_context_distance+1:idx+self.max_context_distance*2+1]\n\t\ttarget = self.orig_vocabulary_as_ids[idx+self.max_context_distance]\n\t\treturn torch_iType(window), target\n\n#######\n# various code omitted for brevity\n#######\ndataset = CBOWWindowedTextDataset(\n\torig_vocabulary_as_ids=orig_vocabulary_as_ids,\n\tmax_context_distance=MAX_CONTEXT_DISTANCE,\n)\n\nif USE_CUDA:\n\tnum_workers = 1\nelse:\n\tnum_workers = 4\n\ndata_loader = DataLoader(dataset, batch_size=MINIBATCH_SIZE, num_workers=num_workers)\n###code gets here without error\nfor batch_idx, (windows, targets) in enumerate(data_loader):\n\t### error occurs before this line executes, so when starting enumeration...\n\tprobs_var = net(autograd.Variable(windows))", "body": "@peterjc123 I got the same error when trying to use a dataloader (even with num_threads=1). The error only occurs when I enable CUDA (Works fine in cpu mode).\r\n\r\nI will excerpt relevant code here, please reply if you want me to post the the full code somewhere. CBOWWindowedTextDataset is a fairly simple Dataset wrapper around an in memory array of integer ids that returns a window of IDs (in the form of a tensor) and a target ID (just an integer) for each sample.\r\n\r\n```python\r\nif USE_CUDA:\r\n\ttorch_fType = torch.cuda.FloatTensor\r\n\ttorch_iType = torch.cuda.LongTensor\r\nelse:\r\n\ttorch_fType = torch.FloatTensor\r\n\ttorch_iType = torch.LongTensor\r\n\r\nclass CBOWWindowedTextDataset(Dataset):\r\n\t#orig_vocabulary_as_ids should be a list of int\r\n\tdef __init__(self, orig_vocabulary_as_ids, max_context_distance=MAX_CONTEXT_DISTANCE):\r\n\t\tsuper()\r\n\t\tself.orig_vocabulary_as_ids = orig_vocabulary_as_ids\r\n\t\tself.max_context_distance = max_context_distance\r\n\r\n\tdef __len__(self):\r\n\t\treturn max(len(self.orig_vocabulary_as_ids) - 2*self.max_context_distance, 0)\r\n\r\n\tdef __getitem__(self, idx):\r\n\t\twindow = self.orig_vocabulary_as_ids[idx:idx+self.max_context_distance] + self.orig_vocabulary_as_ids[idx+self.max_context_distance+1:idx+self.max_context_distance*2+1]\r\n\t\ttarget = self.orig_vocabulary_as_ids[idx+self.max_context_distance]\r\n\t\treturn torch_iType(window), target\r\n\r\n#######\r\n# various code omitted for brevity\r\n#######\r\ndataset = CBOWWindowedTextDataset(\r\n\torig_vocabulary_as_ids=orig_vocabulary_as_ids,\r\n\tmax_context_distance=MAX_CONTEXT_DISTANCE,\r\n)\r\n\r\nif USE_CUDA:\r\n\tnum_workers = 1\r\nelse:\r\n\tnum_workers = 4\r\n\r\ndata_loader = DataLoader(dataset, batch_size=MINIBATCH_SIZE, num_workers=num_workers)\r\n###code gets here without error\r\nfor batch_idx, (windows, targets) in enumerate(data_loader):\r\n\t### error occurs before this line executes, so when starting enumeration...\r\n\tprobs_var = net(autograd.Variable(windows))\r\n```"}