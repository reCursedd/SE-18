{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357565157", "html_url": "https://github.com/pytorch/pytorch/issues/494#issuecomment-357565157", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/494", "id": 357565157, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzU2NTE1Nw==", "user": {"login": "dreness", "id": 5242016, "node_id": "MDQ6VXNlcjUyNDIwMTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/5242016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dreness", "html_url": "https://github.com/dreness", "followers_url": "https://api.github.com/users/dreness/followers", "following_url": "https://api.github.com/users/dreness/following{/other_user}", "gists_url": "https://api.github.com/users/dreness/gists{/gist_id}", "starred_url": "https://api.github.com/users/dreness/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dreness/subscriptions", "organizations_url": "https://api.github.com/users/dreness/orgs", "repos_url": "https://api.github.com/users/dreness/repos", "events_url": "https://api.github.com/users/dreness/events{/privacy}", "received_events_url": "https://api.github.com/users/dreness/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T02:06:31Z", "updated_at": "2018-01-15T02:06:31Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9998726\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/peterjc123\">@peterjc123</a> thanks for the replies :) Is it fair to think that at least right now for my current training case on this host, I wouldn't gain a great deal with proper multi-processing? The GPU Usage % graph above isn't pegged at 100%, but it averages about 93%. This machine is also pretty fast, with samsung 960 pro SSD, 32 GB of RAM, and 4.5 GHz quad core intel cpu.</p>\n<p>I don't really know enough Windows to say for sure, but it doesn't seem like the remaining 7% is lost to context switching or process launch overhead. I enabled the \"audit process creation\" event in Local Security Policy --&gt; Advanced Audit Policy Configuration --&gt; System Audit Policies - Local Group Policy Object --&gt; Detailed Tracking, and I don't see the train.py activity (or anything else) cycling through processes quickly, so I suspect that 7% is lost somewhere else. (but guess how many processes are spawned in the course of launching Anaconda Prompt: 23!)</p>\n<p>I'm very new to pytorch (this is my first training), so I'm totally unfamiliar with the expected shapes and sizes of the various workloads. Perhaps your comments are intended to refer to other workloads associated with pytorch? Or perhaps my theories above are just wrong...</p>", "body_text": "@SsnL and @peterjc123 thanks for the replies :) Is it fair to think that at least right now for my current training case on this host, I wouldn't gain a great deal with proper multi-processing? The GPU Usage % graph above isn't pegged at 100%, but it averages about 93%. This machine is also pretty fast, with samsung 960 pro SSD, 32 GB of RAM, and 4.5 GHz quad core intel cpu.\nI don't really know enough Windows to say for sure, but it doesn't seem like the remaining 7% is lost to context switching or process launch overhead. I enabled the \"audit process creation\" event in Local Security Policy --> Advanced Audit Policy Configuration --> System Audit Policies - Local Group Policy Object --> Detailed Tracking, and I don't see the train.py activity (or anything else) cycling through processes quickly, so I suspect that 7% is lost somewhere else. (but guess how many processes are spawned in the course of launching Anaconda Prompt: 23!)\nI'm very new to pytorch (this is my first training), so I'm totally unfamiliar with the expected shapes and sizes of the various workloads. Perhaps your comments are intended to refer to other workloads associated with pytorch? Or perhaps my theories above are just wrong...", "body": "@SsnL and @peterjc123 thanks for the replies :) Is it fair to think that at least right now for my current training case on this host, I wouldn't gain a great deal with proper multi-processing? The GPU Usage % graph above isn't pegged at 100%, but it averages about 93%. This machine is also pretty fast, with samsung 960 pro SSD, 32 GB of RAM, and 4.5 GHz quad core intel cpu.\r\n\r\nI don't really know enough Windows to say for sure, but it doesn't seem like the remaining 7% is lost to context switching or process launch overhead. I enabled the \"audit process creation\" event in Local Security Policy --> Advanced Audit Policy Configuration --> System Audit Policies - Local Group Policy Object --> Detailed Tracking, and I don't see the train.py activity (or anything else) cycling through processes quickly, so I suspect that 7% is lost somewhere else. (but guess how many processes are spawned in the course of launching Anaconda Prompt: 23!)\r\n\r\nI'm very new to pytorch (this is my first training), so I'm totally unfamiliar with the expected shapes and sizes of the various workloads. Perhaps your comments are intended to refer to other workloads associated with pytorch? Or perhaps my theories above are just wrong..."}