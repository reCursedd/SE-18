{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360020192", "html_url": "https://github.com/pytorch/pytorch/issues/3814#issuecomment-360020192", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3814", "id": 360020192, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDAyMDE5Mg==", "user": {"login": "xforceco", "id": 12808889, "node_id": "MDQ6VXNlcjEyODA4ODg5", "avatar_url": "https://avatars0.githubusercontent.com/u/12808889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xforceco", "html_url": "https://github.com/xforceco", "followers_url": "https://api.github.com/users/xforceco/followers", "following_url": "https://api.github.com/users/xforceco/following{/other_user}", "gists_url": "https://api.github.com/users/xforceco/gists{/gist_id}", "starred_url": "https://api.github.com/users/xforceco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xforceco/subscriptions", "organizations_url": "https://api.github.com/users/xforceco/orgs", "repos_url": "https://api.github.com/users/xforceco/repos", "events_url": "https://api.github.com/users/xforceco/events{/privacy}", "received_events_url": "https://api.github.com/users/xforceco/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-24T04:51:01Z", "updated_at": "2018-01-24T04:51:01Z", "author_association": "NONE", "body_html": "<p>This confuses me as well, instead I was thinking scheduler.step() will implicitly perform optimizer.step() as well since scheduler wraps optimizer.<br>\nIt would be appreciated if one comment (or example code) can be added to the official documentation, saying that scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.<br>\nMany thanks</p>", "body_text": "This confuses me as well, instead I was thinking scheduler.step() will implicitly perform optimizer.step() as well since scheduler wraps optimizer.\nIt would be appreciated if one comment (or example code) can be added to the official documentation, saying that scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.\nMany thanks", "body": "This confuses me as well, instead I was thinking scheduler.step() will implicitly perform optimizer.step() as well since scheduler wraps optimizer.\r\nIt would be appreciated if one comment (or example code) can be added to the official documentation, saying that scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.\r\nMany thanks"}