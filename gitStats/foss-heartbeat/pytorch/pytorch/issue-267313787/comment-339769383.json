{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/339769383", "html_url": "https://github.com/pytorch/pytorch/pull/3211#issuecomment-339769383", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3211", "id": 339769383, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTc2OTM4Mw==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-26T19:11:33Z", "updated_at": "2017-10-27T17:58:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Okay, this is ready for review. In the updated code, I</p>\n<ol>\n<li>rewrote <code>_take_tensor</code> so that it groups tensors by types <strong>and return them only in the same order within the same type</strong>. To reorder them, back <code>_reorder_tensors_as</code> is also updated. Theoretically performance should improve when relevant methods are called with a mix of tensors of various types.</li>\n<li>Separated flatten &amp; unflatten behavior on sparse and dense tensors into different methods: <code>_[flatten/unflatten]_[sparse/dense]_tensors</code> so that they are not doing things like returning different types (tuple vs tensor) basing on inputs. Therefore, the logic to handle sparse/dense difference is naturally moved to the caller of these functions, i.e. <code>broadcast_coalesced</code> and <code>reduce_add_coalesced</code>. This improves readability since the functions should handle sparse tensors differently.</li>\n<li>Added some comment about the boolean flag on <code>THCSTensor_(newFlattenedIndices)</code>. Clarified that it is Thrust that modifies the index tensor.</li>\n</ol>", "body_text": "Okay, this is ready for review. In the updated code, I\n\nrewrote _take_tensor so that it groups tensors by types and return them only in the same order within the same type. To reorder them, back _reorder_tensors_as is also updated. Theoretically performance should improve when relevant methods are called with a mix of tensors of various types.\nSeparated flatten & unflatten behavior on sparse and dense tensors into different methods: _[flatten/unflatten]_[sparse/dense]_tensors so that they are not doing things like returning different types (tuple vs tensor) basing on inputs. Therefore, the logic to handle sparse/dense difference is naturally moved to the caller of these functions, i.e. broadcast_coalesced and reduce_add_coalesced. This improves readability since the functions should handle sparse tensors differently.\nAdded some comment about the boolean flag on THCSTensor_(newFlattenedIndices). Clarified that it is Thrust that modifies the index tensor.", "body": "Okay, this is ready for review. In the updated code, I\r\n\r\n1. rewrote `_take_tensor` so that it groups tensors by types **and return them only in the same order within the same type**. To reorder them, back `_reorder_tensors_as` is also updated. Theoretically performance should improve when relevant methods are called with a mix of tensors of various types.\r\n2. Separated flatten & unflatten behavior on sparse and dense tensors into different methods: `_[flatten/unflatten]_[sparse/dense]_tensors` so that they are not doing things like returning different types (tuple vs tensor) basing on inputs. Therefore, the logic to handle sparse/dense difference is naturally moved to the caller of these functions, i.e. `broadcast_coalesced` and `reduce_add_coalesced`. This improves readability since the functions should handle sparse tensors differently.\r\n3. Added some comment about the boolean flag on `THCSTensor_(newFlattenedIndices)`. Clarified that it is Thrust that modifies the index tensor."}