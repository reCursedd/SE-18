{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3211", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3211/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3211/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3211/events", "html_url": "https://github.com/pytorch/pytorch/pull/3211", "id": 267313787, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQ3OTAyMTM3", "number": 3211, "title": "Fix sparse bugs; add reduce_add, broadcase, data_parallel for sparse", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-10-20T21:47:11Z", "updated_at": "2018-11-23T15:35:45Z", "closed_at": "2017-10-28T22:52:36Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3211", "html_url": "https://github.com/pytorch/pytorch/pull/3211", "diff_url": "https://github.com/pytorch/pytorch/pull/3211.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3211.patch"}, "body_html": "<p>This PR adds various fixes for sparse tensors. In particular, it contains the following:</p>\n<p>Fixes:</p>\n<ol>\n<li>Fixes <code>.type()</code> not converting indices tensor.</li>\n<li>Fixes ATen not considering sparse cuda tensors as cuda and missing <code>get_device</code> bindings, causing <code>AutoGPU</code> checks to fail in backward engine.</li>\n<li>Fixes <code>AutoGPU</code> missing args in <code>pynew</code>.</li>\n<li>Fixes sparse tensor coalesce. The bug is caused by using <code>thrust::unique_by_key</code>, which modifies the key tensor. In case where key is 1D, original implementation is operating on the same tensor indices data storage, and thus modifies it. This is fixed by adding a flag to <code>THCSTensor_(newFlattenedIndices)</code>, controlling whether the returned value is forced to be a clone.</li>\n</ol>\n<p>New functionalities:<br>\nAdded support for sparse tensors in <code>broadcast_coalesced</code> and <code>reduce_add_coalesced</code>, and thus enabling <code>data_parallel</code> (fixing <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"226009368\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1456\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1456/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1456\">#1456</a> ). In particular, the <code>_take_tensors</code> function is modified:</p>\n<ol>\n<li>~~~In case of <code>reduce_add_coalesced</code>, it returns each sparse tensor in a separate chunk because reduce_add on combined tensors only makes sense in dense case. In addition, the yielded tensors are now only maintaining order within sparse/dense class, as specified in code comments. Hence, an additional method <code>_reorder_tensors_as</code> is provided to get back the original order.~~~</li>\n<li>~~~In case <code>broadcast_coalesced</code>, sparse tensors of same type are returned together in chunk. Furthermore, <code>_flatten_tensors</code> and <code>_unflatten_tensors</code> are updated to support combining sparse tensors, allowing faster broadcasting.~~~</li>\n</ol>\n<p>See comment below for details.</p>\n<p>Various tests are added for the fixes and new functionalities above.</p>\n<p>cc: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "This PR adds various fixes for sparse tensors. In particular, it contains the following:\nFixes:\n\nFixes .type() not converting indices tensor.\nFixes ATen not considering sparse cuda tensors as cuda and missing get_device bindings, causing AutoGPU checks to fail in backward engine.\nFixes AutoGPU missing args in pynew.\nFixes sparse tensor coalesce. The bug is caused by using thrust::unique_by_key, which modifies the key tensor. In case where key is 1D, original implementation is operating on the same tensor indices data storage, and thus modifies it. This is fixed by adding a flag to THCSTensor_(newFlattenedIndices), controlling whether the returned value is forced to be a clone.\n\nNew functionalities:\nAdded support for sparse tensors in broadcast_coalesced and reduce_add_coalesced, and thus enabling data_parallel (fixing #1456 ). In particular, the _take_tensors function is modified:\n\n~~~In case of reduce_add_coalesced, it returns each sparse tensor in a separate chunk because reduce_add on combined tensors only makes sense in dense case. In addition, the yielded tensors are now only maintaining order within sparse/dense class, as specified in code comments. Hence, an additional method _reorder_tensors_as is provided to get back the original order.~~~\n~~~In case broadcast_coalesced, sparse tensors of same type are returned together in chunk. Furthermore, _flatten_tensors and _unflatten_tensors are updated to support combining sparse tensors, allowing faster broadcasting.~~~\n\nSee comment below for details.\nVarious tests are added for the fixes and new functionalities above.\ncc: @ezyang @colesbury", "body": "This PR adds various fixes for sparse tensors. In particular, it contains the following:\r\n\r\nFixes:\r\n1. Fixes `.type()` not converting indices tensor.\r\n2. Fixes ATen not considering sparse cuda tensors as cuda and missing `get_device` bindings, causing `AutoGPU` checks to fail in backward engine.\r\n3. Fixes `AutoGPU` missing args in `pynew`.\r\n4. Fixes sparse tensor coalesce. The bug is caused by using `thrust::unique_by_key`, which modifies the key tensor. In case where key is 1D, original implementation is operating on the same tensor indices data storage, and thus modifies it. This is fixed by adding a flag to `THCSTensor_(newFlattenedIndices)`, controlling whether the returned value is forced to be a clone.\r\n\r\nNew functionalities:\r\nAdded support for sparse tensors in `broadcast_coalesced` and `reduce_add_coalesced`, and thus enabling `data_parallel` (fixing #1456 ). In particular, the `_take_tensors` function is modified:\r\n\r\n1.  ~~~In case of `reduce_add_coalesced`, it returns each sparse tensor in a separate chunk because reduce_add on combined tensors only makes sense in dense case. In addition, the yielded tensors are now only maintaining order within sparse/dense class, as specified in code comments. Hence, an additional method `_reorder_tensors_as` is provided to get back the original order.~~~\r\n2. ~~~In case `broadcast_coalesced`, sparse tensors of same type are returned together in chunk. Furthermore, `_flatten_tensors` and `_unflatten_tensors` are updated to support combining sparse tensors, allowing faster broadcasting.~~~\r\n\r\nSee comment below for details.\r\n\r\nVarious tests are added for the fixes and new functionalities above.\r\n\r\ncc: @ezyang @colesbury\r\n"}