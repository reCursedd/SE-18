{"url": "https://api.github.com/repos/pytorch/pytorch/issues/976", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/976/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/976/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/976/events", "html_url": "https://github.com/pytorch/pytorch/issues/976", "id": 213444594, "node_id": "MDU6SXNzdWUyMTM0NDQ1OTQ=", "number": 976, "title": "autograd functions implemented in C++ do not expose previous_functions", "user": {"login": "Maratyszcza", "id": 1093985, "node_id": "MDQ6VXNlcjEwOTM5ODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1093985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Maratyszcza", "html_url": "https://github.com/Maratyszcza", "followers_url": "https://api.github.com/users/Maratyszcza/followers", "following_url": "https://api.github.com/users/Maratyszcza/following{/other_user}", "gists_url": "https://api.github.com/users/Maratyszcza/gists{/gist_id}", "starred_url": "https://api.github.com/users/Maratyszcza/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Maratyszcza/subscriptions", "organizations_url": "https://api.github.com/users/Maratyszcza/orgs", "repos_url": "https://api.github.com/users/Maratyszcza/repos", "events_url": "https://api.github.com/users/Maratyszcza/events{/privacy}", "received_events_url": "https://api.github.com/users/Maratyszcza/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 526654084, "node_id": "MDU6TGFiZWw1MjY2NTQwODQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/on%20hold", "name": "on hold", "color": "cccccc", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-10T20:31:21Z", "updated_at": "2017-05-24T13:53:27Z", "closed_at": "2017-05-24T13:53:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>According to <a href=\"http://pytorch.org/docs/autograd.html#function\" rel=\"nofollow\">PyTorch autograd documentation</a>, autograd <code>Function</code>s provide <code>previous_functions</code> instance variables to track which autograd Functions produced their inputs. <code>previous_functions</code> is necessary to reconstruct the DAG in Python code and potentially export it to visualizers/analyzers/other frameworks. I found that autograd <code>Function</code> objects implemented in C++ code do not implement <code>previous_functions</code> instance variables, which makes reconstructing the DAG impossible (I believe this information is preserved somewhere in the C++ classes, but didn't find a way to access it from Python).</p>\n<p>Repro:</p>\n<pre><code>import torch.autograd\nimport torch.nn\n\ndef qualified_type(var):\n    if var is None:\n        return \"None\"\n    else:\n        module = var.__class__.__module__\n        type = var.__class__.__name__\n        if module is None or module == \"__builtin__\":\n            return type\n        else:\n            return module + \".\" + type\n\nimport torchvision.models as models\nmodel = models.alexnet(pretrained=True)\n\ninput = torch.FloatTensor(1, 3, 224, 224)\ninput_var = torch.autograd.Variable(input)\noutput_var = model(input_var)\n\nseen = set()\ndef add_nodes(var):\n    if var not in seen:\n        if isinstance(var, torch.autograd.Variable):\n            print(\"Variable: \" + qualified_type(var))\n        else:\n            print(\"Function: \" + qualified_type(var))\n        seen.add(var)\n        if hasattr(var, 'previous_functions'):\n            for u in var.previous_functions:\n                add_nodes(u[0])\n        elif hasattr(var, 'creator'):\n            if var.creator is not None:\n                add_nodes(var.creator)\n\nadd_nodes(output_var.creator)\n</code></pre>", "body_text": "According to PyTorch autograd documentation, autograd Functions provide previous_functions instance variables to track which autograd Functions produced their inputs. previous_functions is necessary to reconstruct the DAG in Python code and potentially export it to visualizers/analyzers/other frameworks. I found that autograd Function objects implemented in C++ code do not implement previous_functions instance variables, which makes reconstructing the DAG impossible (I believe this information is preserved somewhere in the C++ classes, but didn't find a way to access it from Python).\nRepro:\nimport torch.autograd\nimport torch.nn\n\ndef qualified_type(var):\n    if var is None:\n        return \"None\"\n    else:\n        module = var.__class__.__module__\n        type = var.__class__.__name__\n        if module is None or module == \"__builtin__\":\n            return type\n        else:\n            return module + \".\" + type\n\nimport torchvision.models as models\nmodel = models.alexnet(pretrained=True)\n\ninput = torch.FloatTensor(1, 3, 224, 224)\ninput_var = torch.autograd.Variable(input)\noutput_var = model(input_var)\n\nseen = set()\ndef add_nodes(var):\n    if var not in seen:\n        if isinstance(var, torch.autograd.Variable):\n            print(\"Variable: \" + qualified_type(var))\n        else:\n            print(\"Function: \" + qualified_type(var))\n        seen.add(var)\n        if hasattr(var, 'previous_functions'):\n            for u in var.previous_functions:\n                add_nodes(u[0])\n        elif hasattr(var, 'creator'):\n            if var.creator is not None:\n                add_nodes(var.creator)\n\nadd_nodes(output_var.creator)", "body": "According to [PyTorch autograd documentation](http://pytorch.org/docs/autograd.html#function), autograd `Function`s provide `previous_functions` instance variables to track which autograd Functions produced their inputs. `previous_functions` is necessary to reconstruct the DAG in Python code and potentially export it to visualizers/analyzers/other frameworks. I found that autograd `Function` objects implemented in C++ code do not implement `previous_functions` instance variables, which makes reconstructing the DAG impossible (I believe this information is preserved somewhere in the C++ classes, but didn't find a way to access it from Python).\r\n\r\nRepro:\r\n```\r\nimport torch.autograd\r\nimport torch.nn\r\n\r\ndef qualified_type(var):\r\n    if var is None:\r\n        return \"None\"\r\n    else:\r\n        module = var.__class__.__module__\r\n        type = var.__class__.__name__\r\n        if module is None or module == \"__builtin__\":\r\n            return type\r\n        else:\r\n            return module + \".\" + type\r\n\r\nimport torchvision.models as models\r\nmodel = models.alexnet(pretrained=True)\r\n\r\ninput = torch.FloatTensor(1, 3, 224, 224)\r\ninput_var = torch.autograd.Variable(input)\r\noutput_var = model(input_var)\r\n\r\nseen = set()\r\ndef add_nodes(var):\r\n    if var not in seen:\r\n        if isinstance(var, torch.autograd.Variable):\r\n            print(\"Variable: \" + qualified_type(var))\r\n        else:\r\n            print(\"Function: \" + qualified_type(var))\r\n        seen.add(var)\r\n        if hasattr(var, 'previous_functions'):\r\n            for u in var.previous_functions:\r\n                add_nodes(u[0])\r\n        elif hasattr(var, 'creator'):\r\n            if var.creator is not None:\r\n                add_nodes(var.creator)\r\n\r\nadd_nodes(output_var.creator)\r\n```"}