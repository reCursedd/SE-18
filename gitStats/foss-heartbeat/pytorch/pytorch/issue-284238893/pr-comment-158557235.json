{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158557235", "pull_request_review_id": 85394783, "id": 158557235, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODU1NzIzNQ==", "diff_hunk": "@@ -0,0 +1,181 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Check.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include <cstring>\n+#include <memory>\n+#include <sstream>\n+#include <vector>\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+\n+namespace at { namespace native {\n+\n+Tensor embedding(const Tensor & weight, const Tensor & indices,\n+                 int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {\n+  auto indices_arg = TensorArg(indices, \"indices\", 1);\n+  checkScalarType(\"embedding\", indices_arg, kLong);\n+  checkContiguous(\"embedding\", indices_arg);\n+\n+  // TODO: use tensor.index() after improving perf\n+  if (indices.dim() == 1) {\n+    return weight.index_select(0, indices);\n+  }\n+\n+  auto size = std::vector<int64_t>(indices.sizes());\n+  for (auto d : weight.sizes().slice(1)) {\n+    size.push_back(d);\n+  }\n+  return weight.index_select(0, indices.view(-1)).view(size);\n+}\n+\n+Tensor embedding_backward(\n+    const Tensor & grad, const Tensor & indices, int64_t num_weights,\n+    int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {\n+\n+  if (sparse) {\n+    return at::embedding_sparse_backward(\n+        grad, indices, num_weights, padding_idx, scale_grad_by_freq);\n+  } else {\n+    return at::embedding_dense_backward(\n+        grad, indices, num_weights, padding_idx, scale_grad_by_freq);\n+  }\n+}\n+\n+Tensor embedding_sparse_backward(\n+    const Tensor & grad_, const Tensor & indices_, int64_t num_weights,\n+    int64_t padding_idx, bool scale_grad_by_freq) {\n+\n+  auto indices_arg = TensorArg(indices_, \"indices\", 2);\n+  checkScalarType(\"embedding_backward\", indices_arg, kLong);\n+  checkContiguous(\"embedding_backward\", indices_arg);\n+\n+  // TODO: implement scale_grad_by_freq\n+  if (scale_grad_by_freq) {\n+    runtime_error(\"embedding_backward: scale_grad_by_freq not supported with sparse gradients\");\n+  }\n+\n+  Tensor indices = indices_;\n+  Tensor grad = grad_;\n+  if (padding_idx != -1) {\n+    auto c = indices != padding_idx;\n+    indices = indices.index(c);\n+    grad = grad.index(c);\n+  }\n+\n+  auto index = indices.view({1, -1});\n+  auto values = grad.contiguous().view({-1, grad.size(1)});\n+\n+  auto& sparse_type = grad.type().toBackend(grad.is_cuda() ? kSparseCUDA : kSparseCPU);\n+  return sparse_type.sparse_coo_tensor(index, values);\n+}\n+\n+Tensor embedding_backward_cpu(\n+    const Tensor & grad_, const Tensor & indices, int64_t num_weights,\n+    int64_t padding_idx, bool scale_grad_by_freq) {\n+\n+  auto indices_arg = TensorArg(indices, \"indices\", 2);\n+  checkScalarType(\"embedding_backward\", indices_arg, kLong);\n+  checkContiguous(\"embedding_backward\", indices_arg);\n+\n+  auto indices_data = indices.data<int64_t>();\n+  int64_t numel = indices.numel();\n+\n+  std::unique_ptr<int64_t[]> counts;\n+  if (scale_grad_by_freq) {\n+    counts.reset(new int64_t[num_weights]);\n+    for (int i = 0; i < numel; i++) {\n+      counts[indices_data[i]] = 0;\n+    }\n+    for (int i = 0; i < numel; i++) {\n+      counts[indices_data[i]]++;\n+    }\n+  }\n+\n+  auto grad = grad_.contiguous().view({numel, grad_.size(-1)});\n+  auto grad_weight = grad_.type().zeros({num_weights, grad_.size(-1)});\n+\n+#ifdef _OPENMP\n+  if (numel > 1000) {\n+    // The strategy is to parallelize over sections of the vocabulary, so that\n+    // thread 1 handles updates to gradWeight[0..nVocab/nThreads]. Every thread\n+    // has to traverse the entire input, but the dominating factor is the axpy\n+    // BLAS call.\n+    #pragma omp parallel\n+    {\n+      int tid = omp_get_thread_num();\n+      int nthreads = omp_get_num_threads();\n+      int64_t start = tid * (num_weights/nthreads + 1);\n+      int64_t end = start + (num_weights/nthreads + 1);\n+      for (int64_t i = 0; i < numel; i++) {\n+        if (indices_data[i] != padding_idx) {\n+          int64_t k = indices_data[i] - TH_INDEX_BASE;\n+          if (k >= start && k < end) {\n+            double scale = 1.0;\n+            if (scale_grad_by_freq) {\n+              scale /= counts[k];\n+            }\n+            grad_weight[k].add_(grad[i], scale);\n+          }\n+        }\n+      }\n+    }\n+    return grad_weight;\n+  }\n+#endif\n+\n+  for (int64_t i = 0; i < numel; i++) {\n+    if (indices_data[i] != padding_idx) {\n+      int64_t k = indices_data[i];\n+      double scale = 1.0;\n+      if (scale_grad_by_freq) {\n+        scale /= counts[k];\n+      }\n+      grad_weight[k].add_(grad[i], scale);\n+    }\n+  }\n+\n+  return grad_weight;\n+}\n+\n+Tensor & embedding_renorm_cpu_(\n+    Tensor & self, const Tensor & indices, double max_norm, double norm_type) {\n+  auto self_arg = TensorArg(self, \"self\", 1);\n+  auto indices_arg = TensorArg(indices, \"indices\", 2);\n+  checkContiguous(\"embedding_renorm_\", self_arg);\n+  checkDim(\"embedding_renorm_\", self_arg, 2);\n+  checkContiguous(\"embedding_renorm_\", indices_arg);\n+  checkScalarType(\"embedding_renorm_\", indices_arg, kLong);\n+\n+  auto num_indices = indices.numel();\n+  auto indices_data = std::unique_ptr<int64_t[]>(new int64_t[num_indices]);\n+  std::memcpy(indices_data.get(), indices.data_ptr(), num_indices * sizeof(int64_t));\n+  std::qsort(indices_data.get(), num_indices, sizeof(int64_t),", "path": "aten/src/ATen/native/Embedding.cpp", "position": null, "original_position": 156, "commit_id": "8dab6c62f8e579ea93f407aeeafc57bedfb0ca37", "original_commit_id": "ce025518fdbbabc7074e4038d7a6e3e75cd626d9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Isn't `std::sort` more efficient than `std::qsort` (don't think the compiler can inline the check as a lambda, but `std::sort` is a template, so it's inlined for sure)? Just use `std::vector` for indices, and take its `.data()`", "created_at": "2017-12-22T20:57:08Z", "updated_at": "2018-11-23T15:37:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/4322#discussion_r158557235", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4322", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158557235"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4322#discussion_r158557235"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4322"}}, "body_html": "<p>Isn't <code>std::sort</code> more efficient than <code>std::qsort</code> (don't think the compiler can inline the check as a lambda, but <code>std::sort</code> is a template, so it's inlined for sure)? Just use <code>std::vector</code> for indices, and take its <code>.data()</code></p>", "body_text": "Isn't std::sort more efficient than std::qsort (don't think the compiler can inline the check as a lambda, but std::sort is a template, so it's inlined for sure)? Just use std::vector for indices, and take its .data()"}