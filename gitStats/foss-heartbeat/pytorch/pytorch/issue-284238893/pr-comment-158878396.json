{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158878396", "pull_request_review_id": 85741245, "id": 158878396, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODg3ODM5Ng==", "diff_hunk": "@@ -0,0 +1,348 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Check.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include \"AccumulateType.h\"\n+\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCNumerics.cuh>\n+#include <THC/THCTensorMathReduce.cuh>\n+#include <THC/THCTensorSort.cuh>\n+#include <THC/THCThrustAllocator.cuh>\n+#include <THCUNN/THCHalfAutoNumerics.cuh>\n+\n+#include <thrust/execution_policy.h>\n+#include <thrust/unique.h>\n+\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+static const int WARP_SIZE = 32;\n+\n+__device__ __forceinline__ bool warp_has_collision(int val) {\n+  // Compare our value to the values stored in the next 16 lanes,\n+  // wrapping around at 32. If any pair of values is the same than\n+  // there is a collision in the warp.\n+  bool dup = 0;\n+  const int laneId = threadIdx.x % 32;\n+  #pragma unroll\n+  for (int i = 1; i <= 16; i++) {\n+    dup |= (WARP_SHFL(val, (laneId + i) % 32) == val);\n+  }\n+  return __any(dup) != 0;\n+}\n+\n+// parallelizes over features\n+template <typename scalar_t>\n+__global__ void embedding_backward_feature_kernel(\n+  int64_t* indices, scalar_t* grad, scalar_t* grad_weight,\n+  int64_t num_indices, int64_t stride, int padding_idx) {\n+\n+  const int feature_dim = blockIdx.x * 4 + threadIdx.x / 32;\n+  if (feature_dim >= stride) {\n+    return;\n+  }\n+\n+  // The strategy here is that each warp handles a single feature\n+  // dimension.\n+  // Within that feature dimension, points in the [batch][element]\n+  // dimension can overlap, and we need to determine if threads want\n+  // to add to the gradient in a colliding manner.\n+  // Typically one would use floating-point atomicAdd() to resolve\n+  // these collisions, but that is non-deterministic if there are\n+  // collisions. Non-determinism for this code is really bad,\n+  // especially in RNNs, and is prone to snowballing error.\n+  // In order to get a deterministic order of execution, we handle\n+  // non-colliding updates separately from colliding ones. Colliding\n+  // updates are serialized in their order of execution by using the\n+  // warp-wide collision detector `warp_has_collision`.\n+  const int laneId = threadIdx.x % 32;\n+  for (int64_t i = laneId; i < num_indices; i += WARP_SIZE) {\n+    const int weight_index = (int)indices[i];\n+    if (weight_index == padding_idx) {\n+      continue;\n+    }\n+\n+    auto value = grad[i * stride + feature_dim];\n+\n+    // FIXME: should we accumulate as accreal?\n+    // Check for collision\n+    if (warp_has_collision(weight_index)) {\n+      // Run all lanes sequentially; warp divergence\n+      for (int i = 0; i < WARP_SIZE; ++i) {\n+        if (laneId == i) {\n+          grad_weight[weight_index * stride + feature_dim] += value;\n+        }\n+      }\n+    } else {\n+      // No collision; warp coherence\n+      grad_weight[weight_index * stride + feature_dim] += value;\n+    }\n+  }\n+}\n+\n+\n+template <typename scalar_t>\n+__global__ void embedding_backward_kernel(\n+  int64_t* input, int64_t* indices, scalar_t* grad_output, scalar_t* grad_weight,\n+  int64_t* count, int64_t numel, int64_t stride, int padding_idx) {\n+\n+  using accscalar_t = cuda::acc_type<scalar_t>;\n+  int idx = blockIdx.x * 4 + threadIdx.y;\n+\n+  // Each warp is responsible for an input into the LookupTable.\n+  // If the preceding input has the same as this input, then the warp\n+  // exits immediately. The warp also processes subsequent inputs with the\n+  // same value.\n+  //\n+  // Input Warp\n+  // 1     <warp 1>\n+  // 1     <warp 1> (<warp 2> exits without doing any work)\n+  // 5     <warp 3>\n+  // 8     <warp 4>\n+\n+  // Number of values proceessed by each thread (grain size)\n+  const int SZ = 4;\n+\n+  if (idx < numel\n+      && (idx == 0 || input[idx] != input[idx - 1])\n+      && input[idx] != padding_idx) {\n+    do {\n+      const int start_feature = threadIdx.x + blockIdx.y * blockDim.x * SZ;\n+      const int weight_row = ((int) input[idx]) * stride;\n+      const int grad_row = ((int) indices[idx]) * stride;\n+      const accscalar_t scale = count ? (accscalar_t)1.0 / count[idx] : 1.0;", "path": "aten/src/ATen/native/cuda/Embedding.cu", "position": 117, "original_position": 117, "commit_id": "8dab6c62f8e579ea93f407aeeafc57bedfb0ca37", "original_commit_id": "68e748279abc32d650a527282223faf2b703e3f5", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "why do you need the conversion on only one of the cases? (there's no warning on the false case?)", "created_at": "2017-12-27T23:06:48Z", "updated_at": "2018-11-23T15:37:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/4322#discussion_r158878396", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4322", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158878396"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4322#discussion_r158878396"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4322"}}, "body_html": "<p>why do you need the conversion on only one of the cases? (there's no warning on the false case?)</p>", "body_text": "why do you need the conversion on only one of the cases? (there's no warning on the false case?)"}