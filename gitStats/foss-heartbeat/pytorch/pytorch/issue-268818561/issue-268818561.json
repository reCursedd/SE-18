{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3307", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3307/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3307/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3307/events", "html_url": "https://github.com/pytorch/pytorch/issues/3307", "id": 268818561, "node_id": "MDU6SXNzdWUyNjg4MTg1NjE=", "number": 3307, "title": "`copy.deepcopy` does not copy gradient buffers of `torch.autograd.Variable` instance", "user": {"login": "geoffroeder", "id": 19354674, "node_id": "MDQ6VXNlcjE5MzU0Njc0", "avatar_url": "https://avatars2.githubusercontent.com/u/19354674?v=4", "gravatar_id": "", "url": "https://api.github.com/users/geoffroeder", "html_url": "https://github.com/geoffroeder", "followers_url": "https://api.github.com/users/geoffroeder/followers", "following_url": "https://api.github.com/users/geoffroeder/following{/other_user}", "gists_url": "https://api.github.com/users/geoffroeder/gists{/gist_id}", "starred_url": "https://api.github.com/users/geoffroeder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/geoffroeder/subscriptions", "organizations_url": "https://api.github.com/users/geoffroeder/orgs", "repos_url": "https://api.github.com/users/geoffroeder/repos", "events_url": "https://api.github.com/users/geoffroeder/events{/privacy}", "received_events_url": "https://api.github.com/users/geoffroeder/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-26T16:00:39Z", "updated_at": "2018-05-21T17:14:05Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I ran into unexpected behaviour with <code>copy.deepcopy</code> applied to a <code>Variable</code>. The gradient buffer of the <code>Variable</code> is not copied.</p>\n<pre><code>a = torch.autograd.Variable(torch.ones(1))\na.grad = torch.autograd.Variable(torch.ones(1))\nb = copy.deepcopy(a)\nprint(b.grad)\n</code></pre>\n<p>I think it would be a good idea to copy the gradient buffer during a deep copy. My use case is recording the gradient of a model's parameter space for optimization research. This would also be useful for debugging/development of complex models that involve atypical gradient operations.</p>\n<p>This is handled here: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/5760b036fb338eacd641418321f23aee51b1aee9/torch/autograd/variable.py#L89-L97\">pytorch/torch/autograd/variable.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 89 to 97\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/5760b036fb338eacd641418321f23aee51b1aee9\">5760b03</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L89\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"89\"></td>\n          <td id=\"LC89\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__deepcopy__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">memo</span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L90\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"90\"></td>\n          <td id=\"LC90\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>.is_leaf: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L91\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"91\"></td>\n          <td id=\"LC91\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Only Variables created explicitly by the user <span class=\"pl-pds\">\"</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L92\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"92\"></td>\n          <td id=\"LC92\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>(graph leaves) support the deepcopy protocol at the moment<span class=\"pl-pds\">\"</span></span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L93\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"93\"></td>\n          <td id=\"LC93\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>)(<span class=\"pl-c1\">self</span>.data.clone()) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L94\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"94\"></td>\n          <td id=\"LC94\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     result.requires_grad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.requires_grad </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L95\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"95\"></td>\n          <td id=\"LC95\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     result.volatile <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.volatile </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L96\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"96\"></td>\n          <td id=\"LC96\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     memo[<span class=\"pl-c1\">id</span>(<span class=\"pl-c1\">self</span>)] <span class=\"pl-k\">=</span> result </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L97\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"97\"></td>\n          <td id=\"LC97\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">return</span> result </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>A solution would be to also copy the <code>grad</code> attribute of the current <code>Variable</code>, which would involve a recursion of the deep copy since the grad attribute is also a <code>Variable</code>.</p>", "body_text": "I ran into unexpected behaviour with copy.deepcopy applied to a Variable. The gradient buffer of the Variable is not copied.\na = torch.autograd.Variable(torch.ones(1))\na.grad = torch.autograd.Variable(torch.ones(1))\nb = copy.deepcopy(a)\nprint(b.grad)\n\nI think it would be a good idea to copy the gradient buffer during a deep copy. My use case is recording the gradient of a model's parameter space for optimization research. This would also be useful for debugging/development of complex models that involve atypical gradient operations.\nThis is handled here: \n  \n    \n      pytorch/torch/autograd/variable.py\n    \n    \n        Lines 89 to 97\n      in\n      5760b03\n    \n    \n    \n    \n\n        \n          \n           def __deepcopy__(self, memo): \n        \n\n        \n          \n               if not self.is_leaf: \n        \n\n        \n          \n                   raise RuntimeError(\"Only Variables created explicitly by the user \" \n        \n\n        \n          \n                                      \"(graph leaves) support the deepcopy protocol at the moment\") \n        \n\n        \n          \n               result = type(self)(self.data.clone()) \n        \n\n        \n          \n               result.requires_grad = self.requires_grad \n        \n\n        \n          \n               result.volatile = self.volatile \n        \n\n        \n          \n               memo[id(self)] = result \n        \n\n        \n          \n               return result \n        \n    \n  \n\n\nA solution would be to also copy the grad attribute of the current Variable, which would involve a recursion of the deep copy since the grad attribute is also a Variable.", "body": "I ran into unexpected behaviour with `copy.deepcopy` applied to a `Variable`. The gradient buffer of the `Variable` is not copied.\r\n```\r\na = torch.autograd.Variable(torch.ones(1))\r\na.grad = torch.autograd.Variable(torch.ones(1))\r\nb = copy.deepcopy(a)\r\nprint(b.grad)\r\n```\r\nI think it would be a good idea to copy the gradient buffer during a deep copy. My use case is recording the gradient of a model's parameter space for optimization research. This would also be useful for debugging/development of complex models that involve atypical gradient operations.\r\n\r\nThis is handled here: https://github.com/pytorch/pytorch/blob/5760b036fb338eacd641418321f23aee51b1aee9/torch/autograd/variable.py#L89-L97\r\n\r\nA solution would be to also copy the `grad` attribute of the current `Variable`, which would involve a recursion of the deep copy since the grad attribute is also a `Variable`."}