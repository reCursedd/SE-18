{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1257", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1257/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1257/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1257/events", "html_url": "https://github.com/pytorch/pytorch/issues/1257", "id": 221714389, "node_id": "MDU6SXNzdWUyMjE3MTQzODk=", "number": 1257, "title": "[feature request] Support tuple scale_factor in Bilinear upsampling", "user": {"login": "andrewgiessel", "id": 1160997, "node_id": "MDQ6VXNlcjExNjA5OTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1160997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewgiessel", "html_url": "https://github.com/andrewgiessel", "followers_url": "https://api.github.com/users/andrewgiessel/followers", "following_url": "https://api.github.com/users/andrewgiessel/following{/other_user}", "gists_url": "https://api.github.com/users/andrewgiessel/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewgiessel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewgiessel/subscriptions", "organizations_url": "https://api.github.com/users/andrewgiessel/orgs", "repos_url": "https://api.github.com/users/andrewgiessel/repos", "events_url": "https://api.github.com/users/andrewgiessel/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewgiessel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-04-14T00:22:28Z", "updated_at": "2017-05-03T23:03:46Z", "closed_at": "2017-05-03T23:03:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have a simple and straightforward proposal:  allow more flexible upsampling specifications in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/upsampling.py#L72\">UpsamplingBilinear2d</a>.</p>\n<p>In particular, I would like to relax the constraint that <code>scaling_factor</code> be equal in both dimensions.  This is not required for bilinear interpolation (but is for nearest neighbor as far as I can tell).  If you know what size of image you'd like to rescale too, you can directly specify even non-integer scaling by using the <code>size</code> parameter.  However, knowing these values beforehand can be cumbersome to calculate and is limiting in fully convolutional networks, for instance.</p>\n<p>As a point of reference, Keras allows independent scaling factors in their UpSampling2D layers.  In the tensorflow keras backend, these propagate to a bilinear interpolation routine.</p>\n<p>I've implemented this locally and it works- it's pretty straightforward and only touches python.</p>\n<p>Thoughts?  If approved I'll clean it up, figure out some tests, and open a PR.</p>", "body_text": "I have a simple and straightforward proposal:  allow more flexible upsampling specifications in UpsamplingBilinear2d.\nIn particular, I would like to relax the constraint that scaling_factor be equal in both dimensions.  This is not required for bilinear interpolation (but is for nearest neighbor as far as I can tell).  If you know what size of image you'd like to rescale too, you can directly specify even non-integer scaling by using the size parameter.  However, knowing these values beforehand can be cumbersome to calculate and is limiting in fully convolutional networks, for instance.\nAs a point of reference, Keras allows independent scaling factors in their UpSampling2D layers.  In the tensorflow keras backend, these propagate to a bilinear interpolation routine.\nI've implemented this locally and it works- it's pretty straightforward and only touches python.\nThoughts?  If approved I'll clean it up, figure out some tests, and open a PR.", "body": "I have a simple and straightforward proposal:  allow more flexible upsampling specifications in [UpsamplingBilinear2d](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/upsampling.py#L72).  \r\n\r\nIn particular, I would like to relax the constraint that `scaling_factor` be equal in both dimensions.  This is not required for bilinear interpolation (but is for nearest neighbor as far as I can tell).  If you know what size of image you'd like to rescale too, you can directly specify even non-integer scaling by using the `size` parameter.  However, knowing these values beforehand can be cumbersome to calculate and is limiting in fully convolutional networks, for instance.  \r\n\r\nAs a point of reference, Keras allows independent scaling factors in their UpSampling2D layers.  In the tensorflow keras backend, these propagate to a bilinear interpolation routine.\r\n\r\nI've implemented this locally and it works- it's pretty straightforward and only touches python.\r\n\r\nThoughts?  If approved I'll clean it up, figure out some tests, and open a PR.\r\n\r\n"}