{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1498", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1498/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1498/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1498/events", "html_url": "https://github.com/pytorch/pytorch/issues/1498", "id": 226783558, "node_id": "MDU6SXNzdWUyMjY3ODM1NTg=", "number": 1498, "title": "Conv1d only accepts FloatTensor", "user": {"login": "episodeyang", "id": 630490, "node_id": "MDQ6VXNlcjYzMDQ5MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/630490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/episodeyang", "html_url": "https://github.com/episodeyang", "followers_url": "https://api.github.com/users/episodeyang/followers", "following_url": "https://api.github.com/users/episodeyang/following{/other_user}", "gists_url": "https://api.github.com/users/episodeyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/episodeyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/episodeyang/subscriptions", "organizations_url": "https://api.github.com/users/episodeyang/orgs", "repos_url": "https://api.github.com/users/episodeyang/repos", "events_url": "https://api.github.com/users/episodeyang/events{/privacy}", "received_events_url": "https://api.github.com/users/episodeyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-06T16:13:44Z", "updated_at": "2017-05-06T20:38:52Z", "closed_at": "2017-05-06T18:57:16Z", "author_association": "NONE", "body_html": "<p>It looks like Conv1d only accepts <code>FloatTensor</code>, and when it is fed <code>DoubleTensor</code> it errors out.</p>\n<p>Here is a short example</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\nx_stub <span class=\"pl-k\">=</span> Variable(torch.DoubleTensor(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">15</span>, <span class=\"pl-c1\">12</span>).normal_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>))\nconv_1 <span class=\"pl-k\">=</span> nn.Conv1d(<span class=\"pl-c1\">15</span>, <span class=\"pl-c1\">15</span>, <span class=\"pl-c1\">3</span>)\ny <span class=\"pl-k\">=</span> conv_1(x_stub)</pre></div>\n<p>so show what's going on, I added the following line to the source</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">conv1d</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-smi\">dilation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n           <span class=\"pl-smi\">groups</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n    f <span class=\"pl-k\">=</span> ConvNd(_single(stride), _single(padding), _single(dilation), <span class=\"pl-c1\">False</span>,\n               _single(<span class=\"pl-c1\">0</span>), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n    <span class=\"pl-k\">=</span>this line<span class=\"pl-k\">=</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>, weight, bias)\n    <span class=\"pl-k\">return</span> f(<span class=\"pl-c1\">input</span>, weight, bias)\n</pre></div>\n<p>when running code, it gives me the following error message:</p>\n<div class=\"highlight highlight-source-shell\"><pre>Variable containing:\n( 0 ,.,.) = \n   1   1   1  ...    0   0   0\n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   0\n     ...       \u22f1       ...    \n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   1\n... \n\n(127,.,.) = \n   1   1   1  ...    0   0   0\n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   0\n     ...       \u22f1       ...    \n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   1   0\n   0   0   0  ...    0   0   1\n[torch.DoubleTensor of size 128x12x15]\n\n Parameter containing:\n(0 ,.,.) = \n  0.1286 -0.1301\n -0.0871  0.0397\n  0.0317  0.0072\n -0.0406  0.0803\n -0.1885  0.1544\n  0.1090 -0.1772\n -0.1818  0.0865\n -0.1696 -0.0973\n -0.1179 -0.0781\n -0.0745 -0.1268\n  0.1303 -0.0950\n  0.0804 -0.1008\n...\n(11,.,.) = \n -0.1984 -0.1655\n -0.0531 -0.0365\n -0.1009  0.2038\n -0.0382  0.1492\n -0.1048 -0.1378\n  0.0774  0.0515\n -0.0548 -0.1791\n -0.1805  0.0558\n -0.1805 -0.0603\n -0.1938  0.0465\n -0.1470 -0.0298\n -0.1597 -0.1718\n[torch.FloatTensor of size 12x12x2]\n \n\nParameter containing:\n 0.2023\n 0.0939\n-0.2037\n 0.1501\n-0.0270\n 0.0494\n 0.0637\n-0.1420\n 0.1512\n-0.1538\n-0.1828\n 0.0366\n[torch.FloatTensor of size 12]</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/grammar_vae.py<span class=\"pl-pds\">\"</span></span>, line 69, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    losses += sess.train(train_loader, epoch)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/grammar_vae.py<span class=\"pl-pds\">\"</span></span>, line 26, <span class=\"pl-k\">in</span> train\n    recon_batch, mu, log_var = self.model(data)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 206, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/model.py<span class=\"pl-pds\">\"</span></span>, line 90, <span class=\"pl-k\">in</span> forward\n    mu, log_var = self.encoder(x)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 206, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/model.py<span class=\"pl-pds\">\"</span></span>, line 47, <span class=\"pl-k\">in</span> forward\n    h = self.conv_1(x)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 206, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/conv.py<span class=\"pl-pds\">\"</span></span>, line 143, <span class=\"pl-k\">in</span> forward\n    self.padding, self.dilation, self.groups)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/functional.py<span class=\"pl-pds\">\"</span></span>, line 69, <span class=\"pl-k\">in</span> conv1d\n    <span class=\"pl-k\">return</span> f(input, weight, bias)\nRuntimeError: expected Double tensor (got Float tensor)\n</pre></div>", "body_text": "It looks like Conv1d only accepts FloatTensor, and when it is fed DoubleTensor it errors out.\nHere is a short example\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nx_stub = Variable(torch.DoubleTensor(100, 15, 12).normal_(0, 1))\nconv_1 = nn.Conv1d(15, 15, 3)\ny = conv_1(x_stub)\nso show what's going on, I added the following line to the source\ndef conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1,\n           groups=1):\n    f = ConvNd(_single(stride), _single(padding), _single(dilation), False,\n               _single(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n    =this line=> print(input, weight, bias)\n    return f(input, weight, bias)\n\nwhen running code, it gives me the following error message:\nVariable containing:\n( 0 ,.,.) = \n   1   1   1  ...    0   0   0\n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   0\n     ...       \u22f1       ...    \n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   1\n... \n\n(127,.,.) = \n   1   1   1  ...    0   0   0\n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   0   0\n     ...       \u22f1       ...    \n   0   0   0  ...    0   0   0\n   0   0   0  ...    0   1   0\n   0   0   0  ...    0   0   1\n[torch.DoubleTensor of size 128x12x15]\n\n Parameter containing:\n(0 ,.,.) = \n  0.1286 -0.1301\n -0.0871  0.0397\n  0.0317  0.0072\n -0.0406  0.0803\n -0.1885  0.1544\n  0.1090 -0.1772\n -0.1818  0.0865\n -0.1696 -0.0973\n -0.1179 -0.0781\n -0.0745 -0.1268\n  0.1303 -0.0950\n  0.0804 -0.1008\n...\n(11,.,.) = \n -0.1984 -0.1655\n -0.0531 -0.0365\n -0.1009  0.2038\n -0.0382  0.1492\n -0.1048 -0.1378\n  0.0774  0.0515\n -0.0548 -0.1791\n -0.1805  0.0558\n -0.1805 -0.0603\n -0.1938  0.0465\n -0.1470 -0.0298\n -0.1597 -0.1718\n[torch.FloatTensor of size 12x12x2]\n \n\nParameter containing:\n 0.2023\n 0.0939\n-0.2037\n 0.1501\n-0.0270\n 0.0494\n 0.0637\n-0.1420\n 0.1512\n-0.1538\n-0.1828\n 0.0366\n[torch.FloatTensor of size 12]\nTraceback (most recent call last):\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/grammar_vae.py\", line 69, in <module>\n    losses += sess.train(train_loader, epoch)\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/grammar_vae.py\", line 26, in train\n    recon_batch, mu, log_var = self.model(data)\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/model.py\", line 90, in forward\n    mu, log_var = self.encoder(x)\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/model.py\", line 47, in forward\n    h = self.conv_1(x)\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/conv.py\", line 143, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/functional.py\", line 69, in conv1d\n    return f(input, weight, bias)\nRuntimeError: expected Double tensor (got Float tensor)", "body": "It looks like Conv1d only accepts `FloatTensor`, and when it is fed `DoubleTensor` it errors out.\r\n\r\nHere is a short example\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nx_stub = Variable(torch.DoubleTensor(100, 15, 12).normal_(0, 1))\r\nconv_1 = nn.Conv1d(15, 15, 3)\r\ny = conv_1(x_stub)\r\n```\r\n\r\nso show what's going on, I added the following line to the source\r\n\r\n```python\r\ndef conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1,\r\n           groups=1):\r\n    f = ConvNd(_single(stride), _single(padding), _single(dilation), False,\r\n               _single(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\r\n    =this line=> print(input, weight, bias)\r\n    return f(input, weight, bias)\r\n\r\n```\r\n\r\nwhen running code, it gives me the following error message:\r\n\r\n```bash\r\nVariable containing:\r\n( 0 ,.,.) = \r\n   1   1   1  ...    0   0   0\r\n   0   0   0  ...    0   0   0\r\n   0   0   0  ...    0   0   0\r\n     ...       \u22f1       ...    \r\n   0   0   0  ...    0   0   0\r\n   0   0   0  ...    0   0   0\r\n   0   0   0  ...    0   0   1\r\n... \r\n\r\n(127,.,.) = \r\n   1   1   1  ...    0   0   0\r\n   0   0   0  ...    0   0   0\r\n   0   0   0  ...    0   0   0\r\n     ...       \u22f1       ...    \r\n   0   0   0  ...    0   0   0\r\n   0   0   0  ...    0   1   0\r\n   0   0   0  ...    0   0   1\r\n[torch.DoubleTensor of size 128x12x15]\r\n\r\n Parameter containing:\r\n(0 ,.,.) = \r\n  0.1286 -0.1301\r\n -0.0871  0.0397\r\n  0.0317  0.0072\r\n -0.0406  0.0803\r\n -0.1885  0.1544\r\n  0.1090 -0.1772\r\n -0.1818  0.0865\r\n -0.1696 -0.0973\r\n -0.1179 -0.0781\r\n -0.0745 -0.1268\r\n  0.1303 -0.0950\r\n  0.0804 -0.1008\r\n...\r\n(11,.,.) = \r\n -0.1984 -0.1655\r\n -0.0531 -0.0365\r\n -0.1009  0.2038\r\n -0.0382  0.1492\r\n -0.1048 -0.1378\r\n  0.0774  0.0515\r\n -0.0548 -0.1791\r\n -0.1805  0.0558\r\n -0.1805 -0.0603\r\n -0.1938  0.0465\r\n -0.1470 -0.0298\r\n -0.1597 -0.1718\r\n[torch.FloatTensor of size 12x12x2]\r\n \r\n\r\nParameter containing:\r\n 0.2023\r\n 0.0939\r\n-0.2037\r\n 0.1501\r\n-0.0270\r\n 0.0494\r\n 0.0637\r\n-0.1420\r\n 0.1512\r\n-0.1538\r\n-0.1828\r\n 0.0366\r\n[torch.FloatTensor of size 12]\r\n```\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/grammar_vae.py\", line 69, in <module>\r\n    losses += sess.train(train_loader, epoch)\r\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/grammar_vae.py\", line 26, in train\r\n    recon_batch, mu, log_var = self.model(data)\r\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/model.py\", line 90, in forward\r\n    mu, log_var = self.encoder(x)\r\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/Users/usr/projects/deep_learning_notes/pytorch_playground/grammar_variational_autoencoder/model.py\", line 47, in forward\r\n    h = self.conv_1(x)\r\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/modules/conv.py\", line 143, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"/Users/usr/anaconda/envs/deep-learning/lib/python3.6/site-packausrs/torch/nn/functional.py\", line 69, in conv1d\r\n    return f(input, weight, bias)\r\nRuntimeError: expected Double tensor (got Float tensor)\r\n\r\n```"}