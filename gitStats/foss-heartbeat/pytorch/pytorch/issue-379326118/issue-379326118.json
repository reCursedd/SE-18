{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13787", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13787/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13787/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13787/events", "html_url": "https://github.com/pytorch/pytorch/issues/13787", "id": 379326118, "node_id": "MDU6SXNzdWUzNzkzMjYxMTg=", "number": 13787, "title": "Dataloader Segmentation Fault when using MPI backend & single process per gpu", "user": {"login": "ygfeng", "id": 43613210, "node_id": "MDQ6VXNlcjQzNjEzMjEw", "avatar_url": "https://avatars1.githubusercontent.com/u/43613210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ygfeng", "html_url": "https://github.com/ygfeng", "followers_url": "https://api.github.com/users/ygfeng/followers", "following_url": "https://api.github.com/users/ygfeng/following{/other_user}", "gists_url": "https://api.github.com/users/ygfeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/ygfeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ygfeng/subscriptions", "organizations_url": "https://api.github.com/users/ygfeng/orgs", "repos_url": "https://api.github.com/users/ygfeng/repos", "events_url": "https://api.github.com/users/ygfeng/events{/privacy}", "received_events_url": "https://api.github.com/users/ygfeng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1002728630, "node_id": "MDU6TGFiZWwxMDAyNzI4NjMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/1.0", "name": "1.0", "color": "f9db31", "default": false}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-11-09T21:21:37Z", "updated_at": "2018-11-16T23:14:06Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>When using DistributedDataParallel with mpi backend and assigning each gpu a single process on the host, program crashes at the end of an epoch. Failure is not always consistent.</p>\n<pre><code>[82362] *** Process received signal ***\n[82362] Signal: Segmentation fault (11)\n[82362] Signal code: Address not mapped (1)\n[82362] Failing at address: 0x1d3fca368\n[82362] [ 0] [0x7fffa5f504d8]\n[82362] [ 1] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c258)[0x7fffa5d8c258]\n[82362] [ 2] /lib64/libc.so.6(__libc_malloc+0x8c)[0x7fffa597945c]\n[82362] [ 3] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xdabe0)[0x7fffa5d4abe0]\n[82362] [ 4] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(_PyObject_GenericSetAttrWithDict+0x11c)[0x7fffa5d5915c]\n[82362] [ 5] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_GenericSetAttr+0x1c)[0x7fffa5d5937c]\n[82362] [ 6] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_SetAttr+0xb0)[0x7fffa5d588a0]\n[82362] [ 7] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x698c)[0x7fffa5df989c]\n[82362] [ 8] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] [ 9] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x8150)[0x7fffa5dfb060]\n[82362] [10] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\n[82362] [11] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] [12] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\n[82362] [13] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [14] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_CallMethod+0xd0)[0x7fffa5cd9cc0]\n[82362] [15] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_ReInitThreads+0xa0)[0x7fffa5df17d0]\n[82362] [16] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyOS_AfterFork+0x68)[0x7fffa5e5fa08]\n[82362] [17] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1f63b4)[0x7fffa5e663b4]\n[82362] [18] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x823c)[0x7fffa5dfb14c]\n[82362] [19] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] [20] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\n[82362] [21] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [22] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x812e0)[0x7fffa5cf12e0]\n[82362] [23] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [24] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c220)[0x7fffa5d8c220]\n[82362] [25] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1179a8)[0x7fffa5d879a8]\n[82362] [26] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [27] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1788)[0x7fffa5df4698]\n[82362] [28] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\n[82362] [29] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] *** End of error message ***\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"pytorch-examples/imagenet/main.py\", line 415, in &lt;module&gt;\n    main()\n  File \"pytorch-examples/imagenet/main.py\", line 242, in main\n    prec1 = validate(val_loader, model, criterion)\n  File \"pytorch-examples/imagenet/main.py\", line 332, in validate\n    for i, (input, target) in enumerate(val_loader):\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 819, in __iter__\n    return _DataLoaderIter(self)\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 584, in __init__\n    self._put_indices()\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 646, in _put_indices\n    indices = next(self.sample_iter, None)\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/sampler.py\", line 161, in __iter__\n    batch.append(idx)\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 274, in handler\n    _error_if_any_worker_fails()\nRuntimeError: DataLoader worker (pid 82360) is killed by signal: Segmentation fault. \n</code></pre>\n<h2>To Reproduce</h2>\n<p><a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a></p>\n<p>run<br>\n<code>mpirun -n 4 python pytorch-examples/imagenet/main.py --dist-backend mpi --batch-size 64 -a resnet50 --epochs 2 /mnt/imagenetPyTorch/</code></p>\n<p>Modifications:</p>\n<pre><code>        local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n        world_size = int(os.getenv('OMPI_COMM_WORLD_SIZE'))\n...\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                world_size=world_size)\n...\n        torch.cuda.set_device(local_rank)\n        model = torch.nn.parallel.DistributedDataParallel(model,\n                                                          device_ids=[local_rank],\n                                                          output_device=local_rank)\n</code></pre>\n<h2>Environment</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 1.0.0\nIs debug build: No\nCUDA used to build PyTorch: 10.0.130\n\nOS: Red Hat Enterprise Linux Server 7.5 (Maipo)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\nCMake version: Could not collect\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 10.0.130\nGPU models and configuration: \nGPU 0: Tesla V100-SXM2-32GB\nGPU 1: Tesla V100-SXM2-32GB\nGPU 2: Tesla V100-SXM2-32GB\nGPU 3: Tesla V100-SXM2-32GB\n\nNvidia driver version: 410.72\ncuDNN version: Probably one of the following:\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn.so.7.3.0\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn_static.a\n</code></pre>\n<h2>Additional context</h2>\n<p>Crash is less likely to occur if the <code>num_workers</code> is decreased,  or the <code>bucket_cap_mb</code> is increased.</p>\n<p>core dump generated</p>\n<pre><code>(gdb) bt\n#0  0x00007fffa24065e0 in _int_malloc () from /lib64/libc.so.6\n#1  0x00007fffa240945c in malloc () from /lib64/libc.so.6\n#2  0x00007fffa27c7490 in PyList_New () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n#3  0x00007fffa27c7948 in list_concat () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n#4  0x00007fffa275ebf0 in PyNumber_Add () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n#5  0x00007fffa2886e90 in PyEval_EvalFrameEx () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n</code></pre>\n<p><a href=\"https://github.com/pytorch/pytorch/files/2572658/pytorchsegfault.txt\">pytorchsegfault</a></p>", "body_text": "\ud83d\udc1b Bug\nWhen using DistributedDataParallel with mpi backend and assigning each gpu a single process on the host, program crashes at the end of an epoch. Failure is not always consistent.\n[82362] *** Process received signal ***\n[82362] Signal: Segmentation fault (11)\n[82362] Signal code: Address not mapped (1)\n[82362] Failing at address: 0x1d3fca368\n[82362] [ 0] [0x7fffa5f504d8]\n[82362] [ 1] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c258)[0x7fffa5d8c258]\n[82362] [ 2] /lib64/libc.so.6(__libc_malloc+0x8c)[0x7fffa597945c]\n[82362] [ 3] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xdabe0)[0x7fffa5d4abe0]\n[82362] [ 4] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(_PyObject_GenericSetAttrWithDict+0x11c)[0x7fffa5d5915c]\n[82362] [ 5] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_GenericSetAttr+0x1c)[0x7fffa5d5937c]\n[82362] [ 6] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_SetAttr+0xb0)[0x7fffa5d588a0]\n[82362] [ 7] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x698c)[0x7fffa5df989c]\n[82362] [ 8] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] [ 9] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x8150)[0x7fffa5dfb060]\n[82362] [10] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\n[82362] [11] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] [12] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\n[82362] [13] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [14] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_CallMethod+0xd0)[0x7fffa5cd9cc0]\n[82362] [15] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_ReInitThreads+0xa0)[0x7fffa5df17d0]\n[82362] [16] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyOS_AfterFork+0x68)[0x7fffa5e5fa08]\n[82362] [17] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1f63b4)[0x7fffa5e663b4]\n[82362] [18] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x823c)[0x7fffa5dfb14c]\n[82362] [19] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] [20] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\n[82362] [21] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [22] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x812e0)[0x7fffa5cf12e0]\n[82362] [23] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [24] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c220)[0x7fffa5d8c220]\n[82362] [25] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1179a8)[0x7fffa5d879a8]\n[82362] [26] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\n[82362] [27] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1788)[0x7fffa5df4698]\n[82362] [28] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\n[82362] [29] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\n[82362] *** End of error message ***\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"pytorch-examples/imagenet/main.py\", line 415, in <module>\n    main()\n  File \"pytorch-examples/imagenet/main.py\", line 242, in main\n    prec1 = validate(val_loader, model, criterion)\n  File \"pytorch-examples/imagenet/main.py\", line 332, in validate\n    for i, (input, target) in enumerate(val_loader):\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 819, in __iter__\n    return _DataLoaderIter(self)\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 584, in __init__\n    self._put_indices()\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 646, in _put_indices\n    indices = next(self.sample_iter, None)\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/sampler.py\", line 161, in __iter__\n    batch.append(idx)\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 274, in handler\n    _error_if_any_worker_fails()\nRuntimeError: DataLoader worker (pid 82360) is killed by signal: Segmentation fault. \n\nTo Reproduce\nhttps://github.com/pytorch/examples/blob/master/imagenet/main.py\nrun\nmpirun -n 4 python pytorch-examples/imagenet/main.py --dist-backend mpi --batch-size 64 -a resnet50 --epochs 2 /mnt/imagenetPyTorch/\nModifications:\n        local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\n        world_size = int(os.getenv('OMPI_COMM_WORLD_SIZE'))\n...\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                world_size=world_size)\n...\n        torch.cuda.set_device(local_rank)\n        model = torch.nn.parallel.DistributedDataParallel(model,\n                                                          device_ids=[local_rank],\n                                                          output_device=local_rank)\n\nEnvironment\nCollecting environment information...\nPyTorch version: 1.0.0\nIs debug build: No\nCUDA used to build PyTorch: 10.0.130\n\nOS: Red Hat Enterprise Linux Server 7.5 (Maipo)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\nCMake version: Could not collect\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 10.0.130\nGPU models and configuration: \nGPU 0: Tesla V100-SXM2-32GB\nGPU 1: Tesla V100-SXM2-32GB\nGPU 2: Tesla V100-SXM2-32GB\nGPU 3: Tesla V100-SXM2-32GB\n\nNvidia driver version: 410.72\ncuDNN version: Probably one of the following:\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn.so.7.3.0\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn_static.a\n\nAdditional context\nCrash is less likely to occur if the num_workers is decreased,  or the bucket_cap_mb is increased.\ncore dump generated\n(gdb) bt\n#0  0x00007fffa24065e0 in _int_malloc () from /lib64/libc.so.6\n#1  0x00007fffa240945c in malloc () from /lib64/libc.so.6\n#2  0x00007fffa27c7490 in PyList_New () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n#3  0x00007fffa27c7948 in list_concat () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n#4  0x00007fffa275ebf0 in PyNumber_Add () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n#5  0x00007fffa2886e90 in PyEval_EvalFrameEx () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\n\npytorchsegfault", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using DistributedDataParallel with mpi backend and assigning each gpu a single process on the host, program crashes at the end of an epoch. Failure is not always consistent.\r\n\r\n```\r\n[82362] *** Process received signal ***\r\n[82362] Signal: Segmentation fault (11)\r\n[82362] Signal code: Address not mapped (1)\r\n[82362] Failing at address: 0x1d3fca368\r\n[82362] [ 0] [0x7fffa5f504d8]\r\n[82362] [ 1] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c258)[0x7fffa5d8c258]\r\n[82362] [ 2] /lib64/libc.so.6(__libc_malloc+0x8c)[0x7fffa597945c]\r\n[82362] [ 3] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xdabe0)[0x7fffa5d4abe0]\r\n[82362] [ 4] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(_PyObject_GenericSetAttrWithDict+0x11c)[0x7fffa5d5915c]\r\n[82362] [ 5] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_GenericSetAttr+0x1c)[0x7fffa5d5937c]\r\n[82362] [ 6] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_SetAttr+0xb0)[0x7fffa5d588a0]\r\n[82362] [ 7] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x698c)[0x7fffa5df989c]\r\n[82362] [ 8] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] [ 9] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x8150)[0x7fffa5dfb060]\r\n[82362] [10] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\r\n[82362] [11] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] [12] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\r\n[82362] [13] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [14] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_CallMethod+0xd0)[0x7fffa5cd9cc0]\r\n[82362] [15] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_ReInitThreads+0xa0)[0x7fffa5df17d0]\r\n[82362] [16] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyOS_AfterFork+0x68)[0x7fffa5e5fa08]\r\n[82362] [17] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1f63b4)[0x7fffa5e663b4]\r\n[82362] [18] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x823c)[0x7fffa5dfb14c]\r\n[82362] [19] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] [20] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\r\n[82362] [21] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [22] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x812e0)[0x7fffa5cf12e0]\r\n[82362] [23] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [24] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c220)[0x7fffa5d8c220]\r\n[82362] [25] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1179a8)[0x7fffa5d879a8]\r\n[82362] [26] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [27] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1788)[0x7fffa5df4698]\r\n[82362] [28] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\r\n[82362] [29] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] *** End of error message ***\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"pytorch-examples/imagenet/main.py\", line 415, in <module>\r\n    main()\r\n  File \"pytorch-examples/imagenet/main.py\", line 242, in main\r\n    prec1 = validate(val_loader, model, criterion)\r\n  File \"pytorch-examples/imagenet/main.py\", line 332, in validate\r\n    for i, (input, target) in enumerate(val_loader):\r\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 819, in __iter__\r\n    return _DataLoaderIter(self)\r\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 584, in __init__\r\n    self._put_indices()\r\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 646, in _put_indices\r\n    indices = next(self.sample_iter, None)\r\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/sampler.py\", line 161, in __iter__\r\n    batch.append(idx)\r\n  File \"/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 274, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 82360) is killed by signal: Segmentation fault. \r\n```\r\n\r\n\r\n## To Reproduce\r\n[https://github.com/pytorch/examples/blob/master/imagenet/main.py](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\r\n\r\nrun\r\n`mpirun -n 4 python pytorch-examples/imagenet/main.py --dist-backend mpi --batch-size 64 -a resnet50 --epochs 2 /mnt/imagenetPyTorch/`\r\n\r\nModifications:\r\n```\r\n        local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\r\n        world_size = int(os.getenv('OMPI_COMM_WORLD_SIZE'))\r\n...\r\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\r\n                                world_size=world_size)\r\n...\r\n        torch.cuda.set_device(local_rank)\r\n        model = torch.nn.parallel.DistributedDataParallel(model,\r\n                                                          device_ids=[local_rank],\r\n                                                          output_device=local_rank)\r\n```\r\n\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Red Hat Enterprise Linux Server 7.5 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: Could not collect\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 410.72\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn.so.7.3.0\r\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn_static.a\r\n```\r\n## Additional context\r\n\r\nCrash is less likely to occur if the `num_workers` is decreased,  or the `bucket_cap_mb` is increased.\r\n\r\ncore dump generated\r\n```\r\n(gdb) bt\r\n#0  0x00007fffa24065e0 in _int_malloc () from /lib64/libc.so.6\r\n#1  0x00007fffa240945c in malloc () from /lib64/libc.so.6\r\n#2  0x00007fffa27c7490 in PyList_New () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#3  0x00007fffa27c7948 in list_concat () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fffa275ebf0 in PyNumber_Add () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fffa2886e90 in PyEval_EvalFrameEx () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n```\r\n\r\n[pytorchsegfault](https://github.com/pytorch/pytorch/files/2572658/pytorchsegfault.txt)\r\n\r\n"}