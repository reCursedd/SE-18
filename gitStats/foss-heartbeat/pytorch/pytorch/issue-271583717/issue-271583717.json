{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3512", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3512/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3512/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3512/events", "html_url": "https://github.com/pytorch/pytorch/issues/3512", "id": 271583717, "node_id": "MDU6SXNzdWUyNzE1ODM3MTc=", "number": 3512, "title": "CuDNN ConvTranspose1d issue with transposed cuda Variable (can't convert to contiguous)", "user": {"login": "qbx2", "id": 5977817, "node_id": "MDQ6VXNlcjU5Nzc4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5977817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qbx2", "html_url": "https://github.com/qbx2", "followers_url": "https://api.github.com/users/qbx2/followers", "following_url": "https://api.github.com/users/qbx2/following{/other_user}", "gists_url": "https://api.github.com/users/qbx2/gists{/gist_id}", "starred_url": "https://api.github.com/users/qbx2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qbx2/subscriptions", "organizations_url": "https://api.github.com/users/qbx2/orgs", "repos_url": "https://api.github.com/users/qbx2/repos", "events_url": "https://api.github.com/users/qbx2/events{/privacy}", "received_events_url": "https://api.github.com/users/qbx2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-06T19:00:32Z", "updated_at": "2017-11-07T05:40:24Z", "closed_at": "2017-11-06T19:22:06Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> \n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>).cuda())\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> m <span class=\"pl-k\">=</span> nn.ConvTranspose1d(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> \n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x.is_contiguous()\n<span class=\"pl-c1\">True</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).is_contiguous()\n<span class=\"pl-c1\">True</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> m(x.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>))\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;stdin&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">259</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>\n    result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">463</span>, <span class=\"pl-k\">in</span> forward\n    output_padding, <span class=\"pl-c1\">self</span>.groups, <span class=\"pl-c1\">self</span>.dilation)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.6/site-packages/torch/nn/functional.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">149</span>, <span class=\"pl-k\">in</span> conv_transpose1d\n    <span class=\"pl-k\">return</span> f(<span class=\"pl-c1\">input</span>, weight, bias)\n<span class=\"pl-c1\">RuntimeError</span>: <span class=\"pl-c1\">CUDNN_STATUS_NOT_SUPPORTED</span>. This error may appear <span class=\"pl-k\">if</span> you passed <span class=\"pl-k\">in</span> a non<span class=\"pl-k\">-</span>contiguous <span class=\"pl-c1\">input</span>.\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> m(x.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).contiguous())\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;stdin&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">259</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>\n    result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">463</span>, <span class=\"pl-k\">in</span> forward\n    output_padding, <span class=\"pl-c1\">self</span>.groups, <span class=\"pl-c1\">self</span>.dilation)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.6/site-packages/torch/nn/functional.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">149</span>, <span class=\"pl-k\">in</span> conv_transpose1d\n    <span class=\"pl-k\">return</span> f(<span class=\"pl-c1\">input</span>, weight, bias)\n<span class=\"pl-c1\">RuntimeError</span>: <span class=\"pl-c1\">CUDNN_STATUS_NOT_SUPPORTED</span>. This error may appear <span class=\"pl-k\">if</span> you passed <span class=\"pl-k\">in</span> a non<span class=\"pl-k\">-</span>contiguous <span class=\"pl-c1\">input</span>.\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> \n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y <span class=\"pl-k\">=</span> x.cpu().transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> m(y)\nVariable containing:\n(<span class=\"pl-c1\">0</span> ,.,.) <span class=\"pl-k\">=</span> \n  <span class=\"pl-c1\">0.7149</span>\n[torch.cuda.FloatTensor of size <span class=\"pl-ii\">1x1x1</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.version.<span class=\"pl-c1\">__version__</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.2.0+c74f7d8<span class=\"pl-pds\">'</span></span></pre></div>\n<p>What I have found out:</p>\n<ul>\n<li>m(x.transpose(1, 2).contiguous()) is not working because x.transpose(1, 2).is_contiguous() is evaluated to True, letting .contiguous() does nothing.</li>\n<li>This code works with cudnn disabled.</li>\n<li>This code works with Conv1d.</li>\n</ul>", "body_text": ">>> import torch\n>>> import torch.nn as nn\n>>> from torch.autograd import Variable\n>>> \n>>> x = Variable(torch.randn(1, 1, 1024).cuda())\n>>> m = nn.ConvTranspose1d(1024, 1, 1).cuda()\n>>> \n>>> x.is_contiguous()\nTrue\n>>> x.transpose(1, 2).is_contiguous()\nTrue\n>>> m(x.transpose(1, 2))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n    output_padding, self.groups, self.dilation)\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\", line 149, in conv_transpose1d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n>>> m(x.transpose(1, 2).contiguous())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n    output_padding, self.groups, self.dilation)\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\", line 149, in conv_transpose1d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n>>> \n>>> y = x.cpu().transpose(1, 2).cuda()\n>>> m(y)\nVariable containing:\n(0 ,.,.) = \n  0.7149\n[torch.cuda.FloatTensor of size 1x1x1 (GPU 0)]\n\n>>> torch.version.__version__\n'0.2.0+c74f7d8'\nWhat I have found out:\n\nm(x.transpose(1, 2).contiguous()) is not working because x.transpose(1, 2).is_contiguous() is evaluated to True, letting .contiguous() does nothing.\nThis code works with cudnn disabled.\nThis code works with Conv1d.", "body": "```python\r\n>>> import torch\r\n>>> import torch.nn as nn\r\n>>> from torch.autograd import Variable\r\n>>> \r\n>>> x = Variable(torch.randn(1, 1, 1024).cuda())\r\n>>> m = nn.ConvTranspose1d(1024, 1, 1).cuda()\r\n>>> \r\n>>> x.is_contiguous()\r\nTrue\r\n>>> x.transpose(1, 2).is_contiguous()\r\nTrue\r\n>>> m(x.transpose(1, 2))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    output_padding, self.groups, self.dilation)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\", line 149, in conv_transpose1d\r\n    return f(input, weight, bias)\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\r\n>>> m(x.transpose(1, 2).contiguous())\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n    output_padding, self.groups, self.dilation)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\", line 149, in conv_transpose1d\r\n    return f(input, weight, bias)\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\r\n>>> \r\n>>> y = x.cpu().transpose(1, 2).cuda()\r\n>>> m(y)\r\nVariable containing:\r\n(0 ,.,.) = \r\n  0.7149\r\n[torch.cuda.FloatTensor of size 1x1x1 (GPU 0)]\r\n\r\n>>> torch.version.__version__\r\n'0.2.0+c74f7d8'\r\n```\r\n\r\nWhat I have found out:\r\n- m(x.transpose(1, 2).contiguous()) is not working because x.transpose(1, 2).is_contiguous() is evaluated to True, letting .contiguous() does nothing.\r\n- This code works with cudnn disabled.\r\n- This code works with Conv1d.\r\n"}