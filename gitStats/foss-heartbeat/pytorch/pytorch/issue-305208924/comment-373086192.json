{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373086192", "html_url": "https://github.com/pytorch/pytorch/issues/5778#issuecomment-373086192", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5778", "id": 373086192, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzA4NjE5Mg==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T16:28:55Z", "updated_at": "2018-03-14T16:28:55Z", "author_association": "MEMBER", "body_html": "<p>It's summed precisely because that makes the gradients independent from the number of GPUs. The gradients may depend on the batch size depending on the loss function you use.</p>\n<p>See, for example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>).cuda()\nlinear <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>).cuda()\ndp <span class=\"pl-k\">=</span> torch.nn.DataParallel(linear, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n\nlinear(<span class=\"pl-c1\">input</span>).sum().backward()\n<span class=\"pl-c1\">print</span>(linear.weight.grad)\n\nlinear.zero_grad()\ndp(<span class=\"pl-c1\">input</span>).sum().backward()\n<span class=\"pl-c1\">print</span>(linear.weight.grad)</pre></div>\n<p>If you use BatchNorm, your gradients will also depend on the number of GPUs because batch norm is computed per-GPU.</p>", "body_text": "It's summed precisely because that makes the gradients independent from the number of GPUs. The gradients may depend on the batch size depending on the loss function you use.\nSee, for example:\nimport torch\ninput = torch.randn(4, 128, 128).cuda()\nlinear = torch.nn.Linear(128, 128).cuda()\ndp = torch.nn.DataParallel(linear, [0, 1])\n\nlinear(input).sum().backward()\nprint(linear.weight.grad)\n\nlinear.zero_grad()\ndp(input).sum().backward()\nprint(linear.weight.grad)\nIf you use BatchNorm, your gradients will also depend on the number of GPUs because batch norm is computed per-GPU.", "body": "It's summed precisely because that makes the gradients independent from the number of GPUs. The gradients may depend on the batch size depending on the loss function you use.\r\n\r\nSee, for example:\r\n\r\n```python\r\nimport torch\r\ninput = torch.randn(4, 128, 128).cuda()\r\nlinear = torch.nn.Linear(128, 128).cuda()\r\ndp = torch.nn.DataParallel(linear, [0, 1])\r\n\r\nlinear(input).sum().backward()\r\nprint(linear.weight.grad)\r\n\r\nlinear.zero_grad()\r\ndp(input).sum().backward()\r\nprint(linear.weight.grad)\r\n```\r\n\r\nIf you use BatchNorm, your gradients will also depend on the number of GPUs because batch norm is computed per-GPU."}