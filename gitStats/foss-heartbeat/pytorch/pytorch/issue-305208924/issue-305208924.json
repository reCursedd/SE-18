{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5778", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5778/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5778/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5778/events", "html_url": "https://github.com/pytorch/pytorch/issues/5778", "id": 305208924, "node_id": "MDU6SXNzdWUzMDUyMDg5MjQ=", "number": 5778, "title": "[feature request] same behavior in multi-gpu DataParallel vs single GPU", "user": {"login": "wenwei202", "id": 12142066, "node_id": "MDQ6VXNlcjEyMTQyMDY2", "avatar_url": "https://avatars0.githubusercontent.com/u/12142066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenwei202", "html_url": "https://github.com/wenwei202", "followers_url": "https://api.github.com/users/wenwei202/followers", "following_url": "https://api.github.com/users/wenwei202/following{/other_user}", "gists_url": "https://api.github.com/users/wenwei202/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenwei202/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenwei202/subscriptions", "organizations_url": "https://api.github.com/users/wenwei202/orgs", "repos_url": "https://api.github.com/users/wenwei202/repos", "events_url": "https://api.github.com/users/wenwei202/events{/privacy}", "received_events_url": "https://api.github.com/users/wenwei202/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-03-14T15:29:32Z", "updated_at": "2018-03-14T18:38:08Z", "closed_at": "2018-03-14T18:38:08Z", "author_association": "MEMBER", "body_html": "<p>In <code>DataParallel</code>, gradients from GPUs are <strong>summed</strong> to update parameters. This design results in the subtlety that training convergence depends on the number of GPUs you used to train, which makes experiments less duplicable and trackable when the number of gpus is different in hardware systems or when it is controlled by augment like <code>--gpus 0,1</code>. Is it better to <strong>average</strong> gradients from all GPUs like <code>torch.nn.parallel.DistributedDataParallel</code> does, so that the training convergence is independent on the number of gpus? Thanks!</p>", "body_text": "In DataParallel, gradients from GPUs are summed to update parameters. This design results in the subtlety that training convergence depends on the number of GPUs you used to train, which makes experiments less duplicable and trackable when the number of gpus is different in hardware systems or when it is controlled by augment like --gpus 0,1. Is it better to average gradients from all GPUs like torch.nn.parallel.DistributedDataParallel does, so that the training convergence is independent on the number of gpus? Thanks!", "body": "In `DataParallel`, gradients from GPUs are **summed** to update parameters. This design results in the subtlety that training convergence depends on the number of GPUs you used to train, which makes experiments less duplicable and trackable when the number of gpus is different in hardware systems or when it is controlled by augment like `--gpus 0,1`. Is it better to **average** gradients from all GPUs like `torch.nn.parallel.DistributedDataParallel` does, so that the training convergence is independent on the number of gpus? Thanks!"}