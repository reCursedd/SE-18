{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373129316", "html_url": "https://github.com/pytorch/pytorch/issues/5778#issuecomment-373129316", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5778", "id": 373129316, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzEyOTMxNg==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T18:32:45Z", "updated_at": "2018-03-14T18:32:45Z", "author_association": "MEMBER", "body_html": "<p>Your criterion call is outside the data parallel. The size_average divides by the <em>full</em> batch size. There's still no dependence on the number of GPUs:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">10</span>).cuda())\ntarget <span class=\"pl-k\">=</span> Variable(torch.ones(<span class=\"pl-c1\">8</span>).long().cuda())\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss().cuda()\n\nlinear <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>).cuda()\ndp <span class=\"pl-k\">=</span> nn.DataParallel(linear, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n\ncriterion(linear(<span class=\"pl-c1\">input</span>), target).backward()\n<span class=\"pl-c1\">print</span>(linear.weight.grad)\n\ndp.zero_grad()\ncriterion(dp(<span class=\"pl-c1\">input</span>), target).backward()\n<span class=\"pl-c1\">print</span>(linear.weight.grad)</pre></div>", "body_text": "Your criterion call is outside the data parallel. The size_average divides by the full batch size. There's still no dependence on the number of GPUs:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\ninput = Variable(torch.randn(8, 10).cuda())\ntarget = Variable(torch.ones(8).long().cuda())\ncriterion = nn.CrossEntropyLoss().cuda()\n\nlinear = nn.Linear(10, 10).cuda()\ndp = nn.DataParallel(linear, [0, 1])\n\ncriterion(linear(input), target).backward()\nprint(linear.weight.grad)\n\ndp.zero_grad()\ncriterion(dp(input), target).backward()\nprint(linear.weight.grad)", "body": "Your criterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\ninput = Variable(torch.randn(8, 10).cuda())\r\ntarget = Variable(torch.ones(8).long().cuda())\r\ncriterion = nn.CrossEntropyLoss().cuda()\r\n\r\nlinear = nn.Linear(10, 10).cuda()\r\ndp = nn.DataParallel(linear, [0, 1])\r\n\r\ncriterion(linear(input), target).backward()\r\nprint(linear.weight.grad)\r\n\r\ndp.zero_grad()\r\ncriterion(dp(input), target).backward()\r\nprint(linear.weight.grad)\r\n``` "}