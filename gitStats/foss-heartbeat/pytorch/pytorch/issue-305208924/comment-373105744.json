{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373105744", "html_url": "https://github.com/pytorch/pytorch/issues/5778#issuecomment-373105744", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5778", "id": 373105744, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzEwNTc0NA==", "user": {"login": "wenwei202", "id": 12142066, "node_id": "MDQ6VXNlcjEyMTQyMDY2", "avatar_url": "https://avatars0.githubusercontent.com/u/12142066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenwei202", "html_url": "https://github.com/wenwei202", "followers_url": "https://api.github.com/users/wenwei202/followers", "following_url": "https://api.github.com/users/wenwei202/following{/other_user}", "gists_url": "https://api.github.com/users/wenwei202/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenwei202/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenwei202/subscriptions", "organizations_url": "https://api.github.com/users/wenwei202/orgs", "repos_url": "https://api.github.com/users/wenwei202/repos", "events_url": "https://api.github.com/users/wenwei202/events{/privacy}", "received_events_url": "https://api.github.com/users/wenwei202/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T17:23:42Z", "updated_at": "2018-03-14T17:23:42Z", "author_association": "MEMBER", "body_html": "<p>Thanks for quick reply. You are right, that's going to be equivalent if the loss function is a sum. However, CrossEntropyLoss averages gradients over the batch. If I want to make them equivalent, Can I do this so far:</p>\n<pre><code>model = torch.nn.DataParallel(model, args.gpus).cuda()\n...\ncriterion = nn.CrossEntropyLoss().cuda()\n...\nfor i, (input, target) in enumerate(train_loader):\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        if not args.distributed:\n            for p in model.parameters():\n                p.grad.data.div_(len(args.gpus))\n        optimizer.step()\n</code></pre>\n<p>Is it right?</p>", "body_text": "Thanks for quick reply. You are right, that's going to be equivalent if the loss function is a sum. However, CrossEntropyLoss averages gradients over the batch. If I want to make them equivalent, Can I do this so far:\nmodel = torch.nn.DataParallel(model, args.gpus).cuda()\n...\ncriterion = nn.CrossEntropyLoss().cuda()\n...\nfor i, (input, target) in enumerate(train_loader):\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        if not args.distributed:\n            for p in model.parameters():\n                p.grad.data.div_(len(args.gpus))\n        optimizer.step()\n\nIs it right?", "body": "Thanks for quick reply. You are right, that's going to be equivalent if the loss function is a sum. However, CrossEntropyLoss averages gradients over the batch. If I want to make them equivalent, Can I do this so far:\r\n```\r\nmodel = torch.nn.DataParallel(model, args.gpus).cuda()\r\n...\r\ncriterion = nn.CrossEntropyLoss().cuda()\r\n...\r\nfor i, (input, target) in enumerate(train_loader):\r\n        target = target.cuda(async=True)\r\n        input_var = torch.autograd.Variable(input)\r\n        target_var = torch.autograd.Variable(target)\r\n\r\n        # compute output\r\n        output = model(input_var)\r\n        loss = criterion(output, target_var)\r\n\r\n        # compute gradient and do SGD step\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        if not args.distributed:\r\n            for p in model.parameters():\r\n                p.grad.data.div_(len(args.gpus))\r\n        optimizer.step()\r\n```\r\nIs it right?"}