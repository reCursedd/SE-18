{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7198", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7198/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7198/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7198/events", "html_url": "https://github.com/pytorch/pytorch/issues/7198", "id": 319678128, "node_id": "MDU6SXNzdWUzMTk2NzgxMjg=", "number": 7198, "title": "Why this script caused stack overflow exception?", "user": {"login": "ylxie", "id": 11731942, "node_id": "MDQ6VXNlcjExNzMxOTQy", "avatar_url": "https://avatars0.githubusercontent.com/u/11731942?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ylxie", "html_url": "https://github.com/ylxie", "followers_url": "https://api.github.com/users/ylxie/followers", "following_url": "https://api.github.com/users/ylxie/following{/other_user}", "gists_url": "https://api.github.com/users/ylxie/gists{/gist_id}", "starred_url": "https://api.github.com/users/ylxie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ylxie/subscriptions", "organizations_url": "https://api.github.com/users/ylxie/orgs", "repos_url": "https://api.github.com/users/ylxie/repos", "events_url": "https://api.github.com/users/ylxie/events{/privacy}", "received_events_url": "https://api.github.com/users/ylxie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-05-02T19:15:00Z", "updated_at": "2018-05-06T06:08:02Z", "closed_at": "2018-05-06T06:08:02Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I got a stack overflow exception for the following codes. Any help will be appreciated!!!</p>\n<p>The exception happens when calling \"loss.item()\".</p>\n<h2>Code example</h2>\n<pre><code>import torch\nimport torch.autograd\n\nimport random\n\nrandom.seed(1)\ntorch.manual_seed(1)\n\nclass DataGenerator:\n    def __init__(self, feature_dim, max_seq_len, num_train_samples):\n        self.feature_dim = feature_dim\n        self.max_seq_len = max_seq_len\n        self.num_train_samples = num_train_samples\n    \n    def get_next_batch(self, batch_size=1, num_batches=10):                \n        for b in range(num_batches):\n            samples = []\n            for i in range(batch_size):\n                length = random.randint(self.max_seq_len - 100, self.max_seq_len)\n                features = [self.create_feature() for j in range(length)]\n                target = random.randint(0, 1)\n                samples.append((features, target))\n            yield self.create_padded_batch(samples)\n    \n    def create_feature(self):\n        tensor = torch.zeros(1, self.feature_dim)\n        d = random.randint(0, self.feature_dim - 1)\n        tensor[0][d] = 1.0\n        return tensor\n\n    def create_padded_batch(self, samples):\n        sorted_samples = sorted(samples, key=lambda x:len(x[0]), reverse=True)\n        inputs = [torch.cat(x[0], dim=0) for x in sorted_samples]\n        input_lengths = [len(x) for x in inputs]\n        targets = [x[1] for x in sorted_samples]\n        target_batch = torch.FloatTensor(targets).view(-1,1)\n        packed_input = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n        return packed_input, input_lengths, target_batch\n\n\nclass LSTMRNNModule(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(LSTMRNNModule, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True, dropout=0)\n        self.output_layer = torch.nn.Linear(hidden_size, 1)\n    \n    def forward(self, input, input_lengths):\n        packed_input = torch.nn.utils.rnn.pack_padded_sequence(input, input_lengths, batch_first=True)\n        out, (hidden, cstate) = self.lstm(packed_input)\n        output = self.output_layer(hidden)\n        return output\n\nclass Trainer(object):\n\n    def __init__(self, feature_dim, hidden_size, data_set):\n        self.data_set = data_set\n        self.rnn = LSTMRNNModule(feature_dim, hidden_size)\n        self.criterion = torch.nn.BCEWithLogitsLoss(size_average=True, reduce=True)\n        self.optimizer = torch.optim.SGD(self.rnn.parameters(), lr=0.001, momentum=0.9)\n    \n    def train(self, max_iter, eval_every, batch_size, num_train_samples):\n\n        for it in range(max_iter):            \n            batch_num = 0\n            for X,L,Y in self.data_set.get_next_batch(batch_size, num_train_samples):\n                batch_num += 1\n                loss = self.train_iter(X,L,Y)                \n                print(\"{3}/{0}/{1}, Loss: {2:0.3f}\".format(it + 1, max_iter, loss, batch_num))\n    \n    def train_iter(self, input, input_lengths, target):\n        self.optimizer.zero_grad()\n        output = self.rnn(input, input_lengths)\n        output = output[0]\n        loss = self.criterion(output, target)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n\nFEATURE_DIM = 1000\nHIDDEN_SIZE = 32\nMAX_SEQ_LEN = 1500\nNUM_TRAIN_SAMPLES = 2000\nMAX_ITER = 30\nEVAL_EVERY = 1\nBATCH_SIZE = 1\n\ndata_set = DataGenerator(FEATURE_DIM, MAX_SEQ_LEN, NUM_TRAIN_SAMPLES)\n\ntrainer = Trainer(FEATURE_DIM, HIDDEN_SIZE, data_set)\n\ntrainer.train(MAX_ITER, EVAL_EVERY, BATCH_SIZE, NUM_TRAIN_SAMPLES)\n\n</code></pre>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0</p>\n<p>OS: Microsoft Windows 10 Enterprise<br>\nGCC version: Could not collect<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.60<br>\nGPU models and configuration: Could not collect<br>\nNvidia driver version: Could not collect<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.13.1)<br>\n[pip] numpydoc (0.7.0)<br>\n[pip] torch (0.4.0)<br>\n[pip] torchvision (0.2.0)<br>\n[conda] pytorch                   0.4.0           py36_cuda80_cudnn7he774522_1    pytorch<br>\n[conda] torchvision               0.2.0                     </p>", "body_text": "Issue description\nI got a stack overflow exception for the following codes. Any help will be appreciated!!!\nThe exception happens when calling \"loss.item()\".\nCode example\nimport torch\nimport torch.autograd\n\nimport random\n\nrandom.seed(1)\ntorch.manual_seed(1)\n\nclass DataGenerator:\n    def __init__(self, feature_dim, max_seq_len, num_train_samples):\n        self.feature_dim = feature_dim\n        self.max_seq_len = max_seq_len\n        self.num_train_samples = num_train_samples\n    \n    def get_next_batch(self, batch_size=1, num_batches=10):                \n        for b in range(num_batches):\n            samples = []\n            for i in range(batch_size):\n                length = random.randint(self.max_seq_len - 100, self.max_seq_len)\n                features = [self.create_feature() for j in range(length)]\n                target = random.randint(0, 1)\n                samples.append((features, target))\n            yield self.create_padded_batch(samples)\n    \n    def create_feature(self):\n        tensor = torch.zeros(1, self.feature_dim)\n        d = random.randint(0, self.feature_dim - 1)\n        tensor[0][d] = 1.0\n        return tensor\n\n    def create_padded_batch(self, samples):\n        sorted_samples = sorted(samples, key=lambda x:len(x[0]), reverse=True)\n        inputs = [torch.cat(x[0], dim=0) for x in sorted_samples]\n        input_lengths = [len(x) for x in inputs]\n        targets = [x[1] for x in sorted_samples]\n        target_batch = torch.FloatTensor(targets).view(-1,1)\n        packed_input = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n        return packed_input, input_lengths, target_batch\n\n\nclass LSTMRNNModule(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(LSTMRNNModule, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True, dropout=0)\n        self.output_layer = torch.nn.Linear(hidden_size, 1)\n    \n    def forward(self, input, input_lengths):\n        packed_input = torch.nn.utils.rnn.pack_padded_sequence(input, input_lengths, batch_first=True)\n        out, (hidden, cstate) = self.lstm(packed_input)\n        output = self.output_layer(hidden)\n        return output\n\nclass Trainer(object):\n\n    def __init__(self, feature_dim, hidden_size, data_set):\n        self.data_set = data_set\n        self.rnn = LSTMRNNModule(feature_dim, hidden_size)\n        self.criterion = torch.nn.BCEWithLogitsLoss(size_average=True, reduce=True)\n        self.optimizer = torch.optim.SGD(self.rnn.parameters(), lr=0.001, momentum=0.9)\n    \n    def train(self, max_iter, eval_every, batch_size, num_train_samples):\n\n        for it in range(max_iter):            \n            batch_num = 0\n            for X,L,Y in self.data_set.get_next_batch(batch_size, num_train_samples):\n                batch_num += 1\n                loss = self.train_iter(X,L,Y)                \n                print(\"{3}/{0}/{1}, Loss: {2:0.3f}\".format(it + 1, max_iter, loss, batch_num))\n    \n    def train_iter(self, input, input_lengths, target):\n        self.optimizer.zero_grad()\n        output = self.rnn(input, input_lengths)\n        output = output[0]\n        loss = self.criterion(output, target)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n\nFEATURE_DIM = 1000\nHIDDEN_SIZE = 32\nMAX_SEQ_LEN = 1500\nNUM_TRAIN_SAMPLES = 2000\nMAX_ITER = 30\nEVAL_EVERY = 1\nBATCH_SIZE = 1\n\ndata_set = DataGenerator(FEATURE_DIM, MAX_SEQ_LEN, NUM_TRAIN_SAMPLES)\n\ntrainer = Trainer(FEATURE_DIM, HIDDEN_SIZE, data_set)\n\ntrainer.train(MAX_ITER, EVAL_EVERY, BATCH_SIZE, NUM_TRAIN_SAMPLES)\n\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0\nOS: Microsoft Windows 10 Enterprise\nGCC version: Could not collect\nCMake version: Could not collect\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.60\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] numpy (1.13.1)\n[pip] numpydoc (0.7.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.0)\n[conda] pytorch                   0.4.0           py36_cuda80_cudnn7he774522_1    pytorch\n[conda] torchvision               0.2.0", "body": "## Issue description\r\n\r\nI got a stack overflow exception for the following codes. Any help will be appreciated!!!\r\n\r\nThe exception happens when calling \"loss.item()\".\r\n\r\n## Code example\r\n```\r\nimport torch\r\nimport torch.autograd\r\n\r\nimport random\r\n\r\nrandom.seed(1)\r\ntorch.manual_seed(1)\r\n\r\nclass DataGenerator:\r\n    def __init__(self, feature_dim, max_seq_len, num_train_samples):\r\n        self.feature_dim = feature_dim\r\n        self.max_seq_len = max_seq_len\r\n        self.num_train_samples = num_train_samples\r\n    \r\n    def get_next_batch(self, batch_size=1, num_batches=10):                \r\n        for b in range(num_batches):\r\n            samples = []\r\n            for i in range(batch_size):\r\n                length = random.randint(self.max_seq_len - 100, self.max_seq_len)\r\n                features = [self.create_feature() for j in range(length)]\r\n                target = random.randint(0, 1)\r\n                samples.append((features, target))\r\n            yield self.create_padded_batch(samples)\r\n    \r\n    def create_feature(self):\r\n        tensor = torch.zeros(1, self.feature_dim)\r\n        d = random.randint(0, self.feature_dim - 1)\r\n        tensor[0][d] = 1.0\r\n        return tensor\r\n\r\n    def create_padded_batch(self, samples):\r\n        sorted_samples = sorted(samples, key=lambda x:len(x[0]), reverse=True)\r\n        inputs = [torch.cat(x[0], dim=0) for x in sorted_samples]\r\n        input_lengths = [len(x) for x in inputs]\r\n        targets = [x[1] for x in sorted_samples]\r\n        target_batch = torch.FloatTensor(targets).view(-1,1)\r\n        packed_input = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\r\n        return packed_input, input_lengths, target_batch\r\n\r\n\r\nclass LSTMRNNModule(torch.nn.Module):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(LSTMRNNModule, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True, dropout=0)\r\n        self.output_layer = torch.nn.Linear(hidden_size, 1)\r\n    \r\n    def forward(self, input, input_lengths):\r\n        packed_input = torch.nn.utils.rnn.pack_padded_sequence(input, input_lengths, batch_first=True)\r\n        out, (hidden, cstate) = self.lstm(packed_input)\r\n        output = self.output_layer(hidden)\r\n        return output\r\n\r\nclass Trainer(object):\r\n\r\n    def __init__(self, feature_dim, hidden_size, data_set):\r\n        self.data_set = data_set\r\n        self.rnn = LSTMRNNModule(feature_dim, hidden_size)\r\n        self.criterion = torch.nn.BCEWithLogitsLoss(size_average=True, reduce=True)\r\n        self.optimizer = torch.optim.SGD(self.rnn.parameters(), lr=0.001, momentum=0.9)\r\n    \r\n    def train(self, max_iter, eval_every, batch_size, num_train_samples):\r\n\r\n        for it in range(max_iter):            \r\n            batch_num = 0\r\n            for X,L,Y in self.data_set.get_next_batch(batch_size, num_train_samples):\r\n                batch_num += 1\r\n                loss = self.train_iter(X,L,Y)                \r\n                print(\"{3}/{0}/{1}, Loss: {2:0.3f}\".format(it + 1, max_iter, loss, batch_num))\r\n    \r\n    def train_iter(self, input, input_lengths, target):\r\n        self.optimizer.zero_grad()\r\n        output = self.rnn(input, input_lengths)\r\n        output = output[0]\r\n        loss = self.criterion(output, target)\r\n        loss.backward()\r\n        self.optimizer.step()\r\n        return loss.item()\r\n\r\n\r\nFEATURE_DIM = 1000\r\nHIDDEN_SIZE = 32\r\nMAX_SEQ_LEN = 1500\r\nNUM_TRAIN_SAMPLES = 2000\r\nMAX_ITER = 30\r\nEVAL_EVERY = 1\r\nBATCH_SIZE = 1\r\n\r\ndata_set = DataGenerator(FEATURE_DIM, MAX_SEQ_LEN, NUM_TRAIN_SAMPLES)\r\n\r\ntrainer = Trainer(FEATURE_DIM, HIDDEN_SIZE, data_set)\r\n\r\ntrainer.train(MAX_ITER, EVAL_EVERY, BATCH_SIZE, NUM_TRAIN_SAMPLES)\r\n\r\n```\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.60\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.1)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.0)\r\n[conda] pytorch                   0.4.0           py36_cuda80_cudnn7he774522_1    pytorch\r\n[conda] torchvision               0.2.0                     <pip>\r\n"}