{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/318186180", "html_url": "https://github.com/pytorch/pytorch/pull/2211#issuecomment-318186180", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2211", "id": 318186180, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODE4NjE4MA==", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T21:20:45Z", "updated_at": "2017-07-26T22:43:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Not a bug per se, just not implemented in a numerically stable way. i.e.</p>\n<pre><code>log(sigmoid(x)) \n= log(1/(1 + e^-x))\n= log(1) - log(1 + e^-x)\n= -log(1 + e^-x) \n</code></pre>\n<p>when x is large and negative (i.e. -300) <code>log(1 + e^300)</code> returns <code>inf</code></p>\n<p>In this PR, I use the log-sum-exp trick to avoid the overflow:</p>\n<pre><code>-log(1 + e^-x)\n=-log(e^0 + e^-x) \n= max(-x, 0) + log(e^-max(-x, 0) + e^(-x - max(-x,0)))\n</code></pre>\n<p>so now <code>nn.LogSigmoid(Variable(torch.FloatTensor([-300.0]))</code> returns <code>-300</code>.</p>", "body_text": "@apaszke Not a bug per se, just not implemented in a numerically stable way. i.e.\nlog(sigmoid(x)) \n= log(1/(1 + e^-x))\n= log(1) - log(1 + e^-x)\n= -log(1 + e^-x) \n\nwhen x is large and negative (i.e. -300) log(1 + e^300) returns inf\nIn this PR, I use the log-sum-exp trick to avoid the overflow:\n-log(1 + e^-x)\n=-log(e^0 + e^-x) \n= max(-x, 0) + log(e^-max(-x, 0) + e^(-x - max(-x,0)))\n\nso now nn.LogSigmoid(Variable(torch.FloatTensor([-300.0])) returns -300.", "body": "@apaszke Not a bug per se, just not implemented in a numerically stable way. i.e.\r\n```\r\nlog(sigmoid(x)) \r\n= log(1/(1 + e^-x))\r\n= log(1) - log(1 + e^-x)\r\n= -log(1 + e^-x) \r\n```\r\nwhen x is large and negative (i.e. -300) `log(1 + e^300)` returns `inf`\r\n\r\nIn this PR, I use the log-sum-exp trick to avoid the overflow:\r\n```\r\n-log(1 + e^-x)\r\n=-log(e^0 + e^-x) \r\n= max(-x, 0) + log(e^-max(-x, 0) + e^(-x - max(-x,0)))\r\n```\r\nso now `nn.LogSigmoid(Variable(torch.FloatTensor([-300.0]))` returns `-300`."}