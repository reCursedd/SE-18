{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/289371215", "html_url": "https://github.com/pytorch/pytorch/issues/1081#issuecomment-289371215", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1081", "id": 289371215, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTM3MTIxNQ==", "user": {"login": "david-leon", "id": 23473037, "node_id": "MDQ6VXNlcjIzNDczMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/23473037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/david-leon", "html_url": "https://github.com/david-leon", "followers_url": "https://api.github.com/users/david-leon/followers", "following_url": "https://api.github.com/users/david-leon/following{/other_user}", "gists_url": "https://api.github.com/users/david-leon/gists{/gist_id}", "starred_url": "https://api.github.com/users/david-leon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/david-leon/subscriptions", "organizations_url": "https://api.github.com/users/david-leon/orgs", "repos_url": "https://api.github.com/users/david-leon/repos", "events_url": "https://api.github.com/users/david-leon/events{/privacy}", "received_events_url": "https://api.github.com/users/david-leon/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-27T07:08:57Z", "updated_at": "2017-03-27T07:11:53Z", "author_association": "NONE", "body_html": "<p>I modified the issue title, because except for the above issues, I've also experienced the following inconvenience:</p>\n<p>Though I love pytorch, I've to say that the design of current tensor constructors of pytorch is UGLY.<br>\nThere're so many different constructors (Tensor, FloatTensor, ByteTensor ... on CPU, and cuda.FloatTensor, cuda.xxx on GPU), and making the situation worse these constructors are behaving differently. To name a few, we can create a FloatTensor by:</p>\n<p><code> x_t = torch.FloatTensor(np.random.rand(3,5))</code>  or<br>\n<code> x_t = torch.from_numpy(np.random.rand(3,5).astype(np.float32))</code></p>\n<p>whereas when it comes to GPU, the same syntax won't work. Even both on CPU, the FloatTensor constructor behaves differently with ByteTensor.</p>\n<p>I'd suggest to unify all these constructors and their behaviors, making them more consistent with numpy interface. For example, I'd want:<br>\n<code> x_t = torch.Tensor(np.random.randint(0,5, (3,5), np.uint8), torch.Byte, device=1)</code><br>\nto create a ByteTensor on GPU1 initialized with numpy array.</p>", "body_text": "I modified the issue title, because except for the above issues, I've also experienced the following inconvenience:\nThough I love pytorch, I've to say that the design of current tensor constructors of pytorch is UGLY.\nThere're so many different constructors (Tensor, FloatTensor, ByteTensor ... on CPU, and cuda.FloatTensor, cuda.xxx on GPU), and making the situation worse these constructors are behaving differently. To name a few, we can create a FloatTensor by:\n x_t = torch.FloatTensor(np.random.rand(3,5))  or\n x_t = torch.from_numpy(np.random.rand(3,5).astype(np.float32))\nwhereas when it comes to GPU, the same syntax won't work. Even both on CPU, the FloatTensor constructor behaves differently with ByteTensor.\nI'd suggest to unify all these constructors and their behaviors, making them more consistent with numpy interface. For example, I'd want:\n x_t = torch.Tensor(np.random.randint(0,5, (3,5), np.uint8), torch.Byte, device=1)\nto create a ByteTensor on GPU1 initialized with numpy array.", "body": "I modified the issue title, because except for the above issues, I've also experienced the following inconvenience:\r\n\r\nThough I love pytorch, I've to say that the design of current tensor constructors of pytorch is UGLY.\r\nThere're so many different constructors (Tensor, FloatTensor, ByteTensor ... on CPU, and cuda.FloatTensor, cuda.xxx on GPU), and making the situation worse these constructors are behaving differently. To name a few, we can create a FloatTensor by:\r\n\r\n` x_t = torch.FloatTensor(np.random.rand(3,5))`  or\r\n` x_t = torch.from_numpy(np.random.rand(3,5).astype(np.float32))`\r\n\r\nwhereas when it comes to GPU, the same syntax won't work. Even both on CPU, the FloatTensor constructor behaves differently with ByteTensor.\r\n\r\nI'd suggest to unify all these constructors and their behaviors, making them more consistent with numpy interface. For example, I'd want:\r\n` x_t = torch.Tensor(np.random.randint(0,5, (3,5), np.uint8), torch.Byte, device=1)`\r\n to create a ByteTensor on GPU1 initialized with numpy array."}