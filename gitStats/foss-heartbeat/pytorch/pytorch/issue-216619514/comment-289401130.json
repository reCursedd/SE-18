{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/289401130", "html_url": "https://github.com/pytorch/pytorch/issues/1081#issuecomment-289401130", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1081", "id": 289401130, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTQwMTEzMA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-27T09:28:32Z", "updated_at": "2017-03-27T09:28:32Z", "author_association": "MEMBER", "body_html": "<p>I don't understand how that makes it more consistent and less ugly. All tensor constructors are necessary, because tensors containing the data of different types are different classes by design too, and taking the path you suggest seems to add a lot of redundancy and limits extensibility.</p>\n<p>First thing, there are many ways to construct a tensor and these lines are not equivalent:</p>\n<div class=\"highlight highlight-source-python\"><pre>x_t <span class=\"pl-k\">=</span> torch.FloatTensor(np.random.rand(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">5</span>))\nx_t <span class=\"pl-k\">=</span> torch.from_numpy(np.random.rand(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">5</span>).astype(np.float32))</pre></div>\n<p>The first one guarantees to give you a FloatTensor, no matter what you give it, and if it happens to be a float ndarray, then it will construct it without performing any memory copy.<br>\nThe second one guarantees to give you a CPU tensor, that matches the type of a given array, and that shares the same memory.<br>\nYou can't use this syntax with GPU, because there is no GPU numpy array, and it wouldn't have the same semantics (it wouldn't share the data in memory). I have no idea how CPU FloatTensor constructors behave differently than CPU ByteTensor constructors, please be more specific.</p>\n<p>This line is equivalent to the one you proposed. There's no need for putting everything in the constructor.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> note that the .type(...) part is unnecessary</span>\nx_t <span class=\"pl-k\">=</span> torch.from_numpy(np.random.randing(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">5</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>), np.uint8)).type(torch.ByteTensor).cuda(<span class=\"pl-c1\">1</span>)</pre></div>", "body_text": "I don't understand how that makes it more consistent and less ugly. All tensor constructors are necessary, because tensors containing the data of different types are different classes by design too, and taking the path you suggest seems to add a lot of redundancy and limits extensibility.\nFirst thing, there are many ways to construct a tensor and these lines are not equivalent:\nx_t = torch.FloatTensor(np.random.rand(3,5))\nx_t = torch.from_numpy(np.random.rand(3,5).astype(np.float32))\nThe first one guarantees to give you a FloatTensor, no matter what you give it, and if it happens to be a float ndarray, then it will construct it without performing any memory copy.\nThe second one guarantees to give you a CPU tensor, that matches the type of a given array, and that shares the same memory.\nYou can't use this syntax with GPU, because there is no GPU numpy array, and it wouldn't have the same semantics (it wouldn't share the data in memory). I have no idea how CPU FloatTensor constructors behave differently than CPU ByteTensor constructors, please be more specific.\nThis line is equivalent to the one you proposed. There's no need for putting everything in the constructor.\n# note that the .type(...) part is unnecessary\nx_t = torch.from_numpy(np.random.randing(0, 5, (3, 5), np.uint8)).type(torch.ByteTensor).cuda(1)", "body": "I don't understand how that makes it more consistent and less ugly. All tensor constructors are necessary, because tensors containing the data of different types are different classes by design too, and taking the path you suggest seems to add a lot of redundancy and limits extensibility.\r\n\r\nFirst thing, there are many ways to construct a tensor and these lines are not equivalent:\r\n```python\r\nx_t = torch.FloatTensor(np.random.rand(3,5))\r\nx_t = torch.from_numpy(np.random.rand(3,5).astype(np.float32))\r\n```\r\n\r\nThe first one guarantees to give you a FloatTensor, no matter what you give it, and if it happens to be a float ndarray, then it will construct it without performing any memory copy.\r\nThe second one guarantees to give you a CPU tensor, that matches the type of a given array, and that shares the same memory.\r\nYou can't use this syntax with GPU, because there is no GPU numpy array, and it wouldn't have the same semantics (it wouldn't share the data in memory). I have no idea how CPU FloatTensor constructors behave differently than CPU ByteTensor constructors, please be more specific.\r\n\r\nThis line is equivalent to the one you proposed. There's no need for putting everything in the constructor.\r\n```python\r\n# note that the .type(...) part is unnecessary\r\nx_t = torch.from_numpy(np.random.randing(0, 5, (3, 5), np.uint8)).type(torch.ByteTensor).cuda(1)\r\n```\r\n\r\n"}