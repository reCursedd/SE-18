{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388931028", "html_url": "https://github.com/pytorch/pytorch/issues/7014#issuecomment-388931028", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7014", "id": 388931028, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODkzMTAyOA==", "user": {"login": "kylemcdonald", "id": 157106, "node_id": "MDQ6VXNlcjE1NzEwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/157106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kylemcdonald", "html_url": "https://github.com/kylemcdonald", "followers_url": "https://api.github.com/users/kylemcdonald/followers", "following_url": "https://api.github.com/users/kylemcdonald/following{/other_user}", "gists_url": "https://api.github.com/users/kylemcdonald/gists{/gist_id}", "starred_url": "https://api.github.com/users/kylemcdonald/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kylemcdonald/subscriptions", "organizations_url": "https://api.github.com/users/kylemcdonald/orgs", "repos_url": "https://api.github.com/users/kylemcdonald/repos", "events_url": "https://api.github.com/users/kylemcdonald/events{/privacy}", "received_events_url": "https://api.github.com/users/kylemcdonald/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-14T19:16:16Z", "updated_at": "2018-05-14T19:31:02Z", "author_association": "NONE", "body_html": "<p>Thank you <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a>. I didn't make the connection that a log probability distribution is equivalent to another that has been shifted by a constant amount (because shifting/addition of log probabilities is equivalent to multiplication of regular probabilities, and is therefore removed by the normalization step).</p>\n<p>With that in mind, I think I have been able to define a more direct way of computing <code>multinomial</code> with a temperature, on probabilities or log probabilities, with no chance of getting NaN/infinities/crashes.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">multinomial</span>(<span class=\"pl-smi\">probs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">logits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">temperature</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">num_samples</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                     <span class=\"pl-smi\">min_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-20</span>, <span class=\"pl-smi\">max_logit</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e+20</span>,\n                     <span class=\"pl-smi\">min_temperature</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-20</span>, <span class=\"pl-smi\">max_temperature</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e+20</span>):\n    <span class=\"pl-k\">if</span> probs <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        probs <span class=\"pl-k\">=</span> probs.clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span>min_prob)\n        logits <span class=\"pl-k\">=</span> torch.log(probs)\n    logits <span class=\"pl-k\">=</span> logits.clamp(<span class=\"pl-v\">max</span><span class=\"pl-k\">=</span>max_logit)\n    temperature <span class=\"pl-k\">=</span> np.clip(temperature, min_temperature, max_temperature)\n    logits <span class=\"pl-k\">=</span> (logits <span class=\"pl-k\">-</span> logits.max()) <span class=\"pl-k\">/</span> temperature\n    probs <span class=\"pl-k\">=</span> torch.exp(logits)\n    <span class=\"pl-k\">return</span> torch.multinomial(probs, num_samples)</pre></div>\n<p>Note that I still need to clamp the max logit, but because of <code>logits - logits.max()</code> it can be clamped to a much larger value.</p>\n<p>Here are the cases I tested with. Obviously many of these are undefined, but none of them crash or return NaNs:</p>\n<div class=\"highlight highlight-source-python\"><pre>test_cases <span class=\"pl-k\">=</span> [\n    [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>+inf<span class=\"pl-pds\">'</span></span>)],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>)],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1e+20</span>],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1e-20</span>],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1e+20</span>],\n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1e-20</span>]\n]\n\n<span class=\"pl-k\">for</span> temperature <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>), <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1e-20</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1e+20</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>+inf<span class=\"pl-pds\">'</span></span>)]: \n    <span class=\"pl-k\">for</span> test_case <span class=\"pl-k\">in</span> test_cases:\n        <span class=\"pl-k\">for</span> device <span class=\"pl-k\">in</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>]:\n            x <span class=\"pl-k\">=</span> torch.Tensor(test_case)\n            <span class=\"pl-c1\">print</span>(multinomial(<span class=\"pl-v\">probs</span><span class=\"pl-k\">=</span>x.to(device), <span class=\"pl-v\">temperature</span><span class=\"pl-k\">=</span>temperature))\n            <span class=\"pl-c1\">print</span>(multinomial(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>x.to(device), <span class=\"pl-v\">temperature</span><span class=\"pl-k\">=</span>temperature))</pre></div>", "body_text": "Thank you @t-vi. I didn't make the connection that a log probability distribution is equivalent to another that has been shifted by a constant amount (because shifting/addition of log probabilities is equivalent to multiplication of regular probabilities, and is therefore removed by the normalization step).\nWith that in mind, I think I have been able to define a more direct way of computing multinomial with a temperature, on probabilities or log probabilities, with no chance of getting NaN/infinities/crashes.\ndef multinomial(probs=None, logits=None, temperature=1, num_samples=1,\n                     min_prob=1e-20, max_logit=1e+20,\n                     min_temperature=1e-20, max_temperature=1e+20):\n    if probs is not None:\n        probs = probs.clamp(min=min_prob)\n        logits = torch.log(probs)\n    logits = logits.clamp(max=max_logit)\n    temperature = np.clip(temperature, min_temperature, max_temperature)\n    logits = (logits - logits.max()) / temperature\n    probs = torch.exp(logits)\n    return torch.multinomial(probs, num_samples)\nNote that I still need to clamp the max logit, but because of logits - logits.max() it can be clamped to a much larger value.\nHere are the cases I tested with. Obviously many of these are undefined, but none of them crash or return NaNs:\ntest_cases = [\n    [0,0,0],\n    [1,1,1],\n    [1,0,float('+inf')],\n    [1,0,float('-inf')],\n    [1,0,1e+20],\n    [1,0,1e-20],\n    [1,0,-1e+20],\n    [1,0,-1e-20]\n]\n\nfor temperature in [float('-inf'), 0, 1e-20, 1, 1e+20, float('+inf')]: \n    for test_case in test_cases:\n        for device in ['cpu', 'cuda']:\n            x = torch.Tensor(test_case)\n            print(multinomial(probs=x.to(device), temperature=temperature))\n            print(multinomial(logits=x.to(device), temperature=temperature))", "body": "Thank you @t-vi. I didn't make the connection that a log probability distribution is equivalent to another that has been shifted by a constant amount (because shifting/addition of log probabilities is equivalent to multiplication of regular probabilities, and is therefore removed by the normalization step).\r\n\r\nWith that in mind, I think I have been able to define a more direct way of computing `multinomial` with a temperature, on probabilities or log probabilities, with no chance of getting NaN/infinities/crashes.\r\n\r\n```python\r\ndef multinomial(probs=None, logits=None, temperature=1, num_samples=1,\r\n                     min_prob=1e-20, max_logit=1e+20,\r\n                     min_temperature=1e-20, max_temperature=1e+20):\r\n    if probs is not None:\r\n        probs = probs.clamp(min=min_prob)\r\n        logits = torch.log(probs)\r\n    logits = logits.clamp(max=max_logit)\r\n    temperature = np.clip(temperature, min_temperature, max_temperature)\r\n    logits = (logits - logits.max()) / temperature\r\n    probs = torch.exp(logits)\r\n    return torch.multinomial(probs, num_samples)\r\n```\r\n\r\nNote that I still need to clamp the max logit, but because of `logits - logits.max()` it can be clamped to a much larger value.\r\n\r\nHere are the cases I tested with. Obviously many of these are undefined, but none of them crash or return NaNs:\r\n\r\n```python\r\ntest_cases = [\r\n    [0,0,0],\r\n    [1,1,1],\r\n    [1,0,float('+inf')],\r\n    [1,0,float('-inf')],\r\n    [1,0,1e+20],\r\n    [1,0,1e-20],\r\n    [1,0,-1e+20],\r\n    [1,0,-1e-20]\r\n]\r\n\r\nfor temperature in [float('-inf'), 0, 1e-20, 1, 1e+20, float('+inf')]: \r\n    for test_case in test_cases:\r\n        for device in ['cpu', 'cuda']:\r\n            x = torch.Tensor(test_case)\r\n            print(multinomial(probs=x.to(device), temperature=temperature))\r\n            print(multinomial(logits=x.to(device), temperature=temperature))\r\n```"}