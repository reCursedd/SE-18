{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388641152", "html_url": "https://github.com/pytorch/pytorch/issues/7014#issuecomment-388641152", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7014", "id": 388641152, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODY0MTE1Mg==", "user": {"login": "kylemcdonald", "id": 157106, "node_id": "MDQ6VXNlcjE1NzEwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/157106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kylemcdonald", "html_url": "https://github.com/kylemcdonald", "followers_url": "https://api.github.com/users/kylemcdonald/followers", "following_url": "https://api.github.com/users/kylemcdonald/following{/other_user}", "gists_url": "https://api.github.com/users/kylemcdonald/gists{/gist_id}", "starred_url": "https://api.github.com/users/kylemcdonald/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kylemcdonald/subscriptions", "organizations_url": "https://api.github.com/users/kylemcdonald/orgs", "repos_url": "https://api.github.com/users/kylemcdonald/repos", "events_url": "https://api.github.com/users/kylemcdonald/events{/privacy}", "received_events_url": "https://api.github.com/users/kylemcdonald/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-13T17:03:08Z", "updated_at": "2018-05-13T17:03:15Z", "author_association": "NONE", "body_html": "<p><code>CUDA_LAUNCH_BLOCKING=1</code> doesn't actually change the error in this case. I get the same obscure error message whether I run with or without that flag.</p>\n<p>By <code>logits</code> I mean the same kind of parameter that is passed into any of the <a href=\"https://pytorch.org/docs/master/distributions.html\" rel=\"nofollow\">https://pytorch.org/docs/master/distributions.html</a> objects. I'm not great with probabilities, but I'm pretty sure sampling from log probabilities normalized by the sum of their values is not the same as sampling after normalizing after taking the <code>exp</code> of their values, and that's why all the <code>distribution</code> objects offer both? The logits aren't infinities until they are <code>exp</code>'d so I can imagine an implementation that goes like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">multinomial</span>(<span class=\"pl-smi\">probs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">logits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">temperature</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">if</span> probs:\n        logits <span class=\"pl-k\">=</span> torch.log(probs)\n    <span class=\"pl-k\">if</span> temperature:\n        logits <span class=\"pl-k\">/=</span> temperature\n    logits <span class=\"pl-k\">=</span> torch.min(logits, torch.Tensor([<span class=\"pl-c1\">40</span>.]))\n    probs <span class=\"pl-k\">=</span> torch.exp(logits)\n    <span class=\"pl-k\">return</span> torch.multinomial(probs, <span class=\"pl-c1\">1</span>)</pre></div>\n<p><code>40.</code> being near the point where <code>exp</code> starts to create infinity. But I would hope someone with more experience than me has a solution to this conundrum :)</p>", "body_text": "CUDA_LAUNCH_BLOCKING=1 doesn't actually change the error in this case. I get the same obscure error message whether I run with or without that flag.\nBy logits I mean the same kind of parameter that is passed into any of the https://pytorch.org/docs/master/distributions.html objects. I'm not great with probabilities, but I'm pretty sure sampling from log probabilities normalized by the sum of their values is not the same as sampling after normalizing after taking the exp of their values, and that's why all the distribution objects offer both? The logits aren't infinities until they are exp'd so I can imagine an implementation that goes like:\ndef multinomial(probs=None, logits=None, temperature=None):\n    if probs:\n        logits = torch.log(probs)\n    if temperature:\n        logits /= temperature\n    logits = torch.min(logits, torch.Tensor([40.]))\n    probs = torch.exp(logits)\n    return torch.multinomial(probs, 1)\n40. being near the point where exp starts to create infinity. But I would hope someone with more experience than me has a solution to this conundrum :)", "body": "`CUDA_LAUNCH_BLOCKING=1` doesn't actually change the error in this case. I get the same obscure error message whether I run with or without that flag.\r\n\r\nBy `logits` I mean the same kind of parameter that is passed into any of the https://pytorch.org/docs/master/distributions.html objects. I'm not great with probabilities, but I'm pretty sure sampling from log probabilities normalized by the sum of their values is not the same as sampling after normalizing after taking the `exp` of their values, and that's why all the `distribution` objects offer both? The logits aren't infinities until they are `exp`'d so I can imagine an implementation that goes like:\r\n\r\n```python\r\ndef multinomial(probs=None, logits=None, temperature=None):\r\n    if probs:\r\n        logits = torch.log(probs)\r\n    if temperature:\r\n        logits /= temperature\r\n    logits = torch.min(logits, torch.Tensor([40.]))\r\n    probs = torch.exp(logits)\r\n    return torch.multinomial(probs, 1)\r\n```\r\n\r\n`40.` being near the point where `exp` starts to create infinity. But I would hope someone with more experience than me has a solution to this conundrum :)"}