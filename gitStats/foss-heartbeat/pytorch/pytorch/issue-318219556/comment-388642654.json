{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388642654", "html_url": "https://github.com/pytorch/pytorch/issues/7014#issuecomment-388642654", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7014", "id": 388642654, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODY0MjY1NA==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-13T17:26:51Z", "updated_at": "2018-05-13T19:13:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Using <code>CUDA_LAUNCH_BLOCKING=1</code> won't change the error message, but it removes the asynchronous behaviour that keeps you from seeing the place where it happens.<br>\n~~~I think the <code>min</code> is probably a good solution in your specific case, but it might be more idiomatic to use  <code>probs = logits.clamp(max=40.).exp()</code>.~~~<br>\n<strong>Edit:</strong> As you ask: You can safely do <code>probs = torch.exp(logits - logits.max())</code>. This will subtract the same term from the <code>logits</code> and thus scale the <code>probs</code> by the same value. This handles the <code>inf</code> but gives the same result in all other cases. If you have more than one set of logits, use <code>logits.max(1)[0]</code> or so.</p>", "body_text": "Using CUDA_LAUNCH_BLOCKING=1 won't change the error message, but it removes the asynchronous behaviour that keeps you from seeing the place where it happens.\n~~~I think the min is probably a good solution in your specific case, but it might be more idiomatic to use  probs = logits.clamp(max=40.).exp().~~~\nEdit: As you ask: You can safely do probs = torch.exp(logits - logits.max()). This will subtract the same term from the logits and thus scale the probs by the same value. This handles the inf but gives the same result in all other cases. If you have more than one set of logits, use logits.max(1)[0] or so.", "body": "Using `CUDA_LAUNCH_BLOCKING=1` won't change the error message, but it removes the asynchronous behaviour that keeps you from seeing the place where it happens.\r\n~~~I think the `min` is probably a good solution in your specific case, but it might be more idiomatic to use  `probs = logits.clamp(max=40.).exp()`.~~~\r\n**Edit:** As you ask: You can safely do `probs = torch.exp(logits - logits.max())`. This will subtract the same term from the `logits` and thus scale the `probs` by the same value. This handles the `inf` but gives the same result in all other cases. If you have more than one set of logits, use `logits.max(1)[0]` or so."}