{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7014", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7014/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7014/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7014/events", "html_url": "https://github.com/pytorch/pytorch/issues/7014", "id": 318219556, "node_id": "MDU6SXNzdWUzMTgyMTk1NTY=", "number": 7014, "title": "multinomial with inf fails in cuda", "user": {"login": "kylemcdonald", "id": 157106, "node_id": "MDQ6VXNlcjE1NzEwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/157106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kylemcdonald", "html_url": "https://github.com/kylemcdonald", "followers_url": "https://api.github.com/users/kylemcdonald/followers", "following_url": "https://api.github.com/users/kylemcdonald/following{/other_user}", "gists_url": "https://api.github.com/users/kylemcdonald/gists{/gist_id}", "starred_url": "https://api.github.com/users/kylemcdonald/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kylemcdonald/subscriptions", "organizations_url": "https://api.github.com/users/kylemcdonald/orgs", "repos_url": "https://api.github.com/users/kylemcdonald/repos", "events_url": "https://api.github.com/users/kylemcdonald/events{/privacy}", "received_events_url": "https://api.github.com/users/kylemcdonald/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-04-26T22:32:33Z", "updated_at": "2018-06-14T16:47:36Z", "closed_at": "2018-06-14T16:47:36Z", "author_association": "NONE", "body_html": "<p><code>torch.multinomial</code> throws a <code>RuntimeError</code> if the probability distribution includes <code>float('inf')</code>. On CPU there is no similar problem. This can happen if you take a (what is probably naive) approach to sampling logits like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_temp</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">temperature</span>):\n    probs <span class=\"pl-k\">=</span> torch.exp(logits <span class=\"pl-k\">/</span> temperature)\n    <span class=\"pl-k\">return</span> torch.multinomial(probs, <span class=\"pl-c1\">1</span>)</pre></div>\n<p>It's more likely to cause a problem if your logits are very large or your temperature is very small.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nx <span class=\"pl-k\">=</span> torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inf<span class=\"pl-pds\">'</span></span>)])\n<span class=\"pl-c1\">print</span>(torch.multinomial(x, <span class=\"pl-c1\">1</span>))\n<span class=\"pl-c1\">print</span>(torch.multinomial(x.to(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>), <span class=\"pl-c1\">1</span>))</pre></div>\n<div class=\"highlight highlight-text-python-traceback\"><pre>tensor([ 1])\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-7-98dbbe29edf5&gt; in &lt;module&gt;()\n      <span class=\"pl-c1\">2</span> x <span class=\"pl-k\">=</span> torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inf<span class=\"pl-pds\">'</span></span>)])\n      <span class=\"pl-c1\">3</span> <span class=\"pl-c1\">print</span>(torch.multinomial(x, <span class=\"pl-c1\">1</span>))\n----&gt; 4 print(torch.multinomial(x.to('cuda'), 1))\n\n<span class=\"pl-en\">RuntimeError</span>: <span class=\"pl-s\">cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCTensorCopy.c:20</span></pre></div>\n<h2>System Info</h2>\n<pre><code>PyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\n\nNvidia driver version: 390.48\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.2\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] numpy (1.14.2)\n[pip] numpydoc (0.7.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] cuda80                    1.0                           0    soumith\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch\n</code></pre>\n<h2>Back story</h2>\n<p>It may look simple, but this was a particularly difficult bug to track down :) I moved my RNN sampling from CPU (with numpy) to GPU (with torch) and started getting errors like:</p>\n<ul>\n<li><code>RuntimeError: CUDNN_STATUS_EXECUTION_FAILED</code></li>\n<li><code>RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCTensorCopy.c:20</code></li>\n<li><code>RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generated/../THCReduceAll.cuh:339</code></li>\n</ul>\n<p>I saw <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"296785079\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5213\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5213/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5213\">#5213</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"311543343\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6306\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6306/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6306\">#6306</a> had a similar error, which made me think it was related to my sampling code not moving data around correctly, but this didn't explain the intermittent behavior. I audited that and continued searching. Then I saw <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"302149664\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5561\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5561/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5561\">#5561</a> which made me check that my embedding indices were being set correctly (not out of bounds). So I looked closer into multinomial and found this bug.</p>\n<p>One way to get around the sampling bug above is to just do everything on the CPU. Another solution is to use Gumbel-softmax style sampling:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sample</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">temperature</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>):\n    <span class=\"pl-k\">if</span> temperature <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">return</span> torch.argmax(logits, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    noise <span class=\"pl-k\">=</span> torch.FloatTensor(logits.shape).to(logits.device)\n    noise.uniform_(<span class=\"pl-c1\">1e-5</span>, <span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">return</span> torch.argmax(logits <span class=\"pl-k\">/</span> temperature <span class=\"pl-k\">-</span> torch.log(<span class=\"pl-k\">-</span>torch.log(noise)), <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)</pre></div>\n<p>But unfortunately this takes longer to execute than the first round of sampling code.</p>", "body_text": "torch.multinomial throws a RuntimeError if the probability distribution includes float('inf'). On CPU there is no similar problem. This can happen if you take a (what is probably naive) approach to sampling logits like this:\ndef sample_temp(logits, temperature):\n    probs = torch.exp(logits / temperature)\n    return torch.multinomial(probs, 1)\nIt's more likely to cause a problem if your logits are very large or your temperature is very small.\nCode example\nimport torch\nx = torch.Tensor([0, float('inf')])\nprint(torch.multinomial(x, 1))\nprint(torch.multinomial(x.to('cuda'), 1))\ntensor([ 1])\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-7-98dbbe29edf5> in <module>()\n      2 x = torch.Tensor([0, float('inf')])\n      3 print(torch.multinomial(x, 1))\n----> 4 print(torch.multinomial(x.to('cuda'), 1))\n\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCTensorCopy.c:20\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\n\nNvidia driver version: 390.48\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.2\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] numpy (1.14.2)\n[pip] numpydoc (0.7.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] cuda80                    1.0                           0    soumith\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch\n\nBack story\nIt may look simple, but this was a particularly difficult bug to track down :) I moved my RNN sampling from CPU (with numpy) to GPU (with torch) and started getting errors like:\n\nRuntimeError: CUDNN_STATUS_EXECUTION_FAILED\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCTensorCopy.c:20\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generated/../THCReduceAll.cuh:339\n\nI saw #5213 and #6306 had a similar error, which made me think it was related to my sampling code not moving data around correctly, but this didn't explain the intermittent behavior. I audited that and continued searching. Then I saw #5561 which made me check that my embedding indices were being set correctly (not out of bounds). So I looked closer into multinomial and found this bug.\nOne way to get around the sampling bug above is to just do everything on the CPU. Another solution is to use Gumbel-softmax style sampling:\ndef sample(logits, temperature=1.0):\n    if temperature == 0:\n        return torch.argmax(logits, dim=-1)\n    noise = torch.FloatTensor(logits.shape).to(logits.device)\n    noise.uniform_(1e-5, 1-1e-5)\n    return torch.argmax(logits / temperature - torch.log(-torch.log(noise)), dim=-1)\nBut unfortunately this takes longer to execute than the first round of sampling code.", "body": "`torch.multinomial` throws a `RuntimeError` if the probability distribution includes `float('inf')`. On CPU there is no similar problem. This can happen if you take a (what is probably naive) approach to sampling logits like this:\r\n\r\n```python\r\ndef sample_temp(logits, temperature):\r\n    probs = torch.exp(logits / temperature)\r\n    return torch.multinomial(probs, 1)\r\n```\r\n\r\nIt's more likely to cause a problem if your logits are very large or your temperature is very small.\r\n\r\n## Code example\r\n\r\n```python\r\nimport torch\r\nx = torch.Tensor([0, float('inf')])\r\nprint(torch.multinomial(x, 1))\r\nprint(torch.multinomial(x.to('cuda'), 1))\r\n```\r\n\r\n```python-traceback\r\ntensor([ 1])\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-98dbbe29edf5> in <module>()\r\n      2 x = torch.Tensor([0, float('inf')])\r\n      3 print(torch.multinomial(x, 1))\r\n----> 4 print(torch.multinomial(x.to('cuda'), 1))\r\n\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCTensorCopy.c:20\r\n```\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.2\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.2)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda80                    1.0                           0    soumith\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n```\r\n\r\n## Back story\r\n\r\nIt may look simple, but this was a particularly difficult bug to track down :) I moved my RNN sampling from CPU (with numpy) to GPU (with torch) and started getting errors like:\r\n\r\n* `RuntimeError: CUDNN_STATUS_EXECUTION_FAILED`\r\n* `RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCTensorCopy.c:20`\r\n* `RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generated/../THCReduceAll.cuh:339`\r\n\r\nI saw https://github.com/pytorch/pytorch/issues/5213 and https://github.com/pytorch/pytorch/issues/6306 had a similar error, which made me think it was related to my sampling code not moving data around correctly, but this didn't explain the intermittent behavior. I audited that and continued searching. Then I saw https://github.com/pytorch/pytorch/issues/5561 which made me check that my embedding indices were being set correctly (not out of bounds). So I looked closer into multinomial and found this bug.\r\n\r\nOne way to get around the sampling bug above is to just do everything on the CPU. Another solution is to use Gumbel-softmax style sampling:\r\n\r\n```python\r\ndef sample(logits, temperature=1.0):\r\n    if temperature == 0:\r\n        return torch.argmax(logits, dim=-1)\r\n    noise = torch.FloatTensor(logits.shape).to(logits.device)\r\n    noise.uniform_(1e-5, 1-1e-5)\r\n    return torch.argmax(logits / temperature - torch.log(-torch.log(noise)), dim=-1)\r\n```\r\n\r\nBut unfortunately this takes longer to execute than the first round of sampling code."}