{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12316", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12316/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12316/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12316/events", "html_url": "https://github.com/pytorch/pytorch/issues/12316", "id": 366633388, "node_id": "MDU6SXNzdWUzNjY2MzMzODg=", "number": 12316, "title": "[feature request] Add multi-index, dim-as-sequence support to all tensor reduction operations", "user": {"login": "elistevens", "id": 138016, "node_id": "MDQ6VXNlcjEzODAxNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/138016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elistevens", "html_url": "https://github.com/elistevens", "followers_url": "https://api.github.com/users/elistevens/followers", "following_url": "https://api.github.com/users/elistevens/following{/other_user}", "gists_url": "https://api.github.com/users/elistevens/gists{/gist_id}", "starred_url": "https://api.github.com/users/elistevens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elistevens/subscriptions", "organizations_url": "https://api.github.com/users/elistevens/orgs", "repos_url": "https://api.github.com/users/elistevens/repos", "events_url": "https://api.github.com/users/elistevens/events{/privacy}", "received_events_url": "https://api.github.com/users/elistevens/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1076922351, "node_id": "MDU6TGFiZWwxMDc2OTIyMzUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/feature%20request", "name": "feature request", "color": "ffceba", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-04T05:58:25Z", "updated_at": "2018-10-26T10:25:44Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n\n<p>The <code>dim</code> parameter for <code>torch.sum</code> and e.g. <code>torch.max</code> do not have the same set of supported input types.  From <a href=\"https://pytorch.org/docs/master/torch.html#reduction-ops\" rel=\"nofollow\">https://pytorch.org/docs/master/torch.html#reduction-ops</a> :</p>\n<blockquote>\n<p>torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)<br>\nParameters: dim (int) \u2013 the dimension to reduce</p>\n</blockquote>\n<p>vs.</p>\n<blockquote>\n<p>torch.sum(input, dim, keepdim=False, dtype=None) -&gt; Tensor<br>\nParameters: dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce</p>\n</blockquote>\n<p>This means that reducing a 5D tensor looks like one of the following:</p>\n<pre><code>t.sum([2,3,4])\nt.view(t.size(0), t.size(1), -1).max(dim=2)[0]\n</code></pre>\n<p>And it would be nice to have the same style available for all reduction operations.</p>\n<h2>Motivation</h2>\n\n<p>Having to spell similar operations in very different ways results in less readable code. It's harder to verify visually that the two operations do the equivalent thing.</p>\n<h2>Pitch</h2>\n\n<p>I would like the multi-index <code>dim</code> support for <code>torch.sum</code> added to <code>torch.min</code>, <code>torch.max</code>, etc.</p>\n<h2>Alternatives</h2>\n\n<p>The status quo works for my use case, but it's unclear if that remains true for users that need the second item of the <code>torch.max</code> return tuple.</p>", "body_text": "\ud83d\ude80 Feature\n\nThe dim parameter for torch.sum and e.g. torch.max do not have the same set of supported input types.  From https://pytorch.org/docs/master/torch.html#reduction-ops :\n\ntorch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\nParameters: dim (int) \u2013 the dimension to reduce\n\nvs.\n\ntorch.sum(input, dim, keepdim=False, dtype=None) -> Tensor\nParameters: dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce\n\nThis means that reducing a 5D tensor looks like one of the following:\nt.sum([2,3,4])\nt.view(t.size(0), t.size(1), -1).max(dim=2)[0]\n\nAnd it would be nice to have the same style available for all reduction operations.\nMotivation\n\nHaving to spell similar operations in very different ways results in less readable code. It's harder to verify visually that the two operations do the equivalent thing.\nPitch\n\nI would like the multi-index dim support for torch.sum added to torch.min, torch.max, etc.\nAlternatives\n\nThe status quo works for my use case, but it's unclear if that remains true for users that need the second item of the torch.max return tuple.", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe `dim` parameter for `torch.sum` and e.g. `torch.max` do not have the same set of supported input types.  From https://pytorch.org/docs/master/torch.html#reduction-ops :\r\n\r\n> torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\r\n> Parameters: dim (int) \u2013 the dimension to reduce\r\n\r\nvs.\r\n\r\n> torch.sum(input, dim, keepdim=False, dtype=None) -> Tensor\r\n> Parameters: dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce\r\n\r\nThis means that reducing a 5D tensor looks like one of the following:\r\n\r\n    t.sum([2,3,4])\r\n    t.view(t.size(0), t.size(1), -1).max(dim=2)[0]\r\n\r\nAnd it would be nice to have the same style available for all reduction operations.\r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nHaving to spell similar operations in very different ways results in less readable code. It's harder to verify visually that the two operations do the equivalent thing.\r\n\r\n## Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nI would like the multi-index `dim` support for `torch.sum` added to `torch.min`, `torch.max`, etc.\r\n\r\n## Alternatives\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nThe status quo works for my use case, but it's unclear if that remains true for users that need the second item of the `torch.max` return tuple.\r\n"}