{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/293156077", "html_url": "https://github.com/pytorch/pytorch/issues/1220#issuecomment-293156077", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1220", "id": 293156077, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzE1NjA3Nw==", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-11T05:44:31Z", "updated_at": "2017-04-11T05:45:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>You mean using the transposed data (batch, hidden_size, length) for the entire network instead of using (batch, length, hidden), right?<br>\nWhen only the convolutional layers are used then it would be just OK, but if one wants to augment the network to use other modules, for example RNN or Linear, then the input data should still be transposed.<br>\nOf course I know transposing an input, apply the module, and then transposing back can be really easily implemented. There's no reason that I want to avoid that approach, if performance degradation is not severe.</p>", "body_text": "You mean using the transposed data (batch, hidden_size, length) for the entire network instead of using (batch, length, hidden), right?\nWhen only the convolutional layers are used then it would be just OK, but if one wants to augment the network to use other modules, for example RNN or Linear, then the input data should still be transposed.\nOf course I know transposing an input, apply the module, and then transposing back can be really easily implemented. There's no reason that I want to avoid that approach, if performance degradation is not severe.", "body": "You mean using the transposed data (batch, hidden_size, length) for the entire network instead of using (batch, length, hidden), right?\r\nWhen only the convolutional layers are used then it would be just OK, but if one wants to augment the network to use other modules, for example RNN or Linear, then the input data should still be transposed.\r\nOf course I know transposing an input, apply the module, and then transposing back can be really easily implemented. There's no reason that I want to avoid that approach, if performance degradation is not severe."}