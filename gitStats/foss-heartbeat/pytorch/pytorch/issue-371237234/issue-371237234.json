{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12780", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12780/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12780/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12780/events", "html_url": "https://github.com/pytorch/pytorch/issues/12780", "id": 371237234, "node_id": "MDU6SXNzdWUzNzEyMzcyMzQ=", "number": 12780, "title": "[pytorch/jit] all lists get merged during tracing", "user": {"login": "zeryx", "id": 1892175, "node_id": "MDQ6VXNlcjE4OTIxNzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1892175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zeryx", "html_url": "https://github.com/zeryx", "followers_url": "https://api.github.com/users/zeryx/followers", "following_url": "https://api.github.com/users/zeryx/following{/other_user}", "gists_url": "https://api.github.com/users/zeryx/gists{/gist_id}", "starred_url": "https://api.github.com/users/zeryx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zeryx/subscriptions", "organizations_url": "https://api.github.com/users/zeryx/orgs", "repos_url": "https://api.github.com/users/zeryx/repos", "events_url": "https://api.github.com/users/zeryx/events{/privacy}", "received_events_url": "https://api.github.com/users/zeryx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suo", "id": 1617424, "node_id": "MDQ6VXNlcjE2MTc0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1617424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suo", "html_url": "https://github.com/suo", "followers_url": "https://api.github.com/users/suo/followers", "following_url": "https://api.github.com/users/suo/following{/other_user}", "gists_url": "https://api.github.com/users/suo/gists{/gist_id}", "starred_url": "https://api.github.com/users/suo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suo/subscriptions", "organizations_url": "https://api.github.com/users/suo/orgs", "repos_url": "https://api.github.com/users/suo/repos", "events_url": "https://api.github.com/users/suo/events{/privacy}", "received_events_url": "https://api.github.com/users/suo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suo", "id": 1617424, "node_id": "MDQ6VXNlcjE2MTc0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1617424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suo", "html_url": "https://github.com/suo", "followers_url": "https://api.github.com/users/suo/followers", "following_url": "https://api.github.com/users/suo/following{/other_user}", "gists_url": "https://api.github.com/users/suo/gists{/gist_id}", "starred_url": "https://api.github.com/users/suo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suo/subscriptions", "organizations_url": "https://api.github.com/users/suo/orgs", "repos_url": "https://api.github.com/users/suo/repos", "events_url": "https://api.github.com/users/suo/events{/privacy}", "received_events_url": "https://api.github.com/users/suo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-10-17T19:42:15Z", "updated_at": "2018-11-02T02:42:27Z", "closed_at": "2018-11-02T02:42:27Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>If you define a nn.Module that contains a forward func with multiple lists (That you stack together into a tensor at some point) that process works perfectly fine in the conventional workflow.<br>\nHowever, if you attempt to convert your module into a ScriptModule that will be compiled into a graph representation, every list object defined in a <code>script_method</code> function seem to merge together somehow.</p>\n<h2>To Reproduce</h2>\n<p>Create a test bench with the source code below, it should be fully self contained</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.functional <span class=\"pl-k\">import</span> F\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">ACTNN</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">jit</span>.<span class=\"pl-e\">ScriptModule</span>):\n\n    __constants__ <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>hidden_depth<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>hidden_width<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden_width = 125</span>\n   <span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden_depth = 3</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> output_size = 80</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> input_size = 125</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_width</span>, <span class=\"pl-smi\">output_size</span>, <span class=\"pl-smi\">hidden_depth</span>):\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">ACTNN</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.hidden_depth <span class=\"pl-k\">=</span> hidden_depth\n        <span class=\"pl-c1\">self</span>.hidden_width <span class=\"pl-k\">=</span> hidden_width\n        <span class=\"pl-c1\">self</span>.rnn <span class=\"pl-k\">=</span> torch.jit.trace(nn.RNN(input_size, hidden_width,\n                                          <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.hidden_depth, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), <span class=\"pl-v\">example_inputs</span><span class=\"pl-k\">=</span>(torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, input_size), torch.rand(<span class=\"pl-c1\">self</span>.hidden_depth, <span class=\"pl-c1\">1</span>, hidden_width)))\n\n        <span class=\"pl-c1\">self</span>.proc <span class=\"pl-k\">=</span> torch.jit.trace(nn.Linear(hidden_width, output_size), <span class=\"pl-v\">example_inputs</span><span class=\"pl-k\">=</span>torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, hidden_width))\n\n\n    <span class=\"pl-en\">@torch.jit.script_method</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>: torch.Tensor, <span class=\"pl-smi\">hidden</span>: torch.Tensor):\n\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        hidden <span class=\"pl-k\">=</span> hidden.view(<span class=\"pl-c1\">self</span>.hidden_depth, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.hidden_width)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the list objects</span>\n        states <span class=\"pl-k\">=</span> []\n        hiddens <span class=\"pl-k\">=</span> []\n        halt_probs <span class=\"pl-k\">=</span> []\n        n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        rnn_out, hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.rnn(<span class=\"pl-c1\">input</span>, hidden)\n        state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.proc(rnn_out)\n        hiddens.append(hidden)\n        states.append(state)\n        halt_probability <span class=\"pl-k\">=</span> F.sigmoid(hiddens[n].sum())\n        halt_probs.append(halt_probability)\n       <span class=\"pl-c\"><span class=\"pl-c\">#</span> At this point, both the states &amp; halt_probs lists will contain:</span>\n       <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1x 1,1,80 wide tensor</span>\n       <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1x 3,1,125 wide tensor</span>\n       <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1x 1 wide tensor</span>\n\n        <span class=\"pl-c1\">print</span>(states)\n        <span class=\"pl-c1\">print</span>(halt_probs)\n        residual <span class=\"pl-k\">=</span> torch.sum(torch.cat(halt_probs))\n\n        states_tensor <span class=\"pl-k\">=</span> torch.stack(states, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        hiddens_tensor <span class=\"pl-k\">=</span> torch.stack(hiddens, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        halt_subset <span class=\"pl-k\">=</span> halt_probs[<span class=\"pl-c1\">0</span>:n<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n        halt_subset.append(residual)\n        halt_prob_tensor <span class=\"pl-k\">=</span> torch.stack(halt_subset).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        output <span class=\"pl-k\">=</span> torch.mv(states_tensor, halt_prob_tensor)\n        hidden <span class=\"pl-k\">=</span> torch.mv(hiddens_tensor, halt_prob_tensor)\n        <span class=\"pl-k\">return</span> output, hidden</pre></div>\n<p>dimensions of elements in <code>states</code>:</p>\n<pre><code>[ Variable[CPUFloatType]{3,1,125} ],\n[ Variable[CPUFloatType]{1,1,80} ],\n[ Variable[CPUFloatType]{1} ]]\n</code></pre>\n<p>and here is the error message that causes an overall failure of the trace operation:</p>\n<pre><code>RuntimeError: \ninvalid argument 0: Sizes of tensors must match except in dimension 1. Got 3 and 1 in dimension 0 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317:\noperation failed in interpreter:\n    halt_probability = self.halt_noise(halt_probability, training)\n    halt_probs.append(halt_probability)\n    print(halt_probs)\n    residual = torch.sum(torch.cat(halt_probs))\n    states_tensor = torch.stack(states, dim=1)\n                    ~~~~~~~~~~~ &lt;--- HERE\n</code></pre>\n<h2>Expected behavior</h2>\n<p>for the above code sample,<br>\n<code>hiddens</code> should contain:</p>\n<pre><code>[ Variable[CPUFloatType]{3,1,125} ],\n</code></pre>\n<p><code>states</code> should contain:</p>\n<pre><code>[ Variable[CPUFloatType]{1,1,80} ]\n</code></pre>\n<p><code>halt_probs</code> should contain:</p>\n<pre><code>[ Variable[CPUFloatType]{1} ]]\n</code></pre>\n<p>.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 1.0.0.dev20181004</li>\n<li>OS (e.g., Linux): Ubuntu 16.04</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): <code>pip install /tmp/torch_nightly-1.0.0.dev20181004-cp35-cp35m-linux_x86_64.whl</code></li>\n<li>Build command you used (if compiling from source): N/A</li>\n<li>Python version: 3.5.4</li>\n<li>CUDA/cuDNN version: N/A</li>\n<li>GPU models and configuration: N/A</li>\n<li>Any other relevant information: N/A</li>\n</ul>\n<h2>Additional context</h2>\n<p>As I am doing some somewhat advanced pytorch module development, I could understand if my usage here is not supported - however there is currently no error message saying that I can't construct multiple tensors from elements captured in a forward pass.</p>\n<p>It's certainly possible that some other part of my algorithm is failing, or maybe I'm using torch.jit.ScriptModule incorrectly? That would be great if that was the case,</p>\n<p>I could see this posing issues down the road with other types of dynamic process flow during forward pass loops (like ACT, but also NTMs, etc), which are probably mechanisms we want pytorch 1.0 to support properly.</p>", "body_text": "\ud83d\udc1b Bug\nIf you define a nn.Module that contains a forward func with multiple lists (That you stack together into a tensor at some point) that process works perfectly fine in the conventional workflow.\nHowever, if you attempt to convert your module into a ScriptModule that will be compiled into a graph representation, every list object defined in a script_method function seem to merge together somehow.\nTo Reproduce\nCreate a test bench with the source code below, it should be fully self contained\nimport torch\nfrom torch import nn\nfrom torch.functional import F\n\nclass ACTNN(torch.jit.ScriptModule):\n\n    __constants__ = ['hidden_depth', 'hidden_width']\n    # hidden_width = 125\n   # hidden_depth = 3\n    # output_size = 80\n    # input_size = 125\n    def __init__(self, input_size, hidden_width, output_size, hidden_depth):\n        super(ACTNN, self).__init__()\n        self.hidden_depth = hidden_depth\n        self.hidden_width = hidden_width\n        self.rnn = torch.jit.trace(nn.RNN(input_size, hidden_width,\n                                          num_layers=self.hidden_depth, batch_first=True), example_inputs=(torch.rand(1, 1, input_size), torch.rand(self.hidden_depth, 1, hidden_width)))\n\n        self.proc = torch.jit.trace(nn.Linear(hidden_width, output_size), example_inputs=torch.rand(1, 1, hidden_width))\n\n\n    @torch.jit.script_method\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor):\n\n        input = input.view(1, 1, -1)\n        hidden = hidden.view(self.hidden_depth, 1, self.hidden_width)\n        # Define the list objects\n        states = []\n        hiddens = []\n        halt_probs = []\n        n = 0\n        rnn_out, hidden = self.rnn(input, hidden)\n        state = self.proc(rnn_out)\n        hiddens.append(hidden)\n        states.append(state)\n        halt_probability = F.sigmoid(hiddens[n].sum())\n        halt_probs.append(halt_probability)\n       # At this point, both the states & halt_probs lists will contain:\n       # 1x 1,1,80 wide tensor\n       # 1x 3,1,125 wide tensor\n       # 1x 1 wide tensor\n\n        print(states)\n        print(halt_probs)\n        residual = torch.sum(torch.cat(halt_probs))\n\n        states_tensor = torch.stack(states, dim=1)\n        hiddens_tensor = torch.stack(hiddens, dim=1)\n        halt_subset = halt_probs[0:n-1]\n        halt_subset.append(residual)\n        halt_prob_tensor = torch.stack(halt_subset).view(-1)\n        output = torch.mv(states_tensor, halt_prob_tensor)\n        hidden = torch.mv(hiddens_tensor, halt_prob_tensor)\n        return output, hidden\ndimensions of elements in states:\n[ Variable[CPUFloatType]{3,1,125} ],\n[ Variable[CPUFloatType]{1,1,80} ],\n[ Variable[CPUFloatType]{1} ]]\n\nand here is the error message that causes an overall failure of the trace operation:\nRuntimeError: \ninvalid argument 0: Sizes of tensors must match except in dimension 1. Got 3 and 1 in dimension 0 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317:\noperation failed in interpreter:\n    halt_probability = self.halt_noise(halt_probability, training)\n    halt_probs.append(halt_probability)\n    print(halt_probs)\n    residual = torch.sum(torch.cat(halt_probs))\n    states_tensor = torch.stack(states, dim=1)\n                    ~~~~~~~~~~~ <--- HERE\n\nExpected behavior\nfor the above code sample,\nhiddens should contain:\n[ Variable[CPUFloatType]{3,1,125} ],\n\nstates should contain:\n[ Variable[CPUFloatType]{1,1,80} ]\n\nhalt_probs should contain:\n[ Variable[CPUFloatType]{1} ]]\n\n.\nEnvironment\n\nPyTorch Version (e.g., 1.0): 1.0.0.dev20181004\nOS (e.g., Linux): Ubuntu 16.04\nHow you installed PyTorch (conda, pip, source): pip install /tmp/torch_nightly-1.0.0.dev20181004-cp35-cp35m-linux_x86_64.whl\nBuild command you used (if compiling from source): N/A\nPython version: 3.5.4\nCUDA/cuDNN version: N/A\nGPU models and configuration: N/A\nAny other relevant information: N/A\n\nAdditional context\nAs I am doing some somewhat advanced pytorch module development, I could understand if my usage here is not supported - however there is currently no error message saying that I can't construct multiple tensors from elements captured in a forward pass.\nIt's certainly possible that some other part of my algorithm is failing, or maybe I'm using torch.jit.ScriptModule incorrectly? That would be great if that was the case,\nI could see this posing issues down the road with other types of dynamic process flow during forward pass loops (like ACT, but also NTMs, etc), which are probably mechanisms we want pytorch 1.0 to support properly.", "body": "## \ud83d\udc1b Bug\r\n\r\nIf you define a nn.Module that contains a forward func with multiple lists (That you stack together into a tensor at some point) that process works perfectly fine in the conventional workflow. \r\nHowever, if you attempt to convert your module into a ScriptModule that will be compiled into a graph representation, every list object defined in a `script_method` function seem to merge together somehow.\r\n\r\n## To Reproduce\r\n\r\nCreate a test bench with the source code below, it should be fully self contained\r\n```python\r\n\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.functional import F\r\n\r\nclass ACTNN(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['hidden_depth', 'hidden_width']\r\n    # hidden_width = 125\r\n   # hidden_depth = 3\r\n    # output_size = 80\r\n    # input_size = 125\r\n    def __init__(self, input_size, hidden_width, output_size, hidden_depth):\r\n        super(ACTNN, self).__init__()\r\n        self.hidden_depth = hidden_depth\r\n        self.hidden_width = hidden_width\r\n        self.rnn = torch.jit.trace(nn.RNN(input_size, hidden_width,\r\n                                          num_layers=self.hidden_depth, batch_first=True), example_inputs=(torch.rand(1, 1, input_size), torch.rand(self.hidden_depth, 1, hidden_width)))\r\n\r\n        self.proc = torch.jit.trace(nn.Linear(hidden_width, output_size), example_inputs=torch.rand(1, 1, hidden_width))\r\n\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor):\r\n\r\n        input = input.view(1, 1, -1)\r\n        hidden = hidden.view(self.hidden_depth, 1, self.hidden_width)\r\n        # Define the list objects\r\n        states = []\r\n        hiddens = []\r\n        halt_probs = []\r\n        n = 0\r\n        rnn_out, hidden = self.rnn(input, hidden)\r\n        state = self.proc(rnn_out)\r\n        hiddens.append(hidden)\r\n        states.append(state)\r\n        halt_probability = F.sigmoid(hiddens[n].sum())\r\n        halt_probs.append(halt_probability)\r\n       # At this point, both the states & halt_probs lists will contain:\r\n       # 1x 1,1,80 wide tensor\r\n       # 1x 3,1,125 wide tensor\r\n       # 1x 1 wide tensor\r\n\r\n        print(states)\r\n        print(halt_probs)\r\n        residual = torch.sum(torch.cat(halt_probs))\r\n\r\n        states_tensor = torch.stack(states, dim=1)\r\n        hiddens_tensor = torch.stack(hiddens, dim=1)\r\n        halt_subset = halt_probs[0:n-1]\r\n        halt_subset.append(residual)\r\n        halt_prob_tensor = torch.stack(halt_subset).view(-1)\r\n        output = torch.mv(states_tensor, halt_prob_tensor)\r\n        hidden = torch.mv(hiddens_tensor, halt_prob_tensor)\r\n        return output, hidden\r\n```\r\ndimensions of elements in `states`:\r\n```\r\n[ Variable[CPUFloatType]{3,1,125} ],\r\n[ Variable[CPUFloatType]{1,1,80} ],\r\n[ Variable[CPUFloatType]{1} ]]\r\n```\r\n\r\nand here is the error message that causes an overall failure of the trace operation:\r\n```\r\nRuntimeError: \r\ninvalid argument 0: Sizes of tensors must match except in dimension 1. Got 3 and 1 in dimension 0 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317:\r\noperation failed in interpreter:\r\n    halt_probability = self.halt_noise(halt_probability, training)\r\n    halt_probs.append(halt_probability)\r\n    print(halt_probs)\r\n    residual = torch.sum(torch.cat(halt_probs))\r\n    states_tensor = torch.stack(states, dim=1)\r\n                    ~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nfor the above code sample, \r\n`hiddens` should contain:\r\n```\r\n[ Variable[CPUFloatType]{3,1,125} ],\r\n```\r\n`states` should contain:\r\n```\r\n[ Variable[CPUFloatType]{1,1,80} ]\r\n```\r\n`halt_probs` should contain:\r\n```\r\n[ Variable[CPUFloatType]{1} ]]\r\n```\r\n.\r\n## Environment\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.0.dev20181004\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip install /tmp/torch_nightly-1.0.0.dev20181004-cp35-cp35m-linux_x86_64.whl`\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.5.4\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n## Additional context\r\n\r\nAs I am doing some somewhat advanced pytorch module development, I could understand if my usage here is not supported - however there is currently no error message saying that I can't construct multiple tensors from elements captured in a forward pass.\r\n\r\nIt's certainly possible that some other part of my algorithm is failing, or maybe I'm using torch.jit.ScriptModule incorrectly? That would be great if that was the case,\r\n\r\nI could see this posing issues down the road with other types of dynamic process flow during forward pass loops (like ACT, but also NTMs, etc), which are probably mechanisms we want pytorch 1.0 to support properly.\r\n"}