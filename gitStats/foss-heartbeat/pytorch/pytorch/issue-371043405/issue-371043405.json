{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12764", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12764/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12764/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12764/events", "html_url": "https://github.com/pytorch/pytorch/issues/12764", "id": 371043405, "node_id": "MDU6SXNzdWUzNzEwNDM0MDU=", "number": 12764, "title": "[feature request] Operator Overloading", "user": {"login": "Florian1990", "id": 7100594, "node_id": "MDQ6VXNlcjcxMDA1OTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/7100594?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Florian1990", "html_url": "https://github.com/Florian1990", "followers_url": "https://api.github.com/users/Florian1990/followers", "following_url": "https://api.github.com/users/Florian1990/following{/other_user}", "gists_url": "https://api.github.com/users/Florian1990/gists{/gist_id}", "starred_url": "https://api.github.com/users/Florian1990/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Florian1990/subscriptions", "organizations_url": "https://api.github.com/users/Florian1990/orgs", "repos_url": "https://api.github.com/users/Florian1990/repos", "events_url": "https://api.github.com/users/Florian1990/events{/privacy}", "received_events_url": "https://api.github.com/users/Florian1990/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-17T12:17:06Z", "updated_at": "2018-10-22T17:56:12Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I realized that PyTorch does partly prevent operator overloading by raising a <code>TypeError</code> rather than returning <code>NotImplemented</code>. Therefore Python never checks object methods implementing operations with reflected operands, e. g. <code>__rmul__</code>:</p>\n<pre><code>In[1]: import torch\nIn[2]: class Two:\n           def __mul__(self, other):\n               return other * 2\n           def __rmul__(self, other):\n               return self * other\n           \nIn[3]: two = Two()\nIn[4]: two * 3\nOut[4]: 6\nIn[5]: 3 * two\nOut[5]: 6\nIn[6]: two * torch.tensor(3)\nOut[6]: tensor(6)\nIn[7]: torch.tensor(3) * two\nTraceback (most recent call last):\n  File \"/path/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-8-e59deefe7a13&gt;\", line 1, in &lt;module&gt;\n    torch.tensor(3) * two\nTypeError: mul() received an invalid combination of arguments - got (Two), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (!Two!)\n * (float other)\n      didn't match because some of the arguments have invalid types: (!Two!)\n</code></pre>\n<p>Is this behavior intended (e. g. for performance reasons)? I would prefer PyTorch returning <code>NotImplemented</code> (which might result in raising a <code>TypeError</code>) rather than directly raising a <code>TypeError</code>.</p>", "body_text": "I realized that PyTorch does partly prevent operator overloading by raising a TypeError rather than returning NotImplemented. Therefore Python never checks object methods implementing operations with reflected operands, e. g. __rmul__:\nIn[1]: import torch\nIn[2]: class Two:\n           def __mul__(self, other):\n               return other * 2\n           def __rmul__(self, other):\n               return self * other\n           \nIn[3]: two = Two()\nIn[4]: two * 3\nOut[4]: 6\nIn[5]: 3 * two\nOut[5]: 6\nIn[6]: two * torch.tensor(3)\nOut[6]: tensor(6)\nIn[7]: torch.tensor(3) * two\nTraceback (most recent call last):\n  File \"/path/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e59deefe7a13>\", line 1, in <module>\n    torch.tensor(3) * two\nTypeError: mul() received an invalid combination of arguments - got (Two), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (!Two!)\n * (float other)\n      didn't match because some of the arguments have invalid types: (!Two!)\n\nIs this behavior intended (e. g. for performance reasons)? I would prefer PyTorch returning NotImplemented (which might result in raising a TypeError) rather than directly raising a TypeError.", "body": "I realized that PyTorch does partly prevent operator overloading by raising a `TypeError` rather than returning `NotImplemented`. Therefore Python never checks object methods implementing operations with reflected operands, e. g. `__rmul__`:\r\n\r\n```\r\nIn[1]: import torch\r\nIn[2]: class Two:\r\n           def __mul__(self, other):\r\n               return other * 2\r\n           def __rmul__(self, other):\r\n               return self * other\r\n           \r\nIn[3]: two = Two()\r\nIn[4]: two * 3\r\nOut[4]: 6\r\nIn[5]: 3 * two\r\nOut[5]: 6\r\nIn[6]: two * torch.tensor(3)\r\nOut[6]: tensor(6)\r\nIn[7]: torch.tensor(3) * two\r\nTraceback (most recent call last):\r\n  File \"/path/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-e59deefe7a13>\", line 1, in <module>\r\n    torch.tensor(3) * two\r\nTypeError: mul() received an invalid combination of arguments - got (Two), but expected one of:\r\n * (Tensor other)\r\n      didn't match because some of the arguments have invalid types: (!Two!)\r\n * (float other)\r\n      didn't match because some of the arguments have invalid types: (!Two!)\r\n```\r\n\r\nIs this behavior intended (e. g. for performance reasons)? I would prefer PyTorch returning `NotImplemented` (which might result in raising a `TypeError`) rather than directly raising a `TypeError`."}