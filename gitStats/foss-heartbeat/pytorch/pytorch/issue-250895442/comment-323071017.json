{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/323071017", "html_url": "https://github.com/pytorch/pytorch/issues/2470#issuecomment-323071017", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2470", "id": 323071017, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzA3MTAxNw==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T13:19:08Z", "updated_at": "2017-08-17T13:19:08Z", "author_association": "MEMBER", "body_html": "<p>I'd say this is due to precision errors during the computations, which are performed in single precision in the <code>var(0)</code> case, and in double precision in the <code>var()</code> case.<br>\nTo inspect that, just perform the computations one by one on the terminal for example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> corresponds to the biased estimator</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">var_all</span>(<span class=\"pl-smi\">t</span>):\n    s <span class=\"pl-k\">=</span> t.sum() <span class=\"pl-k\">/</span> t.numel()\n    s2 <span class=\"pl-k\">=</span> (t <span class=\"pl-k\">*</span> t).sum() <span class=\"pl-k\">/</span> t.numel()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>E[t]^2:<span class=\"pl-pds\">'</span></span>, s <span class=\"pl-k\">*</span> s)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>E[t^2]:<span class=\"pl-pds\">'</span></span>, s2)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">max</span>((s2 <span class=\"pl-k\">-</span> s <span class=\"pl-k\">*</span> s), <span class=\"pl-c1\">0</span>)\n\ntensor <span class=\"pl-k\">=</span> torch.DoubleTensor([<span class=\"pl-c1\">2281.5</span>, <span class=\"pl-c1\">2281.25</span>])\n<span class=\"pl-c1\">print</span>(var_all(tensor.float()))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-------------<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(var_all(tensor))</pre></div>\n<p>Prints</p>\n<pre><code>E[t]^2: 5204671.890625\nE[t^2]: 5204671.75\n0\n-------------\nE[t]^2: 5204671.890625\nE[t^2]: 5204671.90625\n0.015625\n</code></pre>\n<p>But, looking at the code, it seems that the accumulation <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L2996-L2997\">is done in <code>double</code> precision</a>, so I'm not sure why this is happening. I'm probably missing something obvious here</p>", "body_text": "I'd say this is due to precision errors during the computations, which are performed in single precision in the var(0) case, and in double precision in the var() case.\nTo inspect that, just perform the computations one by one on the terminal for example:\n# corresponds to the biased estimator\ndef var_all(t):\n    s = t.sum() / t.numel()\n    s2 = (t * t).sum() / t.numel()\n    print('E[t]^2:', s * s)\n    print('E[t^2]:', s2)\n    return max((s2 - s * s), 0)\n\ntensor = torch.DoubleTensor([2281.5, 2281.25])\nprint(var_all(tensor.float()))\nprint('-------------')\nprint(var_all(tensor))\nPrints\nE[t]^2: 5204671.890625\nE[t^2]: 5204671.75\n0\n-------------\nE[t]^2: 5204671.890625\nE[t^2]: 5204671.90625\n0.015625\n\nBut, looking at the code, it seems that the accumulation is done in double precision, so I'm not sure why this is happening. I'm probably missing something obvious here", "body": "I'd say this is due to precision errors during the computations, which are performed in single precision in the `var(0)` case, and in double precision in the `var()` case.\r\nTo inspect that, just perform the computations one by one on the terminal for example:\r\n```python\r\n# corresponds to the biased estimator\r\ndef var_all(t):\r\n    s = t.sum() / t.numel()\r\n    s2 = (t * t).sum() / t.numel()\r\n    print('E[t]^2:', s * s)\r\n    print('E[t^2]:', s2)\r\n    return max((s2 - s * s), 0)\r\n\r\ntensor = torch.DoubleTensor([2281.5, 2281.25])\r\nprint(var_all(tensor.float()))\r\nprint('-------------')\r\nprint(var_all(tensor))\r\n```\r\nPrints\r\n```\r\nE[t]^2: 5204671.890625\r\nE[t^2]: 5204671.75\r\n0\r\n-------------\r\nE[t]^2: 5204671.890625\r\nE[t^2]: 5204671.90625\r\n0.015625\r\n```\r\n\r\nBut, looking at the code, it seems that the accumulation [is done in `double` precision](https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L2996-L2997), so I'm not sure why this is happening. I'm probably missing something obvious here"}