{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/323120851", "html_url": "https://github.com/pytorch/pytorch/issues/2470#issuecomment-323120851", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2470", "id": 323120851, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzEyMDg1MQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T16:13:19Z", "updated_at": "2017-08-17T16:13:19Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> I thought that <code>accreal</code> in TH was <code>double</code> for <code>float</code>, and <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatType.h#L6\">it seems to be the case</a> (even though in CUDA it's <code>float</code>). Anyway, the computation is indeed a bit unstable and might be better to improve it.</p>", "body_text": "@ngimel I thought that accreal in TH was double for float, and it seems to be the case (even though in CUDA it's float). Anyway, the computation is indeed a bit unstable and might be better to improve it.", "body": "@ngimel I thought that `accreal` in TH was `double` for `float`, and [it seems to be the case](https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatType.h#L6) (even though in CUDA it's `float`). Anyway, the computation is indeed a bit unstable and might be better to improve it."}