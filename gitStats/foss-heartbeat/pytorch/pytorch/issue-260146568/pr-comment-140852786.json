{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140852786", "pull_request_review_id": 64975798, "id": 140852786, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDg1Mjc4Ng==", "diff_hunk": "@@ -1,26 +1,375 @@\n import torch.autograd.function as function\n import torch._C\n from torch.autograd import Variable\n-from torch.nn import Module\n+from torch.nn import Module, ParameterList, Parameter\n from collections import defaultdict\n import itertools\n import types\n import contextlib\n import os\n+import functools\n import torch.contrib._graph_vis as graph_vis\n-# Example how to use:\n-#\n-# import torch.jit\n-# model = model.RNNModel(args.model, ...)\n-# model = torch.jit.traced(model)\n \n \n-def flatten(x):\n+HOLE = object()\n+VOLATILE = object()\n+\n+\n+# TODO: verify is not implemented yet\n+def compile(arg=None, nderivs=1, params=tuple(), verify=False, optimize=True, enabled=True):\n+    \"\"\"\n+    Mark a function or module as eligible for just-in-time compilation.  The\n+    next time the function/module is executed, it is traced, and the trace is\n+    compiled into an optimized representation which is run in lieu of the\n+    original Python code upon subsequent invocations of the function/module.\n+\n+    The result of this function is stateful, so make sure you invoke it\n+    only once per code you want to JIT compile.\n+\n+    .. note::\n+\n+        If your function/module takes non-Variable inputs, the JIT compiler\n+        will compile a trace separately for each distinct input configuration.\n+\n+    .. warning::\n+\n+        Just-in-time compilation currently only works for functions/modules\n+        which do not have dynamic control flow; if you compile a function/module\n+        which has dynamic control flow, you will silently get incorrect\n+        results on subsequent invocations of the model.  Use `verify=True` to\n+        check that the original Python code and optimized code are equivalent.\n+\n+    Arguments:\n+        arg (optional, torch.nn.Module or function): the function or module\n+            to be compiled.  If `None`, `compile` returns a decorator which can be\n+            applied to the function or module you want to compile.\n+        verify (bool, default False): if True, upon all invocations of the\n+            function/module, execute both the compiled and interpreted versions\n+            of the model, and verify that their results match.  This is an easy\n+            (albeit slow) way to check if your function/module can be validly\n+            JIT compiled.\n+        nderivs (int, default 1): the number of derivatives which this function/module\n+            will be used with.  You MUST accurately specify this number: set it too\n+            low and you will see an error when you attempt to run `backward`;\n+            set it too high, and the function/module will never be compiled\n+            (as we always wait to see all derivatives before compiling.)\n+        optimize (bool, default False): whether or not to apply optimizations.\n+        enabled (bool, default True): whether or not to actually enable the JIT\n+            compiler.  This is a convenient way to disable a compilation statement\n+            without deleting the actual `compile` invocation.\n+\n+    Example: Compile as function decorator.\n+\n+        >>> @jit.compile\n+        >>> def f(x);\n+        >>>     return x * 2\n+        >>> x = Variable(torch.randn(1))\n+        >>> out1 = f(x)  # interpreted run\n+        >>> out1.sum().backward()  # required!\n+        >>> out2 = f(x)  # compiled run\n+        >>> out2.sum().backward()\n+\n+    Example: Compile as higher order function. (Notice that compile is a *curried*\n+    function; you first apply it with the function/model to trace, and then\n+    apply the result with the arguments.)\n+\n+        >>> compiled_model = jit.compile(nn.LSTMCell())\n+        >>> out = compiled_model(input, hidden)\n+\n+    Example: Compile forwards only as function decorator\n+\n+        >>> @jit.compile(nderivs=0)\n+        >>> def f(x);\n+        >>>     return x * 2\n+        >>> out1 = f(x)  # interpreted run\n+        >>> out2 = f(x)  # compiled run\n+\n+    Example: Compile forwards only as higher order function\n+\n+        >>> compiled_model = jit.compile(nn.LSTMCell(), nderivs=0)\n+        >>> out1 = compiled_model(input, hidden)  # interpreted run\n+        >>> out2 = compiled_model(input, hidden)  # compiled run\n+    \"\"\"\n+    # TODO: handle decorating a class (not an instance)\n+    def _compile(inner):\n+        if enabled:\n+            return CompiledModule(inner, params=params, nderivs=nderivs, optimize=optimize)\n+        else:\n+            return inner\n+    if callable(arg):\n+        return _compile(arg)\n+    else:\n+        return _compile\n+\n+\n+def trace(arg=None, nderivs=0, params=tuple()):\n+    \"\"\"\n+    Instrument a function or module for tracing, wrapping it in a\n+    :class:`TracedModule`, whose forward accepts the same arguments as the\n+    original function/module, but returns a tuple consisting of the\n+    *trace* of an execution, as well as the original return value.\n+\n+    Tracing is guaranteed not to change the semantics of the function/module\n+    that is traced.\n+\n+    Arguments:\n+        arg (optional, torch.nn.Module or function): the function or module\n+            to be traced.  If `None`, `trace` returns a decorator which can be\n+            applied to the function or module you want to trace.\n+        nderivs (int, default 0): the number of derivatives to trace.\n+            Traces of derivatives are recorded into the same trace returned\n+            after executing the `forward` of the resulting module, but\n+            are not present until you run `backward()` (an appropriate\n+            number of times) on the resulting model.\n+        params (tuple of torch.nn.Parameter): extra parameters for a traced\n+            function, which do not occur as arguments to the function in\n+            question.  You generally do not need this for tracing modules, as\n+            the parameters of a module are automatically computed.\n+\n+    Example: Trace as function decorator.\n+\n+        >>> @jit.trace", "path": "torch/jit.py", "position": null, "original_position": 138, "commit_id": "32f59e2ce8acb246bd9091e44732a5f339e4cc4b", "original_commit_id": "720239f566d9dc62f13970a32ef4963f3ca64e8d", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "I am not sure I like this as an annotation. The common case will be recording a trace once. ", "created_at": "2017-09-25T18:04:24Z", "updated_at": "2018-11-23T15:34:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/2852#discussion_r140852786", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2852", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140852786"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2852#discussion_r140852786"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2852"}}, "body_html": "<p>I am not sure I like this as an annotation. The common case will be recording a trace once.</p>", "body_text": "I am not sure I like this as an annotation. The common case will be recording a trace once."}