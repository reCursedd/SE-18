{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/141648986", "pull_request_review_id": 65664733, "id": 141648986, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTY0ODk4Ng==", "diff_hunk": "@@ -53,224 +523,61 @@ def _fork_rng(enabled=True):\n         torch.cuda.set_rng_state(gpu_rng_state)\n \n \n-@contextlib.contextmanager\n-def _time(name, enabled=True):\n-    if not enabled or not torch.cuda.is_available():\n-        yield\n-        return\n-    stream = torch.cuda.current_stream()\n-    start = torch.cuda.Event(enable_timing=True)\n-    end = torch.cuda.Event(enable_timing=True)\n-    stream.record_event(start)\n-    yield\n-    stream.record_event(end)\n-    end.synchronize()\n-    print(\"{} time: {} ms\".format(name, start.elapsed_time(end)))\n-\n-\n-def _varify(args):\n-    return tuple(a if isinstance(a, Variable) else Variable(a, requires_grad=False) for a in args)\n-\n+# _flatten and _unflatten are inverses\n+def _unflatten(input, proto):\n+    def unflatten_helper(input, proto):\n+        res = []\n+        if not isinstance(proto, (list, tuple)):\n+            return input[0], input[1:]\n+        for e in proto:\n+            res_e, input = unflatten_helper(input, e)\n+            res.append(res_e)\n+        return type(proto)(res), input\n \n-def _clone_inputs(all_args):\n-    for a in all_args:\n-        if isinstance(a, Variable):\n-            yield Variable(a.data.clone(), requires_grad=a.requires_grad, volatile=a.volatile)\n-        else:\n-            yield a.clone()\n+    return unflatten_helper(input, proto)\n \n \n-def _verify(flat_trace_out, flat_real_out):\n-    # test for equality\n-    for x, y in zip(flat_trace_out, flat_real_out):\n-        if not (isinstance(x, Variable) and isinstance(y, Variable)):\n-            raise RuntimeError(\"non-Variable output\")\n-        if x.data.sub(y.data).abs().max() > 1e-6:\n-            raise RuntimeError(\"JIT and real computation mismatch\")\n+def _flatten(obj, params=tuple()):\n+    obj_vars = tuple(itertools.chain(function._iter_variables(obj), params))\n+    obj_struct = function._nested_map(lambda o: isinstance(o, Variable), lambda x: HOLE)(obj)\n+    return obj_vars, obj_struct\n \n \n-_dump_traces = os.environ.get('PYTORCH_JIT_DUMP', False)\n+# This is purely for developer debugging.  We are not going to advertise it.\n+_JIT_DUMP = os.environ.get('PYTORCH_JIT_DUMP', False)\n+_JIT_TIME = os.environ.get('PYTORCH_JIT_TIME', False)  # CUDA-only timing\n+_JIT_DISABLE = os.environ.get('PYTORCH_JIT_DISABLE', False)\n \n \n-def _dump_trace(trace_name, name, suffix, complete_trace):\n-    if not _dump_traces:\n+def _dump_trace(trace_name, pass_name, input_key, trace):\n+    if not _JIT_DUMP:\n         return\n-    filename = \"{}_{}_{}\".format(trace_name, name, suffix)\n-    with open(filename + \".ir\", \"w\") as f:\n-        f.write(str(complete_trace))\n-    graph_vis.write(complete_trace.graph(), filename + \".html\")\n-\n-\n-# holds run() to run the function and self.inputs which\n-# are all the variable inputs\n-class Traceable(object):\n-    _next_trace_id = 0\n-    VOLATILE = object()\n-\n-    # Traceable holds multiple traces and switches between them based on\n-    # inputs provided to a call. Things that need to be considered include\n-    # non-Variable argument (e.g. num_layers=3; compared by equality) or\n-    # Variable flags and sizes. TraceInfo is the object that is used to\n-    # hold a trace for a single input configuration aka input_key.\n-    class TraceInfo(object):\n-        def __init__(self, trace_name):\n-            self.traces = []\n-            self.complete_trace = None\n-            self.closure = None\n-            self.trace_name = trace_name\n-            self.proto = None\n-\n-        def _run_pass(self, p):\n-            name = p.__name__.replace('_jit_pass_', '')\n-            _dump_trace(self.trace_name, name, 'input', self.complete_trace)\n-            p(self.complete_trace)\n-            _dump_trace(self.trace_name, name, 'output', self.complete_trace)\n-            # TODO: Make linting optional\n-            torch._C._jit_pass_lint(self.complete_trace)\n-\n-        def compile_trace(self, optimize):\n-            # It's important to always run DCE, because backward can create a lot of unnecessary nodes\n-            self._run_pass(torch._C._jit_pass_dce)\n-            if optimize:\n-                self._run_pass(torch._C._jit_pass_onnx)\n-                self._run_pass(torch._C._jit_pass_fuse)\n-\n-            self.closure = torch._C._jit_createAutogradClosure(self.complete_trace)\n-\n-        def check_traces(self):\n-            self.traces = [t for t in self.traces if not t.is_expired]\n-            for trace in self.traces:\n-                if trace.is_complete:\n-                    self.complete_trace = trace\n-                    self.traces = []\n-\n-    def __init__(self, function_or_module, num_derivatives=1, parameters=None, trace_name=None,\n-                 optimize=False, verify=False, time=False, enabled=True):\n-        \"\"\"\n-        time - collect cuda timing stats for perf debugging\n-        verify - run the original code, and check it is within threshold\n-        optimize - run optimizations like fusion on the trace before running\n-        enabled - flag to turn off tracing so you can check timing of stuff that cannot be traced\n-        \"\"\"\n-\n-        if isinstance(function_or_module, Module):\n-            self._run = function_or_module.forward\n-            self._state_values = lambda: function_or_module.state_dict(keep_vars=True).values()\n-        else:\n-            self._run = function_or_module\n-            param_list = list(parameters) if parameters is not None else []\n-            self._state_values = lambda: param_list\n-\n-        if trace_name is None:\n-            trace_name = \"trace_{}\".format(Traceable._next_trace_id)\n-            Traceable._next_trace_id += 1\n-\n-        self.trace_name = trace_name\n-        self.optimize = optimize\n-        self.verify = verify\n-        self.time = time\n-        self.enabled = enabled\n-        self.num_derivatives = num_derivatives\n-        self.traces = defaultdict(lambda: Traceable.TraceInfo(trace_name))\n-\n-    def get_input_key(self, args):\n-        is_volatile = any(arg.volatile if isinstance(arg, Variable) else False for arg in args)\n-        if is_volatile:\n-            def get_var_key(var):\n-                return (var.size(), self.VOLATILE)\n-        else:\n-            def get_var_key(var):\n-                return (var.size(), var.requires_grad)\n-        return tuple(get_var_key(arg) if isinstance(arg, Variable) else arg for arg in args)\n-\n-    def get_trace_inputs(self, args, extra=()):\n-        return tuple(itertools.chain(self._state_values(), flatten(args), extra))\n-\n-    def run_closure(self, trace_info, args, trace_inputs):\n-        if self.verify:\n-            cloned_args = tuple(_clone_inputs(args))\n-            with _time(\"run_real\", self.time), _fork_rng(self.verify):\n-                flat_real_out = flatten((self._run(*cloned_args),))\n-\n-        with _time(\"run_trace\", self.time):\n-            flat_out = trace_info.closure()(*_varify(trace_inputs))\n-        if not isinstance(flat_out, tuple):\n-            flat_out = (flat_out,)\n-\n-        if self.verify:\n-            _verify(flat_out, flat_real_out)\n \n-        return function._unflatten(flat_out, trace_info.proto)\n+    import torch.contrib._graph_vis as graph_vis\n \n-    def record_trace(self, args, extra=()):\n-        is_volatile = any(arg.volatile if isinstance(arg, Variable) else False for arg in args)\n-        trace_inputs = self.get_trace_inputs(args, extra)\n-\n-        trace = torch._C._tracer_enter(trace_inputs, 0 if is_volatile else self.num_derivatives)\n-        out = self._run(*args)\n-        torch._C._tracer_exit(flatten(out))\n-\n-        return trace, out\n-\n-    def has_trace_for(self, *args):\n-        trace_inputs = self.get_trace_inputs(args)\n-        trace_info = self.traces.get(self.get_input_key(trace_inputs))\n-        if trace_info is None:\n-            return False\n-        trace_info.check_traces()\n-        return trace_info.complete_trace is not None\n-\n-    def __call__(self, *args):\n-        # Run the real thing if tracing is disabled\n-        if not self.enabled:\n-            with _time(\"run_real\", self.time):\n-                return self._run(*args)\n-\n-        trace_inputs = self.get_trace_inputs(args)\n-        input_key = self.get_input_key(trace_inputs)\n-        trace_info = self.traces[input_key]\n-        # Use the compiled closure if we have it already\n-        if trace_info.closure is not None:\n-            return self.run_closure(trace_info, args, trace_inputs)\n-\n-        # Check if any of the traces in our pool are complete now\n-        trace_info.check_traces()\n-        if trace_info.complete_trace is not None:\n-            trace_info.compile_trace(self.optimize)\n-            return self.run_closure(trace_info, args, trace_inputs)\n-\n-        # Otherwise, we have to collect a new trace\n-        trace, out = self.record_trace(args)\n-        trace_info.traces.append(trace)\n-        if trace_info.proto is None:\n-            trace_info.proto = function._to_proto(out)\n-        return out\n-\n-\n-def record_trace(traceable, *args, **kwargs):\n-    \"\"\"\n-    Record a trace for a traceable object (either a function or a Module),\n-    returning a tuple (trace, output).  Positional arguments are passed\n-    as arguments to the model, while keyword arguments are used to control\n-    how we go about performing the trace.\n-\n-    TODO: document kwargs\n-    \"\"\"\n-    parameters = kwargs.pop('parameters', ())\n-    return Traceable(traceable, **kwargs).record_trace(args, extra=parameters)\n-\n-\n-def traced(traceable, **traced_kwargs):\n-    t = Traceable(traceable, **traced_kwargs)\n-    if isinstance(traceable, Module):\n-        traceable.forward = t\n-        return traceable\n-    else:\n-        return t\n+    filename = \"{}_{}\".format(trace_name, pass_name)\n+    # TODO: Also paste out the backtrace when the trace was compiled\n+    # (and maybe also when it was run?)\n+    with open(filename + \".ir\", \"w\") as f:\n+        f.write(\"Input key: {}\\n\\n{}\".format(input_key, str(trace)))\n+    graph_vis.write(trace.graph(), filename + \".html\")\n \n \n-def trace(**kwargs):\n-    return lambda traceable: traced(traceable, **kwargs)\n+@contextlib.contextmanager\n+def _time(trace_name, name, time=True):\n+    if (not _JIT_TIME and not time) or not torch.cuda.is_available():\n+        yield\n+        return\n+    stream = torch.cuda.current_stream()\n+    start = torch.cuda.Event(enable_timing=True)\n+    end = torch.cuda.Event(enable_timing=True)\n+    stream.record_event(start)\n+    try:\n+        yield\n+    finally:\n+        stream.record_event(end)\n+        end.synchronize()\n+        print(\"{} {} time: {} ms\".format(trace_name, name, start.elapsed_time(end)))", "path": "torch/jit.py", "position": 770, "original_position": 771, "commit_id": "32f59e2ce8acb246bd9091e44732a5f339e4cc4b", "original_commit_id": "8997935b09e86e5c8748b48efc6fb1a9e001410f", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "You're aware that this only records CUDA time, so it will show 0 if your model is CPU only, right?", "created_at": "2017-09-28T15:18:01Z", "updated_at": "2018-11-23T15:34:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/2852#discussion_r141648986", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2852", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/141648986"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2852#discussion_r141648986"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2852"}}, "body_html": "<p>You're aware that this only records CUDA time, so it will show 0 if your model is CPU only, right?</p>", "body_text": "You're aware that this only records CUDA time, so it will show 0 if your model is CPU only, right?"}