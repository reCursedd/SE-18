{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140809110", "pull_request_review_id": 64929758, "id": 140809110, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDgwOTExMA==", "diff_hunk": "@@ -1,26 +1,375 @@\n import torch.autograd.function as function\n import torch._C\n from torch.autograd import Variable\n-from torch.nn import Module\n+from torch.nn import Module, ParameterList, Parameter\n from collections import defaultdict\n import itertools\n import types\n import contextlib\n import os\n+import functools\n import torch.contrib._graph_vis as graph_vis\n-# Example how to use:\n-#\n-# import torch.jit\n-# model = model.RNNModel(args.model, ...)\n-# model = torch.jit.traced(model)\n \n \n-def flatten(x):\n+HOLE = object()\n+VOLATILE = object()\n+\n+\n+# TODO: verify is not implemented yet\n+def compile(arg=None, nderivs=1, params=tuple(), verify=False, optimize=True, enabled=True):\n+    \"\"\"\n+    Mark a function or module as eligible for just-in-time compilation.  The\n+    next time the function/module is executed, it is traced, and the trace is\n+    compiled into an optimized representation which is run in lieu of the\n+    original Python code upon subsequent invocations of the function/module.\n+\n+    The result of this function is stateful, so make sure you invoke it\n+    only once per code you want to JIT compile.\n+\n+    .. note::\n+\n+        If your function/module takes non-Variable inputs, the JIT compiler\n+        will compile a trace separately for each distinct input configuration.\n+\n+    .. warning::\n+\n+        Just-in-time compilation currently only works for functions/modules\n+        which do not have dynamic control flow; if you compile a function/module\n+        which has dynamic control flow, you will silently get incorrect\n+        results on subsequent invocations of the model.  Use `verify=True` to\n+        check that the original Python code and optimized code are equivalent.\n+\n+    Arguments:\n+        arg (optional, torch.nn.Module or function): the function or module\n+            to be compiled.  If `None`, `compile` returns a decorator which can be\n+            applied to the function or module you want to compile.\n+        verify (bool, default False): if True, upon all invocations of the\n+            function/module, execute both the compiled and interpreted versions\n+            of the model, and verify that their results match.  This is an easy\n+            (albeit slow) way to check if your function/module can be validly\n+            JIT compiled.\n+        nderivs (int, default 1): the number of derivatives which this function/module\n+            will be used with.  You MUST accurately specify this number: set it too\n+            low and you will see an error when you attempt to run `backward`;\n+            set it too high, and the function/module will never be compiled\n+            (as we always wait to see all derivatives before compiling.)\n+        optimize (bool, default False): whether or not to apply optimizations.\n+        enabled (bool, default True): whether or not to actually enable the JIT\n+            compiler.  This is a convenient way to disable a compilation statement\n+            without deleting the actual `compile` invocation.\n+\n+    Example: Compile as function decorator.\n+\n+        >>> @jit.compile\n+        >>> def f(x);\n+        >>>     return x * 2\n+        >>> x = Variable(torch.randn(1))\n+        >>> out1 = f(x)  # interpreted run\n+        >>> out1.sum().backward()  # required!\n+        >>> out2 = f(x)  # compiled run\n+        >>> out2.sum().backward()\n+\n+    Example: Compile as higher order function. (Notice that compile is a *curried*\n+    function; you first apply it with the function/model to trace, and then\n+    apply the result with the arguments.)\n+\n+        >>> compiled_model = jit.compile(nn.LSTMCell())\n+        >>> out = compiled_model(input, hidden)\n+\n+    Example: Compile forwards only as function decorator\n+\n+        >>> @jit.compile(nderivs=0)\n+        >>> def f(x);\n+        >>>     return x * 2\n+        >>> out1 = f(x)  # interpreted run\n+        >>> out2 = f(x)  # compiled run\n+\n+    Example: Compile forwards only as higher order function\n+\n+        >>> compiled_model = jit.compile(nn.LSTMCell(), nderivs=0)\n+        >>> out1 = compiled_model(input, hidden)  # interpreted run\n+        >>> out2 = compiled_model(input, hidden)  # compiled run\n+    \"\"\"\n+    # TODO: handle decorating a class (not an instance)\n+    def _compile(inner):\n+        if enabled:\n+            return CompiledModule(inner, params=params, nderivs=nderivs, optimize=optimize)\n+        else:\n+            return inner\n+    if callable(arg):\n+        return _compile(arg)\n+    else:\n+        return _compile\n+\n+\n+def trace(arg=None, nderivs=0, params=tuple()):\n+    \"\"\"\n+    Instrument a function or module for tracing, wrapping it in a\n+    :class:`TracedModule`, whose forward accepts the same arguments as the\n+    original function/module, but returns a tuple consisting of the\n+    *trace* of an execution, as well as the original return value.\n+\n+    Tracing is guaranteed not to change the semantics of the function/module\n+    that is traced.\n+\n+    Arguments:\n+        arg (optional, torch.nn.Module or function): the function or module\n+            to be traced.  If `None`, `trace` returns a decorator which can be\n+            applied to the function or module you want to trace.\n+        nderivs (int, default 0): the number of derivatives to trace.\n+            Traces of derivatives are recorded into the same trace returned\n+            after executing the `forward` of the resulting module, but\n+            are not present until you run `backward()` (an appropriate\n+            number of times) on the resulting model.\n+        params (tuple of torch.nn.Parameter): extra parameters for a traced\n+            function, which do not occur as arguments to the function in\n+            question.  You generally do not need this for tracing modules, as\n+            the parameters of a module are automatically computed.\n+\n+    Example: Trace as function decorator.\n+\n+        >>> @jit.trace\n+        >>> def f(x);\n+        >>>     return x * 2\n+        >>> trace, out = f(Variable(torch.randn(1)))\n+\n+    Example: Trace as higher order function. (Notice that trace is a *curried*\n+    function; you first apply it with the function/model to trace, and then\n+    apply the result with the arguments.)\n+\n+        >>> traced_model = jit.trace(nn.LSTMCell())\n+        >>> trace, out = traced_model(input, hidden)\n+\n+    Example: Trace the backwards pass as function decorator.\n+\n+        >>> @jit.trace(nderivs=1)\n+        >>> def f(x);\n+        >>>     return x * 2\n+        >>> trace, out = f(Variable(torch.randn(1)))\n+        >>> out.sum().backward()\n+        >>> print(trace)\n+\n+    Example: Trace the backwards pass as higher order function.\n+\n+        >>> traced_model = jit.trace(nn.LSTMCell(), nderivs=1)\n+        >>> trace, out = traced_model(input, hidden)\n+        >>> out.sum().backward()\n+        >>> print(trace)\n+\n+    Example: Trace a function with extra parameters. (If you trace\n+    a Module, parameters are automatically computed via `state_dict`).\n+\n+        >>> lstm = nn.LSTMCell(10, 20)\n+        >>> @jit.trace(params=lstm.parameters())\n+        >>> def f(a, b):\n+        >>>     return lstm(a, b)\n+    \"\"\"\n+    # TODO: handle decorating a class (not a callable)\n+    def _trace(inner):\n+        return TracedModule(inner, nderivs=nderivs, params=params)\n+    if callable(arg):\n+        return _trace(arg)\n+    else:\n+        return _trace\n+\n+\n+# TODO: Formulating it this way means that state_dict of the traced thing\n+# looks different, etc (because there's extra nesting).  Figure out a\n+# way to avoid this\n+class TracedModule(Module):\n+    def __init__(self, inner, params=tuple(), nderivs=0):\n+        super(TracedModule, self).__init__()\n+        # inner may be a Module, or it may be an arbitrary callable\n+        self.inner = inner\n+        self.params = ParameterList(list(params))\n+        self.nderivs = nderivs\n+\n+    def forward(self, *args):\n+        # TODO: Possible optimization: use the unflattened\n+        # output so we don't unflatten it when we get out\n+        # NB: Not a method because trace_func_raw can't deal\n+        # with methods\n+        @raw_trace(nderivs=self.nderivs)\n+        def traced_inner(in_vars, in_struct):\n+            # NB: Uncommenting this line should be equivalent\n+            # args, _params = _unflatten(in_vars, in_struct)\n+            return _flatten(self.inner(*args))\n+\n+        in_vars, in_struct = _flatten(args, self.state_dict(keep_vars=True).values())\n+        trace, (out_vars, out_struct) = traced_inner(in_vars, in_struct)\n+        out, extra = _unflatten(out_vars, out_struct)\n+        assert len(extra) == 0\n+        return trace, out\n+\n+\n+# Functional version that assumes that all parameters are explicitly\n+# specified\n+def raw_trace(nderivs=0):\n+    def _raw_trace(f):\n+        @functools.wraps(f)\n+        def wrapper(in_vars, in_struct=None):\n+            trace = torch._C._tracer_enter(in_vars, nderivs)\n+            out_vars, out_struct = f(in_vars, in_struct)\n+            torch._C._tracer_exit(out_vars)\n+            return trace, (out_vars, out_struct)\n+        return wrapper\n+    return _raw_trace\n+\n+\n+# Lifecycle of a CompiledModule:\n+# - It is given an underlying function, which knows how to actually\n+#   execute the code that we want to compile.\n+# - When we encounter an input configuration for which we don't\n+#   have an optimized trace, we run the underlying function, tracing its\n+#   result.  The trace is not done yet, so we save it into our set of pending\n+#   traces for that configuration.\n+# - When we encounter an input configuration whose trace is \"ready\"\n+#   (that is, we've seen all of the passes, so the trace contains\n+#   forwards/backwards/etc), we compile it, and then register this\n+#   as the compiled trace.\n+# - When we encounter an input configuration whose trace is compiled,\n+#   we just directly run the compiled trace.\n+class CompiledModule(Module):\n+    def __init__(self, inner, params=tuple(), **kwargs):\n+        super(CompiledModule, self).__init__()\n+        self.inner = inner\n+        self.params = ParameterList(list(params))\n+        self.kwargs = kwargs\n+        self.ktrace_cache = {}", "path": "torch/jit.py", "position": null, "original_position": 245, "commit_id": "32f59e2ce8acb246bd9091e44732a5f339e4cc4b", "original_commit_id": "53b0dea5c0344533ee76434dae2987e7e7a97f7c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "why the k prefix?", "created_at": "2017-09-25T15:23:25Z", "updated_at": "2018-11-23T15:34:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/2852#discussion_r140809110", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2852", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140809110"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2852#discussion_r140809110"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2852"}}, "body_html": "<p>why the k prefix?</p>", "body_text": "why the k prefix?"}