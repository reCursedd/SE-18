{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/141526564", "pull_request_review_id": 65750367, "id": 141526564, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTUyNjU2NA==", "diff_hunk": "@@ -1,26 +1,496 @@\n import torch.autograd.function as function\n import torch._C\n from torch.autograd import Variable\n-from torch.nn import Module\n+from torch.nn import Module, ParameterList, Parameter\n+from torch._six import raise_from\n from collections import defaultdict\n import itertools\n import types\n import contextlib\n import os\n-import torch.contrib._graph_vis as graph_vis\n-# Example how to use:\n+import functools\n+import inspect\n+\n+\n+class Placeholder(object):\n+    def __init__(self, s):\n+        self.s = s\n+\n+    def __str__(self):\n+        return self.s\n+\n+    def __repr__(self):\n+        return self.s\n+\n+\n+HOLE = Placeholder(\"HOLE\")\n+VOLATILE = Placeholder(\"VOLATILE\")\n+\n+\n+# TODO: verify is not implemented yet\n+def compile(arg=None, verify=False, **kwargs):\n+    \"\"\"\n+    Decorator which marks a function or module class as eligible for\n+    just-in-time compilation.  The next time the function/module is executed, it\n+    is traced, and the trace is compiled into an optimized representation which\n+    is run in lieu of the original Python code upon subsequent invocations of\n+    the function/module.\n+\n+    .. note::\n+\n+        A JIT compiled function/module may be compiled multiple times, as\n+        different inputs can result in different traces.  Currently, the\n+        JIT compiler conservatively assumes the trace may change if the\n+        `size` or `requires_grad` of `Variable` inputs change, or if\n+        any of the non-Variable inputs change.  For example, if you JIT\n+        compile an RNN which takes the number of hidden units as a parameter,\n+        we will compile a trace for every RNN length you use at runtime.\n+\n+        When a module class is JIT compiled, each instantiation of the module\n+        gets a separate trace cache.\n+\n+    .. warning::\n+\n+        Just-in-time compilation currently only works for functions/modules\n+        which are not data dependent (e.g., have conditionals on data in\n+        tensors) and do not have any untracked external dependencies (e.g.,\n+        perform input/output or access global variables). If you trace such\n+        models, you will silently get incorrect results on subsequent\n+        invocations of the model.  You can use `verify=True` to check that the\n+        original Python code and optimized code are equivalent.\n+\n+    Keyword arguments:\n+        verify (bool, optional): if True, upon all invocations of the\n+            function/module, execute both the compiled and interpreted versions\n+            of the model, and verify that their results match.  This is an easy\n+            (albeit slow) way to check if your function/module can be validly\n+            JIT compiled.  Default: False.\n+        nderivs (int, optional): the number of derivatives which this function/module\n+            will be used with.  You MUST accurately specify this number: set it too\n+            low and you will see an error when you attempt to run `backward`;\n+            set it too high, and the function/module will never be compiled\n+            (as we always wait to see all derivatives before compiling.)\n+            Default: 1 (i.e., we will compile forwards and backwards, but not\n+            double-backwards).\n+        optimize (bool, optional): whether or not to apply optimizations.  Default: True.\n+\n+    Debug arguments:\n+        time (bool, optional): if True, whenever we execute the model in question, we\n+            will also print out some timing information for how long the model\n+            took to execute.  At the moment, there are three types of timings we\n+            emit:\n+\n+                - unoptimized: the time it took to execute the vanilla Python\n+                  model.  This only occurs when tracing is disabled, e.g., via\n+                  `enabled=False`\n+\n+                - tracing: the time it took to execute the vanilla Python model\n+                  with tracing enabled.\n+\n+                - optimized: the time it took to execute the optimized model.\n+\n+            At the moment, all of these timings are for the forward pass only.\n+            Default: False.\n+        enabled (bool, optional): if False, compilation is disabled and you\n+            will get back your original model.  This is a convenient way to\n+            disable tracing without having to delete the annotation. Default: True.\n+\n+    Example: Compile as class decorator.\n+\n+        >>> @jit.compile\n+        >>> class MyModel(nn.Module):\n+        >>>     ...\n+        >>> model = MyModel()\n+        >>> out1 = model(x)        # interpreted run\n+        >>> out1.sum().backward()  # won't compile without this line\n+        >>> out2 = model(x)        # compiled run\n+        >>> out2.sum().backward()  # also compiled\n+\n+    Example: Compile forward pass only as class decorator.\n+\n+        >>> @jit.compile(nderivs=0)\n+        >>> class MyModel(nn.Module):\n+        >>>     ...\n+        >>> model = MyModel()\n+        >>> out1 = model(x)        # interpreted run\n+        >>> out2 = model(x)        # compiled run\n+\n+    Example: Compile as function decorator.  The same modes of use for the class\n+    decorator are also supported for functions; however, the decorated\n+    function must declare *all* Variable inputs in its arguments.\n+\n+        >>> @jit.compile\n+        >>> def f(x);\n+        >>>     return x * 2\n+    \"\"\"\n+    # TODO: handle decorating a class (not an instance)\n+    def _compile(arg):\n+        if inspect.isclass(arg):\n+            if issubclass(arg, _CompiledMixin):\n+                raise TypeError(\"Cannot compile a model class that already is compiled\")\n+\n+            # NB: It might seem natural to create a subclass here, rather than\n+            # make a copy of the class to insert the mixin.  Unfortunately, this\n+            # will break many user classes.  Suppose you have:\n+            #\n+            #     @torch.jit.compile\n+            #     class Foo(Module):\n+            #         def __init__(self):\n+            #             super(Foo, self).__init__() # Python 2 syntax!\n+            #\n+            # within the class definition, 'Foo' refers to the *decorated*\n+            # class, not the undecorated class.  This is bad juju if the\n+            # decorator returns a subclass, since super(Foo, self) is going to\n+            # refer to the *undecorated* Foo (and thus you have an infinite\n+            # loop.)  Python 3's argument-less super() does not have this\n+            # problem, but in general we cannot ask users to rewrite their code.\n+            #\n+            # If we create a *copy* of the class (unrelated to the class the\n+            # user passed in), this problem goes away, because the class\n+            # __init__ is a part of is indeed Foo.\n+\n+            # Make a copy of the class, with the extra _CompiledMixin base\n+            cls = type(arg.__name__, (_CompiledMixin,) + arg.__bases__, dict(arg.__dict__))\n+\n+            # Monkey-patch forward and __init__ with the compiler versions\n+            cls.init_compiler(**kwargs)\n+            return cls\n+        elif isinstance(arg, Module):\n+            # It requires work to compile module instances, because you would\n+            # like the resulting compiled module to look just like the uncompiled\n+            # version; actually achieving this requires a bit of fanciness.\n+            # So for now, we just only support the class mechanism.\n+            raise TypeError(\"Compiling model instances is not supported.  \"\n+                            \"Use @torch.jit.compile on a class instead.\")\n+        elif callable(arg):\n+            @compile(**kwargs)\n+            class FuncModule(Module):\n+                def __init__(self, f):\n+                    super(FuncModule, self).__init__()\n+                    self.f = f\n+\n+                def forward(self, *args):\n+                    return self.f(*args)\n+\n+            return FuncModule(arg)\n+        else:\n+            raise TypeError(\"Cannot handle arg with type {}\".format(type(arg)))\n+    if arg is None:\n+        return _compile\n+    else:\n+        return _compile(arg)\n+\n+\n+def trace(arg=None, nderivs=0):\n+    \"\"\"\n+    Instrument a function or module for tracing, wrapping it in a\n+    :class:`TracedModule`, whose forward accepts the same arguments as the\n+    original function/module, but returns a tuple consisting of the\n+    *trace* of an execution, as well as the original return value.\n+\n+    Tracing is guaranteed not to change the semantics of the function/module\n+    that is traced.\n+\n+    Arguments:\n+        arg (optional, torch.nn.Module or function): the function or module\n+            to be traced.  If `None`, `trace` returns a decorator which can be\n+            applied to the function or module you want to trace.\n+        nderivs (int, default 0): the number of derivatives to trace.\n+            Traces of derivatives are recorded into the same trace returned\n+            after executing the `forward` of the resulting module, but\n+            are not present until you run `backward()` (an appropriate\n+            number of times) on the resulting model.\n+\n+    Example: Trace as higher order function. (Notice that trace is a *curried*\n+    function; you first apply it with the function/model to trace, and then\n+    apply the result with the arguments.)\n+\n+        >>> traced_model = jit.trace(nn.LSTMCell())\n+        >>> trace, out = traced_model(input, hidden)\n+\n+    Example: Trace the backwards pass as higher order function.\n+\n+        >>> traced_model = jit.trace(nn.LSTMCell(), nderivs=1)\n+        >>> trace, out = traced_model(input, hidden)\n+        >>> out.sum().backward()\n+        >>> print(trace)\n+    \"\"\"\n+    # TODO: handle decorating a class (not a callable)\n+    def _trace(inner):\n+        return TracedModule(inner, nderivs=nderivs)\n+    if callable(arg):\n+        return _trace(arg)\n+    else:\n+        return _trace\n+\n+\n+# It's OK for TracedModule to look different from the inner module, since\n+# the forward() return type changed anyway.\n+class TracedModule(Module):\n+    def __init__(self, inner, nderivs=0):\n+        super(TracedModule, self).__init__()\n+        # inner may be a Module, or it may be an arbitrary callable\n+        self.inner = inner\n+        self.nderivs = nderivs\n+\n+    def forward(self, *args):\n+        # TODO: Possible optimization: use the unflattened\n+        # output so we don't unflatten it when we get out\n+        # NB: Not a method because trace_func_raw can't deal\n+        # with methods\n+        @_raw_trace(nderivs=self.nderivs)\n+        def traced_inner(in_vars, in_struct):\n+            return _flatten(self.inner(*args))\n+\n+        in_vars, in_struct = _flatten(args, self.state_dict(keep_vars=True).values())\n+        trace, (out_vars, out_struct) = traced_inner(in_vars, in_struct)\n+        out, extra = _unflatten(out_vars, out_struct)\n+        assert len(extra) == 0\n+        return trace, out\n+\n+\n+# Functional version that assumes that all parameters are explicitly\n+# specified\n+def _raw_trace(nderivs=0):\n+    def raw_trace(f):\n+        # f takes two arguments, (in_vars, in_struct) (as determined\n+        # by _flatten); furthermore, it must be the case that in_vars\n+        # contains all Variable inputs (including parameters.)  It must\n+        # produce two outputs, (out_vars, out_struct) (also as determined\n+        # by _flatten).\n+        @functools.wraps(f)\n+        def wrapper(in_vars, in_struct=None):\n+            trace = torch._C._tracer_enter(in_vars, nderivs)\n+            out_vars, out_struct = f(in_vars, in_struct)\n+            torch._C._tracer_exit(out_vars)\n+            return trace, (out_vars, out_struct)\n+        return wrapper\n+    return raw_trace\n+\n+\n+# Lifecycle of a compiler:\n+#\n+# - It is given an underlying function, which knows how to actually\n+#   execute the code that we want to compile.\n+# - When we encounter an input configuration for which we don't\n+#   have an optimized trace, we run the underlying function, tracing its\n+#   result.  The trace is not done yet, so we save it into our set of pending\n+#   traces for that configuration.\n+# - When we encounter an input configuration whose trace is \"ready\"\n+#   (that is, we've seen all of the passes, so the trace contains\n+#   forwards/backwards/etc), we compile it, and then register this\n+#   as the compiled trace.\n+# - When we encounter an input configuration whose trace is compiled,\n+#   we just directly run the compiled trace.\n #\n-# import torch.jit\n-# model = model.RNNModel(args.model, ...)\n-# model = torch.jit.traced(model)\n+# You should never use this class directly; instead, use compile.  However,\n+# the intended manual usage of this class looks like this:\n+#\n+#     class CompiledModel(_CompiledMixin, nn.Module):\n+#         def forward(self, x):\n+#             ...\n+#     CompiledModule.init_compiler()\n+#     model = CompiledModule()\n+#\n+class _CompiledMixin(object):\n+    # Global over ALL compilations!  This helps us disambig if two Modules have\n+    # the same __name__ but actually are different\n+    __next_id = 0\n+\n+    @classmethod\n+    def init_compiler(cls, name=None, enabled=True, time=False, **kwargs):\n+        # Ensure we are not shadowing this method on the class we mixed with\n+        assert not hasattr(super(_CompiledMixin, cls), \"init_compiler\")\n+        # TODO: Consider saving the backtrace of this constructor, so it's easier\n+        # to correlate dump files with invocations in Python\n+        #\n+        # NB: Use private methods/variables here in order to prevent a class\n+        # we mix with from accidentally scrambling us\n+        #\n+        # NB: Class variables are also accessible via self!\n+        kwargs[\"time\"] = time  # also want to pass onto ktrace\n+        cls.__ktrace_kwargs = kwargs\n+        cls.__enabled = enabled\n+        cls.__time = time\n+        cls.__model_name = name\n+\n+        # Monkey patch the constructor and forward functions *inplace*\n+        cls.__old_forward = cls.forward\n+        cls.forward = cls.__new_forward\n+        cls.__old_init = cls.__init__\n+        cls.__init__ = cls.__new_init\n+\n+    def __new_init(self, *args, **kwargs):\n+        try:\n+            # __old_init is assumed to handle super call\n+            self.__old_init(*args, **kwargs)\n+        except TypeError as e:\n+            # If this fails here, the user probably didn't use this as a class\n+            # decorator\n+            if \"super\" in str(e):\n+                raise_from(TypeError(\"torch.jit.compile must be used as a class decorator; \"\n+                                     \"using it on an already defined class is not valid.\"\n+                                     \"\\n\\nOriginal error: {}\".format(str(e))), e)\n+            else:\n+                raise\n+        model_name = self.__model_name if self.__model_name else type(self).__name__\n+        self.__name = \"jit_{}_{}\".format(model_name, _CompiledMixin.__next_id)\n+        _CompiledMixin.__next_id += 1\n+        self.__ktrace_cache = {}\n+        self.__next_ktrace_id = 0\n+\n+    def __process_args(self, args):\n+        in_vars, in_struct = _flatten(args, self.state_dict(keep_vars=True).values())\n+        is_volatile, in_vars_key = vars_key(in_vars)\n+        in_key = (in_vars_key, in_struct)\n+        return in_vars, in_struct, is_volatile, in_key\n+\n+    # NB: In principle, there could also be a 'raw' version of this compiler,\n+    # but since the logic is so complicated, testing code wouldn't benefit much\n+    def __new_forward(self, *args):\n+        if _JIT_DISABLE or not self.__enabled:\n+            with _time(self.__name, \"unoptimized\", self.__time):\n+                # Call to the saved old forward function\n+                return self.__old_forward(*args)\n+        in_vars, in_struct, is_volatile, in_key = self.__process_args(args)\n+        ktrace = self.__ktrace_cache.get(in_key)\n+        if ktrace is None:\n+            ktrace_name = '{}_{}'.format(self.__name, self.__next_ktrace_id)\n+            self.__next_ktrace_id += 1\n+            ktrace = TraceForKey(ktrace_name, in_key, volatile=is_volatile, **self.__ktrace_kwargs)\n+            self.__ktrace_cache[in_key] = ktrace\n+        closure = ktrace.maybe_closure()\n+        if closure is not None:\n+            # We already compiled it!  Run it directly, and\n+            # use the saved out_struct to unflatten.\n+            with _time(ktrace.name, \"optimized\", self.__time):\n+                out_vars = closure()(*in_vars)\n+                out_struct = ktrace.out_struct\n+        else:", "path": "torch/jit.py", "position": 374, "original_position": 375, "commit_id": "32f59e2ce8acb246bd9091e44732a5f339e4cc4b", "original_commit_id": "ae144714b9f502e917113076da4076a0ab1d383a", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Is it possible to check here that the cache already has an active (but incomplete) trace for this configuration, and just skip adding another trace such that TraceForKey only needs 1 trace?", "created_at": "2017-09-28T05:00:37Z", "updated_at": "2018-11-23T15:34:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/2852#discussion_r141526564", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2852", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/141526564"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2852#discussion_r141526564"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2852"}}, "body_html": "<p>Is it possible to check here that the cache already has an active (but incomplete) trace for this configuration, and just skip adding another trace such that TraceForKey only needs 1 trace?</p>", "body_text": "Is it possible to check here that the cache already has an active (but incomplete) trace for this configuration, and just skip adding another trace such that TraceForKey only needs 1 trace?"}