{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4245", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4245/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4245/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4245/events", "html_url": "https://github.com/pytorch/pytorch/issues/4245", "id": 283065324, "node_id": "MDU6SXNzdWUyODMwNjUzMjQ=", "number": 4245, "title": "Memory leak when calculating higher order gradients for graph using ReLU after Conv2d", "user": {"login": "vic4ever", "id": 1460336, "node_id": "MDQ6VXNlcjE0NjAzMzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1460336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vic4ever", "html_url": "https://github.com/vic4ever", "followers_url": "https://api.github.com/users/vic4ever/followers", "following_url": "https://api.github.com/users/vic4ever/following{/other_user}", "gists_url": "https://api.github.com/users/vic4ever/gists{/gist_id}", "starred_url": "https://api.github.com/users/vic4ever/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vic4ever/subscriptions", "organizations_url": "https://api.github.com/users/vic4ever/orgs", "repos_url": "https://api.github.com/users/vic4ever/repos", "events_url": "https://api.github.com/users/vic4ever/events{/privacy}", "received_events_url": "https://api.github.com/users/vic4ever/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-12-19T00:22:40Z", "updated_at": "2017-12-19T19:25:42Z", "closed_at": "2017-12-19T19:25:42Z", "author_association": "NONE", "body_html": "<p>The following code causes memory leak each time backward is called. It only happens when Conv2D layer is followed by either ReLU or LeakyReLU layer. However, Conv2d then Tanh works fine.</p>\n<pre><code>import torch.autograd as autograd\nimport gpustat\nfrom torch import optim\nimport torch\nimport torch.nn as nn\n\nclass DCGANDiscriminator(nn.Module):\n    def __init__(self, dim=64, bn=True):\n        super(DCGANDiscriminator, self).__init__()\n        self.dim = dim\n        self.bn = bn\n\n        self.conv1 = nn.Conv2d(3, 8*self.dim, 5,stride=2,padding=2,bias=True)\n        #self.relu1 = nn.LeakyReLU(0.2)\n        self.relu1 = nn.ReLU()\n\n    def forward(self, input):\n        output = input.contiguous()\n        output = self.conv1(output)\n        output = self.relu1(output)\n        output = output.view(-1)\n        return output\n\ndef show_memusage(device=1):\n    gpu_stats = gpustat.GPUStatCollection.new_query()\n    item = gpu_stats.jsonify()[\"gpus\"][device]\n    print('Used/total: ' + \"{}/{}\".format(item[\"memory.used\"], item[\"memory.total\"]))\n\naD = DCGANDiscriminator()\naD = aD.cuda()\nfor i in range(1000):\n    print(\"Critic iter: \" + str(i))\n    aD.zero_grad()\n    real_data = autograd.Variable(torch.randn(1,3,64,64),requires_grad=True).cuda()\n\n    show_memusage(0)\n    out = aD(real_data)\n    gradients = autograd.grad(outputs=out, inputs=real_data,\n                              grad_outputs=torch.ones(out.size()).cuda(),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = gradients.mean()\n    gradient_penalty.backward()\n    show_memusage(0)\n</code></pre>\n<p>Here is some output:</p>\n<pre><code>Critic iter: 0\nUsed/total: 1725/11172\nUsed/total: 1835/11172\nCritic iter: 1\nUsed/total: 1835/11172\nUsed/total: 1837/11172\nCritic iter: 2\nUsed/total: 1837/11172\nUsed/total: 1839/11172\nCritic iter: 3\nUsed/total: 1839/11172\nUsed/total: 1841/11172\nCritic iter: 4\nUsed/total: 1841/11172\nUsed/total: 1843/11172\nCritic iter: 5\nUsed/total: 1843/11172\nUsed/total: 1845/11172\nCritic iter: 6\nUsed/total: 1845/11172\nUsed/total: 1847/11172\n</code></pre>\n<p>I'm using pytorch version 0.4.0a0+e2c75d3 on Ubuntu 16.04, python 3.6, CUDA 8.</p>", "body_text": "The following code causes memory leak each time backward is called. It only happens when Conv2D layer is followed by either ReLU or LeakyReLU layer. However, Conv2d then Tanh works fine.\nimport torch.autograd as autograd\nimport gpustat\nfrom torch import optim\nimport torch\nimport torch.nn as nn\n\nclass DCGANDiscriminator(nn.Module):\n    def __init__(self, dim=64, bn=True):\n        super(DCGANDiscriminator, self).__init__()\n        self.dim = dim\n        self.bn = bn\n\n        self.conv1 = nn.Conv2d(3, 8*self.dim, 5,stride=2,padding=2,bias=True)\n        #self.relu1 = nn.LeakyReLU(0.2)\n        self.relu1 = nn.ReLU()\n\n    def forward(self, input):\n        output = input.contiguous()\n        output = self.conv1(output)\n        output = self.relu1(output)\n        output = output.view(-1)\n        return output\n\ndef show_memusage(device=1):\n    gpu_stats = gpustat.GPUStatCollection.new_query()\n    item = gpu_stats.jsonify()[\"gpus\"][device]\n    print('Used/total: ' + \"{}/{}\".format(item[\"memory.used\"], item[\"memory.total\"]))\n\naD = DCGANDiscriminator()\naD = aD.cuda()\nfor i in range(1000):\n    print(\"Critic iter: \" + str(i))\n    aD.zero_grad()\n    real_data = autograd.Variable(torch.randn(1,3,64,64),requires_grad=True).cuda()\n\n    show_memusage(0)\n    out = aD(real_data)\n    gradients = autograd.grad(outputs=out, inputs=real_data,\n                              grad_outputs=torch.ones(out.size()).cuda(),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = gradients.mean()\n    gradient_penalty.backward()\n    show_memusage(0)\n\nHere is some output:\nCritic iter: 0\nUsed/total: 1725/11172\nUsed/total: 1835/11172\nCritic iter: 1\nUsed/total: 1835/11172\nUsed/total: 1837/11172\nCritic iter: 2\nUsed/total: 1837/11172\nUsed/total: 1839/11172\nCritic iter: 3\nUsed/total: 1839/11172\nUsed/total: 1841/11172\nCritic iter: 4\nUsed/total: 1841/11172\nUsed/total: 1843/11172\nCritic iter: 5\nUsed/total: 1843/11172\nUsed/total: 1845/11172\nCritic iter: 6\nUsed/total: 1845/11172\nUsed/total: 1847/11172\n\nI'm using pytorch version 0.4.0a0+e2c75d3 on Ubuntu 16.04, python 3.6, CUDA 8.", "body": "The following code causes memory leak each time backward is called. It only happens when Conv2D layer is followed by either ReLU or LeakyReLU layer. However, Conv2d then Tanh works fine. \r\n\r\n    import torch.autograd as autograd\r\n    import gpustat\r\n    from torch import optim\r\n    import torch\r\n    import torch.nn as nn\r\n\r\n    class DCGANDiscriminator(nn.Module):\r\n        def __init__(self, dim=64, bn=True):\r\n            super(DCGANDiscriminator, self).__init__()\r\n            self.dim = dim\r\n            self.bn = bn\r\n\r\n            self.conv1 = nn.Conv2d(3, 8*self.dim, 5,stride=2,padding=2,bias=True)\r\n            #self.relu1 = nn.LeakyReLU(0.2)\r\n            self.relu1 = nn.ReLU()\r\n\r\n        def forward(self, input):\r\n            output = input.contiguous()\r\n            output = self.conv1(output)\r\n            output = self.relu1(output)\r\n            output = output.view(-1)\r\n            return output\r\n\r\n    def show_memusage(device=1):\r\n        gpu_stats = gpustat.GPUStatCollection.new_query()\r\n        item = gpu_stats.jsonify()[\"gpus\"][device]\r\n        print('Used/total: ' + \"{}/{}\".format(item[\"memory.used\"], item[\"memory.total\"]))\r\n\r\n    aD = DCGANDiscriminator()\r\n    aD = aD.cuda()\r\n    for i in range(1000):\r\n        print(\"Critic iter: \" + str(i))\r\n        aD.zero_grad()\r\n        real_data = autograd.Variable(torch.randn(1,3,64,64),requires_grad=True).cuda()\r\n\r\n        show_memusage(0)\r\n        out = aD(real_data)\r\n        gradients = autograd.grad(outputs=out, inputs=real_data,\r\n                                  grad_outputs=torch.ones(out.size()).cuda(),\r\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\r\n        gradients = gradients.view(gradients.size(0), -1)\r\n        gradient_penalty = gradients.mean()\r\n        gradient_penalty.backward()\r\n        show_memusage(0)\r\n\r\nHere is some output:\r\n\r\n    Critic iter: 0\r\n    Used/total: 1725/11172\r\n    Used/total: 1835/11172\r\n    Critic iter: 1\r\n    Used/total: 1835/11172\r\n    Used/total: 1837/11172\r\n    Critic iter: 2\r\n    Used/total: 1837/11172\r\n    Used/total: 1839/11172\r\n    Critic iter: 3\r\n    Used/total: 1839/11172\r\n    Used/total: 1841/11172\r\n    Critic iter: 4\r\n    Used/total: 1841/11172\r\n    Used/total: 1843/11172\r\n    Critic iter: 5\r\n    Used/total: 1843/11172\r\n    Used/total: 1845/11172\r\n    Critic iter: 6\r\n    Used/total: 1845/11172\r\n    Used/total: 1847/11172\r\n\r\nI'm using pytorch version 0.4.0a0+e2c75d3 on Ubuntu 16.04, python 3.6, CUDA 8."}