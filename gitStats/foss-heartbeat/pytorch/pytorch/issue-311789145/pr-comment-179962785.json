{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179962785", "pull_request_review_id": 110301503, "id": 179962785, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTk2Mjc4NQ==", "diff_hunk": "@@ -52,8 +52,51 @@ class DataParallel(Module):\n \n     .. warning::\n         Forward and backward hooks defined on :attr:`module` and its submodules\n-        won't be invoked anymore, unless the hooks are initialized in the\n-        :meth:`forward` method.\n+        will be invoked ``len(device_ids)`` times, each with inputs located on\n+        a particular device. Particularly, the hooks are only guaranteed to be\n+        executed in correct order with respect to operations on corresponding\n+        devices. For example, it is not guaranteed that hooks set via\n+        :meth:`~torch.nn.Module.register_forward_pre_hook` be executed before\n+        `all` ``len(device_ids)`` :meth:`~torch.nn.Module.forward` calls, but\n+        that each such hook be executed before the corresponding\n+        :meth:`~torch.nn.Module.forward` call of that device.\n+\n+    .. note::\n+        There is a subtlety in using the\n+        ``pack sequence -> recurrent network -> unpack sequence`` pattern in a\n+        :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`.\n+        Input to each the :meth:`forward` on each device will only be part of\n+        the entire input. Because the unpack operation\n+        :func:`torch.nn.utils.rnn.pad_packed_sequence` by default only pads up\n+        to the longest input it sees, i.e., the longest on that particular\n+        device, size mismatches will happen when results are gathered together.\n+        Therefore, you can instead take advantage of the :attr:`total_length`\n+        argument of :func:`~torch.nn.utils.rnn.pad_packed_sequence` to make sure\n+        that the :meth:`forward` calls return sequences of same length. For\n+        example, you can write::\n+\n+            from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n+\n+            class MyModule(nn.Module):\n+                # ... __init__, other methods, etc.\n+\n+                # padding_input is of shape [B x T x *] (batch_first mode) and contains\n+                # the sorted sequences\n+                # B is the batch size\n+                # T is max sequence length\n+                def forward(self, padded_input, input_lengths):\n+                    total_length = padded_input.size(1)  # get the max sequence length\n+                    packed_input = pack_padded_sequence(padded_input, input_lengths,\n+                                                        batch_first=True)\n+                    packed_output, _ = self.my_lstm(packed_input)\n+                    output, _ = pad_packed_sequence(packed_output, batch_first=True,\n+                                                    total_length=total_length)\n+                    return output\n+\n+\n+            m = MyModule().cuda()\n+            dp_m = nn.DataParallel(m)", "path": "torch/nn/parallel/data_parallel.py", "position": null, "original_position": 49, "commit_id": "7d2c810e79c01c9c6f2431b444d77a79f148e26b", "original_commit_id": "9a84c1af130cad4ca4f7bca59b5d935e2b4e69f4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think that's a bit too much for the API docs. We might want to put it in some kind of FAQ and put a reference to it here", "created_at": "2018-04-08T21:59:52Z", "updated_at": "2018-11-23T15:42:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/6327#discussion_r179962785", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6327", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179962785"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6327#discussion_r179962785"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6327"}}, "body_html": "<p>I think that's a bit too much for the API docs. We might want to put it in some kind of FAQ and put a reference to it here</p>", "body_text": "I think that's a bit too much for the API docs. We might want to put it in some kind of FAQ and put a reference to it here"}