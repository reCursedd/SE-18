{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/224090018", "pull_request_review_id": 163372982, "id": 224090018, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNDA5MDAxOA==", "diff_hunk": "@@ -0,0 +1,182 @@\n+#include \"torch/csrc/autograd/generated/VariableType.h\"\n+\n+#include \"torch/csrc/autograd/variable.h\"\n+#include \"torch/csrc/autograd/function.h\"\n+#include \"torch/csrc/autograd/edge.h\"\n+#include \"torch/csrc/autograd/grad_mode.h\"\n+#include \"torch/csrc/autograd/saved_variable.h\"\n+#include \"torch/csrc/autograd/generated/Functions.h\"\n+#include \"torch/csrc/autograd/functions/tensor.h\"\n+#include \"torch/csrc/autograd/functions/basic_ops.h\"\n+#include \"torch/csrc/jit/tracer.h\"\n+#include \"torch/csrc/jit/constants.h\"\n+#include \"torch/csrc/jit/symbolic_variable.h\"\n+#include \"torch/csrc/jit/ir.h\"\n+\n+#include \"torch/csrc/utils/variadic.h\"\n+#include \"torch/csrc/autograd/functions/utils.h\"\n+\n+#include <ATen/core/VariableHooksInterface.h>\n+\n+#include <array>\n+#include <cstddef>\n+#include <functional>\n+#include <initializer_list>\n+#include <memory>\n+#include <stdexcept>\n+#include <string>\n+#include <tuple>\n+#include <utility>\n+#include <vector>\n+\n+#ifdef _MSC_VER\n+#ifdef Type\n+#undef Type\n+#endif\n+#endif\n+\n+using namespace at;\n+using namespace torch::autograd::generated;\n+\n+namespace torch { namespace autograd {\n+\n+extern std::vector<std::unique_ptr<Type>> type_to_variable_type;\n+\n+static void check_inplace(const Tensor& tensor) {\n+  auto& var = static_cast<const Variable&>(tensor);\n+  if (var.requires_grad() && var.is_leaf() && GradMode::is_enabled()) {\n+    AT_ERROR(\n+      \"a leaf Variable that requires grad has been used in an in-place operation.\");\n+  }\n+}\n+\n+static void throw_error_out_requires_grad(const char* name) {\n+  AT_ERROR(\n+      name, \"(): functions with out=... arguments don't support automatic differentiation, \"\n+      \"but one of the arguments requires grad.\");\n+}\n+\n+// TODO: Blegh, bare references\n+\n+static void rebase_history(Variable& var, std::shared_ptr<Function> grad_fn) {\n+  if (grad_fn && var.defined()) {\n+    grad_fn->add_input_metadata(var);\n+    var.rebase_history({std::move(grad_fn), 0});\n+  }\n+}\n+\n+static void rebase_history(ArrayRef<Variable> vars, std::shared_ptr<Function> grad_fn) {\n+  if (grad_fn) {\n+    for (auto& var : vars) {\n+      if (var.defined()) {\n+        // TODO: eliminate const_cast\n+        auto output_nr = grad_fn->add_input_metadata(var);\n+        const_cast<Variable&>(var).rebase_history({grad_fn, output_nr});\n+      } else {\n+        grad_fn->add_input_metadata(Function::undefined_input());\n+      }\n+    }\n+  }\n+}\n+\n+static void increment_version(Tensor & t) {\n+  as_variable_ref(t).bump_version();\n+}\n+\n+static bool isFloatingPoint(ScalarType s) {\n+  return s == kFloat || s == kDouble || s == kHalf;\n+}\n+\n+struct Flatten : IterArgs<Flatten> {\n+  Flatten(variable_list& out) : out(out) {}\n+  variable_list& out;\n+  void operator()(const at::Tensor& x) { out.emplace_back(x); }\n+  void operator()(at::ArrayRef<at::Tensor> xs) {\n+    out.insert(out.end(), xs.begin(), xs.end());\n+  }\n+};\n+\n+template<typename... Args> inline variable_list flatten_tensor_args(Args&&... args) {\n+  variable_list out;\n+  out.reserve(count_tensors(std::forward<Args>(args)...));\n+  Flatten(out).apply(std::forward<Args>(args)...);\n+  return out; // RVO\n+}\n+\n+static Tensor as_view(const Tensor & base, Tensor tensor) {\n+  auto base_var = Variable(base);\n+  if (base_var.is_view()) {\n+    base_var = base_var.base();\n+  }\n+  return make_variable_view(std::move(base_var), std::move(tensor));\n+}\n+\n+static std::vector<Tensor> as_view(const Tensor & base, std::vector<Tensor> tensors) {\n+  auto base_var = Variable(base);\n+  if (base_var.is_view()) {\n+    base_var = base_var.base();\n+  }\n+  for(Tensor &tensor : tensors) {\n+    tensor = make_variable_view(base_var, std::move(tensor));\n+  }\n+  return tensors;\n+}\n+\n+static void check_no_requires_grad(const Tensor& tensor, const char* name) {\n+  auto& var = static_cast<const Variable&>(tensor);\n+  if (var.defined() && var.requires_grad()) {\n+    std::string msg = \"the derivative for '\";\n+    msg += name;\n+    msg += \"' is not implemented\";\n+    throw std::runtime_error(msg);\n+  }\n+}\n+\n+// Assumed that saved tensor lists are never inplace outputs\n+static std::vector<SavedVariable> make_saved_variable_list(TensorList tensors) {\n+  return fmap(tensors, [](const Tensor& tensor) -> SavedVariable {\n+      return SavedVariable{tensor, false /* is output */}; });\n+}\n+\n+static Tensor as_variable(Tensor tensor) {\n+  return make_variable(std::move(tensor), /*requires_grad=*/false);\n+}\n+\n+static std::vector<Tensor> as_variable(TensorList tl) {\n+  return fmap(tl, [](const Tensor& t) -> Tensor {\n+      return make_variable(std::move(t), /*requires_grad=*/false);\n+  });\n+}\n+\n+template <typename... Tensors, size_t... Is>\n+std::tuple<Tensors...> as_variable_impl(\n+    std::tuple<Tensors...> tensors,\n+    Indices<Is...>) {\n+  // Expand the integer parameter pack into a sequence of Variable\n+  // constructions. This turns into (boolean omitted):\n+  // Variable(std::get<0>(tensors)), Variable(std::get<1>(tensors)), ...\n+  return std::tuple<Tensors...>(\n+      as_variable(std::get<Is>(tensors))...);\n+}\n+\n+// NB: Because this was not forward declared, recursive std::tuple won't work.\n+// You can probably rejigger this to make it supported if you really need it.\n+template <typename... Tensors>\n+std::tuple<Tensors...> as_variable(std::tuple<Tensors...> tensors) {\n+  // `sizeof...(Tensors)` gets us the size of the `Tensors` parameter pack at\n+  // compile time. We use it to parameterize a `MakeIndices` class, which will\n+  // expand into an Indices object containing the numbers 0 to\n+  // sizeof...(Tensors) - 1.\n+  return as_variable_impl(\n+      tensors, typename MakeIndices<sizeof...(Tensors)>::indices());\n+}\n+\n+static std::vector<std::vector<int64_t>> to_args_sizes(TensorList tensors) {", "path": "torch/csrc/autograd/VariableTypeImplementation.h", "position": null, "original_position": 174, "commit_id": "58f1def0f9b3d7f103cc2ee94fd2a5236ca67cee", "original_commit_id": "76f33f9b9ad946f63278eca11c55025fa25f9994", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Those should probably be inline instead of static now.", "created_at": "2018-10-10T14:04:34Z", "updated_at": "2018-11-23T15:52:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/12493#discussion_r224090018", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12493", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/224090018"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12493#discussion_r224090018"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12493"}}, "body_html": "<p>Those should probably be inline instead of static now.</p>", "body_text": "Those should probably be inline instead of static now."}