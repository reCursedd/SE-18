{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10856", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10856/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10856/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10856/events", "html_url": "https://github.com/pytorch/pytorch/issues/10856", "id": 353852329, "node_id": "MDU6SXNzdWUzNTM4NTIzMjk=", "number": 10856, "title": "Upgrade BatchNorm.cpp so the stats could be frozen (sharing bn)", "user": {"login": "fvarno", "id": 40249495, "node_id": "MDQ6VXNlcjQwMjQ5NDk1", "avatar_url": "https://avatars2.githubusercontent.com/u/40249495?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fvarno", "html_url": "https://github.com/fvarno", "followers_url": "https://api.github.com/users/fvarno/followers", "following_url": "https://api.github.com/users/fvarno/following{/other_user}", "gists_url": "https://api.github.com/users/fvarno/gists{/gist_id}", "starred_url": "https://api.github.com/users/fvarno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fvarno/subscriptions", "organizations_url": "https://api.github.com/users/fvarno/orgs", "repos_url": "https://api.github.com/users/fvarno/repos", "events_url": "https://api.github.com/users/fvarno/events{/privacy}", "received_events_url": "https://api.github.com/users/fvarno/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-24T16:38:20Z", "updated_at": "2018-08-24T17:04:16Z", "closed_at": "2018-08-24T17:04:16Z", "author_association": "NONE", "body_html": "<p>Hello developers,<br>\nCurrently the only way of freezing the batchnorm stats dynamically is to play with training flag. But one may need to stop updating batchnorm's stats at some point and still be able to train delta_gammas and betas. This is addressed in Tensorflow with \"reuse\" flag. Please consider updating BatchNorm.cpp, nn.Functional and nn.modules.batchnorm.</p>\n<p>Regards,<br>\nFarshid</p>", "body_text": "Hello developers,\nCurrently the only way of freezing the batchnorm stats dynamically is to play with training flag. But one may need to stop updating batchnorm's stats at some point and still be able to train delta_gammas and betas. This is addressed in Tensorflow with \"reuse\" flag. Please consider updating BatchNorm.cpp, nn.Functional and nn.modules.batchnorm.\nRegards,\nFarshid", "body": "Hello developers,\r\nCurrently the only way of freezing the batchnorm stats dynamically is to play with training flag. But one may need to stop updating batchnorm's stats at some point and still be able to train delta_gammas and betas. This is addressed in Tensorflow with \"reuse\" flag. Please consider updating BatchNorm.cpp, nn.Functional and nn.modules.batchnorm.\r\n\r\nRegards,\r\nFarshid"}