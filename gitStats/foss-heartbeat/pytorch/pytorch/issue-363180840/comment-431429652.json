{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431429652", "html_url": "https://github.com/pytorch/pytorch/pull/12011#issuecomment-431429652", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12011", "id": 431429652, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTQyOTY1Mg==", "user": {"login": "JackWindows", "id": 1560135, "node_id": "MDQ6VXNlcjE1NjAxMzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/1560135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JackWindows", "html_url": "https://github.com/JackWindows", "followers_url": "https://api.github.com/users/JackWindows/followers", "following_url": "https://api.github.com/users/JackWindows/following{/other_user}", "gists_url": "https://api.github.com/users/JackWindows/gists{/gist_id}", "starred_url": "https://api.github.com/users/JackWindows/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JackWindows/subscriptions", "organizations_url": "https://api.github.com/users/JackWindows/orgs", "repos_url": "https://api.github.com/users/JackWindows/repos", "events_url": "https://api.github.com/users/JackWindows/events{/privacy}", "received_events_url": "https://api.github.com/users/JackWindows/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-19T16:58:33Z", "updated_at": "2018-10-19T16:58:33Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> Sorry but I can't share the code that reproduce this. We can look at the part that has problem:<br>\nSo the background thread is stuck at <code>_pin_memory_loop</code>:</p>\n<pre><code>def _pin_memory_loop(in_queue, out_queue, device_id, done_event):\n    torch.cuda.set_device(device_id)\n\n    # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the\n    # logic of this function.\n    while True:\n        try:\n            r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n        except queue.Empty:\n            continue\n        except Exception:\n            if done_event.is_set():\n                # Weird things can happen when shutting down, e.g., fd being\n                # closed when tensors are shared via fds.\n                break\n            raise\n        if r is None:\n            assert done_event.is_set()\n            return\n        elif done_event.is_set():\n            # Haven't seen the final signal yet. Keep getting until None.\n            continue\n        elif isinstance(r[1], ExceptionWrapper):\n            out_queue.put(r)\n        else:\n            idx, batch = r\n            try:\n                batch = pin_memory_batch(batch)\n            except Exception:\n                out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n            else:\n                out_queue.put((idx, batch))\n</code></pre>\n<p>The main thread is stuck at <code>_get_batch</code>:</p>\n<pre><code>    def _get_batch(self):\n        # In the non-timeout case, worker exit is covered by SIGCHLD handler.\n        # But if `pin_memory=True`, we still need account for the possibility\n        # that `pin_memory_thread` dies.\n        if self.timeout &gt; 0:\n            try:\n                return self.data_queue.get(timeout=self.timeout)\n            except queue.Empty:\n                raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n        elif self.pin_memory:\n            while self.pin_memory_thread.is_alive():\n                try:\n                    return self.data_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n                except queue.Empty:\n                    continue\n            else:\n                # while condition is false, i.e., pin_memory_thread died.\n                raise RuntimeError('Pin memory thread exited unexpectedly')\n            # In this case, `self.data_queue` is a `queue.Queue`,. But we don't\n            # need to call `.task_done()` because we don't use `.join()`.\n        else:\n            return self.data_queue.get()\n</code></pre>\n<p>Basically if <code>self.worker_result_queue</code> is empty, the dataloader will get into a deadlock state.</p>\n<p>My training script destructs DataLoader iterator object before all samples are drained, which means the shutdown signal of data workers can happen while works are in the process of fetching samples. I'm not sure if this has any side effect to the dataloader, may be you can shed some light on this.</p>", "body_text": "@SsnL Sorry but I can't share the code that reproduce this. We can look at the part that has problem:\nSo the background thread is stuck at _pin_memory_loop:\ndef _pin_memory_loop(in_queue, out_queue, device_id, done_event):\n    torch.cuda.set_device(device_id)\n\n    # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the\n    # logic of this function.\n    while True:\n        try:\n            r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n        except queue.Empty:\n            continue\n        except Exception:\n            if done_event.is_set():\n                # Weird things can happen when shutting down, e.g., fd being\n                # closed when tensors are shared via fds.\n                break\n            raise\n        if r is None:\n            assert done_event.is_set()\n            return\n        elif done_event.is_set():\n            # Haven't seen the final signal yet. Keep getting until None.\n            continue\n        elif isinstance(r[1], ExceptionWrapper):\n            out_queue.put(r)\n        else:\n            idx, batch = r\n            try:\n                batch = pin_memory_batch(batch)\n            except Exception:\n                out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n            else:\n                out_queue.put((idx, batch))\n\nThe main thread is stuck at _get_batch:\n    def _get_batch(self):\n        # In the non-timeout case, worker exit is covered by SIGCHLD handler.\n        # But if `pin_memory=True`, we still need account for the possibility\n        # that `pin_memory_thread` dies.\n        if self.timeout > 0:\n            try:\n                return self.data_queue.get(timeout=self.timeout)\n            except queue.Empty:\n                raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n        elif self.pin_memory:\n            while self.pin_memory_thread.is_alive():\n                try:\n                    return self.data_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n                except queue.Empty:\n                    continue\n            else:\n                # while condition is false, i.e., pin_memory_thread died.\n                raise RuntimeError('Pin memory thread exited unexpectedly')\n            # In this case, `self.data_queue` is a `queue.Queue`,. But we don't\n            # need to call `.task_done()` because we don't use `.join()`.\n        else:\n            return self.data_queue.get()\n\nBasically if self.worker_result_queue is empty, the dataloader will get into a deadlock state.\nMy training script destructs DataLoader iterator object before all samples are drained, which means the shutdown signal of data workers can happen while works are in the process of fetching samples. I'm not sure if this has any side effect to the dataloader, may be you can shed some light on this.", "body": "@SsnL Sorry but I can't share the code that reproduce this. We can look at the part that has problem:\r\nSo the background thread is stuck at `_pin_memory_loop`:\r\n```\r\ndef _pin_memory_loop(in_queue, out_queue, device_id, done_event):\r\n    torch.cuda.set_device(device_id)\r\n\r\n    # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the\r\n    # logic of this function.\r\n    while True:\r\n        try:\r\n            r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\r\n        except queue.Empty:\r\n            continue\r\n        except Exception:\r\n            if done_event.is_set():\r\n                # Weird things can happen when shutting down, e.g., fd being\r\n                # closed when tensors are shared via fds.\r\n                break\r\n            raise\r\n        if r is None:\r\n            assert done_event.is_set()\r\n            return\r\n        elif done_event.is_set():\r\n            # Haven't seen the final signal yet. Keep getting until None.\r\n            continue\r\n        elif isinstance(r[1], ExceptionWrapper):\r\n            out_queue.put(r)\r\n        else:\r\n            idx, batch = r\r\n            try:\r\n                batch = pin_memory_batch(batch)\r\n            except Exception:\r\n                out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\r\n            else:\r\n                out_queue.put((idx, batch))\r\n```\r\nThe main thread is stuck at `_get_batch`:\r\n```\r\n    def _get_batch(self):\r\n        # In the non-timeout case, worker exit is covered by SIGCHLD handler.\r\n        # But if `pin_memory=True`, we still need account for the possibility\r\n        # that `pin_memory_thread` dies.\r\n        if self.timeout > 0:\r\n            try:\r\n                return self.data_queue.get(timeout=self.timeout)\r\n            except queue.Empty:\r\n                raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\r\n        elif self.pin_memory:\r\n            while self.pin_memory_thread.is_alive():\r\n                try:\r\n                    return self.data_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\r\n                except queue.Empty:\r\n                    continue\r\n            else:\r\n                # while condition is false, i.e., pin_memory_thread died.\r\n                raise RuntimeError('Pin memory thread exited unexpectedly')\r\n            # In this case, `self.data_queue` is a `queue.Queue`,. But we don't\r\n            # need to call `.task_done()` because we don't use `.join()`.\r\n        else:\r\n            return self.data_queue.get()\r\n```\r\nBasically if `self.worker_result_queue` is empty, the dataloader will get into a deadlock state.\r\n\r\nMy training script destructs DataLoader iterator object before all samples are drained, which means the shutdown signal of data workers can happen while works are in the process of fetching samples. I'm not sure if this has any side effect to the dataloader, may be you can shed some light on this."}