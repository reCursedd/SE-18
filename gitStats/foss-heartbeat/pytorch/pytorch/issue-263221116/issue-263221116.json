{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2992", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2992/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2992/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2992/events", "html_url": "https://github.com/pytorch/pytorch/issues/2992", "id": 263221116, "node_id": "MDU6SXNzdWUyNjMyMjExMTY=", "number": 2992, "title": "DataParallel Gather works only with iterable outputs", "user": {"login": "matteorr", "id": 3580181, "node_id": "MDQ6VXNlcjM1ODAxODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3580181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matteorr", "html_url": "https://github.com/matteorr", "followers_url": "https://api.github.com/users/matteorr/followers", "following_url": "https://api.github.com/users/matteorr/following{/other_user}", "gists_url": "https://api.github.com/users/matteorr/gists{/gist_id}", "starred_url": "https://api.github.com/users/matteorr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matteorr/subscriptions", "organizations_url": "https://api.github.com/users/matteorr/orgs", "repos_url": "https://api.github.com/users/matteorr/repos", "events_url": "https://api.github.com/users/matteorr/events{/privacy}", "received_events_url": "https://api.github.com/users/matteorr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-05T18:25:09Z", "updated_at": "2018-04-02T13:16:44Z", "closed_at": "2018-04-02T13:16:44Z", "author_association": "NONE", "body_html": "<p>I'm running a network across multiple GPUs and pass the input data through a dictionary. The Variables stored as items of the input dictionary are properly scattered across the batch dimension, and the forward pass terminates correctly.</p>\n<p>However, when returning the output as a dictionary I get the following runtime error:</p>\n<pre><code>torch/nn/parallel/scatter_gather.pyc in gather_map(outputs)\n     47         if out is None:\n     48             return None\n---&gt; 49         return type(out)(map(gather_map, zip(*outputs)))\n     50     return gather_map(outputs)\ntorch/nn/parallel/scatter_gather.pyc in gather_map(outputs)\n     43     def gather_map(outputs):\n     44         out = outputs[0]\n---&gt; 45         if isinstance(out, Variable):\n     46             return Gather(target_device, dim=dim)(*outputs)\n     47         if out is None:\nRuntimeError: maximum recursion depth exceeded in __instancecheck__\n</code></pre>\n<p>The reason is that the function <code>gather_map</code> in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L46-L52\">scatter_gather.py</a> only supports Variables or iterables of variables as its input.</p>\n<p>However, <code>scatter_map</code> in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L21-L23\">scatter_gather.py</a> also supports dictionaries.</p>\n<p>Is there a reason for this discrepancy? Would it be useful if I made a pull request and added this functionality?</p>\n<p>I am implementing a network with multiple sub-networks whose outputs may or may not be computed (based on a config file) and it would be useful to be able to pass all of them in a compact way out of the data parallel wrapper.</p>\n<p><strong>SNIPPET REPLICATING ERROR:</strong></p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass MyModel(nn.Module):\n\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.block1 = nn.Linear(10, 20)\n        self.block2 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return x\n\nclass MyModelDictInput(nn.Module):\n\n    def __init__(self):\n        super(MyModelDictInput, self).__init__()\n        self.block1 = nn.Linear(10, 20)\n        self.block2 = nn.Linear(20, 20)\n\n    def forward(self, d):\n        x = d['an_input']\n        x = self.block1(x)\n        x = self.block2(x)\n        return x\n\nclass MyModelDictOutput(nn.Module):\n\n    def __init__(self):\n        super(MyModelDictOutput, self).__init__()\n        self.block1 = nn.Linear(10, 20)\n        self.block2 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        d = dict()\n        d['an_output'] = x\n        return d\n\n# create random input\ni = Variable(torch.rand((4,10)))\nd = {'an_input':i}\n\n# example 1:\nprint('input is a Variable, output is a Variable')\nnet = nn.DataParallel(MyModel()).cuda()\no   = net.forward(i)\nprint(o)\n\n# example 2:\nprint('input is a dict, output is a Variable')\nnet = nn.DataParallel(MyModelDictInput()).cuda()\no   = net.forward(d)\nprint(o)\n\n# example 3:\nprint('input is a Variable, output is a dict')\nnet = nn.DataParallel(MyModelDictOutput()).cuda()\no   = net.forward(i)\nprint(o)\n</code></pre>", "body_text": "I'm running a network across multiple GPUs and pass the input data through a dictionary. The Variables stored as items of the input dictionary are properly scattered across the batch dimension, and the forward pass terminates correctly.\nHowever, when returning the output as a dictionary I get the following runtime error:\ntorch/nn/parallel/scatter_gather.pyc in gather_map(outputs)\n     47         if out is None:\n     48             return None\n---> 49         return type(out)(map(gather_map, zip(*outputs)))\n     50     return gather_map(outputs)\ntorch/nn/parallel/scatter_gather.pyc in gather_map(outputs)\n     43     def gather_map(outputs):\n     44         out = outputs[0]\n---> 45         if isinstance(out, Variable):\n     46             return Gather(target_device, dim=dim)(*outputs)\n     47         if out is None:\nRuntimeError: maximum recursion depth exceeded in __instancecheck__\n\nThe reason is that the function gather_map in scatter_gather.py only supports Variables or iterables of variables as its input.\nHowever, scatter_map in scatter_gather.py also supports dictionaries.\nIs there a reason for this discrepancy? Would it be useful if I made a pull request and added this functionality?\nI am implementing a network with multiple sub-networks whose outputs may or may not be computed (based on a config file) and it would be useful to be able to pass all of them in a compact way out of the data parallel wrapper.\nSNIPPET REPLICATING ERROR:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass MyModel(nn.Module):\n\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.block1 = nn.Linear(10, 20)\n        self.block2 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return x\n\nclass MyModelDictInput(nn.Module):\n\n    def __init__(self):\n        super(MyModelDictInput, self).__init__()\n        self.block1 = nn.Linear(10, 20)\n        self.block2 = nn.Linear(20, 20)\n\n    def forward(self, d):\n        x = d['an_input']\n        x = self.block1(x)\n        x = self.block2(x)\n        return x\n\nclass MyModelDictOutput(nn.Module):\n\n    def __init__(self):\n        super(MyModelDictOutput, self).__init__()\n        self.block1 = nn.Linear(10, 20)\n        self.block2 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        d = dict()\n        d['an_output'] = x\n        return d\n\n# create random input\ni = Variable(torch.rand((4,10)))\nd = {'an_input':i}\n\n# example 1:\nprint('input is a Variable, output is a Variable')\nnet = nn.DataParallel(MyModel()).cuda()\no   = net.forward(i)\nprint(o)\n\n# example 2:\nprint('input is a dict, output is a Variable')\nnet = nn.DataParallel(MyModelDictInput()).cuda()\no   = net.forward(d)\nprint(o)\n\n# example 3:\nprint('input is a Variable, output is a dict')\nnet = nn.DataParallel(MyModelDictOutput()).cuda()\no   = net.forward(i)\nprint(o)", "body": "I'm running a network across multiple GPUs and pass the input data through a dictionary. The Variables stored as items of the input dictionary are properly scattered across the batch dimension, and the forward pass terminates correctly.\r\n\r\nHowever, when returning the output as a dictionary I get the following runtime error:\r\n```\r\ntorch/nn/parallel/scatter_gather.pyc in gather_map(outputs)\r\n     47         if out is None:\r\n     48             return None\r\n---> 49         return type(out)(map(gather_map, zip(*outputs)))\r\n     50     return gather_map(outputs)\r\ntorch/nn/parallel/scatter_gather.pyc in gather_map(outputs)\r\n     43     def gather_map(outputs):\r\n     44         out = outputs[0]\r\n---> 45         if isinstance(out, Variable):\r\n     46             return Gather(target_device, dim=dim)(*outputs)\r\n     47         if out is None:\r\nRuntimeError: maximum recursion depth exceeded in __instancecheck__\r\n```\r\n\r\nThe reason is that the function `gather_map` in [scatter_gather.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L46-L52) only supports Variables or iterables of variables as its input.\r\n\r\nHowever, `scatter_map` in [scatter_gather.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L21-L23) also supports dictionaries.\r\n\r\nIs there a reason for this discrepancy? Would it be useful if I made a pull request and added this functionality?\r\n\r\nI am implementing a network with multiple sub-networks whose outputs may or may not be computed (based on a config file) and it would be useful to be able to pass all of them in a compact way out of the data parallel wrapper.\r\n\r\n**SNIPPET REPLICATING ERROR:**\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass MyModel(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.block1 = nn.Linear(10, 20)\r\n        self.block2 = nn.Linear(20, 20)\r\n\r\n    def forward(self, x):\r\n        x = self.block1(x)\r\n        x = self.block2(x)\r\n        return x\r\n\r\nclass MyModelDictInput(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MyModelDictInput, self).__init__()\r\n        self.block1 = nn.Linear(10, 20)\r\n        self.block2 = nn.Linear(20, 20)\r\n\r\n    def forward(self, d):\r\n        x = d['an_input']\r\n        x = self.block1(x)\r\n        x = self.block2(x)\r\n        return x\r\n\r\nclass MyModelDictOutput(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MyModelDictOutput, self).__init__()\r\n        self.block1 = nn.Linear(10, 20)\r\n        self.block2 = nn.Linear(20, 20)\r\n\r\n    def forward(self, x):\r\n        x = self.block1(x)\r\n        x = self.block2(x)\r\n        d = dict()\r\n        d['an_output'] = x\r\n        return d\r\n\r\n# create random input\r\ni = Variable(torch.rand((4,10)))\r\nd = {'an_input':i}\r\n\r\n# example 1:\r\nprint('input is a Variable, output is a Variable')\r\nnet = nn.DataParallel(MyModel()).cuda()\r\no   = net.forward(i)\r\nprint(o)\r\n\r\n# example 2:\r\nprint('input is a dict, output is a Variable')\r\nnet = nn.DataParallel(MyModelDictInput()).cuda()\r\no   = net.forward(d)\r\nprint(o)\r\n\r\n# example 3:\r\nprint('input is a Variable, output is a dict')\r\nnet = nn.DataParallel(MyModelDictOutput()).cuda()\r\no   = net.forward(i)\r\nprint(o)\r\n```"}