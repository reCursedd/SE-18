{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/327882651", "html_url": "https://github.com/pytorch/pytorch/pull/2622#issuecomment-327882651", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2622", "id": 327882651, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzg4MjY1MQ==", "user": {"login": "GuillaumeLeclerc", "id": 2017051, "node_id": "MDQ6VXNlcjIwMTcwNTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2017051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GuillaumeLeclerc", "html_url": "https://github.com/GuillaumeLeclerc", "followers_url": "https://api.github.com/users/GuillaumeLeclerc/followers", "following_url": "https://api.github.com/users/GuillaumeLeclerc/following{/other_user}", "gists_url": "https://api.github.com/users/GuillaumeLeclerc/gists{/gist_id}", "starred_url": "https://api.github.com/users/GuillaumeLeclerc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GuillaumeLeclerc/subscriptions", "organizations_url": "https://api.github.com/users/GuillaumeLeclerc/orgs", "repos_url": "https://api.github.com/users/GuillaumeLeclerc/repos", "events_url": "https://api.github.com/users/GuillaumeLeclerc/events{/privacy}", "received_events_url": "https://api.github.com/users/GuillaumeLeclerc/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-07T18:20:54Z", "updated_at": "2017-09-07T18:20:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> You are right. I tried using it on large datasets and I could not make it work because threads get locked in <a href=\"https://github.com/GuillaumeLeclerc/pytorch/blob/98677bc7ce9cefa6ff694db3cf7d7c241a843ea4/torch/utils/data/dataloader.py#L78\">here</a>. I don't know enough about python multiprocessing to guess how such a thing can happen. I will have to dig into it</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> That would be perfect, but you are right, guessing the size of the batch might be super complicated especially when elements are dictionaries or with custom collate functions. I will try to think about it. In a sense it's awesome that datasets and data loaders are so flexible but it makes things more complicated. We could make assumptions that each batch has the same size and get the size from the first batch and speed up upcoming ones. It would not work in some situations (eg, some RNNs). We could compute the first element and get the width of the tensors and it would be safe to allocate <code>width x batch_size</code> but that requires finishing the first element before allocating the memory and that would slow things.</p>", "body_text": "@colesbury You are right. I tried using it on large datasets and I could not make it work because threads get locked in here. I don't know enough about python multiprocessing to guess how such a thing can happen. I will have to dig into it\n@apaszke That would be perfect, but you are right, guessing the size of the batch might be super complicated especially when elements are dictionaries or with custom collate functions. I will try to think about it. In a sense it's awesome that datasets and data loaders are so flexible but it makes things more complicated. We could make assumptions that each batch has the same size and get the size from the first batch and speed up upcoming ones. It would not work in some situations (eg, some RNNs). We could compute the first element and get the width of the tensors and it would be safe to allocate width x batch_size but that requires finishing the first element before allocating the memory and that would slow things.", "body": "@colesbury You are right. I tried using it on large datasets and I could not make it work because threads get locked in [here](https://github.com/GuillaumeLeclerc/pytorch/blob/98677bc7ce9cefa6ff694db3cf7d7c241a843ea4/torch/utils/data/dataloader.py#L78). I don't know enough about python multiprocessing to guess how such a thing can happen. I will have to dig into it\r\n\r\n@apaszke That would be perfect, but you are right, guessing the size of the batch might be super complicated especially when elements are dictionaries or with custom collate functions. I will try to think about it. In a sense it's awesome that datasets and data loaders are so flexible but it makes things more complicated. We could make assumptions that each batch has the same size and get the size from the first batch and speed up upcoming ones. It would not work in some situations (eg, some RNNs). We could compute the first element and get the width of the tensors and it would be safe to allocate `width x batch_size` but that requires finishing the first element before allocating the memory and that would slow things."}