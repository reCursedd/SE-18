{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2622", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2622/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2622/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2622/events", "html_url": "https://github.com/pytorch/pytorch/pull/2622", "id": 255167302, "node_id": "MDExOlB1bGxSZXF1ZXN0MTM5MjQyNTMw", "number": 2622, "title": "Dynamically scheduled dataloader", "user": {"login": "GuillaumeLeclerc", "id": 2017051, "node_id": "MDQ6VXNlcjIwMTcwNTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2017051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GuillaumeLeclerc", "html_url": "https://github.com/GuillaumeLeclerc", "followers_url": "https://api.github.com/users/GuillaumeLeclerc/followers", "following_url": "https://api.github.com/users/GuillaumeLeclerc/following{/other_user}", "gists_url": "https://api.github.com/users/GuillaumeLeclerc/gists{/gist_id}", "starred_url": "https://api.github.com/users/GuillaumeLeclerc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GuillaumeLeclerc/subscriptions", "organizations_url": "https://api.github.com/users/GuillaumeLeclerc/orgs", "repos_url": "https://api.github.com/users/GuillaumeLeclerc/repos", "events_url": "https://api.github.com/users/GuillaumeLeclerc/events{/privacy}", "received_events_url": "https://api.github.com/users/GuillaumeLeclerc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-09-05T05:58:09Z", "updated_at": "2018-08-14T22:49:39Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2622", "html_url": "https://github.com/pytorch/pytorch/pull/2622", "diff_url": "https://github.com/pytorch/pytorch/pull/2622.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2622.patch"}, "body_html": "<p>Hello,</p>\n<p>After using pytorch for a while I realized that the dataloader could be improved. (I saw few things on the discuss <a href=\"https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/3\" rel=\"nofollow\">forum</a>).</p>\n<h1>The current architecture</h1>\n<p>Here is what I understand from the current architecture of the multiprocess dataloader.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2017051/30046195-d61eda48-91d6-11e7-98a1-ae4afeee22bc.png\"><img src=\"https://user-images.githubusercontent.com/2017051/30046195-d61eda48-91d6-11e7-98a1-ae4afeee22bc.png\" alt=\"existing architecture\" style=\"max-width:100%;\"></a></p>\n<ol>\n<li>The sample generate batches</li>\n<li>Batches are put in a queue</li>\n<li>A Worker take a batch, process it, collate and put the batch in another queue</li>\n<li>Main thread take processed batch from the output queue and reorder them</li>\n</ol>\n<h2>Disadvantages</h2>\n<ul>\n<li>Since an entire batch is assigned to a single worker, the first batch will not be outputed before the first worker process <code>batch_size</code> elements. Adding more workers will not reduce this starting time.</li>\n<li>The amount of memory is proportinal to the number of workers (<code>2 x num_workers</code> batches are primed).</li>\n<li>If some baches takes longer than the others, workers might be left without work while the main thread is wating for another.</li>\n<li>The computation power is not focused on the next batch, even if the main thread is starving.</li>\n</ul>\n<h1>Suggested Architecture</h1>\n<p>The key idea is to have an dataset element as a working unit instead of a batch. The second key idea is to use a circular buffer of k lists (like double/triple buffering in computer graphics application)</p>\n<h2>Summary</h2>\n<h3>Step 1</h3>\n<p>The main thread append to a first queue all the elements in the batch, <code>num_workers - 1</code> makers <code>Next</code> and one maker <code>Collate</code>.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2017051/30046421-a8295cf6-91d8-11e7-989e-491a9e8b791a.png\"><img src=\"https://user-images.githubusercontent.com/2017051/30046421-a8295cf6-91d8-11e7-989e-491a9e8b791a.png\" alt=\"step 1\" style=\"max-width:100%;\"></a></p>\n<h3>Step 2</h3>\n<p>Workers start consuming the queue and retrieve the data from the dataset and put it in another queue the <code>collate_queue</code>. Elements might be reorderd during this phase. In the meantime the main thread start filling the queue the queue for the next batch (if the <code>preload_batches</code> parameter allows it).</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2017051/30046424-aabed9aa-91d8-11e7-8a49-0723f7a956fa.png\"><img src=\"https://user-images.githubusercontent.com/2017051/30046424-aabed9aa-91d8-11e7-8a49-0723f7a956fa.png\" alt=\"step 2\" style=\"max-width:100%;\"></a></p>\n<h3>Step 3</h3>\n<p>When workers reach the end of the queue they will start consuming <code>Next</code> markers. When it happens. It jumps from the current queue to the next one. The last remaining one will consume a <code>Collate</code>. The one who pick this one is will jump to the <code>collate_queue</code>. It consume the entire queue. Reorder elements in the batch and put the collated batch in the <code>data_queue</code>. In the meantime, if there are elements in the next queue, other workers start working on the next batch.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2017051/30046426-ad01970c-91d8-11e7-8af0-8d238a0bef6b.png\"><img src=\"https://user-images.githubusercontent.com/2017051/30046426-ad01970c-91d8-11e7-8af0-8d238a0bef6b.png\" alt=\"step 3\" style=\"max-width:100%;\"></a></p>\n<h3>Step 4</h3>\n<p>We continue the process for all batches. When the main_thread reach the last queue (the number of queue is <code>preload_batches + 2</code>), it waits until the first one is completely empty and reuse it. This way we don't allocate a queue per batch.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2017051/30046433-b0e69fd4-91d8-11e7-8188-101f6eef528f.png\"><img src=\"https://user-images.githubusercontent.com/2017051/30046433-b0e69fd4-91d8-11e7-8188-101f6eef528f.png\" alt=\"step 4\" style=\"max-width:100%;\"></a></p>\n<h2>Key points</h2>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> We limit the memory usage</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> All workers work in priority for the next batch</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Users can control the amount of prefetching</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> No matter what, we always use 100% of the CPU unless we have prefetched what the user asked</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> All existing test pass</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Elements are loaded in order. This is not the case because we prallelize every element in a batch</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> All Exception raised are reported. This is not the case because to mimic the behavior of the old architecture, only the first exception of the batch is raised.</li>\n</ul>\n<h1>Conclusion</h1>\n<ul>\n<li>What do you think ?</li>\n<li>Should it be a separate module or should it replace the existing <code>DataLoader</code> ?</li>\n<li>Would it be considered a breaking change since user code could assume that <code>__getitem__</code> is called in order ?</li>\n<li>Anything should be improved ?</li>\n</ul>\n<p>Thank you for your feedback</p>", "body_text": "Hello,\nAfter using pytorch for a while I realized that the dataloader could be improved. (I saw few things on the discuss forum).\nThe current architecture\nHere is what I understand from the current architecture of the multiprocess dataloader.\n\n\nThe sample generate batches\nBatches are put in a queue\nA Worker take a batch, process it, collate and put the batch in another queue\nMain thread take processed batch from the output queue and reorder them\n\nDisadvantages\n\nSince an entire batch is assigned to a single worker, the first batch will not be outputed before the first worker process batch_size elements. Adding more workers will not reduce this starting time.\nThe amount of memory is proportinal to the number of workers (2 x num_workers batches are primed).\nIf some baches takes longer than the others, workers might be left without work while the main thread is wating for another.\nThe computation power is not focused on the next batch, even if the main thread is starving.\n\nSuggested Architecture\nThe key idea is to have an dataset element as a working unit instead of a batch. The second key idea is to use a circular buffer of k lists (like double/triple buffering in computer graphics application)\nSummary\nStep 1\nThe main thread append to a first queue all the elements in the batch, num_workers - 1 makers Next and one maker Collate.\n\nStep 2\nWorkers start consuming the queue and retrieve the data from the dataset and put it in another queue the collate_queue. Elements might be reorderd during this phase. In the meantime the main thread start filling the queue the queue for the next batch (if the preload_batches parameter allows it).\n\nStep 3\nWhen workers reach the end of the queue they will start consuming Next markers. When it happens. It jumps from the current queue to the next one. The last remaining one will consume a Collate. The one who pick this one is will jump to the collate_queue. It consume the entire queue. Reorder elements in the batch and put the collated batch in the data_queue. In the meantime, if there are elements in the next queue, other workers start working on the next batch.\n\nStep 4\nWe continue the process for all batches. When the main_thread reach the last queue (the number of queue is preload_batches + 2), it waits until the first one is completely empty and reuse it. This way we don't allocate a queue per batch.\n\nKey points\n\n We limit the memory usage\n All workers work in priority for the next batch\n Users can control the amount of prefetching\n No matter what, we always use 100% of the CPU unless we have prefetched what the user asked\n All existing test pass\n Elements are loaded in order. This is not the case because we prallelize every element in a batch\n All Exception raised are reported. This is not the case because to mimic the behavior of the old architecture, only the first exception of the batch is raised.\n\nConclusion\n\nWhat do you think ?\nShould it be a separate module or should it replace the existing DataLoader ?\nWould it be considered a breaking change since user code could assume that __getitem__ is called in order ?\nAnything should be improved ?\n\nThank you for your feedback", "body": "Hello,\r\n\r\nAfter using pytorch for a while I realized that the dataloader could be improved. (I saw few things on the discuss [forum](https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/3)).\r\n\r\n# The current architecture\r\n\r\nHere is what I understand from the current architecture of the multiprocess dataloader.\r\n\r\n![existing architecture](https://user-images.githubusercontent.com/2017051/30046195-d61eda48-91d6-11e7-98a1-ae4afeee22bc.png)\r\n\r\n1. The sample generate batches\r\n2. Batches are put in a queue\r\n3. A Worker take a batch, process it, collate and put the batch in another queue\r\n4. Main thread take processed batch from the output queue and reorder them\r\n\r\n## Disadvantages\r\n\r\n- Since an entire batch is assigned to a single worker, the first batch will not be outputed before the first worker process `batch_size` elements. Adding more workers will not reduce this starting time.\r\n- The amount of memory is proportinal to the number of workers (`2 x num_workers` batches are primed).\r\n- If some baches takes longer than the others, workers might be left without work while the main thread is wating for another.\r\n- The computation power is not focused on the next batch, even if the main thread is starving.\r\n\r\n# Suggested Architecture\r\n\r\nThe key idea is to have an dataset element as a working unit instead of a batch. The second key idea is to use a circular buffer of k lists (like double/triple buffering in computer graphics application)\r\n\r\n## Summary\r\n\r\n### Step 1\r\n\r\nThe main thread append to a first queue all the elements in the batch, `num_workers - 1` makers `Next` and one maker `Collate`. \r\n![step 1](https://user-images.githubusercontent.com/2017051/30046421-a8295cf6-91d8-11e7-989e-491a9e8b791a.png)\r\n\r\n### Step 2\r\n\r\nWorkers start consuming the queue and retrieve the data from the dataset and put it in another queue the `collate_queue`. Elements might be reorderd during this phase. In the meantime the main thread start filling the queue the queue for the next batch (if the `preload_batches` parameter allows it).\r\n\r\n![step 2](https://user-images.githubusercontent.com/2017051/30046424-aabed9aa-91d8-11e7-8a49-0723f7a956fa.png)\r\n\r\n### Step 3 \r\n\r\nWhen workers reach the end of the queue they will start consuming `Next` markers. When it happens. It jumps from the current queue to the next one. The last remaining one will consume a `Collate`. The one who pick this one is will jump to the `collate_queue`. It consume the entire queue. Reorder elements in the batch and put the collated batch in the `data_queue`. In the meantime, if there are elements in the next queue, other workers start working on the next batch.\r\n\r\n![step 3](https://user-images.githubusercontent.com/2017051/30046426-ad01970c-91d8-11e7-8af0-8d238a0bef6b.png)\r\n\r\n### Step 4\r\n\r\nWe continue the process for all batches. When the main_thread reach the last queue (the number of queue is `preload_batches + 2`), it waits until the first one is completely empty and reuse it. This way we don't allocate a queue per batch.\r\n\r\n![step 4](https://user-images.githubusercontent.com/2017051/30046433-b0e69fd4-91d8-11e7-8188-101f6eef528f.png)\r\n\r\n## Key points\r\n\r\n- [x] We limit the memory usage\r\n- [x] All workers work in priority for the next batch\r\n- [x] Users can control the amount of prefetching\r\n- [x] No matter what, we always use 100% of the CPU unless we have prefetched what the user asked\r\n- [x] All existing test pass\r\n- [ ] Elements are loaded in order. This is not the case because we prallelize every element in a batch\r\n- [ ] All Exception raised are reported. This is not the case because to mimic the behavior of the old architecture, only the first exception of the batch is raised.\r\n\r\n# Conclusion\r\n\r\n- What do you think ?\r\n- Should it be a separate module or should it replace the existing `DataLoader` ?\r\n- Would it be considered a breaking change since user code could assume that `__getitem__` is called in order ?\r\n- Anything should be improved ?\r\n\r\nThank you for your feedback\r\n"}