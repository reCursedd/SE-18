{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394547015", "html_url": "https://github.com/pytorch/pytorch/issues/7359#issuecomment-394547015", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7359", "id": 394547015, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDU0NzAxNQ==", "user": {"login": "kris-singh", "id": 11256139, "node_id": "MDQ6VXNlcjExMjU2MTM5", "avatar_url": "https://avatars1.githubusercontent.com/u/11256139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kris-singh", "html_url": "https://github.com/kris-singh", "followers_url": "https://api.github.com/users/kris-singh/followers", "following_url": "https://api.github.com/users/kris-singh/following{/other_user}", "gists_url": "https://api.github.com/users/kris-singh/gists{/gist_id}", "starred_url": "https://api.github.com/users/kris-singh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kris-singh/subscriptions", "organizations_url": "https://api.github.com/users/kris-singh/orgs", "repos_url": "https://api.github.com/users/kris-singh/repos", "events_url": "https://api.github.com/users/kris-singh/events{/privacy}", "received_events_url": "https://api.github.com/users/kris-singh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T01:02:44Z", "updated_at": "2018-06-05T12:36:29Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a>  I think I figured out a way to loss based sampling. Here is the code.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets, transforms\n<span class=\"pl-k\">from</span> torch.utils.data.sampler <span class=\"pl-k\">import</span> Sampler\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> np.random.seed(1337)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.manual_seed(1337)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.cuda.manual_seed_all(1337)</span>\n\ndevice <span class=\"pl-k\">=</span> torch.device (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda:2<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> torch.cuda.is_available () <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RandomSampler</span>(<span class=\"pl-e\">Sampler</span>):\n    <span class=\"pl-s\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span>Samples elements randomly, without replacement.</span>\n<span class=\"pl-s\">    Arguments:</span>\n<span class=\"pl-s\">        data_source (Dataset): dataset to sample from</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">data_source</span>, <span class=\"pl-smi\">train_data</span>, <span class=\"pl-smi\">train_target</span>, <span class=\"pl-smi\">batch_size</span>):\n        <span class=\"pl-c1\">self</span>.model <span class=\"pl-k\">=</span> model\n        <span class=\"pl-c1\">self</span>.data_source <span class=\"pl-k\">=</span> data_source\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n        <span class=\"pl-c1\">self</span>.data <span class=\"pl-k\">=</span> train_data\n        <span class=\"pl-c1\">self</span>.data <span class=\"pl-k\">=</span> torch.unsqueeze(<span class=\"pl-c1\">self</span>.data, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.data.type(torch.cuda.FloatTensor)\n        <span class=\"pl-c1\">self</span>.target <span class=\"pl-k\">=</span> train_target\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_scores</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        output, feat <span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.model.forward(<span class=\"pl-c1\">self</span>.data)\n        criterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss (<span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        loss <span class=\"pl-k\">=</span> criterion(output, <span class=\"pl-c1\">self</span>.target)\n        <span class=\"pl-k\">return</span> loss\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__iter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        num_batches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span> (<span class=\"pl-c1\">self</span>.data_source) <span class=\"pl-k\">//</span> <span class=\"pl-c1\">self</span>.batch_size\n        <span class=\"pl-k\">while</span> num_batches <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n            scores <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.get_scores ()\n            sampled <span class=\"pl-k\">=</span> []\n            <span class=\"pl-k\">while</span> <span class=\"pl-c1\">len</span>(sampled) <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">self</span>.batch_size:\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(sampled) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                    sampled.append(torch.argmax(scores))\n                <span class=\"pl-k\">else</span>:\n                    sampled.append(torch.argmax(scores))\n            <span class=\"pl-k\">yield</span> sampled\n            num_batches <span class=\"pl-k\">-=</span><span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.data_source)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">BatchSampler</span>(<span class=\"pl-e\">Sampler</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">sampler</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">drop_last</span>):\n        <span class=\"pl-c1\">self</span>.sampler <span class=\"pl-k\">=</span> sampler\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n        <span class=\"pl-c1\">self</span>.drop_last <span class=\"pl-k\">=</span> drop_last\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__iter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        batch <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> _, idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">iter</span>(<span class=\"pl-c1\">self</span>.sampler)):\n            batch <span class=\"pl-k\">=</span> idx\n            <span class=\"pl-k\">yield</span> batch\n\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(batch) <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>.drop_last:\n            <span class=\"pl-k\">yield</span> batch\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.sampler) <span class=\"pl-k\">//</span> <span class=\"pl-c1\">self</span>.batch_size\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the Network</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span> (<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span> (Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span> ()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d (<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv2_drop <span class=\"pl-k\">=</span> nn.Dropout2d ()\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear (<span class=\"pl-c1\">320</span>, <span class=\"pl-c1\">50</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear (<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> F.relu (F.max_pool2d (<span class=\"pl-c1\">self</span>.conv1 (x), <span class=\"pl-c1\">2</span>))\n        x <span class=\"pl-k\">=</span> F.relu (F.max_pool2d (<span class=\"pl-c1\">self</span>.conv2_drop (<span class=\"pl-c1\">self</span>.conv2 (x)), <span class=\"pl-c1\">2</span>))\n        x <span class=\"pl-k\">=</span> x.view (<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">320</span>)\n        x <span class=\"pl-k\">=</span> F.relu (<span class=\"pl-c1\">self</span>.fc1 (x))\n        feat_x <span class=\"pl-k\">=</span> x\n        x <span class=\"pl-k\">=</span> F.dropout (x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.training)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc2 (x)\n        <span class=\"pl-k\">return</span> F.log_softmax (x, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>), feat_x\n\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train_normal</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">device</span>, <span class=\"pl-smi\">train_loader</span>, <span class=\"pl-smi\">optimizer</span>, <span class=\"pl-smi\">epoch</span>):\n    model.train ()\n    <span class=\"pl-k\">for</span> batch_idx, (data, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span> (train_loader):\n        data <span class=\"pl-k\">=</span> data.to (device)\n        target <span class=\"pl-k\">=</span> target.to (device)\n        optimizer.zero_grad ()\n        output, feat <span class=\"pl-k\">=</span> model (data)\n        loss <span class=\"pl-k\">=</span> F.nll_loss (output, target)\n        loss.backward ()\n        optimizer.step ()\n        <span class=\"pl-k\">if</span> batch_idx <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Train Epoch: <span class=\"pl-c1\">{}</span> [<span class=\"pl-c1\">{}</span>/<span class=\"pl-c1\">{}</span> (<span class=\"pl-c1\">{<span class=\"pl-k\">:.0f</span>}</span>%)]<span class=\"pl-cce\">\\t</span>Loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span><span class=\"pl-pds\">'</span></span>.format (\n                epoch, batch_idx <span class=\"pl-k\">*</span> <span class=\"pl-c1\">len</span> (data), <span class=\"pl-c1\">len</span> (train_loader.dataset),\n                       <span class=\"pl-c1\">100</span>. <span class=\"pl-k\">*</span> batch_idx <span class=\"pl-k\">/</span> <span class=\"pl-c1\">len</span> (train_loader), loss.item ()))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">device</span>, <span class=\"pl-smi\">test_loader</span>, <span class=\"pl-smi\">epoch</span>):\n    model.eval ()\n    test_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    correct <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">with</span> torch.no_grad ():\n        <span class=\"pl-k\">for</span> data, target <span class=\"pl-k\">in</span> test_loader:\n            data <span class=\"pl-k\">=</span> data.to (device)\n            target <span class=\"pl-k\">=</span> target.to (device)\n            output, _ <span class=\"pl-k\">=</span> model (data)\n            test_loss <span class=\"pl-k\">+=</span> F.nll_loss (output, target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>).item ()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> sum up batch loss</span>\n            pred <span class=\"pl-k\">=</span> output.max (<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keepdim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">1</span>]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> get the index of the max log-probability</span>\n            correct <span class=\"pl-k\">+=</span> pred.eq (target.view_as (pred)).sum ().item ()\n    test_loss <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">len</span> (test_loader.dataset)\n    <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span>Test set: Average loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span>, Accuracy: <span class=\"pl-c1\">{}</span>/<span class=\"pl-c1\">{}</span> (<span class=\"pl-c1\">{<span class=\"pl-k\">:.0f</span>}</span>%)<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>.format (\n        test_loss, correct, <span class=\"pl-c1\">len</span> (test_loader.dataset),\n        <span class=\"pl-c1\">100</span>. <span class=\"pl-k\">*</span> correct <span class=\"pl-k\">/</span> <span class=\"pl-c1\">len</span> (test_loader.dataset)))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    device <span class=\"pl-k\">=</span> torch.device (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda:0<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> torch.cuda.is_available () <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu<span class=\"pl-pds\">\"</span></span>)\n    train_batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\n    test_batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n    learning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.01</span>\n    momentum <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.5</span>\n    epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Read Dataset</span>\n    dataset <span class=\"pl-k\">=</span> datasets.MNIST (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/MNIST_data/<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                              <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose ([\n                                  transforms.ToTensor (),\n                                  transforms.Normalize ((<span class=\"pl-c1\">0.1307</span>,), (<span class=\"pl-c1\">0.3081</span>,))\n                              ]))\n\n    test_dataset <span class=\"pl-k\">=</span> datasets.MNIST (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/MNIST_data/<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                   <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose ([\n                                       transforms.ToTensor (),\n                                       transforms.Normalize ((<span class=\"pl-c1\">0.1307</span>,), (<span class=\"pl-c1\">0.3081</span>,))\n                                   ]))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Training loop</span>\n    model <span class=\"pl-k\">=</span> Net ().to (device)\n    train_data <span class=\"pl-k\">=</span> dataset.train_data\n    train_target <span class=\"pl-k\">=</span> dataset.train_labels\n    train_target <span class=\"pl-k\">=</span> train_target.to(device)\n    train_data <span class=\"pl-k\">=</span> train_data.to(device)\n    sampler <span class=\"pl-k\">=</span> RandomSampler (model, dataset, train_data, train_target, train_batch_size)\n    batch_sampler <span class=\"pl-k\">=</span> BatchSampler (sampler, train_batch_size, <span class=\"pl-c1\">True</span>)\n    train_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader (dataset, <span class=\"pl-v\">batch_sampler</span><span class=\"pl-k\">=</span>batch_sampler, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    test_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader (test_dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>test_batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    optimizer <span class=\"pl-k\">=</span> optim.SGD (model.parameters (), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>learning_rate, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span> (epochs):\n        train_normal (model, device, train_loader, optimizer, i)\n        test (model, device, test_loader, i)\n\nmain ()</pre></div>\n<p>Please let me know if there are any comments. Otherwise this can be closed.<br>\nAlso as a suggestion I think we can change BatchSampler in Pytorch as I have implemented it.<br>\nIt is much more generalisable, Sampler would need to implement generators now, which i feel is more intutive. I did not really understand the num_workers=0 part and how it would effect the sampler. Can you let me know if the present implemention is effected by it. Thanks.</p>", "body_text": "@apaszke @SsnL  I think I figured out a way to loss based sampling. Here is the code.\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data.sampler import Sampler\n# np.random.seed(1337)\n# torch.manual_seed(1337)\n# torch.cuda.manual_seed_all(1337)\n\ndevice = torch.device (\"cuda:2\" if torch.cuda.is_available () else \"cpu\")\n\n\nclass RandomSampler(Sampler):\n    r\"\"\"Samples elements randomly, without replacement.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, model, data_source, train_data, train_target, batch_size):\n        self.model = model\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.data = train_data\n        self.data = torch.unsqueeze(self.data, 1)\n        self.data = self.data.type(torch.cuda.FloatTensor)\n        self.target = train_target\n\n    def get_scores(self):\n        output, feat =self.model.forward(self.data)\n        criterion = nn.CrossEntropyLoss (reduce=False)\n        loss = criterion(output, self.target)\n        return loss\n\n    def __iter__(self):\n        num_batches = len (self.data_source) // self.batch_size\n        while num_batches > 0:\n            scores = self.get_scores ()\n            sampled = []\n            while len(sampled) < self.batch_size:\n                if len(sampled) == 0:\n                    sampled.append(torch.argmax(scores))\n                else:\n                    sampled.append(torch.argmax(scores))\n            yield sampled\n            num_batches -=1\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass BatchSampler(Sampler):\n\n    def __init__(self, sampler, batch_size, drop_last):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for _, idx in enumerate(iter(self.sampler)):\n            batch = idx\n            yield batch\n\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        return len(self.sampler) // self.batch_size\n\n\n# Create the Network\nclass Net (nn.Module):\n    def __init__(self):\n        super (Net, self).__init__ ()\n        self.conv1 = nn.Conv2d (1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d (10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d ()\n        self.fc1 = nn.Linear (320, 50)\n        self.fc2 = nn.Linear (50, 10)\n\n    def forward(self, x):\n        x = F.relu (F.max_pool2d (self.conv1 (x), 2))\n        x = F.relu (F.max_pool2d (self.conv2_drop (self.conv2 (x)), 2))\n        x = x.view (-1, 320)\n        x = F.relu (self.fc1 (x))\n        feat_x = x\n        x = F.dropout (x, training=self.training)\n        x = self.fc2 (x)\n        return F.log_softmax (x, dim=1), feat_x\n\n\n\ndef train_normal(model, device, train_loader, optimizer, epoch):\n    model.train ()\n    for batch_idx, (data, target) in enumerate (train_loader):\n        data = data.to (device)\n        target = target.to (device)\n        optimizer.zero_grad ()\n        output, feat = model (data)\n        loss = F.nll_loss (output, target)\n        loss.backward ()\n        optimizer.step ()\n        if batch_idx % 100 == 0:\n            print ('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format (\n                epoch, batch_idx * len (data), len (train_loader.dataset),\n                       100. * batch_idx / len (train_loader), loss.item ()))\n\n\ndef test(model, device, test_loader, epoch):\n    model.eval ()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad ():\n        for data, target in test_loader:\n            data = data.to (device)\n            target = target.to (device)\n            output, _ = model (data)\n            test_loss += F.nll_loss (output, target, size_average=False).item ()  # sum up batch loss\n            pred = output.max (1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq (target.view_as (pred)).sum ().item ()\n    test_loss /= len (test_loader.dataset)\n    print ('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format (\n        test_loss, correct, len (test_loader.dataset),\n        100. * correct / len (test_loader.dataset)))\n\n\ndef main():\n    device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n    train_batch_size = 64\n    test_batch_size = 1000\n    learning_rate = 0.01\n    momentum = 0.5\n    epochs = 10\n    # Read Dataset\n    dataset = datasets.MNIST (\"~/MNIST_data/\", train=True, download=False,\n                              transform=transforms.Compose ([\n                                  transforms.ToTensor (),\n                                  transforms.Normalize ((0.1307,), (0.3081,))\n                              ]))\n\n    test_dataset = datasets.MNIST (\"~/MNIST_data/\", train=False, download=False,\n                                   transform=transforms.Compose ([\n                                       transforms.ToTensor (),\n                                       transforms.Normalize ((0.1307,), (0.3081,))\n                                   ]))\n\n    # Training loop\n    model = Net ().to (device)\n    train_data = dataset.train_data\n    train_target = dataset.train_labels\n    train_target = train_target.to(device)\n    train_data = train_data.to(device)\n    sampler = RandomSampler (model, dataset, train_data, train_target, train_batch_size)\n    batch_sampler = BatchSampler (sampler, train_batch_size, True)\n    train_loader = torch.utils.data.DataLoader (dataset, batch_sampler=batch_sampler, num_workers=0)\n    test_loader = torch.utils.data.DataLoader (test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n    optimizer = optim.SGD (model.parameters (), lr=learning_rate, momentum=momentum)\n    for i in range (epochs):\n        train_normal (model, device, train_loader, optimizer, i)\n        test (model, device, test_loader, i)\n\nmain ()\nPlease let me know if there are any comments. Otherwise this can be closed.\nAlso as a suggestion I think we can change BatchSampler in Pytorch as I have implemented it.\nIt is much more generalisable, Sampler would need to implement generators now, which i feel is more intutive. I did not really understand the num_workers=0 part and how it would effect the sampler. Can you let me know if the present implemention is effected by it. Thanks.", "body": "@apaszke @SsnL  I think I figured out a way to loss based sampling. Here is the code.\r\n```python\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data.sampler import Sampler\r\n# np.random.seed(1337)\r\n# torch.manual_seed(1337)\r\n# torch.cuda.manual_seed_all(1337)\r\n\r\ndevice = torch.device (\"cuda:2\" if torch.cuda.is_available () else \"cpu\")\r\n\r\n\r\nclass RandomSampler(Sampler):\r\n    r\"\"\"Samples elements randomly, without replacement.\r\n    Arguments:\r\n        data_source (Dataset): dataset to sample from\r\n    \"\"\"\r\n\r\n    def __init__(self, model, data_source, train_data, train_target, batch_size):\r\n        self.model = model\r\n        self.data_source = data_source\r\n        self.batch_size = batch_size\r\n        self.data = train_data\r\n        self.data = torch.unsqueeze(self.data, 1)\r\n        self.data = self.data.type(torch.cuda.FloatTensor)\r\n        self.target = train_target\r\n\r\n    def get_scores(self):\r\n        output, feat =self.model.forward(self.data)\r\n        criterion = nn.CrossEntropyLoss (reduce=False)\r\n        loss = criterion(output, self.target)\r\n        return loss\r\n\r\n    def __iter__(self):\r\n        num_batches = len (self.data_source) // self.batch_size\r\n        while num_batches > 0:\r\n            scores = self.get_scores ()\r\n            sampled = []\r\n            while len(sampled) < self.batch_size:\r\n                if len(sampled) == 0:\r\n                    sampled.append(torch.argmax(scores))\r\n                else:\r\n                    sampled.append(torch.argmax(scores))\r\n            yield sampled\r\n            num_batches -=1\r\n\r\n    def __len__(self):\r\n        return len(self.data_source)\r\n\r\n\r\nclass BatchSampler(Sampler):\r\n\r\n    def __init__(self, sampler, batch_size, drop_last):\r\n        self.sampler = sampler\r\n        self.batch_size = batch_size\r\n        self.drop_last = drop_last\r\n\r\n    def __iter__(self):\r\n        batch = []\r\n        for _, idx in enumerate(iter(self.sampler)):\r\n            batch = idx\r\n            yield batch\r\n\r\n        if len(batch) > 0 and not self.drop_last:\r\n            yield batch\r\n\r\n    def __len__(self):\r\n        return len(self.sampler) // self.batch_size\r\n\r\n\r\n# Create the Network\r\nclass Net (nn.Module):\r\n    def __init__(self):\r\n        super (Net, self).__init__ ()\r\n        self.conv1 = nn.Conv2d (1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d (10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d ()\r\n        self.fc1 = nn.Linear (320, 50)\r\n        self.fc2 = nn.Linear (50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu (F.max_pool2d (self.conv1 (x), 2))\r\n        x = F.relu (F.max_pool2d (self.conv2_drop (self.conv2 (x)), 2))\r\n        x = x.view (-1, 320)\r\n        x = F.relu (self.fc1 (x))\r\n        feat_x = x\r\n        x = F.dropout (x, training=self.training)\r\n        x = self.fc2 (x)\r\n        return F.log_softmax (x, dim=1), feat_x\r\n\r\n\r\n\r\ndef train_normal(model, device, train_loader, optimizer, epoch):\r\n    model.train ()\r\n    for batch_idx, (data, target) in enumerate (train_loader):\r\n        data = data.to (device)\r\n        target = target.to (device)\r\n        optimizer.zero_grad ()\r\n        output, feat = model (data)\r\n        loss = F.nll_loss (output, target)\r\n        loss.backward ()\r\n        optimizer.step ()\r\n        if batch_idx % 100 == 0:\r\n            print ('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format (\r\n                epoch, batch_idx * len (data), len (train_loader.dataset),\r\n                       100. * batch_idx / len (train_loader), loss.item ()))\r\n\r\n\r\ndef test(model, device, test_loader, epoch):\r\n    model.eval ()\r\n    test_loss = 0\r\n    correct = 0\r\n    with torch.no_grad ():\r\n        for data, target in test_loader:\r\n            data = data.to (device)\r\n            target = target.to (device)\r\n            output, _ = model (data)\r\n            test_loss += F.nll_loss (output, target, size_average=False).item ()  # sum up batch loss\r\n            pred = output.max (1, keepdim=True)[1]  # get the index of the max log-probability\r\n            correct += pred.eq (target.view_as (pred)).sum ().item ()\r\n    test_loss /= len (test_loader.dataset)\r\n    print ('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format (\r\n        test_loss, correct, len (test_loader.dataset),\r\n        100. * correct / len (test_loader.dataset)))\r\n\r\n\r\ndef main():\r\n    device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\r\n    train_batch_size = 64\r\n    test_batch_size = 1000\r\n    learning_rate = 0.01\r\n    momentum = 0.5\r\n    epochs = 10\r\n    # Read Dataset\r\n    dataset = datasets.MNIST (\"~/MNIST_data/\", train=True, download=False,\r\n                              transform=transforms.Compose ([\r\n                                  transforms.ToTensor (),\r\n                                  transforms.Normalize ((0.1307,), (0.3081,))\r\n                              ]))\r\n\r\n    test_dataset = datasets.MNIST (\"~/MNIST_data/\", train=False, download=False,\r\n                                   transform=transforms.Compose ([\r\n                                       transforms.ToTensor (),\r\n                                       transforms.Normalize ((0.1307,), (0.3081,))\r\n                                   ]))\r\n\r\n    # Training loop\r\n    model = Net ().to (device)\r\n    train_data = dataset.train_data\r\n    train_target = dataset.train_labels\r\n    train_target = train_target.to(device)\r\n    train_data = train_data.to(device)\r\n    sampler = RandomSampler (model, dataset, train_data, train_target, train_batch_size)\r\n    batch_sampler = BatchSampler (sampler, train_batch_size, True)\r\n    train_loader = torch.utils.data.DataLoader (dataset, batch_sampler=batch_sampler, num_workers=0)\r\n    test_loader = torch.utils.data.DataLoader (test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\r\n    optimizer = optim.SGD (model.parameters (), lr=learning_rate, momentum=momentum)\r\n    for i in range (epochs):\r\n        train_normal (model, device, train_loader, optimizer, i)\r\n        test (model, device, test_loader, i)\r\n\r\nmain ()\r\n```\r\nPlease let me know if there are any comments. Otherwise this can be closed. \r\nAlso as a suggestion I think we can change BatchSampler in Pytorch as I have implemented it.\r\nIt is much more generalisable, Sampler would need to implement generators now, which i feel is more intutive. I did not really understand the num_workers=0 part and how it would effect the sampler. Can you let me know if the present implemention is effected by it. Thanks."}