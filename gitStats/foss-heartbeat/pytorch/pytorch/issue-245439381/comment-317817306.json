{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/317817306", "html_url": "https://github.com/pytorch/pytorch/issues/2201#issuecomment-317817306", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2201", "id": 317817306, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzgxNzMwNg==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-25T17:53:49Z", "updated_at": "2017-07-25T17:53:49Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi,<br>\nI have already seen this kind of behavior for cuda operations that are non deterministic.<br>\nI guess changing the number of threads here can also change the order of the floating point operations, leading to different values. Following these by gradient descent then leads to completely different models.<br>\nCan you check what is the order of magnitude of the difference for a single operation? If it is at the level of float precision then this might be it, if they are exactly the same (to the last bit) then there might be something else wrong.</p>", "body_text": "Hi,\nI have already seen this kind of behavior for cuda operations that are non deterministic.\nI guess changing the number of threads here can also change the order of the floating point operations, leading to different values. Following these by gradient descent then leads to completely different models.\nCan you check what is the order of magnitude of the difference for a single operation? If it is at the level of float precision then this might be it, if they are exactly the same (to the last bit) then there might be something else wrong.", "body": "Hi,\r\nI have already seen this kind of behavior for cuda operations that are non deterministic.\r\nI guess changing the number of threads here can also change the order of the floating point operations, leading to different values. Following these by gradient descent then leads to completely different models.\r\nCan you check what is the order of magnitude of the difference for a single operation? If it is at the level of float precision then this might be it, if they are exactly the same (to the last bit) then there might be something else wrong."}