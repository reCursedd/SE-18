{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2201", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2201/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2201/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2201/events", "html_url": "https://github.com/pytorch/pytorch/issues/2201", "id": 245439381, "node_id": "MDU6SXNzdWUyNDU0MzkzODE=", "number": 2201, "title": "Inconsistent results based on number of threads used", "user": {"login": "snapbug", "id": 945966, "node_id": "MDQ6VXNlcjk0NTk2Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/945966?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snapbug", "html_url": "https://github.com/snapbug", "followers_url": "https://api.github.com/users/snapbug/followers", "following_url": "https://api.github.com/users/snapbug/following{/other_user}", "gists_url": "https://api.github.com/users/snapbug/gists{/gist_id}", "starred_url": "https://api.github.com/users/snapbug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snapbug/subscriptions", "organizations_url": "https://api.github.com/users/snapbug/orgs", "repos_url": "https://api.github.com/users/snapbug/repos", "events_url": "https://api.github.com/users/snapbug/events{/privacy}", "received_events_url": "https://api.github.com/users/snapbug/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-25T15:24:03Z", "updated_at": "2017-07-27T13:48:27Z", "closed_at": "2017-07-27T13:48:27Z", "author_association": "NONE", "body_html": "<p>Our lab has been having a minor reproducibility crisis - training the same model on different computers was leading to different results, and the differences were substantial. After trying a large amount of things, long story short, setting <code>OMP_NUM_THREADS</code> and <code>MKL_NUM_THREADS</code> changes the results of the network, even when controlling random seeds etc.</p>\n<p>The effect is observable in the example networks from pytorch/example. To test, I spun up a t2.medium instance on ec2 running amazon linux (to be entirely sure that it wasn't some other environment issue), and installed miniconda. After which I installed pytorch and torch vision:</p>\n<pre><code>[ec2-user@ip-172-31-38-212 ~]$ conda install pytorch torchvision -c soumith\nFetching package metadata ...........\nSolving package specifications: .\nWarning: 2 possible package resolutions (only showing differing packages):\n  - soumith::pytorch-0.1.12-py360_2cu75\n  - soumith::pytorch-0.1.12-py36_2cu75\n\nPackage plan for installation in environment /home/ec2-user/miniconda3:\n\nThe following NEW packages will be INSTALLED:\n\n    freetype:    2.5.5-2\n    jbig:        2.1-0\n    jpeg:        9b-0\n    libgcc:      5.2.0-0\n    libpng:      1.6.27-0\n    libtiff:     4.0.6-3\n    mkl:         2017.0.3-0\n    numpy:       1.13.1-py36_0\n    olefile:     0.44-py36_0\n    pillow:      4.2.1-py36_0\n    pytorch:     0.1.12-py360_2cu75 soumith\n    torchvision: 0.1.8-py36_2       soumith\n\nThe following packages will be UPDATED:\n\n    conda:       4.3.14-py36_0              --&gt; 4.3.22-py36_0\n</code></pre>\n<p>I then cloned the pytorch/examples repository:</p>\n<pre><code>[ec2-user@ip-172-31-38-212 ~]$ cd examples\n[ec2-user@ip-172-31-38-212 examples]$ git rev-parse HEAD\n0722b2f4f73a220109dd2c0681d37df25f4a4116\n</code></pre>\n<p>From there I ran the mnist example:</p>\n<pre><code>[ec2-user@ip-172-31-38-212 examples]$ cd mnist\n[ec2-user@ip-172-31-38-212 mnist]$ python main.py --epochs 1 --no-cuda\n# output omitted\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.338396\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.666101\n\nTest set: Average loss: 0.1978, Accuracy: 9436/10000 (94%)\n</code></pre>\n<p>I then re-ran the example, having set <code>OMP_NUM_THREADS</code> and <code>MKL_NUM_THREADS</code>:</p>\n<pre><code>[ec2-user@ip-172-31-38-212 mnist]$ export OMP_NUM_THREADS=1\n[ec2-user@ip-172-31-38-212 mnist]$ export MKL_NUM_THREADS=1\n[ec2-user@ip-172-31-38-212 mnist]$ python main.py --epochs 1 --no-cuda\n# output omitted\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.329742\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.654222\n\nTest set: Average loss: 0.2003, Accuracy: 9438/10000 (94%)\n</code></pre>\n<p>And the loss/accuracy are different after training.</p>\n<p>In this example the differences are small, but in the network we're running the differences have been quite large, covering a range of the last few years of progress on our problem. And really it shouldn't matter how many threads are being run, the resulting model should be the same for the same network with the same seeds.</p>", "body_text": "Our lab has been having a minor reproducibility crisis - training the same model on different computers was leading to different results, and the differences were substantial. After trying a large amount of things, long story short, setting OMP_NUM_THREADS and MKL_NUM_THREADS changes the results of the network, even when controlling random seeds etc.\nThe effect is observable in the example networks from pytorch/example. To test, I spun up a t2.medium instance on ec2 running amazon linux (to be entirely sure that it wasn't some other environment issue), and installed miniconda. After which I installed pytorch and torch vision:\n[ec2-user@ip-172-31-38-212 ~]$ conda install pytorch torchvision -c soumith\nFetching package metadata ...........\nSolving package specifications: .\nWarning: 2 possible package resolutions (only showing differing packages):\n  - soumith::pytorch-0.1.12-py360_2cu75\n  - soumith::pytorch-0.1.12-py36_2cu75\n\nPackage plan for installation in environment /home/ec2-user/miniconda3:\n\nThe following NEW packages will be INSTALLED:\n\n    freetype:    2.5.5-2\n    jbig:        2.1-0\n    jpeg:        9b-0\n    libgcc:      5.2.0-0\n    libpng:      1.6.27-0\n    libtiff:     4.0.6-3\n    mkl:         2017.0.3-0\n    numpy:       1.13.1-py36_0\n    olefile:     0.44-py36_0\n    pillow:      4.2.1-py36_0\n    pytorch:     0.1.12-py360_2cu75 soumith\n    torchvision: 0.1.8-py36_2       soumith\n\nThe following packages will be UPDATED:\n\n    conda:       4.3.14-py36_0              --> 4.3.22-py36_0\n\nI then cloned the pytorch/examples repository:\n[ec2-user@ip-172-31-38-212 ~]$ cd examples\n[ec2-user@ip-172-31-38-212 examples]$ git rev-parse HEAD\n0722b2f4f73a220109dd2c0681d37df25f4a4116\n\nFrom there I ran the mnist example:\n[ec2-user@ip-172-31-38-212 examples]$ cd mnist\n[ec2-user@ip-172-31-38-212 mnist]$ python main.py --epochs 1 --no-cuda\n# output omitted\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.338396\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.666101\n\nTest set: Average loss: 0.1978, Accuracy: 9436/10000 (94%)\n\nI then re-ran the example, having set OMP_NUM_THREADS and MKL_NUM_THREADS:\n[ec2-user@ip-172-31-38-212 mnist]$ export OMP_NUM_THREADS=1\n[ec2-user@ip-172-31-38-212 mnist]$ export MKL_NUM_THREADS=1\n[ec2-user@ip-172-31-38-212 mnist]$ python main.py --epochs 1 --no-cuda\n# output omitted\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.329742\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.654222\n\nTest set: Average loss: 0.2003, Accuracy: 9438/10000 (94%)\n\nAnd the loss/accuracy are different after training.\nIn this example the differences are small, but in the network we're running the differences have been quite large, covering a range of the last few years of progress on our problem. And really it shouldn't matter how many threads are being run, the resulting model should be the same for the same network with the same seeds.", "body": "Our lab has been having a minor reproducibility crisis - training the same model on different computers was leading to different results, and the differences were substantial. After trying a large amount of things, long story short, setting `OMP_NUM_THREADS` and `MKL_NUM_THREADS` changes the results of the network, even when controlling random seeds etc.\r\n\r\nThe effect is observable in the example networks from pytorch/example. To test, I spun up a t2.medium instance on ec2 running amazon linux (to be entirely sure that it wasn't some other environment issue), and installed miniconda. After which I installed pytorch and torch vision:\r\n\r\n```\r\n[ec2-user@ip-172-31-38-212 ~]$ conda install pytorch torchvision -c soumith\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\nWarning: 2 possible package resolutions (only showing differing packages):\r\n  - soumith::pytorch-0.1.12-py360_2cu75\r\n  - soumith::pytorch-0.1.12-py36_2cu75\r\n\r\nPackage plan for installation in environment /home/ec2-user/miniconda3:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    freetype:    2.5.5-2\r\n    jbig:        2.1-0\r\n    jpeg:        9b-0\r\n    libgcc:      5.2.0-0\r\n    libpng:      1.6.27-0\r\n    libtiff:     4.0.6-3\r\n    mkl:         2017.0.3-0\r\n    numpy:       1.13.1-py36_0\r\n    olefile:     0.44-py36_0\r\n    pillow:      4.2.1-py36_0\r\n    pytorch:     0.1.12-py360_2cu75 soumith\r\n    torchvision: 0.1.8-py36_2       soumith\r\n\r\nThe following packages will be UPDATED:\r\n\r\n    conda:       4.3.14-py36_0              --> 4.3.22-py36_0\r\n```\r\n\r\nI then cloned the pytorch/examples repository:\r\n\r\n```\r\n[ec2-user@ip-172-31-38-212 ~]$ cd examples\r\n[ec2-user@ip-172-31-38-212 examples]$ git rev-parse HEAD\r\n0722b2f4f73a220109dd2c0681d37df25f4a4116\r\n```\r\n\r\nFrom there I ran the mnist example:\r\n\r\n```\r\n[ec2-user@ip-172-31-38-212 examples]$ cd mnist\r\n[ec2-user@ip-172-31-38-212 mnist]$ python main.py --epochs 1 --no-cuda\r\n# output omitted\r\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.338396\r\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.666101\r\n\r\nTest set: Average loss: 0.1978, Accuracy: 9436/10000 (94%)\r\n```\r\n\r\nI then re-ran the example, having set `OMP_NUM_THREADS` and `MKL_NUM_THREADS`:\r\n\r\n```\r\n[ec2-user@ip-172-31-38-212 mnist]$ export OMP_NUM_THREADS=1\r\n[ec2-user@ip-172-31-38-212 mnist]$ export MKL_NUM_THREADS=1\r\n[ec2-user@ip-172-31-38-212 mnist]$ python main.py --epochs 1 --no-cuda\r\n# output omitted\r\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.329742\r\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.654222\r\n\r\nTest set: Average loss: 0.2003, Accuracy: 9438/10000 (94%)\r\n```\r\n\r\nAnd the loss/accuracy are different after training.\r\n\r\nIn this example the differences are small, but in the network we're running the differences have been quite large, covering a range of the last few years of progress on our problem. And really it shouldn't matter how many threads are being run, the resulting model should be the same for the same network with the same seeds."}