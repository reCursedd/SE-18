{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3578", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3578/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3578/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3578/events", "html_url": "https://github.com/pytorch/pytorch/issues/3578", "id": 272376824, "node_id": "MDU6SXNzdWUyNzIzNzY4MjQ=", "number": 3578, "title": "Move PyTorch Convolution into ATen", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-08T22:43:15Z", "updated_at": "2018-01-23T20:35:11Z", "closed_at": "2018-01-23T20:35:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>ATen does have a THNN binding for convolution, but PyTorch supports a far richer interface to convolution that hooks up to CuDNN and nnpack. We should move this code into ATen.</p>\n<p>Task list:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Support computing derivatives for multiple inputs simultaneously</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Do warmup porting prelu to ATen, which has many similar characteristics to convolution</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Teach ATen build system to link against CuDNN</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Port <code>torch/csrc/cudnn</code> to use ATen tensors instead of THVoidTensor</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Restructure <code>torch/csrc/cudnn</code> so that it stratifies types and implementations, so it can be included from core ATen headers</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Redesign the API of cudnn so that it more closely matches THNN's (e.g., drop THCState parameter, reorder arguments, get rid of Convolution)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Rename the THNN binding from conv2d to conv2d_thnn</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Implement ATen backend functions for cudnn and nnpack; these skip backend selection and always use their respective backend.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Implement a native ATen function to dispatch to an implementation appropriately</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Teach ATen build system to link against nnpack</li>\n</ul>\n<p>Some design flex:</p>\n<ul>\n<li>The old PyTorch cudnn library took the CuDNN handle in as arguments; at all call sites, these CuDNN handles were retrieved from a global, mutex protected hash. Because I'm going to expose a generic <code>conv</code> method which is backend agnostic, my current plan is to NOT expose any ATen functions which take explicit CuDNN handles. (<del>This simplifies codegen, since I don't have to teach the code generator to automatically insert code to fill in these arguments on the backend functions.</del> Blah I should probably teach the codegen to do this properly.)</li>\n<li>The old cudnn Convolution interface had a \"Convolution\" struct which it allocated on the heap and then reused on subsequent invocations, saving us from having to rewrite the descriptors each time. I've undone this optimization because it's a bit annoying to actually achieve this.</li>\n</ul>", "body_text": "ATen does have a THNN binding for convolution, but PyTorch supports a far richer interface to convolution that hooks up to CuDNN and nnpack. We should move this code into ATen.\nTask list:\n\n Support computing derivatives for multiple inputs simultaneously\n Do warmup porting prelu to ATen, which has many similar characteristics to convolution\n Teach ATen build system to link against CuDNN\n Port torch/csrc/cudnn to use ATen tensors instead of THVoidTensor\n Restructure torch/csrc/cudnn so that it stratifies types and implementations, so it can be included from core ATen headers\n Redesign the API of cudnn so that it more closely matches THNN's (e.g., drop THCState parameter, reorder arguments, get rid of Convolution)\n Rename the THNN binding from conv2d to conv2d_thnn\n Implement ATen backend functions for cudnn and nnpack; these skip backend selection and always use their respective backend.\n Implement a native ATen function to dispatch to an implementation appropriately\n Teach ATen build system to link against nnpack\n\nSome design flex:\n\nThe old PyTorch cudnn library took the CuDNN handle in as arguments; at all call sites, these CuDNN handles were retrieved from a global, mutex protected hash. Because I'm going to expose a generic conv method which is backend agnostic, my current plan is to NOT expose any ATen functions which take explicit CuDNN handles. (This simplifies codegen, since I don't have to teach the code generator to automatically insert code to fill in these arguments on the backend functions. Blah I should probably teach the codegen to do this properly.)\nThe old cudnn Convolution interface had a \"Convolution\" struct which it allocated on the heap and then reused on subsequent invocations, saving us from having to rewrite the descriptors each time. I've undone this optimization because it's a bit annoying to actually achieve this.", "body": "ATen does have a THNN binding for convolution, but PyTorch supports a far richer interface to convolution that hooks up to CuDNN and nnpack. We should move this code into ATen.\r\n\r\nTask list:\r\n\r\n- [X] Support computing derivatives for multiple inputs simultaneously\r\n- [X] Do warmup porting prelu to ATen, which has many similar characteristics to convolution\r\n- [X] Teach ATen build system to link against CuDNN\r\n- [x] Port `torch/csrc/cudnn` to use ATen tensors instead of THVoidTensor\r\n- [x] Restructure `torch/csrc/cudnn` so that it stratifies types and implementations, so it can be included from core ATen headers\r\n- [x] Redesign the API of cudnn so that it more closely matches THNN's (e.g., drop THCState parameter, reorder arguments, get rid of Convolution)\r\n- [x] Rename the THNN binding from conv2d to conv2d_thnn\r\n- [x] Implement ATen backend functions for cudnn and nnpack; these skip backend selection and always use their respective backend.\r\n- [x] Implement a native ATen function to dispatch to an implementation appropriately\r\n- [x] Teach ATen build system to link against nnpack\r\n\r\nSome design flex:\r\n\r\n- The old PyTorch cudnn library took the CuDNN handle in as arguments; at all call sites, these CuDNN handles were retrieved from a global, mutex protected hash. Because I'm going to expose a generic `conv` method which is backend agnostic, my current plan is to NOT expose any ATen functions which take explicit CuDNN handles. (~~This simplifies codegen, since I don't have to teach the code generator to automatically insert code to fill in these arguments on the backend functions.~~ Blah I should probably teach the codegen to do this properly.)\r\n- The old cudnn Convolution interface had a \"Convolution\" struct which it allocated on the heap and then reused on subsequent invocations, saving us from having to rewrite the descriptors each time. I've undone this optimization because it's a bit annoying to actually achieve this."}