{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/350041662", "html_url": "https://github.com/pytorch/pytorch/issues/229#issuecomment-350041662", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/229", "id": 350041662, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDA0MTY2Mg==", "user": {"login": "sytrus-in-github", "id": 12224616, "node_id": "MDQ6VXNlcjEyMjI0NjE2", "avatar_url": "https://avatars0.githubusercontent.com/u/12224616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytrus-in-github", "html_url": "https://github.com/sytrus-in-github", "followers_url": "https://api.github.com/users/sytrus-in-github/followers", "following_url": "https://api.github.com/users/sytrus-in-github/following{/other_user}", "gists_url": "https://api.github.com/users/sytrus-in-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytrus-in-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytrus-in-github/subscriptions", "organizations_url": "https://api.github.com/users/sytrus-in-github/orgs", "repos_url": "https://api.github.com/users/sytrus-in-github/repos", "events_url": "https://api.github.com/users/sytrus-in-github/events{/privacy}", "received_events_url": "https://api.github.com/users/sytrus-in-github/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-07T17:42:24Z", "updated_at": "2018-05-23T20:29:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To make it work also with <code>torch.autograd.Variable</code>which does not have <code>new</code> attribute as well as avoiding the RuntimeError <code>Assertion 'ndim &lt;= MAX_ADVINDEX_CALC_DIMS' failed.</code> for input with dimension bigger than 6 in the cuda case, I had to change the code from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7605917\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dmarnerides\">@dmarnerides</a>  and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1103714\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wassname\">@wassname</a> as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> https://github.com/pytorch/pytorch/issues/229</span>\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">flip</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">dim</span>):\n    xsize <span class=\"pl-k\">=</span> x.size()\n    dim <span class=\"pl-k\">=</span> x.dim() <span class=\"pl-k\">+</span> dim <span class=\"pl-k\">if</span> dim <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">else</span> dim\n    x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">*</span>xsize[dim:])\n    x <span class=\"pl-k\">=</span> x.view(x.size(<span class=\"pl-c1\">0</span>), x.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)[:, <span class=\"pl-c1\">getattr</span>(torch.arange(x.size(<span class=\"pl-c1\">1</span>)<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, \n                      <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>), (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)[x.is_cuda])().long(), :]\n    <span class=\"pl-k\">return</span> x.view(xsize)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Code to test it with cuda Variable</span>\na <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">25</span>)]).view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).cuda())\n<span class=\"pl-c1\">print</span>(a)\n<span class=\"pl-c1\">print</span>(flip(a, <span class=\"pl-c1\">0</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or -6</span>\n<span class=\"pl-c1\">print</span>(flip(a, <span class=\"pl-c1\">1</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or -5</span>\n<span class=\"pl-c1\">print</span>(flip(a, <span class=\"pl-c1\">2</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or -4</span>\n<span class=\"pl-c1\">print</span>(flip(a, <span class=\"pl-c1\">3</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or -3</span>\n<span class=\"pl-c1\">print</span>(flip(a, <span class=\"pl-c1\">4</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or -2</span>\n<span class=\"pl-c1\">print</span>(flip(a, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or 5</span></pre></div>\n<p>A pytorch 0.4.0+ version:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> https://github.com/pytorch/pytorch/issues/229</span>\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">flip</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">dim</span>):\n    indices <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">slice</span>(<span class=\"pl-c1\">None</span>)] <span class=\"pl-k\">*</span> x.dim()\n    indices[dim] <span class=\"pl-k\">=</span> torch.arange(x.size(dim) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,\n                                <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>x.device)\n    <span class=\"pl-k\">return</span> x[<span class=\"pl-c1\">tuple</span>(indices)]</pre></div>\n<p>Of course, it would be nicer if we could use negative strides directly. This is a useful operation. At least I personally need it for a project that I am working on.</p>", "body_text": "To make it work also with torch.autograd.Variablewhich does not have new attribute as well as avoiding the RuntimeError Assertion 'ndim <= MAX_ADVINDEX_CALC_DIMS' failed. for input with dimension bigger than 6 in the cuda case, I had to change the code from @dmarnerides  and @wassname as follows:\n# https://github.com/pytorch/pytorch/issues/229\nimport torch\nfrom torch.autograd import Variable\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\n# Code to test it with cuda Variable\na = Variable(torch.Tensor([range(1, 25)]).view(1, 2, 3, 4, 1, 1).cuda())\nprint(a)\nprint(flip(a, 0)) # Or -6\nprint(flip(a, 1)) # Or -5\nprint(flip(a, 2)) # Or -4\nprint(flip(a, 3)) # Or -3\nprint(flip(a, 4)) # Or -2\nprint(flip(a, -1)) # Or 5\nA pytorch 0.4.0+ version:\n# https://github.com/pytorch/pytorch/issues/229\nimport torch\ndef flip(x, dim):\n    indices = [slice(None)] * x.dim()\n    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\n                                dtype=torch.long, device=x.device)\n    return x[tuple(indices)]\nOf course, it would be nicer if we could use negative strides directly. This is a useful operation. At least I personally need it for a project that I am working on.", "body": "To make it work also with `torch.autograd.Variable`which does not have `new` attribute as well as avoiding the RuntimeError `Assertion 'ndim <= MAX_ADVINDEX_CALC_DIMS' failed.` for input with dimension bigger than 6 in the cuda case, I had to change the code from @dmarnerides  and @wassname as follows:\r\n```python\r\n# https://github.com/pytorch/pytorch/issues/229\r\nimport torch\r\nfrom torch.autograd import Variable\r\ndef flip(x, dim):\r\n    xsize = x.size()\r\n    dim = x.dim() + dim if dim < 0 else dim\r\n    x = x.view(-1, *xsize[dim:])\r\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \r\n                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\r\n    return x.view(xsize)\r\n\r\n# Code to test it with cuda Variable\r\na = Variable(torch.Tensor([range(1, 25)]).view(1, 2, 3, 4, 1, 1).cuda())\r\nprint(a)\r\nprint(flip(a, 0)) # Or -6\r\nprint(flip(a, 1)) # Or -5\r\nprint(flip(a, 2)) # Or -4\r\nprint(flip(a, 3)) # Or -3\r\nprint(flip(a, 4)) # Or -2\r\nprint(flip(a, -1)) # Or 5\r\n```\r\nA pytorch 0.4.0+ version:\r\n```python\r\n# https://github.com/pytorch/pytorch/issues/229\r\nimport torch\r\ndef flip(x, dim):\r\n    indices = [slice(None)] * x.dim()\r\n    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\r\n                                dtype=torch.long, device=x.device)\r\n    return x[tuple(indices)]\r\n```\r\n\r\nOf course, it would be nicer if we could use negative strides directly. This is a useful operation. At least I personally need it for a project that I am working on."}