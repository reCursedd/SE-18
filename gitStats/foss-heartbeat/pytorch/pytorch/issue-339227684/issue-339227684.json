{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9243", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9243/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9243/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9243/events", "html_url": "https://github.com/pytorch/pytorch/issues/9243", "id": 339227684, "node_id": "MDU6SXNzdWUzMzkyMjc2ODQ=", "number": 9243, "title": "what if all gradients goes to zeros?", "user": {"login": "MrWanter", "id": 18298163, "node_id": "MDQ6VXNlcjE4Mjk4MTYz", "avatar_url": "https://avatars0.githubusercontent.com/u/18298163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrWanter", "html_url": "https://github.com/MrWanter", "followers_url": "https://api.github.com/users/MrWanter/followers", "following_url": "https://api.github.com/users/MrWanter/following{/other_user}", "gists_url": "https://api.github.com/users/MrWanter/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrWanter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrWanter/subscriptions", "organizations_url": "https://api.github.com/users/MrWanter/orgs", "repos_url": "https://api.github.com/users/MrWanter/repos", "events_url": "https://api.github.com/users/MrWanter/events{/privacy}", "received_events_url": "https://api.github.com/users/MrWanter/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-08T13:52:20Z", "updated_at": "2018-07-08T14:45:30Z", "closed_at": "2018-07-08T14:45:30Z", "author_association": "NONE", "body_html": "<p>Hi! I'm modifying some code <a href=\"https://github.com/jwyang/faster-rcnn.pytorch\">pytorch-faster-rcnn</a>, I after adding some layers and loss term, in the training, the loss is going down, but after several steps in inital training, all gradients goes to zeros, but loss still not low, I checked when beginning training the gradients are normal, very puzelled about this behavior, can someone give me some general clue of how to debug this kind of problem?</p>", "body_text": "Hi! I'm modifying some code pytorch-faster-rcnn, I after adding some layers and loss term, in the training, the loss is going down, but after several steps in inital training, all gradients goes to zeros, but loss still not low, I checked when beginning training the gradients are normal, very puzelled about this behavior, can someone give me some general clue of how to debug this kind of problem?", "body": "Hi! I'm modifying some code [pytorch-faster-rcnn](https://github.com/jwyang/faster-rcnn.pytorch), I after adding some layers and loss term, in the training, the loss is going down, but after several steps in inital training, all gradients goes to zeros, but loss still not low, I checked when beginning training the gradients are normal, very puzelled about this behavior, can someone give me some general clue of how to debug this kind of problem?"}