{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152067053", "pull_request_review_id": 77871643, "id": 152067053, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MjA2NzA1Mw==", "diff_hunk": "@@ -17,6 +18,8 @@ struct Identity : public TraceableFunction {\n \n struct CopyBackwards : public Function {\n   virtual variable_list apply(const variable_list& inputs) override;\n+\n+  at::Type *src_type;", "path": "torch/csrc/autograd/functions/tensor.h", "position": 13, "original_position": 13, "commit_id": "5854d3887b53f7c5a8130d4136e28daa1afc2038", "original_commit_id": "c7352142e7bf2eb4a6bff0d89b6d705c90ee1f93", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "These casting functions should be ok, but only because we take care to pin engine threads to a single device as long as they are alive (they used to all run on GPU 0 for a long time). Still, `CopyBackwards` sounds like something that should be able to do backwards of `a.copy_(b)` and this can cross device boundaries in any way, and you need to save the device if you want to make it work. It really doesn't cost us anything and I'd rather add it to be safer to save debugging in the future.", "created_at": "2017-11-20T18:05:23Z", "updated_at": "2018-11-23T15:36:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/3762#discussion_r152067053", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3762", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152067053"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3762#discussion_r152067053"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3762"}}, "body_html": "<p>These casting functions should be ok, but only because we take care to pin engine threads to a single device as long as they are alive (they used to all run on GPU 0 for a long time). Still, <code>CopyBackwards</code> sounds like something that should be able to do backwards of <code>a.copy_(b)</code> and this can cross device boundaries in any way, and you need to save the device if you want to make it work. It really doesn't cost us anything and I'd rather add it to be safer to save debugging in the future.</p>", "body_text": "These casting functions should be ok, but only because we take care to pin engine threads to a single device as long as they are alive (they used to all run on GPU 0 for a long time). Still, CopyBackwards sounds like something that should be able to do backwards of a.copy_(b) and this can cross device boundaries in any way, and you need to save the device if you want to make it work. It really doesn't cost us anything and I'd rather add it to be safer to save debugging in the future.", "in_reply_to_id": 151867181}