{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178568473", "pull_request_review_id": 108644797, "id": 178568473, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODU2ODQ3Mw==", "diff_hunk": "@@ -171,32 +171,37 @@ class CosineAnnealingLR(_LRScheduler):\n     .. math::\n \n         \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n-        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n+        \\cos(\\frac{T_{cur}}{T_{max}\\times T_{mult}}\\pi))\n \n     When last_epoch=-1, sets initial lr as lr.\n \n     It has been proposed in\n-    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n-    implements the cosine annealing part of SGDR, and not the restarts.\n+    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n \n     Args:\n         optimizer (Optimizer): Wrapped optimizer.\n         T_max (int): Maximum number of iterations.\n         eta_min (float): Minimum learning rate. Default: 0.\n+        T_mult (int): Multiplicative factor of T_max. Default: 2\n+        restart (bool): If True, warm restart policy will be used.\n         last_epoch (int): The index of last epoch. Default: -1.\n \n     .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n         https://arxiv.org/abs/1608.03983\n     \"\"\"\n \n-    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n+    def __init__(self, optimizer, T_max, eta_min=0, T_mult=2, restart=False, last_epoch=-1):\n         self.T_max = T_max\n         self.eta_min = eta_min\n+        self.T_mult = T_mult\n+        self.restart = restart\n         super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n \n     def get_lr(self):\n-        return [self.eta_min + (base_lr - self.eta_min) *\n-                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n+        if self.restart and self.last_epoch == self.T_max:\n+            self.last_epoch = 0\n+            self.T_max *= self.T_mult", "path": "torch/optim/lr_scheduler.py", "position": null, "original_position": 39, "commit_id": "67d26ccb4e7540788c873ee20f448018b2d44c1f", "original_commit_id": "92e47efa385806ecc1e3343676a34e55dec74d95", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Can't you also do something like\r\n```\r\nif self.restart:\r\n    k = math.log(self.last_epoch / self.T_max * (self.T_mult - 1) + 1, self.T_mult)\r\n    T_max = self.T_max * (self.T_mult ** int(math.floor(k)))\r\nelse:\r\n    T_max = self.T_max\r\n...\r\n```", "created_at": "2018-04-02T15:28:49Z", "updated_at": "2018-11-23T15:41:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/6130#discussion_r178568473", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6130", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178568473"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6130#discussion_r178568473"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6130"}}, "body_html": "<p>Can't you also do something like</p>\n<pre><code>if self.restart:\n    k = math.log(self.last_epoch / self.T_max * (self.T_mult - 1) + 1, self.T_mult)\n    T_max = self.T_max * (self.T_mult ** int(math.floor(k)))\nelse:\n    T_max = self.T_max\n...\n</code></pre>", "body_text": "Can't you also do something like\nif self.restart:\n    k = math.log(self.last_epoch / self.T_max * (self.T_mult - 1) + 1, self.T_mult)\n    T_max = self.T_max * (self.T_mult ** int(math.floor(k)))\nelse:\n    T_max = self.T_max\n...", "in_reply_to_id": 178553216}