{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409689807", "html_url": "https://github.com/pytorch/pytorch/pull/9960#issuecomment-409689807", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9960", "id": 409689807, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTY4OTgwNw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-01T19:14:17Z", "updated_at": "2018-08-01T19:14:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So interestingly, when I use the identical kernel with a caller that calls torch.rand and then the kernel from a PyTorch extension, that seems to not have the error. I'll see to extract the random part from the actual call.<br>\nA random thing I've noticed and been wondering is whether there is a typo regarding the shared memory requirements: There is the product <a href=\"https://github.com/pytorch/pytorch/blob/e8f27311aa22dc565fa310d6c820783d9c273cd1/aten/src/THC/generic/THCTensorRandom.cu#L184\"><code>sizeof(real)*sizeof(accreal)</code></a> looks pretty suspicious and probably should be a sum (possibly with coeffs). But that likely is unrelated to our present bug.</p>", "body_text": "So interestingly, when I use the identical kernel with a caller that calls torch.rand and then the kernel from a PyTorch extension, that seems to not have the error. I'll see to extract the random part from the actual call.\nA random thing I've noticed and been wondering is whether there is a typo regarding the shared memory requirements: There is the product sizeof(real)*sizeof(accreal) looks pretty suspicious and probably should be a sum (possibly with coeffs). But that likely is unrelated to our present bug.", "body": "So interestingly, when I use the identical kernel with a caller that calls torch.rand and then the kernel from a PyTorch extension, that seems to not have the error. I'll see to extract the random part from the actual call.\r\nA random thing I've noticed and been wondering is whether there is a typo regarding the shared memory requirements: There is the product [`sizeof(real)*sizeof(accreal)`](https://github.com/pytorch/pytorch/blob/e8f27311aa22dc565fa310d6c820783d9c273cd1/aten/src/THC/generic/THCTensorRandom.cu#L184) looks pretty suspicious and probably should be a sum (possibly with coeffs). But that likely is unrelated to our present bug."}