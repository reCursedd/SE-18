{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216085587", "pull_request_review_id": 153497817, "id": 216085587, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjA4NTU4Nw==", "diff_hunk": "@@ -184,6 +183,144 @@ static PyObject * THPVariable_range(PyObject* self, PyObject* args, PyObject* kw\n   END_HANDLE_TH_ERRORS\n }\n \n+inline Tensor dispatch_randint(int64_t high, IntList size, Generator * generator, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, high, size, generator);\n+}\n+inline Tensor dispatch_randint(int64_t high, IntList size, Generator * generator, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(high, size, generator, options);\n+}\n+inline Tensor dispatch_randint(int64_t high, IntList size, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, high, size);\n+}\n+inline Tensor dispatch_randint(int64_t high, IntList size, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(high, size, options);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, Generator * generator, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, low, high, size, generator);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, Generator * generator, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(low, high, size, generator, options);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, low, high, size);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(low, high, size, options);\n+}\n+\n+static PyObject * THPVariable_randint(PyObject* self_, PyObject* args, PyObject* kwargs)\n+{\n+  HANDLE_TH_ERRORS\n+  static PythonArgParser parser({\n+    \"randint(int64_t high, IntList size, *, Generator generator, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+    \"randint(int64_t high, IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+    \"randint(int64_t low, int64_t high, IntList size, *, Generator generator, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+    \"randint(int64_t low, int64_t high, IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+  }, /*traceable=*/false);\n+\n+  ParsedArgs<9> parsed_args;\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+  if (r.idx == 0) {\n+    if (r.isNone(3)) {\n+      auto high = r.toInt64(0);\n+      auto size = r.intlist(1);\n+      auto generator = r.generator(2);\n+      // NOTE: r.scalartype(X) gives the default dtype if r.isNone(X)\n+      auto dtype = r.isNone(4) ? at::ScalarType::Long : r.scalartype(4);\n+      auto device = r.device(6);\n+      const auto options = TensorOptions()\n+          .dtype(dtype)\n+          .device(device)\n+          .layout(r.layout(5).layout)\n+          .requires_grad(r.toBool(7));\n+      return wrap(dispatch_randint(high, size, generator, options));\n+    } else {\n+      check_out_type_matches(r.tensor(3), r.scalartype(4), r.isNone(4),\n+                             r.layout(5), r.isNone(5),\n+                             r.device(6), r.isNone(6));\n+      return wrap(dispatch_randint(r.toInt64(0), r.intlist(1), r.generator(2), r.tensor(3)).set_requires_grad(r.toBool(7)));\n+    }\n+  } else if (r.idx == 1) {\n+    if (r.isNone(2)) {\n+      auto high = r.toInt64(0);\n+      auto size = r.intlist(1);\n+      // NOTE: r.scalartype(X) gives the default dtype if r.isNone(X)\n+      auto dtype = r.isNone(3) ? at::ScalarType::Long : r.scalartype(3);\n+      auto device = r.device(5);\n+      const auto options = TensorOptions()\n+          .dtype(dtype)\n+          .device(device)\n+          .layout(r.layout(4).layout)\n+          .requires_grad(r.toBool(6));\n+      return wrap(dispatch_randint(high, size, options));\n+    } else {\n+      check_out_type_matches(r.tensor(2), r.scalartype(3), r.isNone(3),\n+                             r.layout(4), r.isNone(4),\n+                             r.device(5), r.isNone(5));\n+      return wrap(dispatch_randint(r.toInt64(0), r.intlist(1), r.tensor(2)).set_requires_grad(r.toBool(6)));\n+    }\n+  } else if (r.idx == 2) {\n+    if (r.isNone(4)) {\n+      auto low = r.toInt64(0);\n+      auto high = r.toInt64(1);\n+      auto size = r.intlist(2);\n+      auto generator = r.generator(3);\n+      // NOTE: r.scalartype(X) gives the default dtype if r.isNone(X)\n+      auto dtype = r.isNone(5) ? at::ScalarType::Long : r.scalartype(5);\n+      auto device = r.device(7);\n+      const auto options = TensorOptions()\n+          .dtype(dtype)\n+          .device(device)\n+          .layout(r.layout(6).layout)\n+          .requires_grad(r.toBool(8));\n+      return wrap(dispatch_randint(low, high, size, generator, options));\n+    } else {\n+      check_out_type_matches(r.tensor(4), r.scalartype(5), r.isNone(5),\n+                             r.layout(6), r.isNone(6),\n+                             r.device(7), r.isNone(7));\n+      return wrap(dispatch_randint(r.toInt64(0), r.toInt64(1), r.intlist(2), r.generator(3), r.tensor(4)).set_requires_grad(r.toBool(8)));\n+    }\n+  } else if (r.idx == 3) {\n+    if (r.isNone(3)) {\n+      auto low = r.toInt64(0);\n+      auto high = r.toInt64(1);\n+      auto size = r.intlist(2);\n+      // NOTE: r.scalartype(X) gives the default dtype if r.isNone(X)\n+      auto dtype = r.isNone(4) ? at::ScalarType::Long : r.scalartype(4);", "path": "tools/autograd/templates/python_torch_functions.cpp", "position": null, "original_position": 135, "commit_id": "6c3075e61c6b447f1feeeb36605b231564a9b475", "original_commit_id": "4210ddc399d047c5e6c1845aeaa8c29b322f2c5d", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "same here.", "created_at": "2018-09-07T20:58:46Z", "updated_at": "2018-11-23T15:50:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/11040#discussion_r216085587", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11040", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216085587"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11040#discussion_r216085587"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11040"}}, "body_html": "<p>same here.</p>", "body_text": "same here."}