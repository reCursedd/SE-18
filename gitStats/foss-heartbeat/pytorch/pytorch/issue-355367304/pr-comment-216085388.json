{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216085388", "pull_request_review_id": 153497817, "id": 216085388, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjA4NTM4OA==", "diff_hunk": "@@ -184,6 +183,144 @@ static PyObject * THPVariable_range(PyObject* self, PyObject* args, PyObject* kw\n   END_HANDLE_TH_ERRORS\n }\n \n+inline Tensor dispatch_randint(int64_t high, IntList size, Generator * generator, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, high, size, generator);\n+}\n+inline Tensor dispatch_randint(int64_t high, IntList size, Generator * generator, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(high, size, generator, options);\n+}\n+inline Tensor dispatch_randint(int64_t high, IntList size, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, high, size);\n+}\n+inline Tensor dispatch_randint(int64_t high, IntList size, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(high, size, options);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, Generator * generator, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, low, high, size, generator);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, Generator * generator, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(low, high, size, generator, options);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, Tensor result) {\n+\n+  AutoNoGIL no_gil;\n+  return at::randint_out(result, low, high, size);\n+}\n+inline Tensor dispatch_randint(int64_t low, int64_t high, IntList size, const TensorOptions & options) {\n+  maybe_initialize_cuda(options.type());\n+  AutoNoGIL no_gil;\n+  return torch::randint(low, high, size, options);\n+}\n+\n+static PyObject * THPVariable_randint(PyObject* self_, PyObject* args, PyObject* kwargs)\n+{\n+  HANDLE_TH_ERRORS\n+  static PythonArgParser parser({\n+    \"randint(int64_t high, IntList size, *, Generator generator, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+    \"randint(int64_t high, IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+    \"randint(int64_t low, int64_t high, IntList size, *, Generator generator, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+    \"randint(int64_t low, int64_t high, IntList size, *, Tensor out=None, ScalarType dtype=None, Layout layout=torch.strided, Device device=None, bool requires_grad=False)\",\n+  }, /*traceable=*/false);\n+\n+  ParsedArgs<9> parsed_args;\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+  if (r.idx == 0) {\n+    if (r.isNone(3)) {\n+      auto high = r.toInt64(0);\n+      auto size = r.intlist(1);\n+      auto generator = r.generator(2);\n+      // NOTE: r.scalartype(X) gives the default dtype if r.isNone(X)\n+      auto dtype = r.isNone(4) ? at::ScalarType::Long : r.scalartype(4);", "path": "tools/autograd/templates/python_torch_functions.cpp", "position": null, "original_position": 75, "commit_id": "6c3075e61c6b447f1feeeb36605b231564a9b475", "original_commit_id": "4210ddc399d047c5e6c1845aeaa8c29b322f2c5d", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "you can probably use scalarTypeOptional here.", "created_at": "2018-09-07T20:57:53Z", "updated_at": "2018-11-23T15:50:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/11040#discussion_r216085388", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11040", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216085388"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11040#discussion_r216085388"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11040"}}, "body_html": "<p>you can probably use scalarTypeOptional here.</p>", "body_text": "you can probably use scalarTypeOptional here."}