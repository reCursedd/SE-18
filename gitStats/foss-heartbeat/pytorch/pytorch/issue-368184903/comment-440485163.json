{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440485163", "html_url": "https://github.com/pytorch/pytorch/issues/12482#issuecomment-440485163", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12482", "id": 440485163, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDQ4NTE2Mw==", "user": {"login": "lisovskey", "id": 24867733, "node_id": "MDQ6VXNlcjI0ODY3NzMz", "avatar_url": "https://avatars2.githubusercontent.com/u/24867733?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lisovskey", "html_url": "https://github.com/lisovskey", "followers_url": "https://api.github.com/users/lisovskey/followers", "following_url": "https://api.github.com/users/lisovskey/following{/other_user}", "gists_url": "https://api.github.com/users/lisovskey/gists{/gist_id}", "starred_url": "https://api.github.com/users/lisovskey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lisovskey/subscriptions", "organizations_url": "https://api.github.com/users/lisovskey/orgs", "repos_url": "https://api.github.com/users/lisovskey/repos", "events_url": "https://api.github.com/users/lisovskey/events{/privacy}", "received_events_url": "https://api.github.com/users/lisovskey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-21T00:47:51Z", "updated_at": "2018-11-21T01:21:56Z", "author_association": "NONE", "body_html": "<p>I just made a similar implementation of <code>LayerNormGRUCell</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">LayerNormGRUCell</span>(<span class=\"pl-e\">RNNCellBase</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">ln_preact</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>(LayerNormGRUCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(input_size, hidden_size, bias, <span class=\"pl-v\">num_chunks</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n        <span class=\"pl-c1\">self</span>.ln_preact <span class=\"pl-k\">=</span> ln_preact\n        <span class=\"pl-k\">if</span> ln_preact:\n            <span class=\"pl-c1\">self</span>.ln_ih <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.hidden_size)\n            <span class=\"pl-c1\">self</span>.ln_hh <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.hidden_size)\n        <span class=\"pl-c1\">self</span>.ln_in <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">self</span>.hidden_size)\n        <span class=\"pl-c1\">self</span>.ln_hn <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">self</span>.hidden_size)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">hx</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.check_forward_input(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">if</span> hx <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            hx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.new_zeros(<span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.check_forward_hidden(<span class=\"pl-c1\">input</span>, hx)\n        \n        ih <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span> <span class=\"pl-k\">@</span> <span class=\"pl-c1\">self</span>.weight_ih.t() <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.bias_ih\n        hh <span class=\"pl-k\">=</span> hx <span class=\"pl-k\">@</span> <span class=\"pl-c1\">self</span>.weight_hh.t() <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.bias_hh\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.ln_preact:\n            ih <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_ih(ih)\n            hh <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_hh(hh)\n\n        i_r, i_z, i_n <span class=\"pl-k\">=</span> ih.chunk(<span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        h_r, h_z, h_n <span class=\"pl-k\">=</span> hh.chunk(<span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        i_n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_in(i_n)\n        h_n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_hn(h_n)\n\n        r <span class=\"pl-k\">=</span> torch.sigmoid(i_r <span class=\"pl-k\">+</span> h_r)\n        z <span class=\"pl-k\">=</span> torch.sigmoid(i_z <span class=\"pl-k\">+</span> h_z)\n        n <span class=\"pl-k\">=</span> torch.tanh(i_n <span class=\"pl-k\">+</span> r<span class=\"pl-k\">*</span>h_n)\n        h <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> z)<span class=\"pl-k\">*</span>n <span class=\"pl-k\">+</span> z<span class=\"pl-k\">*</span>hx\n\n        <span class=\"pl-k\">return</span> h</pre></div>\n<p>And <code>LayerNormGRU</code> to use it, but without <code>bidirectional</code> for now:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">LayerNormGRU</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                 <span class=\"pl-smi\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>):\n        <span class=\"pl-c1\">super</span>(LayerNormGRU, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.num_layers <span class=\"pl-k\">=</span> num_layers\n        <span class=\"pl-c1\">self</span>.batch_first <span class=\"pl-k\">=</span> batch_first\n        <span class=\"pl-c1\">self</span>.dropout <span class=\"pl-k\">=</span> dropout\n        <span class=\"pl-c1\">self</span>.cells <span class=\"pl-k\">=</span> nn.ModuleList([\n            LayerNormGRUCell(input_size <span class=\"pl-k\">if</span> i <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">else</span> hidden_size, hidden_size, bias)\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">self</span>.num_layers)\n        ])\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">hx</span>):\n        seq_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.batch_first <span class=\"pl-k\">else</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">for</span> i, cell <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">self</span>.cells):\n            y <span class=\"pl-k\">=</span> []\n            <span class=\"pl-k\">for</span> xc <span class=\"pl-k\">in</span> x.chunk(x.size(seq_dim), <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>seq_dim):\n                xc <span class=\"pl-k\">=</span> xc.squeeze(seq_dim)\n                hx[i] <span class=\"pl-k\">=</span> cell(xc, hx[i].clone())\n                y.append(hx[i].unsqueeze(<span class=\"pl-c1\">0</span>))\n            x <span class=\"pl-k\">=</span> torch.stack(y, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>seq_dim <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>).squeeze(<span class=\"pl-c1\">0</span>)\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.dropout <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> i <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">self</span>.num_layers <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>:\n                x <span class=\"pl-k\">=</span> F.dropout(x, <span class=\"pl-c1\">self</span>.dropout, <span class=\"pl-c1\">self</span>.training)\n        <span class=\"pl-k\">return</span> x, hx</pre></div>\n<p>One can use JIT to speed up training</p>", "body_text": "I just made a similar implementation of LayerNormGRUCell:\nclass LayerNormGRUCell(RNNCellBase):\n    def __init__(self, input_size, hidden_size, bias=True, ln_preact=True):\n        super(LayerNormGRUCell, self).__init__(input_size, hidden_size, bias, num_chunks=3)\n\n        self.ln_preact = ln_preact\n        if ln_preact:\n            self.ln_ih = nn.LayerNorm(3*self.hidden_size)\n            self.ln_hh = nn.LayerNorm(3*self.hidden_size)\n        self.ln_in = nn.LayerNorm(self.hidden_size)\n        self.ln_hn = nn.LayerNorm(self.hidden_size)\n\n    def forward(self, input, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n        self.check_forward_hidden(input, hx)\n        \n        ih = input @ self.weight_ih.t() + self.bias_ih\n        hh = hx @ self.weight_hh.t() + self.bias_hh\n        if self.ln_preact:\n            ih = self.ln_ih(ih)\n            hh = self.ln_hh(hh)\n\n        i_r, i_z, i_n = ih.chunk(3, dim=1)\n        h_r, h_z, h_n = hh.chunk(3, dim=1)\n        i_n = self.ln_in(i_n)\n        h_n = self.ln_hn(h_n)\n\n        r = torch.sigmoid(i_r + h_r)\n        z = torch.sigmoid(i_z + h_z)\n        n = torch.tanh(i_n + r*h_n)\n        h = (1 - z)*n + z*hx\n\n        return h\nAnd LayerNormGRU to use it, but without bidirectional for now:\nclass LayerNormGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True,\n                 batch_first=False, dropout=0):\n        super(LayerNormGRU, self).__init__()\n\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.dropout = dropout\n        self.cells = nn.ModuleList([\n            LayerNormGRUCell(input_size if i == 0 else hidden_size, hidden_size, bias)\n            for i in range(self.num_layers)\n        ])\n\n    def forward(self, x, hx):\n        seq_dim = 1 if self.batch_first else 0\n        for i, cell in enumerate(self.cells):\n            y = []\n            for xc in x.chunk(x.size(seq_dim), dim=seq_dim):\n                xc = xc.squeeze(seq_dim)\n                hx[i] = cell(xc, hx[i].clone())\n                y.append(hx[i].unsqueeze(0))\n            x = torch.stack(y, dim=seq_dim + 1).squeeze(0)\n            if self.dropout > 0 and i != self.num_layers - 1:\n                x = F.dropout(x, self.dropout, self.training)\n        return x, hx\nOne can use JIT to speed up training", "body": "I just made a similar implementation of `LayerNormGRUCell`:\r\n```python\r\nclass LayerNormGRUCell(RNNCellBase):\r\n    def __init__(self, input_size, hidden_size, bias=True, ln_preact=True):\r\n        super(LayerNormGRUCell, self).__init__(input_size, hidden_size, bias, num_chunks=3)\r\n\r\n        self.ln_preact = ln_preact\r\n        if ln_preact:\r\n            self.ln_ih = nn.LayerNorm(3*self.hidden_size)\r\n            self.ln_hh = nn.LayerNorm(3*self.hidden_size)\r\n        self.ln_in = nn.LayerNorm(self.hidden_size)\r\n        self.ln_hn = nn.LayerNorm(self.hidden_size)\r\n\r\n    def forward(self, input, hx=None):\r\n        self.check_forward_input(input)\r\n        if hx is None:\r\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n        self.check_forward_hidden(input, hx)\r\n        \r\n        ih = input @ self.weight_ih.t() + self.bias_ih\r\n        hh = hx @ self.weight_hh.t() + self.bias_hh\r\n        if self.ln_preact:\r\n            ih = self.ln_ih(ih)\r\n            hh = self.ln_hh(hh)\r\n\r\n        i_r, i_z, i_n = ih.chunk(3, dim=1)\r\n        h_r, h_z, h_n = hh.chunk(3, dim=1)\r\n        i_n = self.ln_in(i_n)\r\n        h_n = self.ln_hn(h_n)\r\n\r\n        r = torch.sigmoid(i_r + h_r)\r\n        z = torch.sigmoid(i_z + h_z)\r\n        n = torch.tanh(i_n + r*h_n)\r\n        h = (1 - z)*n + z*hx\r\n\r\n        return h\r\n```\r\nAnd `LayerNormGRU` to use it, but without `bidirectional` for now:\r\n```python\r\nclass LayerNormGRU(nn.Module):\r\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True,\r\n                 batch_first=False, dropout=0):\r\n        super(LayerNormGRU, self).__init__()\r\n\r\n        self.num_layers = num_layers\r\n        self.batch_first = batch_first\r\n        self.dropout = dropout\r\n        self.cells = nn.ModuleList([\r\n            LayerNormGRUCell(input_size if i == 0 else hidden_size, hidden_size, bias)\r\n            for i in range(self.num_layers)\r\n        ])\r\n\r\n    def forward(self, x, hx):\r\n        seq_dim = 1 if self.batch_first else 0\r\n        for i, cell in enumerate(self.cells):\r\n            y = []\r\n            for xc in x.chunk(x.size(seq_dim), dim=seq_dim):\r\n                xc = xc.squeeze(seq_dim)\r\n                hx[i] = cell(xc, hx[i].clone())\r\n                y.append(hx[i].unsqueeze(0))\r\n            x = torch.stack(y, dim=seq_dim + 1).squeeze(0)\r\n            if self.dropout > 0 and i != self.num_layers - 1:\r\n                x = F.dropout(x, self.dropout, self.training)\r\n        return x, hx\r\n```\r\nOne can use JIT to speed up training"}