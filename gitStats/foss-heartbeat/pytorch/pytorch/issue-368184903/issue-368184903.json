{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12482", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12482/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12482/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12482/events", "html_url": "https://github.com/pytorch/pytorch/issues/12482", "id": 368184903, "node_id": "MDU6SXNzdWUzNjgxODQ5MDM=", "number": 12482, "title": "[Feature request]:  add `LayerNormLSTMCell`", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-09T12:27:01Z", "updated_at": "2018-11-21T01:21:56Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>As layer normalization recently becomes a standard trick to train RNN, it would be very convenient to support it.</p>\n<p>Here I write up an initial version</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">LayerNormLSTMCell</span>(<span class=\"pl-e\">RNNCellBase</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">ln_preact</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>(LayerNormLSTMCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(input_size, hidden_size, bias, <span class=\"pl-v\">num_chunks</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\n        \n        <span class=\"pl-c1\">self</span>.ln_preact <span class=\"pl-k\">=</span> ln_preact\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.ln_preact:\n            <span class=\"pl-c1\">self</span>.ln_ih <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">4</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.hidden_size)\n            <span class=\"pl-c1\">self</span>.ln_hh <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">4</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.hidden_size)\n        <span class=\"pl-c1\">self</span>.ln_cell <span class=\"pl-k\">=</span> nn.LayerNorm(<span class=\"pl-c1\">self</span>.hidden_size)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">hx</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.check_forward_input(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">if</span> hx <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            hx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.new_zeros(<span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n            hx <span class=\"pl-k\">=</span> (hx, hx)\n        <span class=\"pl-c1\">self</span>.check_forward_hidden(<span class=\"pl-c1\">input</span>, hx[<span class=\"pl-c1\">0</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>[0]<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c1\">self</span>.check_forward_hidden(<span class=\"pl-c1\">input</span>, hx[<span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>[1]<span class=\"pl-pds\">'</span></span>)\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden states and preactivations</span>\n        h, c <span class=\"pl-k\">=</span> hx\n        ih <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span> <span class=\"pl-k\">@</span> <span class=\"pl-c1\">self</span>.weight_ih.t() <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.bias_ih\n        hh <span class=\"pl-k\">=</span> h <span class=\"pl-k\">@</span> <span class=\"pl-c1\">self</span>.weight_hh.t() <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.bias_hh\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.ln_preact:\n            ih <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_ih(ih)\n            hh <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_hh(hh)\n        preact <span class=\"pl-k\">=</span> ih <span class=\"pl-k\">+</span> hh\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Gates</span>\n        f, i, o, g <span class=\"pl-k\">=</span> preact.chunk(<span class=\"pl-c1\">4</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        g <span class=\"pl-k\">=</span> g.tanh()\n        f <span class=\"pl-k\">=</span> f.sigmoid()\n        i <span class=\"pl-k\">=</span> i.sigmoid()\n        o <span class=\"pl-k\">=</span> o.sigmoid()\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> cell computations</span>\n        c <span class=\"pl-k\">=</span> f<span class=\"pl-k\">*</span>c <span class=\"pl-k\">+</span> i<span class=\"pl-k\">*</span>g\n        c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.ln_cell(c)\n        h <span class=\"pl-k\">=</span> o<span class=\"pl-k\">*</span>c.tanh()\n        \n        <span class=\"pl-k\">return</span> h, c</pre></div>", "body_text": "As layer normalization recently becomes a standard trick to train RNN, it would be very convenient to support it.\nHere I write up an initial version\nclass LayerNormLSTMCell(RNNCellBase):\n\n    def __init__(self, input_size, hidden_size, bias=True, ln_preact=True):\n        super(LayerNormLSTMCell, self).__init__(input_size, hidden_size, bias, num_chunks=4)\n        \n        self.ln_preact = ln_preact\n        if self.ln_preact:\n            self.ln_ih = nn.LayerNorm(4*self.hidden_size)\n            self.ln_hh = nn.LayerNorm(4*self.hidden_size)\n        self.ln_cell = nn.LayerNorm(self.hidden_size)\n\n    def forward(self, input, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            hx = (hx, hx)\n        self.check_forward_hidden(input, hx[0], '[0]')\n        self.check_forward_hidden(input, hx[1], '[1]')\n        \n        # hidden states and preactivations\n        h, c = hx\n        ih = input @ self.weight_ih.t() + self.bias_ih\n        hh = h @ self.weight_hh.t() + self.bias_hh\n        if self.ln_preact:\n            ih = self.ln_ih(ih)\n            hh = self.ln_hh(hh)\n        preact = ih + hh\n        \n        # Gates\n        f, i, o, g = preact.chunk(4, dim=1)\n        g = g.tanh()\n        f = f.sigmoid()\n        i = i.sigmoid()\n        o = o.sigmoid()\n        \n        # cell computations\n        c = f*c + i*g\n        c = self.ln_cell(c)\n        h = o*c.tanh()\n        \n        return h, c", "body": "As layer normalization recently becomes a standard trick to train RNN, it would be very convenient to support it.\r\n\r\nHere I write up an initial version\r\n\r\n```python\r\nclass LayerNormLSTMCell(RNNCellBase):\r\n\r\n    def __init__(self, input_size, hidden_size, bias=True, ln_preact=True):\r\n        super(LayerNormLSTMCell, self).__init__(input_size, hidden_size, bias, num_chunks=4)\r\n        \r\n        self.ln_preact = ln_preact\r\n        if self.ln_preact:\r\n            self.ln_ih = nn.LayerNorm(4*self.hidden_size)\r\n            self.ln_hh = nn.LayerNorm(4*self.hidden_size)\r\n        self.ln_cell = nn.LayerNorm(self.hidden_size)\r\n\r\n    def forward(self, input, hx=None):\r\n        self.check_forward_input(input)\r\n        if hx is None:\r\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n            hx = (hx, hx)\r\n        self.check_forward_hidden(input, hx[0], '[0]')\r\n        self.check_forward_hidden(input, hx[1], '[1]')\r\n        \r\n        # hidden states and preactivations\r\n        h, c = hx\r\n        ih = input @ self.weight_ih.t() + self.bias_ih\r\n        hh = h @ self.weight_hh.t() + self.bias_hh\r\n        if self.ln_preact:\r\n            ih = self.ln_ih(ih)\r\n            hh = self.ln_hh(hh)\r\n        preact = ih + hh\r\n        \r\n        # Gates\r\n        f, i, o, g = preact.chunk(4, dim=1)\r\n        g = g.tanh()\r\n        f = f.sigmoid()\r\n        i = i.sigmoid()\r\n        o = o.sigmoid()\r\n        \r\n        # cell computations\r\n        c = f*c + i*g\r\n        c = self.ln_cell(c)\r\n        h = o*c.tanh()\r\n        \r\n        return h, c\r\n````"}