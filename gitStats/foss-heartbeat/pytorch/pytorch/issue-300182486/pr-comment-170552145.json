{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170552145", "pull_request_review_id": 99252772, "id": 170552145, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDU1MjE0NQ==", "diff_hunk": "@@ -636,6 +636,102 @@ def forward(self, input, output_size=None):\n             output_padding, self.groups, self.dilation)\n \n \n+class Conv2dBackward(_ConvTransposeMixin, _ConvNd):", "path": "torch/nn/modules/conv.py", "position": null, "original_position": 4, "commit_id": "e19881ef0ac35bb42a3da4205702f45296eee9f3", "original_commit_id": "c909be9e2bfc44f4d8dd924c2c1e3ee263ad9024", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I don't think that adding this module is a good idea. First, as noticed in this docstring, it's pretty much just conv transpose. Secondly, once we have `Conv2dBackward`, why shouldn't we also add `Conv3dBackward`, `LinearBackward`, `ReLUBackward`, etc...\r\n\r\nWhat's the use case for this? Maybe we should just have an interface for accessing the gradients we implement in autograd, but as a functional interface that doesn't need the first stage to happen and capture the relevant elements?", "created_at": "2018-02-26T10:42:20Z", "updated_at": "2018-11-23T15:39:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/5408#discussion_r170552145", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5408", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170552145"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5408#discussion_r170552145"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5408"}}, "body_html": "<p>I don't think that adding this module is a good idea. First, as noticed in this docstring, it's pretty much just conv transpose. Secondly, once we have <code>Conv2dBackward</code>, why shouldn't we also add <code>Conv3dBackward</code>, <code>LinearBackward</code>, <code>ReLUBackward</code>, etc...</p>\n<p>What's the use case for this? Maybe we should just have an interface for accessing the gradients we implement in autograd, but as a functional interface that doesn't need the first stage to happen and capture the relevant elements?</p>", "body_text": "I don't think that adding this module is a good idea. First, as noticed in this docstring, it's pretty much just conv transpose. Secondly, once we have Conv2dBackward, why shouldn't we also add Conv3dBackward, LinearBackward, ReLUBackward, etc...\nWhat's the use case for this? Maybe we should just have an interface for accessing the gradients we implement in autograd, but as a functional interface that doesn't need the first stage to happen and capture the relevant elements?"}