{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170967248", "pull_request_review_id": 99736749, "id": 170967248, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDk2NzI0OA==", "diff_hunk": "@@ -636,6 +636,102 @@ def forward(self, input, output_size=None):\n             output_padding, self.groups, self.dilation)\n \n \n+class Conv2dBackward(_ConvTransposeMixin, _ConvNd):", "path": "torch/nn/modules/conv.py", "position": null, "original_position": 4, "commit_id": "e19881ef0ac35bb42a3da4205702f45296eee9f3", "original_commit_id": "c909be9e2bfc44f4d8dd924c2c1e3ee263ad9024", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "> Implementing a backward of an autograd function using a module seems like a bad idea to me. Adding it to functional would be better, but \r\n\r\nHaving just the functional seems fine to me.\r\n\r\n> I still don't like the fact that it's another function we have to maintain manually, when we're auto-generating them for internal purposes.\r\n\r\nBut we're not auto-generating them! We have explicitly implemented kernels for each of the backwards. We're just giving aliases to them to help users out.", "created_at": "2018-02-27T15:48:02Z", "updated_at": "2018-11-23T15:40:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/5408#discussion_r170967248", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5408", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170967248"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5408#discussion_r170967248"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5408"}}, "body_html": "<blockquote>\n<p>Implementing a backward of an autograd function using a module seems like a bad idea to me. Adding it to functional would be better, but</p>\n</blockquote>\n<p>Having just the functional seems fine to me.</p>\n<blockquote>\n<p>I still don't like the fact that it's another function we have to maintain manually, when we're auto-generating them for internal purposes.</p>\n</blockquote>\n<p>But we're not auto-generating them! We have explicitly implemented kernels for each of the backwards. We're just giving aliases to them to help users out.</p>", "body_text": "Implementing a backward of an autograd function using a module seems like a bad idea to me. Adding it to functional would be better, but\n\nHaving just the functional seems fine to me.\n\nI still don't like the fact that it's another function we have to maintain manually, when we're auto-generating them for internal purposes.\n\nBut we're not auto-generating them! We have explicitly implemented kernels for each of the backwards. We're just giving aliases to them to help users out.", "in_reply_to_id": 170552145}