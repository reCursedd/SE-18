{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170882261", "pull_request_review_id": 99635311, "id": 170882261, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDg4MjI2MQ==", "diff_hunk": "@@ -636,6 +636,102 @@ def forward(self, input, output_size=None):\n             output_padding, self.groups, self.dilation)\n \n \n+class Conv2dBackward(_ConvTransposeMixin, _ConvNd):", "path": "torch/nn/modules/conv.py", "position": null, "original_position": 4, "commit_id": "e19881ef0ac35bb42a3da4205702f45296eee9f3", "original_commit_id": "c909be9e2bfc44f4d8dd924c2c1e3ee263ad9024", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I agree that having *some* way of supporting people who need this would be nice. I'm just saying that making the nn API 2x larger doesn't seem like a great solution, especially considering that it doesn't solve the original problem mentioned in that forum thread:\r\n\r\n> I want to use a coustom forward function with the standard convolutional backward function.\r\n> Maybe I should use the standard function when I need a backpropagation. Thank you.\r\n\r\nImplementing a backward of an autograd function using a module seems like a bad idea to me. Adding it to functional would be better, but I still don't like the fact that it's another function we have to maintain manually, when we're auto-generating them for internal purposes.\r\n\r\nSymbolic differentiation is never meant to be exposed to users, and is designed for internal use in the JIT only. We don't even plan to expand its scope to cover all of autograd in the core library.", "created_at": "2018-02-27T10:47:13Z", "updated_at": "2018-11-23T15:40:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/5408#discussion_r170882261", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5408", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170882261"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5408#discussion_r170882261"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5408"}}, "body_html": "<p>I agree that having <em>some</em> way of supporting people who need this would be nice. I'm just saying that making the nn API 2x larger doesn't seem like a great solution, especially considering that it doesn't solve the original problem mentioned in that forum thread:</p>\n<blockquote>\n<p>I want to use a coustom forward function with the standard convolutional backward function.<br>\nMaybe I should use the standard function when I need a backpropagation. Thank you.</p>\n</blockquote>\n<p>Implementing a backward of an autograd function using a module seems like a bad idea to me. Adding it to functional would be better, but I still don't like the fact that it's another function we have to maintain manually, when we're auto-generating them for internal purposes.</p>\n<p>Symbolic differentiation is never meant to be exposed to users, and is designed for internal use in the JIT only. We don't even plan to expand its scope to cover all of autograd in the core library.</p>", "body_text": "I agree that having some way of supporting people who need this would be nice. I'm just saying that making the nn API 2x larger doesn't seem like a great solution, especially considering that it doesn't solve the original problem mentioned in that forum thread:\n\nI want to use a coustom forward function with the standard convolutional backward function.\nMaybe I should use the standard function when I need a backpropagation. Thank you.\n\nImplementing a backward of an autograd function using a module seems like a bad idea to me. Adding it to functional would be better, but I still don't like the fact that it's another function we have to maintain manually, when we're auto-generating them for internal purposes.\nSymbolic differentiation is never meant to be exposed to users, and is designed for internal use in the JIT only. We don't even plan to expand its scope to cover all of autograd in the core library.", "in_reply_to_id": 170552145}