{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/301744059", "html_url": "https://github.com/pytorch/pytorch/pull/1561#issuecomment-301744059", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1561", "id": 301744059, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTc0NDA1OQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-16T10:41:55Z", "updated_at": "2017-05-16T10:41:55Z", "author_association": "MEMBER", "body_html": "<p>Actually I just realized that doing these mallocs + frees isn't the best idea in our case. We're using a caching allocator and this implementation of findAlgorithm can disrupt it's state quite heavily (e.g. if FFT requires 8GB of workspace, we'll allocate that and cache this block!). It'd be better to avoid these allocs and use <code>cudaMemGetInfo</code> + use <code>cacheInfo</code> from <code>THCDeviceAllocator</code> to determine a cap on the workspace size.</p>", "body_text": "Actually I just realized that doing these mallocs + frees isn't the best idea in our case. We're using a caching allocator and this implementation of findAlgorithm can disrupt it's state quite heavily (e.g. if FFT requires 8GB of workspace, we'll allocate that and cache this block!). It'd be better to avoid these allocs and use cudaMemGetInfo + use cacheInfo from THCDeviceAllocator to determine a cap on the workspace size.", "body": "Actually I just realized that doing these mallocs + frees isn't the best idea in our case. We're using a caching allocator and this implementation of findAlgorithm can disrupt it's state quite heavily (e.g. if FFT requires 8GB of workspace, we'll allocate that and cache this block!). It'd be better to avoid these allocs and use `cudaMemGetInfo` + use `cacheInfo` from `THCDeviceAllocator` to determine a cap on the workspace size."}