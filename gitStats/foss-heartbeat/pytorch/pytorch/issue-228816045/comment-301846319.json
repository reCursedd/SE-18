{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/301846319", "html_url": "https://github.com/pytorch/pytorch/pull/1561#issuecomment-301846319", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1561", "id": 301846319, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTg0NjMxOQ==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-16T16:55:45Z", "updated_at": "2017-05-16T16:55:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>You still need to allocate a workspace after you determined a cap on it, which will mean that you'd allocate and cache an even bigger block. It might be Ok, you'll be able to split it later if needed, but I'm not sure it is strictly better than allocating max you possibly need for this convolution.<br>\nOn the other hand, I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> that you should not be allocating/freeing at each iteration of the algo loop, I think it is best to find maximum workspace required by applicable algos and try to allocate that. Keep in mind that sometimes an inordinate (~40GB) workspace is returned, you'd have to ignore that.</p>", "body_text": "You still need to allocate a workspace after you determined a cap on it, which will mean that you'd allocate and cache an even bigger block. It might be Ok, you'll be able to split it later if needed, but I'm not sure it is strictly better than allocating max you possibly need for this convolution.\nOn the other hand, I agree with @apaszke that you should not be allocating/freeing at each iteration of the algo loop, I think it is best to find maximum workspace required by applicable algos and try to allocate that. Keep in mind that sometimes an inordinate (~40GB) workspace is returned, you'd have to ignore that.", "body": "You still need to allocate a workspace after you determined a cap on it, which will mean that you'd allocate and cache an even bigger block. It might be Ok, you'll be able to split it later if needed, but I'm not sure it is strictly better than allocating max you possibly need for this convolution. \r\nOn the other hand, I agree with @apaszke that you should not be allocating/freeing at each iteration of the algo loop, I think it is best to find maximum workspace required by applicable algos and try to allocate that. Keep in mind that sometimes an inordinate (~40GB) workspace is returned, you'd have to ignore that. "}