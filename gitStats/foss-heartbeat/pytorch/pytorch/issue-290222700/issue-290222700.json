{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4761", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4761/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4761/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4761/events", "html_url": "https://github.com/pytorch/pytorch/issues/4761", "id": 290222700, "node_id": "MDU6SXNzdWUyOTAyMjI3MDA=", "number": 4761, "title": "nn.Parameter not showing up in model.parameters but it is getting trained.", "user": {"login": "plstory", "id": 7537477, "node_id": "MDQ6VXNlcjc1Mzc0Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7537477?v=4", "gravatar_id": "", "url": "https://api.github.com/users/plstory", "html_url": "https://github.com/plstory", "followers_url": "https://api.github.com/users/plstory/followers", "following_url": "https://api.github.com/users/plstory/following{/other_user}", "gists_url": "https://api.github.com/users/plstory/gists{/gist_id}", "starred_url": "https://api.github.com/users/plstory/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/plstory/subscriptions", "organizations_url": "https://api.github.com/users/plstory/orgs", "repos_url": "https://api.github.com/users/plstory/repos", "events_url": "https://api.github.com/users/plstory/events{/privacy}", "received_events_url": "https://api.github.com/users/plstory/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-20T20:56:40Z", "updated_at": "2018-01-20T20:59:40Z", "closed_at": "2018-01-20T20:59:40Z", "author_association": "NONE", "body_html": "<p>I define both a bias parameter and some other module in MyModule, the bias parameter doesn't show up in model.parameters. However, I can see its value in list(model.parameters()). Furthermore, after a few training iterations, the value of the bias parameter does change, indicating that it's indeed part of the model parameters. Note: MyModule is also a submodule constructed as a ModuleList in some other module TopModule.</p>\n<p><code>class MyModule(nn.Module):</code><br>\n<code>    def __init__(self, state_len):</code><br>\n<code>self.bias = nn.Parameter(torch.ones(state_len))</code><br>\n<code> self.m = AnotherModule(state_len)</code><br>\n<code> def forward(input):</code><br>\n<code>some calculations that uses bias</code></p>\n<p>let <code>model=TopModule()</code>,<code>model.parameters</code> returns<br>\n<code>&lt;bound method TopModule.parameters of TopModule( blah blah (layers): ModuleList ( (0): MyModule ( (m): AnotherModule( (layers): ModuleList ( (0): Linear (192 -&gt; 128) (1): Linear (128 -&gt; 64) ) ) ) ) )&gt;</code><br>\nas you see, the bias parameter is missing.<br>\nBut if I define <code>l=list(model.layers[0].parameters())</code><br>\n<code>l[0]</code> is in fact the bias parameter as it prints all 1s before any training.</p>", "body_text": "I define both a bias parameter and some other module in MyModule, the bias parameter doesn't show up in model.parameters. However, I can see its value in list(model.parameters()). Furthermore, after a few training iterations, the value of the bias parameter does change, indicating that it's indeed part of the model parameters. Note: MyModule is also a submodule constructed as a ModuleList in some other module TopModule.\nclass MyModule(nn.Module):\n    def __init__(self, state_len):\nself.bias = nn.Parameter(torch.ones(state_len))\n self.m = AnotherModule(state_len)\n def forward(input):\nsome calculations that uses bias\nlet model=TopModule(),model.parameters returns\n<bound method TopModule.parameters of TopModule( blah blah (layers): ModuleList ( (0): MyModule ( (m): AnotherModule( (layers): ModuleList ( (0): Linear (192 -> 128) (1): Linear (128 -> 64) ) ) ) ) )>\nas you see, the bias parameter is missing.\nBut if I define l=list(model.layers[0].parameters())\nl[0] is in fact the bias parameter as it prints all 1s before any training.", "body": "I define both a bias parameter and some other module in MyModule, the bias parameter doesn't show up in model.parameters. However, I can see its value in list(model.parameters()). Furthermore, after a few training iterations, the value of the bias parameter does change, indicating that it's indeed part of the model parameters. Note: MyModule is also a submodule constructed as a ModuleList in some other module TopModule.\r\n\r\n`class MyModule(nn.Module):`\r\n`    def __init__(self, state_len):`\r\n        `self.bias = nn.Parameter(torch.ones(state_len))`\r\n       ` self.m = AnotherModule(state_len)`\r\n   ` def forward(input):`\r\n        `some calculations that uses bias`\r\n\r\nlet `model=TopModule()`,`model.parameters` returns \r\n`\r\n<bound method TopModule.parameters of TopModule(\r\n  blah\r\n  blah\r\n  (layers): ModuleList (\r\n    (0): MyModule (\r\n      (m): AnotherModule(\r\n        (layers): ModuleList (\r\n          (0): Linear (192 -> 128)\r\n          (1): Linear (128 -> 64)\r\n        )\r\n      )\r\n    )\r\n  )\r\n)>\r\n`\r\nas you see, the bias parameter is missing. \r\nBut if I define `l=list(model.layers[0].parameters())`\r\n`l[0]` is in fact the bias parameter as it prints all 1s before any training."}