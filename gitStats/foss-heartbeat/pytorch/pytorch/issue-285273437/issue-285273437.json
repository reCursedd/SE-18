{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4420", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4420/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4420/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4420/events", "html_url": "https://github.com/pytorch/pytorch/issues/4420", "id": 285273437, "node_id": "MDU6SXNzdWUyODUyNzM0Mzc=", "number": 4420, "title": "out of memory cuda runtime error (2) on p2.xlarge AWS instance with batch_size  = 1", "user": {"login": "hassanshallal", "id": 19214052, "node_id": "MDQ6VXNlcjE5MjE0MDUy", "avatar_url": "https://avatars1.githubusercontent.com/u/19214052?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hassanshallal", "html_url": "https://github.com/hassanshallal", "followers_url": "https://api.github.com/users/hassanshallal/followers", "following_url": "https://api.github.com/users/hassanshallal/following{/other_user}", "gists_url": "https://api.github.com/users/hassanshallal/gists{/gist_id}", "starred_url": "https://api.github.com/users/hassanshallal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hassanshallal/subscriptions", "organizations_url": "https://api.github.com/users/hassanshallal/orgs", "repos_url": "https://api.github.com/users/hassanshallal/repos", "events_url": "https://api.github.com/users/hassanshallal/events{/privacy}", "received_events_url": "https://api.github.com/users/hassanshallal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-31T19:39:27Z", "updated_at": "2018-01-02T18:34:47Z", "closed_at": "2018-01-02T12:47:31Z", "author_association": "NONE", "body_html": "<p>Hi folks,</p>\n<p>I know this issue has been reported earlier and the common wisdom has converged to decreasing the batch_size. In my current situation, decreasing the batch_size all the way to 1 didn't solve the out of memory runtime error indicating to me that there must be something else that's causing this issue. Here are some relevant information:</p>\n<ol>\n<li>\n<p>I am trying to fine tune pretrained resnet-34 on images with standard 3<em>224</em>224 size</p>\n</li>\n<li>\n<p>I was able to accomplish the fine-tuning with a batch-size of 32 on my mac-book air with only 8GB memory.</p>\n</li>\n<li>\n<p>Right now, I am trying to perform the same routine in a p2.xlarge instance on AWS with a 61 GB memory as indicated by AWS <a href=\"url\">https://aws.amazon.com/ec2/instance-types/p2/</a></p>\n</li>\n<li>\n<p>I followed this tutorial to set up the AWS EC2 instance: <a href=\"url\">https://kevinzakka.github.io/2017/08/13/aws-pytorch/</a></p>\n</li>\n<li>\n<p>I am reproducing the pipeline in this amazing tutorial: <a href=\"url\">http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html</a></p>\n</li>\n<li>\n<p>On the AWS instance: everything seems to be running fine. I was able to run a pre-training inference on the dataset but only after setting volatile =True. If not volatile, the out of memory runtime issue obtains!</p>\n</li>\n<li>\n<p>Here is the error message I obtain irrelevant to the batch_size, I get this error upon trying to train the resnet-34 even with a batch_size of 1:</p>\n</li>\n</ol>\n<pre><code>/home/ubuntu/envs/deepL/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/home/ubuntu/envs/deepL/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nfinetunacuda.py:147: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  train_props = m(Variable(train_props))\nfinetunacuda.py:183: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  test_props = m(Variable(test_props))\nTHCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory\nTraceback (most recent call last):\n  File \"finetunacuda.py\", line 311, in &lt;module&gt;\n    main()\n  File \"finetunacuda.py\", line 309, in main\n    fine_tuna_protocol()\n  File \"finetunacuda.py\", line 287, in fine_tuna_protocol\n    model_ft = train_model(dataloaders, dataset_sizes, model_ft, criterion, optimizer_ft, num_epochs = nep, temp_save_name = name_of_results_output_txt_file)\n  File \"finetunacuda.py\", line 228, in train_model\n    outputs = model(inputs)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torchvision/models/resnet.py\", line 142, in forward\n    x = self.maxpool(x)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/pooling.py\", line 143, in forward\n    self.return_indices)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/functional.py\", line 334, in max_pool2d\n    ret = torch._C._nn.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nRuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58\n</code></pre>\n<ol start=\"8\">\n<li>Here is the gpu in the instance:</li>\n</ol>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |\n| N/A   42C    P0    73W / 149W |      0MiB / 11439MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<ol start=\"9\">\n<li>I don't know whether the above information is enough to troubleshoot the issue. Please feel free to ask for any further information. I hope you guys can help with this troubleshooting, I am a newbie in GPU computing, this is my first time trying to train a model using GPU. Any hint/feedback is very much appreciated.</li>\n</ol>\n<p>Happy new year<br>\nCiao</p>", "body_text": "Hi folks,\nI know this issue has been reported earlier and the common wisdom has converged to decreasing the batch_size. In my current situation, decreasing the batch_size all the way to 1 didn't solve the out of memory runtime error indicating to me that there must be something else that's causing this issue. Here are some relevant information:\n\n\nI am trying to fine tune pretrained resnet-34 on images with standard 3224224 size\n\n\nI was able to accomplish the fine-tuning with a batch-size of 32 on my mac-book air with only 8GB memory.\n\n\nRight now, I am trying to perform the same routine in a p2.xlarge instance on AWS with a 61 GB memory as indicated by AWS https://aws.amazon.com/ec2/instance-types/p2/\n\n\nI followed this tutorial to set up the AWS EC2 instance: https://kevinzakka.github.io/2017/08/13/aws-pytorch/\n\n\nI am reproducing the pipeline in this amazing tutorial: http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n\n\nOn the AWS instance: everything seems to be running fine. I was able to run a pre-training inference on the dataset but only after setting volatile =True. If not volatile, the out of memory runtime issue obtains!\n\n\nHere is the error message I obtain irrelevant to the batch_size, I get this error upon trying to train the resnet-34 even with a batch_size of 1:\n\n\n/home/ubuntu/envs/deepL/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/home/ubuntu/envs/deepL/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nfinetunacuda.py:147: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  train_props = m(Variable(train_props))\nfinetunacuda.py:183: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  test_props = m(Variable(test_props))\nTHCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory\nTraceback (most recent call last):\n  File \"finetunacuda.py\", line 311, in <module>\n    main()\n  File \"finetunacuda.py\", line 309, in main\n    fine_tuna_protocol()\n  File \"finetunacuda.py\", line 287, in fine_tuna_protocol\n    model_ft = train_model(dataloaders, dataset_sizes, model_ft, criterion, optimizer_ft, num_epochs = nep, temp_save_name = name_of_results_output_txt_file)\n  File \"finetunacuda.py\", line 228, in train_model\n    outputs = model(inputs)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torchvision/models/resnet.py\", line 142, in forward\n    x = self.maxpool(x)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/pooling.py\", line 143, in forward\n    self.return_indices)\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/functional.py\", line 334, in max_pool2d\n    ret = torch._C._nn.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nRuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58\n\n\nHere is the gpu in the instance:\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |\n| N/A   42C    P0    73W / 149W |      0MiB / 11439MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\nI don't know whether the above information is enough to troubleshoot the issue. Please feel free to ask for any further information. I hope you guys can help with this troubleshooting, I am a newbie in GPU computing, this is my first time trying to train a model using GPU. Any hint/feedback is very much appreciated.\n\nHappy new year\nCiao", "body": "Hi folks, \r\n\r\nI know this issue has been reported earlier and the common wisdom has converged to decreasing the batch_size. In my current situation, decreasing the batch_size all the way to 1 didn't solve the out of memory runtime error indicating to me that there must be something else that's causing this issue. Here are some relevant information:\r\n\r\n1) I am trying to fine tune pretrained resnet-34 on images with standard 3*224*224 size\r\n2) I was able to accomplish the fine-tuning with a batch-size of 32 on my mac-book air with only 8GB memory.\r\n3) Right now, I am trying to perform the same routine in a p2.xlarge instance on AWS with a 61 GB memory as indicated by AWS [https://aws.amazon.com/ec2/instance-types/p2/](url)\r\n4) I followed this tutorial to set up the AWS EC2 instance: [https://kevinzakka.github.io/2017/08/13/aws-pytorch/](url)\r\n5) I am reproducing the pipeline in this amazing tutorial: [http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html](url)\r\n\r\n6) On the AWS instance: everything seems to be running fine. I was able to run a pre-training inference on the dataset but only after setting volatile =True. If not volatile, the out of memory runtime issue obtains!\r\n7) Here is the error message I obtain irrelevant to the batch_size, I get this error upon trying to train the resnet-34 even with a batch_size of 1:\r\n\r\n```\r\n/home/ubuntu/envs/deepL/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\r\n  \"This module will be removed in 0.20.\", DeprecationWarning)\r\n/home/ubuntu/envs/deepL/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\r\n  DeprecationWarning)\r\nfinetunacuda.py:147: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\r\n  train_props = m(Variable(train_props))\r\nfinetunacuda.py:183: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\r\n  test_props = m(Variable(test_props))\r\nTHCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory\r\nTraceback (most recent call last):\r\n  File \"finetunacuda.py\", line 311, in <module>\r\n    main()\r\n  File \"finetunacuda.py\", line 309, in main\r\n    fine_tuna_protocol()\r\n  File \"finetunacuda.py\", line 287, in fine_tuna_protocol\r\n    model_ft = train_model(dataloaders, dataset_sizes, model_ft, criterion, optimizer_ft, num_epochs = nep, temp_save_name = name_of_results_output_txt_file)\r\n  File \"finetunacuda.py\", line 228, in train_model\r\n    outputs = model(inputs)\r\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torchvision/models/resnet.py\", line 142, in forward\r\n    x = self.maxpool(x)\r\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/modules/pooling.py\", line 143, in forward\r\n    self.return_indices)\r\n  File \"/home/ubuntu/envs/deepL/lib/python3.5/site-packages/torch/nn/functional.py\", line 334, in max_pool2d\r\n    ret = torch._C._nn.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\r\nRuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58\r\n```\r\n8) Here is the gpu in the instance:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   42C    P0    73W / 149W |      0MiB / 11439MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n9) I don't know whether the above information is enough to troubleshoot the issue. Please feel free to ask for any further information. I hope you guys can help with this troubleshooting, I am a newbie in GPU computing, this is my first time trying to train a model using GPU. Any hint/feedback is very much appreciated.\r\n\r\nHappy new year\r\nCiao"}