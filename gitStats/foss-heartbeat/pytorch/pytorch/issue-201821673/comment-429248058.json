{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/429248058", "html_url": "https://github.com/pytorch/pytorch/issues/499#issuecomment-429248058", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/499", "id": 429248058, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTI0ODA1OA==", "user": {"login": "bwesen", "id": 3290190, "node_id": "MDQ6VXNlcjMyOTAxOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/3290190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bwesen", "html_url": "https://github.com/bwesen", "followers_url": "https://api.github.com/users/bwesen/followers", "following_url": "https://api.github.com/users/bwesen/following{/other_user}", "gists_url": "https://api.github.com/users/bwesen/gists{/gist_id}", "starred_url": "https://api.github.com/users/bwesen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bwesen/subscriptions", "organizations_url": "https://api.github.com/users/bwesen/orgs", "repos_url": "https://api.github.com/users/bwesen/repos", "events_url": "https://api.github.com/users/bwesen/events{/privacy}", "received_events_url": "https://api.github.com/users/bwesen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-12T08:30:38Z", "updated_at": "2018-10-12T08:30:38Z", "author_association": "NONE", "body_html": "<p>I was also hit by this breakage, and ended up re-learning CUDA and wrote the kernels I needed myself and added them as pyTorch extensions. This turned out to be far less work than I spent trying to figure out how to mash the other tensor functions together to do the same thing and it was 10-20x faster as well than those functions even while they worked (the im2col-based unfold things). Mainly due to the massive memory usage of the latter (it has to keep an intermediary gigantonourmous array in RAM).</p>\n<p>But for a general case, building up functions to work in a library with multiple frontends and backends, it's much more work than for the very specific fixed kernels I needed. I guess my point being, if you have a specific case that doesn't need flexibility, adding a cuda extension is not as hard as it might seem.</p>", "body_text": "I was also hit by this breakage, and ended up re-learning CUDA and wrote the kernels I needed myself and added them as pyTorch extensions. This turned out to be far less work than I spent trying to figure out how to mash the other tensor functions together to do the same thing and it was 10-20x faster as well than those functions even while they worked (the im2col-based unfold things). Mainly due to the massive memory usage of the latter (it has to keep an intermediary gigantonourmous array in RAM).\nBut for a general case, building up functions to work in a library with multiple frontends and backends, it's much more work than for the very specific fixed kernels I needed. I guess my point being, if you have a specific case that doesn't need flexibility, adding a cuda extension is not as hard as it might seem.", "body": "I was also hit by this breakage, and ended up re-learning CUDA and wrote the kernels I needed myself and added them as pyTorch extensions. This turned out to be far less work than I spent trying to figure out how to mash the other tensor functions together to do the same thing and it was 10-20x faster as well than those functions even while they worked (the im2col-based unfold things). Mainly due to the massive memory usage of the latter (it has to keep an intermediary gigantonourmous array in RAM).\r\n\r\nBut for a general case, building up functions to work in a library with multiple frontends and backends, it's much more work than for the very specific fixed kernels I needed. I guess my point being, if you have a specific case that doesn't need flexibility, adding a cuda extension is not as hard as it might seem.\r\n"}