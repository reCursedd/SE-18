{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430229148", "html_url": "https://github.com/pytorch/pytorch/issues/499#issuecomment-430229148", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/499", "id": 430229148, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDIyOTE0OA==", "user": {"login": "bwesen", "id": 3290190, "node_id": "MDQ6VXNlcjMyOTAxOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/3290190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bwesen", "html_url": "https://github.com/bwesen", "followers_url": "https://api.github.com/users/bwesen/followers", "following_url": "https://api.github.com/users/bwesen/following{/other_user}", "gists_url": "https://api.github.com/users/bwesen/gists{/gist_id}", "starred_url": "https://api.github.com/users/bwesen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bwesen/subscriptions", "organizations_url": "https://api.github.com/users/bwesen/orgs", "repos_url": "https://api.github.com/users/bwesen/repos", "events_url": "https://api.github.com/users/bwesen/events{/privacy}", "received_events_url": "https://api.github.com/users/bwesen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-16T13:04:28Z", "updated_at": "2018-10-16T13:04:28Z", "author_association": "NONE", "body_html": "<p>My conv2d_local function I used before rewriting it in custom CUDA, if anyone is interested. It's actually from an unmerged PR somewhere else here.</p>\n<p>Note that it's <em>extremely</em> slow and memory-inefficient to use im2col. It's surely easy and flexible, but it requires the GPU to unpack all the overlapping kernel possibilities in the input array to an intermediary storage and then apply the weight matrix in that space. Doing it \"in place\" in RAM is about 100x faster (yes, I benchmarked it). For small arrays I'm sure there's much less difference though.</p>\n<pre><code># This is from this unmerged PR:\n# https://github.com/pytorch/pytorch/pull/1583\n\ndef conv2d_local(input, weight, bias=None, padding=0, stride=1, dilation=1):\n    if input.dim() != 4:\n        raise NotImplementedError(\"Input Error: Only 4D input Tensors supported (got {}D)\".format(input.dim()))\n    if weight.dim() != 6:\n        # outH x outW x outC x inC x kH x kW\n        raise NotImplementedError(\"Input Error: Only 6D weight Tensors supported (got {}D)\".format(weight.dim()))\n\n    outH, outW, outC, inC, kH, kW = weight.size()\n    kernel_size = (kH, kW)\n\n    # TODO: this requires a huge amount of temporary RAM, for the unfold and matmul.\n    \n    # N x [inC * kH * kW] x [outH * outW]\n    cols = torch.nn.functional.unfold(input, kernel_size, dilation=dilation, padding=padding, stride=stride)\n    cols = cols.view(cols.size(0), cols.size(1), cols.size(2), 1).permute(0, 2, 3, 1)\n    \n    out = torch.matmul(cols, weight.view(outH * outW, outC, inC * kH * kW).permute(0, 2, 1))\n    out = out.view(cols.size(0), outH, outW, outC).permute(0, 3, 1, 2)\n    \n    if bias is not None:\n        out = out + bias.expand_as(out)\n    return out\n</code></pre>", "body_text": "My conv2d_local function I used before rewriting it in custom CUDA, if anyone is interested. It's actually from an unmerged PR somewhere else here.\nNote that it's extremely slow and memory-inefficient to use im2col. It's surely easy and flexible, but it requires the GPU to unpack all the overlapping kernel possibilities in the input array to an intermediary storage and then apply the weight matrix in that space. Doing it \"in place\" in RAM is about 100x faster (yes, I benchmarked it). For small arrays I'm sure there's much less difference though.\n# This is from this unmerged PR:\n# https://github.com/pytorch/pytorch/pull/1583\n\ndef conv2d_local(input, weight, bias=None, padding=0, stride=1, dilation=1):\n    if input.dim() != 4:\n        raise NotImplementedError(\"Input Error: Only 4D input Tensors supported (got {}D)\".format(input.dim()))\n    if weight.dim() != 6:\n        # outH x outW x outC x inC x kH x kW\n        raise NotImplementedError(\"Input Error: Only 6D weight Tensors supported (got {}D)\".format(weight.dim()))\n\n    outH, outW, outC, inC, kH, kW = weight.size()\n    kernel_size = (kH, kW)\n\n    # TODO: this requires a huge amount of temporary RAM, for the unfold and matmul.\n    \n    # N x [inC * kH * kW] x [outH * outW]\n    cols = torch.nn.functional.unfold(input, kernel_size, dilation=dilation, padding=padding, stride=stride)\n    cols = cols.view(cols.size(0), cols.size(1), cols.size(2), 1).permute(0, 2, 3, 1)\n    \n    out = torch.matmul(cols, weight.view(outH * outW, outC, inC * kH * kW).permute(0, 2, 1))\n    out = out.view(cols.size(0), outH, outW, outC).permute(0, 3, 1, 2)\n    \n    if bias is not None:\n        out = out + bias.expand_as(out)\n    return out", "body": "My conv2d_local function I used before rewriting it in custom CUDA, if anyone is interested. It's actually from an unmerged PR somewhere else here.\r\n\r\nNote that it's *extremely* slow and memory-inefficient to use im2col. It's surely easy and flexible, but it requires the GPU to unpack all the overlapping kernel possibilities in the input array to an intermediary storage and then apply the weight matrix in that space. Doing it \"in place\" in RAM is about 100x faster (yes, I benchmarked it). For small arrays I'm sure there's much less difference though.\r\n\r\n```\r\n# This is from this unmerged PR:\r\n# https://github.com/pytorch/pytorch/pull/1583\r\n\r\ndef conv2d_local(input, weight, bias=None, padding=0, stride=1, dilation=1):\r\n    if input.dim() != 4:\r\n        raise NotImplementedError(\"Input Error: Only 4D input Tensors supported (got {}D)\".format(input.dim()))\r\n    if weight.dim() != 6:\r\n        # outH x outW x outC x inC x kH x kW\r\n        raise NotImplementedError(\"Input Error: Only 6D weight Tensors supported (got {}D)\".format(weight.dim()))\r\n\r\n    outH, outW, outC, inC, kH, kW = weight.size()\r\n    kernel_size = (kH, kW)\r\n\r\n    # TODO: this requires a huge amount of temporary RAM, for the unfold and matmul.\r\n    \r\n    # N x [inC * kH * kW] x [outH * outW]\r\n    cols = torch.nn.functional.unfold(input, kernel_size, dilation=dilation, padding=padding, stride=stride)\r\n    cols = cols.view(cols.size(0), cols.size(1), cols.size(2), 1).permute(0, 2, 3, 1)\r\n    \r\n    out = torch.matmul(cols, weight.view(outH * outW, outC, inC * kH * kW).permute(0, 2, 1))\r\n    out = out.view(cols.size(0), outH, outW, outC).permute(0, 3, 1, 2)\r\n    \r\n    if bias is not None:\r\n        out = out + bias.expand_as(out)\r\n    return out\r\n```\r\n"}