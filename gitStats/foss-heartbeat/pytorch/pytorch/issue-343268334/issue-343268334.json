{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9664", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9664/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9664/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9664/events", "html_url": "https://github.com/pytorch/pytorch/pull/9664", "id": 343268334, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAyOTcyMzg5", "number": 9664, "title": "Extend DispatchStub to support CUDA dispatch", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-20T22:48:19Z", "updated_at": "2018-07-23T20:41:33Z", "closed_at": "2018-07-23T20:41:33Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9664", "html_url": "https://github.com/pytorch/pytorch/pull/9664", "diff_url": "https://github.com/pytorch/pytorch/pull/9664.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9664.patch"}, "body_html": "<p>This is a modification of the strategy from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336004262\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8919\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8919/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8919\">#8919</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"342549661\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9579\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9579/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9579\">#9579</a>.</p>\n<pre><code>Previously, the CPU architecture-specific kernels self-registered with\nthe DispatchStub. When linking as part of a static library, this requires\nthe flag --whole-archive to be passed to the linker to ensure that the\nobject files for the kernels are included. Caffe2 and TensorFlow use that\nstrategy.\n\nWe ran into some issues with --whole-archive blowing up the binary size\nof some downstream projects in Facebook. This PR avoids --whole-archive\nfor CPU kernels. The downside is that the generic code needs to be aware\nof whether kernels are compiled with AVX and with AVX2 (via\nHAVE_AVX_CPU_DEFINITION and HAVE_AVX2_CPU_DEFINITION).\n\nThe CUDA kernels still self-register with DispatchStub because the CPU\nlibrary is not aware of whether the CUDA library will be available at\nruntime.\n\nThere are a few major changes to DispatchStub\n\n - The environment variable ATEN_CPU_CAPABILITY overrides the CPU\n   capability detection code (Previous ATEN_DISABLE_AVX/AVX2)\n\n - DispatchStub is defined in the generic native code instead of the\n   CPU_CAPABILITY_DEFAULT kernel.\n</code></pre>", "body_text": "This is a modification of the strategy from #8919 and #9579.\nPreviously, the CPU architecture-specific kernels self-registered with\nthe DispatchStub. When linking as part of a static library, this requires\nthe flag --whole-archive to be passed to the linker to ensure that the\nobject files for the kernels are included. Caffe2 and TensorFlow use that\nstrategy.\n\nWe ran into some issues with --whole-archive blowing up the binary size\nof some downstream projects in Facebook. This PR avoids --whole-archive\nfor CPU kernels. The downside is that the generic code needs to be aware\nof whether kernels are compiled with AVX and with AVX2 (via\nHAVE_AVX_CPU_DEFINITION and HAVE_AVX2_CPU_DEFINITION).\n\nThe CUDA kernels still self-register with DispatchStub because the CPU\nlibrary is not aware of whether the CUDA library will be available at\nruntime.\n\nThere are a few major changes to DispatchStub\n\n - The environment variable ATEN_CPU_CAPABILITY overrides the CPU\n   capability detection code (Previous ATEN_DISABLE_AVX/AVX2)\n\n - DispatchStub is defined in the generic native code instead of the\n   CPU_CAPABILITY_DEFAULT kernel.", "body": "This is a modification of the strategy from https://github.com/pytorch/pytorch/pull/8919 and https://github.com/pytorch/pytorch/pull/9579.\r\n\r\n```\r\nPreviously, the CPU architecture-specific kernels self-registered with\r\nthe DispatchStub. When linking as part of a static library, this requires\r\nthe flag --whole-archive to be passed to the linker to ensure that the\r\nobject files for the kernels are included. Caffe2 and TensorFlow use that\r\nstrategy.\r\n\r\nWe ran into some issues with --whole-archive blowing up the binary size\r\nof some downstream projects in Facebook. This PR avoids --whole-archive\r\nfor CPU kernels. The downside is that the generic code needs to be aware\r\nof whether kernels are compiled with AVX and with AVX2 (via\r\nHAVE_AVX_CPU_DEFINITION and HAVE_AVX2_CPU_DEFINITION).\r\n\r\nThe CUDA kernels still self-register with DispatchStub because the CPU\r\nlibrary is not aware of whether the CUDA library will be available at\r\nruntime.\r\n\r\nThere are a few major changes to DispatchStub\r\n\r\n - The environment variable ATEN_CPU_CAPABILITY overrides the CPU\r\n   capability detection code (Previous ATEN_DISABLE_AVX/AVX2)\r\n\r\n - DispatchStub is defined in the generic native code instead of the\r\n   CPU_CAPABILITY_DEFAULT kernel.\r\n```"}