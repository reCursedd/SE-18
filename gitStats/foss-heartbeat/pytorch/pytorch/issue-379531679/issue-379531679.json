{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13826", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13826/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13826/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13826/events", "html_url": "https://github.com/pytorch/pytorch/issues/13826", "id": 379531679, "node_id": "MDU6SXNzdWUzNzk1MzE2Nzk=", "number": 13826, "title": "The right way to distribute the training over multiple GPUs and nodes.", "user": {"login": "curry111", "id": 25343445, "node_id": "MDQ6VXNlcjI1MzQzNDQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/25343445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/curry111", "html_url": "https://github.com/curry111", "followers_url": "https://api.github.com/users/curry111/followers", "following_url": "https://api.github.com/users/curry111/following{/other_user}", "gists_url": "https://api.github.com/users/curry111/gists{/gist_id}", "starred_url": "https://api.github.com/users/curry111/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/curry111/subscriptions", "organizations_url": "https://api.github.com/users/curry111/orgs", "repos_url": "https://api.github.com/users/curry111/repos", "events_url": "https://api.github.com/users/curry111/events{/privacy}", "received_events_url": "https://api.github.com/users/curry111/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-11-11T15:10:14Z", "updated_at": "2018-11-14T02:23:51Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi guys <g-emoji class=\"g-emoji\" alias=\"relaxed\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/263a.png\">\u263a\ufe0f</g-emoji> I read a lot about how to distribute my training over multiple GPUs and nodes in pytorch Documentations. And I am confused, I had to read about multiprocessing in general(because i didn't have any knowledge about it <g-emoji class=\"g-emoji\" alias=\"sweat_smile\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f605.png\">\ud83d\ude05</g-emoji>) I don't know if my code is right or not but every time i run it i see error<br>\n<code>THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/THC/THCTensorRandom.cu line=25 error=46 : all CUDA-capable devices are busy or unavailable </code><br>\nAnd<br>\n<code>RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/THC/THCTensorRandom.cu:25 </code></p>\n<pre><code>def run(rank, size):\n    device = torch.device(\"cuda\")\n    dataset = datasets.MNIST('./data', train=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))]))\n    # from torch.utils.data.distributed import DistributedSampler\n    train_sampler = DistributedSampler(dataset)\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n                                                                       num_workers=4,\n                                                                       pin_memory=True, \n                                                                       sampler=train_sampler)\n    torch.manual_seed(1234)\n\n    model = Net().cuda()\n    model = torch.nn.parallel.DistributedDataParallel(model)\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    for epoch in range(10):\n        epoch_loss = 0.0\n        train_sampler.set_epoch(epoch)\n        for data, target in train_loader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)\n\n</code></pre>\n<h3>Initializing processes group</h3>\n<pre><code>def init_processes(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n</code></pre>\n<h3>Creating 4 processes (for 2 nodes and each node has 2 GPUs)</h3>\n<pre><code>size = 4\nprocesses = []\nfor rank in range(size):\n    p = Process(target=init_processes, args=(rank, size, run))\n    processes.append(p)\n    p.start()\n</code></pre>\n<p>I also read <a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">distributed imagenet example</a> to try to do the same.<br>\nIs that the correct way of using distributed training and DistributedDataParallel() <g-emoji class=\"g-emoji\" alias=\"sweat_smile\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f605.png\">\ud83d\ude05</g-emoji>?</p>\n<p>PS: btw the first error appears 3 times the second error appears 4 times(I only took the last line of the second error). Both errors appear at the same time<br>\nI also tried to google that error but I could not use it to fix my error</p>", "body_text": "Hi guys \u263a\ufe0f I read a lot about how to distribute my training over multiple GPUs and nodes in pytorch Documentations. And I am confused, I had to read about multiprocessing in general(because i didn't have any knowledge about it \ud83d\ude05) I don't know if my code is right or not but every time i run it i see error\nTHCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/THC/THCTensorRandom.cu line=25 error=46 : all CUDA-capable devices are busy or unavailable \nAnd\nRuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/THC/THCTensorRandom.cu:25 \ndef run(rank, size):\n    device = torch.device(\"cuda\")\n    dataset = datasets.MNIST('./data', train=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))]))\n    # from torch.utils.data.distributed import DistributedSampler\n    train_sampler = DistributedSampler(dataset)\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n                                                                       num_workers=4,\n                                                                       pin_memory=True, \n                                                                       sampler=train_sampler)\n    torch.manual_seed(1234)\n\n    model = Net().cuda()\n    model = torch.nn.parallel.DistributedDataParallel(model)\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    for epoch in range(10):\n        epoch_loss = 0.0\n        train_sampler.set_epoch(epoch)\n        for data, target in train_loader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)\n\n\nInitializing processes group\ndef init_processes(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\nCreating 4 processes (for 2 nodes and each node has 2 GPUs)\nsize = 4\nprocesses = []\nfor rank in range(size):\n    p = Process(target=init_processes, args=(rank, size, run))\n    processes.append(p)\n    p.start()\n\nI also read distributed imagenet example to try to do the same.\nIs that the correct way of using distributed training and DistributedDataParallel() \ud83d\ude05?\nPS: btw the first error appears 3 times the second error appears 4 times(I only took the last line of the second error). Both errors appear at the same time\nI also tried to google that error but I could not use it to fix my error", "body": "Hi guys :relaxed: I read a lot about how to distribute my training over multiple GPUs and nodes in pytorch Documentations. And I am confused, I had to read about multiprocessing in general(because i didn't have any knowledge about it :sweat_smile:) I don't know if my code is right or not but every time i run it i see error\r\n`THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/THC/THCTensorRandom.cu line=25 error=46 : all CUDA-capable devices are busy or unavailable\r\n`\r\nAnd\r\n`RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/THC/THCTensorRandom.cu:25\r\n`\r\n```\r\ndef run(rank, size):\r\n    device = torch.device(\"cuda\")\r\n    dataset = datasets.MNIST('./data', train=True,\r\n                             transform=transforms.Compose([\r\n                                 transforms.ToTensor(),\r\n                                 transforms.Normalize((0.1307,), (0.3081,))]))\r\n    # from torch.utils.data.distributed import DistributedSampler\r\n    train_sampler = DistributedSampler(dataset)\r\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \r\n                                                                       num_workers=4,\r\n                                                                       pin_memory=True, \r\n                                                                       sampler=train_sampler)\r\n    torch.manual_seed(1234)\r\n\r\n    model = Net().cuda()\r\n    model = torch.nn.parallel.DistributedDataParallel(model)\r\n    optimizer = optim.SGD(model.parameters(),\r\n                          lr=0.01, momentum=0.5)\r\n\r\n    for epoch in range(10):\r\n        epoch_loss = 0.0\r\n        train_sampler.set_epoch(epoch)\r\n        for data, target in train_loader:\r\n            data, target = data.to(device), target.to(device)\r\n            optimizer.zero_grad()\r\n            output = model(data)\r\n            loss = F.nll_loss(output, target)\r\n            epoch_loss += loss.item()\r\n            loss.backward()\r\n            optimizer.step()\r\n        print('Rank ', dist.get_rank(), ', epoch ',\r\n              epoch, ': ', epoch_loss / num_batches)\r\n\r\n```\r\n### Initializing processes group \r\n```\r\ndef init_processes(rank, size, fn, backend='gloo'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\n```\r\n### Creating 4 processes (for 2 nodes and each node has 2 GPUs)\r\n```\r\nsize = 4\r\nprocesses = []\r\nfor rank in range(size):\r\n    p = Process(target=init_processes, args=(rank, size, run))\r\n    processes.append(p)\r\n    p.start()\r\n```\r\nI also read [distributed imagenet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py) to try to do the same.\r\nIs that the correct way of using distributed training and DistributedDataParallel() :sweat_smile:?\r\n\r\nPS: btw the first error appears 3 times the second error appears 4 times(I only took the last line of the second error). Both errors appear at the same time\r\nI also tried to google that error but I could not use it to fix my error"}