{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419799406", "html_url": "https://github.com/pytorch/pytorch/pull/11292#issuecomment-419799406", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11292", "id": 419799406, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTc5OTQwNg==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-10T06:13:29Z", "updated_at": "2018-09-10T06:35:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20233731\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mingfeima\">@mingfeima</a>  Thank you for the detailed comments!<br>\nI think I need some help more detail to follow your suggestions:</p>\n<ol>\n<li>Use mkl to perform gemm means that I use your batched gemm? I have added that locally, but the performance seems bad compared to spelling out the matrix multiplication when I have very small matrices (as in fritzo's benchmark).</li>\n<li>for non-mkl, I'm not sure how to avoid parallel for (though using Parallel.h is much better than trying to do this myself, thanks for pointing that out!).</li>\n<li>my impression was that CPU is for specialized, accellerated (like AVX) code and \"generic\" CPU code is in native (e.g. Embedding, ...)</li>\n<li>I think I have that locally. :)</li>\n</ol>\n<p>I currently run this on my laptop (Thinkpad with Intel Core i7-5600U on Linux), I could move it to my GPU host (i5-7500).</p>\n<p><strong>Update:</strong></p>\n<p>So on the i7-5600U, the modified benchmark by fritzo gives me</p>\n<pre><code>torch: 32.022253560018726\nnumpy: 0.017791966965887696\ntorch.bmm: 31.382055553025566\n</code></pre>\n<p>for plain <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357901574\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11365\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11365/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11365\">#11365</a> . It seems that batched gemm only is advantageous for larger matrices, indeed with 2000 200x200 / 200x1 matrices, the torch's MKL bmm outperforms numpy (numpy is stock from Debian).</p>\n<p>For reference:</p>\n<pre><code>print (torch.__version__)\nx = torch.randn((2, 2000))\ny = torch.randn((2, 2, 2000))\nxp = x.permute(1, 0).view(2000, 1, 2)\nyp = y.permute(2, 0, 1)\nequation = 'ac,abc-&gt;cb'\n\ntime0 = timeit.default_timer()\nfor _ in range(1000):\n    _ = torch.einsum(equation, [x, y])\n\ntime1 = timeit.default_timer()\nfor _ in range(1000):\n    _ = numpy.einsum(equation, x.numpy(), y.numpy())\n\ntime2 = timeit.default_timer()\n\nfor _ in range(1000):\n    _ = torch.bmm(xp, yp)\ntime3 = timeit.default_timer()\n\nprint((torch.einsum(equation, [x, y])-torch.bmm(xp, yp).squeeze()).abs().max().item())\n\nprint('torch: {}'.format(time1 - time0))\nprint('numpy: {}'.format(time2 - time1))\nprint('torch.bmm: {}'.format(time3 - time2))\n</code></pre>", "body_text": "@mingfeima  Thank you for the detailed comments!\nI think I need some help more detail to follow your suggestions:\n\nUse mkl to perform gemm means that I use your batched gemm? I have added that locally, but the performance seems bad compared to spelling out the matrix multiplication when I have very small matrices (as in fritzo's benchmark).\nfor non-mkl, I'm not sure how to avoid parallel for (though using Parallel.h is much better than trying to do this myself, thanks for pointing that out!).\nmy impression was that CPU is for specialized, accellerated (like AVX) code and \"generic\" CPU code is in native (e.g. Embedding, ...)\nI think I have that locally. :)\n\nI currently run this on my laptop (Thinkpad with Intel Core i7-5600U on Linux), I could move it to my GPU host (i5-7500).\nUpdate:\nSo on the i7-5600U, the modified benchmark by fritzo gives me\ntorch: 32.022253560018726\nnumpy: 0.017791966965887696\ntorch.bmm: 31.382055553025566\n\nfor plain #11365 . It seems that batched gemm only is advantageous for larger matrices, indeed with 2000 200x200 / 200x1 matrices, the torch's MKL bmm outperforms numpy (numpy is stock from Debian).\nFor reference:\nprint (torch.__version__)\nx = torch.randn((2, 2000))\ny = torch.randn((2, 2, 2000))\nxp = x.permute(1, 0).view(2000, 1, 2)\nyp = y.permute(2, 0, 1)\nequation = 'ac,abc->cb'\n\ntime0 = timeit.default_timer()\nfor _ in range(1000):\n    _ = torch.einsum(equation, [x, y])\n\ntime1 = timeit.default_timer()\nfor _ in range(1000):\n    _ = numpy.einsum(equation, x.numpy(), y.numpy())\n\ntime2 = timeit.default_timer()\n\nfor _ in range(1000):\n    _ = torch.bmm(xp, yp)\ntime3 = timeit.default_timer()\n\nprint((torch.einsum(equation, [x, y])-torch.bmm(xp, yp).squeeze()).abs().max().item())\n\nprint('torch: {}'.format(time1 - time0))\nprint('numpy: {}'.format(time2 - time1))\nprint('torch.bmm: {}'.format(time3 - time2))", "body": "@mingfeima  Thank you for the detailed comments!\r\nI think I need some help more detail to follow your suggestions:\r\n1. Use mkl to perform gemm means that I use your batched gemm? I have added that locally, but the performance seems bad compared to spelling out the matrix multiplication when I have very small matrices (as in fritzo's benchmark).\r\n2. for non-mkl, I'm not sure how to avoid parallel for (though using Parallel.h is much better than trying to do this myself, thanks for pointing that out!).\r\n3. my impression was that CPU is for specialized, accellerated (like AVX) code and \"generic\" CPU code is in native (e.g. Embedding, ...)\r\n4. I think I have that locally. :)\r\n\r\nI currently run this on my laptop (Thinkpad with Intel Core i7-5600U on Linux), I could move it to my GPU host (i5-7500).\r\n\r\n**Update:**\r\n\r\nSo on the i7-5600U, the modified benchmark by fritzo gives me\r\n```\r\ntorch: 32.022253560018726\r\nnumpy: 0.017791966965887696\r\ntorch.bmm: 31.382055553025566\r\n```\r\nfor plain #11365 . It seems that batched gemm only is advantageous for larger matrices, indeed with 2000 200x200 / 200x1 matrices, the torch's MKL bmm outperforms numpy (numpy is stock from Debian).\r\n\r\nFor reference:\r\n```\r\nprint (torch.__version__)\r\nx = torch.randn((2, 2000))\r\ny = torch.randn((2, 2, 2000))\r\nxp = x.permute(1, 0).view(2000, 1, 2)\r\nyp = y.permute(2, 0, 1)\r\nequation = 'ac,abc->cb'\r\n\r\ntime0 = timeit.default_timer()\r\nfor _ in range(1000):\r\n    _ = torch.einsum(equation, [x, y])\r\n\r\ntime1 = timeit.default_timer()\r\nfor _ in range(1000):\r\n    _ = numpy.einsum(equation, x.numpy(), y.numpy())\r\n\r\ntime2 = timeit.default_timer()\r\n\r\nfor _ in range(1000):\r\n    _ = torch.bmm(xp, yp)\r\ntime3 = timeit.default_timer()\r\n\r\nprint((torch.einsum(equation, [x, y])-torch.bmm(xp, yp).squeeze()).abs().max().item())\r\n\r\nprint('torch: {}'.format(time1 - time0))\r\nprint('numpy: {}'.format(time2 - time1))\r\nprint('torch.bmm: {}'.format(time3 - time2))\r\n```\r\n"}