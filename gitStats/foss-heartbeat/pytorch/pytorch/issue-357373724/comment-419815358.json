{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419815358", "html_url": "https://github.com/pytorch/pytorch/pull/11292#issuecomment-419815358", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11292", "id": 419815358, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTgxNTM1OA==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-10T07:29:41Z", "updated_at": "2018-09-10T13:30:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> <del>perhaps you will get different result on Xeon.<br>\nI test benchmarks on server CPUs, a.k.a. Xeon. And the current most highend model is skylake 8180 with 56 cores. Desktop CPU is actually not the major optimization target of Intel. I suggest you try Xeon, they are available on clouds, e.g. AWS, etc.</del></p>\n<p>Sorry for the misleading comment. the problem size with <code>einsum</code> here is much smaller than my case.<br>\nmy problem size is <code>batch_size</code> around <code>64</code> and <code>gemm</code> size  around <code>1*50*800</code>.  This type of scenario is perfect for <code>batched gemm</code>.</p>\n<p>The problem size with <code>einsum</code> is <code>batch_size</code> 2000 and <code>gemm</code> <code>1*2*2</code>, using batch gemm will not help for it, <code>1*2*2</code> should not go to gemm at all, this is too small. Using single thread vectorized code should have the best performance.  And this is not related to CPU model, i see similar result on Xeon as well.</p>\n<p>Mathematically, they are both <code>bmm</code>, but from optimization point of view, they are actually two diverse problems.</p>\n<p>Any information on typical <code>einsum</code> problem size? Is the gemm always <code>1*2*2</code>? Need to figure out how can we come up with a better solution.</p>\n<p>First thought here is that</p>\n<ol>\n<li>put a threshold on <code>MNK</code>. For example, if <code>MNK</code> &gt; <code>16*16*16</code>, go mkl batched gemm, otherwise go the manually written one as proposed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a></li>\n<li>as for the manually written one, the threshold for OpenMP parallelization needs to be tuned a little bit. From my experience, good candidates for the threshold is probably 5000~20000. Because computation within each thread is really small, so you will only see performance gain with OpenMP parallelization only if you have enough works.</li>\n<li>if this can't get satisfied performance for <code>einsum</code>, perhaps need to consider to isolate <code>einsum</code> from <code>bmm</code> and use specific vectorized CPU kernels.</li>\n</ol>", "body_text": "@t-vi perhaps you will get different result on Xeon.\nI test benchmarks on server CPUs, a.k.a. Xeon. And the current most highend model is skylake 8180 with 56 cores. Desktop CPU is actually not the major optimization target of Intel. I suggest you try Xeon, they are available on clouds, e.g. AWS, etc.\nSorry for the misleading comment. the problem size with einsum here is much smaller than my case.\nmy problem size is batch_size around 64 and gemm size  around 1*50*800.  This type of scenario is perfect for batched gemm.\nThe problem size with einsum is batch_size 2000 and gemm 1*2*2, using batch gemm will not help for it, 1*2*2 should not go to gemm at all, this is too small. Using single thread vectorized code should have the best performance.  And this is not related to CPU model, i see similar result on Xeon as well.\nMathematically, they are both bmm, but from optimization point of view, they are actually two diverse problems.\nAny information on typical einsum problem size? Is the gemm always 1*2*2? Need to figure out how can we come up with a better solution.\nFirst thought here is that\n\nput a threshold on MNK. For example, if MNK > 16*16*16, go mkl batched gemm, otherwise go the manually written one as proposed by @t-vi\nas for the manually written one, the threshold for OpenMP parallelization needs to be tuned a little bit. From my experience, good candidates for the threshold is probably 5000~20000. Because computation within each thread is really small, so you will only see performance gain with OpenMP parallelization only if you have enough works.\nif this can't get satisfied performance for einsum, perhaps need to consider to isolate einsum from bmm and use specific vectorized CPU kernels.", "body": "@t-vi ~~perhaps you will get different result on Xeon.\r\nI test benchmarks on server CPUs, a.k.a. Xeon. And the current most highend model is skylake 8180 with 56 cores. Desktop CPU is actually not the major optimization target of Intel. I suggest you try Xeon, they are available on clouds, e.g. AWS, etc.~~\r\n\r\nSorry for the misleading comment. the problem size with `einsum` here is much smaller than my case.\r\nmy problem size is `batch_size` around `64` and `gemm` size  around `1*50*800`.  This type of scenario is perfect for `batched gemm`. \r\n\r\nThe problem size with `einsum` is `batch_size` 2000 and `gemm` `1*2*2`, using batch gemm will not help for it, `1*2*2` should not go to gemm at all, this is too small. Using single thread vectorized code should have the best performance.  And this is not related to CPU model, i see similar result on Xeon as well.\r\n\r\nMathematically, they are both `bmm`, but from optimization point of view, they are actually two diverse problems.\r\n\r\nAny information on typical `einsum` problem size? Is the gemm always `1*2*2`? Need to figure out how can we come up with a better solution.\r\n\r\nFirst thought here is that\r\n\r\n1. put a threshold on `MNK`. For example, if `MNK` > `16*16*16`, go mkl batched gemm, otherwise go the manually written one as proposed by @t-vi \r\n2. as for the manually written one, the threshold for OpenMP parallelization needs to be tuned a little bit. From my experience, good candidates for the threshold is probably 5000~20000. Because computation within each thread is really small, so you will only see performance gain with OpenMP parallelization only if you have enough works.\r\n3. if this can't get satisfied performance for `einsum`, perhaps need to consider to isolate `einsum` from `bmm` and use specific vectorized CPU kernels.\r\n\r\n"}