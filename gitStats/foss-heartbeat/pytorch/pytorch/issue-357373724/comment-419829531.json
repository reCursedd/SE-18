{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419829531", "html_url": "https://github.com/pytorch/pytorch/pull/11292#issuecomment-419829531", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11292", "id": 419829531, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTgyOTUzMQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-10T08:25:23Z", "updated_at": "2018-09-10T14:20:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20233731\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mingfeima\">@mingfeima</a> OK, I'll see to not benchmarking MKL on my hardware.</p>\n<p>In your opinion, on the Xeon hardware that is the optimization focus of MKL, should MKL batch gemm be unconditionally used or should it depend on matrix size?</p>\n<p>Edit: Thank you for your input! I'll use <code>m*k*n &lt; 400</code> for now (a factor ~10 off your suggestion, but I hope it is OK and I MKL seems to work great for that). I also changed to parallel_for and and specified grain size with <code>MIN_GRAIN/(m*k*n)</code> rather than meddling with OMP myself.</p>", "body_text": "@mingfeima OK, I'll see to not benchmarking MKL on my hardware.\nIn your opinion, on the Xeon hardware that is the optimization focus of MKL, should MKL batch gemm be unconditionally used or should it depend on matrix size?\nEdit: Thank you for your input! I'll use m*k*n < 400 for now (a factor ~10 off your suggestion, but I hope it is OK and I MKL seems to work great for that). I also changed to parallel_for and and specified grain size with MIN_GRAIN/(m*k*n) rather than meddling with OMP myself.", "body": "@mingfeima OK, I'll see to not benchmarking MKL on my hardware. \r\n\r\nIn your opinion, on the Xeon hardware that is the optimization focus of MKL, should MKL batch gemm be unconditionally used or should it depend on matrix size?\r\n\r\nEdit: Thank you for your input! I'll use `m*k*n < 400` for now (a factor ~10 off your suggestion, but I hope it is OK and I MKL seems to work great for that). I also changed to parallel_for and and specified grain size with `MIN_GRAIN/(m*k*n)` rather than meddling with OMP myself."}