{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419771940", "html_url": "https://github.com/pytorch/pytorch/pull/11292#issuecomment-419771940", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11292", "id": 419771940, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTc3MTk0MA==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-10T02:46:02Z", "updated_at": "2018-09-10T02:51:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>  fold <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357901574\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11365\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11365/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11365\">#11365</a> would be better :)<br>\nsorry, i wasn't looking at <code>baddbmm</code> at the beginning, it's better to have it optimized with <code>bmm</code>.<br>\ni have been looking at <a href=\"https://github.com/OpenNMT/OpenNMT-py\">opennmt</a>, <code>bmm</code> is used quite a lot in global attention computation.</p>\n<p>from the optimization point of view, the optimization can be done in two manners.</p>\n<ol>\n<li>mkl batched gemm: as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357901574\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11365\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11365/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11365\">#11365</a></li>\n<li>OpenMP + sequential gemm: do OpenMP parallelization across <code>batch_size</code> dimension and call sequential MKL gemm.</li>\n</ol>\n<p>i actually did both. Performance gain from mkl batched gemm is only mediocre. <code>openmp</code> + <code>sequential gemm</code> actually performs much better than <code>mkl batched gemm</code> on unit benchmark. But once tested from PyTorch, it is not that good, needs further refinement with pytorch threading model here.</p>\n<p>anyway, at this moment it is fair enough to stick <code>mkl batched gemm</code> for this PR. I checked implementations from other major frameworks, no better approach found.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> a few suggestions on this PR</p>\n<ol>\n<li>use mkl to perform gemm. gemm is actually quite a complex topic, you have to deal with blocking, vectorization, etc.</li>\n<li>i suppose you don't have to explicitly use <code>#pragma omp parallel for</code> now. Even if you need, use <code>parallel_for</code> wrapper from <code>/aten/src/ATen/Parallel.h</code> would be probably a better idea.</li>\n<li><code>aten/src/ATen/native/LinearAlgebra.cpp</code> is kind of like a unified entrance for all backends. So put mkl related functions under <code>aten/src/ATen/native/mkl/</code> would be better. And you can guard compilation with <code>AT_MKL_ENABLED()</code> and guard runtime selection with <code>at::hasMKL()</code>. Similarly, in case i figure out how to fix <code>openmp</code> + <code>sequential gemm</code> issue, it should go to <code>aten/src/ATen/native/cpu</code>.</li>\n<li><code>baddbmm</code> and <code>bmm</code> can share the same underlying mkl kernels. <code>bmm</code> is a specialization of <code>baddbmm</code> when <code>alpha</code> = 1 and <code>beta</code> = 0.</li>\n</ol>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> one more question, what type of CPU are you using?</p>", "body_text": "@t-vi @soumith  fold #11365 would be better :)\nsorry, i wasn't looking at baddbmm at the beginning, it's better to have it optimized with bmm.\ni have been looking at opennmt, bmm is used quite a lot in global attention computation.\nfrom the optimization point of view, the optimization can be done in two manners.\n\nmkl batched gemm: as #11365\nOpenMP + sequential gemm: do OpenMP parallelization across batch_size dimension and call sequential MKL gemm.\n\ni actually did both. Performance gain from mkl batched gemm is only mediocre. openmp + sequential gemm actually performs much better than mkl batched gemm on unit benchmark. But once tested from PyTorch, it is not that good, needs further refinement with pytorch threading model here.\nanyway, at this moment it is fair enough to stick mkl batched gemm for this PR. I checked implementations from other major frameworks, no better approach found.\n@t-vi a few suggestions on this PR\n\nuse mkl to perform gemm. gemm is actually quite a complex topic, you have to deal with blocking, vectorization, etc.\ni suppose you don't have to explicitly use #pragma omp parallel for now. Even if you need, use parallel_for wrapper from /aten/src/ATen/Parallel.h would be probably a better idea.\naten/src/ATen/native/LinearAlgebra.cpp is kind of like a unified entrance for all backends. So put mkl related functions under aten/src/ATen/native/mkl/ would be better. And you can guard compilation with AT_MKL_ENABLED() and guard runtime selection with at::hasMKL(). Similarly, in case i figure out how to fix openmp + sequential gemm issue, it should go to aten/src/ATen/native/cpu.\nbaddbmm and bmm can share the same underlying mkl kernels. bmm is a specialization of baddbmm when alpha = 1 and beta = 0.\n\n@t-vi one more question, what type of CPU are you using?", "body": "@t-vi @soumith  fold #11365 would be better :)\r\nsorry, i wasn't looking at `baddbmm` at the beginning, it's better to have it optimized with `bmm`.\r\ni have been looking at [opennmt](https://github.com/OpenNMT/OpenNMT-py), `bmm` is used quite a lot in global attention computation.\r\n\r\nfrom the optimization point of view, the optimization can be done in two manners.\r\n\r\n1. mkl batched gemm: as #11365 \r\n2. OpenMP + sequential gemm: do OpenMP parallelization across `batch_size` dimension and call sequential MKL gemm.\r\n\r\ni actually did both. Performance gain from mkl batched gemm is only mediocre. `openmp` + `sequential gemm` actually performs much better than `mkl batched gemm` on unit benchmark. But once tested from PyTorch, it is not that good, needs further refinement with pytorch threading model here.\r\n\r\nanyway, at this moment it is fair enough to stick `mkl batched gemm` for this PR. I checked implementations from other major frameworks, no better approach found.\r\n\r\n@t-vi a few suggestions on this PR\r\n\r\n1. use mkl to perform gemm. gemm is actually quite a complex topic, you have to deal with blocking, vectorization, etc.\r\n2. i suppose you don't have to explicitly use `#pragma omp parallel for` now. Even if you need, use `parallel_for` wrapper from `/aten/src/ATen/Parallel.h` would be probably a better idea.\r\n3. `aten/src/ATen/native/LinearAlgebra.cpp` is kind of like a unified entrance for all backends. So put mkl related functions under `aten/src/ATen/native/mkl/` would be better. And you can guard compilation with `AT_MKL_ENABLED()` and guard runtime selection with `at::hasMKL()`. Similarly, in case i figure out how to fix `openmp` + `sequential gemm` issue, it should go to `aten/src/ATen/native/cpu`.\r\n 4. `baddbmm` and `bmm` can share the same underlying mkl kernels. `bmm` is a specialization of `baddbmm` when `alpha` = 1 and `beta` = 0. \r\n\r\n@t-vi one more question, what type of CPU are you using?"}