{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216147869", "pull_request_review_id": 153572109, "id": 216147869, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjE0Nzg2OQ==", "diff_hunk": "@@ -222,6 +228,151 @@ Tensor& addr_out(Tensor &result, const Tensor& self, const Tensor& vec1, const T\n   return at::_addr_out(result, self, vec1, vec2, beta, alpha);\n }\n \n+template <typename scalar_t>\n+inline void bmm_cpu_kernel(const Tensor& result, const Tensor& self, const Tensor& mat2) {\n+  int64_t bs = result.size(0);\n+  int64_t is = result.size(1);\n+  int64_t js = result.size(2);\n+  int64_t ks = self.size(2);\n+\n+  auto r0 = result.accessor<scalar_t, 3>();\n+  auto s0 = self.accessor<scalar_t, 3>();\n+  auto m0 = mat2.accessor<scalar_t, 3>();\n+\n+#ifdef _OPENMP\n+  #pragma omp parallel for if(bs > 100) // or small ks?\n+#endif\n+  for (int64_t b = 0; b < bs; b++) {\n+    auto r1 = r0[b];\n+    auto s1 = s0[b];\n+    auto m1 = m0[b];\n+    for (int64_t i = 0; i < is; i++) {\n+      auto r2 = r1[i];\n+      auto s2 = s1[i];\n+      for (int64_t j = 0; j < js; j++) {\n+        scalar_t &r = r2[j];\n+\tr = 0;\n+        for (int64_t k = 0; k < ks; k++) {\n+          r += s2[k] * m1[k][j];\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline void baddbmm_cpu_kernel(const Tensor& result, const Tensor& self, const Tensor& mat2, Scalar beta_, Scalar alpha_) {\n+  int64_t bs = result.size(0);\n+  int64_t is = result.size(1);\n+  int64_t js = result.size(2);\n+  int64_t ks = self.size(2);\n+\n+  scalar_t alpha = alpha_.to<scalar_t>();\n+  scalar_t beta = beta_.to<scalar_t>();\n+\n+  auto r0 = result.accessor<scalar_t, 3>();\n+  auto s0 = self.accessor<scalar_t, 3>();\n+  auto m0 = mat2.accessor<scalar_t, 3>();\n+\n+#ifdef _OPENMP\n+  #pragma omp parallel for if(bs > 100) // or small ks?\n+#endif\n+  for (int64_t b = 0; b < bs; b++) {\n+    auto r1 = r0[b];\n+    auto s1 = s0[b];\n+    auto m1 = m0[b];\n+    for (int64_t i = 0; i < is; i++) {\n+      auto r2 = r1[i];\n+      auto s2 = s1[i];\n+      for (int64_t j = 0; j < js; j++) {\n+        scalar_t &r = r2[j];\n+\tr *= beta;\n+        for (int64_t k = 0; k < ks; k++) {\n+          r += alpha * s2[k] * m1[k][j];\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+static inline Tensor& bmm_out_or_baddbmm_(Tensor& self_or_result, const Tensor& batch1, const Tensor& batch2, Scalar beta, Scalar alpha, bool is_bmm_out) {\n+  // is_bmm_out: true for bmm_out, false for baddbmm_\n+  // self_or_result is \"self\" for baddbmm_ and \"result\" for bmm_out\n+  CheckedFrom c = (is_bmm_out ? \"bmm\" : \"baddbmm\");\n+  TensorArg self_arg(self_or_result, is_bmm_out ? \"self\" : \"result\", 0);\n+  TensorArg b1_arg(batch1, \"batch1\", 1);\n+  TensorArg b2_arg(batch2, \"batch2\", 2);\n+  checkDim(c, b1_arg, 3);\n+  checkDim(c, b2_arg, 3);\n+\n+  int64_t bs = batch1.size(0);\n+  checkSize(c, b2_arg, 0, bs);\n+  int64_t contraction_size = batch1.size(2);\n+  int64_t res_rows = batch1.size(1);\n+  int64_t res_cols = batch2.size(2);\n+  checkSize(c, b2_arg, 1, contraction_size); // or have custom messages?\n+\n+  if (is_bmm_out) {\n+    self_or_result.resize_({bs, res_rows, res_cols});\n+  } else {\n+    checkSize(c, self_arg, 0, bs);\n+    checkSize(c, self_arg, 1, res_rows);\n+    checkSize(c, self_arg, 2, res_cols);\n+  }\n+\n+  if (bs > contraction_size * 10) { // better criterion?\n+    if (is_bmm_out) {\n+      AT_DISPATCH_ALL_TYPES(batch1.type(), \"bmm\", [&] {\n+\t  bmm_cpu_kernel<scalar_t>(self_or_result, batch1, batch2);\n+\t});\n+    } else {\n+      AT_DISPATCH_ALL_TYPES(batch1.type(), \"baddbmm\", [&] {\n+\t  baddbmm_cpu_kernel<scalar_t>(self_or_result, batch1, batch2, beta, alpha);\n+\t});\n+    }\n+  } else { // split along batch dimension\n+    if (is_bmm_out) {\n+      for (int64_t b = 0; b < bs; b++) {\n+\tauto r = self_or_result.select(0, b);", "path": "aten/src/ATen/native/LinearAlgebra.cpp", "position": null, "original_position": 127, "commit_id": "4bb9a0670313152916955add409d4d9798137f85", "original_commit_id": "4e13b9dd2a55ceddd8a835852500b6c2e92d5b51", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "nit: misdented", "created_at": "2018-09-09T05:46:28Z", "updated_at": "2018-11-23T15:50:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/11292#discussion_r216147869", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11292", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216147869"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11292#discussion_r216147869"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11292"}}, "body_html": "<p>nit: misdented</p>", "body_text": "nit: misdented"}