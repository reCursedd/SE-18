{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155939275", "pull_request_review_id": 82342369, "id": 155939275, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTkzOTI3NQ==", "diff_hunk": "@@ -266,30 +266,31 @@ def repeat(self, *sizes):\n         # If args == (torch.Size,), then we need to unpack the tuple\n         if len(sizes) == 1 and isinstance(sizes[0], torch.Size):\n             sizes = sizes[0]\n+\n         repeats = list(sizes)\n-        result = self.new()\n-        src = self.contiguous()\n \n-        if len(repeats) < src.dim():\n+        if len(repeats) < self.dim():\n             raise ValueError('Number of dimensions of repeat dims can not be '\n                              'smaller than number of dimensions of tensor')\n \n-        xtensor = src.new().set_(src)\n-        xsize = list(xtensor.size())\n-        for i in _range(len(repeats) - src.dim()):\n-            xsize = [1] + xsize\n+        # Add new leading dimensions to the tensor if the\n+        # number of target dimensions is larger than the\n+        # number of source dimensions.\n+        num_new_dimensions = len(repeats) - self.dim()\n+        padded_size = [1] * num_new_dimensions + list(self.size())\n+        target_size = torch.Size([a * b for a, b in zip(padded_size, repeats)])\n+\n+        xtensor = self.new().set_(self)\n+        xtensor = xtensor.expand(padded_size)\n \n-        size = torch.Size([a * b for a, b in zip(xsize, repeats)])\n-        xtensor.resize_(torch.Size(xsize))\n-        result.resize_(size)\n+        result = self.new()\n+        result.resize_(target_size)\n         urtensor = result.new(result)\n         for i in _range(xtensor.dim()):\n             urtensor = urtensor.unfold(i, xtensor.size(i), xtensor.size(i))\n-        for i in _range(urtensor.dim() - xtensor.dim()):\n-            xsize = [1] + xsize\n-        xtensor.resize_(torch.Size(xsize))\n-        xxtensor = xtensor.expand_as(urtensor)\n-        urtensor.copy_(xxtensor)\n+\n+        urtensor.copy_(xtensor.expand_as(urtensor))", "path": "torch/tensor.py", "position": 42, "original_position": 42, "commit_id": "0570712b68d7b4cc5b3150e819ca305fbc98d8e3", "original_commit_id": "983e676e2cef1a8f26fcb8bf0ffb492ef310a816", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Is this `expand_as` really needed? I'm not 100% how these unfolds let us achieve repeating, but it seems that there's no guarantee that `xtensor` is contiguous, so if this wasn't a no-op I would expect it to break.", "created_at": "2017-12-10T06:16:06Z", "updated_at": "2018-11-23T15:37:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/4084#discussion_r155939275", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4084", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155939275"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4084#discussion_r155939275"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4084"}}, "body_html": "<p>Is this <code>expand_as</code> really needed? I'm not 100% how these unfolds let us achieve repeating, but it seems that there's no guarantee that <code>xtensor</code> is contiguous, so if this wasn't a no-op I would expect it to break.</p>", "body_text": "Is this expand_as really needed? I'm not 100% how these unfolds let us achieve repeating, but it seems that there's no guarantee that xtensor is contiguous, so if this wasn't a no-op I would expect it to break."}