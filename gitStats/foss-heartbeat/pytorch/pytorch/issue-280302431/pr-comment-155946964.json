{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155946964", "pull_request_review_id": 82349678, "id": 155946964, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTk0Njk2NA==", "diff_hunk": "@@ -266,30 +266,31 @@ def repeat(self, *sizes):\n         # If args == (torch.Size,), then we need to unpack the tuple\n         if len(sizes) == 1 and isinstance(sizes[0], torch.Size):\n             sizes = sizes[0]\n+\n         repeats = list(sizes)\n-        result = self.new()\n-        src = self.contiguous()\n \n-        if len(repeats) < src.dim():\n+        if len(repeats) < self.dim():\n             raise ValueError('Number of dimensions of repeat dims can not be '\n                              'smaller than number of dimensions of tensor')\n \n-        xtensor = src.new().set_(src)\n-        xsize = list(xtensor.size())\n-        for i in _range(len(repeats) - src.dim()):\n-            xsize = [1] + xsize\n+        # Add new leading dimensions to the tensor if the\n+        # number of target dimensions is larger than the\n+        # number of source dimensions.\n+        num_new_dimensions = len(repeats) - self.dim()\n+        padded_size = [1] * num_new_dimensions + list(self.size())\n+        target_size = torch.Size([a * b for a, b in zip(padded_size, repeats)])\n+\n+        xtensor = self.new().set_(self)\n+        xtensor = xtensor.expand(padded_size)\n \n-        size = torch.Size([a * b for a, b in zip(xsize, repeats)])\n-        xtensor.resize_(torch.Size(xsize))\n-        result.resize_(size)\n+        result = self.new()\n+        result.resize_(target_size)\n         urtensor = result.new(result)\n         for i in _range(xtensor.dim()):\n             urtensor = urtensor.unfold(i, xtensor.size(i), xtensor.size(i))\n-        for i in _range(urtensor.dim() - xtensor.dim()):\n-            xsize = [1] + xsize\n-        xtensor.resize_(torch.Size(xsize))\n-        xxtensor = xtensor.expand_as(urtensor)\n-        urtensor.copy_(xxtensor)\n+\n+        urtensor.copy_(xtensor.expand_as(urtensor))", "path": "torch/tensor.py", "position": 42, "original_position": 42, "commit_id": "0570712b68d7b4cc5b3150e819ca305fbc98d8e3", "original_commit_id": "983e676e2cef1a8f26fcb8bf0ffb492ef310a816", "user": {"login": "maciejkula", "id": 2392579, "node_id": "MDQ6VXNlcjIzOTI1Nzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2392579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maciejkula", "html_url": "https://github.com/maciejkula", "followers_url": "https://api.github.com/users/maciejkula/followers", "following_url": "https://api.github.com/users/maciejkula/following{/other_user}", "gists_url": "https://api.github.com/users/maciejkula/gists{/gist_id}", "starred_url": "https://api.github.com/users/maciejkula/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maciejkula/subscriptions", "organizations_url": "https://api.github.com/users/maciejkula/orgs", "repos_url": "https://api.github.com/users/maciejkula/repos", "events_url": "https://api.github.com/users/maciejkula/events{/privacy}", "received_events_url": "https://api.github.com/users/maciejkula/received_events", "type": "User", "site_admin": false}, "body": "To be perfectly frank I have no idea what is going on there. I think `urtensor` gets some new dimensions beyond `xtensor`, then the copy semantics somehow make it work after expanding.", "created_at": "2017-12-10T13:03:21Z", "updated_at": "2018-11-23T15:37:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/4084#discussion_r155946964", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4084", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155946964"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4084#discussion_r155946964"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4084"}}, "body_html": "<p>To be perfectly frank I have no idea what is going on there. I think <code>urtensor</code> gets some new dimensions beyond <code>xtensor</code>, then the copy semantics somehow make it work after expanding.</p>", "body_text": "To be perfectly frank I have no idea what is going on there. I think urtensor gets some new dimensions beyond xtensor, then the copy semantics somehow make it work after expanding.", "in_reply_to_id": 155939275}