{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6627", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6627/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6627/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6627/events", "html_url": "https://github.com/pytorch/pytorch/issues/6627", "id": 314652984, "node_id": "MDU6SXNzdWUzMTQ2NTI5ODQ=", "number": 6627, "title": "Seg Fault when using more than one layer in torch.nn.GRU", "user": {"login": "Wahou", "id": 9056297, "node_id": "MDQ6VXNlcjkwNTYyOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9056297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wahou", "html_url": "https://github.com/Wahou", "followers_url": "https://api.github.com/users/Wahou/followers", "following_url": "https://api.github.com/users/Wahou/following{/other_user}", "gists_url": "https://api.github.com/users/Wahou/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wahou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wahou/subscriptions", "organizations_url": "https://api.github.com/users/Wahou/orgs", "repos_url": "https://api.github.com/users/Wahou/repos", "events_url": "https://api.github.com/users/Wahou/events{/privacy}", "received_events_url": "https://api.github.com/users/Wahou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/crash", "name": "crash", "color": "d93f0b", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-16T13:33:12Z", "updated_at": "2018-04-16T14:09:54Z", "closed_at": "2018-04-16T14:09:54Z", "author_association": "NONE", "body_html": "<ul>\n<li>\n<p>using PyTorch</p>\n</li>\n<li>\n<p>OS: Ubuntu 17.10</p>\n</li>\n<li>\n<p>PyTorch version: 0.3.1</p>\n</li>\n<li>\n<p>How you installed PyTorch (conda, pip, source): conda</p>\n</li>\n<li>\n<p>Python version: 3.6.4</p>\n</li>\n<li>\n<p>CUDA/cuDNN version: not using gpu</p>\n</li>\n<li>\n<p>GPU models and configuration: not using gpu</p>\n</li>\n<li>\n<p>GCC version (if compiling from source):</p>\n</li>\n<li>\n<p>CMake version:</p>\n</li>\n<li>\n<p>Build command you used (if compiling from source):</p>\n</li>\n<li>\n<p>Versions of any other relevant libraries:</p>\n</li>\n<li>\n<p>A script to reproduce the bug. Please try to provide as minimal of a test case as possible.<br>\nHere is a minimal code to reproduce the bug (with 10 hidden layers) :</p>\n</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">GRU</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\n        <span class=\"pl-c1\">self</span>.d_data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">9</span>\n        <span class=\"pl-c1\">self</span>.d_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n        <span class=\"pl-c1\">self</span>.d_hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n        nb_hidden_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">GRU</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.input_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.d_data <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.d_hidden\n\n        <span class=\"pl-c1\">self</span>.gru_layer <span class=\"pl-k\">=</span> torch.nn.GRU(<span class=\"pl-c1\">self</span>.input_size, <span class=\"pl-c1\">self</span>.d_hidden, nb_hidden_layers, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.output_layer <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">self</span>.d_hidden, <span class=\"pl-c1\">self</span>.d_out, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        hidden <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">len</span>(x), <span class=\"pl-c1\">self</span>.d_hidden))\n\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.cat((x, hidden), <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.unsqueeze(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>.size())\n        output, hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru_layer(<span class=\"pl-c1\">input</span>)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.output_layer(output)\n        output <span class=\"pl-k\">=</span> output.squeeze(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">return</span> output\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">fit</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\n        X <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>()\n        Y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>()\n\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1000</span>):\n            sizes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">500</span>)\n            seq_size <span class=\"pl-k\">=</span> random.choice(sizes)\n            X.append(Variable(torch.rand(seq_size, <span class=\"pl-c1\">9</span>)))\n            label <span class=\"pl-k\">=</span> random.choice([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>])\n            Y.append(Variable(torch.Tensor([label] <span class=\"pl-k\">*</span> seq_size).long()))\n\n        batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\n        criterion <span class=\"pl-k\">=</span> torch.nn.CrossEntropyLoss(<span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        optimizer <span class=\"pl-k\">=</span> torch.optim.Adagrad(<span class=\"pl-c1\">self</span>.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\n        epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epochs):\n            batch_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            batch_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>()\n            batch_y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>()\n\n            <span class=\"pl-k\">for</span> x, y <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(X, Y):\n                batch_x.append(x)\n                batch_y.append(y)\n                batch_count <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n                <span class=\"pl-k\">if</span> batch_count <span class=\"pl-k\">==</span> batch_size:\n                    x <span class=\"pl-k\">=</span> torch.cat(batch_x)\n                    y <span class=\"pl-k\">=</span> torch.cat(batch_y)\n                    y_pred <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>(x)\n                    loss <span class=\"pl-k\">=</span> criterion(y_pred, y)\n\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Zero gradients, perform a backward pass, and update the weights.</span>\n                    optimizer.zero_grad()\n                    loss.backward(<span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                    optimizer.step()\n\n                    batch_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n                    batch_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>()\n                    batch_y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>()\nmodel <span class=\"pl-k\">=</span> GRU()\nmodel.fit()</pre></div>\n<ul>\n<li>\n<p>Error messages and/or stack traces of the bug<br>\nsegmentation fault (core dumped) (no other error message)</p>\n</li>\n<li>\n<p>Context around what you are trying to do :<br>\nI'm trying to train a simple GRU (torch.nn.gru() ) on sequential data. My program works fine when I'm only using a one layered GRU. As soon as I put more layers, the program crashes when computing the criterion at the second computation of the criterion.</p>\n</li>\n</ul>", "body_text": "using PyTorch\n\n\nOS: Ubuntu 17.10\n\n\nPyTorch version: 0.3.1\n\n\nHow you installed PyTorch (conda, pip, source): conda\n\n\nPython version: 3.6.4\n\n\nCUDA/cuDNN version: not using gpu\n\n\nGPU models and configuration: not using gpu\n\n\nGCC version (if compiling from source):\n\n\nCMake version:\n\n\nBuild command you used (if compiling from source):\n\n\nVersions of any other relevant libraries:\n\n\nA script to reproduce the bug. Please try to provide as minimal of a test case as possible.\nHere is a minimal code to reproduce the bug (with 10 hidden layers) :\n\n\nimport torch\nfrom torch.autograd import Variable\nimport random\n\nclass GRU(torch.nn.Module):\n\n    def __init__(self):\n\n        self.d_data = 9\n        self.d_out = 5\n        self.d_hidden = 8\n        nb_hidden_layers = 10\n        super(GRU, self).__init__()\n        self.input_size = self.d_data + self.d_hidden\n\n        self.gru_layer = torch.nn.GRU(self.input_size, self.d_hidden, nb_hidden_layers, batch_first=True)\n        self.output_layer = torch.nn.Linear(self.d_hidden, self.d_out, bias=False)\n\n    def forward(self, x):\n        hidden = Variable(torch.zeros(len(x), self.d_hidden))\n\n        input = torch.cat((x, hidden), 1)\n        input = input.unsqueeze(0)\n        print(input.size())\n        output, hidden = self.gru_layer(input)\n        output = self.output_layer(output)\n        output = output.squeeze(0)\n        return output\n\n    def fit(self):\n\n        X = list()\n        Y = list()\n\n        for i in range(0, 1000):\n            sizes = range(500)\n            seq_size = random.choice(sizes)\n            X.append(Variable(torch.rand(seq_size, 9)))\n            label = random.choice([0,1,2,3,4])\n            Y.append(Variable(torch.Tensor([label] * seq_size).long()))\n\n        batch_size = 10\n\n        criterion = torch.nn.CrossEntropyLoss(size_average=False)\n        optimizer = torch.optim.Adagrad(self.parameters(), lr=0.01)\n        epochs = 1\n        for t in range(epochs):\n            batch_count = 0\n            batch_x = list()\n            batch_y = list()\n\n            for x, y in zip(X, Y):\n                batch_x.append(x)\n                batch_y.append(y)\n                batch_count += 1\n\n                if batch_count == batch_size:\n                    x = torch.cat(batch_x)\n                    y = torch.cat(batch_y)\n                    y_pred = self(x)\n                    loss = criterion(y_pred, y)\n\n                    # Zero gradients, perform a backward pass, and update the weights.\n                    optimizer.zero_grad()\n                    loss.backward(retain_graph=True)\n                    optimizer.step()\n\n                    batch_count = 0\n                    batch_x = list()\n                    batch_y = list()\nmodel = GRU()\nmodel.fit()\n\n\nError messages and/or stack traces of the bug\nsegmentation fault (core dumped) (no other error message)\n\n\nContext around what you are trying to do :\nI'm trying to train a simple GRU (torch.nn.gru() ) on sequential data. My program works fine when I'm only using a one layered GRU. As soon as I put more layers, the program crashes when computing the criterion at the second computation of the criterion.", "body": "- using PyTorch\r\n- OS: Ubuntu 17.10\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: not using gpu \r\n- GPU models and configuration: not using gpu\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Build command you used (if compiling from source):\r\n- Versions of any other relevant libraries:\r\n\r\n\r\n- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.\r\nHere is a minimal code to reproduce the bug (with 10 hidden layers) :\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport random\r\n\r\nclass GRU(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n\r\n        self.d_data = 9\r\n        self.d_out = 5\r\n        self.d_hidden = 8\r\n        nb_hidden_layers = 10\r\n        super(GRU, self).__init__()\r\n        self.input_size = self.d_data + self.d_hidden\r\n\r\n        self.gru_layer = torch.nn.GRU(self.input_size, self.d_hidden, nb_hidden_layers, batch_first=True)\r\n        self.output_layer = torch.nn.Linear(self.d_hidden, self.d_out, bias=False)\r\n\r\n    def forward(self, x):\r\n        hidden = Variable(torch.zeros(len(x), self.d_hidden))\r\n\r\n        input = torch.cat((x, hidden), 1)\r\n        input = input.unsqueeze(0)\r\n        print(input.size())\r\n        output, hidden = self.gru_layer(input)\r\n        output = self.output_layer(output)\r\n        output = output.squeeze(0)\r\n        return output\r\n\r\n    def fit(self):\r\n\r\n        X = list()\r\n        Y = list()\r\n\r\n        for i in range(0, 1000):\r\n            sizes = range(500)\r\n            seq_size = random.choice(sizes)\r\n            X.append(Variable(torch.rand(seq_size, 9)))\r\n            label = random.choice([0,1,2,3,4])\r\n            Y.append(Variable(torch.Tensor([label] * seq_size).long()))\r\n\r\n        batch_size = 10\r\n\r\n        criterion = torch.nn.CrossEntropyLoss(size_average=False)\r\n        optimizer = torch.optim.Adagrad(self.parameters(), lr=0.01)\r\n        epochs = 1\r\n        for t in range(epochs):\r\n            batch_count = 0\r\n            batch_x = list()\r\n            batch_y = list()\r\n\r\n            for x, y in zip(X, Y):\r\n                batch_x.append(x)\r\n                batch_y.append(y)\r\n                batch_count += 1\r\n\r\n                if batch_count == batch_size:\r\n                    x = torch.cat(batch_x)\r\n                    y = torch.cat(batch_y)\r\n                    y_pred = self(x)\r\n                    loss = criterion(y_pred, y)\r\n\r\n                    # Zero gradients, perform a backward pass, and update the weights.\r\n                    optimizer.zero_grad()\r\n                    loss.backward(retain_graph=True)\r\n                    optimizer.step()\r\n\r\n                    batch_count = 0\r\n                    batch_x = list()\r\n                    batch_y = list()\r\nmodel = GRU()\r\nmodel.fit()\r\n```\r\n\r\n- Error messages and/or stack traces of the bug \r\nsegmentation fault (core dumped) (no other error message)\r\n\r\n- Context around what you are trying to do : \r\nI'm trying to train a simple GRU (torch.nn.gru() ) on sequential data. My program works fine when I'm only using a one layered GRU. As soon as I put more layers, the program crashes when computing the criterion at the second computation of the criterion."}