{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12633", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12633/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12633/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12633/events", "html_url": "https://github.com/pytorch/pytorch/issues/12633", "id": 369906943, "node_id": "MDU6SXNzdWUzNjk5MDY5NDM=", "number": 12633, "title": "odd behavior w/ add_out_dense_sparse_cuda", "user": {"login": "realdoug", "id": 2383488, "node_id": "MDQ6VXNlcjIzODM0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2383488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/realdoug", "html_url": "https://github.com/realdoug", "followers_url": "https://api.github.com/users/realdoug/followers", "following_url": "https://api.github.com/users/realdoug/following{/other_user}", "gists_url": "https://api.github.com/users/realdoug/gists{/gist_id}", "starred_url": "https://api.github.com/users/realdoug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/realdoug/subscriptions", "organizations_url": "https://api.github.com/users/realdoug/orgs", "repos_url": "https://api.github.com/users/realdoug/repos", "events_url": "https://api.github.com/users/realdoug/events{/privacy}", "received_events_url": "https://api.github.com/users/realdoug/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/sparse", "name": "sparse", "color": "bfd4f2", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-10-14T14:22:37Z", "updated_at": "2018-10-29T21:51:02Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Given the following code, the two sparse tensors constructed here appear identical, but produce two different results when added to a 0 tensor of the same shape.</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/torch.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>ATen/SparseTensorImpl.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>vector<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"pl-k\">using</span> at::Tensor;\n<span class=\"pl-k\">using</span> at::SparseTensorImpl;\n\nTensor <span class=\"pl-en\">make_sparse_from_indices</span>(Tensor&amp; indices, at::TensorOptions opts){\n  at::Tensor vals = <span class=\"pl-c1\">at::tensor</span>({<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>}, opts).<span class=\"pl-c1\">view</span>({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>});\n  at::Tensor sparse = <span class=\"pl-c1\">at::sparse_coo_tensor</span>(indices, vals, {<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>}, opts);\n  <span class=\"pl-c1\">_get_sparse_impl</span>(sparse)-&gt;<span class=\"pl-c1\">set_coalesced</span>(<span class=\"pl-c1\">true</span>);\n\n  <span class=\"pl-k\">return</span> sparse;\n}\n\n<span class=\"pl-k\">int</span> <span class=\"pl-en\">main</span>() {\n  at::TensorOptions opts = <span class=\"pl-c1\">at::device</span>(at::<span class=\"pl-c1\">kCUDA</span>).<span class=\"pl-c1\">dtype</span>(at::<span class=\"pl-c1\">kLong</span>);\n\n  Tensor inds1 = <span class=\"pl-c1\">at::tensor</span>({<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>}, opts).<span class=\"pl-c1\">view</span>({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>});\n  Tensor inds2 = inds1.<span class=\"pl-c1\">transpose</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>).<span class=\"pl-c1\">clone</span>().<span class=\"pl-c1\">transpose</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>);<span class=\"pl-c\"><span class=\"pl-c\">//</span>.clone();</span>\n\n  Tensor sparse1 = <span class=\"pl-c1\">make_sparse_from_indices</span>(inds1, opts);\n  Tensor sparse2 = <span class=\"pl-c1\">make_sparse_from_indices</span>(inds2, opts);\n\n  Tensor z1 = <span class=\"pl-c1\">at::zeros</span>({<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>}, opts);\n  std::cout &lt;&lt; z1.<span class=\"pl-c1\">add</span>(sparse1) &lt;&lt; std::endl;\n  <span class=\"pl-c1\">printf</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span>===<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>);\n  std::cout &lt;&lt; z1.<span class=\"pl-c1\">add</span>(sparse2) &lt;&lt; std::endl;\n\n  <span class=\"pl-c\"><span class=\"pl-c\">/*</span> line 28 and line 29 do not produce the same result <span class=\"pl-c\">*/</span></span>\n}</pre></div>\n<p>Un-commenting the call to <code>clone()</code> fixes the issue.  The answer might be  \"Just use <code>coalesce()</code> instead of manually setting the flag\", but the only reason this works is because <code>coalesce()</code> appears to always make a clone of indices*.  That strikes me as just another way of accomplishing the same thing &amp; not fully answering why an extra call to <code>clone()</code> is needed.</p>\n<p>This is a simplified reproduction of an issue that came up when trying to implement <code>dense.to_sparse()</code> (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"364941082\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12171\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12171/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12171\">#12171</a>).  When converting a dense tensor to sparse, we know there won't be any dupe indices, so I'm trying to determine whether I really need to call the entire <code>coalesce()</code> routine or not.</p>\n<p>*unless <code>is_coalesced</code> is already true.</p>\n<h2>To Reproduce</h2>\n<p>See code snippet above.  Issue only appears on CUDA.</p>\n<h2>Expected behavior</h2>\n<p>both calls to <code>add</code> should return the same result</p>\n<h2>Environment</h2>\n<p>Please copy and paste the output from our<br>\nCollecting environment information...<br>\nPyTorch version: 1.0.0a0+894e4ec<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.2.148</p>\n<p>OS: Ubuntu 16.04.5 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.12.2</p>\n<p>Python version: 3.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: Tesla P4<br>\nNvidia driver version: 396.54<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so<br>\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7<br>\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.0.5<br>\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.15.1)<br>\n[pip] torch (1.0.0a0+2745997, /home/dougfriedman/anaconda3/envs/pytorch-dev/lib/python3.7/site-packages)<br>\n[conda] magma-cuda92              2.3.0                         1    pytorch<br>\n[conda] torch                     0.5.0a0+7561bbb           <br>\n[conda] torch                     1.0.0a0+3ab005f           <br>\n[conda] torch                     1.0.0a0+90965fc           <br>\n[conda] torch                     1.0.0a0+efcc8a0           <br>\n[conda] torch                     0.5.0a0+2c31971           <br>\n[conda] torch                     1.0.0a0+1d35ab2           <br>\n[conda] torch                     1.0.0a0+a6f8553           <br>\n[conda] torch                     1.0.0a0+faa60ee           <br>\n[conda] torch                     1.0.0a0+1e70500           <br>\n[conda] torch                     1.0.0a0+15539b0           <br>\n[conda] torch                     1.0.0a0+61264b7           <br>\n[conda] torch                     1.0.0a0+2745997           <br>\n[conda] torch                     1.0.0a0+367874            <br>\n[conda] torch                     1.0.0a0+e8a828e           <br>\n[conda] torch                     1.0.0a0+c4f0628           <br>\n[conda] torch                     0.5.0a0+13f9214           <br>\n[conda] torch                     1.0.0a0+894e4ec           <br>\n[conda] torch                     1.0.0a0+5cb2b23           </p>", "body_text": "\ud83d\udc1b Bug\nGiven the following code, the two sparse tensors constructed here appear identical, but produce two different results when added to a 0 tensor of the same shape.\n#include <torch/torch.h>\n#include <ATen/SparseTensorImpl.h>\n#include <iostream>\n#include <vector>\n\nusing at::Tensor;\nusing at::SparseTensorImpl;\n\nTensor make_sparse_from_indices(Tensor& indices, at::TensorOptions opts){\n  at::Tensor vals = at::tensor({3, 0, 0, 9, 0, 0}, opts).view({2, 3});\n  at::Tensor sparse = at::sparse_coo_tensor(indices, vals, {3, 3, 3}, opts);\n  _get_sparse_impl(sparse)->set_coalesced(true);\n\n  return sparse;\n}\n\nint main() {\n  at::TensorOptions opts = at::device(at::kCUDA).dtype(at::kLong);\n\n  Tensor inds1 = at::tensor({0, 1, 0, 0}, opts).view({2, 2});\n  Tensor inds2 = inds1.transpose(0,1).clone().transpose(0,1);//.clone();\n\n  Tensor sparse1 = make_sparse_from_indices(inds1, opts);\n  Tensor sparse2 = make_sparse_from_indices(inds2, opts);\n\n  Tensor z1 = at::zeros({3, 3, 3}, opts);\n  std::cout << z1.add(sparse1) << std::endl;\n  printf(\"\\n===\\n\");\n  std::cout << z1.add(sparse2) << std::endl;\n\n  /* line 28 and line 29 do not produce the same result */\n}\nUn-commenting the call to clone() fixes the issue.  The answer might be  \"Just use coalesce() instead of manually setting the flag\", but the only reason this works is because coalesce() appears to always make a clone of indices*.  That strikes me as just another way of accomplishing the same thing & not fully answering why an extra call to clone() is needed.\nThis is a simplified reproduction of an issue that came up when trying to implement dense.to_sparse() (#12171).  When converting a dense tensor to sparse, we know there won't be any dupe indices, so I'm trying to determine whether I really need to call the entire coalesce() routine or not.\n*unless is_coalesced is already true.\nTo Reproduce\nSee code snippet above.  Issue only appears on CUDA.\nExpected behavior\nboth calls to add should return the same result\nEnvironment\nPlease copy and paste the output from our\nCollecting environment information...\nPyTorch version: 1.0.0a0+894e4ec\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.12.2\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: Tesla P4\nNvidia driver version: 396.54\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.0.5\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.15.1)\n[pip] torch (1.0.0a0+2745997, /home/dougfriedman/anaconda3/envs/pytorch-dev/lib/python3.7/site-packages)\n[conda] magma-cuda92              2.3.0                         1    pytorch\n[conda] torch                     0.5.0a0+7561bbb           \n[conda] torch                     1.0.0a0+3ab005f           \n[conda] torch                     1.0.0a0+90965fc           \n[conda] torch                     1.0.0a0+efcc8a0           \n[conda] torch                     0.5.0a0+2c31971           \n[conda] torch                     1.0.0a0+1d35ab2           \n[conda] torch                     1.0.0a0+a6f8553           \n[conda] torch                     1.0.0a0+faa60ee           \n[conda] torch                     1.0.0a0+1e70500           \n[conda] torch                     1.0.0a0+15539b0           \n[conda] torch                     1.0.0a0+61264b7           \n[conda] torch                     1.0.0a0+2745997           \n[conda] torch                     1.0.0a0+367874            \n[conda] torch                     1.0.0a0+e8a828e           \n[conda] torch                     1.0.0a0+c4f0628           \n[conda] torch                     0.5.0a0+13f9214           \n[conda] torch                     1.0.0a0+894e4ec           \n[conda] torch                     1.0.0a0+5cb2b23", "body": "## \ud83d\udc1b Bug\r\n\r\nGiven the following code, the two sparse tensors constructed here appear identical, but produce two different results when added to a 0 tensor of the same shape.\r\n\r\n```c++\r\n#include <torch/torch.h>\r\n#include <ATen/SparseTensorImpl.h>\r\n#include <iostream>\r\n#include <vector>\r\n\r\nusing at::Tensor;\r\nusing at::SparseTensorImpl;\r\n\r\nTensor make_sparse_from_indices(Tensor& indices, at::TensorOptions opts){\r\n  at::Tensor vals = at::tensor({3, 0, 0, 9, 0, 0}, opts).view({2, 3});\r\n  at::Tensor sparse = at::sparse_coo_tensor(indices, vals, {3, 3, 3}, opts);\r\n  _get_sparse_impl(sparse)->set_coalesced(true);\r\n\r\n  return sparse;\r\n}\r\n\r\nint main() {\r\n  at::TensorOptions opts = at::device(at::kCUDA).dtype(at::kLong);\r\n\r\n  Tensor inds1 = at::tensor({0, 1, 0, 0}, opts).view({2, 2});\r\n  Tensor inds2 = inds1.transpose(0,1).clone().transpose(0,1);//.clone();\r\n\r\n  Tensor sparse1 = make_sparse_from_indices(inds1, opts);\r\n  Tensor sparse2 = make_sparse_from_indices(inds2, opts);\r\n\r\n  Tensor z1 = at::zeros({3, 3, 3}, opts);\r\n  std::cout << z1.add(sparse1) << std::endl;\r\n  printf(\"\\n===\\n\");\r\n  std::cout << z1.add(sparse2) << std::endl;\r\n\r\n  /* line 28 and line 29 do not produce the same result */\r\n}\r\n```\r\n\r\nUn-commenting the call to ```clone()``` fixes the issue.  The answer might be  \"Just use ```coalesce()``` instead of manually setting the flag\", but the only reason this works is because ```coalesce()``` appears to always make a clone of indices*.  That strikes me as just another way of accomplishing the same thing & not fully answering why an extra call to ```clone()``` is needed.\r\n\r\nThis is a simplified reproduction of an issue that came up when trying to implement ```dense.to_sparse()``` (#12171).  When converting a dense tensor to sparse, we know there won't be any dupe indices, so I'm trying to determine whether I really need to call the entire ```coalesce()``` routine or not.\r\n\r\n*unless ```is_coalesced``` is already true.\r\n\r\n## To Reproduce\r\n\r\nSee code snippet above.  Issue only appears on CUDA.\r\n\r\n## Expected behavior\r\nboth calls to ```add``` should return the same result\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+894e4ec\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Tesla P4\r\nNvidia driver version: 396.54\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.0.5\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.1)\r\n[pip] torch (1.0.0a0+2745997, /home/dougfriedman/anaconda3/envs/pytorch-dev/lib/python3.7/site-packages)\r\n[conda] magma-cuda92              2.3.0                         1    pytorch\r\n[conda] torch                     0.5.0a0+7561bbb           <pip>\r\n[conda] torch                     1.0.0a0+3ab005f           <pip>\r\n[conda] torch                     1.0.0a0+90965fc           <pip>\r\n[conda] torch                     1.0.0a0+efcc8a0           <pip>\r\n[conda] torch                     0.5.0a0+2c31971           <pip>\r\n[conda] torch                     1.0.0a0+1d35ab2           <pip>\r\n[conda] torch                     1.0.0a0+a6f8553           <pip>\r\n[conda] torch                     1.0.0a0+faa60ee           <pip>\r\n[conda] torch                     1.0.0a0+1e70500           <pip>\r\n[conda] torch                     1.0.0a0+15539b0           <pip>\r\n[conda] torch                     1.0.0a0+61264b7           <pip>\r\n[conda] torch                     1.0.0a0+2745997           <pip>\r\n[conda] torch                     1.0.0a0+367874            <pip>\r\n[conda] torch                     1.0.0a0+e8a828e           <pip>\r\n[conda] torch                     1.0.0a0+c4f0628           <pip>\r\n[conda] torch                     0.5.0a0+13f9214           <pip>\r\n[conda] torch                     1.0.0a0+894e4ec           <pip>\r\n[conda] torch                     1.0.0a0+5cb2b23           <pip>\r\n"}