{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8115", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8115/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8115/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8115/events", "html_url": "https://github.com/pytorch/pytorch/issues/8115", "id": 329118595, "node_id": "MDU6SXNzdWUzMjkxMTg1OTU=", "number": 8115, "title": "NCCL backend hangs at initialization", "user": {"login": "VanDavv", "id": 5244214, "node_id": "MDQ6VXNlcjUyNDQyMTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5244214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VanDavv", "html_url": "https://github.com/VanDavv", "followers_url": "https://api.github.com/users/VanDavv/followers", "following_url": "https://api.github.com/users/VanDavv/following{/other_user}", "gists_url": "https://api.github.com/users/VanDavv/gists{/gist_id}", "starred_url": "https://api.github.com/users/VanDavv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VanDavv/subscriptions", "organizations_url": "https://api.github.com/users/VanDavv/orgs", "repos_url": "https://api.github.com/users/VanDavv/repos", "events_url": "https://api.github.com/users/VanDavv/events{/privacy}", "received_events_url": "https://api.github.com/users/VanDavv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-06-04T16:00:59Z", "updated_at": "2018-08-03T16:53:41Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I'm pretty new to pytorch and I may lack some of the understanding of PyTorch concepts.<br>\nI try to launch distributed learning on 4 GPUs connected with NV-Link, so NCCL seemed the best backend. However, after <code>init_process_group</code>, process hangs up with no response or error. The same thing happens for gloo backend.</p>\n<h2>Code example</h2>\n<pre><code>import torch \nimport multiprocessing as mp\ntry:\n    mp.set_start_method('spawn') # spawn, forkserver, and fork\nexcept RuntimeError:\n    pass\n\ntorch.distributed.init_process_group(backend='nccl', world_size=2, init_method='tcp://224.66.41.62:23456')\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0a0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: Tesla V100-DGXS-16GB<br>\nGPU 1: Tesla V100-DGXS-16GB<br>\nGPU 2: Tesla V100-DGXS-16GB<br>\nGPU 3: Tesla V100-DGXS-16GB</p>\n<p>Nvidia driver version: 384.125<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2</p>\n<p>Versions of relevant libraries:<br>\n[pip] msgpack-numpy (0.4.1)<br>\n[pip] numpy (1.14.2)<br>\n[pip] torch (0.4.0a0)<br>\n[pip] torchtext (0.2.1)<br>\n[pip] torchvision (0.2.0)<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nI'm pretty new to pytorch and I may lack some of the understanding of PyTorch concepts.\nI try to launch distributed learning on 4 GPUs connected with NV-Link, so NCCL seemed the best backend. However, after init_process_group, process hangs up with no response or error. The same thing happens for gloo backend.\nCode example\nimport torch \nimport multiprocessing as mp\ntry:\n    mp.set_start_method('spawn') # spawn, forkserver, and fork\nexcept RuntimeError:\n    pass\n\ntorch.distributed.init_process_group(backend='nccl', world_size=2, init_method='tcp://224.66.41.62:23456')\n\nSystem Info\nPyTorch version: 0.4.0a0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: Tesla V100-DGXS-16GB\nGPU 1: Tesla V100-DGXS-16GB\nGPU 2: Tesla V100-DGXS-16GB\nGPU 3: Tesla V100-DGXS-16GB\nNvidia driver version: 384.125\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\nVersions of relevant libraries:\n[pip] msgpack-numpy (0.4.1)\n[pip] numpy (1.14.2)\n[pip] torch (0.4.0a0)\n[pip] torchtext (0.2.1)\n[pip] torchvision (0.2.0)\n[conda] Could not collect", "body": "## Issue description\r\n\r\nI'm pretty new to pytorch and I may lack some of the understanding of PyTorch concepts. \r\nI try to launch distributed learning on 4 GPUs connected with NV-Link, so NCCL seemed the best backend. However, after `init_process_group`, process hangs up with no response or error. The same thing happens for gloo backend.\r\n## Code example\r\n```\r\nimport torch \r\nimport multiprocessing as mp\r\ntry:\r\n    mp.set_start_method('spawn') # spawn, forkserver, and fork\r\nexcept RuntimeError:\r\n    pass\r\n\r\ntorch.distributed.init_process_group(backend='nccl', world_size=2, init_method='tcp://224.66.41.62:23456')\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.0a0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-DGXS-16GB\r\nGPU 1: Tesla V100-DGXS-16GB\r\nGPU 2: Tesla V100-DGXS-16GB\r\nGPU 3: Tesla V100-DGXS-16GB\r\n\r\nNvidia driver version: 384.125\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy (0.4.1)\r\n[pip] numpy (1.14.2)\r\n[pip] torch (0.4.0a0)\r\n[pip] torchtext (0.2.1)\r\n[pip] torchvision (0.2.0)\r\n[conda] Could not collect\r\n"}