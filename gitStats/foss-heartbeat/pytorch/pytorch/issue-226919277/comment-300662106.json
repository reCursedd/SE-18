{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/300662106", "html_url": "https://github.com/pytorch/pytorch/pull/1507#issuecomment-300662106", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1507", "id": 300662106, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDY2MjEwNg==", "user": {"login": "caogang", "id": 2537027, "node_id": "MDQ6VXNlcjI1MzcwMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2537027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caogang", "html_url": "https://github.com/caogang", "followers_url": "https://api.github.com/users/caogang/followers", "following_url": "https://api.github.com/users/caogang/following{/other_user}", "gists_url": "https://api.github.com/users/caogang/gists{/gist_id}", "starred_url": "https://api.github.com/users/caogang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caogang/subscriptions", "organizations_url": "https://api.github.com/users/caogang/orgs", "repos_url": "https://api.github.com/users/caogang/repos", "events_url": "https://api.github.com/users/caogang/events{/privacy}", "received_events_url": "https://api.github.com/users/caogang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-11T02:04:52Z", "updated_at": "2017-05-11T02:04:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> , current commit maybe still something wrong with the <code>mask_fill</code>. Because it works fine using <code>mask.type_as(grad_output) * grad_output</code></p>\n<p>With the same test code above. I got another error</p>\n<pre><code>TypeErrorTraceback (most recent call last)\n&lt;ipython-input-3-a841c9970f31&gt; in &lt;module&gt;()\n     44 print netD\n     45 gp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\n---&gt; 46 gp.backward()\n     47 for p in netD.parameters():\n     48     print p.grad\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    143             Defaults to False, unless ``gradient`` is a volatile Variable.\n    144         \"\"\"\n--&gt; 145         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    146 \n    147     def register_hook(self, hook):\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/__init__.pyc in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     96 \n     97     Variable._execution_engine.run_backward(\n---&gt; 98         variables, grad_variables, retain_graph)\n     99 \n    100 \n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/function.pyc in apply(self, *args)\n     88 \n     89     def apply(self, *args):\n---&gt; 90         return self._forward_cls.backward(self, *args)\n     91 \n     92 \n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/nn/_functions/linear.pyc in backward(ctx, grad_output)\n     21         grad_input = grad_weight = grad_bias = None\n     22         if ctx.needs_input_grad[0]:\n---&gt; 23             grad_input = torch.mm(grad_output, weight)\n     24         if ctx.needs_input_grad[1]:\n     25             grad_weight = torch.mm(grad_output.t(), input)\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in mm(self, matrix)\n    526     def mm(self, matrix):\n    527         output = Variable(self.data.new(self.data.size(0), matrix.data.size(1)))\n--&gt; 528         return self._static_blas(Addmm, (output, 0, 1, self, matrix), False)\n    529 \n    530     def bmm(self, batch):\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in _static_blas(cls, args, inplace)\n    519         if num_args == 4:\n    520             alpha = args[1]\n--&gt; 521         return cls.apply(*(args[:1] + args[-2:] + (alpha, beta, inplace)))\n    522 \n    523     def _blas(self, cls, args, inplace):\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc in forward(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\n     22         output = _get_output(ctx, add_matrix, inplace=inplace)\n     23         return torch.addmm(alpha, add_matrix, beta,\n---&gt; 24                            matrix1, matrix2, out=output)\n     25 \n     26     @staticmethod\n\nTypeError: torch.addmm received an invalid combination of arguments - got (int, torch.ByteTensor, int, torch.ByteTensor, torch.FloatTensor, out=torch.ByteTensor), but expected one of:\n * (torch.ByteTensor source, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (torch.ByteTensor source, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (int beta, torch.ByteTensor source, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (torch.ByteTensor source, int alpha, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (int beta, torch.ByteTensor source, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (torch.ByteTensor source, int alpha, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (int beta, torch.ByteTensor source, int alpha, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.ByteTensor, int, torch.ByteTensor, !torch.FloatTensor!, out=torch.ByteTensor)\n * (int beta, torch.ByteTensor source, int alpha, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.ByteTensor, int, !torch.ByteTensor!, !torch.FloatTensor!, out=torch.ByteTensor)\n</code></pre>", "body_text": "Hi @apaszke , current commit maybe still something wrong with the mask_fill. Because it works fine using mask.type_as(grad_output) * grad_output\nWith the same test code above. I got another error\nTypeErrorTraceback (most recent call last)\n<ipython-input-3-a841c9970f31> in <module>()\n     44 print netD\n     45 gp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\n---> 46 gp.backward()\n     47 for p in netD.parameters():\n     48     print p.grad\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    143             Defaults to False, unless ``gradient`` is a volatile Variable.\n    144         \"\"\"\n--> 145         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    146 \n    147     def register_hook(self, hook):\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/__init__.pyc in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     96 \n     97     Variable._execution_engine.run_backward(\n---> 98         variables, grad_variables, retain_graph)\n     99 \n    100 \n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/function.pyc in apply(self, *args)\n     88 \n     89     def apply(self, *args):\n---> 90         return self._forward_cls.backward(self, *args)\n     91 \n     92 \n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/nn/_functions/linear.pyc in backward(ctx, grad_output)\n     21         grad_input = grad_weight = grad_bias = None\n     22         if ctx.needs_input_grad[0]:\n---> 23             grad_input = torch.mm(grad_output, weight)\n     24         if ctx.needs_input_grad[1]:\n     25             grad_weight = torch.mm(grad_output.t(), input)\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in mm(self, matrix)\n    526     def mm(self, matrix):\n    527         output = Variable(self.data.new(self.data.size(0), matrix.data.size(1)))\n--> 528         return self._static_blas(Addmm, (output, 0, 1, self, matrix), False)\n    529 \n    530     def bmm(self, batch):\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in _static_blas(cls, args, inplace)\n    519         if num_args == 4:\n    520             alpha = args[1]\n--> 521         return cls.apply(*(args[:1] + args[-2:] + (alpha, beta, inplace)))\n    522 \n    523     def _blas(self, cls, args, inplace):\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc in forward(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\n     22         output = _get_output(ctx, add_matrix, inplace=inplace)\n     23         return torch.addmm(alpha, add_matrix, beta,\n---> 24                            matrix1, matrix2, out=output)\n     25 \n     26     @staticmethod\n\nTypeError: torch.addmm received an invalid combination of arguments - got (int, torch.ByteTensor, int, torch.ByteTensor, torch.FloatTensor, out=torch.ByteTensor), but expected one of:\n * (torch.ByteTensor source, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (torch.ByteTensor source, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (int beta, torch.ByteTensor source, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (torch.ByteTensor source, int alpha, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (int beta, torch.ByteTensor source, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (torch.ByteTensor source, int alpha, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n * (int beta, torch.ByteTensor source, int alpha, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.ByteTensor, int, torch.ByteTensor, !torch.FloatTensor!, out=torch.ByteTensor)\n * (int beta, torch.ByteTensor source, int alpha, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.ByteTensor, int, !torch.ByteTensor!, !torch.FloatTensor!, out=torch.ByteTensor)", "body": "Hi @apaszke , current commit maybe still something wrong with the `mask_fill`. Because it works fine using `mask.type_as(grad_output) * grad_output`\r\n\r\nWith the same test code above. I got another error\r\n```\r\nTypeErrorTraceback (most recent call last)\r\n<ipython-input-3-a841c9970f31> in <module>()\r\n     44 print netD\r\n     45 gp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\r\n---> 46 gp.backward()\r\n     47 for p in netD.parameters():\r\n     48     print p.grad\r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    143             Defaults to False, unless ``gradient`` is a volatile Variable.\r\n    144         \"\"\"\r\n--> 145         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    146 \r\n    147     def register_hook(self, hook):\r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/__init__.pyc in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\r\n     96 \r\n     97     Variable._execution_engine.run_backward(\r\n---> 98         variables, grad_variables, retain_graph)\r\n     99 \r\n    100 \r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/function.pyc in apply(self, *args)\r\n     88 \r\n     89     def apply(self, *args):\r\n---> 90         return self._forward_cls.backward(self, *args)\r\n     91 \r\n     92 \r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/nn/_functions/linear.pyc in backward(ctx, grad_output)\r\n     21         grad_input = grad_weight = grad_bias = None\r\n     22         if ctx.needs_input_grad[0]:\r\n---> 23             grad_input = torch.mm(grad_output, weight)\r\n     24         if ctx.needs_input_grad[1]:\r\n     25             grad_weight = torch.mm(grad_output.t(), input)\r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in mm(self, matrix)\r\n    526     def mm(self, matrix):\r\n    527         output = Variable(self.data.new(self.data.size(0), matrix.data.size(1)))\r\n--> 528         return self._static_blas(Addmm, (output, 0, 1, self, matrix), False)\r\n    529 \r\n    530     def bmm(self, batch):\r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in _static_blas(cls, args, inplace)\r\n    519         if num_args == 4:\r\n    520             alpha = args[1]\r\n--> 521         return cls.apply(*(args[:1] + args[-2:] + (alpha, beta, inplace)))\r\n    522 \r\n    523     def _blas(self, cls, args, inplace):\r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc in forward(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\r\n     22         output = _get_output(ctx, add_matrix, inplace=inplace)\r\n     23         return torch.addmm(alpha, add_matrix, beta,\r\n---> 24                            matrix1, matrix2, out=output)\r\n     25 \r\n     26     @staticmethod\r\n\r\nTypeError: torch.addmm received an invalid combination of arguments - got (int, torch.ByteTensor, int, torch.ByteTensor, torch.FloatTensor, out=torch.ByteTensor), but expected one of:\r\n * (torch.ByteTensor source, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n * (torch.ByteTensor source, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n * (int beta, torch.ByteTensor source, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n * (torch.ByteTensor source, int alpha, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n * (int beta, torch.ByteTensor source, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n * (torch.ByteTensor source, int alpha, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n * (int beta, torch.ByteTensor source, int alpha, torch.ByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n      didn't match because some of the arguments have invalid types: (int, torch.ByteTensor, int, torch.ByteTensor, !torch.FloatTensor!, out=torch.ByteTensor)\r\n * (int beta, torch.ByteTensor source, int alpha, torch.SparseByteTensor mat1, torch.ByteTensor mat2, *, torch.ByteTensor out)\r\n      didn't match because some of the arguments have invalid types: (int, torch.ByteTensor, int, !torch.ByteTensor!, !torch.FloatTensor!, out=torch.ByteTensor)\r\n```"}