{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216536979", "pull_request_review_id": 154039622, "id": 216536979, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjUzNjk3OQ==", "diff_hunk": "@@ -654,8 +654,7 @@ def _load_from_state_dict(self, state_dict, prefix, metadata, strict, missing_ke\n                     continue\n \n                 if isinstance(input_param, Parameter):\n-                    # backwards compatibility for serialized parameters\n-                    input_param = input_param.data\n+                    self._parameters[name] = input_param.to(param.device, param.dtype)", "path": "torch/nn/modules/module.py", "position": 6, "original_position": 6, "commit_id": "046ba710c6ece3d144d625738ee164c5695c9131", "original_commit_id": "046ba710c6ece3d144d625738ee164c5695c9131", "user": {"login": "nehz", "id": 151358, "node_id": "MDQ6VXNlcjE1MTM1OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/151358?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nehz", "html_url": "https://github.com/nehz", "followers_url": "https://api.github.com/users/nehz/followers", "following_url": "https://api.github.com/users/nehz/following{/other_user}", "gists_url": "https://api.github.com/users/nehz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nehz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nehz/subscriptions", "organizations_url": "https://api.github.com/users/nehz/orgs", "repos_url": "https://api.github.com/users/nehz/repos", "events_url": "https://api.github.com/users/nehz/events{/privacy}", "received_events_url": "https://api.github.com/users/nehz/received_events", "type": "User", "site_admin": false}, "body": "If you're loading a serialized Parameter, then you are expecting `requires_grad` to be replaced, which is fine I think (i.e the purpose of this PR)\r\n\r\nConcerns with optimizer params requires thinking though, so, we could call \"rebuild/copy/clone\" function on Parameter, ideally in addition with some sort of `buffer` storage like in `modules`. (Use case scenario: I am writing some pytorch extensions for FP16 mixed mode, and would like to attach metadata to Parameters for serialization)", "created_at": "2018-09-11T03:37:58Z", "updated_at": "2018-11-23T15:50:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/11464#discussion_r216536979", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11464", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216536979"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11464#discussion_r216536979"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11464"}}, "body_html": "<p>If you're loading a serialized Parameter, then you are expecting <code>requires_grad</code> to be replaced, which is fine I think (i.e the purpose of this PR)</p>\n<p>Concerns with optimizer params requires thinking though, so, we could call \"rebuild/copy/clone\" function on Parameter, ideally in addition with some sort of <code>buffer</code> storage like in <code>modules</code>. (Use case scenario: I am writing some pytorch extensions for FP16 mixed mode, and would like to attach metadata to Parameters for serialization)</p>", "body_text": "If you're loading a serialized Parameter, then you are expecting requires_grad to be replaced, which is fine I think (i.e the purpose of this PR)\nConcerns with optimizer params requires thinking though, so, we could call \"rebuild/copy/clone\" function on Parameter, ideally in addition with some sort of buffer storage like in modules. (Use case scenario: I am writing some pytorch extensions for FP16 mixed mode, and would like to attach metadata to Parameters for serialization)", "in_reply_to_id": 216425577}