{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174935318", "pull_request_review_id": 104383729, "id": 174935318, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDkzNTMxOA==", "diff_hunk": "@@ -116,80 +120,33 @@ static void THPVariable_dealloc(THPVariable* self)\n   Py_TYPE(self)->tp_free((PyObject*)self);\n }\n \n-PyObject *THPVariable_pynew(PyTypeObject *type, PyObject *args, PyObject *kwds)\n+PyObject *THPVariable_pynew(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n {\n   HANDLE_TH_ERRORS\n-  THPObjectPtr _data;\n-  PyObject *data = nullptr;\n-  PyObject *grad_fn = nullptr;\n-  char is_volatile = 0;\n-  char requires_grad = 0;\n-  const char* name = nullptr;\n-\n-  const char *accepted_args[] = {\"data\", \"requires_grad\", \"volatile\", \"_grad_fn\", \"name\", nullptr};\n-  if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|ObbOz\", (char**)accepted_args,\n-      &data, &requires_grad, &is_volatile, &grad_fn, &name))\n-    return nullptr;\n-\n-  if (grad_fn == Py_None)\n-    grad_fn = nullptr;\n-\n-  if (is_volatile) {\n-    PyErr_WarnEx(PyExc_UserWarning, VOLATILE_WARNING, 1);\n+  auto& default_type = torch::tensor::get_default_tensor_type();\n+  if (default_type.is_cuda()) {\n+    torch::utils::cuda_lazy_init();\n   }\n-\n-  THPUtils_assert(!(is_volatile && requires_grad),\n-          \"Variable can't be volatile and require_grad at the same time!\");\n-  THPUtils_assert(!grad_fn || THPFunction_Check(grad_fn),\n-          \"Variable _grad_fn has to be a Function object or None, but got %s\",\n-          THPUtils_typename(grad_fn));\n-  Tensor tensor;\n-  if (!data || data == Py_None) {\n-    // For legacy serialization code, create an empty tensor. This is also used\n-    // by nn.Parameter() with no arguments.\n-    auto var = torch::tensor::get_default_tensor_type().tensor();\n-    tensor = static_cast<Variable&>(var).data();\n-  } else if (THPVariable_Check(data)) {\n-    tensor = ((THPVariable*)data)->cdata.data();\n-  } else {\n-    throw torch::TypeError(\"Variable data has to be a tensor, but got %s\",\n-        THPUtils_typename(data));\n-  }\n-\n-  Variable var;\n-  if (grad_fn) {\n-    auto grad_fn_ = THPFunction_asFunction((THPFunction*)grad_fn);\n-    Edge edge(grad_fn_, grad_fn_->bump_inputs());\n-    var = make_variable(std::move(tensor), std::move(edge));\n-  } else {\n-    var = make_variable(std::move(tensor), requires_grad);\n-  }\n-\n-  if (name) {\n-    var.set_name(name);\n-  }\n-\n-  return THPVariable_NewWithVar(type, std::move(var));\n+  auto tensor = torch::utils::legacy_tensor_ctor(default_type, args, kwargs);\n+  return THPVariable_NewWithVar(type, std::move(tensor));\n   END_HANDLE_TH_ERRORS\n }\n \n-int THPVariable_pyinit(PyObject *self, PyObject *args, PyObject *kwds)\n-{\n-  // Ensures that calls to Variable() and subclasses contain data argument.\n-  // The 'data' argument is optional in __new__ to handle legacy serialized\n-  // Variables.\n-  PyObject *data;\n-  PyObject *grad_fn = nullptr;\n-  char is_volatile = 0;\n-  char requires_grad = 0;\n-  const char* name = nullptr;\n-\n-  const char *accepted_args[] = {\"data\", \"requires_grad\", \"volatile\", \"_grad_fn\", \"name\", nullptr};\n-  if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|ObbOz\", (char**)accepted_args,\n-      &data, &requires_grad, &is_volatile, &grad_fn, &name))\n-    return -1;\n-\n-  return 0;\n+static PyObject* THPVariable_make_subclass(PyObject* _ignored, PyObject* args, PyObject* kwargs) {", "path": "torch/csrc/autograd/python_variable.cpp", "position": null, "original_position": 103, "commit_id": "906c39b9979307ac94cb706a7d5ba03b2befaef4", "original_commit_id": "6e29955fe6a7f5b319d30b536982e1ad5748c0be", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "It instantiates a subclass of torch.Tensor given an existing Tensor. It's used by `nn.Parameter` because that class now subclasses `torch.Tensor` instead of `Variable`.\r\n\r\nWe need a way to create a subclass without calling torch.Tensor() (i.e. `__new__`) because of the restrictions that `Tensor.__new__` imposes.\r\n\r\nFor example,\r\n\r\n```python\r\n>>> torch.Tensor(torch.randn().double()) \r\nTypeError: expected Variable[torch.FloatTensor] (got Variable[torch.DoubleTensor])\r\n```\r\n\r\nBut this works:\r\n\r\n```python\r\n>>> torch.nn.Parameter(torch.randn().double())\r\n```", "created_at": "2018-03-15T21:16:23Z", "updated_at": "2018-11-23T15:40:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/5785#discussion_r174935318", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5785", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174935318"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5785#discussion_r174935318"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5785"}}, "body_html": "<p>It instantiates a subclass of torch.Tensor given an existing Tensor. It's used by <code>nn.Parameter</code> because that class now subclasses <code>torch.Tensor</code> instead of <code>Variable</code>.</p>\n<p>We need a way to create a subclass without calling torch.Tensor() (i.e. <code>__new__</code>) because of the restrictions that <code>Tensor.__new__</code> imposes.</p>\n<p>For example,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.Tensor(torch.randn().double()) \n<span class=\"pl-c1\">TypeError</span>: expected Variable[torch.FloatTensor] (got Variable[torch.DoubleTensor])</pre></div>\n<p>But this works:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.nn.Parameter(torch.randn().double())</pre></div>", "body_text": "It instantiates a subclass of torch.Tensor given an existing Tensor. It's used by nn.Parameter because that class now subclasses torch.Tensor instead of Variable.\nWe need a way to create a subclass without calling torch.Tensor() (i.e. __new__) because of the restrictions that Tensor.__new__ imposes.\nFor example,\n>>> torch.Tensor(torch.randn().double()) \nTypeError: expected Variable[torch.FloatTensor] (got Variable[torch.DoubleTensor])\nBut this works:\n>>> torch.nn.Parameter(torch.randn().double())", "in_reply_to_id": 174930687}