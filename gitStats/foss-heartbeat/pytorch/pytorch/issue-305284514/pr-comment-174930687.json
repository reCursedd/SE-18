{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174930687", "pull_request_review_id": 104377901, "id": 174930687, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDkzMDY4Nw==", "diff_hunk": "@@ -116,80 +120,33 @@ static void THPVariable_dealloc(THPVariable* self)\n   Py_TYPE(self)->tp_free((PyObject*)self);\n }\n \n-PyObject *THPVariable_pynew(PyTypeObject *type, PyObject *args, PyObject *kwds)\n+PyObject *THPVariable_pynew(PyTypeObject *type, PyObject *args, PyObject *kwargs)\n {\n   HANDLE_TH_ERRORS\n-  THPObjectPtr _data;\n-  PyObject *data = nullptr;\n-  PyObject *grad_fn = nullptr;\n-  char is_volatile = 0;\n-  char requires_grad = 0;\n-  const char* name = nullptr;\n-\n-  const char *accepted_args[] = {\"data\", \"requires_grad\", \"volatile\", \"_grad_fn\", \"name\", nullptr};\n-  if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|ObbOz\", (char**)accepted_args,\n-      &data, &requires_grad, &is_volatile, &grad_fn, &name))\n-    return nullptr;\n-\n-  if (grad_fn == Py_None)\n-    grad_fn = nullptr;\n-\n-  if (is_volatile) {\n-    PyErr_WarnEx(PyExc_UserWarning, VOLATILE_WARNING, 1);\n+  auto& default_type = torch::tensor::get_default_tensor_type();\n+  if (default_type.is_cuda()) {\n+    torch::utils::cuda_lazy_init();\n   }\n-\n-  THPUtils_assert(!(is_volatile && requires_grad),\n-          \"Variable can't be volatile and require_grad at the same time!\");\n-  THPUtils_assert(!grad_fn || THPFunction_Check(grad_fn),\n-          \"Variable _grad_fn has to be a Function object or None, but got %s\",\n-          THPUtils_typename(grad_fn));\n-  Tensor tensor;\n-  if (!data || data == Py_None) {\n-    // For legacy serialization code, create an empty tensor. This is also used\n-    // by nn.Parameter() with no arguments.\n-    auto var = torch::tensor::get_default_tensor_type().tensor();\n-    tensor = static_cast<Variable&>(var).data();\n-  } else if (THPVariable_Check(data)) {\n-    tensor = ((THPVariable*)data)->cdata.data();\n-  } else {\n-    throw torch::TypeError(\"Variable data has to be a tensor, but got %s\",\n-        THPUtils_typename(data));\n-  }\n-\n-  Variable var;\n-  if (grad_fn) {\n-    auto grad_fn_ = THPFunction_asFunction((THPFunction*)grad_fn);\n-    Edge edge(grad_fn_, grad_fn_->bump_inputs());\n-    var = make_variable(std::move(tensor), std::move(edge));\n-  } else {\n-    var = make_variable(std::move(tensor), requires_grad);\n-  }\n-\n-  if (name) {\n-    var.set_name(name);\n-  }\n-\n-  return THPVariable_NewWithVar(type, std::move(var));\n+  auto tensor = torch::utils::legacy_tensor_ctor(default_type, args, kwargs);\n+  return THPVariable_NewWithVar(type, std::move(tensor));\n   END_HANDLE_TH_ERRORS\n }\n \n-int THPVariable_pyinit(PyObject *self, PyObject *args, PyObject *kwds)\n-{\n-  // Ensures that calls to Variable() and subclasses contain data argument.\n-  // The 'data' argument is optional in __new__ to handle legacy serialized\n-  // Variables.\n-  PyObject *data;\n-  PyObject *grad_fn = nullptr;\n-  char is_volatile = 0;\n-  char requires_grad = 0;\n-  const char* name = nullptr;\n-\n-  const char *accepted_args[] = {\"data\", \"requires_grad\", \"volatile\", \"_grad_fn\", \"name\", nullptr};\n-  if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|ObbOz\", (char**)accepted_args,\n-      &data, &requires_grad, &is_volatile, &grad_fn, &name))\n-    return -1;\n-\n-  return 0;\n+static PyObject* THPVariable_make_subclass(PyObject* _ignored, PyObject* args, PyObject* kwargs) {", "path": "torch/csrc/autograd/python_variable.cpp", "position": null, "original_position": 103, "commit_id": "906c39b9979307ac94cb706a7d5ba03b2befaef4", "original_commit_id": "6e29955fe6a7f5b319d30b536982e1ad5748c0be", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Is `_make_subclass` a hacky optimization to the constructor of `torch.Tensor`, that doesn't really care which class are you instantiating?", "created_at": "2018-03-15T20:59:13Z", "updated_at": "2018-11-23T15:40:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/5785#discussion_r174930687", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5785", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174930687"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5785#discussion_r174930687"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5785"}}, "body_html": "<p>Is <code>_make_subclass</code> a hacky optimization to the constructor of <code>torch.Tensor</code>, that doesn't really care which class are you instantiating?</p>", "body_text": "Is _make_subclass a hacky optimization to the constructor of torch.Tensor, that doesn't really care which class are you instantiating?"}