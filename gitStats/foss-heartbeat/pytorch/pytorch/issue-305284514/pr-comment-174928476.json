{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174928476", "pull_request_review_id": 104375791, "id": 174928476, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDkyODQ3Ng==", "diff_hunk": "@@ -0,0 +1,126 @@\n+#include \"python_variable.h\"\n+\n+#include <ATen/ATen.h>\n+\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/autograd/python_function.h\"\n+#include \"torch/csrc/autograd/python_variable.h\"\n+#include \"torch/csrc/tensor/python_tensor.h\"\n+\n+using namespace at;\n+\n+namespace torch { namespace autograd {\n+\n+static PyObject *THPVariable_pynew(PyTypeObject* type, PyObject *args, PyObject *kwds) {\n+  HANDLE_TH_ERRORS\n+  THPObjectPtr _data;\n+  PyObject *data = nullptr;\n+  PyObject *grad_fn = nullptr;\n+  char is_volatile = 0;\n+  char requires_grad = 0;\n+  const char* name = nullptr;\n+\n+  const char *accepted_args[] = {\"data\", \"requires_grad\", \"volatile\", \"_grad_fn\", \"name\", nullptr};\n+  if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|ObbOz\", (char**)accepted_args,\n+      &data, &requires_grad, &is_volatile, &grad_fn, &name))\n+    return nullptr;\n+\n+  if (grad_fn == Py_None)\n+    grad_fn = nullptr;\n+\n+  if (is_volatile) {\n+    PyErr_WarnEx(PyExc_UserWarning,\n+        \"volatile was removed and now has no effect. Use `with torch.no_grad():` \"\n+        \"instead.\", 1);\n+  }\n+\n+  if (is_volatile && requires_grad) {\n+    throw ValueError(\"Variable can't be volatile and require_grad at the same time!\");\n+  }\n+  if (grad_fn && !THPFunction_Check(grad_fn)) {\n+    throw TypeError(\"_grad_fn has to be a Function object or None, but got %s\",\n+        Py_TYPE(grad_fn)->tp_name);\n+  }\n+  Tensor tensor;\n+  if (!data || data == Py_None) {\n+    // For legacy serialization code, create an empty tensor. This is also used\n+    // by nn.Parameter() with no arguments.\n+    auto var = torch::tensor::get_default_tensor_type().tensor();\n+    tensor = static_cast<Variable&>(var).data();\n+  } else if (THPVariable_Check(data)) {\n+    tensor = ((THPVariable*)data)->cdata.data();\n+  } else {\n+    throw torch::TypeError(\"Variable data has to be a tensor, but got %s\",\n+        Py_TYPE(data)->tp_name);\n+  }\n+\n+  Variable var;\n+  if (grad_fn) {\n+    auto grad_fn_ = THPFunction_asFunction((THPFunction*)grad_fn);\n+    Edge edge(grad_fn_, grad_fn_->bump_inputs());\n+    var = make_variable(std::move(tensor), std::move(edge));", "path": "torch/csrc/autograd/python_legacy_variable.cpp", "position": 61, "original_position": 61, "commit_id": "906c39b9979307ac94cb706a7d5ba03b2befaef4", "original_commit_id": "6e29955fe6a7f5b319d30b536982e1ad5748c0be", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Yeah a non-empty edge implies `var.requires_grad() == true`. `make_variable` has two constructors now, so if you pass an edge you can't pass `requires_grad`:\r\n\r\n```\r\nmake_variable(at::Tensor data, bool requires_grad = false);\r\nmake_variable(at::Tensor data, Edge gradient_edge);\r\n```", "created_at": "2018-03-15T20:51:40Z", "updated_at": "2018-11-23T15:40:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/5785#discussion_r174928476", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5785", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174928476"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5785#discussion_r174928476"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5785"}}, "body_html": "<p>Yeah a non-empty edge implies <code>var.requires_grad() == true</code>. <code>make_variable</code> has two constructors now, so if you pass an edge you can't pass <code>requires_grad</code>:</p>\n<pre><code>make_variable(at::Tensor data, bool requires_grad = false);\nmake_variable(at::Tensor data, Edge gradient_edge);\n</code></pre>", "body_text": "Yeah a non-empty edge implies var.requires_grad() == true. make_variable has two constructors now, so if you pass an edge you can't pass requires_grad:\nmake_variable(at::Tensor data, bool requires_grad = false);\nmake_variable(at::Tensor data, Edge gradient_edge);", "in_reply_to_id": 174892650}