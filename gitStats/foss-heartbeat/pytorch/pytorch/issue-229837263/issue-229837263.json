{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1591", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1591/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1591/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1591/events", "html_url": "https://github.com/pytorch/pytorch/issues/1591", "id": 229837263, "node_id": "MDU6SXNzdWUyMjk4MzcyNjM=", "number": 1591, "title": "Pad PackedSequences to original batch length", "user": {"login": "ajfisch", "id": 11946777, "node_id": "MDQ6VXNlcjExOTQ2Nzc3", "avatar_url": "https://avatars0.githubusercontent.com/u/11946777?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajfisch", "html_url": "https://github.com/ajfisch", "followers_url": "https://api.github.com/users/ajfisch/followers", "following_url": "https://api.github.com/users/ajfisch/following{/other_user}", "gists_url": "https://api.github.com/users/ajfisch/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajfisch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajfisch/subscriptions", "organizations_url": "https://api.github.com/users/ajfisch/orgs", "repos_url": "https://api.github.com/users/ajfisch/repos", "events_url": "https://api.github.com/users/ajfisch/events{/privacy}", "received_events_url": "https://api.github.com/users/ajfisch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 838476895, "node_id": "MDU6TGFiZWw4Mzg0NzY4OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/hackamonth", "name": "hackamonth", "color": "0e8a16", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-05-19T00:10:18Z", "updated_at": "2018-02-15T19:11:28Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>The current flow for handling variable length sequences in RNNs is:</p>\n<div class=\"highlight highlight-source-python\"><pre>packed_input <span class=\"pl-k\">=</span> torch.nn.utils.rnn.pack_padded_sequence(<span class=\"pl-c1\">input</span>, lengths)\npacked_output <span class=\"pl-k\">=</span> rnn(packed)[<span class=\"pl-c1\">0</span>]\noutput <span class=\"pl-k\">=</span> torch.nn.utils.rnn.pad_packed_sequence(packed_output)</pre></div>\n<p>The output size of the Variable returned by pad_packed_sequence is determined by the max length in lengths: <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106\">https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106</a>.</p>\n<p>However, this only works in recovering the original size of the input if the max length sequence has no padding (max length == length dim of batched input). For normal, sensible batching this makes sense and should be true.</p>\n<p>But if a model is using, say, DataParallel, the batch might be split such that there <strong>is</strong> extra padding. And the output size from the RNN will be truncated (which might break other things down-stream).</p>\n<p>To fix this potentially unexpected behavior, I propose two possible simple patches.</p>\n<ol>\n<li>\n<p>A <code>max_batch_size</code> field is calculated from the original input and added to the PackedSequence namedtuple: <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L6\">https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L6</a> and used instead of <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106\">https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106</a></p>\n</li>\n<li>\n<p>An optional <code>max_batch_size</code> parameter is added to pad_packed_sequence which would be used as an override.</p>\n</li>\n</ol>\n<p>I prefer 1), but I suppose 2) has the advantage of being fully backwards compatible (even though it is discouraged to be directly creating or meddling with PackedSequence tuples, it is possible).</p>", "body_text": "The current flow for handling variable length sequences in RNNs is:\npacked_input = torch.nn.utils.rnn.pack_padded_sequence(input, lengths)\npacked_output = rnn(packed)[0]\noutput = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\nThe output size of the Variable returned by pad_packed_sequence is determined by the max length in lengths: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106.\nHowever, this only works in recovering the original size of the input if the max length sequence has no padding (max length == length dim of batched input). For normal, sensible batching this makes sense and should be true.\nBut if a model is using, say, DataParallel, the batch might be split such that there is extra padding. And the output size from the RNN will be truncated (which might break other things down-stream).\nTo fix this potentially unexpected behavior, I propose two possible simple patches.\n\n\nA max_batch_size field is calculated from the original input and added to the PackedSequence namedtuple: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L6 and used instead of https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106\n\n\nAn optional max_batch_size parameter is added to pad_packed_sequence which would be used as an override.\n\n\nI prefer 1), but I suppose 2) has the advantage of being fully backwards compatible (even though it is discouraged to be directly creating or meddling with PackedSequence tuples, it is possible).", "body": "The current flow for handling variable length sequences in RNNs is:\r\n\r\n```python\r\npacked_input = torch.nn.utils.rnn.pack_padded_sequence(input, lengths)\r\npacked_output = rnn(packed)[0]\r\noutput = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\r\n```\r\n\r\nThe output size of the Variable returned by pad_packed_sequence is determined by the max length in lengths: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106.\r\n\r\nHowever, this only works in recovering the original size of the input if the max length sequence has no padding (max length == length dim of batched input). For normal, sensible batching this makes sense and should be true.\r\n\r\nBut if a model is using, say, DataParallel, the batch might be split such that there __is__ extra padding. And the output size from the RNN will be truncated (which might break other things down-stream).\r\n\r\nTo fix this potentially unexpected behavior, I propose two possible simple patches.\r\n\r\n1) A `max_batch_size` field is calculated from the original input and added to the PackedSequence namedtuple: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L6 and used instead of https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106\r\n\r\n2) An optional `max_batch_size` parameter is added to pad_packed_sequence which would be used as an override.\r\n\r\nI prefer 1), but I suppose 2) has the advantage of being fully backwards compatible (even though it is discouraged to be directly creating or meddling with PackedSequence tuples, it is possible)."}