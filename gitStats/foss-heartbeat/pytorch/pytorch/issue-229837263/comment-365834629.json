{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/365834629", "html_url": "https://github.com/pytorch/pytorch/issues/1591#issuecomment-365834629", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1591", "id": 365834629, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTgzNDYyOQ==", "user": {"login": "manasRK", "id": 6917857, "node_id": "MDQ6VXNlcjY5MTc4NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6917857?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manasRK", "html_url": "https://github.com/manasRK", "followers_url": "https://api.github.com/users/manasRK/followers", "following_url": "https://api.github.com/users/manasRK/following{/other_user}", "gists_url": "https://api.github.com/users/manasRK/gists{/gist_id}", "starred_url": "https://api.github.com/users/manasRK/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manasRK/subscriptions", "organizations_url": "https://api.github.com/users/manasRK/orgs", "repos_url": "https://api.github.com/users/manasRK/repos", "events_url": "https://api.github.com/users/manasRK/events{/privacy}", "received_events_url": "https://api.github.com/users/manasRK/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-15T06:09:27Z", "updated_at": "2018-02-15T06:09:27Z", "author_association": "NONE", "body_html": "<p>For people who stumble upon this thread in the future, this is how we are handling it currently (for now);</p>\n<p>First, the usual <code>pack_padded_sequence</code> and <code>pad_packed_sequence</code> for handling variable length sequences;</p>\n<div class=\"highlight highlight-source-python\"><pre>seq_len, bsz, n_dims <span class=\"pl-k\">=</span> feats.size()\npacked_input <span class=\"pl-k\">=</span> pack_padded_sequence(feats, lengths, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\npacked_output, <span class=\"pl-c1\">self</span>.hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(packed_input, <span class=\"pl-c1\">self</span>.hidden)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> lstm_out --&gt; seqlen X bsz X hidden_dim</span>\nlstm_out, output_lengths <span class=\"pl-k\">=</span> pad_packed_sequence(packed_output, <span class=\"pl-v\">batch_first</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)</pre></div>\n<p>Then the hack is implemented as the output size of the Variable returned by pad_packed_sequence is determined by the max length in <code>output_lengths</code>, not <code>seqlen</code> in batch. Also, you may have to hardcode <code>MAXLEN</code> in sequence/loss masking procedures;</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> lstm_out.size(<span class=\"pl-c1\">0</span>)<span class=\"pl-k\">&lt;</span>seq_len:\n    dummy_tensor <span class=\"pl-k\">=</span> autograd.Variable(torch.zeros(seq_len<span class=\"pl-k\">-</span>lstm_out.size(<span class=\"pl-c1\">0</span>), bsz, <span class=\"pl-c1\">self</span>.hidden_dim))\n    lstm_out <span class=\"pl-k\">=</span> torch.cat([lstm_out, dummy_tensor], <span class=\"pl-c1\">0</span>)</pre></div>\n<p>Our accuracy metrics have remained stable and predictions in line with our expectations, so I think this hack works well.</p>", "body_text": "For people who stumble upon this thread in the future, this is how we are handling it currently (for now);\nFirst, the usual pack_padded_sequence and pad_packed_sequence for handling variable length sequences;\nseq_len, bsz, n_dims = feats.size()\npacked_input = pack_padded_sequence(feats, lengths, batch_first=False)\npacked_output, self.hidden = self.lstm(packed_input, self.hidden)\n# lstm_out --> seqlen X bsz X hidden_dim\nlstm_out, output_lengths = pad_packed_sequence(packed_output, batch_first = False)\nThen the hack is implemented as the output size of the Variable returned by pad_packed_sequence is determined by the max length in output_lengths, not seqlen in batch. Also, you may have to hardcode MAXLEN in sequence/loss masking procedures;\nif lstm_out.size(0)<seq_len:\n    dummy_tensor = autograd.Variable(torch.zeros(seq_len-lstm_out.size(0), bsz, self.hidden_dim))\n    lstm_out = torch.cat([lstm_out, dummy_tensor], 0)\nOur accuracy metrics have remained stable and predictions in line with our expectations, so I think this hack works well.", "body": "For people who stumble upon this thread in the future, this is how we are handling it currently (for now);\r\n\r\nFirst, the usual `pack_padded_sequence` and `pad_packed_sequence` for handling variable length sequences;\r\n\r\n```python\r\nseq_len, bsz, n_dims = feats.size()\r\npacked_input = pack_padded_sequence(feats, lengths, batch_first=False)\r\npacked_output, self.hidden = self.lstm(packed_input, self.hidden)\r\n# lstm_out --> seqlen X bsz X hidden_dim\r\nlstm_out, output_lengths = pad_packed_sequence(packed_output, batch_first = False)\r\n```\r\n\r\nThen the hack is implemented as the output size of the Variable returned by pad_packed_sequence is determined by the max length in `output_lengths`, not `seqlen` in batch. Also, you may have to hardcode `MAXLEN` in sequence/loss masking procedures;\r\n\r\n```python    \r\nif lstm_out.size(0)<seq_len:\r\n    dummy_tensor = autograd.Variable(torch.zeros(seq_len-lstm_out.size(0), bsz, self.hidden_dim))\r\n    lstm_out = torch.cat([lstm_out, dummy_tensor], 0)\r\n```\r\n\r\nOur accuracy metrics have remained stable and predictions in line with our expectations, so I think this hack works well. "}