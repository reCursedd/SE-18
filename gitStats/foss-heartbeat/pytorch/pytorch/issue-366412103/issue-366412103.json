{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12286", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12286/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12286/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12286/events", "html_url": "https://github.com/pytorch/pytorch/issues/12286", "id": 366412103, "node_id": "MDU6SXNzdWUzNjY0MTIxMDM=", "number": 12286, "title": "TestJit.test_lstm_fusion_concat_cuda fails for some inputs", "user": {"login": "hartb", "id": 18429659, "node_id": "MDQ6VXNlcjE4NDI5NjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/18429659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hartb", "html_url": "https://github.com/hartb", "followers_url": "https://api.github.com/users/hartb/followers", "following_url": "https://api.github.com/users/hartb/following{/other_user}", "gists_url": "https://api.github.com/users/hartb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hartb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hartb/subscriptions", "organizations_url": "https://api.github.com/users/hartb/orgs", "repos_url": "https://api.github.com/users/hartb/repos", "events_url": "https://api.github.com/users/hartb/events{/privacy}", "received_events_url": "https://api.github.com/users/hartb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-10-03T16:00:59Z", "updated_at": "2018-10-16T16:34:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Test <code>test_lstm_fusion_concat_cuda</code> in the <code>jit</code> test bucket fails for<br>\ncertain inputs:</p>\n<pre><code>$ python run_test.py -i jit -- TestJit.test_lstm_fusion_concat_cuda\nRunning test_jit ...\nF\n======================================================================\nFAIL: test_lstm_fusion_concat_cuda (__main__.TestJit)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/hartb/test2/common.py\", line 275, in wrapper\n    method(*args, **kwargs)\n  File \"/home/hartb/test2/common.py\", line 109, in wrapper\n    fn(*args, **kwargs)\n  File \"test_jit.py\", line 691, in test_lstm_fusion_concat_cuda\n    ge = self.checkTrace(LSTMCellC, inputs)\n  File \"test_jit.py\", line 368, in checkTrace\n    self.assertTrue(torch.allclose(g2, g2_ge, atol=7e-4, rtol=1e-4))\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 1 test in 5.639s\n\nFAILED (failures=1)\n</code></pre>\n<p>The failure occurs in current <code>master</code> (through at least <code>v1.0rc1</code>).</p>\n<p>The failure is seen on both:</p>\n<ul>\n<li>ppc64le / CUDA 10.0 / NVIDIA V100 GPU / RHEL 7.5</li>\n<li>x86 / CUDA 9.2 / NVIDIA K80 GPU / Ubuntu 16.04</li>\n</ul>\n<p>The failure depends on the input data. The test will <em>usually</em> pass,<br>\ndepending on the state of the RNG when the test is reached.</p>\n<p>A failure can be forced by seeding the RNG with specific values. For<br>\nexample:</p>\n<pre><code>$ git diff -- test_jit.py\ndiff --git a/test/test_jit.py b/test/test_jit.py\nindex 24d8076..a9f6a83 100644\n--- a/test/test_jit.py\n+++ b/test/test_jit.py\n@@ -686,6 +686,7 @@ class TestJit(JitTestCase):\n     @unittest.skipIf(not RUN_CUDA, \"fuser requires CUDA\")\n     @skipIfRocm\n     def test_lstm_fusion_concat_cuda(self):\n+        torch.manual_seed(122)\n         inputs = get_lstm_inputs('cuda')\n         ge = self.checkTrace(LSTMCellC, inputs)\n         self.assertExpectedGraph(ge.graph_for(*inputs))\n</code></pre>\n<p>The cause of the failure is that the difference in two sets of<br>\ncalculated gradients is out of tolerance (<code>7e-4</code>). Inspecting the<br>\nout-of-tolerance differences shows that both platforms calculate the<br>\nsame gradient pairs, and that the differences between the gradients in<br>\neach pair is close but somewhat outside the tolerance of the test:</p>\n<p>On ppc64le / CUDA 10 / V100 / RHEL 7.5:</p>\n<pre><code>Running test_jit ...\ncommand run python test_jit.py TestJit.test_lstm_fusion_concat_cuda\nge[1][5] =  -2698.955078125, g2_ge[1][5] =  -2698.956298828, abs diff = 0.001220703125\nge[1][6] =     60.063949585, g2_ge[1][6] =     60.062801361, abs diff = 0.00114822387695\nge[1][7] =  -2317.295166016, g2_ge[1][7] =  -2317.296142578, abs diff = 0.0009765625\nge[2][0] =   1820.975341797, g2_ge[2][0] =   1820.976318359, abs diff = 0.0009765625\nge[2][3] =  -9474.621093750, g2_ge[2][3] =  -9474.622070312, abs diff = 0.0009765625\n...\nge[71][6] =  -2849.033447266, g2_ge[71][6] =  -2849.034179688, abs diff = 0.000732421875\nge[73][0] =  -4872.507324219, g2_ge[73][0] =  -4872.508789062, abs diff = 0.00146484375\nge[73][4] =   1764.805541992, g2_ge[73][4] =   1764.806274414, abs diff = 0.000732421875\nge[73][6] =   2349.417968750, g2_ge[73][6] =   2349.418701172, abs diff = 0.000732421875\nge[73][8] =  -3767.620849609, g2_ge[73][8] =  -3767.622070312, abs diff = 0.001220703125\n...\n</code></pre>\n<p>On x86 / CUDA 9.2 / K80 / Ubuntu 16.04:</p>\n<pre><code>Running test_jit ...\nge[1][5] =  -2698.955078125, g2_ge[1][5] =  -2698.956298828, abs diff = 0.001220703125\nge[1][6] =     60.063949585, g2_ge[1][6] =     60.062801361, abs diff = 0.00114822387695\nge[1][7] =  -2317.295166016, g2_ge[1][7] =  -2317.296142578, abs diff = 0.0009765625\nge[2][0] =   1820.975341797, g2_ge[2][0] =   1820.976318359, abs diff = 0.0009765625\nge[2][3] =  -9474.621093750, g2_ge[2][3] =  -9474.622070312, abs diff = 0.0009765625\n...\nge[71][6] =  -2849.033447266, g2_ge[71][6] =  -2849.034179688, abs diff = 0.000732421875\nge[73][0] =  -4872.507324219, g2_ge[73][0] =  -4872.508789062, abs diff = 0.00146484375\nge[73][4] =   1764.805541992, g2_ge[73][4] =   1764.806274414, abs diff = 0.000732421875\nge[73][6] =   2349.417968750, g2_ge[73][6] =   2349.418701172, abs diff = 0.000732421875\nge[73][8] =  -3767.620849609, g2_ge[73][8] =  -3767.622070312, abs diff = 0.001220703125\n</code></pre>\n<p>Given the absolute values of the gradients (often falling in the <code>2^10</code><br>\nto <code>2^16</code> range) and the data type (<code>float32</code>), the differences amount to<br>\nbeing off by a few units (~3-5) in the least significant bits.</p>\n<p>So I think there's not a fundamental problem here, just rounding errors<br>\nfor certain inputs that pushes the result outside the test's tolerance.</p>\n<p>If so, then possible solutions for this would include, at least:</p>\n<ul>\n<li>relax the tolerance for this test</li>\n<li>seed the RNG with a value that will generate well-behaved inputs (any<br>\nin the range 1 - 121 should pass)</li>\n</ul>", "body_text": "Test test_lstm_fusion_concat_cuda in the jit test bucket fails for\ncertain inputs:\n$ python run_test.py -i jit -- TestJit.test_lstm_fusion_concat_cuda\nRunning test_jit ...\nF\n======================================================================\nFAIL: test_lstm_fusion_concat_cuda (__main__.TestJit)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/hartb/test2/common.py\", line 275, in wrapper\n    method(*args, **kwargs)\n  File \"/home/hartb/test2/common.py\", line 109, in wrapper\n    fn(*args, **kwargs)\n  File \"test_jit.py\", line 691, in test_lstm_fusion_concat_cuda\n    ge = self.checkTrace(LSTMCellC, inputs)\n  File \"test_jit.py\", line 368, in checkTrace\n    self.assertTrue(torch.allclose(g2, g2_ge, atol=7e-4, rtol=1e-4))\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 1 test in 5.639s\n\nFAILED (failures=1)\n\nThe failure occurs in current master (through at least v1.0rc1).\nThe failure is seen on both:\n\nppc64le / CUDA 10.0 / NVIDIA V100 GPU / RHEL 7.5\nx86 / CUDA 9.2 / NVIDIA K80 GPU / Ubuntu 16.04\n\nThe failure depends on the input data. The test will usually pass,\ndepending on the state of the RNG when the test is reached.\nA failure can be forced by seeding the RNG with specific values. For\nexample:\n$ git diff -- test_jit.py\ndiff --git a/test/test_jit.py b/test/test_jit.py\nindex 24d8076..a9f6a83 100644\n--- a/test/test_jit.py\n+++ b/test/test_jit.py\n@@ -686,6 +686,7 @@ class TestJit(JitTestCase):\n     @unittest.skipIf(not RUN_CUDA, \"fuser requires CUDA\")\n     @skipIfRocm\n     def test_lstm_fusion_concat_cuda(self):\n+        torch.manual_seed(122)\n         inputs = get_lstm_inputs('cuda')\n         ge = self.checkTrace(LSTMCellC, inputs)\n         self.assertExpectedGraph(ge.graph_for(*inputs))\n\nThe cause of the failure is that the difference in two sets of\ncalculated gradients is out of tolerance (7e-4). Inspecting the\nout-of-tolerance differences shows that both platforms calculate the\nsame gradient pairs, and that the differences between the gradients in\neach pair is close but somewhat outside the tolerance of the test:\nOn ppc64le / CUDA 10 / V100 / RHEL 7.5:\nRunning test_jit ...\ncommand run python test_jit.py TestJit.test_lstm_fusion_concat_cuda\nge[1][5] =  -2698.955078125, g2_ge[1][5] =  -2698.956298828, abs diff = 0.001220703125\nge[1][6] =     60.063949585, g2_ge[1][6] =     60.062801361, abs diff = 0.00114822387695\nge[1][7] =  -2317.295166016, g2_ge[1][7] =  -2317.296142578, abs diff = 0.0009765625\nge[2][0] =   1820.975341797, g2_ge[2][0] =   1820.976318359, abs diff = 0.0009765625\nge[2][3] =  -9474.621093750, g2_ge[2][3] =  -9474.622070312, abs diff = 0.0009765625\n...\nge[71][6] =  -2849.033447266, g2_ge[71][6] =  -2849.034179688, abs diff = 0.000732421875\nge[73][0] =  -4872.507324219, g2_ge[73][0] =  -4872.508789062, abs diff = 0.00146484375\nge[73][4] =   1764.805541992, g2_ge[73][4] =   1764.806274414, abs diff = 0.000732421875\nge[73][6] =   2349.417968750, g2_ge[73][6] =   2349.418701172, abs diff = 0.000732421875\nge[73][8] =  -3767.620849609, g2_ge[73][8] =  -3767.622070312, abs diff = 0.001220703125\n...\n\nOn x86 / CUDA 9.2 / K80 / Ubuntu 16.04:\nRunning test_jit ...\nge[1][5] =  -2698.955078125, g2_ge[1][5] =  -2698.956298828, abs diff = 0.001220703125\nge[1][6] =     60.063949585, g2_ge[1][6] =     60.062801361, abs diff = 0.00114822387695\nge[1][7] =  -2317.295166016, g2_ge[1][7] =  -2317.296142578, abs diff = 0.0009765625\nge[2][0] =   1820.975341797, g2_ge[2][0] =   1820.976318359, abs diff = 0.0009765625\nge[2][3] =  -9474.621093750, g2_ge[2][3] =  -9474.622070312, abs diff = 0.0009765625\n...\nge[71][6] =  -2849.033447266, g2_ge[71][6] =  -2849.034179688, abs diff = 0.000732421875\nge[73][0] =  -4872.507324219, g2_ge[73][0] =  -4872.508789062, abs diff = 0.00146484375\nge[73][4] =   1764.805541992, g2_ge[73][4] =   1764.806274414, abs diff = 0.000732421875\nge[73][6] =   2349.417968750, g2_ge[73][6] =   2349.418701172, abs diff = 0.000732421875\nge[73][8] =  -3767.620849609, g2_ge[73][8] =  -3767.622070312, abs diff = 0.001220703125\n\nGiven the absolute values of the gradients (often falling in the 2^10\nto 2^16 range) and the data type (float32), the differences amount to\nbeing off by a few units (~3-5) in the least significant bits.\nSo I think there's not a fundamental problem here, just rounding errors\nfor certain inputs that pushes the result outside the test's tolerance.\nIf so, then possible solutions for this would include, at least:\n\nrelax the tolerance for this test\nseed the RNG with a value that will generate well-behaved inputs (any\nin the range 1 - 121 should pass)", "body": "Test `test_lstm_fusion_concat_cuda` in the `jit` test bucket fails for\r\ncertain inputs:\r\n\r\n```\r\n$ python run_test.py -i jit -- TestJit.test_lstm_fusion_concat_cuda\r\nRunning test_jit ...\r\nF\r\n======================================================================\r\nFAIL: test_lstm_fusion_concat_cuda (__main__.TestJit)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/hartb/test2/common.py\", line 275, in wrapper\r\n    method(*args, **kwargs)\r\n  File \"/home/hartb/test2/common.py\", line 109, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_jit.py\", line 691, in test_lstm_fusion_concat_cuda\r\n    ge = self.checkTrace(LSTMCellC, inputs)\r\n  File \"test_jit.py\", line 368, in checkTrace\r\n    self.assertTrue(torch.allclose(g2, g2_ge, atol=7e-4, rtol=1e-4))\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 5.639s\r\n\r\nFAILED (failures=1)\r\n```\r\n\r\nThe failure occurs in current `master` (through at least `v1.0rc1`).\r\n\r\nThe failure is seen on both:\r\n\r\n- ppc64le / CUDA 10.0 / NVIDIA V100 GPU / RHEL 7.5\r\n- x86 / CUDA 9.2 / NVIDIA K80 GPU / Ubuntu 16.04\r\n\r\nThe failure depends on the input data. The test will _usually_ pass,\r\ndepending on the state of the RNG when the test is reached.\r\n\r\nA failure can be forced by seeding the RNG with specific values. For\r\nexample:\r\n\r\n```\r\n$ git diff -- test_jit.py\r\ndiff --git a/test/test_jit.py b/test/test_jit.py\r\nindex 24d8076..a9f6a83 100644\r\n--- a/test/test_jit.py\r\n+++ b/test/test_jit.py\r\n@@ -686,6 +686,7 @@ class TestJit(JitTestCase):\r\n     @unittest.skipIf(not RUN_CUDA, \"fuser requires CUDA\")\r\n     @skipIfRocm\r\n     def test_lstm_fusion_concat_cuda(self):\r\n+        torch.manual_seed(122)\r\n         inputs = get_lstm_inputs('cuda')\r\n         ge = self.checkTrace(LSTMCellC, inputs)\r\n         self.assertExpectedGraph(ge.graph_for(*inputs))\r\n```\r\n\r\n\r\nThe cause of the failure is that the difference in two sets of\r\ncalculated gradients is out of tolerance (`7e-4`). Inspecting the\r\nout-of-tolerance differences shows that both platforms calculate the\r\nsame gradient pairs, and that the differences between the gradients in\r\neach pair is close but somewhat outside the tolerance of the test:\r\n\r\nOn ppc64le / CUDA 10 / V100 / RHEL 7.5:\r\n\r\n```\r\nRunning test_jit ...\r\ncommand run python test_jit.py TestJit.test_lstm_fusion_concat_cuda\r\nge[1][5] =  -2698.955078125, g2_ge[1][5] =  -2698.956298828, abs diff = 0.001220703125\r\nge[1][6] =     60.063949585, g2_ge[1][6] =     60.062801361, abs diff = 0.00114822387695\r\nge[1][7] =  -2317.295166016, g2_ge[1][7] =  -2317.296142578, abs diff = 0.0009765625\r\nge[2][0] =   1820.975341797, g2_ge[2][0] =   1820.976318359, abs diff = 0.0009765625\r\nge[2][3] =  -9474.621093750, g2_ge[2][3] =  -9474.622070312, abs diff = 0.0009765625\r\n...\r\nge[71][6] =  -2849.033447266, g2_ge[71][6] =  -2849.034179688, abs diff = 0.000732421875\r\nge[73][0] =  -4872.507324219, g2_ge[73][0] =  -4872.508789062, abs diff = 0.00146484375\r\nge[73][4] =   1764.805541992, g2_ge[73][4] =   1764.806274414, abs diff = 0.000732421875\r\nge[73][6] =   2349.417968750, g2_ge[73][6] =   2349.418701172, abs diff = 0.000732421875\r\nge[73][8] =  -3767.620849609, g2_ge[73][8] =  -3767.622070312, abs diff = 0.001220703125\r\n...\r\n```\r\n\r\nOn x86 / CUDA 9.2 / K80 / Ubuntu 16.04:\r\n\r\n```\r\nRunning test_jit ...\r\nge[1][5] =  -2698.955078125, g2_ge[1][5] =  -2698.956298828, abs diff = 0.001220703125\r\nge[1][6] =     60.063949585, g2_ge[1][6] =     60.062801361, abs diff = 0.00114822387695\r\nge[1][7] =  -2317.295166016, g2_ge[1][7] =  -2317.296142578, abs diff = 0.0009765625\r\nge[2][0] =   1820.975341797, g2_ge[2][0] =   1820.976318359, abs diff = 0.0009765625\r\nge[2][3] =  -9474.621093750, g2_ge[2][3] =  -9474.622070312, abs diff = 0.0009765625\r\n...\r\nge[71][6] =  -2849.033447266, g2_ge[71][6] =  -2849.034179688, abs diff = 0.000732421875\r\nge[73][0] =  -4872.507324219, g2_ge[73][0] =  -4872.508789062, abs diff = 0.00146484375\r\nge[73][4] =   1764.805541992, g2_ge[73][4] =   1764.806274414, abs diff = 0.000732421875\r\nge[73][6] =   2349.417968750, g2_ge[73][6] =   2349.418701172, abs diff = 0.000732421875\r\nge[73][8] =  -3767.620849609, g2_ge[73][8] =  -3767.622070312, abs diff = 0.001220703125\r\n```\r\n\r\nGiven the absolute values of the gradients (often falling in the `2^10`\r\nto `2^16` range) and the data type (`float32`), the differences amount to\r\nbeing off by a few units (~3-5) in the least significant bits.\r\n\r\nSo I think there's not a fundamental problem here, just rounding errors\r\nfor certain inputs that pushes the result outside the test's tolerance.\r\n\r\nIf so, then possible solutions for this would include, at least:\r\n\r\n- relax the tolerance for this test\r\n- seed the RNG with a value that will generate well-behaved inputs (any\r\n  in the range 1 - 121 should pass)\r\n"}