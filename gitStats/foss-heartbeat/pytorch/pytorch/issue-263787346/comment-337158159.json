{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337158159", "html_url": "https://github.com/pytorch/pytorch/pull/3028#issuecomment-337158159", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3028", "id": 337158159, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzE1ODE1OQ==", "user": {"login": "ahkarami", "id": 28620609, "node_id": "MDQ6VXNlcjI4NjIwNjA5", "avatar_url": "https://avatars0.githubusercontent.com/u/28620609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahkarami", "html_url": "https://github.com/ahkarami", "followers_url": "https://api.github.com/users/ahkarami/followers", "following_url": "https://api.github.com/users/ahkarami/following{/other_user}", "gists_url": "https://api.github.com/users/ahkarami/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahkarami/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahkarami/subscriptions", "organizations_url": "https://api.github.com/users/ahkarami/orgs", "repos_url": "https://api.github.com/users/ahkarami/repos", "events_url": "https://api.github.com/users/ahkarami/events{/privacy}", "received_events_url": "https://api.github.com/users/ahkarami/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T08:32:11Z", "updated_at": "2017-10-17T08:32:11Z", "author_association": "NONE", "body_html": "<p>Dear <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> &amp; <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>,<br>\nThank you for your time &amp; response. PyTorch is an excellent deep learning framework (nice &amp; user friendly syntax, appropriate documentation and etc.). However, unfortunately PyTorch has some major &amp; important problems with Multi-GPU utilization under training (especially on extremely Large-scale data sets such as Full ImageNet). We have used PyTorch for training some standard CNN models (e.g., DenseNet and ResNet)  on such data set on a machines with Multi-GPU (e.g., a machine with 8 K80 GPU, and a machine with 7 1080Ti GPU), but there are serious problems with it as below:<br>\n1- The Main GPU (e.g., the GPU Id = 0 in most cases) becomes a bottleneck and when we increase the number of GPUs this problem amplified. It means by increasing the number of GPUs  the memory usage of the main GPU increases.<br>\n2- Especially for training on Large-Scale Data Sets, the GPU utilization (and RAM &amp; CPU utilization) is not appropriate. I mean, we saw that the behavior of GPU utilization is swinging and also it rarely used 100% of power of the GPUs and in most of the time GPU utilization percentage is about 30-40% for example. maybe GPUs are waiting for the Data Loader for loading data. This important issue caused that for example the required time for one epoch is about <strong>20 hours</strong> differs from other frameworks such as MXNet.<br>\nIt is worth noting that we have used standard training code such as (<a href=\"https://github.com/pytorch/examples/tree/master/imagenet\">https://github.com/pytorch/examples/tree/master/imagenet</a>) and also I think It is better that the PyTorch team arrange a program for tackling with Multi-GPU utilization problems and handling appropriately large-scale data sets (for example by adding classes to convert such data sets' samples into an appropriate format (like TF-Record in TensorFlow)).<br>\nI hope these problems and issues can be solved soon and we will enjoy to use PyTorch as the best deep learning framework for our research works.<br>\nThank you very much</p>", "body_text": "Dear @ngimel & @soumith,\nThank you for your time & response. PyTorch is an excellent deep learning framework (nice & user friendly syntax, appropriate documentation and etc.). However, unfortunately PyTorch has some major & important problems with Multi-GPU utilization under training (especially on extremely Large-scale data sets such as Full ImageNet). We have used PyTorch for training some standard CNN models (e.g., DenseNet and ResNet)  on such data set on a machines with Multi-GPU (e.g., a machine with 8 K80 GPU, and a machine with 7 1080Ti GPU), but there are serious problems with it as below:\n1- The Main GPU (e.g., the GPU Id = 0 in most cases) becomes a bottleneck and when we increase the number of GPUs this problem amplified. It means by increasing the number of GPUs  the memory usage of the main GPU increases.\n2- Especially for training on Large-Scale Data Sets, the GPU utilization (and RAM & CPU utilization) is not appropriate. I mean, we saw that the behavior of GPU utilization is swinging and also it rarely used 100% of power of the GPUs and in most of the time GPU utilization percentage is about 30-40% for example. maybe GPUs are waiting for the Data Loader for loading data. This important issue caused that for example the required time for one epoch is about 20 hours differs from other frameworks such as MXNet.\nIt is worth noting that we have used standard training code such as (https://github.com/pytorch/examples/tree/master/imagenet) and also I think It is better that the PyTorch team arrange a program for tackling with Multi-GPU utilization problems and handling appropriately large-scale data sets (for example by adding classes to convert such data sets' samples into an appropriate format (like TF-Record in TensorFlow)).\nI hope these problems and issues can be solved soon and we will enjoy to use PyTorch as the best deep learning framework for our research works.\nThank you very much", "body": "Dear @ngimel & @soumith,\r\nThank you for your time & response. PyTorch is an excellent deep learning framework (nice & user friendly syntax, appropriate documentation and etc.). However, unfortunately PyTorch has some major & important problems with Multi-GPU utilization under training (especially on extremely Large-scale data sets such as Full ImageNet). We have used PyTorch for training some standard CNN models (e.g., DenseNet and ResNet)  on such data set on a machines with Multi-GPU (e.g., a machine with 8 K80 GPU, and a machine with 7 1080Ti GPU), but there are serious problems with it as below:\r\n1- The Main GPU (e.g., the GPU Id = 0 in most cases) becomes a bottleneck and when we increase the number of GPUs this problem amplified. It means by increasing the number of GPUs  the memory usage of the main GPU increases.\r\n2- Especially for training on Large-Scale Data Sets, the GPU utilization (and RAM & CPU utilization) is not appropriate. I mean, we saw that the behavior of GPU utilization is swinging and also it rarely used 100% of power of the GPUs and in most of the time GPU utilization percentage is about 30-40% for example. maybe GPUs are waiting for the Data Loader for loading data. This important issue caused that for example the required time for one epoch is about **20 hours** differs from other frameworks such as MXNet. \r\nIt is worth noting that we have used standard training code such as (https://github.com/pytorch/examples/tree/master/imagenet) and also I think It is better that the PyTorch team arrange a program for tackling with Multi-GPU utilization problems and handling appropriately large-scale data sets (for example by adding classes to convert such data sets' samples into an appropriate format (like TF-Record in TensorFlow)). \r\nI hope these problems and issues can be solved soon and we will enjoy to use PyTorch as the best deep learning framework for our research works. \r\nThank you very much "}