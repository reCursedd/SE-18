{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212690209", "pull_request_review_id": 149380463, "id": 212690209, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjY5MDIwOQ==", "diff_hunk": "@@ -0,0 +1,75 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/TensorUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include <cstring>\n+#include <memory>\n+#include <sstream>\n+#include <vector>\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+\n+namespace at { \n+namespace native {\n+\n+// Staying faithful to the Python for now for clarity, look for optimizations later\n+// (eg single return statement for RVO)\n+Tensor norm_except_dim(const Tensor & v, int64_t pow, int64_t dim)\n+{\n+  // I assume tensor.contiguous(), view(), norm(), etc. here will dispatch through VariableType.\n+  if(dim == -1)\n+    return v.norm(pow);\n+  else if(dim == 0)\n+  {\n+    std::vector<int64_t> output_size(v.dim(), 1);\n+    output_size[0] = v.size(0);\n+    return v.contiguous().view({v.size(0), -1}).norm(pow, 1).view(output_size);", "path": "aten/src/ATen/native/WeightNorm.cpp", "position": null, "original_position": 29, "commit_id": "a8e98ce8c02e0469fb907621eaa61b0a9486d68e", "original_commit_id": "59f140757461de2d65e7263bbc9f3961aecfb8e5", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "body": "Thank you for the quick review!  I had a couple reasons for choosing this pattern:\r\n1.  I'm mimicking the original Python:  https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py#L13\r\n2.  norm_except_dim is accessible from the Python, and my implement [uses it](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py#L13) on the Python side as well as in _weight_norm on the C++ side.  IMO it's also a useful independent Python function to have.  For these two reasons, in general it's possible the incoming v may be noncontiguous, have pathologically permuted indices, etc.  I need the tensor's raw data to be C-style contiguous, so that it can be viewed with all but one dimension flattened and so that the norm will be performant.  I'm not sure if reshape does all these things, so I decided to stick with what I knew would work.\r\n\r\nIf 2. is not actually a problem with reshape, I can use reshape instead.", "created_at": "2018-08-24T16:55:23Z", "updated_at": "2018-11-23T15:49:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/10842#discussion_r212690209", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10842", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212690209"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10842#discussion_r212690209"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10842"}}, "body_html": "<p>Thank you for the quick review!  I had a couple reasons for choosing this pattern:</p>\n<ol>\n<li>I'm mimicking the original Python:  <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py#L13\">https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py#L13</a></li>\n<li>norm_except_dim is accessible from the Python, and my implement <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py#L13\">uses it</a> on the Python side as well as in _weight_norm on the C++ side.  IMO it's also a useful independent Python function to have.  For these two reasons, in general it's possible the incoming v may be noncontiguous, have pathologically permuted indices, etc.  I need the tensor's raw data to be C-style contiguous, so that it can be viewed with all but one dimension flattened and so that the norm will be performant.  I'm not sure if reshape does all these things, so I decided to stick with what I knew would work.</li>\n</ol>\n<p>If 2. is not actually a problem with reshape, I can use reshape instead.</p>", "body_text": "Thank you for the quick review!  I had a couple reasons for choosing this pattern:\n\nI'm mimicking the original Python:  https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py#L13\nnorm_except_dim is accessible from the Python, and my implement uses it on the Python side as well as in _weight_norm on the C++ side.  IMO it's also a useful independent Python function to have.  For these two reasons, in general it's possible the incoming v may be noncontiguous, have pathologically permuted indices, etc.  I need the tensor's raw data to be C-style contiguous, so that it can be viewed with all but one dimension flattened and so that the norm will be performant.  I'm not sure if reshape does all these things, so I decided to stick with what I knew would work.\n\nIf 2. is not actually a problem with reshape, I can use reshape instead.", "in_reply_to_id": 212545769}