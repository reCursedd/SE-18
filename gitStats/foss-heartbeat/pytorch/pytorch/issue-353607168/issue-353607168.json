{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10842", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10842/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10842/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10842/events", "html_url": "https://github.com/pytorch/pytorch/pull/10842", "id": 353607168, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEwNjE0Mzgw", "number": 10842, "title": "Fused weightnorm for ATen", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-08-24T01:06:51Z", "updated_at": "2018-11-23T15:50:53Z", "closed_at": "2018-09-12T20:56:54Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10842", "html_url": "https://github.com/pytorch/pytorch/pull/10842", "diff_url": "https://github.com/pytorch/pytorch/pull/10842.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10842.patch"}, "body_html": "<p>This PR contains a C++ implementation of weight norm.  The user-side exposure of weight norm through torch.nn.utils.weight_norm is unchanged.</p>\n<p>If running on the GPU, and the norm is requested over the first or last dimension of the weight tensor, the forward pass is carried out using the fused kernels I wrote for our Fairseq GTC hero run, which offer superior performance to primitive ops and superior numerical stability when running in FP16.  In the common case that the backward pass is not itself constructing a graph (ie not attempting to set up double backward) the backward pass will be carried out using another fused kernel.  If the backward pass is constructing a graph, an alternate code path is taken, which does the math using differentiable primitive ops. In this way, the implementation allows double backward, even if the fused kernel was used in forward (although in this case, you don't benefit from the performance and stability of the fused backward kernel).</p>\n<p>If running on the CPU, or if norming over an interior dim, the forward pass is carried out using double-differentiable primitive ops.</p>\n<p>Figuring out how to generate all the right plumbing for this was tricky, but it was a fun experience learning how the autogenerator works and how the graph is constructed.  Thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> for useful guidance on this front.</p>\n<p>I do have a few lingering questions:</p>\n<ul>\n<li>Should I unify my return statements (ie by default-constructing Tensors outside if blocks and using operator= within)?</li>\n<li>What is the significance of <code>non_blocking</code> when calling e.g. <code>auto norms = saved_norms.to(saved_g.type().scalarType(), non_blocking=True/False);</code>?  I am currently omitting <code>non_blocking</code>, so it defaults to False, but I didn't see any associated synchronizes on the timeline, so I'm wondering what it means.</li>\n<li>Is there an \"official\" mapping from at::ScalarTypes to corresponding accumulate types, as there are for the PODs + Half in <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/AccumulateType.h\">AccumulateType.h</a>?  I looked for an equivalent mapping for ScalarTypes, didn't find one, and ended up rigging it myself (<code>  at::ScalarType AccType = g.type().scalarType() == at::ScalarType::Half ? at::ScalarType::Float : g.type().scalarType();</code>).</li>\n<li>Are sparse tensors a concern?  Should I include another check for sparse tensors in the <code>_weight_norm</code> entry point, and send those along the fallback CPU path as well?</li>\n</ul>", "body_text": "This PR contains a C++ implementation of weight norm.  The user-side exposure of weight norm through torch.nn.utils.weight_norm is unchanged.\nIf running on the GPU, and the norm is requested over the first or last dimension of the weight tensor, the forward pass is carried out using the fused kernels I wrote for our Fairseq GTC hero run, which offer superior performance to primitive ops and superior numerical stability when running in FP16.  In the common case that the backward pass is not itself constructing a graph (ie not attempting to set up double backward) the backward pass will be carried out using another fused kernel.  If the backward pass is constructing a graph, an alternate code path is taken, which does the math using differentiable primitive ops. In this way, the implementation allows double backward, even if the fused kernel was used in forward (although in this case, you don't benefit from the performance and stability of the fused backward kernel).\nIf running on the CPU, or if norming over an interior dim, the forward pass is carried out using double-differentiable primitive ops.\nFiguring out how to generate all the right plumbing for this was tricky, but it was a fun experience learning how the autogenerator works and how the graph is constructed.  Thanks to @colesbury for useful guidance on this front.\nI do have a few lingering questions:\n\nShould I unify my return statements (ie by default-constructing Tensors outside if blocks and using operator= within)?\nWhat is the significance of non_blocking when calling e.g. auto norms = saved_norms.to(saved_g.type().scalarType(), non_blocking=True/False);?  I am currently omitting non_blocking, so it defaults to False, but I didn't see any associated synchronizes on the timeline, so I'm wondering what it means.\nIs there an \"official\" mapping from at::ScalarTypes to corresponding accumulate types, as there are for the PODs + Half in AccumulateType.h?  I looked for an equivalent mapping for ScalarTypes, didn't find one, and ended up rigging it myself (  at::ScalarType AccType = g.type().scalarType() == at::ScalarType::Half ? at::ScalarType::Float : g.type().scalarType();).\nAre sparse tensors a concern?  Should I include another check for sparse tensors in the _weight_norm entry point, and send those along the fallback CPU path as well?", "body": "This PR contains a C++ implementation of weight norm.  The user-side exposure of weight norm through torch.nn.utils.weight_norm is unchanged.\r\n\r\nIf running on the GPU, and the norm is requested over the first or last dimension of the weight tensor, the forward pass is carried out using the fused kernels I wrote for our Fairseq GTC hero run, which offer superior performance to primitive ops and superior numerical stability when running in FP16.  In the common case that the backward pass is not itself constructing a graph (ie not attempting to set up double backward) the backward pass will be carried out using another fused kernel.  If the backward pass is constructing a graph, an alternate code path is taken, which does the math using differentiable primitive ops. In this way, the implementation allows double backward, even if the fused kernel was used in forward (although in this case, you don't benefit from the performance and stability of the fused backward kernel).\r\n\r\nIf running on the CPU, or if norming over an interior dim, the forward pass is carried out using double-differentiable primitive ops.\r\n\r\nFiguring out how to generate all the right plumbing for this was tricky, but it was a fun experience learning how the autogenerator works and how the graph is constructed.  Thanks to @colesbury for useful guidance on this front.\r\n\r\nI do have a few lingering questions:\r\n\r\n- Should I unify my return statements (ie by default-constructing Tensors outside if blocks and using operator= within)?\r\n- What is the significance of `non_blocking` when calling e.g. `auto norms = saved_norms.to(saved_g.type().scalarType(), non_blocking=True/False);`?  I am currently omitting `non_blocking`, so it defaults to False, but I didn't see any associated synchronizes on the timeline, so I'm wondering what it means.\r\n- Is there an \"official\" mapping from at::ScalarTypes to corresponding accumulate types, as there are for the PODs + Half in [AccumulateType.h](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/AccumulateType.h)?  I looked for an equivalent mapping for ScalarTypes, didn't find one, and ended up rigging it myself (`  at::ScalarType AccType = g.type().scalarType() == at::ScalarType::Half ? at::ScalarType::Float : g.type().scalarType();`).\r\n- Are sparse tensors a concern?  Should I include another check for sparse tensors in the `_weight_norm` entry point, and send those along the fallback CPU path as well?\r\n\r\n"}