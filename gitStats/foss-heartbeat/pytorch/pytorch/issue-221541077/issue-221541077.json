{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1253", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1253/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1253/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1253/events", "html_url": "https://github.com/pytorch/pytorch/issues/1253", "id": 221541077, "node_id": "MDU6SXNzdWUyMjE1NDEwNzc=", "number": 1253, "title": "Memory usage increasing after first batch (possibly not freeing some potentially free memory)", "user": {"login": "MatthiasKohl", "id": 344856, "node_id": "MDQ6VXNlcjM0NDg1Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/344856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MatthiasKohl", "html_url": "https://github.com/MatthiasKohl", "followers_url": "https://api.github.com/users/MatthiasKohl/followers", "following_url": "https://api.github.com/users/MatthiasKohl/following{/other_user}", "gists_url": "https://api.github.com/users/MatthiasKohl/gists{/gist_id}", "starred_url": "https://api.github.com/users/MatthiasKohl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MatthiasKohl/subscriptions", "organizations_url": "https://api.github.com/users/MatthiasKohl/orgs", "repos_url": "https://api.github.com/users/MatthiasKohl/repos", "events_url": "https://api.github.com/users/MatthiasKohl/events{/privacy}", "received_events_url": "https://api.github.com/users/MatthiasKohl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-04-13T11:55:26Z", "updated_at": "2017-04-18T14:10:07Z", "closed_at": "2017-04-18T14:10:07Z", "author_association": "NONE", "body_html": "<p>This may be related to issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"219365203\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1184\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1184/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1184\">#1184</a> but I believe there is something else to it:<br>\nI am working on version <code>v0.1.11</code>, using micro batches: split mini-batches into smaller batches and calculate/backprop loss in each micro batch, accumulating it over the mini batch. I noticed multiple times that my code goes out of memory after several micro batches. This seems strange to me as each micro batch should require the same amount of memory. Maybe there is something I don't fully understand here, so if anyone can explain it to me, I'd be happy !</p>\n<p>Anyway, I tried to look into it a bit and attached is the smallest code snippet I came up with demonstrating my problem (around 100 lines).<br>\nEssentially, I am training a network on images, using micro-batches to reduce memory. However, after the first (and sometimes second) micro-batch, the memory usage increases and then stays the same after further micro-batches.</p>\n<p>When using a functional approach for the mini and micro batches, the memory usage does not increase the same way, so I don't believe that this has to do with the fact that there may be 'lazy' memory freeing involved (freeing only when necessary).</p>\n<p><a href=\"https://github.com/pytorch/pytorch/files/919384/torch_memory.zip\">torch_memory.zip</a></p>", "body_text": "This may be related to issue #1184 but I believe there is something else to it:\nI am working on version v0.1.11, using micro batches: split mini-batches into smaller batches and calculate/backprop loss in each micro batch, accumulating it over the mini batch. I noticed multiple times that my code goes out of memory after several micro batches. This seems strange to me as each micro batch should require the same amount of memory. Maybe there is something I don't fully understand here, so if anyone can explain it to me, I'd be happy !\nAnyway, I tried to look into it a bit and attached is the smallest code snippet I came up with demonstrating my problem (around 100 lines).\nEssentially, I am training a network on images, using micro-batches to reduce memory. However, after the first (and sometimes second) micro-batch, the memory usage increases and then stays the same after further micro-batches.\nWhen using a functional approach for the mini and micro batches, the memory usage does not increase the same way, so I don't believe that this has to do with the fact that there may be 'lazy' memory freeing involved (freeing only when necessary).\ntorch_memory.zip", "body": "This may be related to issue #1184 but I believe there is something else to it:\r\nI am working on version `v0.1.11`, using micro batches: split mini-batches into smaller batches and calculate/backprop loss in each micro batch, accumulating it over the mini batch. I noticed multiple times that my code goes out of memory after several micro batches. This seems strange to me as each micro batch should require the same amount of memory. Maybe there is something I don't fully understand here, so if anyone can explain it to me, I'd be happy !\r\n\r\nAnyway, I tried to look into it a bit and attached is the smallest code snippet I came up with demonstrating my problem (around 100 lines).\r\nEssentially, I am training a network on images, using micro-batches to reduce memory. However, after the first (and sometimes second) micro-batch, the memory usage increases and then stays the same after further micro-batches.\r\n\r\nWhen using a functional approach for the mini and micro batches, the memory usage does not increase the same way, so I don't believe that this has to do with the fact that there may be 'lazy' memory freeing involved (freeing only when necessary).\r\n\r\n[torch_memory.zip](https://github.com/pytorch/pytorch/files/919384/torch_memory.zip)"}