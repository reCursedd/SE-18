{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/294154698", "html_url": "https://github.com/pytorch/pytorch/issues/1253#issuecomment-294154698", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1253", "id": 294154698, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDE1NDY5OA==", "user": {"login": "MatthiasKohl", "id": 344856, "node_id": "MDQ6VXNlcjM0NDg1Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/344856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MatthiasKohl", "html_url": "https://github.com/MatthiasKohl", "followers_url": "https://api.github.com/users/MatthiasKohl/followers", "following_url": "https://api.github.com/users/MatthiasKohl/following{/other_user}", "gists_url": "https://api.github.com/users/MatthiasKohl/gists{/gist_id}", "starred_url": "https://api.github.com/users/MatthiasKohl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MatthiasKohl/subscriptions", "organizations_url": "https://api.github.com/users/MatthiasKohl/orgs", "repos_url": "https://api.github.com/users/MatthiasKohl/repos", "events_url": "https://api.github.com/users/MatthiasKohl/events{/privacy}", "received_events_url": "https://api.github.com/users/MatthiasKohl/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-14T13:37:04Z", "updated_at": "2017-04-14T13:37:04Z", "author_association": "NONE", "body_html": "<p>I've looked into this a bit further and first found that simply running my (larger) code in an iPython kernel rather than directly using Python, the memory usage is quite different. So I assume that the garbage collection (or tracking memory allocations) are different in iPython vs Python. Unfortunately, I cannot easily demonstrate this because the smaller code I posted does not behave differently in Python vs iPython.</p>\n<p>I then simply used <code>gc.collect()</code> at the end of each micro-batch and my memory consumption dropped significantly (at least 4x) in a way that I don't go out of memory anymore. This is surprising to me since I assumed that any non-referenced Variable/Tensor should be overwritten sooner or later, so <code>gc.collect()</code> shouldn't have any impact. Anyway, if I find a more concise example of this behaviour, I will update you.</p>", "body_text": "I've looked into this a bit further and first found that simply running my (larger) code in an iPython kernel rather than directly using Python, the memory usage is quite different. So I assume that the garbage collection (or tracking memory allocations) are different in iPython vs Python. Unfortunately, I cannot easily demonstrate this because the smaller code I posted does not behave differently in Python vs iPython.\nI then simply used gc.collect() at the end of each micro-batch and my memory consumption dropped significantly (at least 4x) in a way that I don't go out of memory anymore. This is surprising to me since I assumed that any non-referenced Variable/Tensor should be overwritten sooner or later, so gc.collect() shouldn't have any impact. Anyway, if I find a more concise example of this behaviour, I will update you.", "body": "I've looked into this a bit further and first found that simply running my (larger) code in an iPython kernel rather than directly using Python, the memory usage is quite different. So I assume that the garbage collection (or tracking memory allocations) are different in iPython vs Python. Unfortunately, I cannot easily demonstrate this because the smaller code I posted does not behave differently in Python vs iPython.\r\n\r\nI then simply used `gc.collect()` at the end of each micro-batch and my memory consumption dropped significantly (at least 4x) in a way that I don't go out of memory anymore. This is surprising to me since I assumed that any non-referenced Variable/Tensor should be overwritten sooner or later, so `gc.collect()` shouldn't have any impact. Anyway, if I find a more concise example of this behaviour, I will update you."}