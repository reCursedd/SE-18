{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5569", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5569/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5569/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5569/events", "html_url": "https://github.com/pytorch/pytorch/issues/5569", "id": 302372287, "node_id": "MDU6SXNzdWUzMDIzNzIyODc=", "number": 5569, "title": "torch.autograd.Function memory leak", "user": {"login": "btgraham", "id": 11717476, "node_id": "MDQ6VXNlcjExNzE3NDc2", "avatar_url": "https://avatars3.githubusercontent.com/u/11717476?v=4", "gravatar_id": "", "url": "https://api.github.com/users/btgraham", "html_url": "https://github.com/btgraham", "followers_url": "https://api.github.com/users/btgraham/followers", "following_url": "https://api.github.com/users/btgraham/following{/other_user}", "gists_url": "https://api.github.com/users/btgraham/gists{/gist_id}", "starred_url": "https://api.github.com/users/btgraham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/btgraham/subscriptions", "organizations_url": "https://api.github.com/users/btgraham/orgs", "repos_url": "https://api.github.com/users/btgraham/repos", "events_url": "https://api.github.com/users/btgraham/events{/privacy}", "received_events_url": "https://api.github.com/users/btgraham/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-03-05T16:39:19Z", "updated_at": "2018-03-05T20:45:48Z", "closed_at": "2018-03-05T17:30:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I get a memory leak running the code below.<br>\nThe code is a contrived, for example the weight parameter seemingly does nothing, and I know I could use ctx.save_for_backward to save the input_features. Nonetheless, it may be unexpected behavior.</p>\n<p>[PyTorch master <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/66547ca06171812a65afbd82473372aa739a8b2a/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/66547ca06171812a65afbd82473372aa739a8b2a\"><tt>66547ca</tt></a> / Linux / Python Miniconda 3.6.1 ]</p>\n<pre><code>import torch\n\nclass CopyFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_features, weight):\n        ctx.output_features=input_features.clone()\n        return ctx.output_features\n    @staticmethod\n    def backward(ctx, grad_output):\n        print('backward is never called; that is OK.')\n\nfor _ in range(1000000):\n    weight = torch.nn.Parameter(torch.Tensor(100,100).normal_())\n    input=torch.FloatTensor(100,100).uniform_().cuda()\n    output = CopyFunction.apply(input,weight)\n    print(output.shape, output.type())\n</code></pre>\n<p>P.S. Thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> for helping to localize this.</p>", "body_text": "I get a memory leak running the code below.\nThe code is a contrived, for example the weight parameter seemingly does nothing, and I know I could use ctx.save_for_backward to save the input_features. Nonetheless, it may be unexpected behavior.\n[PyTorch master 66547ca / Linux / Python Miniconda 3.6.1 ]\nimport torch\n\nclass CopyFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_features, weight):\n        ctx.output_features=input_features.clone()\n        return ctx.output_features\n    @staticmethod\n    def backward(ctx, grad_output):\n        print('backward is never called; that is OK.')\n\nfor _ in range(1000000):\n    weight = torch.nn.Parameter(torch.Tensor(100,100).normal_())\n    input=torch.FloatTensor(100,100).uniform_().cuda()\n    output = CopyFunction.apply(input,weight)\n    print(output.shape, output.type())\n\nP.S. Thanks to @fmassa for helping to localize this.", "body": "I get a memory leak running the code below.\r\nThe code is a contrived, for example the weight parameter seemingly does nothing, and I know I could use ctx.save_for_backward to save the input_features. Nonetheless, it may be unexpected behavior.\r\n\r\n[PyTorch master 66547ca06171812a65afbd82473372aa739a8b2a / Linux / Python Miniconda 3.6.1 ]\r\n```\r\nimport torch\r\n\r\nclass CopyFunction(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, input_features, weight):\r\n        ctx.output_features=input_features.clone()\r\n        return ctx.output_features\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        print('backward is never called; that is OK.')\r\n\r\nfor _ in range(1000000):\r\n    weight = torch.nn.Parameter(torch.Tensor(100,100).normal_())\r\n    input=torch.FloatTensor(100,100).uniform_().cuda()\r\n    output = CopyFunction.apply(input,weight)\r\n    print(output.shape, output.type())\r\n```\r\n\r\nP.S. Thanks to @fmassa for helping to localize this.\r\n\r\n\r\n"}