{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6987", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6987/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6987/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6987/events", "html_url": "https://github.com/pytorch/pytorch/issues/6987", "id": 317998123, "node_id": "MDU6SXNzdWUzMTc5OTgxMjM=", "number": 6987, "title": "Segmentation fault with cpp_extensions example", "user": {"login": "tom-roddick", "id": 4833079, "node_id": "MDQ6VXNlcjQ4MzMwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/4833079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tom-roddick", "html_url": "https://github.com/tom-roddick", "followers_url": "https://api.github.com/users/tom-roddick/followers", "following_url": "https://api.github.com/users/tom-roddick/following{/other_user}", "gists_url": "https://api.github.com/users/tom-roddick/gists{/gist_id}", "starred_url": "https://api.github.com/users/tom-roddick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tom-roddick/subscriptions", "organizations_url": "https://api.github.com/users/tom-roddick/orgs", "repos_url": "https://api.github.com/users/tom-roddick/repos", "events_url": "https://api.github.com/users/tom-roddick/events{/privacy}", "received_events_url": "https://api.github.com/users/tom-roddick/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 884152160, "node_id": "MDU6TGFiZWw4ODQxNTIxNjA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cpp-extensions", "name": "cpp-extensions", "color": "1c689e", "default": false}, {"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/crash", "name": "crash", "color": "d93f0b", "default": false}, {"id": 897288569, "node_id": "MDU6TGFiZWw4OTcyODg1Njk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/pytorch", "name": "pytorch", "color": "f05732", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-26T12:01:50Z", "updated_at": "2018-04-27T07:19:18Z", "closed_at": "2018-04-27T07:19:18Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Running the C++ extensions example from the Pytorch 0.4.0 release notes results in a segmentation fault.</p>\n<h2>Code example</h2>\n<h3>C++ code</h3>\n<p>(Code was copied directly from the release notes)</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> my_implementation.cpp</span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/torch.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>unordered_set<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> can use templates as well. But let's keep it</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> simple</span>\n<span class=\"pl-k\">using</span> <span class=\"pl-c1\">scalar_t</span> = <span class=\"pl-k\">float</span>;\n\nat::Tensor <span class=\"pl-en\">unique_float</span>(at::Tensor input_) {\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> only works for floats</span>\n  <span class=\"pl-c1\">AT_ASSERT</span>(input_.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">scalarType</span>() == at::ScalarType::Float, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input must be a float tensor<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> and CPU tensors</span>\n  <span class=\"pl-c1\">AT_ASSERT</span>(!input_.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">is_cuda</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input must be a CPU tensor<span class=\"pl-pds\">\"</span></span>);\n  \n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> make the input contiguous, to simplify the implementation</span>\n  at::Tensor input = input_.<span class=\"pl-c1\">contiguous</span>();\n  \n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> get the pointer that holds the data</span>\n  <span class=\"pl-c1\">scalar_t</span>* input_data = input.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;();\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> let's use a function from the std library to implement</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> the unique function</span>\n  std::unordered_set&lt;<span class=\"pl-c1\">scalar_t</span>&gt; <span class=\"pl-c1\">set</span>(input_data, input_data + input.<span class=\"pl-c1\">numel</span>());\n  \n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> create the output tensor, with size set.size()</span>\n  at::Tensor output = input.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">tensor</span>({<span class=\"pl-k\">static_cast</span>&lt;<span class=\"pl-c1\">int64_t</span>&gt;(set.<span class=\"pl-c1\">size</span>())});\n  <span class=\"pl-c1\">scalar_t</span>* output_data = output.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;();\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> copy the content of the set to the output tensor</span>\n  <span class=\"pl-c1\">std::copy</span>(set.<span class=\"pl-c1\">begin</span>(), set.<span class=\"pl-c1\">end</span>(), output_data);\n  \n  <span class=\"pl-k\">return</span> output;\n}\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> this defines the functions exposed to Python</span>\n<span class=\"pl-en\">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {\n  m.<span class=\"pl-c1\">def</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>unique_float<span class=\"pl-pds\">\"</span></span>, &amp;unique_float, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Unique for float tensors<span class=\"pl-pds\">\"</span></span>);\n}</pre></div>\n<h3>Python code</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test_ext.py</span>\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.utils.cpp_extension <span class=\"pl-k\">import</span> load <span class=\"pl-k\">as</span> load_ext\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pass the source files, they will be compiled on the fly </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> and will return a python module</span>\n_C <span class=\"pl-k\">=</span> load_ext(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>my_unique_lib<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">sources</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>my_implementation.cpp<span class=\"pl-pds\">'</span></span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> now can use the functions implemented in C++</span>\nunique <span class=\"pl-k\">=</span> _C.unique_float\n\na <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>, <span class=\"pl-c1\">1.0</span>])\n<span class=\"pl-c1\">print</span>(unique(a))</pre></div>\n<h3>Stack trace from GDB</h3>\n<pre><code>Program received signal SIGSEGV, Segmentation fault.\n#0  0x00007fffabdea0ae in void __gnu_cxx::new_allocator&lt;_object*&gt;::construct&lt;_object*, _object*&gt;(_object**, _object*&amp;&amp;) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#1  0x00007fffabde69aa in std::enable_if&lt;std::allocator_traits&lt;std::allocator&lt;_object*&gt; &gt;::__construct_helper&lt;_object*, _object*&gt;::value, void&gt;::type std::allocator_traits&lt;std::allocator&lt;_object*&gt; &gt;::_S_construct&lt;_object*, _object*&gt;(std::allocator&lt;_object*&gt;&amp;, _object**, _object*&amp;&amp;) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#2  0x00007fffabde21c6 in decltype (_S_construct({parm#1}, {parm#2}, (forward&lt;_object*&gt;)({parm#3}))) std::allocator_traits&lt;std::allocator&lt;_object*&gt; &gt;::construct&lt;_object*, _object*&gt;(std::allocator&lt;_object*&gt;&amp;, _object**, _object*&amp;&amp;) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#3  0x00007fffabdde355 in void std::vector&lt;_object*, std::allocator&lt;_object*&gt; &gt;::emplace_back&lt;_object*&gt;(_object*&amp;&amp;) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#4  0x00007fffabddaa8a in std::vector&lt;_object*, std::allocator&lt;_object*&gt; &gt;::push_back(_object*&amp;&amp;) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#5  0x00007fffabdd155d in pybind11::detail::loader_life_support::loader_life_support() () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#6  0x00007fffabdd6c99 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#7  0x00007ffff7997902 in _PyCFunction_FastCallDict (func_obj=0x7fffac35d360, args=0x7ffff7f75ba8, nargs=&lt;optimised out&gt;, kwargs=0x0) at Objects/methodobject.c:231\n#8  0x00007ffff7a1cf4c in call_function (pp_stack=0x7fffffffdaa8, oparg=&lt;optimised out&gt;, kwnames=0x0) at Python/ceval.c:4788\n#9  0x00007ffff7a1fbbd in _PyEval_EvalFrameDefault (f=&lt;optimised out&gt;, throwflag=&lt;optimised out&gt;) at Python/ceval.c:3275\n#10 0x00007ffff7a1b4c0 in _PyEval_EvalCodeWithName (_co=0x7ffff7f018a0, globals=&lt;optimised out&gt;, locals=&lt;optimised out&gt;, args=&lt;optimised out&gt;, argcount=0, kwnames=0x0, kwargs=0x8,\n    kwcount=0, kwstep=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at Python/ceval.c:4119\n#11 0x00007ffff7a1b943 in PyEval_EvalCodeEx (_co=&lt;optimised out&gt;, globals=&lt;optimised out&gt;, locals=&lt;optimised out&gt;, args=&lt;optimised out&gt;, argcount=&lt;optimised out&gt;, kws=&lt;optimised out&gt;,\n    kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4140\n#12 0x00007ffff7a1b98b in PyEval_EvalCode (co=&lt;optimised out&gt;, globals=&lt;optimised out&gt;, locals=&lt;optimised out&gt;) at Python/ceval.c:695\n#13 0x00007ffff7a4e100 in run_mod (arena=0x7ffff7f5f2d0, flags=0x7fffffffde00, locals=0x7ffff7f46090, globals=0x7ffff7f46090, filename=0x7ffff66c1bb0, mod=0x6a70a0)\n---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---\n    at Python/pythonrun.c:980\n#14 PyRun_FileExFlags (fp=0x6926c0, filename_str=&lt;optimised out&gt;, start=&lt;optimised out&gt;, globals=0x7ffff7f46090, locals=0x7ffff7f46090, closeit=&lt;optimised out&gt;, flags=0x7fffffffde00)\n    at Python/pythonrun.c:933\n#15 0x00007ffff7a4f6f3 in PyRun_SimpleFileExFlags (fp=0x6926c0, filename=&lt;optimised out&gt;, closeit=1, flags=0x7fffffffde00) at Python/pythonrun.c:396\n#16 0x00007ffff7a6aa41 in run_file (p_cf=0x7fffffffde00, filename=0x6032d0 L\"test_ext.py\", fp=0x6926c0) at Modules/main.c:320\n#17 Py_Main (argc=&lt;optimised out&gt;, argv=&lt;optimised out&gt;) at Modules/main.c:781\n#18 0x0000000000400c1d in main (argc=2, argv=&lt;optimised out&gt;) at ./Programs/python.c:69\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 14.04.5 LTS<br>\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4<br>\nCMake version: version 3.2.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.44<br>\nGPU models and configuration: GPU 0: GeForce GTX 980<br>\nNvidia driver version: 384.111<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5<br>\n/usr/local/cuda-8.0/lib64/libcudnn_static.a<br>\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.2)<br>\n[pip3] numpydoc (0.6.0)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.0, ~/packages/vision)<br>\n[conda] cuda80                    1.0                           0    soumith<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torchvision               0.2.0                     </p>", "body_text": "Issue description\nRunning the C++ extensions example from the Pytorch 0.4.0 release notes results in a segmentation fault.\nCode example\nC++ code\n(Code was copied directly from the release notes)\n// my_implementation.cpp\n#include <torch/torch.h>\n#include <unordered_set>\n\n// can use templates as well. But let's keep it\n// simple\nusing scalar_t = float;\n\nat::Tensor unique_float(at::Tensor input_) {\n  // only works for floats\n  AT_ASSERT(input_.type().scalarType() == at::ScalarType::Float, \"input must be a float tensor\");\n  // and CPU tensors\n  AT_ASSERT(!input_.type().is_cuda(), \"input must be a CPU tensor\");\n  \n  // make the input contiguous, to simplify the implementation\n  at::Tensor input = input_.contiguous();\n  \n  // get the pointer that holds the data\n  scalar_t* input_data = input.data<scalar_t>();\n  // let's use a function from the std library to implement\n  // the unique function\n  std::unordered_set<scalar_t> set(input_data, input_data + input.numel());\n  \n  // create the output tensor, with size set.size()\n  at::Tensor output = input.type().tensor({static_cast<int64_t>(set.size())});\n  scalar_t* output_data = output.data<scalar_t>();\n  // copy the content of the set to the output tensor\n  std::copy(set.begin(), set.end(), output_data);\n  \n  return output;\n}\n\n// this defines the functions exposed to Python\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"unique_float\", &unique_float, \"Unique for float tensors\");\n}\nPython code\n# test_ext.py\nimport torch\nfrom torch.utils.cpp_extension import load as load_ext\n# pass the source files, they will be compiled on the fly \n# and will return a python module\n_C = load_ext('my_unique_lib', sources=['my_implementation.cpp'])\n\n# now can use the functions implemented in C++\nunique = _C.unique_float\n\na = torch.tensor([1.0, 2.0, 1.0])\nprint(unique(a))\nStack trace from GDB\nProgram received signal SIGSEGV, Segmentation fault.\n#0  0x00007fffabdea0ae in void __gnu_cxx::new_allocator<_object*>::construct<_object*, _object*>(_object**, _object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#1  0x00007fffabde69aa in std::enable_if<std::allocator_traits<std::allocator<_object*> >::__construct_helper<_object*, _object*>::value, void>::type std::allocator_traits<std::allocator<_object*> >::_S_construct<_object*, _object*>(std::allocator<_object*>&, _object**, _object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#2  0x00007fffabde21c6 in decltype (_S_construct({parm#1}, {parm#2}, (forward<_object*>)({parm#3}))) std::allocator_traits<std::allocator<_object*> >::construct<_object*, _object*>(std::allocator<_object*>&, _object**, _object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#3  0x00007fffabdde355 in void std::vector<_object*, std::allocator<_object*> >::emplace_back<_object*>(_object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#4  0x00007fffabddaa8a in std::vector<_object*, std::allocator<_object*> >::push_back(_object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#5  0x00007fffabdd155d in pybind11::detail::loader_life_support::loader_life_support() () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#6  0x00007fffabdd6c99 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\n#7  0x00007ffff7997902 in _PyCFunction_FastCallDict (func_obj=0x7fffac35d360, args=0x7ffff7f75ba8, nargs=<optimised out>, kwargs=0x0) at Objects/methodobject.c:231\n#8  0x00007ffff7a1cf4c in call_function (pp_stack=0x7fffffffdaa8, oparg=<optimised out>, kwnames=0x0) at Python/ceval.c:4788\n#9  0x00007ffff7a1fbbd in _PyEval_EvalFrameDefault (f=<optimised out>, throwflag=<optimised out>) at Python/ceval.c:3275\n#10 0x00007ffff7a1b4c0 in _PyEval_EvalCodeWithName (_co=0x7ffff7f018a0, globals=<optimised out>, locals=<optimised out>, args=<optimised out>, argcount=0, kwnames=0x0, kwargs=0x8,\n    kwcount=0, kwstep=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at Python/ceval.c:4119\n#11 0x00007ffff7a1b943 in PyEval_EvalCodeEx (_co=<optimised out>, globals=<optimised out>, locals=<optimised out>, args=<optimised out>, argcount=<optimised out>, kws=<optimised out>,\n    kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4140\n#12 0x00007ffff7a1b98b in PyEval_EvalCode (co=<optimised out>, globals=<optimised out>, locals=<optimised out>) at Python/ceval.c:695\n#13 0x00007ffff7a4e100 in run_mod (arena=0x7ffff7f5f2d0, flags=0x7fffffffde00, locals=0x7ffff7f46090, globals=0x7ffff7f46090, filename=0x7ffff66c1bb0, mod=0x6a70a0)\n---Type <return> to continue, or q <return> to quit---\n    at Python/pythonrun.c:980\n#14 PyRun_FileExFlags (fp=0x6926c0, filename_str=<optimised out>, start=<optimised out>, globals=0x7ffff7f46090, locals=0x7ffff7f46090, closeit=<optimised out>, flags=0x7fffffffde00)\n    at Python/pythonrun.c:933\n#15 0x00007ffff7a4f6f3 in PyRun_SimpleFileExFlags (fp=0x6926c0, filename=<optimised out>, closeit=1, flags=0x7fffffffde00) at Python/pythonrun.c:396\n#16 0x00007ffff7a6aa41 in run_file (p_cf=0x7fffffffde00, filename=0x6032d0 L\"test_ext.py\", fp=0x6926c0) at Modules/main.c:320\n#17 Py_Main (argc=<optimised out>, argv=<optimised out>) at Modules/main.c:781\n#18 0x0000000000400c1d in main (argc=2, argv=<optimised out>) at ./Programs/python.c:69\n\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 14.04.5 LTS\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nCMake version: version 3.2.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.44\nGPU models and configuration: GPU 0: GeForce GTX 980\nNvidia driver version: 384.111\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] numpydoc (0.6.0)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.0, ~/packages/vision)\n[conda] cuda80                    1.0                           0    soumith\n[conda] torch                     0.4.0                     \n[conda] torchvision               0.2.0", "body": "## Issue description\r\nRunning the C++ extensions example from the Pytorch 0.4.0 release notes results in a segmentation fault.\r\n\r\n## Code example\r\n### C++ code\r\n(Code was copied directly from the release notes)\r\n```c++\r\n// my_implementation.cpp\r\n#include <torch/torch.h>\r\n#include <unordered_set>\r\n\r\n// can use templates as well. But let's keep it\r\n// simple\r\nusing scalar_t = float;\r\n\r\nat::Tensor unique_float(at::Tensor input_) {\r\n  // only works for floats\r\n  AT_ASSERT(input_.type().scalarType() == at::ScalarType::Float, \"input must be a float tensor\");\r\n  // and CPU tensors\r\n  AT_ASSERT(!input_.type().is_cuda(), \"input must be a CPU tensor\");\r\n  \r\n  // make the input contiguous, to simplify the implementation\r\n  at::Tensor input = input_.contiguous();\r\n  \r\n  // get the pointer that holds the data\r\n  scalar_t* input_data = input.data<scalar_t>();\r\n  // let's use a function from the std library to implement\r\n  // the unique function\r\n  std::unordered_set<scalar_t> set(input_data, input_data + input.numel());\r\n  \r\n  // create the output tensor, with size set.size()\r\n  at::Tensor output = input.type().tensor({static_cast<int64_t>(set.size())});\r\n  scalar_t* output_data = output.data<scalar_t>();\r\n  // copy the content of the set to the output tensor\r\n  std::copy(set.begin(), set.end(), output_data);\r\n  \r\n  return output;\r\n}\r\n\r\n// this defines the functions exposed to Python\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(\"unique_float\", &unique_float, \"Unique for float tensors\");\r\n}\r\n```\r\n\r\n### Python code\r\n```python\r\n# test_ext.py\r\nimport torch\r\nfrom torch.utils.cpp_extension import load as load_ext\r\n# pass the source files, they will be compiled on the fly \r\n# and will return a python module\r\n_C = load_ext('my_unique_lib', sources=['my_implementation.cpp'])\r\n\r\n# now can use the functions implemented in C++\r\nunique = _C.unique_float\r\n\r\na = torch.tensor([1.0, 2.0, 1.0])\r\nprint(unique(a))\r\n```\r\n\r\n### Stack trace from GDB\r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n#0  0x00007fffabdea0ae in void __gnu_cxx::new_allocator<_object*>::construct<_object*, _object*>(_object**, _object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#1  0x00007fffabde69aa in std::enable_if<std::allocator_traits<std::allocator<_object*> >::__construct_helper<_object*, _object*>::value, void>::type std::allocator_traits<std::allocator<_object*> >::_S_construct<_object*, _object*>(std::allocator<_object*>&, _object**, _object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#2  0x00007fffabde21c6 in decltype (_S_construct({parm#1}, {parm#2}, (forward<_object*>)({parm#3}))) std::allocator_traits<std::allocator<_object*> >::construct<_object*, _object*>(std::allocator<_object*>&, _object**, _object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#3  0x00007fffabdde355 in void std::vector<_object*, std::allocator<_object*> >::emplace_back<_object*>(_object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#4  0x00007fffabddaa8a in std::vector<_object*, std::allocator<_object*> >::push_back(_object*&&) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#5  0x00007fffabdd155d in pybind11::detail::loader_life_support::loader_life_support() () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#6  0x00007fffabdd6c99 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /tmp/torch_extensions/my_unique_lib/my_unique_lib.so\r\n#7  0x00007ffff7997902 in _PyCFunction_FastCallDict (func_obj=0x7fffac35d360, args=0x7ffff7f75ba8, nargs=<optimised out>, kwargs=0x0) at Objects/methodobject.c:231\r\n#8  0x00007ffff7a1cf4c in call_function (pp_stack=0x7fffffffdaa8, oparg=<optimised out>, kwnames=0x0) at Python/ceval.c:4788\r\n#9  0x00007ffff7a1fbbd in _PyEval_EvalFrameDefault (f=<optimised out>, throwflag=<optimised out>) at Python/ceval.c:3275\r\n#10 0x00007ffff7a1b4c0 in _PyEval_EvalCodeWithName (_co=0x7ffff7f018a0, globals=<optimised out>, locals=<optimised out>, args=<optimised out>, argcount=0, kwnames=0x0, kwargs=0x8,\r\n    kwcount=0, kwstep=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at Python/ceval.c:4119\r\n#11 0x00007ffff7a1b943 in PyEval_EvalCodeEx (_co=<optimised out>, globals=<optimised out>, locals=<optimised out>, args=<optimised out>, argcount=<optimised out>, kws=<optimised out>,\r\n    kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4140\r\n#12 0x00007ffff7a1b98b in PyEval_EvalCode (co=<optimised out>, globals=<optimised out>, locals=<optimised out>) at Python/ceval.c:695\r\n#13 0x00007ffff7a4e100 in run_mod (arena=0x7ffff7f5f2d0, flags=0x7fffffffde00, locals=0x7ffff7f46090, globals=0x7ffff7f46090, filename=0x7ffff66c1bb0, mod=0x6a70a0)\r\n---Type <return> to continue, or q <return> to quit---\r\n    at Python/pythonrun.c:980\r\n#14 PyRun_FileExFlags (fp=0x6926c0, filename_str=<optimised out>, start=<optimised out>, globals=0x7ffff7f46090, locals=0x7ffff7f46090, closeit=<optimised out>, flags=0x7fffffffde00)\r\n    at Python/pythonrun.c:933\r\n#15 0x00007ffff7a4f6f3 in PyRun_SimpleFileExFlags (fp=0x6926c0, filename=<optimised out>, closeit=1, flags=0x7fffffffde00) at Python/pythonrun.c:396\r\n#16 0x00007ffff7a6aa41 in run_file (p_cf=0x7fffffffde00, filename=0x6032d0 L\"test_ext.py\", fp=0x6926c0) at Modules/main.c:320\r\n#17 Py_Main (argc=<optimised out>, argv=<optimised out>) at Modules/main.c:781\r\n#18 0x0000000000400c1d in main (argc=2, argv=<optimised out>) at ./Programs/python.c:69\r\n```\r\n\r\n\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration: GPU 0: GeForce GTX 980\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] numpydoc (0.6.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.0, ~/packages/vision)\r\n[conda] cuda80                    1.0                           0    soumith\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.0                     <pip>\r\n"}