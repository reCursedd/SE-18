{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1845", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1845/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1845/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1845/events", "html_url": "https://github.com/pytorch/pytorch/issues/1845", "id": 236892037, "node_id": "MDU6SXNzdWUyMzY4OTIwMzc=", "number": 1845, "title": "the default of torch.max keepdim is False, which is  inconsistent with the document", "user": {"login": "xwgeng", "id": 7008668, "node_id": "MDQ6VXNlcjcwMDg2Njg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7008668?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xwgeng", "html_url": "https://github.com/xwgeng", "followers_url": "https://api.github.com/users/xwgeng/followers", "following_url": "https://api.github.com/users/xwgeng/following{/other_user}", "gists_url": "https://api.github.com/users/xwgeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/xwgeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xwgeng/subscriptions", "organizations_url": "https://api.github.com/users/xwgeng/orgs", "repos_url": "https://api.github.com/users/xwgeng/repos", "events_url": "https://api.github.com/users/xwgeng/events{/privacy}", "received_events_url": "https://api.github.com/users/xwgeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-19T13:32:37Z", "updated_at": "2017-06-19T14:22:32Z", "closed_at": "2017-06-19T14:22:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>According to the doc <a href=\"http://pytorch.org/docs/torch.html#torch.max\" rel=\"nofollow\">torch.max</a>, the default of <code>keepdim</code> is <code>True</code>, but actually setted to <code>False</code> .  Maybe the issue occurs due to <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/reduce.py#L136\">this line</a></p>\n<pre><code>\nIn [2]: x = torch.ones(1,2)\n\nIn [3]: x\nOut[3]: \n\n 1  1\n[torch.FloatTensor of size 1x2]\n\nIn [4]: x.max(1)\nOut[4]: \n(\n  1\n [torch.FloatTensor of size 1], \n  0\n [torch.LongTensor of size 1])\n\nIn [5]: x.min(1)\nOut[5]: \n(\n  1\n [torch.FloatTensor of size 1], \n  0\n [torch.LongTensor of size 1])\n</code></pre>", "body_text": "According to the doc torch.max, the default of keepdim is True, but actually setted to False .  Maybe the issue occurs due to this line\n\nIn [2]: x = torch.ones(1,2)\n\nIn [3]: x\nOut[3]: \n\n 1  1\n[torch.FloatTensor of size 1x2]\n\nIn [4]: x.max(1)\nOut[4]: \n(\n  1\n [torch.FloatTensor of size 1], \n  0\n [torch.LongTensor of size 1])\n\nIn [5]: x.min(1)\nOut[5]: \n(\n  1\n [torch.FloatTensor of size 1], \n  0\n [torch.LongTensor of size 1])", "body": "According to the doc [torch.max](http://pytorch.org/docs/torch.html#torch.max), the default of `keepdim` is `True`, but actually setted to `False` .  Maybe the issue occurs due to [this line](https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/reduce.py#L136)\r\n```\r\n\r\nIn [2]: x = torch.ones(1,2)\r\n\r\nIn [3]: x\r\nOut[3]: \r\n\r\n 1  1\r\n[torch.FloatTensor of size 1x2]\r\n\r\nIn [4]: x.max(1)\r\nOut[4]: \r\n(\r\n  1\r\n [torch.FloatTensor of size 1], \r\n  0\r\n [torch.LongTensor of size 1])\r\n\r\nIn [5]: x.min(1)\r\nOut[5]: \r\n(\r\n  1\r\n [torch.FloatTensor of size 1], \r\n  0\r\n [torch.LongTensor of size 1])\r\n```"}