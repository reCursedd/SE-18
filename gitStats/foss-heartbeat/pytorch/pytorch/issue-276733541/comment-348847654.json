{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348847654", "html_url": "https://github.com/pytorch/pytorch/issues/3867#issuecomment-348847654", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3867", "id": 348847654, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODg0NzY1NA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-04T02:52:27Z", "updated_at": "2017-12-04T02:52:27Z", "author_association": "MEMBER", "body_html": "<p>I think the biggest source of inefficiency will come from the fact that we will need to add a <code>F.pad</code> layer before every other convolution that requires the <code>padding=same</code> case (because the amount of padding might not the same on the left and right sides), see for example <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L568-L605\">how TensorFlow has to handle that in the <code>cudnn</code> case</a>. So that means that the <code>nn.CalcPadConv*d</code> would be normally as expensive as a <code>nn.Conv*d(..., padding=\"same\")</code>.</p>\n<p>This could be made more efficient if we supported different paddings for each side of the convolution (like in Caffe2, so left, right, top, bottom), but cudnn still doesn't support that so we would require the extra padding in those cases.</p>\n<p>Also, I think if we add the <code>padding=\"same\"</code> to <code>nn.Conv*d</code>, we should probably do the same for <code>nn.*Pool*d</code>, right?</p>\n<p>I think what bothers me a bit is that users might expect the behavior of <code>padding=same</code> to be equivalent to TF, but they might not be expecting a performance drop.</p>\n<p>What do you think?</p>", "body_text": "I think the biggest source of inefficiency will come from the fact that we will need to add a F.pad layer before every other convolution that requires the padding=same case (because the amount of padding might not the same on the left and right sides), see for example how TensorFlow has to handle that in the cudnn case. So that means that the nn.CalcPadConv*d would be normally as expensive as a nn.Conv*d(..., padding=\"same\").\nThis could be made more efficient if we supported different paddings for each side of the convolution (like in Caffe2, so left, right, top, bottom), but cudnn still doesn't support that so we would require the extra padding in those cases.\nAlso, I think if we add the padding=\"same\" to nn.Conv*d, we should probably do the same for nn.*Pool*d, right?\nI think what bothers me a bit is that users might expect the behavior of padding=same to be equivalent to TF, but they might not be expecting a performance drop.\nWhat do you think?", "body": "I think the biggest source of inefficiency will come from the fact that we will need to add a `F.pad` layer before every other convolution that requires the `padding=same` case (because the amount of padding might not the same on the left and right sides), see for example [how TensorFlow has to handle that in the `cudnn` case](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L568-L605). So that means that the `nn.CalcPadConv*d` would be normally as expensive as a `nn.Conv*d(..., padding=\"same\")`.\r\n\r\nThis could be made more efficient if we supported different paddings for each side of the convolution (like in Caffe2, so left, right, top, bottom), but cudnn still doesn't support that so we would require the extra padding in those cases.\r\n\r\nAlso, I think if we add the `padding=\"same\"` to `nn.Conv*d`, we should probably do the same for `nn.*Pool*d`, right?\r\n\r\nI think what bothers me a bit is that users might expect the behavior of `padding=same` to be equivalent to TF, but they might not be expecting a performance drop.\r\n\r\nWhat do you think?"}