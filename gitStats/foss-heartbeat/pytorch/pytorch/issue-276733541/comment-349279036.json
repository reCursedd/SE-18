{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349279036", "html_url": "https://github.com/pytorch/pytorch/issues/3867#issuecomment-349279036", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3867", "id": 349279036, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTI3OTAzNg==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-05T11:37:42Z", "updated_at": "2017-12-05T11:37:42Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5977817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/qbx2\">@qbx2</a> maybe I don't understand fully your proposal, but if we want to replicate TensorFlow behavior I don't think this is enough.</p>\n<p>Here is a snippet of what I think mimics TensorFlow <code>SAME</code> padding (I'm writing it down into the functional interface, so that <code>nn.Conv2d</code> can just call into <code>F.conv2d_same_padding</code>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">conv2d_same_padding</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">dilation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">groups</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n  input_rows <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">2</span>)\n  filter_rows <span class=\"pl-k\">=</span> weight.size(<span class=\"pl-c1\">2</span>)\n  effective_filter_size_rows <span class=\"pl-k\">=</span> (filter_rows <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> dilation[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n  out_rows <span class=\"pl-k\">=</span> (input_rows <span class=\"pl-k\">+</span> stride[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">//</span> stride[<span class=\"pl-c1\">0</span>]\n  padding_needed <span class=\"pl-k\">=</span>\n          <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">0</span>, (out_rows <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> stride[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> effective_filter_size_rows <span class=\"pl-k\">-</span>\n                  input_rows)\n  padding_rows <span class=\"pl-k\">=</span> <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">0</span>, (out_rows <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> stride[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span>\n                        (filter_rows <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> dilation[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> input_rows)\n  rows_odd <span class=\"pl-k\">=</span> (padding_rows <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> same for padding_cols</span>\n\n  <span class=\"pl-k\">if</span> rows_odd <span class=\"pl-k\">or</span> cols_odd:\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> F.pad(<span class=\"pl-c1\">input</span>, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">int</span>(cols_odd), <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">int</span>(rows_odd)])\n\n  <span class=\"pl-k\">return</span> F.conv2d(<span class=\"pl-c1\">input</span>, weight, bias, stride,\n                  <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span>(padding_rows <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, padding_cols <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>),\n                  <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation, <span class=\"pl-v\">groups</span><span class=\"pl-k\">=</span>groups)\n  </pre></div>\n<p>It was mostly copy-pasted from TensorFlow code in <a href=\"https://github.com/tensorflow/tensorflow/blob/3c3c0481ec087aca4fa875d6d936f19b31191fc1/tensorflow/core/framework/common_shape_fns.cc#L40-L48\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L568-L605\">here</a>.</p>\n<p>As you can see, there is a lot of hidden things going on there, and that's why I think it might not be worth it adding a <code>padding='same'</code>. And I think not replicating the <code>SAME</code> behavior in TensorFlow is not ideal either.</p>\n<p>Thoughts?</p>", "body_text": "@qbx2 maybe I don't understand fully your proposal, but if we want to replicate TensorFlow behavior I don't think this is enough.\nHere is a snippet of what I think mimics TensorFlow SAME padding (I'm writing it down into the functional interface, so that nn.Conv2d can just call into F.conv2d_same_padding):\ndef conv2d_same_padding(input, weight, bias=None, stride=1, dilation=1, groups=1):\n  input_rows = input.size(2)\n  filter_rows = weight.size(2)\n  effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n  out_rows = (input_rows + stride[0] - 1) // stride[0]\n  padding_needed =\n          max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n                  input_rows)\n  padding_rows = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n  rows_odd = (padding_rows % 2 != 0)\n  # same for padding_cols\n\n  if rows_odd or cols_odd:\n    input = F.pad(input, [0, int(cols_odd), 0, int(rows_odd)])\n\n  return F.conv2d(input, weight, bias, stride,\n                  padding=(padding_rows // 2, padding_cols // 2),\n                  dilation=dilation, groups=groups)\n  \nIt was mostly copy-pasted from TensorFlow code in here and here.\nAs you can see, there is a lot of hidden things going on there, and that's why I think it might not be worth it adding a padding='same'. And I think not replicating the SAME behavior in TensorFlow is not ideal either.\nThoughts?", "body": "@qbx2 maybe I don't understand fully your proposal, but if we want to replicate TensorFlow behavior I don't think this is enough.\r\n\r\nHere is a snippet of what I think mimics TensorFlow `SAME` padding (I'm writing it down into the functional interface, so that `nn.Conv2d` can just call into `F.conv2d_same_padding`):\r\n```python\r\ndef conv2d_same_padding(input, weight, bias=None, stride=1, dilation=1, groups=1):\r\n  input_rows = input.size(2)\r\n  filter_rows = weight.size(2)\r\n  effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\r\n  out_rows = (input_rows + stride[0] - 1) // stride[0]\r\n  padding_needed =\r\n          max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\r\n                  input_rows)\r\n  padding_rows = max(0, (out_rows - 1) * stride[0] +\r\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\r\n  rows_odd = (padding_rows % 2 != 0)\r\n  # same for padding_cols\r\n\r\n  if rows_odd or cols_odd:\r\n    input = F.pad(input, [0, int(cols_odd), 0, int(rows_odd)])\r\n\r\n  return F.conv2d(input, weight, bias, stride,\r\n                  padding=(padding_rows // 2, padding_cols // 2),\r\n                  dilation=dilation, groups=groups)\r\n  \r\n```\r\n\r\nIt was mostly copy-pasted from TensorFlow code in [here](https://github.com/tensorflow/tensorflow/blob/3c3c0481ec087aca4fa875d6d936f19b31191fc1/tensorflow/core/framework/common_shape_fns.cc#L40-L48) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L568-L605).\r\n\r\nAs you can see, there is a lot of hidden things going on there, and that's why I think it might not be worth it adding a `padding='same'`. And I think not replicating the `SAME` behavior in TensorFlow is not ideal either.\r\n\r\nThoughts?"}