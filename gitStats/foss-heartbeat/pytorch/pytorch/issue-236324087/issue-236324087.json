{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1815", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1815/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1815/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1815/events", "html_url": "https://github.com/pytorch/pytorch/pull/1815", "id": 236324087, "node_id": "MDExOlB1bGxSZXF1ZXN0MTI1OTE5NzM0", "number": 1815, "title": "[WIP] Proof of concept: Forward tracing in PyTorch", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-15T21:49:54Z", "updated_at": "2018-11-23T15:34:09Z", "closed_at": "2017-07-21T15:34:08Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1815", "html_url": "https://github.com/pytorch/pytorch/pull/1815", "diff_url": "https://github.com/pytorch/pytorch/pull/1815.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1815.patch"}, "body_html": "<p>This commit series turns on forward trace construction in PyTorch. These forward traces are automatically saved as you execute your Torch code, and then can be rerun in the future with different input values to do a different computation. This is a PROOF OF CONCEPT because you don't actually want tracing to be turned on all the time.</p>\n<p>Some primary characteristics of the implementation:</p>\n<ul>\n<li>The first four commits are just some refactoring commits, I'll submit them separately.</li>\n<li>The basic IR model is a multigraph, where each Node represents a computation, taking in some number of tensors, and producing some number of tensors. An Output represents a <em>particular</em> output of a function. Unlike the current backwards representation, the pointers in this representation go \"the opposite way.\" Input nodes are identified by pointer equality; every Variable is associated with a unique input node which lets you get a handle into the parameters of the graph.</li>\n<li>The IR is exposed at the Python level using proxy objects; this means that if you don't access the trace from Python, Python objects are never materialized. Note that PyNode operators themselves contain a pointer to the Python class which implements their forward pass, but this cannot cause cycles to arise.</li>\n<li>The IR today uses shared pointers. When we add \"regions\" to trace over, it should become possible to region allocate the IR and then remove the uses of the shared pointers, to make things faster.</li>\n<li>The execution engine is a simple but inefficient memoized, recursive interpreter. In principle we can parallelize it in the same way today's engine is done, but I have not done this yet. The interpreter basically calls into THPFunction_apply, doing the dumbest possible thing (constructing Python objects to pass in, and then destructing Python objects on the way out.)</li>\n</ul>\n<p>Where it's going from here:</p>\n<ul>\n<li>Turn tracing on/off by the user</li>\n<li>Add frontend support for \"dumb\" just-in-time compilation of operators, with no correctness checking</li>\n<li>Implement a peephole rewriter on the resulting trace, allowing us to apply fusion</li>\n<li>Port backward traces to use the new IR format</li>\n<li>Bikeshed the names Output/Node</li>\n<li>Better trace pretty-printing capabilities and a textual language design</li>\n<li>Figure out how to get speed increases even when operators are implemented in Python</li>\n</ul>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=191033\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lantiga\">@lantiga</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a></p>", "body_text": "This commit series turns on forward trace construction in PyTorch. These forward traces are automatically saved as you execute your Torch code, and then can be rerun in the future with different input values to do a different computation. This is a PROOF OF CONCEPT because you don't actually want tracing to be turned on all the time.\nSome primary characteristics of the implementation:\n\nThe first four commits are just some refactoring commits, I'll submit them separately.\nThe basic IR model is a multigraph, where each Node represents a computation, taking in some number of tensors, and producing some number of tensors. An Output represents a particular output of a function. Unlike the current backwards representation, the pointers in this representation go \"the opposite way.\" Input nodes are identified by pointer equality; every Variable is associated with a unique input node which lets you get a handle into the parameters of the graph.\nThe IR is exposed at the Python level using proxy objects; this means that if you don't access the trace from Python, Python objects are never materialized. Note that PyNode operators themselves contain a pointer to the Python class which implements their forward pass, but this cannot cause cycles to arise.\nThe IR today uses shared pointers. When we add \"regions\" to trace over, it should become possible to region allocate the IR and then remove the uses of the shared pointers, to make things faster.\nThe execution engine is a simple but inefficient memoized, recursive interpreter. In principle we can parallelize it in the same way today's engine is done, but I have not done this yet. The interpreter basically calls into THPFunction_apply, doing the dumbest possible thing (constructing Python objects to pass in, and then destructing Python objects on the way out.)\n\nWhere it's going from here:\n\nTurn tracing on/off by the user\nAdd frontend support for \"dumb\" just-in-time compilation of operators, with no correctness checking\nImplement a peephole rewriter on the resulting trace, allowing us to apply fusion\nPort backward traces to use the new IR format\nBikeshed the names Output/Node\nBetter trace pretty-printing capabilities and a textual language design\nFigure out how to get speed increases even when operators are implemented in Python\n\nCC @lantiga @apaszke @zdevito", "body": "This commit series turns on forward trace construction in PyTorch. These forward traces are automatically saved as you execute your Torch code, and then can be rerun in the future with different input values to do a different computation. This is a PROOF OF CONCEPT because you don't actually want tracing to be turned on all the time.\r\n\r\nSome primary characteristics of the implementation:\r\n\r\n* The first four commits are just some refactoring commits, I'll submit them separately.\r\n* The basic IR model is a multigraph, where each Node represents a computation, taking in some number of tensors, and producing some number of tensors. An Output represents a *particular* output of a function. Unlike the current backwards representation, the pointers in this representation go \"the opposite way.\" Input nodes are identified by pointer equality; every Variable is associated with a unique input node which lets you get a handle into the parameters of the graph.\r\n* The IR is exposed at the Python level using proxy objects; this means that if you don't access the trace from Python, Python objects are never materialized. Note that PyNode operators themselves contain a pointer to the Python class which implements their forward pass, but this cannot cause cycles to arise.\r\n* The IR today uses shared pointers. When we add \"regions\" to trace over, it should become possible to region allocate the IR and then remove the uses of the shared pointers, to make things faster.\r\n* The execution engine is a simple but inefficient memoized, recursive interpreter. In principle we can parallelize it in the same way today's engine is done, but I have not done this yet. The interpreter basically calls into THPFunction_apply, doing the dumbest possible thing (constructing Python objects to pass in, and then destructing Python objects on the way out.)\r\n\r\nWhere it's going from here:\r\n\r\n* Turn tracing on/off by the user\r\n* Add frontend support for \"dumb\" just-in-time compilation of operators, with no correctness checking\r\n* Implement a peephole rewriter on the resulting trace, allowing us to apply fusion\r\n* Port backward traces to use the new IR format\r\n* Bikeshed the names Output/Node\r\n* Better trace pretty-printing capabilities and a textual language design\r\n* Figure out how to get speed increases even when operators are implemented in Python\r\n\r\nCC @lantiga @apaszke @zdevito "}