{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11455", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11455/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11455/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11455/events", "html_url": "https://github.com/pytorch/pytorch/issues/11455", "id": 358647681, "node_id": "MDU6SXNzdWUzNTg2NDc2ODE=", "number": 11455, "title": "[jit][script] for-loops hide fusion opportunities", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-10T14:25:27Z", "updated_at": "2018-09-10T16:33:37Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Example:</p>\n<pre><code>In [10]: import torch\n    ...:\n    ...: @torch.jit.script\n    ...: def recurrent_scaleshift(x, scale, shift):\n    ...:     y = x\n    ...:     for i in range(64):\n    ...:         y = scale * y + shift\n    ...:     return y\n    ...:\n    ...:\n    ...: x = torch.randn(2, 2, device='cuda')\n    ...: scale = torch.randn(2, 2, device='cuda', requires_grad=True)\n    ...: shift = torch.randn(2, 2, device='cuda', requires_grad=True)\n    ...: inputs = [x, scale, shift]\n    ...: out = recurrent_scaleshift(x, scale, shift)\n    ...: recurrent_scaleshift.graph_for(x, scale, shift)\n    ...:\n    ...:\nOut[10]:\ngraph(%y.1 : Float(*, *)\n      %scale : Float(*, *)\n      %shift : Float(*, *)) {\n  %44 : int = prim::Constant[value=1]()\n  %45 : int = prim::Constant[value=8]()\n  %46 : int = prim::Constant[value=0]()\n  %y.3 : Float(*, *) = prim::Loop(%45, %44, %y.1)\n    block0(%i.1 : int, %7 : Float(*, *)) {\n      %42 : Float(*, *) = prim::DifferentiableGraph_0(%scale, %7, %shift, %44)\n      -&gt; (%44, %42)\n    }\n  %y : Float(*, *) = prim::Loop(%46, %44, %y.3)\n    block0(%i : int, %14 : Float(*, *)) {\n      %47 : Float(*, *) = aten::mul(%scale, %14)\n      %y.4 : Float(*, *) = aten::add(%47, %shift, %44)\n      -&gt; (%44, %y.4)\n    }\n  return (%y);\n}\nwith prim::DifferentiableGraph_0 = graph(%1 : Float(*, *)\n      %2 : Float(*, *)\n      %4 : Float(*, *)\n      %5 : int) {\n  %0 : Float(*, *) = aten::mul(%1, %2)\n  %y.13 : Float(*, *) = aten::add(%0, %4, %5)\n  %6 : Float(*, *) = aten::mul(%1, %y.13)\n  %y.14 : Float(*, *) = aten::add(%6, %4, %5)\n  %8 : Float(*, *) = aten::mul(%1, %y.14)\n  %y.15 : Float(*, *) = aten::add(%8, %4, %5)\n  %10 : Float(*, *) = aten::mul(%1, %y.15)\n  %y.16 : Float(*, *) = aten::add(%10, %4, %5)\n  %12 : Float(*, *) = aten::mul(%1, %y.16)\n  %y.17 : Float(*, *) = aten::add(%12, %4, %5)\n  %14 : Float(*, *) = aten::mul(%1, %y.17)\n  %y.18 : Float(*, *) = aten::add(%14, %4, %5)\n  %16 : Float(*, *) = aten::mul(%1, %y.18)\n  %y.19 : Float(*, *) = aten::add(%16, %4, %5)\n  %18 : Float(*, *) = aten::mul(%1, %y.19)\n  %y.11 : Float(*, *) = aten::add(%18, %4, %5)\n  return (%y.11, %y.13, %y.14, %y.15, %y.16, %y.17, %y.18, %y.19);\n}\n</code></pre>\n<p>There are no fusion groups. However, on its own, a scale + shift does produce a FusionGroup:</p>\n<pre><code>In [7]: @torch.jit.script\n   ...: def fn(x, scale, shift):\n   ...:     return scale * x + shift\n   ...:\n   ...:\n\nIn [8]:\n\nIn [8]: fn(x, scale, shift)\nOut[8]:\ntensor([[ 2.0678,  0.3705],\n        [ 0.1398, -0.4048]], device='cuda:0',\n       grad_fn=&lt;DifferentiableGraphBackward&gt;)\n\nIn [9]:\n\nIn [9]: fn.graph_for(x, scale, shift)\nOut[9]:\ngraph(%x : Float(*, *)\n      %scale : Float(*, *)\n      %shift : Float(*, *)) {\n  %6 : Float(*, *) = prim::DifferentiableGraph_0(%scale, %x, %shift)\n  return (%6);\n}\nwith prim::DifferentiableGraph_0 = graph(%1 : Float(*, *)\n      %2 : Float(*, *)\n      %5 : Float(*, *)) {\n  %3 : int = prim::Constant[value=1]()\n  %17 : Float(*, *) = prim::FusionGroup_0[device=0](%5, %1, %2)\n  return (%17);\n}\nwith prim::FusionGroup_0 = graph(%1 : Float(*, *)\n      %4 : Float(*, *)\n      %5 : Float(*, *)) {\n  %6 : Float(*, *) = aten::mul(%4, %5)\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(*, *) = aten::add(%6, %1, %2)\n  return (%3);\n}\n</code></pre>", "body_text": "Example:\nIn [10]: import torch\n    ...:\n    ...: @torch.jit.script\n    ...: def recurrent_scaleshift(x, scale, shift):\n    ...:     y = x\n    ...:     for i in range(64):\n    ...:         y = scale * y + shift\n    ...:     return y\n    ...:\n    ...:\n    ...: x = torch.randn(2, 2, device='cuda')\n    ...: scale = torch.randn(2, 2, device='cuda', requires_grad=True)\n    ...: shift = torch.randn(2, 2, device='cuda', requires_grad=True)\n    ...: inputs = [x, scale, shift]\n    ...: out = recurrent_scaleshift(x, scale, shift)\n    ...: recurrent_scaleshift.graph_for(x, scale, shift)\n    ...:\n    ...:\nOut[10]:\ngraph(%y.1 : Float(*, *)\n      %scale : Float(*, *)\n      %shift : Float(*, *)) {\n  %44 : int = prim::Constant[value=1]()\n  %45 : int = prim::Constant[value=8]()\n  %46 : int = prim::Constant[value=0]()\n  %y.3 : Float(*, *) = prim::Loop(%45, %44, %y.1)\n    block0(%i.1 : int, %7 : Float(*, *)) {\n      %42 : Float(*, *) = prim::DifferentiableGraph_0(%scale, %7, %shift, %44)\n      -> (%44, %42)\n    }\n  %y : Float(*, *) = prim::Loop(%46, %44, %y.3)\n    block0(%i : int, %14 : Float(*, *)) {\n      %47 : Float(*, *) = aten::mul(%scale, %14)\n      %y.4 : Float(*, *) = aten::add(%47, %shift, %44)\n      -> (%44, %y.4)\n    }\n  return (%y);\n}\nwith prim::DifferentiableGraph_0 = graph(%1 : Float(*, *)\n      %2 : Float(*, *)\n      %4 : Float(*, *)\n      %5 : int) {\n  %0 : Float(*, *) = aten::mul(%1, %2)\n  %y.13 : Float(*, *) = aten::add(%0, %4, %5)\n  %6 : Float(*, *) = aten::mul(%1, %y.13)\n  %y.14 : Float(*, *) = aten::add(%6, %4, %5)\n  %8 : Float(*, *) = aten::mul(%1, %y.14)\n  %y.15 : Float(*, *) = aten::add(%8, %4, %5)\n  %10 : Float(*, *) = aten::mul(%1, %y.15)\n  %y.16 : Float(*, *) = aten::add(%10, %4, %5)\n  %12 : Float(*, *) = aten::mul(%1, %y.16)\n  %y.17 : Float(*, *) = aten::add(%12, %4, %5)\n  %14 : Float(*, *) = aten::mul(%1, %y.17)\n  %y.18 : Float(*, *) = aten::add(%14, %4, %5)\n  %16 : Float(*, *) = aten::mul(%1, %y.18)\n  %y.19 : Float(*, *) = aten::add(%16, %4, %5)\n  %18 : Float(*, *) = aten::mul(%1, %y.19)\n  %y.11 : Float(*, *) = aten::add(%18, %4, %5)\n  return (%y.11, %y.13, %y.14, %y.15, %y.16, %y.17, %y.18, %y.19);\n}\n\nThere are no fusion groups. However, on its own, a scale + shift does produce a FusionGroup:\nIn [7]: @torch.jit.script\n   ...: def fn(x, scale, shift):\n   ...:     return scale * x + shift\n   ...:\n   ...:\n\nIn [8]:\n\nIn [8]: fn(x, scale, shift)\nOut[8]:\ntensor([[ 2.0678,  0.3705],\n        [ 0.1398, -0.4048]], device='cuda:0',\n       grad_fn=<DifferentiableGraphBackward>)\n\nIn [9]:\n\nIn [9]: fn.graph_for(x, scale, shift)\nOut[9]:\ngraph(%x : Float(*, *)\n      %scale : Float(*, *)\n      %shift : Float(*, *)) {\n  %6 : Float(*, *) = prim::DifferentiableGraph_0(%scale, %x, %shift)\n  return (%6);\n}\nwith prim::DifferentiableGraph_0 = graph(%1 : Float(*, *)\n      %2 : Float(*, *)\n      %5 : Float(*, *)) {\n  %3 : int = prim::Constant[value=1]()\n  %17 : Float(*, *) = prim::FusionGroup_0[device=0](%5, %1, %2)\n  return (%17);\n}\nwith prim::FusionGroup_0 = graph(%1 : Float(*, *)\n      %4 : Float(*, *)\n      %5 : Float(*, *)) {\n  %6 : Float(*, *) = aten::mul(%4, %5)\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(*, *) = aten::add(%6, %1, %2)\n  return (%3);\n}", "body": "Example:\r\n```\r\nIn [10]: import torch\r\n    ...:\r\n    ...: @torch.jit.script\r\n    ...: def recurrent_scaleshift(x, scale, shift):\r\n    ...:     y = x\r\n    ...:     for i in range(64):\r\n    ...:         y = scale * y + shift\r\n    ...:     return y\r\n    ...:\r\n    ...:\r\n    ...: x = torch.randn(2, 2, device='cuda')\r\n    ...: scale = torch.randn(2, 2, device='cuda', requires_grad=True)\r\n    ...: shift = torch.randn(2, 2, device='cuda', requires_grad=True)\r\n    ...: inputs = [x, scale, shift]\r\n    ...: out = recurrent_scaleshift(x, scale, shift)\r\n    ...: recurrent_scaleshift.graph_for(x, scale, shift)\r\n    ...:\r\n    ...:\r\nOut[10]:\r\ngraph(%y.1 : Float(*, *)\r\n      %scale : Float(*, *)\r\n      %shift : Float(*, *)) {\r\n  %44 : int = prim::Constant[value=1]()\r\n  %45 : int = prim::Constant[value=8]()\r\n  %46 : int = prim::Constant[value=0]()\r\n  %y.3 : Float(*, *) = prim::Loop(%45, %44, %y.1)\r\n    block0(%i.1 : int, %7 : Float(*, *)) {\r\n      %42 : Float(*, *) = prim::DifferentiableGraph_0(%scale, %7, %shift, %44)\r\n      -> (%44, %42)\r\n    }\r\n  %y : Float(*, *) = prim::Loop(%46, %44, %y.3)\r\n    block0(%i : int, %14 : Float(*, *)) {\r\n      %47 : Float(*, *) = aten::mul(%scale, %14)\r\n      %y.4 : Float(*, *) = aten::add(%47, %shift, %44)\r\n      -> (%44, %y.4)\r\n    }\r\n  return (%y);\r\n}\r\nwith prim::DifferentiableGraph_0 = graph(%1 : Float(*, *)\r\n      %2 : Float(*, *)\r\n      %4 : Float(*, *)\r\n      %5 : int) {\r\n  %0 : Float(*, *) = aten::mul(%1, %2)\r\n  %y.13 : Float(*, *) = aten::add(%0, %4, %5)\r\n  %6 : Float(*, *) = aten::mul(%1, %y.13)\r\n  %y.14 : Float(*, *) = aten::add(%6, %4, %5)\r\n  %8 : Float(*, *) = aten::mul(%1, %y.14)\r\n  %y.15 : Float(*, *) = aten::add(%8, %4, %5)\r\n  %10 : Float(*, *) = aten::mul(%1, %y.15)\r\n  %y.16 : Float(*, *) = aten::add(%10, %4, %5)\r\n  %12 : Float(*, *) = aten::mul(%1, %y.16)\r\n  %y.17 : Float(*, *) = aten::add(%12, %4, %5)\r\n  %14 : Float(*, *) = aten::mul(%1, %y.17)\r\n  %y.18 : Float(*, *) = aten::add(%14, %4, %5)\r\n  %16 : Float(*, *) = aten::mul(%1, %y.18)\r\n  %y.19 : Float(*, *) = aten::add(%16, %4, %5)\r\n  %18 : Float(*, *) = aten::mul(%1, %y.19)\r\n  %y.11 : Float(*, *) = aten::add(%18, %4, %5)\r\n  return (%y.11, %y.13, %y.14, %y.15, %y.16, %y.17, %y.18, %y.19);\r\n}\r\n```\r\nThere are no fusion groups. However, on its own, a scale + shift does produce a FusionGroup:\r\n```\r\nIn [7]: @torch.jit.script\r\n   ...: def fn(x, scale, shift):\r\n   ...:     return scale * x + shift\r\n   ...:\r\n   ...:\r\n\r\nIn [8]:\r\n\r\nIn [8]: fn(x, scale, shift)\r\nOut[8]:\r\ntensor([[ 2.0678,  0.3705],\r\n        [ 0.1398, -0.4048]], device='cuda:0',\r\n       grad_fn=<DifferentiableGraphBackward>)\r\n\r\nIn [9]:\r\n\r\nIn [9]: fn.graph_for(x, scale, shift)\r\nOut[9]:\r\ngraph(%x : Float(*, *)\r\n      %scale : Float(*, *)\r\n      %shift : Float(*, *)) {\r\n  %6 : Float(*, *) = prim::DifferentiableGraph_0(%scale, %x, %shift)\r\n  return (%6);\r\n}\r\nwith prim::DifferentiableGraph_0 = graph(%1 : Float(*, *)\r\n      %2 : Float(*, *)\r\n      %5 : Float(*, *)) {\r\n  %3 : int = prim::Constant[value=1]()\r\n  %17 : Float(*, *) = prim::FusionGroup_0[device=0](%5, %1, %2)\r\n  return (%17);\r\n}\r\nwith prim::FusionGroup_0 = graph(%1 : Float(*, *)\r\n      %4 : Float(*, *)\r\n      %5 : Float(*, *)) {\r\n  %6 : Float(*, *) = aten::mul(%4, %5)\r\n  %2 : int = prim::Constant[value=1]()\r\n  %3 : Float(*, *) = aten::add(%6, %1, %2)\r\n  return (%3);\r\n}\r\n```"}