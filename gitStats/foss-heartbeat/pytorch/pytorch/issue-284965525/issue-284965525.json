{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4388", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4388/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4388/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4388/events", "html_url": "https://github.com/pytorch/pytorch/issues/4388", "id": 284965525, "node_id": "MDU6SXNzdWUyODQ5NjU1MjU=", "number": 4388, "title": "Dynamic addition of neurons", "user": {"login": "scelesticsiva", "id": 22149278, "node_id": "MDQ6VXNlcjIyMTQ5Mjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/22149278?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scelesticsiva", "html_url": "https://github.com/scelesticsiva", "followers_url": "https://api.github.com/users/scelesticsiva/followers", "following_url": "https://api.github.com/users/scelesticsiva/following{/other_user}", "gists_url": "https://api.github.com/users/scelesticsiva/gists{/gist_id}", "starred_url": "https://api.github.com/users/scelesticsiva/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scelesticsiva/subscriptions", "organizations_url": "https://api.github.com/users/scelesticsiva/orgs", "repos_url": "https://api.github.com/users/scelesticsiva/repos", "events_url": "https://api.github.com/users/scelesticsiva/events{/privacy}", "received_events_url": "https://api.github.com/users/scelesticsiva/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-28T18:21:02Z", "updated_at": "2017-12-28T20:54:25Z", "closed_at": "2017-12-28T20:54:25Z", "author_association": "NONE", "body_html": "<p>I am trying to add hidden units to a 3-layered neural network(input, hidden,output) dynamically as I train it. I want to keep the weights of trained part of the network as I add new hidden units.This is my code,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">my_network</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">input_dim</span>,<span class=\"pl-smi\">hidden_dim</span>,<span class=\"pl-smi\">output_dim</span>):\n        <span class=\"pl-c1\">super</span>(my_network,<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.I <span class=\"pl-k\">=</span> input_dim\n        <span class=\"pl-c1\">self</span>.H <span class=\"pl-k\">=</span> hidden_dim\n        <span class=\"pl-c1\">self</span>.O <span class=\"pl-k\">=</span> output_dim\n        <span class=\"pl-c1\">self</span>.layer1 <span class=\"pl-k\">=</span> torch.nn.Linear(input_dim,hidden_dim)\n        <span class=\"pl-c1\">self</span>.layer2 <span class=\"pl-k\">=</span> torch.nn.Linear(hidden_dim,output_dim)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">add_neurons</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">no_of_neurons</span>,<span class=\"pl-smi\">flag</span>):\n        <span class=\"pl-k\">if</span> flag <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n            weights <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">self</span>.layer1.weight.data,<span class=\"pl-c1\">self</span>.layer2.weight.data]\n            <span class=\"pl-c1\">self</span>.layer1 <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">self</span>.I,<span class=\"pl-c1\">self</span>.H<span class=\"pl-k\">+</span>no_of_neurons)\n            <span class=\"pl-c1\">self</span>.layer2 <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">self</span>.H<span class=\"pl-k\">+</span>no_of_neurons,<span class=\"pl-c1\">self</span>.O)\n            <span class=\"pl-c1\">self</span>.layer1.weight.data[<span class=\"pl-c1\">0</span>:<span class=\"pl-k\">-</span>no_of_neurons,:] <span class=\"pl-k\">=</span> weights[<span class=\"pl-c1\">0</span>]\n            <span class=\"pl-c1\">self</span>.layer2.weight.data[:,<span class=\"pl-c1\">0</span>:<span class=\"pl-k\">-</span>no_of_neurons] <span class=\"pl-k\">=</span> weights[<span class=\"pl-c1\">1</span>]\n            <span class=\"pl-c1\">self</span>.H <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.H <span class=\"pl-k\">+</span> no_of_neurons\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.layer1.weight.shape[<span class=\"pl-c1\">0</span>]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">x</span>):\n        temp <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer1(x)\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer2(temp)\n        <span class=\"pl-k\">return</span> out</pre></div>\n<p>I have noticed that once I call \u201cadd_neurons\u201d method, the weights stop updating(while gradients are generated). Any help would be much appreciated.</p>", "body_text": "I am trying to add hidden units to a 3-layered neural network(input, hidden,output) dynamically as I train it. I want to keep the weights of trained part of the network as I add new hidden units.This is my code,\nclass my_network(torch.nn.Module):\n    def __init__(self,input_dim,hidden_dim,output_dim):\n        super(my_network,self).__init__()\n        self.I = input_dim\n        self.H = hidden_dim\n        self.O = output_dim\n        self.layer1 = torch.nn.Linear(input_dim,hidden_dim)\n        self.layer2 = torch.nn.Linear(hidden_dim,output_dim)\n\n    def add_neurons(self,no_of_neurons,flag):\n        if flag == 1:\n            weights = [self.layer1.weight.data,self.layer2.weight.data]\n            self.layer1 = torch.nn.Linear(self.I,self.H+no_of_neurons)\n            self.layer2 = torch.nn.Linear(self.H+no_of_neurons,self.O)\n            self.layer1.weight.data[0:-no_of_neurons,:] = weights[0]\n            self.layer2.weight.data[:,0:-no_of_neurons] = weights[1]\n            self.H = self.H + no_of_neurons\n        return self.layer1.weight.shape[0]\n\n    def forward(self,x):\n        temp = self.layer1(x)\n        out = self.layer2(temp)\n        return out\nI have noticed that once I call \u201cadd_neurons\u201d method, the weights stop updating(while gradients are generated). Any help would be much appreciated.", "body": "I am trying to add hidden units to a 3-layered neural network(input, hidden,output) dynamically as I train it. I want to keep the weights of trained part of the network as I add new hidden units.This is my code,\r\n\r\n```python\r\nclass my_network(torch.nn.Module):\r\n    def __init__(self,input_dim,hidden_dim,output_dim):\r\n        super(my_network,self).__init__()\r\n        self.I = input_dim\r\n        self.H = hidden_dim\r\n        self.O = output_dim\r\n        self.layer1 = torch.nn.Linear(input_dim,hidden_dim)\r\n        self.layer2 = torch.nn.Linear(hidden_dim,output_dim)\r\n\r\n    def add_neurons(self,no_of_neurons,flag):\r\n        if flag == 1:\r\n            weights = [self.layer1.weight.data,self.layer2.weight.data]\r\n            self.layer1 = torch.nn.Linear(self.I,self.H+no_of_neurons)\r\n            self.layer2 = torch.nn.Linear(self.H+no_of_neurons,self.O)\r\n            self.layer1.weight.data[0:-no_of_neurons,:] = weights[0]\r\n            self.layer2.weight.data[:,0:-no_of_neurons] = weights[1]\r\n            self.H = self.H + no_of_neurons\r\n        return self.layer1.weight.shape[0]\r\n\r\n    def forward(self,x):\r\n        temp = self.layer1(x)\r\n        out = self.layer2(temp)\r\n        return out\r\n```\r\nI have noticed that once I call \u201cadd_neurons\u201d method, the weights stop updating(while gradients are generated). Any help would be much appreciated."}