{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159043526", "pull_request_review_id": 85933143, "id": 159043526, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTA0MzUyNg==", "diff_hunk": "@@ -105,6 +105,193 @@ Tensor sum_backward(const Tensor & grad, IntList sizes, int64_t dim, bool keepdi\n   }\n }\n \n+Tensor reverse_dim(const Tensor& t, int64_t dim) {\n+  Tensor index = t.type().toScalarType(at::ScalarType::Long).arange(t.size(dim) - 1, -1, -1);\n+  return t.index_select(dim, index);\n+}\n+\n+Tensor prod_safe_zeros_backward(const Tensor &grad, const Tensor& inp, int64_t dim) {\n+  if (inp.size(dim) == 1) {\n+    return grad;\n+  }\n+\n+  std::vector<int64_t> ones_size(inp.sizes());\n+  ones_size[dim] = 1;\n+  Tensor ones = grad.type().ones(ones_size);\n+  Tensor exclusive_normal_nocp = at::cat({ones, inp.narrow(dim, 0, inp.size(dim) - 1)}, dim);\n+  Tensor exclusive_normal = exclusive_normal_nocp.cumprod(dim);\n+\n+  Tensor narrow_reverse = reverse_dim(inp.narrow(dim, 1, inp.size(dim) - 1), dim);\n+  Tensor exclusive_reverse_nocp = at::cat({ones, narrow_reverse}, dim);\n+  Tensor exclusive_reverse = reverse_dim(exclusive_reverse_nocp.cumprod(dim), dim);\n+\n+  return grad.expand_as(exclusive_normal).mul(exclusive_normal.mul(exclusive_reverse));\n+}\n+\n+// note that the gradient for prod is equivalent to:\n+// cumprod(exclusive, normal) * cumprod(exclusive, reverse), e.g.:\n+// input:                        [    a,     b,     c]\n+// cumprod(exclusive, normal):   [1    ,     a, a * b]\n+// cumprod(exclusive, reverse):  [b * c,     c,     1]\n+// product:                      [b * c, a * c, a * b]\n+// and this is safe under input with 0s.\n+Tensor prod_backward(const Tensor& grad, const Tensor& input, const Tensor& result) {\n+  Tensor zero_idx = (input == 0).nonzero();\n+  if (zero_idx.numel() == 0) {\n+    return (grad * result) / input;\n+  } else if (zero_idx.size(0) > 1) {\n+    return zeros_like(input);\n+  } else {\n+    return prod_safe_zeros_backward(grad, input.contiguous().view(-1), 0).view_as(input);\n+  }\n+}\n+\n+Tensor prod_backward(Tensor grad, const Tensor& input, Tensor result, int64_t dim, bool keepdim) {\n+  dim = at::maybe_wrap_dim(dim, input.sizes().size());\n+  if (!keepdim && input.dim() != 1) {\n+    grad = grad.unsqueeze(dim);\n+    result = result.unsqueeze(dim);\n+  }\n+\n+  Tensor zero_mask = (input == 0);\n+  Tensor slice_zero_count = zero_mask.sum(dim, true);\n+  int64_t total_zeros = slice_zero_count.sum().toCLong();\n+  if (total_zeros == 0) {\n+    return (grad * result) / input;\n+  } else {\n+    return prod_safe_zeros_backward(grad, input, dim);\n+  }\n+}\n+\n+Tensor sum_scan_exclusive(const Tensor& x, int64_t dim) {\n+  Tensor ret = at::cumsum(-x, dim);\n+\n+  int64_t end_idx = ret.size(dim) - 1;\n+  Tensor ret_sum = ret.narrow(dim, end_idx, 1).clone();\n+  ret -= ret_sum.expand_as(ret);\n+  ret += x;\n+  return ret;\n+}\n+\n+Tensor cumprod_backward(const Tensor &grad, const Tensor &input, int64_t dim) {\n+  /*\n+    There are two algorithms to do this. The first one\n+    is very efficient, but works only when there are no\n+    nonzero elements in the input.\n+\n+    The second one is much more complex, but it doesn't\n+    assume anything on the input. The main downside is\n+    that it takes time O(n^2), where n = input.size(self.dim)\n+    (i.e. the length of the cumulative product). This is in\n+    contrast to the forward pass and the efficient algorithm,\n+    which are both O(n).\n+\n+    The second algorithm is a simple application of the chain\n+    rule. If x is an n-dimensional vector, and y = cumprod(x),\n+    and F is the final cost, then\n+\n+    dF / dx_k = sum_j (dF / dy_j) * (dy_j / dx_k)   (1)\n+\n+    The term dF / dy_j is just grad_output[j] (assuming again\n+    everything is one-dimensional).\n+\n+    The term (dy_j / dx_k) is easilly seen to be\n+\n+    if j >= k\n+      dy_j / dx_k = prod_{1 <= i <= j, i != k} x_i\n+    else:\n+      dy_j / dx_k = 0\n+\n+    Note that the indicator (j>=k) can be taken out\n+    by replacing the sum in (1) with a sum from\n+    j = k to n.\n+\n+    Thus,\n+    df / dx_k = sum_{k <= j <= n} grad_output[j] * (dy_j / dx_k)\n+\n+    with\n+    dy_j / dx_k = prod_{1 <= i <= j, i != k} x_i     (2)\n+\n+    Note that this last term is just the cumulative product\n+    with k omitted. Thus, if x_k (the input) is nonzero, we can\n+    just express this as\n+\n+    dy_j / dx_k = (prod_{1 <= i <= j} x_i) / x_k\n+                = y_j / x_k\n+\n+    So therefore,\n+\n+    df / dx_k = sum_{k <= j <= n} grad_output[j] * y_j / x_k\n+\n+    so\n+\n+    grad_output = sum_scan_exclusiv(grad_output * output) / input\n+\n+    If the input is nonzero, we need to calculate the dy_j / dx_k\n+    by using the formula (2), called in the code omitted_products.\n+\n+    The way the code calculates it is simply by noting that\n+\n+    prod_{1 <= i <= j, i != k} x_i\n+        = (prod_{1 <= i <= k} x_i) * (prod_{k + 1 <= i <= j} x_i)\n+\n+    the first term is calculated as prods_until_k, which since\n+    doesn't depend in j is easy to vectorize.\n+\n+    The second term (indexed by j) is the cumulative product of\n+    x_{k+1}, x_{k+2}, ..., x_n, and it's named in the code\n+    prods_from_k_pkus_1, and it's calculated as a cumprod.\n+\n+    In order to vectorize this properly, we need to add to\n+    omitted_products the dimensions where k > j, and therefore\n+    dy_j / dx_k = 0, which is done right after the assert.\n+  */\n+\n+  dim = at::maybe_wrap_dim(dim, input.sizes().size());\n+  int64_t dim_size = input.size(dim);\n+  if (dim_size == 1) {\n+    return grad;\n+  }\n+\n+  // Simple case with nonzero elements in the input\n+  if ((input != 0).all()) {\n+    Tensor result = at::cumprod(input, dim);\n+    return sum_scan_exclusive(result * grad, dim) / input;\n+  }\n+\n+  std::vector<int64_t> ones_size(input.sizes());\n+  ones_size[dim] = 1;\n+  Tensor ones = grad.type().ones({1}).expand(ones_size);\n+  Tensor grad_input = grad.type().zeros(input.sizes());\n+  Tensor prods_from_k_plus_1;\n+  Tensor omitted_products;\n+  for (int k = 0; k < dim_size; ++k) {\n+    if (k == 0) {\n+      prods_from_k_plus_1 = at::cumprod(input.slice_dim(dim, k + 1), dim);\n+      omitted_products = at::cat({ones, prods_from_k_plus_1}, dim);\n+    } else if (k == dim_size - 1) {\n+      Tensor prods_until_k = at::prod(input.slice_dim(dim, 0, k), dim, true);\n+      omitted_products = prods_until_k;\n+    } else {\n+      Tensor prods_until_k = at::prod(input.slice_dim(dim, 0, k), dim, true);\n+      prods_from_k_plus_1 = at::cumprod(input.slice_dim(dim, k+1), dim);\n+      omitted_products = prods_until_k.expand_as(prods_from_k_plus_1) * prods_from_k_plus_1;\n+      omitted_products = at::cat({prods_until_k, omitted_products}, dim);\n+    }\n+\n+    // At this point omitted_products is the same size\n+    // as input, except on the dimension dim where it's\n+    // dim_size - k\n+    TORCH_ASSERT(omitted_products.size(dim) == dim_size - k);\n+\n+    // should we implement copy_ or _set_item in variable?", "path": "tools/autograd/templates/Functions.cpp", "position": null, "original_position": 183, "commit_id": "2f52b3a11cb0f864f3bea88173df8f17114f1701", "original_commit_id": "685e303ca58c2ab6a6c2165a76a989259d8d2c15", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This comment no longer applies", "created_at": "2017-12-29T10:02:29Z", "updated_at": "2018-11-23T15:37:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/4394#discussion_r159043526", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4394", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159043526"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4394#discussion_r159043526"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4394"}}, "body_html": "<p>This comment no longer applies</p>", "body_text": "This comment no longer applies"}