{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391791231", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391791231", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391791231, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTc5MTIzMQ==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-24T17:12:58Z", "updated_at": "2018-05-26T12:59:42Z", "author_association": "NONE", "body_html": "<p>Status update - playing a bit with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>'s example.</p>\n<p><a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/tree/master/pytorch\">Test-code</a> updated, though it is becoming messy.<br>\nI'll try to clean it up with some examples when it's done.</p>\n<ul>\n<li><a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/batchFuncs.py\">Batch gradient ops</a></li>\n<li><a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/models.py\">Model class</a></li>\n<li><a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/main.py\">Testing script</a> (messy)</li>\n</ul>\n<p>An obvious-in-retrospect issue with the current implementation is that the backward operations involve <code>[N x N x 1]</code> tensors multiplication, and thus scales in O(N^2).</p>\n<hr>\n<p>The following table illustrates the issue with various batch sizes.<br>\nThe model is a MLP with sigmoid activations, 10-dimensional inputs and two hidden layers of 10 nodes.</p>\n<table>\n<thead>\n<tr>\n<th>N=</th>\n<th>10</th>\n<th>100</th>\n<th>1000</th>\n<th>5000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Full</td>\n<td>0.000</td>\n<td>0.003</td>\n<td>0.008</td>\n<td>0.060</td>\n</tr>\n<tr>\n<td>stack</td>\n<td>0.008</td>\n<td>0.005</td>\n<td>0.243</td>\n<td>4.350</td>\n</tr>\n<tr>\n<td>naive</td>\n<td>0.008</td>\n<td>0.050</td>\n<td>0.595</td>\n<td>7.118</td>\n</tr>\n<tr>\n<td>goodf*</td>\n<td>0.000</td>\n<td>0.010</td>\n<td>0.087</td>\n<td>0.303</td>\n</tr>\n<tr>\n<td>multi</td>\n<td>0.005</td>\n<td>0.074</td>\n<td>0.397</td>\n<td>1.803</td>\n</tr>\n</tbody>\n</table>\n<p>(in seconds, truncated to %.3f)</p>\n<p>*This benchmark oversells the performance of this method which has a much higher constant factor on the <code>O(D^2)</code> scaling.<br>\n<em>Edit 26/05/18: This effect turned out to be an implementation issue, where outer products where taken in a for-loop instead of using <a href=\"https://pytorch.org/docs/master/torch.html#torch.bmm\" rel=\"nofollow\"><code>bmm</code></a></em></p>\n<hr>\n<p>This however is not a limitation of the idea, only the current implementation using multiplication with those huge tensors. In every one of those multiplication, one of the dimension is useless - in that only a single line of each dimension is nnz.</p>\n<p>I think a more careful implementation could scale - for example by only computing product for nnz lines or resizing \"on the fly\" instead of storing those huge matrices.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> I only now realise (through your <a href=\"https://discuss.pytorch.org/t/efficient-computation-of-per-sample-examples/18587/2\" rel=\"nofollow\">forum post</a>) that your <a href=\"https://github.com/pytorch/pytorch/issues/7786#issuecomment-391327871\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7786/hovercard\">earlier post</a> was using the same trick. I will try to play around with the re-definition of backward for those ops and see if I can make this work in the next few days (though feel free to ping in if the solution seems obvious :)</p>\n<p>Thanks again!</p>", "body_text": "Status update - playing a bit with @apaszke's example.\nTest-code updated, though it is becoming messy.\nI'll try to clean it up with some examples when it's done.\n\nBatch gradient ops\nModel class\nTesting script (messy)\n\nAn obvious-in-retrospect issue with the current implementation is that the backward operations involve [N x N x 1] tensors multiplication, and thus scales in O(N^2).\n\nThe following table illustrates the issue with various batch sizes.\nThe model is a MLP with sigmoid activations, 10-dimensional inputs and two hidden layers of 10 nodes.\n\n\n\nN=\n10\n100\n1000\n5000\n\n\n\n\nFull\n0.000\n0.003\n0.008\n0.060\n\n\nstack\n0.008\n0.005\n0.243\n4.350\n\n\nnaive\n0.008\n0.050\n0.595\n7.118\n\n\ngoodf*\n0.000\n0.010\n0.087\n0.303\n\n\nmulti\n0.005\n0.074\n0.397\n1.803\n\n\n\n(in seconds, truncated to %.3f)\n*This benchmark oversells the performance of this method which has a much higher constant factor on the O(D^2) scaling.\nEdit 26/05/18: This effect turned out to be an implementation issue, where outer products where taken in a for-loop instead of using bmm\n\nThis however is not a limitation of the idea, only the current implementation using multiplication with those huge tensors. In every one of those multiplication, one of the dimension is useless - in that only a single line of each dimension is nnz.\nI think a more careful implementation could scale - for example by only computing product for nnz lines or resizing \"on the fly\" instead of storing those huge matrices.\n@t-vi I only now realise (through your forum post) that your earlier post was using the same trick. I will try to play around with the re-definition of backward for those ops and see if I can make this work in the next few days (though feel free to ping in if the solution seems obvious :)\nThanks again!", "body": "Status update - playing a bit with @apaszke's example.\r\n\r\n[Test-code](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/tree/master/pytorch) updated, though it is becoming messy.\r\nI'll try to clean it up with some examples when it's done.\r\n* [Batch gradient ops](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/batchFuncs.py)\r\n* [Model class](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/models.py)\r\n* [Testing script](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/main.py) (messy)\r\n\r\nAn obvious-in-retrospect issue with the current implementation is that the backward operations involve `[N x N x 1]` tensors multiplication, and thus scales in O(N^2).\r\n\r\n---\r\n\r\nThe following table illustrates the issue with various batch sizes.\r\nThe model is a MLP with sigmoid activations, 10-dimensional inputs and two hidden layers of 10 nodes.\r\n\r\nN=    | 10 | 100 | 1000 | 5000\r\n--    | -- | -- | -- | --\r\nFull  | 0.000 | 0.003 | 0.008 | 0.060\r\nstack | 0.008 | 0.005 | 0.243 | 4.350\r\nnaive | 0.008 | 0.050 | 0.595 | 7.118\r\ngoodf* | 0.000 | 0.010 | 0.087 | 0.303\r\nmulti | 0.005 | 0.074 | 0.397 | 1.803\r\n\r\n(in seconds, truncated to %.3f)\r\n\r\n*This benchmark oversells the performance of this method which has a much higher constant factor on the `O(D^2)` scaling.\r\n_Edit 26/05/18: This effect turned out to be an implementation issue, where outer products where taken in a for-loop instead of using [`bmm`](https://pytorch.org/docs/master/torch.html#torch.bmm)_\r\n\r\n---\r\n\r\nThis however is not a limitation of the idea, only the current implementation using multiplication with those huge tensors. In every one of those multiplication, one of the dimension is useless - in that only a single line of each dimension is nnz.\r\n\r\nI think a more careful implementation could scale - for example by only computing product for nnz lines or resizing \"on the fly\" instead of storing those huge matrices.\r\n\r\n@t-vi I only now realise (through your [forum post](https://discuss.pytorch.org/t/efficient-computation-of-per-sample-examples/18587/2)) that your [earlier post](https://github.com/pytorch/pytorch/issues/7786#issuecomment-391327871) was using the same trick. I will try to play around with the re-definition of backward for those ops and see if I can make this work in the next few days (though feel free to ping in if the solution seems obvious :)\r\n\r\nThanks again!\r\n"}