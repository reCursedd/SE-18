{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391612473", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391612473", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391612473, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTYxMjQ3Mw==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-24T07:10:26Z", "updated_at": "2018-05-24T07:10:26Z", "author_association": "NONE", "body_html": "<p>I have found <em>something</em> that gives me hope.</p>\n<p>By using (probably abusing) the <code>grad_outputs=None</code> arguments of <code>torch.autograd.grad</code> and passing a <code>torch.eye(N).view(N,N,1)</code>, I am able to get individual gradients when using <code>torch.stack</code> on simple expressions.<br>\nFrom my limited understanding, using this 3D tensor as a starting point makes the \"accumulation\" of the different gradients happen in different dimensions.</p>\n<p><a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/proof_of_concept.py\">Runnable example</a></p>\n<pre><code>########################################\n# Proof of concept - \"Working\", somewhat\n\nx1 = torch.tensor([1,1,1], requires_grad=False).float()\nx2 = torch.tensor([2,2,2], requires_grad=False).float()\nw = torch.tensor([0,0,0], requires_grad=True).float()\n\nf1 = torch.dot(x1, w)\nf2 = torch.dot(x2, w)\nstacked = torch.stack((f1, f2))\n\ngrads, = torch.autograd.grad(stacked, w, torch.eye(2).view(2,2,1), retain_graph=True)\n\n####\n# grads contains tensor([[ 1.,  1.,  1.], [ 2.,  2.,  2.]])\n# which is the desired output, [\u2207w f1, \u2207w f2]\n</code></pre>\n<p>However, this breaks down the backward pass on more involved graphs.<br>\nI am probably messing with the intended structure of the matrix multiplications in the backward pass a bit too much.</p>\n<pre><code>import torch.nn.functional as F\n\nf1 = F.sigmoid(torch.dot(x1, w))\nf2 = F.sigmoid(torch.dot(x2, w))\nstacked = torch.stack((f1, f2))\n\ngrads, = torch.autograd.grad(stacked, w, torch.eye(2).view(2,2,1), retain_graph=True)\n\n####\n# output and gradOutput have different number of elements: \n# output[1] has 1 elements, while gradOutput[2 x 1] has 2 elements \n# at c:\\programdata\\miniconda3\\conda-bld\\pytorch-cpu_[...]\n# \\work\\aten\\src\\thnn\\generic/Sigmoid.c:19\n</code></pre>\n<p>If someone could look at this idea and perhaps give me a few pointer to better understand what is happening under the hood, this would be very much appreciated.</p>\n<hr>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> I did open a <a href=\"https://discuss.pytorch.org/t/efficient-computation-of-per-sample-examples/18587\" rel=\"nofollow\">forum thread</a></p>", "body_text": "I have found something that gives me hope.\nBy using (probably abusing) the grad_outputs=None arguments of torch.autograd.grad and passing a torch.eye(N).view(N,N,1), I am able to get individual gradients when using torch.stack on simple expressions.\nFrom my limited understanding, using this 3D tensor as a starting point makes the \"accumulation\" of the different gradients happen in different dimensions.\nRunnable example\n########################################\n# Proof of concept - \"Working\", somewhat\n\nx1 = torch.tensor([1,1,1], requires_grad=False).float()\nx2 = torch.tensor([2,2,2], requires_grad=False).float()\nw = torch.tensor([0,0,0], requires_grad=True).float()\n\nf1 = torch.dot(x1, w)\nf2 = torch.dot(x2, w)\nstacked = torch.stack((f1, f2))\n\ngrads, = torch.autograd.grad(stacked, w, torch.eye(2).view(2,2,1), retain_graph=True)\n\n####\n# grads contains tensor([[ 1.,  1.,  1.], [ 2.,  2.,  2.]])\n# which is the desired output, [\u2207w f1, \u2207w f2]\n\nHowever, this breaks down the backward pass on more involved graphs.\nI am probably messing with the intended structure of the matrix multiplications in the backward pass a bit too much.\nimport torch.nn.functional as F\n\nf1 = F.sigmoid(torch.dot(x1, w))\nf2 = F.sigmoid(torch.dot(x2, w))\nstacked = torch.stack((f1, f2))\n\ngrads, = torch.autograd.grad(stacked, w, torch.eye(2).view(2,2,1), retain_graph=True)\n\n####\n# output and gradOutput have different number of elements: \n# output[1] has 1 elements, while gradOutput[2 x 1] has 2 elements \n# at c:\\programdata\\miniconda3\\conda-bld\\pytorch-cpu_[...]\n# \\work\\aten\\src\\thnn\\generic/Sigmoid.c:19\n\nIf someone could look at this idea and perhaps give me a few pointer to better understand what is happening under the hood, this would be very much appreciated.\n\n@t-vi I did open a forum thread", "body": "I have found _something_ that gives me hope.\r\n\r\nBy using (probably abusing) the `grad_outputs=None` arguments of `torch.autograd.grad` and passing a `torch.eye(N).view(N,N,1)`, I am able to get individual gradients when using `torch.stack` on simple expressions.\r\nFrom my limited understanding, using this 3D tensor as a starting point makes the \"accumulation\" of the different gradients happen in different dimensions.\r\n\r\n[Runnable example](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/proof_of_concept.py)\r\n\r\n```\r\n########################################\r\n# Proof of concept - \"Working\", somewhat\r\n\r\nx1 = torch.tensor([1,1,1], requires_grad=False).float()\r\nx2 = torch.tensor([2,2,2], requires_grad=False).float()\r\nw = torch.tensor([0,0,0], requires_grad=True).float()\r\n\r\nf1 = torch.dot(x1, w)\r\nf2 = torch.dot(x2, w)\r\nstacked = torch.stack((f1, f2))\r\n\r\ngrads, = torch.autograd.grad(stacked, w, torch.eye(2).view(2,2,1), retain_graph=True)\r\n\r\n####\r\n# grads contains tensor([[ 1.,  1.,  1.], [ 2.,  2.,  2.]])\r\n# which is the desired output, [\u2207w f1, \u2207w f2]\r\n```\r\n\r\nHowever, this breaks down the backward pass on more involved graphs.\r\nI am probably messing with the intended structure of the matrix multiplications in the backward pass a bit too much.\r\n\r\n```\r\nimport torch.nn.functional as F\r\n\r\nf1 = F.sigmoid(torch.dot(x1, w))\r\nf2 = F.sigmoid(torch.dot(x2, w))\r\nstacked = torch.stack((f1, f2))\r\n\r\ngrads, = torch.autograd.grad(stacked, w, torch.eye(2).view(2,2,1), retain_graph=True)\r\n\r\n####\r\n# output and gradOutput have different number of elements: \r\n# output[1] has 1 elements, while gradOutput[2 x 1] has 2 elements \r\n# at c:\\programdata\\miniconda3\\conda-bld\\pytorch-cpu_[...]\r\n# \\work\\aten\\src\\thnn\\generic/Sigmoid.c:19\r\n```\r\n\r\nIf someone could look at this idea and perhaps give me a few pointer to better understand what is happening under the hood, this would be very much appreciated.\r\n\r\n---\r\n\r\n@t-vi I did open a [forum thread](https://discuss.pytorch.org/t/efficient-computation-of-per-sample-examples/18587)\r\n"}