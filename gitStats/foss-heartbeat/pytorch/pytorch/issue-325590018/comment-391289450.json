{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391289450", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391289450", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391289450, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTI4OTQ1MA==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T09:50:58Z", "updated_at": "2018-05-23T09:50:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> Thanks for your suggestion!</p>\n<blockquote>\n<p>Let <code>f1x = f_1(x)</code>, and so forth. Does <code>torch.autograd.grad((f1x, ..., fnx), (x,))</code> compute the information you seek?</p>\n</blockquote>\n<p>It would have been lovely, but sadly no.<br>\nThe documentation from <code>torch.autograd.grad(outputs, ...)</code> mentions</p>\n<blockquote>\n<p>Computes and returns the sum of gradients of outputs w.r.t. the inputs.<br>\n...</p>\n</blockquote>\n<p>I need the gradient of each element of the sum, not the sum of the gradient of each element.</p>\n<p>I <a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/suggestions.py\">double-checked</a>, to see if it was a documentation problem, and I only get one gradient out of the <code>.grad</code> call, not all of them.</p>", "body_text": "@ezyang Thanks for your suggestion!\n\nLet f1x = f_1(x), and so forth. Does torch.autograd.grad((f1x, ..., fnx), (x,)) compute the information you seek?\n\nIt would have been lovely, but sadly no.\nThe documentation from torch.autograd.grad(outputs, ...) mentions\n\nComputes and returns the sum of gradients of outputs w.r.t. the inputs.\n...\n\nI need the gradient of each element of the sum, not the sum of the gradient of each element.\nI double-checked, to see if it was a documentation problem, and I only get one gradient out of the .grad call, not all of them.", "body": "@ezyang Thanks for your suggestion!\r\n\r\n> Let `f1x = f_1(x)`, and so forth. Does `torch.autograd.grad((f1x, ..., fnx), (x,))` compute the information you seek?\r\n\r\nIt would have been lovely, but sadly no.\r\nThe documentation from `torch.autograd.grad(outputs, ...)` mentions \r\n> Computes and returns the sum of gradients of outputs w.r.t. the inputs.\r\n> ...\r\n\r\nI need the gradient of each element of the sum, not the sum of the gradient of each element.\r\n\r\nI [double-checked](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/suggestions.py), to see if it was a documentation problem, and I only get one gradient out of the `.grad` call, not all of them.\r\n"}