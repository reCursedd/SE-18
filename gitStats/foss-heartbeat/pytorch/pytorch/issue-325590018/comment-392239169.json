{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392239169", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-392239169", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 392239169, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjIzOTE2OQ==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-26T05:53:27Z", "updated_at": "2018-05-26T12:58:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8343010\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Mercurial1101\">@Mercurial1101</a> I'm glad this thread is relevant to other people!</p>\n<p>I have some thoughts about what might be happening, but they might very well be wrong.<br>\nMy todolist includes revisiting this approach, I might have a better answer after more digging but here are some ideas if you want to try something now.</p>\n<hr>\n<p>My understanding of AD is poor and the implementation of Goodfellow's trick might be the issue, or the adaptation of the trick to retrieve individual gradients, not the trick in itself.</p>\n<p>The derivation in <a href=\"https://arxiv.org/pdf/1510.01799.pdf\" rel=\"nofollow\">his note</a> is concerned with the specific case of retrieving the L2 norm of the gradient, and it is possible that the intermediate gradients retrieved by the backward pass are optimized for this use. It is possible this information to reconstruct the individual gradients, but it involves re-doing part of the backward pass, which is what is done in the file <a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/goodfellow_backprop.py\"><code>goodfellow_backprop.py</code></a>.</p>\n<p>The backward pass is supposed to scale in <code>O(P^2)</code>, where <code>P</code> is the number of node in the hidden layers, since it involves <code>P x P</code> matrix multiplications, but the way it is done might put a higher-than-necessary constant in front.</p>\n<p>Possible not-mutually-exclusive reasons:</p>\n<ul>\n<li>The current code might be doing a very bad job at matrix multiplication by doing things serially in the for loop instead of using a bigger matrix operation that would leverage parallelization.<br>\n<em>Edit 26/05/18: Yep. This was the issue. I rewrote the procedure using <a href=\"https://pytorch.org/docs/master/torch.html#torch.bmm\" rel=\"nofollow\"><code>Batch Matrix-Multiply (bmm)</code></a>, it makes it less attrocious when the width of the hidden layer increases.</em></li>\n<li>The intermediate gradients being asked might be a good starting point to construct the L2 norm of the gradient, instead of the gradient; asking for the right starting matrix might be the issue.<br>\n<em>Edit 26/05/18: This is wrong, the problem was in code.</em></li>\n<li>The method still adds overhead in that it needs to retrieve and store 2 big matrices per layer and multiply them, which has \"already been done\" by the backward pass, only with a reduction step over the minibatch dimension.<br>\n<em>Edit 26/05/18: This still holds, and an approach based on rewriting backward operations is still better, but is marginal - the difference is not in order of magnitude</em></li>\n</ul>", "body_text": "@Mercurial1101 I'm glad this thread is relevant to other people!\nI have some thoughts about what might be happening, but they might very well be wrong.\nMy todolist includes revisiting this approach, I might have a better answer after more digging but here are some ideas if you want to try something now.\n\nMy understanding of AD is poor and the implementation of Goodfellow's trick might be the issue, or the adaptation of the trick to retrieve individual gradients, not the trick in itself.\nThe derivation in his note is concerned with the specific case of retrieving the L2 norm of the gradient, and it is possible that the intermediate gradients retrieved by the backward pass are optimized for this use. It is possible this information to reconstruct the individual gradients, but it involves re-doing part of the backward pass, which is what is done in the file goodfellow_backprop.py.\nThe backward pass is supposed to scale in O(P^2), where P is the number of node in the hidden layers, since it involves P x P matrix multiplications, but the way it is done might put a higher-than-necessary constant in front.\nPossible not-mutually-exclusive reasons:\n\nThe current code might be doing a very bad job at matrix multiplication by doing things serially in the for loop instead of using a bigger matrix operation that would leverage parallelization.\nEdit 26/05/18: Yep. This was the issue. I rewrote the procedure using Batch Matrix-Multiply (bmm), it makes it less attrocious when the width of the hidden layer increases.\nThe intermediate gradients being asked might be a good starting point to construct the L2 norm of the gradient, instead of the gradient; asking for the right starting matrix might be the issue.\nEdit 26/05/18: This is wrong, the problem was in code.\nThe method still adds overhead in that it needs to retrieve and store 2 big matrices per layer and multiply them, which has \"already been done\" by the backward pass, only with a reduction step over the minibatch dimension.\nEdit 26/05/18: This still holds, and an approach based on rewriting backward operations is still better, but is marginal - the difference is not in order of magnitude", "body": "@Mercurial1101 I'm glad this thread is relevant to other people!\r\n\r\nI have some thoughts about what might be happening, but they might very well be wrong. \r\nMy todolist includes revisiting this approach, I might have a better answer after more digging but here are some ideas if you want to try something now.\r\n\r\n---\r\n\r\nMy understanding of AD is poor and the implementation of Goodfellow's trick might be the issue, or the adaptation of the trick to retrieve individual gradients, not the trick in itself.\r\n\r\nThe derivation in [his note](https://arxiv.org/pdf/1510.01799.pdf) is concerned with the specific case of retrieving the L2 norm of the gradient, and it is possible that the intermediate gradients retrieved by the backward pass are optimized for this use. It is possible this information to reconstruct the individual gradients, but it involves re-doing part of the backward pass, which is what is done in the file [`goodfellow_backprop.py`](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/blob/master/pytorch/goodfellow_backprop.py).\r\n\r\nThe backward pass is supposed to scale in `O(P^2)`, where `P` is the number of node in the hidden layers, since it involves `P x P` matrix multiplications, but the way it is done might put a higher-than-necessary constant in front.\r\n\r\nPossible not-mutually-exclusive reasons:\r\n* The current code might be doing a very bad job at matrix multiplication by doing things serially in the for loop instead of using a bigger matrix operation that would leverage parallelization.\r\n  _Edit 26/05/18: Yep. This was the issue. I rewrote the procedure using [`Batch Matrix-Multiply (bmm)`](https://pytorch.org/docs/master/torch.html#torch.bmm), it makes it less attrocious when the width of the hidden layer increases._\r\n* The intermediate gradients being asked might be a good starting point to construct the L2 norm of the gradient, instead of the gradient; asking for the right starting matrix might be the issue.\r\n_Edit 26/05/18: This is wrong, the problem was in code._\r\n* The method still adds overhead in that it needs to retrieve and store 2 big matrices per layer and multiply them, which has \"already been done\" by the backward pass, only with a reduction step over the minibatch dimension.\r\n_Edit 26/05/18: This still holds, and an approach based on rewriting backward operations is still better, but is marginal - the difference is not in order of magnitude_\r\n\r\n\r\n"}