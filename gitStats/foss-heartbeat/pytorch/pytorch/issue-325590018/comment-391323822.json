{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391323822", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391323822", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391323822, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTMyMzgyMg==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T12:10:48Z", "updated_at": "2018-05-23T12:10:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Thanks for the detailled answer.</p>\n<blockquote>\n<p>I don't think that it's possible to reduce the complexity of the standard AD algorithms from being linear in the number of different differentiated functions [...]</p>\n</blockquote>\n<p>I wouldn't expect it to be better than linear either - the issue is not in terms of asymptotical computational complexity but in the overhead involved in doing the aggregation by repeatedly calling backward vs. doing it in one batch in C.</p>\n<p>I'd bet this is the reason why <code>multi</code> performs better than <code>naive</code>.<br>\nFrom my (admittedly limited) understanding of the inner workings, the batch backward computation to leverage vectorization would be ideal.<br>\nEven a 2-5x slower than \"computing gradients for the sum\" implementation would already be a big leap from the 50-400x slowdown experienced using those neat tricks.</p>\n<blockquote>\n<p>but this would require some pretty significant changes [and] unfortunately I don't think we'll be going down this path anytime soon.</p>\n</blockquote>\n<p>I am a bit sad to read that :(. Trying to work on some algorithms leveraging more information from each batch, I really hoped that I could make my experiments run in minutes instead of hours/hours instead of days :|</p>\n<p>While I do not have a strong background in C, could you point me to the file/function/region of the codebase handling the accumulation of the gradients? I don't have hopes of actually implementing it myself, but I am curious as to how this is done (and if maybe I can hack this instead).</p>\n<p>As a <em>bad</em> workaround, do you know if this is a supported operations in other Autodifferentiation libraries with GPU support?</p>\n<p>Thank you very much for your time.</p>", "body_text": "@apaszke Thanks for the detailled answer.\n\nI don't think that it's possible to reduce the complexity of the standard AD algorithms from being linear in the number of different differentiated functions [...]\n\nI wouldn't expect it to be better than linear either - the issue is not in terms of asymptotical computational complexity but in the overhead involved in doing the aggregation by repeatedly calling backward vs. doing it in one batch in C.\nI'd bet this is the reason why multi performs better than naive.\nFrom my (admittedly limited) understanding of the inner workings, the batch backward computation to leverage vectorization would be ideal.\nEven a 2-5x slower than \"computing gradients for the sum\" implementation would already be a big leap from the 50-400x slowdown experienced using those neat tricks.\n\nbut this would require some pretty significant changes [and] unfortunately I don't think we'll be going down this path anytime soon.\n\nI am a bit sad to read that :(. Trying to work on some algorithms leveraging more information from each batch, I really hoped that I could make my experiments run in minutes instead of hours/hours instead of days :|\nWhile I do not have a strong background in C, could you point me to the file/function/region of the codebase handling the accumulation of the gradients? I don't have hopes of actually implementing it myself, but I am curious as to how this is done (and if maybe I can hack this instead).\nAs a bad workaround, do you know if this is a supported operations in other Autodifferentiation libraries with GPU support?\nThank you very much for your time.", "body": "@apaszke Thanks for the detailled answer.\r\n\r\n> I don't think that it's possible to reduce the complexity of the standard AD algorithms from being linear in the number of different differentiated functions [...]\r\n\r\nI wouldn't expect it to be better than linear either - the issue is not in terms of asymptotical computational complexity but in the overhead involved in doing the aggregation by repeatedly calling backward vs. doing it in one batch in C.\r\n\r\nI'd bet this is the reason why `multi` performs better than `naive`.\r\nFrom my (admittedly limited) understanding of the inner workings, the batch backward computation to leverage vectorization would be ideal.\r\nEven a 2-5x slower than \"computing gradients for the sum\" implementation would already be a big leap from the 50-400x slowdown experienced using those neat tricks.\r\n\r\n> but this would require some pretty significant changes [and] unfortunately I don't think we'll be going down this path anytime soon.\r\n\r\nI am a bit sad to read that :(. Trying to work on some algorithms leveraging more information from each batch, I really hoped that I could make my experiments run in minutes instead of hours/hours instead of days :|\r\n\r\nWhile I do not have a strong background in C, could you point me to the file/function/region of the codebase handling the accumulation of the gradients? I don't have hopes of actually implementing it myself, but I am curious as to how this is done (and if maybe I can hack this instead).\r\n\r\nAs a _bad_ workaround, do you know if this is a supported operations in other Autodifferentiation libraries with GPU support?\r\n\r\nThank you very much for your time.\r\n\r\n\r\n\r\n\r\n\r\n"}