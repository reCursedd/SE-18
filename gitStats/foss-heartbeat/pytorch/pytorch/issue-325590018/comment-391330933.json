{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391330933", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391330933", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391330933, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTMzMDkzMw==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T12:37:53Z", "updated_at": "2018-05-26T13:15:53Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> Thank you for your answer.</p>\n<p>If I understand your recommendation correctly, it is very similar to <a href=\"https://arxiv.org/abs/1510.01799\" rel=\"nofollow\">Goodfellow's derivation</a> - although my implementation is explicit in asking the gradient for each layer using <code>torch.autograd.grad</code> instead of using hooks.</p>\n<p>The implementation is still very slow when compared to the computation of the sum of the gradients done by calling <code>grad</code> only once - on the order of x50 to x100 slower for a minibatch of size 1000 in my experiments.<br>\nAlso, assuming the computational complexity scales quadratically in the dimensionality of the network (I might be wrong on that point), doing that part of the differentiation \"by hand\" in python scales very badly to bigger problems.<br>\n<em>Edit 26/05/18: This turned out to be an implementation issue, in doing things serially by computing outer products in a for-loop instead of using <a href=\"https://pytorch.org/docs/master/torch.html#torch.bmm\" rel=\"nofollow\"><code>bmm</code></a></em></p>\n<p>My hope was to find a way to leverage the fast implementation.</p>\n<p>Do you believe using gradient hooks on the cost function w.r.t. the original parameters would be faster than asking for the gradient of the layers directly?<br>\nEven if only 2x faster, that would still be a useful improvement.</p>", "body_text": "@t-vi Thank you for your answer.\nIf I understand your recommendation correctly, it is very similar to Goodfellow's derivation - although my implementation is explicit in asking the gradient for each layer using torch.autograd.grad instead of using hooks.\nThe implementation is still very slow when compared to the computation of the sum of the gradients done by calling grad only once - on the order of x50 to x100 slower for a minibatch of size 1000 in my experiments.\nAlso, assuming the computational complexity scales quadratically in the dimensionality of the network (I might be wrong on that point), doing that part of the differentiation \"by hand\" in python scales very badly to bigger problems.\nEdit 26/05/18: This turned out to be an implementation issue, in doing things serially by computing outer products in a for-loop instead of using bmm\nMy hope was to find a way to leverage the fast implementation.\nDo you believe using gradient hooks on the cost function w.r.t. the original parameters would be faster than asking for the gradient of the layers directly?\nEven if only 2x faster, that would still be a useful improvement.", "body": "@t-vi Thank you for your answer.\r\n\r\nIf I understand your recommendation correctly, it is very similar to [Goodfellow's derivation](https://arxiv.org/abs/1510.01799) - although my implementation is explicit in asking the gradient for each layer using `torch.autograd.grad` instead of using hooks.\r\n\r\nThe implementation is still very slow when compared to the computation of the sum of the gradients done by calling `grad` only once - on the order of x50 to x100 slower for a minibatch of size 1000 in my experiments.\r\nAlso, assuming the computational complexity scales quadratically in the dimensionality of the network (I might be wrong on that point), doing that part of the differentiation \"by hand\" in python scales very badly to bigger problems.\r\n_Edit 26/05/18: This turned out to be an implementation issue, in doing things serially by computing outer products in a for-loop instead of using [`bmm`](https://pytorch.org/docs/master/torch.html#torch.bmm)_\r\n\r\nMy hope was to find a way to leverage the fast implementation.\r\n\r\nDo you believe using gradient hooks on the cost function w.r.t. the original parameters would be faster than asking for the gradient of the layers directly?\r\nEven if only 2x faster, that would still be a useful improvement.\r\n\r\n"}