{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391637797", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391637797", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391637797, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTYzNzc5Nw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-24T08:46:03Z", "updated_at": "2018-05-24T08:46:03Z", "author_association": "MEMBER", "body_html": "<p>Yes, what you did will work for some simple ops, but really is an implementation detail and is an overuse of the system.</p>\n<p>Provided you don't use too many operations as part of your model there is a relatively simple workaround that would let you achieve perfect batching, although you'll need to reimplement the derivative for all the ops that you need, and use them instead of the library ones (<code>nn</code> won't work either). Here's an example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> These custom ops break the usual assumption that gradients given to</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> backward have the same shapes as outputs. They are expected to have</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> an extra leading dimension, which batches independent reverse mode</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> passes.</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">BatchedReverseMM</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n        ctx.save_for_backward(x, y)\n        <span class=\"pl-k\">return</span> torch.matmul(x, y)\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">batched_grad</span>):\n        x, y <span class=\"pl-k\">=</span> ctx.saved_variables\n        <span class=\"pl-k\">return</span> torch.matmul(batched_grad, y.t()), torch.matmul(x.t(), batched_grad)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">BatchedReLU</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>):\n        ctx.mask <span class=\"pl-k\">=</span> x <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">return</span> torch.relu(x)\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">batched_grad</span>):\n        <span class=\"pl-k\">return</span> batched_grad <span class=\"pl-k\">*</span> ctx.mask.type_as(batched_grad).expand_as(batched_grad)\n\nmatmul <span class=\"pl-k\">=</span> BatchedReverseMM.apply\nrelu <span class=\"pl-k\">=</span> BatchedReLU.apply\n\nbatch_size, input_size, output_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>\nx <span class=\"pl-k\">=</span> torch.randn(batch_size, input_size)\nW <span class=\"pl-k\">=</span> torch.randn(input_size, output_size, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">NOTE</span>: need to use the custom ops here</span>\noutput <span class=\"pl-k\">=</span> relu(matmul(x, W))\njac_elems <span class=\"pl-k\">=</span> output_size <span class=\"pl-k\">*</span> batch_size <span class=\"pl-c\"><span class=\"pl-c\">#</span> it's really the size of one dim of the jacobian</span>\nbatch_grad_output <span class=\"pl-k\">=</span> torch.eye(jac_elems, jac_elems).view(jac_elems, <span class=\"pl-k\">*</span>output.shape)\n\njacobian, <span class=\"pl-k\">=</span> torch.autograd.grad(output, W, batch_grad_output)\n<span class=\"pl-c1\">print</span>(jacobian.shape)</pre></div>", "body_text": "Yes, what you did will work for some simple ops, but really is an implementation detail and is an overuse of the system.\nProvided you don't use too many operations as part of your model there is a relatively simple workaround that would let you achieve perfect batching, although you'll need to reimplement the derivative for all the ops that you need, and use them instead of the library ones (nn won't work either). Here's an example:\nimport torch\n\n# These custom ops break the usual assumption that gradients given to\n# backward have the same shapes as outputs. They are expected to have\n# an extra leading dimension, which batches independent reverse mode\n# passes.\nclass BatchedReverseMM(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, y):\n        ctx.save_for_backward(x, y)\n        return torch.matmul(x, y)\n\n    @staticmethod\n    def backward(ctx, batched_grad):\n        x, y = ctx.saved_variables\n        return torch.matmul(batched_grad, y.t()), torch.matmul(x.t(), batched_grad)\n\nclass BatchedReLU(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.mask = x >= 0\n        return torch.relu(x)\n\n    @staticmethod\n    def backward(ctx, batched_grad):\n        return batched_grad * ctx.mask.type_as(batched_grad).expand_as(batched_grad)\n\nmatmul = BatchedReverseMM.apply\nrelu = BatchedReLU.apply\n\nbatch_size, input_size, output_size = 20, 100, 10\nx = torch.randn(batch_size, input_size)\nW = torch.randn(input_size, output_size, requires_grad=True)\n\n# NOTE: need to use the custom ops here\noutput = relu(matmul(x, W))\njac_elems = output_size * batch_size # it's really the size of one dim of the jacobian\nbatch_grad_output = torch.eye(jac_elems, jac_elems).view(jac_elems, *output.shape)\n\njacobian, = torch.autograd.grad(output, W, batch_grad_output)\nprint(jacobian.shape)", "body": "Yes, what you did will work for some simple ops, but really is an implementation detail and is an overuse of the system.\r\n\r\nProvided you don't use too many operations as part of your model there is a relatively simple workaround that would let you achieve perfect batching, although you'll need to reimplement the derivative for all the ops that you need, and use them instead of the library ones (`nn` won't work either). Here's an example:\r\n\r\n```py\r\nimport torch\r\n\r\n# These custom ops break the usual assumption that gradients given to\r\n# backward have the same shapes as outputs. They are expected to have\r\n# an extra leading dimension, which batches independent reverse mode\r\n# passes.\r\nclass BatchedReverseMM(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x, y):\r\n        ctx.save_for_backward(x, y)\r\n        return torch.matmul(x, y)\r\n\r\n    @staticmethod\r\n    def backward(ctx, batched_grad):\r\n        x, y = ctx.saved_variables\r\n        return torch.matmul(batched_grad, y.t()), torch.matmul(x.t(), batched_grad)\r\n\r\nclass BatchedReLU(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        ctx.mask = x >= 0\r\n        return torch.relu(x)\r\n\r\n    @staticmethod\r\n    def backward(ctx, batched_grad):\r\n        return batched_grad * ctx.mask.type_as(batched_grad).expand_as(batched_grad)\r\n\r\nmatmul = BatchedReverseMM.apply\r\nrelu = BatchedReLU.apply\r\n\r\nbatch_size, input_size, output_size = 20, 100, 10\r\nx = torch.randn(batch_size, input_size)\r\nW = torch.randn(input_size, output_size, requires_grad=True)\r\n\r\n# NOTE: need to use the custom ops here\r\noutput = relu(matmul(x, W))\r\njac_elems = output_size * batch_size # it's really the size of one dim of the jacobian\r\nbatch_grad_output = torch.eye(jac_elems, jac_elems).view(jac_elems, *output.shape)\r\n\r\njacobian, = torch.autograd.grad(output, W, batch_grad_output)\r\nprint(jacobian.shape)\r\n```"}