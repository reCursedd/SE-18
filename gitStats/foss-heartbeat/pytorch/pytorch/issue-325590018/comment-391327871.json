{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391327871", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391327871", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391327871, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTMyNzg3MQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T12:26:42Z", "updated_at": "2018-05-23T12:26:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Really, if you intercept the grad output of a a layer (with backward hook or retain_grad), you get AD \"90% of the way\" for free (except that you retain the grad).<br>\nNow the last step to the individual parameter is more difficult. It's elementary and relatively easy to figure out for linear layers (say in = b x m, weight = m x n, out = b x n, usually grad_weight = in.t() @ out, now you do grad_weight_per_sample = in.unsqueeze(1) * grad_out.unsqueeze(2)). However, for other layers, in particular convolutional layers, that does not work as easily (you could fall back to iterating if you are able to call the backward functions, which you cannot in PyTorch in general).</p>", "body_text": "Really, if you intercept the grad output of a a layer (with backward hook or retain_grad), you get AD \"90% of the way\" for free (except that you retain the grad).\nNow the last step to the individual parameter is more difficult. It's elementary and relatively easy to figure out for linear layers (say in = b x m, weight = m x n, out = b x n, usually grad_weight = in.t() @ out, now you do grad_weight_per_sample = in.unsqueeze(1) * grad_out.unsqueeze(2)). However, for other layers, in particular convolutional layers, that does not work as easily (you could fall back to iterating if you are able to call the backward functions, which you cannot in PyTorch in general).", "body": "Really, if you intercept the grad output of a a layer (with backward hook or retain_grad), you get AD \"90% of the way\" for free (except that you retain the grad).\r\nNow the last step to the individual parameter is more difficult. It's elementary and relatively easy to figure out for linear layers (say in = b x m, weight = m x n, out = b x n, usually grad_weight = in.t() @ out, now you do grad_weight_per_sample = in.unsqueeze(1) * grad_out.unsqueeze(2)). However, for other layers, in particular convolutional layers, that does not work as easily (you could fall back to iterating if you are able to call the backward functions, which you cannot in PyTorch in general).\r\n"}