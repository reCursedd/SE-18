{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392731091", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-392731091", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 392731091, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjczMTA5MQ==", "user": {"login": "fKunstner", "id": 8789455, "node_id": "MDQ6VXNlcjg3ODk0NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8789455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fKunstner", "html_url": "https://github.com/fKunstner", "followers_url": "https://api.github.com/users/fKunstner/followers", "following_url": "https://api.github.com/users/fKunstner/following{/other_user}", "gists_url": "https://api.github.com/users/fKunstner/gists{/gist_id}", "starred_url": "https://api.github.com/users/fKunstner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fKunstner/subscriptions", "organizations_url": "https://api.github.com/users/fKunstner/orgs", "repos_url": "https://api.github.com/users/fKunstner/repos", "events_url": "https://api.github.com/users/fKunstner/events{/privacy}", "received_events_url": "https://api.github.com/users/fKunstner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T10:35:06Z", "updated_at": "2018-05-29T10:35:06Z", "author_association": "NONE", "body_html": "<p>Conclusion:</p>\n<ul>\n<li>If you use <code>bmm</code> instead of computing outer products in a for loop, Goodfellow's trick actually performs well.</li>\n<li>It turns out to be the same as what <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> proposed <a href=\"https://github.com/pytorch/pytorch/issues/7786#issuecomment-391327871\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7786/hovercard\">here</a> and is equivalent to redefining the derivative of the linear model, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> showed how to do <a href=\"https://github.com/pytorch/pytorch/issues/7786#issuecomment-391637797\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7786/hovercard\">here</a> - though doing this outside of the backward pass code might be a bit cleaner.</li>\n</ul>\n<p>For a reasonably big model (50 layers, 300 hidden unit/layer) \u00e4nd minibatch size (~100), it performs only ~10x worse than computing the full gradient, and ~10x better than computing individual gradients by repeatedly calling <code>backward</code>.</p>\n<p>I have some numbers/setups <a href=\"https://raw.githubusercontent.com/fKunstner/fast-individual-gradients-with-autodiff/master/pytorch/results.txt\" rel=\"nofollow\">here</a> and the code to reproduce those results on your machine/setup if you wish to do so is <a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff/tree/master/pytorch\">here</a>, with a (hopefully correct now) version of goodfellow's trick.</p>\n<p>I also tried to explain how the trick works in the <a href=\"https://github.com/fKunstner/fast-individual-gradients-with-autodiff\">readme</a>, but I don't think I did a very good job. Hope it's still useful to other people who end up in this situation in the future though.</p>\n<hr>\n<p>Many thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a>!<br>\nI really appreciated the helping hand and learnt a lot about AD in the past few days :D<br>\nI'm sorry I took your time on something that eventually ended up being an implementation mistake on my part.</p>", "body_text": "Conclusion:\n\nIf you use bmm instead of computing outer products in a for loop, Goodfellow's trick actually performs well.\nIt turns out to be the same as what @t-vi proposed here and is equivalent to redefining the derivative of the linear model, as @apaszke showed how to do here - though doing this outside of the backward pass code might be a bit cleaner.\n\nFor a reasonably big model (50 layers, 300 hidden unit/layer) \u00e4nd minibatch size (~100), it performs only ~10x worse than computing the full gradient, and ~10x better than computing individual gradients by repeatedly calling backward.\nI have some numbers/setups here and the code to reproduce those results on your machine/setup if you wish to do so is here, with a (hopefully correct now) version of goodfellow's trick.\nI also tried to explain how the trick works in the readme, but I don't think I did a very good job. Hope it's still useful to other people who end up in this situation in the future though.\n\nMany thanks to @apaszke and @t-vi!\nI really appreciated the helping hand and learnt a lot about AD in the past few days :D\nI'm sorry I took your time on something that eventually ended up being an implementation mistake on my part.", "body": "Conclusion:\r\n* If you use `bmm` instead of computing outer products in a for loop, Goodfellow's trick actually performs well.\r\n* It turns out to be the same as what @t-vi proposed [here](https://github.com/pytorch/pytorch/issues/7786#issuecomment-391327871) and is equivalent to redefining the derivative of the linear model, as @apaszke showed how to do [here](https://github.com/pytorch/pytorch/issues/7786#issuecomment-391637797) - though doing this outside of the backward pass code might be a bit cleaner.\r\n\r\nFor a reasonably big model (50 layers, 300 hidden unit/layer) \u00e4nd minibatch size (~100), it performs only ~10x worse than computing the full gradient, and ~10x better than computing individual gradients by repeatedly calling `backward`. \r\n\r\nI have some numbers/setups [here](https://raw.githubusercontent.com/fKunstner/fast-individual-gradients-with-autodiff/master/pytorch/results.txt) and the code to reproduce those results on your machine/setup if you wish to do so is [here](https://github.com/fKunstner/fast-individual-gradients-with-autodiff/tree/master/pytorch), with a (hopefully correct now) version of goodfellow's trick.\r\n\r\nI also tried to explain how the trick works in the [readme](https://github.com/fKunstner/fast-individual-gradients-with-autodiff), but I don't think I did a very good job. Hope it's still useful to other people who end up in this situation in the future though.\r\n\r\n---\r\n\r\nMany thanks to @apaszke and @t-vi!\r\nI really appreciated the helping hand and learnt a lot about AD in the past few days :D \r\nI'm sorry I took your time on something that eventually ended up being an implementation mistake on my part. \r\n\r\n"}