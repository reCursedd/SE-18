{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391308732", "html_url": "https://github.com/pytorch/pytorch/issues/7786#issuecomment-391308732", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7786", "id": 391308732, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTMwODczMg==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T11:08:26Z", "updated_at": "2018-05-23T11:08:26Z", "author_association": "MEMBER", "body_html": "<p>Thanks for a detailed writeup!</p>\n<p>Yes, this is currently impossible with AD techniques that we use in PyTorch. In general, I don't think that it's possible to reduce the complexity of the standard AD algorithms from being linear in the number of different differentiated functions (so in <code>N = 1000</code> case, I would really expect it to be 1000x slower than a single backward pass, as the naive example shows). They are not doing the same operations - that's the reason why backprop is so efficient. The only thing we can do would be to batch the backward computation to improve vectorization (especially important in case of GPUs), but this would require some pretty significant changes in our AD backend. While I already heard some requests like that, unfortunately I don't think we'll be going down this path anytime soon.</p>\n<p>I actually have no idea why does the <code>multi</code> approach work any better than the naive one, except that it doesn't return to Python after computing every single gradient, especially that the naive one should batch the operations better.</p>", "body_text": "Thanks for a detailed writeup!\nYes, this is currently impossible with AD techniques that we use in PyTorch. In general, I don't think that it's possible to reduce the complexity of the standard AD algorithms from being linear in the number of different differentiated functions (so in N = 1000 case, I would really expect it to be 1000x slower than a single backward pass, as the naive example shows). They are not doing the same operations - that's the reason why backprop is so efficient. The only thing we can do would be to batch the backward computation to improve vectorization (especially important in case of GPUs), but this would require some pretty significant changes in our AD backend. While I already heard some requests like that, unfortunately I don't think we'll be going down this path anytime soon.\nI actually have no idea why does the multi approach work any better than the naive one, except that it doesn't return to Python after computing every single gradient, especially that the naive one should batch the operations better.", "body": "Thanks for a detailed writeup!\r\n\r\nYes, this is currently impossible with AD techniques that we use in PyTorch. In general, I don't think that it's possible to reduce the complexity of the standard AD algorithms from being linear in the number of different differentiated functions (so in `N = 1000` case, I would really expect it to be 1000x slower than a single backward pass, as the naive example shows). They are not doing the same operations - that's the reason why backprop is so efficient. The only thing we can do would be to batch the backward computation to improve vectorization (especially important in case of GPUs), but this would require some pretty significant changes in our AD backend. While I already heard some requests like that, unfortunately I don't think we'll be going down this path anytime soon.\r\n\r\nI actually have no idea why does the `multi` approach work any better than the naive one, except that it doesn't return to Python after computing every single gradient, especially that the naive one should batch the operations better."}