{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12346", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12346/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12346/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12346/events", "html_url": "https://github.com/pytorch/pytorch/issues/12346", "id": 367000861, "node_id": "MDU6SXNzdWUzNjcwMDA4NjE=", "number": 12346, "title": "RuntimeError: ONNX export failed: Couldn't export Python operator Scatter", "user": {"login": "uhvardhan", "id": 25361013, "node_id": "MDQ6VXNlcjI1MzYxMDEz", "avatar_url": "https://avatars3.githubusercontent.com/u/25361013?v=4", "gravatar_id": "", "url": "https://api.github.com/users/uhvardhan", "html_url": "https://github.com/uhvardhan", "followers_url": "https://api.github.com/users/uhvardhan/followers", "following_url": "https://api.github.com/users/uhvardhan/following{/other_user}", "gists_url": "https://api.github.com/users/uhvardhan/gists{/gist_id}", "starred_url": "https://api.github.com/users/uhvardhan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/uhvardhan/subscriptions", "organizations_url": "https://api.github.com/users/uhvardhan/orgs", "repos_url": "https://api.github.com/users/uhvardhan/repos", "events_url": "https://api.github.com/users/uhvardhan/events{/privacy}", "received_events_url": "https://api.github.com/users/uhvardhan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-04T22:58:24Z", "updated_at": "2018-10-31T19:38:35Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"question\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/2753.png\">\u2753</g-emoji> Questions and Help</h2>\n<p>I am trying to export a pretrained pytorch model (pth model) to onnx format first. I get the above runtime error.<br>\n`Traceback (most recent call last):<br>\nFile \"/tmp/pycharm_project_896/utils/pytorch2onnx.py\", line 15, in <br>\ntorch.onnx.export(model.module, dummy_input, \"/neutrino/models/pretrained/huawei/model.onnx\")<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/onnx/<strong>init</strong>.py\", line 75, in export<br>\n_export(model, args, f, export_params, verbose, training)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/onnx/<strong>init</strong>.py\", line 131, in _export<br>\nproto = trace.export(list(model.state_dict().values()), _onnx_opset_version)<br>\nRuntimeError: ONNX export failed: Couldn't export Python operator Scatter</p>\n<p>Graph we tried to export:<br>\ngraph(%1 : Float(1, 3, 224, 224)<br>\n%2 : Float(4, 3, 7, 7)<br>\n%3 : Float(4)<br>\n%4 : Float(4)<br>\n%5 : Float(4)<br>\n%6 : Float(4)<br>\n%7 : Float(4, 4, 3, 3)<br>\n%8 : Float(4)<br>\n%9 : Float(4)<br>\n%10 : Float(4)<br>\n%11 : Float(4)<br>\n%12 : Float(4, 4, 3, 3)<br>\n%13 : Float(4)<br>\n%14 : Float(4)<br>\n%15 : Float(4)<br>\n%16 : Float(4)<br>\n%17 : Float(6, 4, 3, 3)<br>\n%18 : Float(6)<br>\n%19 : Float(6)<br>\n%20 : Float(6)<br>\n%21 : Float(6)<br>\n%22 : Float(6, 6, 3, 3)<br>\n%23 : Float(6)<br>\n%24 : Float(6)<br>\n%25 : Float(6)<br>\n%26 : Float(6)<br>\n%27 : Float(6, 4, 1, 1)<br>\n%28 : Float(6)<br>\n%29 : Float(6)<br>\n%30 : Float(6)<br>\n%31 : Float(6)<br>\n%32 : Float(98, 6, 3, 3)<br>\n%33 : Float(98)<br>\n%34 : Float(98)<br>\n%35 : Float(98)<br>\n%36 : Float(98)<br>\n%37 : Float(98, 98, 3, 3)<br>\n%38 : Float(98)<br>\n%39 : Float(98)<br>\n%40 : Float(98)<br>\n%41 : Float(98)<br>\n%42 : Float(98, 6, 1, 1)<br>\n%43 : Float(98)<br>\n%44 : Float(98)<br>\n%45 : Float(98)<br>\n%46 : Float(98)<br>\n%47 : Float(160, 98, 3, 3)<br>\n%48 : Float(160)<br>\n%49 : Float(160)<br>\n%50 : Float(160)<br>\n%51 : Float(160)<br>\n%52 : Float(160, 160, 3, 3)<br>\n%53 : Float(160)<br>\n%54 : Float(160)<br>\n%55 : Float(160)<br>\n%56 : Float(160)<br>\n%57 : Float(160, 98, 1, 1)<br>\n%58 : Float(160)<br>\n%59 : Float(160)<br>\n%60 : Float(160)<br>\n%61 : Float(160)<br>\n%62 : Float(16, 160)<br>\n%63 : Float(16)) {<br>\n%65 : Float(1, 3, 224, 224), %66 : Handle = ^Scatter([0], None, 0)(%1), uses = [[%68.i0], []], scope: DataParallel;<br>\n%68 : Float(1, 4, 112, 112) = Conv[kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3], dilations=[1, 1], group=1](%65, %2), uses = [%69.i0], scope: DataParallel/ResNet[module]/Conv2d[conv1];<br>\n%70 : Float(1, 4, 112, 112) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%68, %3, %4, %5, %6), uses = [[%71.i0]], scope: DataParallel/ResNet[module]/BatchNorm2d[bn1];<br>\n%71 : Float(1, 4, 112, 112) = Relu(%70), uses = [%72.i0], scope: DataParallel/ResNet[module]/ReLU[relu];<br>\n%72 : Float(1, 4, 56, 56) = MaxPool<a href=\"%71\">kernel_shape=[3, 3], pads=[1, 1], strides=[2, 2]</a>, uses = [%74.i0, %82.i1], scope: DataParallel/ResNet[module]/MaxPool2d[maxpool];<br>\n%74 : Float(1, 4, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%72, %7), uses = [%75.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1];<br>\n%76 : Float(1, 4, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%74, %8, %9, %10, %11), uses = [[%77.i0]], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1];<br>\n%77 : Float(1, 4, 56, 56) = Relu(%76), uses = [%79.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/ReLU[relu];<br>\n%79 : Float(1, 4, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%77, %12), uses = [%80.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2];<br>\n%81 : Float(1, 4, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%79, %13, %14, %15, %16), uses = [[%82.i0]], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2];<br>\n%82 : Float(1, 4, 56, 56) = Add(%81, %72), uses = [%83.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0];<br>\n%83 : Float(1, 4, 56, 56) = Relu(%82), uses = [%85.i0, %94.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/ReLU[relu];<br>\n%85 : Float(1, 6, 28, 28) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%83, %17), uses = [%86.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1];<br>\n%87 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%85, %18, %19, %20, %21), uses = [[%88.i0]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1];<br>\n%88 : Float(1, 6, 28, 28) = Relu(%87), uses = [%90.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/ReLU[relu];<br>\n%90 : Float(1, 6, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%88, %22), uses = [%91.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2];<br>\n%92 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%90, %23, %24, %25, %26), uses = [[%97.i0]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2];<br>\n%94 : Float(1, 6, 28, 28) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%83, %27), uses = [%95.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];<br>\n%96 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%94, %28, %29, %30, %31), uses = [[%97.i1]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];<br>\n%97 : Float(1, 6, 28, 28) = Add(%92, %96), uses = [%98.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0];<br>\n%98 : Float(1, 6, 28, 28) = Relu(%97), uses = [%100.i0, %109.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/ReLU[relu];<br>\n%100 : Float(1, 98, 14, 14) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%98, %32), uses = [%101.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1];<br>\n%102 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%100, %33, %34, %35, %36), uses = [[%103.i0]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1];<br>\n%103 : Float(1, 98, 14, 14) = Relu(%102), uses = [%105.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/ReLU[relu];<br>\n%105 : Float(1, 98, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%103, %37), uses = [%106.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2];<br>\n%107 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%105, %38, %39, %40, %41), uses = [[%112.i0]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2];<br>\n%109 : Float(1, 98, 14, 14) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%98, %42), uses = [%110.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];<br>\n%111 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%109, %43, %44, %45, %46), uses = [[%112.i1]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];<br>\n%112 : Float(1, 98, 14, 14) = Add(%107, %111), uses = [%113.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0];<br>\n%113 : Float(1, 98, 14, 14) = Relu(%112), uses = [%115.i0, %124.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/ReLU[relu];<br>\n%115 : Float(1, 160, 7, 7) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%113, %47), uses = [%116.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1];<br>\n%117 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%115, %48, %49, %50, %51), uses = [[%118.i0]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1];<br>\n%118 : Float(1, 160, 7, 7) = Relu(%117), uses = [%120.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/ReLU[relu];<br>\n%120 : Float(1, 160, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%118, %52), uses = [%121.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2];<br>\n%122 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%120, %53, %54, %55, %56), uses = [[%127.i0]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2];<br>\n%124 : Float(1, 160, 7, 7) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%113, %57), uses = [%125.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];<br>\n%126 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%124, %58, %59, %60, %61), uses = [[%127.i1]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];<br>\n%127 : Float(1, 160, 7, 7) = Add(%122, %126), uses = [%128.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0];<br>\n%128 : Float(1, 160, 7, 7) = Relu(%127), uses = [%129.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/ReLU[relu];<br>\n%129 : Float(1, 160, 1, 1) = AveragePool<a href=\"%128\">kernel_shape=[7, 7], pads=[0, 0], strides=[1, 1]</a>, uses = [%130.i0], scope: DataParallel/ResNet[module]/AvgPool2d[avgpool];<br>\n%130 : Float(1, 160) = Reshape<a href=\"%129\">shape=[1, -1]</a>, uses = [%133.i0], scope: DataParallel/ResNet[module];<br>\n%133 : Float(1, 16) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%130, %62, %63), uses = [%0.i0], scope: DataParallel/ResNet[module]/Linear[fc];<br>\nreturn (%133);<br>\n}`</p>\n<p>Thanks in advance.</p>", "body_text": "\u2753 Questions and Help\nI am trying to export a pretrained pytorch model (pth model) to onnx format first. I get the above runtime error.\n`Traceback (most recent call last):\nFile \"/tmp/pycharm_project_896/utils/pytorch2onnx.py\", line 15, in \ntorch.onnx.export(model.module, dummy_input, \"/neutrino/models/pretrained/huawei/model.onnx\")\nFile \"/usr/local/lib/python2.7/dist-packages/torch/onnx/init.py\", line 75, in export\n_export(model, args, f, export_params, verbose, training)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/onnx/init.py\", line 131, in _export\nproto = trace.export(list(model.state_dict().values()), _onnx_opset_version)\nRuntimeError: ONNX export failed: Couldn't export Python operator Scatter\nGraph we tried to export:\ngraph(%1 : Float(1, 3, 224, 224)\n%2 : Float(4, 3, 7, 7)\n%3 : Float(4)\n%4 : Float(4)\n%5 : Float(4)\n%6 : Float(4)\n%7 : Float(4, 4, 3, 3)\n%8 : Float(4)\n%9 : Float(4)\n%10 : Float(4)\n%11 : Float(4)\n%12 : Float(4, 4, 3, 3)\n%13 : Float(4)\n%14 : Float(4)\n%15 : Float(4)\n%16 : Float(4)\n%17 : Float(6, 4, 3, 3)\n%18 : Float(6)\n%19 : Float(6)\n%20 : Float(6)\n%21 : Float(6)\n%22 : Float(6, 6, 3, 3)\n%23 : Float(6)\n%24 : Float(6)\n%25 : Float(6)\n%26 : Float(6)\n%27 : Float(6, 4, 1, 1)\n%28 : Float(6)\n%29 : Float(6)\n%30 : Float(6)\n%31 : Float(6)\n%32 : Float(98, 6, 3, 3)\n%33 : Float(98)\n%34 : Float(98)\n%35 : Float(98)\n%36 : Float(98)\n%37 : Float(98, 98, 3, 3)\n%38 : Float(98)\n%39 : Float(98)\n%40 : Float(98)\n%41 : Float(98)\n%42 : Float(98, 6, 1, 1)\n%43 : Float(98)\n%44 : Float(98)\n%45 : Float(98)\n%46 : Float(98)\n%47 : Float(160, 98, 3, 3)\n%48 : Float(160)\n%49 : Float(160)\n%50 : Float(160)\n%51 : Float(160)\n%52 : Float(160, 160, 3, 3)\n%53 : Float(160)\n%54 : Float(160)\n%55 : Float(160)\n%56 : Float(160)\n%57 : Float(160, 98, 1, 1)\n%58 : Float(160)\n%59 : Float(160)\n%60 : Float(160)\n%61 : Float(160)\n%62 : Float(16, 160)\n%63 : Float(16)) {\n%65 : Float(1, 3, 224, 224), %66 : Handle = ^Scatter([0], None, 0)(%1), uses = [[%68.i0], []], scope: DataParallel;\n%68 : Float(1, 4, 112, 112) = Conv[kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3], dilations=[1, 1], group=1](%65, %2), uses = [%69.i0], scope: DataParallel/ResNet[module]/Conv2d[conv1];\n%70 : Float(1, 4, 112, 112) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%68, %3, %4, %5, %6), uses = [[%71.i0]], scope: DataParallel/ResNet[module]/BatchNorm2d[bn1];\n%71 : Float(1, 4, 112, 112) = Relu(%70), uses = [%72.i0], scope: DataParallel/ResNet[module]/ReLU[relu];\n%72 : Float(1, 4, 56, 56) = MaxPoolkernel_shape=[3, 3], pads=[1, 1], strides=[2, 2], uses = [%74.i0, %82.i1], scope: DataParallel/ResNet[module]/MaxPool2d[maxpool];\n%74 : Float(1, 4, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%72, %7), uses = [%75.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1];\n%76 : Float(1, 4, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%74, %8, %9, %10, %11), uses = [[%77.i0]], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1];\n%77 : Float(1, 4, 56, 56) = Relu(%76), uses = [%79.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/ReLU[relu];\n%79 : Float(1, 4, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%77, %12), uses = [%80.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2];\n%81 : Float(1, 4, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%79, %13, %14, %15, %16), uses = [[%82.i0]], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2];\n%82 : Float(1, 4, 56, 56) = Add(%81, %72), uses = [%83.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0];\n%83 : Float(1, 4, 56, 56) = Relu(%82), uses = [%85.i0, %94.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/ReLU[relu];\n%85 : Float(1, 6, 28, 28) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%83, %17), uses = [%86.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1];\n%87 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%85, %18, %19, %20, %21), uses = [[%88.i0]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1];\n%88 : Float(1, 6, 28, 28) = Relu(%87), uses = [%90.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/ReLU[relu];\n%90 : Float(1, 6, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%88, %22), uses = [%91.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2];\n%92 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%90, %23, %24, %25, %26), uses = [[%97.i0]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2];\n%94 : Float(1, 6, 28, 28) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%83, %27), uses = [%95.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n%96 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%94, %28, %29, %30, %31), uses = [[%97.i1]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n%97 : Float(1, 6, 28, 28) = Add(%92, %96), uses = [%98.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0];\n%98 : Float(1, 6, 28, 28) = Relu(%97), uses = [%100.i0, %109.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/ReLU[relu];\n%100 : Float(1, 98, 14, 14) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%98, %32), uses = [%101.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1];\n%102 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%100, %33, %34, %35, %36), uses = [[%103.i0]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1];\n%103 : Float(1, 98, 14, 14) = Relu(%102), uses = [%105.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/ReLU[relu];\n%105 : Float(1, 98, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%103, %37), uses = [%106.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2];\n%107 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%105, %38, %39, %40, %41), uses = [[%112.i0]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2];\n%109 : Float(1, 98, 14, 14) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%98, %42), uses = [%110.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n%111 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%109, %43, %44, %45, %46), uses = [[%112.i1]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n%112 : Float(1, 98, 14, 14) = Add(%107, %111), uses = [%113.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0];\n%113 : Float(1, 98, 14, 14) = Relu(%112), uses = [%115.i0, %124.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/ReLU[relu];\n%115 : Float(1, 160, 7, 7) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%113, %47), uses = [%116.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1];\n%117 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%115, %48, %49, %50, %51), uses = [[%118.i0]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1];\n%118 : Float(1, 160, 7, 7) = Relu(%117), uses = [%120.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/ReLU[relu];\n%120 : Float(1, 160, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%118, %52), uses = [%121.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2];\n%122 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%120, %53, %54, %55, %56), uses = [[%127.i0]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2];\n%124 : Float(1, 160, 7, 7) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%113, %57), uses = [%125.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\n%126 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%124, %58, %59, %60, %61), uses = [[%127.i1]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\n%127 : Float(1, 160, 7, 7) = Add(%122, %126), uses = [%128.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0];\n%128 : Float(1, 160, 7, 7) = Relu(%127), uses = [%129.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/ReLU[relu];\n%129 : Float(1, 160, 1, 1) = AveragePoolkernel_shape=[7, 7], pads=[0, 0], strides=[1, 1], uses = [%130.i0], scope: DataParallel/ResNet[module]/AvgPool2d[avgpool];\n%130 : Float(1, 160) = Reshapeshape=[1, -1], uses = [%133.i0], scope: DataParallel/ResNet[module];\n%133 : Float(1, 16) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%130, %62, %63), uses = [%0.i0], scope: DataParallel/ResNet[module]/Linear[fc];\nreturn (%133);\n}`\nThanks in advance.", "body": "## \u2753 Questions and Help\r\n\r\nI am trying to export a pretrained pytorch model (pth model) to onnx format first. I get the above runtime error.\r\n`Traceback (most recent call last):\r\n  File \"/tmp/pycharm_project_896/utils/pytorch2onnx.py\", line 15, in <module>\r\n    torch.onnx.export(model.module, dummy_input, \"/neutrino/models/pretrained/huawei/model.onnx\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/onnx/__init__.py\", line 75, in export\r\n    _export(model, args, f, export_params, verbose, training)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/onnx/__init__.py\", line 131, in _export\r\n    proto = trace.export(list(model.state_dict().values()), _onnx_opset_version)\r\nRuntimeError: ONNX export failed: Couldn't export Python operator Scatter\r\n\r\nGraph we tried to export:\r\ngraph(%1 : Float(1, 3, 224, 224)\r\n      %2 : Float(4, 3, 7, 7)\r\n      %3 : Float(4)\r\n      %4 : Float(4)\r\n      %5 : Float(4)\r\n      %6 : Float(4)\r\n      %7 : Float(4, 4, 3, 3)\r\n      %8 : Float(4)\r\n      %9 : Float(4)\r\n      %10 : Float(4)\r\n      %11 : Float(4)\r\n      %12 : Float(4, 4, 3, 3)\r\n      %13 : Float(4)\r\n      %14 : Float(4)\r\n      %15 : Float(4)\r\n      %16 : Float(4)\r\n      %17 : Float(6, 4, 3, 3)\r\n      %18 : Float(6)\r\n      %19 : Float(6)\r\n      %20 : Float(6)\r\n      %21 : Float(6)\r\n      %22 : Float(6, 6, 3, 3)\r\n      %23 : Float(6)\r\n      %24 : Float(6)\r\n      %25 : Float(6)\r\n      %26 : Float(6)\r\n      %27 : Float(6, 4, 1, 1)\r\n      %28 : Float(6)\r\n      %29 : Float(6)\r\n      %30 : Float(6)\r\n      %31 : Float(6)\r\n      %32 : Float(98, 6, 3, 3)\r\n      %33 : Float(98)\r\n      %34 : Float(98)\r\n      %35 : Float(98)\r\n      %36 : Float(98)\r\n      %37 : Float(98, 98, 3, 3)\r\n      %38 : Float(98)\r\n      %39 : Float(98)\r\n      %40 : Float(98)\r\n      %41 : Float(98)\r\n      %42 : Float(98, 6, 1, 1)\r\n      %43 : Float(98)\r\n      %44 : Float(98)\r\n      %45 : Float(98)\r\n      %46 : Float(98)\r\n      %47 : Float(160, 98, 3, 3)\r\n      %48 : Float(160)\r\n      %49 : Float(160)\r\n      %50 : Float(160)\r\n      %51 : Float(160)\r\n      %52 : Float(160, 160, 3, 3)\r\n      %53 : Float(160)\r\n      %54 : Float(160)\r\n      %55 : Float(160)\r\n      %56 : Float(160)\r\n      %57 : Float(160, 98, 1, 1)\r\n      %58 : Float(160)\r\n      %59 : Float(160)\r\n      %60 : Float(160)\r\n      %61 : Float(160)\r\n      %62 : Float(16, 160)\r\n      %63 : Float(16)) {\r\n  %65 : Float(1, 3, 224, 224), %66 : Handle = ^Scatter([0], None, 0)(%1), uses = [[%68.i0], []], scope: DataParallel;\r\n  %68 : Float(1, 4, 112, 112) = Conv[kernel_shape=[7, 7], strides=[2, 2], pads=[3, 3, 3, 3], dilations=[1, 1], group=1](%65, %2), uses = [%69.i0], scope: DataParallel/ResNet[module]/Conv2d[conv1];\r\n  %70 : Float(1, 4, 112, 112) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%68, %3, %4, %5, %6), uses = [[%71.i0]], scope: DataParallel/ResNet[module]/BatchNorm2d[bn1];\r\n  %71 : Float(1, 4, 112, 112) = Relu(%70), uses = [%72.i0], scope: DataParallel/ResNet[module]/ReLU[relu];\r\n  %72 : Float(1, 4, 56, 56) = MaxPool[kernel_shape=[3, 3], pads=[1, 1], strides=[2, 2]](%71), uses = [%74.i0, %82.i1], scope: DataParallel/ResNet[module]/MaxPool2d[maxpool];\r\n  %74 : Float(1, 4, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%72, %7), uses = [%75.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1];\r\n  %76 : Float(1, 4, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%74, %8, %9, %10, %11), uses = [[%77.i0]], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1];\r\n  %77 : Float(1, 4, 56, 56) = Relu(%76), uses = [%79.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/ReLU[relu];\r\n  %79 : Float(1, 4, 56, 56) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%77, %12), uses = [%80.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2];\r\n  %81 : Float(1, 4, 56, 56) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%79, %13, %14, %15, %16), uses = [[%82.i0]], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2];\r\n  %82 : Float(1, 4, 56, 56) = Add(%81, %72), uses = [%83.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0];\r\n  %83 : Float(1, 4, 56, 56) = Relu(%82), uses = [%85.i0, %94.i0], scope: DataParallel/ResNet[module]/Sequential[layer1]/BasicBlock[0]/ReLU[relu];\r\n  %85 : Float(1, 6, 28, 28) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%83, %17), uses = [%86.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1];\r\n  %87 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%85, %18, %19, %20, %21), uses = [[%88.i0]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1];\r\n  %88 : Float(1, 6, 28, 28) = Relu(%87), uses = [%90.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/ReLU[relu];\r\n  %90 : Float(1, 6, 28, 28) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%88, %22), uses = [%91.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2];\r\n  %92 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%90, %23, %24, %25, %26), uses = [[%97.i0]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2];\r\n  %94 : Float(1, 6, 28, 28) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%83, %27), uses = [%95.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\r\n  %96 : Float(1, 6, 28, 28) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%94, %28, %29, %30, %31), uses = [[%97.i1]], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\r\n  %97 : Float(1, 6, 28, 28) = Add(%92, %96), uses = [%98.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0];\r\n  %98 : Float(1, 6, 28, 28) = Relu(%97), uses = [%100.i0, %109.i0], scope: DataParallel/ResNet[module]/Sequential[layer2]/BasicBlock[0]/ReLU[relu];\r\n  %100 : Float(1, 98, 14, 14) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%98, %32), uses = [%101.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1];\r\n  %102 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%100, %33, %34, %35, %36), uses = [[%103.i0]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1];\r\n  %103 : Float(1, 98, 14, 14) = Relu(%102), uses = [%105.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/ReLU[relu];\r\n  %105 : Float(1, 98, 14, 14) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%103, %37), uses = [%106.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2];\r\n  %107 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%105, %38, %39, %40, %41), uses = [[%112.i0]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2];\r\n  %109 : Float(1, 98, 14, 14) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%98, %42), uses = [%110.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\r\n  %111 : Float(1, 98, 14, 14) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%109, %43, %44, %45, %46), uses = [[%112.i1]], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\r\n  %112 : Float(1, 98, 14, 14) = Add(%107, %111), uses = [%113.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0];\r\n  %113 : Float(1, 98, 14, 14) = Relu(%112), uses = [%115.i0, %124.i0], scope: DataParallel/ResNet[module]/Sequential[layer3]/BasicBlock[0]/ReLU[relu];\r\n  %115 : Float(1, 160, 7, 7) = Conv[kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%113, %47), uses = [%116.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1];\r\n  %117 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%115, %48, %49, %50, %51), uses = [[%118.i0]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1];\r\n  %118 : Float(1, 160, 7, 7) = Relu(%117), uses = [%120.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/ReLU[relu];\r\n  %120 : Float(1, 160, 7, 7) = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%118, %52), uses = [%121.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2];\r\n  %122 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%120, %53, %54, %55, %56), uses = [[%127.i0]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2];\r\n  %124 : Float(1, 160, 7, 7) = Conv[kernel_shape=[1, 1], strides=[2, 2], pads=[0, 0, 0, 0], dilations=[1, 1], group=1](%113, %57), uses = [%125.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0];\r\n  %126 : Float(1, 160, 7, 7) = BatchNormalization[is_test=1, epsilon=1e-05, momentum=0.9, consumed_inputs=[0, 0, 0, 1, 1]](%124, %58, %59, %60, %61), uses = [[%127.i1]], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1];\r\n  %127 : Float(1, 160, 7, 7) = Add(%122, %126), uses = [%128.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0];\r\n  %128 : Float(1, 160, 7, 7) = Relu(%127), uses = [%129.i0], scope: DataParallel/ResNet[module]/Sequential[layer4]/BasicBlock[0]/ReLU[relu];\r\n  %129 : Float(1, 160, 1, 1) = AveragePool[kernel_shape=[7, 7], pads=[0, 0], strides=[1, 1]](%128), uses = [%130.i0], scope: DataParallel/ResNet[module]/AvgPool2d[avgpool];\r\n  %130 : Float(1, 160) = Reshape[shape=[1, -1]](%129), uses = [%133.i0], scope: DataParallel/ResNet[module];\r\n  %133 : Float(1, 16) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%130, %62, %63), uses = [%0.i0], scope: DataParallel/ResNet[module]/Linear[fc];\r\n  return (%133);\r\n}`\r\n\r\n\r\nThanks in advance.\r\n"}