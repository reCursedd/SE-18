{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2864", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2864/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2864/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2864/events", "html_url": "https://github.com/pytorch/pytorch/issues/2864", "id": 260456770, "node_id": "MDU6SXNzdWUyNjA0NTY3NzA=", "number": 2864, "title": "DataParallel doesn't replicate module's member variables", "user": {"login": "blackyang", "id": 3350930, "node_id": "MDQ6VXNlcjMzNTA5MzA=", "avatar_url": "https://avatars3.githubusercontent.com/u/3350930?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blackyang", "html_url": "https://github.com/blackyang", "followers_url": "https://api.github.com/users/blackyang/followers", "following_url": "https://api.github.com/users/blackyang/following{/other_user}", "gists_url": "https://api.github.com/users/blackyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/blackyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blackyang/subscriptions", "organizations_url": "https://api.github.com/users/blackyang/orgs", "repos_url": "https://api.github.com/users/blackyang/repos", "events_url": "https://api.github.com/users/blackyang/events{/privacy}", "received_events_url": "https://api.github.com/users/blackyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-09-26T00:45:31Z", "updated_at": "2017-09-26T20:17:28Z", "closed_at": "2017-09-26T20:17:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems that DataParallel doesn't replicate a module's member variables, is this intended?</p>\n<p>See the following code for an example. We'll see different outputs depending on <code>data_parallel</code>.</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Simple(nn.Module):\n    '''A simple example'''\n\n    def __init__(self):\n        super(Simple, self).__init__()\n        self.counter = 0\n\n    def forward(self, x):\n        print(self.counter)\n        self.counter += 1\n        print(self.counter)\n        return x\n\nif __name__ == '__main__':\n    x = Variable(torch.randn(10, 10))\n    net = Simple()\n    data_parallel = False  # True\n    if data_parallel:\n        net = torch.nn.DataParallel(net)\n    net = net.cuda()\n    for i in range(10):\n        print('iteration: {}'.format(i))\n        y = net(x)\n</code></pre>", "body_text": "It seems that DataParallel doesn't replicate a module's member variables, is this intended?\nSee the following code for an example. We'll see different outputs depending on data_parallel.\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Simple(nn.Module):\n    '''A simple example'''\n\n    def __init__(self):\n        super(Simple, self).__init__()\n        self.counter = 0\n\n    def forward(self, x):\n        print(self.counter)\n        self.counter += 1\n        print(self.counter)\n        return x\n\nif __name__ == '__main__':\n    x = Variable(torch.randn(10, 10))\n    net = Simple()\n    data_parallel = False  # True\n    if data_parallel:\n        net = torch.nn.DataParallel(net)\n    net = net.cuda()\n    for i in range(10):\n        print('iteration: {}'.format(i))\n        y = net(x)", "body": "It seems that DataParallel doesn't replicate a module's member variables, is this intended?\r\n\r\nSee the following code for an example. We'll see different outputs depending on ``data_parallel``.\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass Simple(nn.Module):\r\n    '''A simple example'''\r\n\r\n    def __init__(self):\r\n        super(Simple, self).__init__()\r\n        self.counter = 0\r\n\r\n    def forward(self, x):\r\n        print(self.counter)\r\n        self.counter += 1\r\n        print(self.counter)\r\n        return x\r\n\r\nif __name__ == '__main__':\r\n    x = Variable(torch.randn(10, 10))\r\n    net = Simple()\r\n    data_parallel = False  # True\r\n    if data_parallel:\r\n        net = torch.nn.DataParallel(net)\r\n    net = net.cuda()\r\n    for i in range(10):\r\n        print('iteration: {}'.format(i))\r\n        y = net(x)\r\n```\r\n"}