{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/268652187", "html_url": "https://github.com/pytorch/pytorch/issues/334#issuecomment-268652187", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/334", "id": 268652187, "node_id": "MDEyOklzc3VlQ29tbWVudDI2ODY1MjE4Nw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-21T21:58:15Z", "updated_at": "2016-12-21T21:58:15Z", "author_association": "MEMBER", "body_html": "<p>Actually CUDA kernel launches are asynchronous, so they're not blocking for the CPU threads. They'll never wait for them to finish before continuing to the next line. Still, using threading for dispatching different parts of computation is actually a good use case, that's also what we do in the data parallel code. I just wanted to make sure you know about the GIL. It can be quite surprising for a lot of people.</p>", "body_text": "Actually CUDA kernel launches are asynchronous, so they're not blocking for the CPU threads. They'll never wait for them to finish before continuing to the next line. Still, using threading for dispatching different parts of computation is actually a good use case, that's also what we do in the data parallel code. I just wanted to make sure you know about the GIL. It can be quite surprising for a lot of people.", "body": "Actually CUDA kernel launches are asynchronous, so they're not blocking for the CPU threads. They'll never wait for them to finish before continuing to the next line. Still, using threading for dispatching different parts of computation is actually a good use case, that's also what we do in the data parallel code. I just wanted to make sure you know about the GIL. It can be quite surprising for a lot of people."}