{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1111", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1111/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1111/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1111/events", "html_url": "https://github.com/pytorch/pytorch/issues/1111", "id": 217119793, "node_id": "MDU6SXNzdWUyMTcxMTk3OTM=", "number": 1111, "title": "Memory Leak in autograd", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 526654084, "node_id": "MDU6TGFiZWw1MjY2NTQwODQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/on%20hold", "name": "on hold", "color": "cccccc", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-03-27T02:20:45Z", "updated_at": "2017-08-30T23:34:59Z", "closed_at": "2017-08-30T23:34:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is an example as small as I could make it that triggers what I assume is a memory leak.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> gc\n<span class=\"pl-k\">import</span> resource\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> inp = Variable(torch.Tensor(1,3), volatile=True) # This one does not have a problem</span>\ninp <span class=\"pl-k\">=</span> Variable(torch.Tensor(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> This one does</span>\nlinear_layer <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>)\n\niter_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n    out <span class=\"pl-k\">=</span> linear_layer(inp).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    a <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">6</span>))\n\n    <span class=\"pl-k\">for</span> elt_idx, out <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(out):\n        a[elt_idx] <span class=\"pl-k\">=</span> out <span class=\"pl-k\">+</span> a[elt_idx]\n\n    <span class=\"pl-k\">if</span> iter_idx <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        gc.collect()\n        max_mem_used <span class=\"pl-k\">=</span> resource.getrusage(resource.<span class=\"pl-c1\">RUSAGE_SELF</span>).ru_maxrss\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span> MB<span class=\"pl-pds\">\"</span></span>.format(max_mem_used <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1024</span>))\n\n\n    iter_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n</pre></div>\n<p>The \"network\" used here is fairly small, a linear layer with 18 parameters. The backward pass is not even called.</p>\n<p>If the input to the network is given as <code>volatile</code>, memory usage is stable (81.23 MB) on my machine.<br>\nIf not, it quickly blows out of proportion, (after 1 min, it already uses 4GB on my machine)</p>\n<p>This is probably not very important, I figured out how to work around this in my implementation but I figured I'd still report it.</p>\n<p>My setup is the following (everything was run from a virtualenv, pytorch was installed today just for this):</p>\n<p><code>python --version</code> is <code>Python 3.6.0</code><br>\n<code>pip freeze</code> returns</p>\n<pre><code>appdirs==1.4.3\nnumpy==1.12.1\nolefile==0.44\npackaging==16.8\nPillow==4.0.0\npyparsing==2.2.0\nPyYAML==3.12\nsix==1.10.0\ntorch==0.1.10.post2\ntorchvision==0.1.7\n</code></pre>\n<p>My OS is a relatively up to date ArchLinux install.<br>\nSame bug (not the minimal example but the original version) was also observed on my workstation which is an Ubuntu 16.04 install.</p>\n<p>Let me know if I can provide more help to debug this.</p>\n<p><strong>Edit:</strong> I tried the same thing with a python 2.7 install and the results are exactly the same.</p>", "body_text": "Here is an example as small as I could make it that triggers what I assume is a memory leak.\nimport torch\nimport gc\nimport resource\nfrom torch import nn\nfrom torch.autograd import Variable\n\n# inp = Variable(torch.Tensor(1,3), volatile=True) # This one does not have a problem\ninp = Variable(torch.Tensor(1,3)) # This one does\nlinear_layer = nn.Linear(3, 6)\n\niter_idx = 0\nwhile True:\n    out = linear_layer(inp).view(-1)\n    a = Variable(torch.zeros(6))\n\n    for elt_idx, out in enumerate(out):\n        a[elt_idx] = out + a[elt_idx]\n\n    if iter_idx % 10000 == 0:\n        gc.collect()\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\n\n\n    iter_idx += 1\n\nThe \"network\" used here is fairly small, a linear layer with 18 parameters. The backward pass is not even called.\nIf the input to the network is given as volatile, memory usage is stable (81.23 MB) on my machine.\nIf not, it quickly blows out of proportion, (after 1 min, it already uses 4GB on my machine)\nThis is probably not very important, I figured out how to work around this in my implementation but I figured I'd still report it.\nMy setup is the following (everything was run from a virtualenv, pytorch was installed today just for this):\npython --version is Python 3.6.0\npip freeze returns\nappdirs==1.4.3\nnumpy==1.12.1\nolefile==0.44\npackaging==16.8\nPillow==4.0.0\npyparsing==2.2.0\nPyYAML==3.12\nsix==1.10.0\ntorch==0.1.10.post2\ntorchvision==0.1.7\n\nMy OS is a relatively up to date ArchLinux install.\nSame bug (not the minimal example but the original version) was also observed on my workstation which is an Ubuntu 16.04 install.\nLet me know if I can provide more help to debug this.\nEdit: I tried the same thing with a python 2.7 install and the results are exactly the same.", "body": "Here is an example as small as I could make it that triggers what I assume is a memory leak.\r\n\r\n\r\n```python\r\nimport torch\r\nimport gc\r\nimport resource\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\n# inp = Variable(torch.Tensor(1,3), volatile=True) # This one does not have a problem\r\ninp = Variable(torch.Tensor(1,3)) # This one does\r\nlinear_layer = nn.Linear(3, 6)\r\n\r\niter_idx = 0\r\nwhile True:\r\n    out = linear_layer(inp).view(-1)\r\n    a = Variable(torch.zeros(6))\r\n\r\n    for elt_idx, out in enumerate(out):\r\n        a[elt_idx] = out + a[elt_idx]\r\n\r\n    if iter_idx % 10000 == 0:\r\n        gc.collect()\r\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\r\n\r\n\r\n    iter_idx += 1\r\n\r\n```\r\nThe \"network\" used here is fairly small, a linear layer with 18 parameters. The backward pass is not even called.\r\n\r\nIf the input to the network is given as `volatile`, memory usage is stable (81.23 MB) on my machine.\r\nIf not, it quickly blows out of proportion, (after 1 min, it already uses 4GB on my machine)\r\n\r\nThis is probably not very important, I figured out how to work around this in my implementation but I figured I'd still report it.\r\n\r\nMy setup is the following (everything was run from a virtualenv, pytorch was installed today just for this):\r\n\r\n`python --version` is `Python 3.6.0`\r\n`pip freeze` returns\r\n```\r\nappdirs==1.4.3\r\nnumpy==1.12.1\r\nolefile==0.44\r\npackaging==16.8\r\nPillow==4.0.0\r\npyparsing==2.2.0\r\nPyYAML==3.12\r\nsix==1.10.0\r\ntorch==0.1.10.post2\r\ntorchvision==0.1.7\r\n```\r\n\r\nMy OS is a relatively up to date ArchLinux install. \r\nSame bug (not the minimal example but the original version) was also observed on my workstation which is an Ubuntu 16.04 install.\r\n\r\n\r\nLet me know if I can provide more help to debug this.\r\n\r\n**Edit:** I tried the same thing with a python 2.7 install and the results are exactly the same.\r\n"}