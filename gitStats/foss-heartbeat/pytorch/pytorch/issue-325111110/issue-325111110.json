{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7751", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7751/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7751/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7751/events", "html_url": "https://github.com/pytorch/pytorch/issues/7751", "id": 325111110, "node_id": "MDU6SXNzdWUzMjUxMTExMTA=", "number": 7751, "title": "[caffe2] failed to complied caffe2 on ubuntu 17.10 + CUDA 9.1 + cuDNN 7.1", "user": {"login": "sirius-ai", "id": 32933617, "node_id": "MDQ6VXNlcjMyOTMzNjE3", "avatar_url": "https://avatars2.githubusercontent.com/u/32933617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sirius-ai", "html_url": "https://github.com/sirius-ai", "followers_url": "https://api.github.com/users/sirius-ai/followers", "following_url": "https://api.github.com/users/sirius-ai/following{/other_user}", "gists_url": "https://api.github.com/users/sirius-ai/gists{/gist_id}", "starred_url": "https://api.github.com/users/sirius-ai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sirius-ai/subscriptions", "organizations_url": "https://api.github.com/users/sirius-ai/orgs", "repos_url": "https://api.github.com/users/sirius-ai/repos", "events_url": "https://api.github.com/users/sirius-ai/events{/privacy}", "received_events_url": "https://api.github.com/users/sirius-ai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-22T01:34:17Z", "updated_at": "2018-11-20T14:27:55Z", "closed_at": "2018-05-25T08:05:11Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I had install CUDA 9.1 and cuDNN 7.1 before complie caffe2, and I don't want to update CUDA version, so when I complied caffe2 on ubuntu 17.10 + CUDA 9.1 + cuDNN 7.1 met that error:</p>\n<p>`/tmp/tmpxft_00000939_00000000-5_abs_op.cudafe1.stub.c:20:27:   required from here<br>\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)<br>\nreturn  _<em>and</em>&lt;_<em>not</em>&lt;is_same&lt;tuple&lt;_Elements...&gt;,<br>\n^<br>\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template&lt;class _From, class _To&gt; struct std::is_convertible\u2019<br>\nstruct is_convertible<br>\n^~~~~~~~~~~~~~<br>\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = const std::tuple&lt;long unsigned int, long unsigned int, long unsigned int&gt;&amp;; bool  = true; _Elements = {long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = std::tuple&lt;long unsigned int, long unsigned int, long unsigned int&gt;&amp;&amp;; bool  = true; _Elements = {long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;}]\u2019:<br>\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template&lt;class ... _UElements, class _Dummy, typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;&gt;::_NonNestedTuple&lt;tuple&lt;_Elements ...&gt;&amp;&amp;&gt;()), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(std::tuple&lt;_Args1 ...&gt;&amp;&amp;) [with _UElements = {long unsigned int, long unsigned int, long unsigned int}; _Dummy = void; typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;&gt;::_NonNestedTuple&lt;tuple&lt;_Elements ...&gt;&amp;&amp;&gt;()), bool&gt;::type  = ]\u2019<br>\n/home/andy/github/pytorch/caffe2/operators/elementwise_op.h:226:26:   required from \u2018bool caffe2::BinaryElementwiseOp&lt;InputTypes, Context, Functor, TypeMap&gt;::DoRunWithType() [with T = float; InputTypes = caffe2::TensorTypes; Context = caffe2::CUDAContext; Functor = caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor; TypeMap = caffe2::SameTypeAsInput]\u2019<br>\n/home/andy/github/pytorch/caffe2/core/operator.h:682:80:   required from \u2018static bool caffe2::DispatchHelper&lt;caffe2::TensorTypes&lt;FirstType, Types ...&gt;, ExtraArgs ...&gt;::call(Op*, const caffe2::TypeMeta&amp;) [with Op = caffe2::BinaryElementwiseOp&lt;caffe2::TensorTypes, caffe2::CUDAContext, caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor &gt;; FirstType = float; Types = {}; ExtraArgs = {}]\u2019<br>\n/home/andy/github/pytorch/caffe2/core/operator.h:684:47:   required from \u2018static bool caffe2::DispatchHelper&lt;caffe2::TensorTypes&lt;FirstType, Types ...&gt;, ExtraArgs ...&gt;::call(Op*, const caffe2::Tensor&amp;) [with Op = caffe2::BinaryElementwiseOp&lt;caffe2::TensorTypes, caffe2::CUDAContext, caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor &gt;; Context = caffe2::CUDAContext; FirstType = float; Types = {}; ExtraArgs = {}]\u2019<br>\n/home/andy/github/pytorch/caffe2/operators/elementwise_op.h:200:42:   required from \u2018bool caffe2::BinaryElementwiseOp&lt;InputTypes, Context, Functor, TypeMap&gt;::RunOnDevice() [with InputTypes = caffe2::TensorTypes; Context = caffe2::CUDAContext; Functor = caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor; TypeMap = caffe2::SameTypeAsInput]\u2019<br>\n/tmp/tmpxft_00000939_00000000-5_abs_op.cudafe1.stub.c:20:27:   required from here<br>\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)<br>\nreturn  _<em>and</em>&lt;_<em>not</em>&lt;is_same&lt;tuple&lt;_Elements...&gt;,<br>\n^<br>\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template&lt;class _From, class _To&gt; struct std::is_convertible\u2019<br>\nstruct is_convertible<br>\n^~~~~~~~~~~~~~<br>\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = std::tuple&lt;long unsigned int, long unsigned int, long unsigned int&gt;&amp;&amp;; bool  = true; _Elements = {long unsigned int&amp;, long unsigned int&amp;, long unsigned int&amp;}]\u2019 not a return-statement<br>\n}<br>\n^<br>\nCMake Error at caffe2_gpu_generated_abs_op.cu.o.Release.cmake:275 (message):<br>\nError generating file<br>\n/home/andy/github/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/operators/./caffe2_gpu_generated_abs_op.cu.o</p>\n<p>caffe2/CMakeFiles/caffe2_gpu.dir/build.make:91: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/operators/caffe2_gpu_generated_abs_op.cu.o' failed<br>\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/operators/caffe2_gpu_generated_abs_op.cu.o] Error 1<br>\nCMakeFiles/Makefile2:2524: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/all' failed<br>\nmake[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2<br>\nMakefile:140: recipe for target 'all' failed<br>\nmake: *** [all] Error 2`</p>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: caffe2</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Build command you used (if compiling from source): <a href=\"https://caffe2.ai/docs/getting-started.html?platform=ubuntu&amp;configuration=compile#install-with-gpu-support\" rel=\"nofollow\">https://caffe2.ai/docs/getting-started.html?platform=ubuntu&amp;configuration=compile#install-with-gpu-support</a></li>\n<li>OS: linux ubuntu 17.10 64bit</li>\n<li>PyTorch version: latest</li>\n<li>Python version: 3.6.3</li>\n<li>CUDA/cuDNN version: CUDA 9.1 + cuDNN 7.1</li>\n<li>GPU models and configuration: gtx 1060 6GB</li>\n<li>GCC version (if compiling from source): GCC 6.4.0</li>\n<li>CMake version: 3.9.1</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>\n<p>cmake .. log is below:<br>\n`(py3caffe2) andy@andy:~/github/pytorch/build$ cmake ..<br>\n-- The CXX compiler identification is GNU 6.4.0<br>\n-- The C compiler identification is GNU 6.4.0<br>\n-- Check for working CXX compiler: /usr/bin/c++<br>\n-- Check for working CXX compiler: /usr/bin/c++ -- works<br>\n-- Detecting CXX compiler ABI info<br>\n-- Detecting CXX compiler ABI info - done<br>\n-- Detecting CXX compile features<br>\n-- Detecting CXX compile features - done<br>\n-- Check for working C compiler: /usr/bin/cc<br>\n-- Check for working C compiler: /usr/bin/cc -- works<br>\n-- Detecting C compiler ABI info<br>\n-- Detecting C compiler ABI info - done<br>\n-- Detecting C compile features<br>\n-- Detecting C compile features - done<br>\n-- Performing Test CAFFE2_LONG_IS_INT32_OR_64<br>\n-- Performing Test CAFFE2_LONG_IS_INT32_OR_64 - Success<br>\n-- Does not need to define long separately.<br>\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED<br>\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success<br>\n-- std::exception_ptr is supported.<br>\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE<br>\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE - Success<br>\n-- NUMA is available<br>\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING<br>\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Success<br>\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS<br>\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Success<br>\n-- Current compiler supports avx2 extention. Will build perfkernels.<br>\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY<br>\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success<br>\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY<br>\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success<br>\n-- Build type not set - defaulting to Release<br>\n-- Building using own protobuf under third_party per request.<br>\n-- Use custom protobuf build.<br>\n-- Looking for pthread.h<br>\n-- Looking for pthread.h - found<br>\n-- Looking for pthread_create<br>\n-- Looking for pthread_create - not found<br>\n-- Looking for pthread_create in pthreads<br>\n-- Looking for pthread_create in pthreads - not found<br>\n-- Looking for pthread_create in pthread<br>\n-- Looking for pthread_create in pthread - found<br>\n-- Found Threads: TRUE<br>\n-- Caffe2 protobuf include directory: $&lt;BUILD_INTERFACE:/home/andy/github/pytorch/third_party/protobuf/src&gt;$&lt;INSTALL_INTERFACE:include&gt;<br>\n-- Found Git: /usr/bin/git (found version \"2.14.1\")<br>\n-- The BLAS backend of choice:Eigen<br>\n-- Brace yourself, we are building NNPACK<br>\n-- The ASM compiler identification is GNU<br>\n-- Found assembler: /usr/bin/cc<br>\n-- Found PythonInterp: /home/andy/py3caffe2/bin/python (found version \"3.6.3\")<br>\n-- Check if compiler accepts -pthread<br>\n-- Check if compiler accepts -pthread - yes<br>\n-- Caffe2: Found gflags with new-style gflags target.<br>\n-- Caffe2: Cannot find glog automatically. Using legacy find.<br>\n-- Found glog: /usr/include<br>\n-- Caffe2: Found glog (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)<br>\n-- Failed to find LLVM FileCheck<br>\n-- git Version: v0.0.0-dirty<br>\n-- Version: 0.0.0<br>\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11<br>\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WALL<br>\n-- Performing Test HAVE_CXX_FLAG_WALL - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WEXTRA<br>\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WSHADOW<br>\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WERROR<br>\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success<br>\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC<br>\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success<br>\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS<br>\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32<br>\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed<br>\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL<br>\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success<br>\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING<br>\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING<br>\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success<br>\n-- Performing Test HAVE_CXX_FLAG_WD654<br>\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed<br>\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY<br>\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed<br>\n-- Performing Test HAVE_CXX_FLAG_COVERAGE<br>\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success<br>\n-- Performing Test HAVE_STD_REGEX<br>\n-- Performing Test HAVE_STD_REGEX<br>\n-- Performing Test HAVE_STD_REGEX -- success<br>\n-- Performing Test HAVE_GNU_POSIX_REGEX<br>\n-- Performing Test HAVE_GNU_POSIX_REGEX<br>\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile<br>\n-- Performing Test HAVE_POSIX_REGEX<br>\n-- Performing Test HAVE_POSIX_REGEX<br>\n-- Performing Test HAVE_POSIX_REGEX -- success<br>\n-- Performing Test HAVE_STEADY_CLOCK<br>\n-- Performing Test HAVE_STEADY_CLOCK<br>\n-- Performing Test HAVE_STEADY_CLOCK -- success<br>\n-- Found LMDB: /usr/include<br>\n-- Found lmdb    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/liblmdb.so)<br>\n-- Found LevelDB: /usr/include<br>\n-- Found LevelDB (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libleveldb.so)<br>\n-- Found Snappy: /usr/include<br>\n-- Found Snappy  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libsnappy.so)<br>\n-- Found Numa: /usr/include<br>\n-- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)<br>\n-- OpenCV found (/usr/local/share/OpenCV)<br>\nCMake Warning at cmake/Dependencies.cmake:289 (find_package):<br>\nBy not providing \"FindEigen3.cmake\" in CMAKE_MODULE_PATH this project has<br>\nasked CMake to find a package configuration file provided by \"Eigen3\", but<br>\nCMake did not find one.</p>\n<p>Could not find a package configuration file provided by \"Eigen3\" with any<br>\nof the following names:</p>\n<pre><code>Eigen3Config.cmake\neigen3-config.cmake\n</code></pre>\n<p>Add the installation prefix of \"Eigen3\" to CMAKE_PREFIX_PATH or set<br>\n\"Eigen3_DIR\" to a directory containing one of the above files.  If \"Eigen3\"<br>\nprovides a separate development package or SDK, be sure it has been<br>\ninstalled.<br>\nCall Stack (most recent call first):<br>\nCMakeLists.txt:182 (include)</p>\n<p>-- Did not find system Eigen. Using third party subdirectory.<br>\n-- Found PythonInterp: /home/andy/py3caffe2/bin/python (found suitable version \"3.6.3\", minimum required is \"2.7\")<br>\n-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython2.7.so (found suitable version \"2.7.14\", minimum required is \"2.7\")<br>\n-- Found NumPy: /home/andy/py3caffe2/lib/python3.6/site-packages/numpy/core/include (found version \"1.14.3\")<br>\n-- NumPy ver. 1.14.3 found (include: /home/andy/py3caffe2/lib/python3.6/site-packages/numpy/core/include)<br>\n-- Could NOT find pybind11 (missing: pybind11_INCLUDE_DIR)<br>\n-- Found MPI_C: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so<br>\n-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so<br>\n-- MPI support found<br>\n-- MPI compile flags:<br>\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include<br>\n-- MPI LINK flags path:<br>\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so<br>\nCMake Warning at cmake/Dependencies.cmake:343 (message):<br>\nOpenMPI found, but it is not built with CUDA support.<br>\nCall Stack (most recent call first):<br>\nCMakeLists.txt:182 (include)</p>\n<p>-- Found CUDA: /usr/local/cuda-9.1 (found suitable version \"9.1\", minimum required is \"7.0\")<br>\n-- Found CUDNN: /usr/local/cuda-9.1/include<br>\n-- Caffe2: CUDA detected: 9.1<br>\n-- Found cuDNN: v7.1.2  (include: /usr/local/cuda-9.1/include, library: /usr/local/cuda-9.1/lib64/libcudnn.so)<br>\n-- Automatic GPU detection returned 6.1.<br>\n-- Added CUDA NVCC flags for: sm_61<br>\n-- Could NOT find NCCL (missing: NCCL_INCLUDE_DIRS NCCL_LIBRARIES)<br>\n-- Could NOT find CUB (missing: CUB_INCLUDE_DIR)<br>\n-- Could NOT find Gloo (missing: Gloo_INCLUDE_DIR Gloo_LIBRARY)<br>\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include<br>\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so<br>\n-- CUDA detected: 9.1<br>\n-- Found libcuda: /usr/lib/x86_64-linux-gnu/libcuda.so<br>\n-- Found libnvrtc: /usr/local/cuda-9.1/lib64/libnvrtc.so<br>\n-- Found nccl: /home/andy/github/pytorch/third_party/nccl/build/include<br>\nCMake Warning at cmake/Dependencies.cmake:486 (message):<br>\nmobile opengl is only used in android or ios builds.<br>\nCall Stack (most recent call first):<br>\nCMakeLists.txt:182 (include)</p>\n<p>CMake Warning at cmake/Dependencies.cmake:562 (message):<br>\nMetal is only used in ios builds.<br>\nCall Stack (most recent call first):<br>\nCMakeLists.txt:182 (include)</p>\n<p>-- GCC 6.4.0: Adding gcc and gcc_s libs to link line<br>\n-- Include NCCL operators<br>\n-- Excluding ideep operators as we are not using ideep<br>\n-- Including image processing operators<br>\n-- Excluding video processing operators due to no opencv<br>\n-- Excluding mkl operators as we are not using mkl<br>\n-- Include Observer library<br>\n-- Using lib/python3.6/site-packages as python relative installation path<br>\n-- Automatically generating missing <strong>init</strong>.py files.<br>\nCMake Warning at CMakeLists.txt:310 (message):<br>\nGenerated cmake files are only fully tested if one builds with system glog,<br>\ngflags, and protobuf.  Other settings may generate files that are not well<br>\ntested.</p>\n<h2>--<br>\n-- ******** Summary ********<br>\n-- General:<br>\n--   CMake version         : 3.9.1<br>\n--   CMake command         : /usr/bin/cmake<br>\n--   Git version           : v0.1.11-8501-g549b4069b-dirty<br>\n--   System                : Linux<br>\n--   C++ compiler          : /usr/bin/c++<br>\n--   C++ compiler version  : 6.4.0<br>\n--   BLAS                  : Eigen<br>\n--   CXX flags             :  -fvisibility-inlines-hidden -DONNX_NAMESPACE=onnx_c2 -O2 -fPIC -Wno-narrowing -Wno-invalid-partial-specialization<br>\n--   Build type            : Release<br>\n--   Compile definitions   :</h2>\n<p>--   BUILD_CAFFE2          : ON<br>\n--   BUILD_ATEN            : OFF<br>\n--   BUILD_BINARY          : ON<br>\n--   BUILD_CUSTOM_PROTOBUF : ON<br>\n--     Link local protobuf : ON<br>\n--   BUILD_DOCS            : OFF<br>\n--   BUILD_PYTHON          : ON<br>\n--     Python version      : 2.7.14<br>\n--     Python includes     : /usr/include/python2.7<br>\n--   BUILD_SHARED_LIBS     : ON<br>\n--   BUILD_TEST            : ON<br>\n--   USE_ASAN              : OFF<br>\n--   USE_CUDA              : ON<br>\n--     CUDA static link    : OFF<br>\n--     USE_CUDNN           : ON<br>\n--     CUDA version        : 9.1<br>\n--     cuDNN version       : 7.1.2<br>\n--     CUDA root directory : /usr/local/cuda-9.1<br>\n--     CUDA library        : /usr/lib/x86_64-linux-gnu/libcuda.so<br>\n--     cudart library      : /usr/local/cuda-9.1/lib64/libcudart_static.a;-pthread;dl;/usr/lib/x86_64-linux-gnu/librt.so<br>\n--     cublas library      : /usr/local/cuda-9.1/lib64/libcublas.so;/usr/local/cuda-9.1/lib64/libcublas_device.a<br>\n--     curand library      : /usr/local/cuda-9.1/lib64/libcurand.so<br>\n--     CuDNN library       : /usr/local/cuda-9.1/lib64/libcudnn.so<br>\n--     nvrtc               : /usr/local/cuda-9.1/lib64/libnvrtc.so<br>\n--     CUDA include path   : /usr/local/cuda-9.1/include<br>\n--     NVCC executable     : /usr/local/cuda-9.1/bin/nvcc<br>\n--     CUDA host compiler  : /usr/bin/cc<br>\n--     USE_TENSORRT        : OFF<br>\n--   USE_EIGEN_FOR_BLAS    : ON<br>\n--   USE_FFMPEG            : OFF<br>\n--   USE_GFLAGS            : ON<br>\n--   USE_GLOG              : ON<br>\n--   USE_GLOO              : ON<br>\n--     USE_GLOO_IBVERBS    : OFF<br>\n--   USE_LEVELDB           : ON<br>\n--     LevelDB version     : 1.20<br>\n--     Snappy version      : ..<br>\n--   USE_LITE_PROTO        : OFF<br>\n--   USE_LMDB              : ON<br>\n--     LMDB version        : 0.9.21<br>\n--   USE_METAL             : OFF<br>\n--   USE_MKL               :<br>\n--   USE_MOBILE_OPENGL     : OFF<br>\n--   USE_MPI               : ON<br>\n--   USE_NCCL              : ON<br>\n--     USE_SYSTEM_NCCL     : OFF<br>\n--   USE_NERVANA_GPU       : OFF<br>\n--   USE_NNPACK            : ON<br>\n--   USE_OBSERVERS         : ON<br>\n--   USE_OPENCL            : OFF<br>\n--   USE_OPENCV            : ON<br>\n--     OpenCV version      : 3.4.0<br>\n--   USE_OPENMP            : OFF<br>\n--   USE_PROF              : OFF<br>\n--   USE_REDIS             : OFF<br>\n--   USE_ROCKSDB           : OFF<br>\n--   USE_ZMQ               : OFF<br>\n-- Configuring done<br>\n-- Generating done<br>\n-- Build files have been written to: /home/andy/github/pytorch/build<br>\n`<br>\nIt's that caff2 now don't support cuda 9.1 + cuDNN 7.1 or another bug?hold for solve this bug.</p>", "body_text": "Issue description\nI had install CUDA 9.1 and cuDNN 7.1 before complie caffe2, and I don't want to update CUDA version, so when I complied caffe2 on ubuntu 17.10 + CUDA 9.1 + cuDNN 7.1 met that error:\n`/tmp/tmpxft_00000939_00000000-5_abs_op.cudafe1.stub.c:20:27:   required from here\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\nreturn  _and<_not<is_same<tuple<_Elements...>,\n^\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\nstruct is_convertible\n^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<long unsigned int, long unsigned int, long unsigned int>&; bool  = true; _Elements = {long unsigned int&, long unsigned int&, long unsigned int&}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<long unsigned int, long unsigned int, long unsigned int>&&; bool  = true; _Elements = {long unsigned int&, long unsigned int&, long unsigned int&}]\u2019:\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), long unsigned int&, long unsigned int&, long unsigned int&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type  > constexpr std::tuple<  >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {long unsigned int, long unsigned int, long unsigned int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), long unsigned int&, long unsigned int&, long unsigned int&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type  = ]\u2019\n/home/andy/github/pytorch/caffe2/operators/elementwise_op.h:226:26:   required from \u2018bool caffe2::BinaryElementwiseOp<InputTypes, Context, Functor, TypeMap>::DoRunWithType() [with T = float; InputTypes = caffe2::TensorTypes; Context = caffe2::CUDAContext; Functor = caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor; TypeMap = caffe2::SameTypeAsInput]\u2019\n/home/andy/github/pytorch/caffe2/core/operator.h:682:80:   required from \u2018static bool caffe2::DispatchHelper<caffe2::TensorTypes<FirstType, Types ...>, ExtraArgs ...>::call(Op*, const caffe2::TypeMeta&) [with Op = caffe2::BinaryElementwiseOp<caffe2::TensorTypes, caffe2::CUDAContext, caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor >; FirstType = float; Types = {}; ExtraArgs = {}]\u2019\n/home/andy/github/pytorch/caffe2/core/operator.h:684:47:   required from \u2018static bool caffe2::DispatchHelper<caffe2::TensorTypes<FirstType, Types ...>, ExtraArgs ...>::call(Op*, const caffe2::Tensor&) [with Op = caffe2::BinaryElementwiseOp<caffe2::TensorTypes, caffe2::CUDAContext, caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor >; Context = caffe2::CUDAContext; FirstType = float; Types = {}; ExtraArgs = {}]\u2019\n/home/andy/github/pytorch/caffe2/operators/elementwise_op.h:200:42:   required from \u2018bool caffe2::BinaryElementwiseOp<InputTypes, Context, Functor, TypeMap>::RunOnDevice() [with InputTypes = caffe2::TensorTypes; Context = caffe2::CUDAContext; Functor = caffe2::WithoutBroadcastcaffe2::AbsGradientCUDAFunctor; TypeMap = caffe2::SameTypeAsInput]\u2019\n/tmp/tmpxft_00000939_00000000-5_abs_op.cudafe1.stub.c:20:27:   required from here\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\nreturn  _and<_not<is_same<tuple<_Elements...>,\n^\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\nstruct is_convertible\n^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<long unsigned int, long unsigned int, long unsigned int>&&; bool  = true; _Elements = {long unsigned int&, long unsigned int&, long unsigned int&}]\u2019 not a return-statement\n}\n^\nCMake Error at caffe2_gpu_generated_abs_op.cu.o.Release.cmake:275 (message):\nError generating file\n/home/andy/github/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/operators/./caffe2_gpu_generated_abs_op.cu.o\ncaffe2/CMakeFiles/caffe2_gpu.dir/build.make:91: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/operators/caffe2_gpu_generated_abs_op.cu.o' failed\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/operators/caffe2_gpu_generated_abs_op.cu.o] Error 1\nCMakeFiles/Makefile2:2524: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/all' failed\nmake[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2\nMakefile:140: recipe for target 'all' failed\nmake: *** [all] Error 2`\nSystem Info\n\nPyTorch or Caffe2: caffe2\nHow you installed PyTorch (conda, pip, source): source\nBuild command you used (if compiling from source): https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=compile#install-with-gpu-support\nOS: linux ubuntu 17.10 64bit\nPyTorch version: latest\nPython version: 3.6.3\nCUDA/cuDNN version: CUDA 9.1 + cuDNN 7.1\nGPU models and configuration: gtx 1060 6GB\nGCC version (if compiling from source): GCC 6.4.0\nCMake version: 3.9.1\nVersions of any other relevant libraries:\n\ncmake .. log is below:\n`(py3caffe2) andy@andy:~/github/pytorch/build$ cmake ..\n-- The CXX compiler identification is GNU 6.4.0\n-- The C compiler identification is GNU 6.4.0\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Performing Test CAFFE2_LONG_IS_INT32_OR_64\n-- Performing Test CAFFE2_LONG_IS_INT32_OR_64 - Success\n-- Does not need to define long separately.\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\n-- std::exception_ptr is supported.\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE - Success\n-- NUMA is available\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Success\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Success\n-- Current compiler supports avx2 extention. Will build perfkernels.\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success\n-- Build type not set - defaulting to Release\n-- Building using own protobuf under third_party per request.\n-- Use custom protobuf build.\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/home/andy/github/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\n-- Found Git: /usr/bin/git (found version \"2.14.1\")\n-- The BLAS backend of choice:Eigen\n-- Brace yourself, we are building NNPACK\n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Found PythonInterp: /home/andy/py3caffe2/bin/python (found version \"3.6.3\")\n-- Check if compiler accepts -pthread\n-- Check if compiler accepts -pthread - yes\n-- Caffe2: Found gflags with new-style gflags target.\n-- Caffe2: Cannot find glog automatically. Using legacy find.\n-- Found glog: /usr/include\n-- Caffe2: Found glog (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\n-- Failed to find LLVM FileCheck\n-- git Version: v0.0.0-dirty\n-- Version: 0.0.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- success\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Found LMDB: /usr/include\n-- Found lmdb    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/liblmdb.so)\n-- Found LevelDB: /usr/include\n-- Found LevelDB (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libleveldb.so)\n-- Found Snappy: /usr/include\n-- Found Snappy  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libsnappy.so)\n-- Found Numa: /usr/include\n-- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)\n-- OpenCV found (/usr/local/share/OpenCV)\nCMake Warning at cmake/Dependencies.cmake:289 (find_package):\nBy not providing \"FindEigen3.cmake\" in CMAKE_MODULE_PATH this project has\nasked CMake to find a package configuration file provided by \"Eigen3\", but\nCMake did not find one.\nCould not find a package configuration file provided by \"Eigen3\" with any\nof the following names:\nEigen3Config.cmake\neigen3-config.cmake\n\nAdd the installation prefix of \"Eigen3\" to CMAKE_PREFIX_PATH or set\n\"Eigen3_DIR\" to a directory containing one of the above files.  If \"Eigen3\"\nprovides a separate development package or SDK, be sure it has been\ninstalled.\nCall Stack (most recent call first):\nCMakeLists.txt:182 (include)\n-- Did not find system Eigen. Using third party subdirectory.\n-- Found PythonInterp: /home/andy/py3caffe2/bin/python (found suitable version \"3.6.3\", minimum required is \"2.7\")\n-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython2.7.so (found suitable version \"2.7.14\", minimum required is \"2.7\")\n-- Found NumPy: /home/andy/py3caffe2/lib/python3.6/site-packages/numpy/core/include (found version \"1.14.3\")\n-- NumPy ver. 1.14.3 found (include: /home/andy/py3caffe2/lib/python3.6/site-packages/numpy/core/include)\n-- Could NOT find pybind11 (missing: pybind11_INCLUDE_DIR)\n-- Found MPI_C: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\n-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\n-- MPI support found\n-- MPI compile flags:\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\n-- MPI LINK flags path:\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\nCMake Warning at cmake/Dependencies.cmake:343 (message):\nOpenMPI found, but it is not built with CUDA support.\nCall Stack (most recent call first):\nCMakeLists.txt:182 (include)\n-- Found CUDA: /usr/local/cuda-9.1 (found suitable version \"9.1\", minimum required is \"7.0\")\n-- Found CUDNN: /usr/local/cuda-9.1/include\n-- Caffe2: CUDA detected: 9.1\n-- Found cuDNN: v7.1.2  (include: /usr/local/cuda-9.1/include, library: /usr/local/cuda-9.1/lib64/libcudnn.so)\n-- Automatic GPU detection returned 6.1.\n-- Added CUDA NVCC flags for: sm_61\n-- Could NOT find NCCL (missing: NCCL_INCLUDE_DIRS NCCL_LIBRARIES)\n-- Could NOT find CUB (missing: CUB_INCLUDE_DIR)\n-- Could NOT find Gloo (missing: Gloo_INCLUDE_DIR Gloo_LIBRARY)\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\n-- CUDA detected: 9.1\n-- Found libcuda: /usr/lib/x86_64-linux-gnu/libcuda.so\n-- Found libnvrtc: /usr/local/cuda-9.1/lib64/libnvrtc.so\n-- Found nccl: /home/andy/github/pytorch/third_party/nccl/build/include\nCMake Warning at cmake/Dependencies.cmake:486 (message):\nmobile opengl is only used in android or ios builds.\nCall Stack (most recent call first):\nCMakeLists.txt:182 (include)\nCMake Warning at cmake/Dependencies.cmake:562 (message):\nMetal is only used in ios builds.\nCall Stack (most recent call first):\nCMakeLists.txt:182 (include)\n-- GCC 6.4.0: Adding gcc and gcc_s libs to link line\n-- Include NCCL operators\n-- Excluding ideep operators as we are not using ideep\n-- Including image processing operators\n-- Excluding video processing operators due to no opencv\n-- Excluding mkl operators as we are not using mkl\n-- Include Observer library\n-- Using lib/python3.6/site-packages as python relative installation path\n-- Automatically generating missing init.py files.\nCMake Warning at CMakeLists.txt:310 (message):\nGenerated cmake files are only fully tested if one builds with system glog,\ngflags, and protobuf.  Other settings may generate files that are not well\ntested.\n--\n-- ******** Summary ********\n-- General:\n--   CMake version         : 3.9.1\n--   CMake command         : /usr/bin/cmake\n--   Git version           : v0.1.11-8501-g549b4069b-dirty\n--   System                : Linux\n--   C++ compiler          : /usr/bin/c++\n--   C++ compiler version  : 6.4.0\n--   BLAS                  : Eigen\n--   CXX flags             :  -fvisibility-inlines-hidden -DONNX_NAMESPACE=onnx_c2 -O2 -fPIC -Wno-narrowing -Wno-invalid-partial-specialization\n--   Build type            : Release\n--   Compile definitions   :\n--   BUILD_CAFFE2          : ON\n--   BUILD_ATEN            : OFF\n--   BUILD_BINARY          : ON\n--   BUILD_CUSTOM_PROTOBUF : ON\n--     Link local protobuf : ON\n--   BUILD_DOCS            : OFF\n--   BUILD_PYTHON          : ON\n--     Python version      : 2.7.14\n--     Python includes     : /usr/include/python2.7\n--   BUILD_SHARED_LIBS     : ON\n--   BUILD_TEST            : ON\n--   USE_ASAN              : OFF\n--   USE_CUDA              : ON\n--     CUDA static link    : OFF\n--     USE_CUDNN           : ON\n--     CUDA version        : 9.1\n--     cuDNN version       : 7.1.2\n--     CUDA root directory : /usr/local/cuda-9.1\n--     CUDA library        : /usr/lib/x86_64-linux-gnu/libcuda.so\n--     cudart library      : /usr/local/cuda-9.1/lib64/libcudart_static.a;-pthread;dl;/usr/lib/x86_64-linux-gnu/librt.so\n--     cublas library      : /usr/local/cuda-9.1/lib64/libcublas.so;/usr/local/cuda-9.1/lib64/libcublas_device.a\n--     curand library      : /usr/local/cuda-9.1/lib64/libcurand.so\n--     CuDNN library       : /usr/local/cuda-9.1/lib64/libcudnn.so\n--     nvrtc               : /usr/local/cuda-9.1/lib64/libnvrtc.so\n--     CUDA include path   : /usr/local/cuda-9.1/include\n--     NVCC executable     : /usr/local/cuda-9.1/bin/nvcc\n--     CUDA host compiler  : /usr/bin/cc\n--     USE_TENSORRT        : OFF\n--   USE_EIGEN_FOR_BLAS    : ON\n--   USE_FFMPEG            : OFF\n--   USE_GFLAGS            : ON\n--   USE_GLOG              : ON\n--   USE_GLOO              : ON\n--     USE_GLOO_IBVERBS    : OFF\n--   USE_LEVELDB           : ON\n--     LevelDB version     : 1.20\n--     Snappy version      : ..\n--   USE_LITE_PROTO        : OFF\n--   USE_LMDB              : ON\n--     LMDB version        : 0.9.21\n--   USE_METAL             : OFF\n--   USE_MKL               :\n--   USE_MOBILE_OPENGL     : OFF\n--   USE_MPI               : ON\n--   USE_NCCL              : ON\n--     USE_SYSTEM_NCCL     : OFF\n--   USE_NERVANA_GPU       : OFF\n--   USE_NNPACK            : ON\n--   USE_OBSERVERS         : ON\n--   USE_OPENCL            : OFF\n--   USE_OPENCV            : ON\n--     OpenCV version      : 3.4.0\n--   USE_OPENMP            : OFF\n--   USE_PROF              : OFF\n--   USE_REDIS             : OFF\n--   USE_ROCKSDB           : OFF\n--   USE_ZMQ               : OFF\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/andy/github/pytorch/build\n`\nIt's that caff2 now don't support cuda 9.1 + cuDNN 7.1 or another bug?hold for solve this bug.", "body": "## Issue description\r\n\r\nI had install CUDA 9.1 and cuDNN 7.1 before complie caffe2, and I don't want to update CUDA version, so when I complied caffe2 on ubuntu 17.10 + CUDA 9.1 + cuDNN 7.1 met that error:\r\n\r\n`/tmp/tmpxft_00000939_00000000-5_abs_op.cudafe1.stub.c:20:27:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<long unsigned int, long unsigned int, long unsigned int>&; bool <anonymous> = true; _Elements = {long unsigned int&, long unsigned int&, long unsigned int&}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<long unsigned int, long unsigned int, long unsigned int>&&; bool <anonymous> = true; _Elements = {long unsigned int&, long unsigned int&, long unsigned int&}]\u2019:\r\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), long unsigned int&, long unsigned int&, long unsigned int&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {long unsigned int, long unsigned int, long unsigned int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), long unsigned int&, long unsigned int&, long unsigned int&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), long unsigned int&, long unsigned int&, long unsigned int&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]\u2019\r\n/home/andy/github/pytorch/caffe2/operators/elementwise_op.h:226:26:   required from \u2018bool caffe2::BinaryElementwiseOp<InputTypes, Context, Functor, TypeMap>::DoRunWithType() [with T = float; InputTypes = caffe2::TensorTypes<float>; Context = caffe2::CUDAContext; Functor = caffe2::WithoutBroadcast<caffe2::AbsGradientCUDAFunctor>; TypeMap = caffe2::SameTypeAsInput]\u2019\r\n/home/andy/github/pytorch/caffe2/core/operator.h:682:80:   required from \u2018static bool caffe2::DispatchHelper<caffe2::TensorTypes<FirstType, Types ...>, ExtraArgs ...>::call(Op*, const caffe2::TypeMeta&) [with Op = caffe2::BinaryElementwiseOp<caffe2::TensorTypes<float>, caffe2::CUDAContext, caffe2::WithoutBroadcast<caffe2::AbsGradientCUDAFunctor> >; FirstType = float; Types = {}; ExtraArgs = {}]\u2019\r\n/home/andy/github/pytorch/caffe2/core/operator.h:684:47:   required from \u2018static bool caffe2::DispatchHelper<caffe2::TensorTypes<FirstType, Types ...>, ExtraArgs ...>::call(Op*, const caffe2::Tensor<Context>&) [with Op = caffe2::BinaryElementwiseOp<caffe2::TensorTypes<float>, caffe2::CUDAContext, caffe2::WithoutBroadcast<caffe2::AbsGradientCUDAFunctor> >; Context = caffe2::CUDAContext; FirstType = float; Types = {}; ExtraArgs = {}]\u2019\r\n/home/andy/github/pytorch/caffe2/operators/elementwise_op.h:200:42:   required from \u2018bool caffe2::BinaryElementwiseOp<InputTypes, Context, Functor, TypeMap>::RunOnDevice() [with InputTypes = caffe2::TensorTypes<float>; Context = caffe2::CUDAContext; Functor = caffe2::WithoutBroadcast<caffe2::AbsGradientCUDAFunctor>; TypeMap = caffe2::SameTypeAsInput]\u2019\r\n/tmp/tmpxft_00000939_00000000-5_abs_op.cudafe1.stub.c:20:27:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<long unsigned int, long unsigned int, long unsigned int>&&; bool <anonymous> = true; _Elements = {long unsigned int&, long unsigned int&, long unsigned int&}]\u2019 not a return-statement\r\n     }\r\n ^\r\nCMake Error at caffe2_gpu_generated_abs_op.cu.o.Release.cmake:275 (message):\r\n  Error generating file\r\n  /home/andy/github/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/operators/./caffe2_gpu_generated_abs_op.cu.o\r\n\r\n\r\ncaffe2/CMakeFiles/caffe2_gpu.dir/build.make:91: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/operators/caffe2_gpu_generated_abs_op.cu.o' failed\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/operators/caffe2_gpu_generated_abs_op.cu.o] Error 1\r\nCMakeFiles/Makefile2:2524: recipe for target 'caffe2/CMakeFiles/caffe2_gpu.dir/all' failed\r\nmake[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2\r\nMakefile:140: recipe for target 'all' failed\r\nmake: *** [all] Error 2`\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: caffe2\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Build command you used (if compiling from source): https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=compile#install-with-gpu-support\r\n- OS: linux ubuntu 17.10 64bit\r\n- PyTorch version: latest\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: CUDA 9.1 + cuDNN 7.1\r\n- GPU models and configuration: gtx 1060 6GB\r\n- GCC version (if compiling from source): GCC 6.4.0\r\n- CMake version: 3.9.1\r\n- Versions of any other relevant libraries:\r\n\r\ncmake .. log is below:\r\n`(py3caffe2) andy@andy:~/github/pytorch/build$ cmake ..\r\n-- The CXX compiler identification is GNU 6.4.0\r\n-- The C compiler identification is GNU 6.4.0\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Performing Test CAFFE2_LONG_IS_INT32_OR_64\r\n-- Performing Test CAFFE2_LONG_IS_INT32_OR_64 - Success\r\n-- Does not need to define long separately.\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\r\n-- std::exception_ptr is supported.\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE - Success\r\n-- NUMA is available\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Success\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Success\r\n-- Current compiler supports avx2 extention. Will build perfkernels.\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success\r\n-- Build type not set - defaulting to Release\r\n-- Building using own protobuf under third_party per request.\r\n-- Use custom protobuf build.\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/home/andy/github/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n-- Found Git: /usr/bin/git (found version \"2.14.1\") \r\n-- The BLAS backend of choice:Eigen\r\n-- Brace yourself, we are building NNPACK\r\n-- The ASM compiler identification is GNU\r\n-- Found assembler: /usr/bin/cc\r\n-- Found PythonInterp: /home/andy/py3caffe2/bin/python (found version \"3.6.3\") \r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- Caffe2: Found gflags with new-style gflags target.\r\n-- Caffe2: Cannot find glog automatically. Using legacy find.\r\n-- Found glog: /usr/include  \r\n-- Caffe2: Found glog (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)\r\n-- Failed to find LLVM FileCheck\r\n-- git Version: v0.0.0-dirty\r\n-- Version: 0.0.0\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\r\n-- Performing Test HAVE_CXX_FLAG_WALL\r\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\r\n-- Performing Test HAVE_CXX_FLAG_WERROR\r\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WD654\r\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\r\n-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX -- success\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\r\n-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX -- success\r\n-- Performing Test HAVE_STEADY_CLOCK\r\n-- Performing Test HAVE_STEADY_CLOCK\r\n-- Performing Test HAVE_STEADY_CLOCK -- success\r\n-- Found LMDB: /usr/include  \r\n-- Found lmdb    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/liblmdb.so)\r\n-- Found LevelDB: /usr/include  \r\n-- Found LevelDB (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libleveldb.so)\r\n-- Found Snappy: /usr/include  \r\n-- Found Snappy  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libsnappy.so)\r\n-- Found Numa: /usr/include  \r\n-- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)\r\n-- OpenCV found (/usr/local/share/OpenCV)\r\nCMake Warning at cmake/Dependencies.cmake:289 (find_package):\r\n  By not providing \"FindEigen3.cmake\" in CMAKE_MODULE_PATH this project has\r\n  asked CMake to find a package configuration file provided by \"Eigen3\", but\r\n  CMake did not find one.\r\n\r\n  Could not find a package configuration file provided by \"Eigen3\" with any\r\n  of the following names:\r\n\r\n    Eigen3Config.cmake\r\n    eigen3-config.cmake\r\n\r\n  Add the installation prefix of \"Eigen3\" to CMAKE_PREFIX_PATH or set\r\n  \"Eigen3_DIR\" to a directory containing one of the above files.  If \"Eigen3\"\r\n  provides a separate development package or SDK, be sure it has been\r\n  installed.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:182 (include)\r\n\r\n\r\n-- Did not find system Eigen. Using third party subdirectory.\r\n-- Found PythonInterp: /home/andy/py3caffe2/bin/python (found suitable version \"3.6.3\", minimum required is \"2.7\") \r\n-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython2.7.so (found suitable version \"2.7.14\", minimum required is \"2.7\") \r\n-- Found NumPy: /home/andy/py3caffe2/lib/python3.6/site-packages/numpy/core/include (found version \"1.14.3\") \r\n-- NumPy ver. 1.14.3 found (include: /home/andy/py3caffe2/lib/python3.6/site-packages/numpy/core/include)\r\n-- Could NOT find pybind11 (missing: pybind11_INCLUDE_DIR) \r\n-- Found MPI_C: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so  \r\n-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so  \r\n-- MPI support found\r\n-- MPI compile flags: \r\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\r\n-- MPI LINK flags path: \r\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\nCMake Warning at cmake/Dependencies.cmake:343 (message):\r\n  OpenMPI found, but it is not built with CUDA support.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:182 (include)\r\n\r\n\r\n-- Found CUDA: /usr/local/cuda-9.1 (found suitable version \"9.1\", minimum required is \"7.0\") \r\n-- Found CUDNN: /usr/local/cuda-9.1/include  \r\n-- Caffe2: CUDA detected: 9.1\r\n-- Found cuDNN: v7.1.2  (include: /usr/local/cuda-9.1/include, library: /usr/local/cuda-9.1/lib64/libcudnn.so)\r\n-- Automatic GPU detection returned 6.1.\r\n-- Added CUDA NVCC flags for: sm_61\r\n-- Could NOT find NCCL (missing: NCCL_INCLUDE_DIRS NCCL_LIBRARIES) \r\n-- Could NOT find CUB (missing: CUB_INCLUDE_DIR) \r\n-- Could NOT find Gloo (missing: Gloo_INCLUDE_DIR Gloo_LIBRARY) \r\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\r\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n-- CUDA detected: 9.1\r\n-- Found libcuda: /usr/lib/x86_64-linux-gnu/libcuda.so\r\n-- Found libnvrtc: /usr/local/cuda-9.1/lib64/libnvrtc.so\r\n-- Found nccl: /home/andy/github/pytorch/third_party/nccl/build/include  \r\nCMake Warning at cmake/Dependencies.cmake:486 (message):\r\n  mobile opengl is only used in android or ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:182 (include)\r\n\r\n\r\nCMake Warning at cmake/Dependencies.cmake:562 (message):\r\n  Metal is only used in ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:182 (include)\r\n\r\n\r\n-- GCC 6.4.0: Adding gcc and gcc_s libs to link line\r\n-- Include NCCL operators\r\n-- Excluding ideep operators as we are not using ideep\r\n-- Including image processing operators\r\n-- Excluding video processing operators due to no opencv\r\n-- Excluding mkl operators as we are not using mkl\r\n-- Include Observer library\r\n-- Using lib/python3.6/site-packages as python relative installation path\r\n-- Automatically generating missing __init__.py files.\r\nCMake Warning at CMakeLists.txt:310 (message):\r\n  Generated cmake files are only fully tested if one builds with system glog,\r\n  gflags, and protobuf.  Other settings may generate files that are not well\r\n  tested.\r\n\r\n\r\n-- \r\n-- ******** Summary ********\r\n-- General:\r\n--   CMake version         : 3.9.1\r\n--   CMake command         : /usr/bin/cmake\r\n--   Git version           : v0.1.11-8501-g549b4069b-dirty\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/bin/c++\r\n--   C++ compiler version  : 6.4.0\r\n--   BLAS                  : Eigen\r\n--   CXX flags             :  -fvisibility-inlines-hidden -DONNX_NAMESPACE=onnx_c2 -O2 -fPIC -Wno-narrowing -Wno-invalid-partial-specialization\r\n--   Build type            : Release\r\n--   Compile definitions   : \r\n-- \r\n--   BUILD_CAFFE2          : ON\r\n--   BUILD_ATEN            : OFF\r\n--   BUILD_BINARY          : ON\r\n--   BUILD_CUSTOM_PROTOBUF : ON\r\n--     Link local protobuf : ON\r\n--   BUILD_DOCS            : OFF\r\n--   BUILD_PYTHON          : ON\r\n--     Python version      : 2.7.14\r\n--     Python includes     : /usr/include/python2.7\r\n--   BUILD_SHARED_LIBS     : ON\r\n--   BUILD_TEST            : ON\r\n--   USE_ASAN              : OFF\r\n--   USE_CUDA              : ON\r\n--     CUDA static link    : OFF\r\n--     USE_CUDNN           : ON\r\n--     CUDA version        : 9.1\r\n--     cuDNN version       : 7.1.2\r\n--     CUDA root directory : /usr/local/cuda-9.1\r\n--     CUDA library        : /usr/lib/x86_64-linux-gnu/libcuda.so\r\n--     cudart library      : /usr/local/cuda-9.1/lib64/libcudart_static.a;-pthread;dl;/usr/lib/x86_64-linux-gnu/librt.so\r\n--     cublas library      : /usr/local/cuda-9.1/lib64/libcublas.so;/usr/local/cuda-9.1/lib64/libcublas_device.a\r\n--     curand library      : /usr/local/cuda-9.1/lib64/libcurand.so\r\n--     CuDNN library       : /usr/local/cuda-9.1/lib64/libcudnn.so\r\n--     nvrtc               : /usr/local/cuda-9.1/lib64/libnvrtc.so\r\n--     CUDA include path   : /usr/local/cuda-9.1/include\r\n--     NVCC executable     : /usr/local/cuda-9.1/bin/nvcc\r\n--     CUDA host compiler  : /usr/bin/cc\r\n--     USE_TENSORRT        : OFF\r\n--   USE_EIGEN_FOR_BLAS    : ON\r\n--   USE_FFMPEG            : OFF\r\n--   USE_GFLAGS            : ON\r\n--   USE_GLOG              : ON\r\n--   USE_GLOO              : ON\r\n--     USE_GLOO_IBVERBS    : OFF\r\n--   USE_LEVELDB           : ON\r\n--     LevelDB version     : 1.20\r\n--     Snappy version      : ..\r\n--   USE_LITE_PROTO        : OFF\r\n--   USE_LMDB              : ON\r\n--     LMDB version        : 0.9.21\r\n--   USE_METAL             : OFF\r\n--   USE_MKL               : \r\n--   USE_MOBILE_OPENGL     : OFF\r\n--   USE_MPI               : ON\r\n--   USE_NCCL              : ON\r\n--     USE_SYSTEM_NCCL     : OFF\r\n--   USE_NERVANA_GPU       : OFF\r\n--   USE_NNPACK            : ON\r\n--   USE_OBSERVERS         : ON\r\n--   USE_OPENCL            : OFF\r\n--   USE_OPENCV            : ON\r\n--     OpenCV version      : 3.4.0\r\n--   USE_OPENMP            : OFF\r\n--   USE_PROF              : OFF\r\n--   USE_REDIS             : OFF\r\n--   USE_ROCKSDB           : OFF\r\n--   USE_ZMQ               : OFF\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/andy/github/pytorch/build\r\n`\r\nIt's that caff2 now don't support cuda 9.1 + cuDNN 7.1 or another bug?hold for solve this bug."}