{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/143486170", "pull_request_review_id": 68004435, "id": 143486170, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MzQ4NjE3MA==", "diff_hunk": "@@ -580,5 +570,77 @@ def _time(trace_name, name, time=True):\n         print(\"{} {} time: {} ms\".format(trace_name, name, start.elapsed_time(end)))\n \n \n+def verify(model, args, loss_fn=torch.sum, devices=None):\n+    \"\"\"\n+    Verify that a JIT compiled model has the same behavior as its uncompiled\n+    version along with its backwards pass.\n+\n+    This function has side-effects (e.g., it executes your model / saves and loads\n+    parameters), so don't expect the model to come out exactly the same as what\n+    you passed in.\n+\n+    Arguments:\n+        model (compiled torch.nn.Module or function): the module/function to be\n+            verified.  The module/function definition MUST have been decorated with\n+            `@torch.jit.compile`.\n+        args (tuple or Variable): the positional arguments to pass to the\n+            compiled function/module to be verified.  A non-tuple is assumed to\n+            be a single positional argument to be passed to the model.\n+        loss_fn (function, optional): the loss function to be applied to\n+            the output of the model, before backwards is invoked.  By default,\n+            we assume that a model returns a single result, and we :func:`torch.sum`\n+            before calling backwards; if this is inappropriate, you can pass your\n+            own loss function.  Note that if a model returns a tuple of results,\n+            these are passed as separate arguments to `loss_fn`.\n+        devices (iterable of device IDs, optional): the GPU devices which the\n+            compiled module will be run on.  This determines the RNG state we\n+            must save when running both compiled and uncompiled versions of the model.\n+    \"\"\"\n+    # TODO: In principle, we track device information in our trace, so it\n+    # should be possible to check if our execution actually obeyed the 'devices'\n+    # the user provided.\n+\n+    # TODO: Consider adding a utility function to torch.jit to test\n+    # for this case\n+    if not isinstance(model, _CompiledMixin):\n+        raise TypeError(\"Cannot verify an uncompiled module.  Add @torch.jit.compile to compile it\")\n+\n+    if not isinstance(args, tuple):\n+        args = (args,)\n+\n+    saved_args = _clone_inputs(args)\n+    saved_state = copy.deepcopy(model.state_dict())\n+\n+    def run_fwd_bwd(args, force_trace=False):\n+        in_vars, _ = _flatten(args, model.state_dict(keep_vars=True).values())", "path": "torch/jit/__init__.py", "position": 195, "original_position": 193, "commit_id": "672a20149aada1a999056fd6074d3726fe8b1d8d", "original_commit_id": "96f9ed9f65d62c65a59f3047e476605ca8a4590e", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Yes, it appears buffers will get dropped. (Though, I suspect the entire buffer mechanism is broken right now...)", "created_at": "2017-10-09T14:39:51Z", "updated_at": "2018-11-23T15:35:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/2995#discussion_r143486170", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2995", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/143486170"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2995#discussion_r143486170"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2995"}}, "body_html": "<p>Yes, it appears buffers will get dropped. (Though, I suspect the entire buffer mechanism is broken right now...)</p>", "body_text": "Yes, it appears buffers will get dropped. (Though, I suspect the entire buffer mechanism is broken right now...)", "in_reply_to_id": 143410945}