{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/143423073", "pull_request_review_id": 67931149, "id": 143423073, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MzQyMzA3Mw==", "diff_hunk": "@@ -580,5 +570,77 @@ def _time(trace_name, name, time=True):\n         print(\"{} {} time: {} ms\".format(trace_name, name, start.elapsed_time(end)))\n \n \n+def verify(model, args, loss_fn=torch.sum, devices=None):\n+    \"\"\"\n+    Verify that a JIT compiled model has the same behavior as its uncompiled\n+    version along with its backwards pass.\n+\n+    This function has side-effects (e.g., it executes your model / saves and loads\n+    parameters), so don't expect the model to come out exactly the same as what\n+    you passed in.\n+\n+    Arguments:\n+        model (compiled torch.nn.Module or function): the module/function to be\n+            verified.  The module/function definition MUST have been decorated with\n+            `@torch.jit.compile`.\n+        args (tuple or Variable): the positional arguments to pass to the\n+            compiled function/module to be verified.  A non-tuple is assumed to\n+            be a single positional argument to be passed to the model.\n+        loss_fn (function, optional): the loss function to be applied to\n+            the output of the model, before backwards is invoked.  By default,\n+            we assume that a model returns a single result, and we :func:`torch.sum`\n+            before calling backwards; if this is inappropriate, you can pass your\n+            own loss function.  Note that if a model returns a tuple of results,\n+            these are passed as separate arguments to `loss_fn`.\n+        devices (iterable of device IDs, optional): the GPU devices which the\n+            compiled module will be run on.  This determines the RNG state we\n+            must save when running both compiled and uncompiled versions of the model.\n+    \"\"\"\n+    # TODO: In principle, we track device information in our trace, so it\n+    # should be possible to check if our execution actually obeyed the 'devices'\n+    # the user provided.\n+\n+    # TODO: Consider adding a utility function to torch.jit to test\n+    # for this case\n+    if not isinstance(model, _CompiledMixin):\n+        raise TypeError(\"Cannot verify an uncompiled module.  Add @torch.jit.compile to compile it\")\n+\n+    if not isinstance(args, tuple):\n+        args = (args,)\n+\n+    saved_args = _clone_inputs(args)\n+    saved_state = copy.deepcopy(model.state_dict())\n+\n+    def run_fwd_bwd(args, force_trace=False):\n+        in_vars, _ = _flatten(args, model.state_dict(keep_vars=True).values())\n+        # We use a special API to reset the trace and compile it from scratch.\n+        out = model(*args, _force_trace=force_trace)\n+        if not isinstance(out, tuple):\n+            out = (out, )\n+        out_vars, _ = _flatten(out)\n+        saved_outs = [v.data.clone() for v in out_vars]\n+        loss = loss_fn(*out)\n+        grads = torch.autograd.grad([loss], in_vars)\n+        # TODO: I'm not sure if the clone here is necessary but it is safer\n+        saved_grads = [v.data.clone() for v in grads]\n+        return (saved_outs, saved_grads)\n+\n+    with torch.random.fork_rng(devices, _caller=\"torch.jit.verify\"):\n+        uncompiled_outs, uncompiled_grads = run_fwd_bwd(args, force_trace=True)\n+        assert model.has_trace_for(*args)\n+\n+    model.load_state_dict(saved_state)", "path": "torch/jit/__init__.py", "position": 215, "original_position": 210, "commit_id": "672a20149aada1a999056fd6074d3726fe8b1d8d", "original_commit_id": "96f9ed9f65d62c65a59f3047e476605ca8a4590e", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "I don't think this is true anymore. Since https://github.com/pytorch/pytorch/pull/451, `load_state_dict` [performs a `copy_` and not an assignment anymore](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L401), so I think this should be fine, right?", "created_at": "2017-10-09T09:46:22Z", "updated_at": "2018-11-23T15:35:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/2995#discussion_r143423073", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2995", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/143423073"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2995#discussion_r143423073"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2995"}}, "body_html": "<p>I don't think this is true anymore. Since <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"200747041\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/451\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/451/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/451\">#451</a>, <code>load_state_dict</code> <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L401\">performs a <code>copy_</code> and not an assignment anymore</a>, so I think this should be fine, right?</p>", "body_text": "I don't think this is true anymore. Since #451, load_state_dict performs a copy_ and not an assignment anymore, so I think this should be fine, right?", "in_reply_to_id": 143411664}