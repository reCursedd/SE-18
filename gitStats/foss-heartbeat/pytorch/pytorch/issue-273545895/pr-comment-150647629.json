{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150647629", "pull_request_review_id": 76228780, "id": 150647629, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY0NzYyOQ==", "diff_hunk": "@@ -0,0 +1,325 @@\n+#include \"NativeFunctions.h\"\n+#include \"ATen/ATen.h\"\n+#include \"ATen/WrapDimUtils.h\"\n+#include \"ATen/ExpandUtils.h\"\n+\n+namespace at {\n+namespace native {\n+\n+Tensor type_as(const Tensor& self, const Tensor& other) {\n+  return self.toType(other.type());;\n+}\n+\n+Tensor expand_as(const Tensor& self, const Tensor& other) {\n+  return self.expand(other.sizes());\n+}\n+\n+std::vector<Tensor> split(const Tensor& self, int64_t split_size, int64_t dim) {\n+  int64_t dim_size = self.size(dim);\n+  int64_t num_splits = (dim_size + split_size - 1) / split_size;\n+  std::vector<Tensor> splits(num_splits);\n+  int64_t last_split_size = split_size - (split_size * num_splits - dim_size);\n+\n+  for (int64_t i = 0; i < num_splits; ++i) {\n+    auto length = i < num_splits - 1 ? split_size : last_split_size;\n+    splits[i] = self.narrow(dim, i * split_size, length);\n+  }\n+  return splits;\n+}\n+\n+std::vector<Tensor> chunk(const Tensor& self, int64_t chunks, int64_t dim) {\n+  int64_t split_size = (self.size(dim) + chunks - 1) / chunks;\n+  // ensure this is dispatched through Tensor/Type, rather than the native function directly.\n+  return self.split(split_size, dim);\n+}\n+\n+int64_t size(const Tensor& self, int64_t dim) {\n+  dim = maybe_wrap_dim(dim, self.dim());\n+  // wrap_dim guarantees bounds are correct.\n+  return self.sizes()[dim];\n+}\n+\n+int64_t stride(const Tensor& self, int64_t dim) {\n+  dim = maybe_wrap_dim(dim, self.dim());\n+  // wrap_dim guarantees bounds are correct.\n+  return self.strides()[dim];\n+}\n+\n+bool is_same_size(const Tensor& self, const Tensor& other) {\n+  return self.sizes().equals(other.sizes());\n+}\n+\n+Tensor permute(const Tensor& self, IntList dims) {\n+  auto nDims = self.dim();\n+  if (dims.size() != (size_t)nDims) {\n+    runtime_error(\"number of dims don't match in permute\");\n+  }\n+  auto oldSizes = self.sizes();\n+  auto oldStrides = self.strides();\n+  std::vector<int64_t> newSizes(nDims);\n+  std::vector<int64_t> newStrides(nDims);\n+  std::vector<bool> seen(nDims);\n+  for (int64_t i = 0; i < nDims; i++) {\n+    auto dim = maybe_wrap_dim(dims[i], nDims);\n+    if (seen[dim]) {\n+      runtime_error(\"repeated dim in permute\");\n+    }\n+    seen[dim] = true;\n+    newSizes[i] = oldSizes[dim];\n+    newStrides[i] = oldStrides[dim];\n+  }\n+  return self.as_strided(newSizes, newStrides);\n+}\n+\n+Tensor expand(const Tensor& self, IntList size) {\n+  if (size.size() < (size_t)self.dim()) {\n+    std::ostringstream ss;\n+    ss << \"expand(\" << self.type() << \"{\" << self.sizes() << \"}, size=\" << size\n+       << \"): the number of sizes provided (\" << size.size() << \") \"\n+       << \"must be greater or equal to the number of dimensions in the tensor (\"\n+       << self.dim() << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+\n+  std::vector<int64_t> expandedSizes;\n+  std::vector<int64_t> expandedStrides;\n+  std::tie(expandedSizes, expandedStrides) = inferExpandGeometry(self, size);\n+\n+  return self.as_strided(expandedSizes, expandedStrides);\n+}\n+\n+std::tuple<std::vector<int64_t>, std::vector<int64_t> >\n+inferSqueezeGeometry(const Tensor &tensor) {\n+  std::vector<int64_t> sizes;\n+  std::vector<int64_t> strides;\n+\n+  for(int64_t d = 0; d < tensor.dim(); d++) {\n+    if(tensor.sizes()[d] != 1) {\n+      sizes.push_back(tensor.sizes()[d]);\n+      strides.push_back(tensor.strides()[d]);\n+    }\n+  }\n+\n+  return std::make_tuple(sizes, strides);\n+}\n+\n+std::tuple<std::vector<int64_t>, std::vector<int64_t> >\n+inferSqueezeGeometry(const Tensor& tensor, int64_t dim) {\n+  std::vector<int64_t> sizes;\n+  std::vector<int64_t> strides;\n+\n+  for(int64_t d = 0; d < tensor.dim(); d++) {\n+    if(d != dim || tensor.sizes()[dim] != 1) {\n+      sizes.push_back(tensor.sizes()[d]);\n+      strides.push_back(tensor.strides()[d]);\n+    }\n+  }\n+  return std::make_tuple(sizes, strides);\n+}\n+\n+std::tuple<std::vector<int64_t>, std::vector<int64_t> >\n+inferUnsqueezeGeometry(const Tensor& tensor, int64_t dim) {\n+  if (tensor.numel() == 0) {\n+    throw std::runtime_error(\"cannot unsqueeze empty tensor\");\n+  }\n+\n+  std::vector<int64_t> sizes(tensor.sizes());\n+  std::vector<int64_t> strides(tensor.strides());\n+  int64_t new_stride = dim >= tensor.dim() - 1 ? 1 : sizes[dim] * strides[dim];\n+  sizes.insert(sizes.begin() + dim, 1);\n+  strides.insert(strides.begin() + dim, new_stride);\n+\n+  return std::make_tuple(sizes, strides);\n+}\n+\n+Tensor squeeze(const Tensor& self) {\n+  auto g = inferSqueezeGeometry(self);\n+  return self.as_strided(std::get<0>(g), std::get<1>(g));\n+}\n+\n+Tensor squeeze(const Tensor& self, int64_t dim) {\n+  dim = maybe_wrap_dim(dim, self.dim());\n+\n+  if (self.sizes()[dim] != 1) {\n+    return self.as_strided(self.sizes().vec(), self.strides().vec());\n+  }\n+  auto g = inferSqueezeGeometry(self, dim);\n+  return self.as_strided(std::get<0>(g), std::get<1>(g));\n+}\n+\n+Tensor & squeeze_(Tensor& self) {\n+  auto g = inferSqueezeGeometry(self);\n+  return self.as_strided_(std::get<0>(g), std::get<1>(g));\n+}\n+\n+Tensor & squeeze_(Tensor& self, int64_t dim) {\n+  dim = maybe_wrap_dim(dim, self.dim());\n+\n+  if (self.sizes()[dim] != 1) {\n+    return self.as_strided_(self.sizes().vec(), self.strides().vec());\n+  }\n+  auto g = inferSqueezeGeometry(self, dim);\n+  return self.as_strided_(std::get<0>(g), std::get<1>(g));\n+}\n+\n+Tensor unsqueeze(const Tensor& self, int64_t dim) {\n+  dim = maybe_wrap_dim(dim, self.dim() + 1);\n+\n+  auto g = inferUnsqueezeGeometry(self, dim);\n+  return self.as_strided(std::get<0>(g), std::get<1>(g));\n+}\n+\n+Tensor & unsqueeze_(Tensor& self, int64_t dim) {\n+  dim = maybe_wrap_dim(dim, self.dim() + 1);\n+\n+  auto g = inferUnsqueezeGeometry(self, dim);\n+  return self.as_strided_(std::get<0>(g), std::get<1>(g));\n+}\n+\n+Tensor stack(TensorList tensors, int64_t dim) {\n+  if (tensors.size() == 0) {\n+    throw std::runtime_error(\"stack expects a non-empty TensorList\");\n+  }\n+  dim = maybe_wrap_dim(dim, tensors[0].dim() + 1);\n+\n+  std::vector<Tensor> inputs(tensors.size());\n+  for (size_t i = 0; i < tensors.size(); ++i) {\n+    inputs[i] = tensors[i].unsqueeze(dim);\n+  }\n+  return at::cat(inputs, dim);\n+}\n+\n+std::tuple<at::Tensor, at::Tensor> SpatialRoIPooling_forward(\n+\tconst Tensor& input,\n+\tconst Tensor& rois,\n+\tint64_t pooledHeight,\n+\tint64_t pooledWidth,\n+\tdouble spatialScale)\n+{\n+\t// Input is the output of the last convolutional layer in the Backbone network, so\n+\t// it should be in the format of NCHW\n+\tAT_ASSERT(input.ndimension() == 4, \"Input to RoI Pooling should be a NCHW Tensor\");\n+\n+\t// ROIs is the set of region proposals to process. It is a 2D Tensor where the first\n+\t// dim is the # of proposals, and the second dim is the proposal itself in the form\n+\t// [batch_index startW startH endW endH]\n+\tAT_ASSERT(rois.ndimension() == 2, \"RoI Proposals should be a 2D Tensor, (batch_sz x proposals)\");\n+\tAT_ASSERT(rois.size(1) == 5, \"Proposals should be of the form [batch_index startW startH endW enH]\");\n+\n+\tauto proposals = rois.size(0);\n+\tauto inputChannels = input.size(1);\n+\tauto inputHeight = input.size(2);\n+\tauto inputWidth = input.size(3);\n+\n+\t// Output Tensor is (num_rois, C, pooledHeight, pooledWidth)\n+\tauto output = input.type().tensor({proposals, inputChannels, pooledHeight, pooledWidth});\n+\n+\t// TODO: need some mechanism for determining train vs. test\n+\n+\t// During training, we need to store the argmaxes for the pooling operation, so\n+\t// the argmaxes Tensor should be the same size as the output Tensor\n+\tauto argmaxes = input.type().toScalarType(kInt).tensor({proposals, inputChannels, pooledHeight, pooledWidth});\n+\n+\tAT_ASSERT(input.is_contiguous(), \"input must be contiguous\");\n+\tAT_ASSERT(rois.is_contiguous(), \"rois must be contiguous\");\n+\n+\tauto *rawInput = input.data<float>();", "path": "aten/src/ATen/native/NativeFunctions.cpp", "position": null, "original_position": 226, "commit_id": "63ed778c9072b62a38365a69636e3b78e29e031f", "original_commit_id": "f0472e403ee4926113035fdb0e3aafd8ea62849e", "user": {"login": "killeent", "id": 4529377, "node_id": "MDQ6VXNlcjQ1MjkzNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4529377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/killeent", "html_url": "https://github.com/killeent", "followers_url": "https://api.github.com/users/killeent/followers", "following_url": "https://api.github.com/users/killeent/following{/other_user}", "gists_url": "https://api.github.com/users/killeent/gists{/gist_id}", "starred_url": "https://api.github.com/users/killeent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/killeent/subscriptions", "organizations_url": "https://api.github.com/users/killeent/orgs", "repos_url": "https://api.github.com/users/killeent/repos", "events_url": "https://api.github.com/users/killeent/events{/privacy}", "received_events_url": "https://api.github.com/users/killeent/received_events", "type": "User", "site_admin": false}, "body": "Based on the bindings it should always be routed to this function. I can add checks if you'd like but I'm not sure if they are necessary.", "created_at": "2017-11-13T19:55:13Z", "updated_at": "2018-11-23T15:36:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/3672#discussion_r150647629", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3672", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150647629"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3672#discussion_r150647629"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3672"}}, "body_html": "<p>Based on the bindings it should always be routed to this function. I can add checks if you'd like but I'm not sure if they are necessary.</p>", "body_text": "Based on the bindings it should always be routed to this function. I can add checks if you'd like but I'm not sure if they are necessary.", "in_reply_to_id": 150646079}