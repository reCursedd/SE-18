{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223481726", "pull_request_review_id": 162619376, "id": 223481726, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzQ4MTcyNg==", "diff_hunk": "@@ -5,18 +5,58 @@\n \n namespace torch { namespace jit {\n \n+namespace {\n+\n // operators where we expect to find tuples as inputs/outputs\n-// this is to assert we are only  doing modifications when we know\n+// this is to assert we are only doing modifications when we know\n // we can flatten tuples\n std::unordered_set<Symbol> white_list = {\n   prim::If,\n   prim::Loop,\n   prim::TupleUnpack,\n   prim::TupleConstruct,\n+  prim::TupleIndex,\n+  prim::TupleSlice,\n   prim::Param,\n   prim::Return,\n };\n \n+void removeTupleNodes(Node *n, bool must_remove_tuples) {\n+  if (n->kind() != prim::TupleUnpack && n->kind() != prim::TupleIndex\n+      && n->kind() != prim::TupleSlice) {\n+    return;\n+  }\n+  auto construct = n->input()->node();\n+  if (construct->kind() != prim::TupleConstruct) {\n+    if (must_remove_tuples) {\n+      AT_ERROR(n->kind().toQualString(), \" not matched to tuple construct\");\n+    }\n+    return;\n+  }\n+  if (n->kind() == prim::TupleUnpack) {\n+    for(size_t i = 0; i < n->outputs().size(); ++i) {\n+      n->outputs()[i]->replaceAllUsesWith(construct->inputs().at(i));\n+    }\n+  } else if (n->kind() == prim::TupleIndex) {\n+    auto idx = n->i(attr::index);\n+    n->output()->replaceAllUsesWith(construct->inputs().at(idx));\n+  } else if (n->kind() == prim::TupleSlice) {\n+    std::vector<Value*> values;\n+    int64_t beg = n->i(attr::beg);\n+    int64_t end = n->i(attr::end);\n+    int64_t step = n->i(attr::step);\n+    for (size_t i = beg; i < end; i += step) {\n+      values.push_back(construct->inputs().at(i));\n+    }\n+    auto graph = n->owningGraph();\n+    auto tuple_out = graph->createTuple(values);\n+    graph->setInsertPoint(n);", "path": "torch/csrc/jit/passes/lower_tuples.cpp", "position": null, "original_position": 50, "commit_id": "b21e0e10937df577e0f3fb71aded15744aabbf2c", "original_commit_id": "03b972bd549702e5d6eec4929dd5b23541e770c6", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Use `WithInsertGuard`. Otherwise you are globally setting the insert point of the graph. This will lead to subtle bugs.", "created_at": "2018-10-08T20:03:34Z", "updated_at": "2018-11-23T15:52:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/11492#discussion_r223481726", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11492", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223481726"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11492#discussion_r223481726"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11492"}}, "body_html": "<p>Use <code>WithInsertGuard</code>. Otherwise you are globally setting the insert point of the graph. This will lead to subtle bugs.</p>", "body_text": "Use WithInsertGuard. Otherwise you are globally setting the insert point of the graph. This will lead to subtle bugs."}