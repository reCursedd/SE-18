{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2630", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2630/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2630/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2630/events", "html_url": "https://github.com/pytorch/pytorch/issues/2630", "id": 255359536, "node_id": "MDU6SXNzdWUyNTUzNTk1MzY=", "number": 2630, "title": "Speed up PyTorch autograd by moving functions to C++/ATen", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-05T17:56:29Z", "updated_at": "2017-10-03T09:20:56Z", "closed_at": "2017-10-03T09:20:40Z", "author_association": "MEMBER", "body_html": "<p>Currently, calling methods on Variable objects is much slower than calling the same methods on Tensor objects. The problem is exacerbated because many of the Variable autograd functions hold on to the GIL for most of the call. At the same time, it's hard to write autograd functions in C++ because we don't have a full C++ Variable API.</p>\n<p>We should speed up autograd by moving nearly all the Variable method implementations to C++ and use ATen to provide a fully differentiable API.</p>\n<ol>\n<li>Implement a <code>Variable</code> <code>at::Type</code> object which provides differentiable implementations of ATen tensor functions.  This means you can write C++ code on <code>at::Tensor</code> that supports automatic differentiation.</li>\n<li>Implement the derivatives for <code>at::Tensor</code> functions using a simple yaml file.</li>\n<li>Generate bindings to <code>at::Tensor</code> functions on <code>_C._VariableBase</code> in a similar way to how we generate <code>TensorMethods.cpp</code></li>\n</ol>\n<p>At least for now, the <code>Variable</code> <code>at::Type</code> object will live in PyTorch instead of ATen because it depends on PyTorch's C++ autograd API.  It'll be generated using a Python script (similar to <a href=\"https://github.com/zdevito/ATen/blob/master/src/ATen/gen.py\">gen.py</a>) that uses Declarations.yaml from ATen and the derivatives YAML file.</p>\n<p>The derivatives YAML file will also live in PyTorch. For each differentiable ATen function, we'll specify the derivative formula for each input.  For example, here's the entry for <code>addbmm</code>:</p>\n<div class=\"highlight highlight-source-yaml\"><pre>- <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">addbmm(Scalar beta=1, Tensor self, Scalar alpha=1, Tensor mat1, Tensor mat2)</span>\n  <span class=\"pl-ent\">self</span>: <span class=\"pl-s\">grad * alpha</span>\n  <span class=\"pl-ent\">mat1</span>: <span class=\"pl-s\">grad.bmm(mat2.transpose(1, 2)) * beta</span>\n  <span class=\"pl-ent\">mat2</span>: <span class=\"pl-s\">mat1.transpose(1, 2).bmm(grad) * beta</span></pre></div>\n<p>Functions that can't be concisely expressed in a single line can be added to a separate file and called from the derivatives YAML:</p>\n<div class=\"highlight highlight-source-yaml\"><pre>- <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">prod(Tensor self, int32_t dim, bool keepdim=False)</span>\n  <span class=\"pl-ent\">self</span>: <span class=\"pl-s\">prod_backward(self.sizes(), dim, keepdim)</span></pre></div>\n<p>Functions from THNN will be bound separately, since they follow a different pattern. We will probably want to move the second derivative definitions to C++ once the other steps are complete.</p>", "body_text": "Currently, calling methods on Variable objects is much slower than calling the same methods on Tensor objects. The problem is exacerbated because many of the Variable autograd functions hold on to the GIL for most of the call. At the same time, it's hard to write autograd functions in C++ because we don't have a full C++ Variable API.\nWe should speed up autograd by moving nearly all the Variable method implementations to C++ and use ATen to provide a fully differentiable API.\n\nImplement a Variable at::Type object which provides differentiable implementations of ATen tensor functions.  This means you can write C++ code on at::Tensor that supports automatic differentiation.\nImplement the derivatives for at::Tensor functions using a simple yaml file.\nGenerate bindings to at::Tensor functions on _C._VariableBase in a similar way to how we generate TensorMethods.cpp\n\nAt least for now, the Variable at::Type object will live in PyTorch instead of ATen because it depends on PyTorch's C++ autograd API.  It'll be generated using a Python script (similar to gen.py) that uses Declarations.yaml from ATen and the derivatives YAML file.\nThe derivatives YAML file will also live in PyTorch. For each differentiable ATen function, we'll specify the derivative formula for each input.  For example, here's the entry for addbmm:\n- name: addbmm(Scalar beta=1, Tensor self, Scalar alpha=1, Tensor mat1, Tensor mat2)\n  self: grad * alpha\n  mat1: grad.bmm(mat2.transpose(1, 2)) * beta\n  mat2: mat1.transpose(1, 2).bmm(grad) * beta\nFunctions that can't be concisely expressed in a single line can be added to a separate file and called from the derivatives YAML:\n- name: prod(Tensor self, int32_t dim, bool keepdim=False)\n  self: prod_backward(self.sizes(), dim, keepdim)\nFunctions from THNN will be bound separately, since they follow a different pattern. We will probably want to move the second derivative definitions to C++ once the other steps are complete.", "body": "Currently, calling methods on Variable objects is much slower than calling the same methods on Tensor objects. The problem is exacerbated because many of the Variable autograd functions hold on to the GIL for most of the call. At the same time, it's hard to write autograd functions in C++ because we don't have a full C++ Variable API.\r\n\r\nWe should speed up autograd by moving nearly all the Variable method implementations to C++ and use ATen to provide a fully differentiable API.\r\n\r\n1. Implement a `Variable` `at::Type` object which provides differentiable implementations of ATen tensor functions.  This means you can write C++ code on `at::Tensor` that supports automatic differentiation.\r\n2. Implement the derivatives for `at::Tensor` functions using a simple yaml file.\r\n3. Generate bindings to `at::Tensor` functions on `_C._VariableBase` in a similar way to how we generate `TensorMethods.cpp`\r\n\r\nAt least for now, the `Variable` `at::Type` object will live in PyTorch instead of ATen because it depends on PyTorch's C++ autograd API.  It'll be generated using a Python script (similar to [gen.py](https://github.com/zdevito/ATen/blob/master/src/ATen/gen.py)) that uses Declarations.yaml from ATen and the derivatives YAML file.\r\n\r\nThe derivatives YAML file will also live in PyTorch. For each differentiable ATen function, we'll specify the derivative formula for each input.  For example, here's the entry for `addbmm`:\r\n\r\n```yaml\r\n- name: addbmm(Scalar beta=1, Tensor self, Scalar alpha=1, Tensor mat1, Tensor mat2)\r\n  self: grad * alpha\r\n  mat1: grad.bmm(mat2.transpose(1, 2)) * beta\r\n  mat2: mat1.transpose(1, 2).bmm(grad) * beta\r\n```\r\n\r\nFunctions that can't be concisely expressed in a single line can be added to a separate file and called from the derivatives YAML:\r\n\r\n```yaml\r\n- name: prod(Tensor self, int32_t dim, bool keepdim=False)\r\n  self: prod_backward(self.sizes(), dim, keepdim)\r\n```\r\n\r\nFunctions from THNN will be bound separately, since they follow a different pattern. We will probably want to move the second derivative definitions to C++ once the other steps are complete."}