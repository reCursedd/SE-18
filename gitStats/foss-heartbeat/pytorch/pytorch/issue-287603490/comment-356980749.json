{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/356980749", "html_url": "https://github.com/pytorch/pytorch/pull/4593#issuecomment-356980749", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4593", "id": 356980749, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Njk4MDc0OQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T16:24:33Z", "updated_at": "2018-01-11T16:27:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think I have an even better strategy for writing these templates with even less duplication. Patch coming.</p>\n<p>The new templates look like this:</p>\n<pre><code>inline bool _compute_requires_grad_single(const at::Tensor&amp; tensor) {\n  const auto&amp; var = static_cast&lt;const Variable&amp;&gt;(tensor);\n  return var.defined() &amp;&amp; var.requires_grad();\n}\ninline bool _compute_requires_grad_single(at::ArrayRef&lt;at::Tensor&gt; tensors) {\n  for (const Tensor&amp; tensor : tensors) {\n    if (_compute_requires_grad_single(tensor)) return true;\n  }\n  return false;\n}\n\ntemplate&lt;typename... Args&gt;\ninline bool _compute_requires_grad() {\n  return false;\n}\ntemplate&lt;typename T, typename... Args&gt;\ninline bool _compute_requires_grad(T arg, Args... args) {\n  if (_compute_requires_grad_single(arg)) return true;\n  return _compute_requires_grad(args...);\n}\n\ntemplate&lt;typename... Args&gt;\nstatic bool compute_requires_grad(Args... args) {\n  if (!GradMode::is_enabled()) {\n    return false;\n  }\n  return _compute_requires_grad(args...);\n}\n\n</code></pre>", "body_text": "I think I have an even better strategy for writing these templates with even less duplication. Patch coming.\nThe new templates look like this:\ninline bool _compute_requires_grad_single(const at::Tensor& tensor) {\n  const auto& var = static_cast<const Variable&>(tensor);\n  return var.defined() && var.requires_grad();\n}\ninline bool _compute_requires_grad_single(at::ArrayRef<at::Tensor> tensors) {\n  for (const Tensor& tensor : tensors) {\n    if (_compute_requires_grad_single(tensor)) return true;\n  }\n  return false;\n}\n\ntemplate<typename... Args>\ninline bool _compute_requires_grad() {\n  return false;\n}\ntemplate<typename T, typename... Args>\ninline bool _compute_requires_grad(T arg, Args... args) {\n  if (_compute_requires_grad_single(arg)) return true;\n  return _compute_requires_grad(args...);\n}\n\ntemplate<typename... Args>\nstatic bool compute_requires_grad(Args... args) {\n  if (!GradMode::is_enabled()) {\n    return false;\n  }\n  return _compute_requires_grad(args...);\n}", "body": "I think I have an even better strategy for writing these templates with even less duplication. Patch coming.\r\n\r\nThe new templates look like this:\r\n\r\n```\r\ninline bool _compute_requires_grad_single(const at::Tensor& tensor) {\r\n  const auto& var = static_cast<const Variable&>(tensor);\r\n  return var.defined() && var.requires_grad();\r\n}\r\ninline bool _compute_requires_grad_single(at::ArrayRef<at::Tensor> tensors) {\r\n  for (const Tensor& tensor : tensors) {\r\n    if (_compute_requires_grad_single(tensor)) return true;\r\n  }\r\n  return false;\r\n}\r\n\r\ntemplate<typename... Args>\r\ninline bool _compute_requires_grad() {\r\n  return false;\r\n}\r\ntemplate<typename T, typename... Args>\r\ninline bool _compute_requires_grad(T arg, Args... args) {\r\n  if (_compute_requires_grad_single(arg)) return true;\r\n  return _compute_requires_grad(args...);\r\n}\r\n\r\ntemplate<typename... Args>\r\nstatic bool compute_requires_grad(Args... args) {\r\n  if (!GradMode::is_enabled()) {\r\n    return false;\r\n  }\r\n  return _compute_requires_grad(args...);\r\n}\r\n\r\n```"}