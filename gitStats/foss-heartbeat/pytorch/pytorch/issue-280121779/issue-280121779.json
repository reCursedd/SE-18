{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4071", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4071/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4071/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4071/events", "html_url": "https://github.com/pytorch/pytorch/issues/4071", "id": 280121779, "node_id": "MDU6SXNzdWUyODAxMjE3Nzk=", "number": 4071, "title": "torch.cat doesn't do proper shape checking, which can lead to incorrect gradients", "user": {"login": "stefdoerr", "id": 7935362, "node_id": "MDQ6VXNlcjc5MzUzNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7935362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefdoerr", "html_url": "https://github.com/stefdoerr", "followers_url": "https://api.github.com/users/stefdoerr/followers", "following_url": "https://api.github.com/users/stefdoerr/following{/other_user}", "gists_url": "https://api.github.com/users/stefdoerr/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefdoerr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefdoerr/subscriptions", "organizations_url": "https://api.github.com/users/stefdoerr/orgs", "repos_url": "https://api.github.com/users/stefdoerr/repos", "events_url": "https://api.github.com/users/stefdoerr/events{/privacy}", "received_events_url": "https://api.github.com/users/stefdoerr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-07T13:02:19Z", "updated_at": "2017-12-18T07:05:59Z", "closed_at": "2017-12-18T07:05:59Z", "author_association": "NONE", "body_html": "<p>This is a <em>very</em> sneaky behaviour so I will call it a bug as it can easily break your code.<br>\nI paste here a self-contained example. Even though <code>axes1</code> is equivalent to <code>axes2</code>, <code>axes2</code> gives wrong gradients.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> autograd\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">norm_vec</span>(<span class=\"pl-smi\">v</span>):\n    <span class=\"pl-k\">return</span> torch.sqrt(torch.pow(v, <span class=\"pl-c1\">2</span>).sum())\n\ncoo <span class=\"pl-k\">=</span> Variable(torch.from_numpy(np.array([[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>],])).float(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\naxes1 <span class=\"pl-k\">=</span> torch.stack([coo[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">/</span> norm_vec(coo[<span class=\"pl-c1\">0</span>]), Variable(torch.zeros(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(torch.zeros(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\naxes2 <span class=\"pl-k\">=</span> torch.stack([coo[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">/</span> norm_vec(coo[<span class=\"pl-c1\">0</span>]), Variable(torch.zeros(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(torch.zeros(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).squeeze()\nnew1 <span class=\"pl-k\">=</span> torch.matmul(coo, axes1)\nnew2 <span class=\"pl-k\">=</span> torch.matmul(coo, axes2)\n<span class=\"pl-c1\">print</span>(autograd.grad(new1[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], coo, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), autograd.grad(new2[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], coo, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))</pre></div>", "body_text": "This is a very sneaky behaviour so I will call it a bug as it can easily break your code.\nI paste here a self-contained example. Even though axes1 is equivalent to axes2, axes2 gives wrong gradients.\nfrom torch.autograd import Variable\nfrom torch import autograd\nimport torch\nimport numpy as np\n\ndef norm_vec(v):\n    return torch.sqrt(torch.pow(v, 2).sum())\n\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\naxes2 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\nnew1 = torch.matmul(coo, axes1)\nnew2 = torch.matmul(coo, axes2)\nprint(autograd.grad(new1[0, 0], coo, create_graph=True), autograd.grad(new2[0, 0], coo, create_graph=True))", "body": "This is a *very* sneaky behaviour so I will call it a bug as it can easily break your code.\r\nI paste here a self-contained example. Even though `axes1` is equivalent to `axes2`, `axes2` gives wrong gradients.\r\n```python\r\nfrom torch.autograd import Variable\r\nfrom torch import autograd\r\nimport torch\r\nimport numpy as np\r\n\r\ndef norm_vec(v):\r\n    return torch.sqrt(torch.pow(v, 2).sum())\r\n\r\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\r\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\r\naxes2 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\r\nnew1 = torch.matmul(coo, axes1)\r\nnew2 = torch.matmul(coo, axes2)\r\nprint(autograd.grad(new1[0, 0], coo, create_graph=True), autograd.grad(new2[0, 0], coo, create_graph=True))\r\n```"}