{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/350013436", "html_url": "https://github.com/pytorch/pytorch/issues/4071#issuecomment-350013436", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4071", "id": 350013436, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDAxMzQzNg==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-07T16:06:30Z", "updated_at": "2017-12-07T16:31:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think this is a torch.stack + indexing + broadcasting problem.</p>\n<p>When I run <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7935362\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/stefdoerr\">@stefdoerr</a>'s code, making sure the tensor arguments to torch.stack all have the same size, everything is fine:</p>\n<pre><code>from torch.autograd import Variable\nfrom torch import autograd\nimport torch\nimport numpy as np\n\ndef norm_vec(v):\n    return torch.sqrt(torch.pow(v, 2).sum())\n\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\naxes2 = torch.stack([(coo[0] / norm_vec(coo[0])).unsqueeze(1), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\nnew1 = torch.matmul(coo, axes1)\nnew2 = torch.matmul(coo, axes2)\nprint(autograd.grad(new1[0, 0], coo, create_graph=True), autograd.grad(new2[0, 0], coo, create_graph=True)) # prints two identical tensors\n</code></pre>", "body_text": "I think this is a torch.stack + indexing + broadcasting problem.\nWhen I run @stefdoerr's code, making sure the tensor arguments to torch.stack all have the same size, everything is fine:\nfrom torch.autograd import Variable\nfrom torch import autograd\nimport torch\nimport numpy as np\n\ndef norm_vec(v):\n    return torch.sqrt(torch.pow(v, 2).sum())\n\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\naxes2 = torch.stack([(coo[0] / norm_vec(coo[0])).unsqueeze(1), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\nnew1 = torch.matmul(coo, axes1)\nnew2 = torch.matmul(coo, axes2)\nprint(autograd.grad(new1[0, 0], coo, create_graph=True), autograd.grad(new2[0, 0], coo, create_graph=True)) # prints two identical tensors", "body": "I think this is a torch.stack + indexing + broadcasting problem.\r\n\r\nWhen I run @stefdoerr's code, making sure the tensor arguments to torch.stack all have the same size, everything is fine:\r\n```\r\nfrom torch.autograd import Variable\r\nfrom torch import autograd\r\nimport torch\r\nimport numpy as np\r\n\r\ndef norm_vec(v):\r\n    return torch.sqrt(torch.pow(v, 2).sum())\r\n\r\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\r\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\r\naxes2 = torch.stack([(coo[0] / norm_vec(coo[0])).unsqueeze(1), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\r\nnew1 = torch.matmul(coo, axes1)\r\nnew2 = torch.matmul(coo, axes2)\r\nprint(autograd.grad(new1[0, 0], coo, create_graph=True), autograd.grad(new2[0, 0], coo, create_graph=True)) # prints two identical tensors\r\n```"}