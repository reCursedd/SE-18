{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349960927", "html_url": "https://github.com/pytorch/pytorch/issues/4071#issuecomment-349960927", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4071", "id": 349960927, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTk2MDkyNw==", "user": {"login": "stefdoerr", "id": 7935362, "node_id": "MDQ6VXNlcjc5MzUzNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7935362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefdoerr", "html_url": "https://github.com/stefdoerr", "followers_url": "https://api.github.com/users/stefdoerr/followers", "following_url": "https://api.github.com/users/stefdoerr/following{/other_user}", "gists_url": "https://api.github.com/users/stefdoerr/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefdoerr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefdoerr/subscriptions", "organizations_url": "https://api.github.com/users/stefdoerr/orgs", "repos_url": "https://api.github.com/users/stefdoerr/repos", "events_url": "https://api.github.com/users/stefdoerr/events{/privacy}", "received_events_url": "https://api.github.com/users/stefdoerr/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-07T13:03:45Z", "updated_at": "2017-12-07T13:03:45Z", "author_association": "NONE", "body_html": "<p>In case you are interested, here is a larger example that shows that indeed <code>axes1</code> is correct. I have a point in space and I project it on it's own normalized vector. Hence the x coordinate in the new coordinate system should be equal to the distance, so <code>x = r</code> meaning that <code>1/r = x/r**2</code> so the gradients of <code>inv</code> should be the same as the gradients of <code>test1</code> and <code>test2</code> (but only <code>test1</code> gives the right gradients)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> autograd\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">norm_vec</span>(<span class=\"pl-smi\">v</span>):\n    <span class=\"pl-k\">return</span> torch.sqrt(torch.pow(v, <span class=\"pl-c1\">2</span>).sum())\n\ncoo <span class=\"pl-k\">=</span> Variable(torch.from_numpy(np.array([[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>],])).float(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nr2 <span class=\"pl-k\">=</span> (coo<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span>).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\naxes1 <span class=\"pl-k\">=</span> torch.stack([coo[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">/</span> norm_vec(coo[<span class=\"pl-c1\">0</span>]), Variable(torch.zeros(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(torch.zeros(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\naxes2 <span class=\"pl-k\">=</span> torch.stack([coo[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">/</span> norm_vec(coo[<span class=\"pl-c1\">0</span>]), Variable(torch.zeros(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(torch.zeros(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).squeeze()\n<span class=\"pl-c1\">print</span>(axes1, axes2)\nnew1 <span class=\"pl-k\">=</span> torch.matmul(coo, axes1)\nnew2 <span class=\"pl-k\">=</span> torch.matmul(coo, axes2)\n\ninv <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span><span class=\"pl-k\">/</span>torch.sqrt(r2[<span class=\"pl-c1\">0</span>])\ntest1 <span class=\"pl-k\">=</span> new1[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]<span class=\"pl-k\">/</span>r2[<span class=\"pl-c1\">0</span>]\ntest2 <span class=\"pl-k\">=</span> new2[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]<span class=\"pl-k\">/</span>r2[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-c1\">print</span>(autograd.grad(inv, coo, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), autograd.grad(test1, coo, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), autograd.grad(test2, coo, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))</pre></div>", "body_text": "In case you are interested, here is a larger example that shows that indeed axes1 is correct. I have a point in space and I project it on it's own normalized vector. Hence the x coordinate in the new coordinate system should be equal to the distance, so x = r meaning that 1/r = x/r**2 so the gradients of inv should be the same as the gradients of test1 and test2 (but only test1 gives the right gradients)\nfrom torch.autograd import Variable\nfrom torch import autograd\nimport torch\nimport numpy as np\n\ndef norm_vec(v):\n    return torch.sqrt(torch.pow(v, 2).sum())\n\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\nr2 = (coo**2).sum(dim=1)\n\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\naxes2 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\nprint(axes1, axes2)\nnew1 = torch.matmul(coo, axes1)\nnew2 = torch.matmul(coo, axes2)\n\ninv = 1/torch.sqrt(r2[0])\ntest1 = new1[0, 0]/r2[0]\ntest2 = new2[0, 0]/r2[0]\n\nprint(autograd.grad(inv, coo, create_graph=True), autograd.grad(test1, coo, create_graph=True), autograd.grad(test2, coo, create_graph=True))", "body": "In case you are interested, here is a larger example that shows that indeed `axes1` is correct. I have a point in space and I project it on it's own normalized vector. Hence the x coordinate in the new coordinate system should be equal to the distance, so `x = r` meaning that `1/r = x/r**2` so the gradients of `inv` should be the same as the gradients of `test1` and `test2` (but only `test1` gives the right gradients)\r\n```python\r\nfrom torch.autograd import Variable\r\nfrom torch import autograd\r\nimport torch\r\nimport numpy as np\r\n\r\ndef norm_vec(v):\r\n    return torch.sqrt(torch.pow(v, 2).sum())\r\n\r\ncoo = Variable(torch.from_numpy(np.array([[3, 4, 2],])).float(), requires_grad=True)\r\nr2 = (coo**2).sum(dim=1)\r\n\r\naxes1 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3), requires_grad=True), Variable(torch.zeros(3), requires_grad=True)], dim=1)\r\naxes2 = torch.stack([coo[0] / norm_vec(coo[0]), Variable(torch.zeros(3, 1), requires_grad=True), Variable(torch.zeros(3, 1), requires_grad=True)], dim=1).squeeze()\r\nprint(axes1, axes2)\r\nnew1 = torch.matmul(coo, axes1)\r\nnew2 = torch.matmul(coo, axes2)\r\n\r\ninv = 1/torch.sqrt(r2[0])\r\ntest1 = new1[0, 0]/r2[0]\r\ntest2 = new2[0, 0]/r2[0]\r\n\r\nprint(autograd.grad(inv, coo, create_graph=True), autograd.grad(test1, coo, create_graph=True), autograd.grad(test2, coo, create_graph=True))\r\n```"}