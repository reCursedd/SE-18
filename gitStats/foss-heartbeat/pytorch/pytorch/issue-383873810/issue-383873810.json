{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14338", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14338/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14338/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14338/events", "html_url": "https://github.com/pytorch/pytorch/issues/14338", "id": 383873810, "node_id": "MDU6SXNzdWUzODM4NzM4MTA=", "number": 14338, "title": "What happened to symbolic_override?", "user": {"login": "szali", "id": 8566703, "node_id": "MDQ6VXNlcjg1NjY3MDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/8566703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szali", "html_url": "https://github.com/szali", "followers_url": "https://api.github.com/users/szali/followers", "following_url": "https://api.github.com/users/szali/following{/other_user}", "gists_url": "https://api.github.com/users/szali/gists{/gist_id}", "starred_url": "https://api.github.com/users/szali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szali/subscriptions", "organizations_url": "https://api.github.com/users/szali/orgs", "repos_url": "https://api.github.com/users/szali/repos", "events_url": "https://api.github.com/users/szali/events{/privacy}", "received_events_url": "https://api.github.com/users/szali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-23T16:06:40Z", "updated_at": "2018-11-23T16:06:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>symbolic_override was removed but no mechanism is provided (at least not in a documented way) to replicate its function, i.e. to specify that a Python function should be exported as a whole, as a specific ONNX op, instead of a traced graph.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<ol>\n<li>Use symbolic_override</li>\n<li>Pull torch master</li>\n<li>symbolic_override no longer works</li>\n</ol>\n<h2>Expected behavior</h2>\n<p>symbolic_override still exists or there is a way to emulate it.</p>\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0a0+8e1e3ba<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.2.88</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.12.0-rc2</p>\n<p>Python version: 3.5<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.2.88<br>\nGPU models and configuration:<br>\nGPU 0: TITAN X (Pascal)<br>\nGPU 1: TITAN X (Pascal)</p>\n<p>Nvidia driver version: 396.26<br>\ncuDNN version: 7.2.1</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.13.3)<br>\n[pip3] torch (1.0.0a0+8e1e3ba)<br>\n[pip3] torchfile (0.1.0)<br>\n[pip3] torchvision (0.1.9)<br>\n[conda] Could not collect</p>\n<h2>Additional context</h2>\n<p>symbolic_override was removed here:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"376257869\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13441\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13441/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13441\">#13441</a><br>\nspecifically:<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/59b75397e1045a2b459c0449bf59b9889fd3ecd6/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/59b75397e1045a2b459c0449bf59b9889fd3ecd6\"><tt>59b7539</tt></a></p>\n<p>One can use a torch.autograd.Function but then you have to implement the backward separately which I want autograd to do automatically. The only thing I need is that the function is exported as a single onnx operation.</p>", "body_text": "\ud83d\udc1b Bug\nsymbolic_override was removed but no mechanism is provided (at least not in a documented way) to replicate its function, i.e. to specify that a Python function should be exported as a whole, as a specific ONNX op, instead of a traced graph.\nTo Reproduce\nSteps to reproduce the behavior:\n\nUse symbolic_override\nPull torch master\nsymbolic_override no longer works\n\nExpected behavior\nsymbolic_override still exists or there is a way to emulate it.\nEnvironment\nPyTorch version: 1.0.0a0+8e1e3ba\nIs debug build: No\nCUDA used to build PyTorch: 9.2.88\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.12.0-rc2\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.2.88\nGPU models and configuration:\nGPU 0: TITAN X (Pascal)\nGPU 1: TITAN X (Pascal)\nNvidia driver version: 396.26\ncuDNN version: 7.2.1\nVersions of relevant libraries:\n[pip3] numpy (1.13.3)\n[pip3] torch (1.0.0a0+8e1e3ba)\n[pip3] torchfile (0.1.0)\n[pip3] torchvision (0.1.9)\n[conda] Could not collect\nAdditional context\nsymbolic_override was removed here:\n#13441\nspecifically:\n59b7539\nOne can use a torch.autograd.Function but then you have to implement the backward separately which I want autograd to do automatically. The only thing I need is that the function is exported as a single onnx operation.", "body": "## \ud83d\udc1b Bug\r\n\r\nsymbolic_override was removed but no mechanism is provided (at least not in a documented way) to replicate its function, i.e. to specify that a Python function should be exported as a whole, as a specific ONNX op, instead of a traced graph.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Use symbolic_override\r\n2. Pull torch master\r\n3. symbolic_override no longer works\r\n\r\n## Expected behavior\r\n\r\nsymbolic_override still exists or there is a way to emulate it.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0a0+8e1e3ba\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.0-rc2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: 7.2.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.13.3)\r\n[pip3] torch (1.0.0a0+8e1e3ba)\r\n[pip3] torchfile (0.1.0)\r\n[pip3] torchvision (0.1.9)\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nsymbolic_override was removed here:\r\nhttps://github.com/pytorch/pytorch/pull/13441\r\nspecifically:\r\nhttps://github.com/pytorch/pytorch/pull/13441/commits/59b75397e1045a2b459c0449bf59b9889fd3ecd6\r\n\r\nOne can use a torch.autograd.Function but then you have to implement the backward separately which I want autograd to do automatically. The only thing I need is that the function is exported as a single onnx operation.\r\n"}