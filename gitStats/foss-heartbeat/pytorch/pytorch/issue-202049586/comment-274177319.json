{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274177319", "html_url": "https://github.com/pytorch/pytorch/issues/517#issuecomment-274177319", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/517", "id": 274177319, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDE3NzMxOQ==", "user": {"login": "braingineer", "id": 1455742, "node_id": "MDQ6VXNlcjE0NTU3NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1455742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/braingineer", "html_url": "https://github.com/braingineer", "followers_url": "https://api.github.com/users/braingineer/followers", "following_url": "https://api.github.com/users/braingineer/following{/other_user}", "gists_url": "https://api.github.com/users/braingineer/gists{/gist_id}", "starred_url": "https://api.github.com/users/braingineer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/braingineer/subscriptions", "organizations_url": "https://api.github.com/users/braingineer/orgs", "repos_url": "https://api.github.com/users/braingineer/repos", "events_url": "https://api.github.com/users/braingineer/events{/privacy}", "received_events_url": "https://api.github.com/users/braingineer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-20T20:54:10Z", "updated_at": "2017-01-20T20:54:10Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>The lengths are necessary because whether or not to mask a certain row is dependent on the time step.</p>\n</blockquote>\n<p>I'm confused by this statement. Why do sentinel values not do this?  e.g. <a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L105\">this embedding layer in keras</a>.</p>\n<blockquote>\n<p>especially for extremely long sequences or extremely large batch sizes.</p>\n</blockquote>\n<p>A binary tensor isn't all that big.   Even at 16 bit ints, if you had batch size 128, seq length 100, and feature dim of 300, the mask is 12 mb.  To store, not huge. to apply, usually it's a <a href=\"https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py#L1082\">switch</a> (which is essentially <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"nofollow\">\"where\" in numpy</a>)</p>\n<p>Though, in looking at the code here, it seems it may be harder than I had thought. I'm not finding a function with <code>where</code> or <code>switch</code> semantics..</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> , is there a function that provides this functionality?  I'll open an issue if not.</p>\n<p>Thanks!</p>", "body_text": "The lengths are necessary because whether or not to mask a certain row is dependent on the time step.\n\nI'm confused by this statement. Why do sentinel values not do this?  e.g. this embedding layer in keras.\n\nespecially for extremely long sequences or extremely large batch sizes.\n\nA binary tensor isn't all that big.   Even at 16 bit ints, if you had batch size 128, seq length 100, and feature dim of 300, the mask is 12 mb.  To store, not huge. to apply, usually it's a switch (which is essentially \"where\" in numpy)\nThough, in looking at the code here, it seems it may be harder than I had thought. I'm not finding a function with where or switch semantics..\n@ngimel , is there a function that provides this functionality?  I'll open an issue if not.\nThanks!", "body": "> The lengths are necessary because whether or not to mask a certain row is dependent on the time step.\r\n\r\nI'm confused by this statement. Why do sentinel values not do this?  e.g. [this embedding layer in keras](https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L105).  \r\n\r\n> especially for extremely long sequences or extremely large batch sizes.\r\n\r\nA binary tensor isn't all that big.   Even at 16 bit ints, if you had batch size 128, seq length 100, and feature dim of 300, the mask is 12 mb.  To store, not huge. to apply, usually it's a [switch](https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py#L1082) (which is essentially [\"where\" in numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html))\r\n\r\nThough, in looking at the code here, it seems it may be harder than I had thought. I'm not finding a function with `where` or `switch` semantics..\r\n\r\n@ngimel , is there a function that provides this functionality?  I'll open an issue if not. \r\n\r\nThanks!"}