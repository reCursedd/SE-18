{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274166060", "html_url": "https://github.com/pytorch/pytorch/issues/517#issuecomment-274166060", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/517", "id": 274166060, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDE2NjA2MA==", "user": {"login": "shawnjhenry", "id": 9464836, "node_id": "MDQ6VXNlcjk0NjQ4MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9464836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnjhenry", "html_url": "https://github.com/shawnjhenry", "followers_url": "https://api.github.com/users/shawnjhenry/followers", "following_url": "https://api.github.com/users/shawnjhenry/following{/other_user}", "gists_url": "https://api.github.com/users/shawnjhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnjhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnjhenry/subscriptions", "organizations_url": "https://api.github.com/users/shawnjhenry/orgs", "repos_url": "https://api.github.com/users/shawnjhenry/repos", "events_url": "https://api.github.com/users/shawnjhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnjhenry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-20T20:01:26Z", "updated_at": "2017-01-20T20:01:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> That actually sounds like what I'm looking for - any chance that will be supported in the future?  How does CuDNN actually handle the variable length sequences?  I assume because of your comment about batch_first, you pass it a list of size max_seq_length consisting of tensors of size &lt;= batch_size x hidden_size?</p>\n<p>Batch first could still work - you pass a list of size batch_size consisting of of tensors of size seq_length_i x hidden_size, and before passing to the CuDNN backend, sort by seq_length and concatenate at each time step to get it into the first form.  You would need to keep track of the permutation of batches and then do the inverse permutation on the output, but there are cases where you might need to do that for the regular CuDNN input anyway.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1455742\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/braingineer\">@braingineer</a> The lengths are necessary because whether or not to mask a certain row is dependent on the time step.  I suppose you could provide the full mask ahead of time and slice it, but it would require a lot less memory just to pass the lengths and compute the mask on the fly, especially for extremely long sequences or extremely large batch sizes.</p>", "body_text": "@ngimel That actually sounds like what I'm looking for - any chance that will be supported in the future?  How does CuDNN actually handle the variable length sequences?  I assume because of your comment about batch_first, you pass it a list of size max_seq_length consisting of tensors of size <= batch_size x hidden_size?\nBatch first could still work - you pass a list of size batch_size consisting of of tensors of size seq_length_i x hidden_size, and before passing to the CuDNN backend, sort by seq_length and concatenate at each time step to get it into the first form.  You would need to keep track of the permutation of batches and then do the inverse permutation on the output, but there are cases where you might need to do that for the regular CuDNN input anyway.\n@braingineer The lengths are necessary because whether or not to mask a certain row is dependent on the time step.  I suppose you could provide the full mask ahead of time and slice it, but it would require a lot less memory just to pass the lengths and compute the mask on the fly, especially for extremely long sequences or extremely large batch sizes.", "body": "@ngimel That actually sounds like what I'm looking for - any chance that will be supported in the future?  How does CuDNN actually handle the variable length sequences?  I assume because of your comment about batch_first, you pass it a list of size max_seq_length consisting of tensors of size <= batch_size x hidden_size?  \r\n\r\nBatch first could still work - you pass a list of size batch_size consisting of of tensors of size seq_length_i x hidden_size, and before passing to the CuDNN backend, sort by seq_length and concatenate at each time step to get it into the first form.  You would need to keep track of the permutation of batches and then do the inverse permutation on the output, but there are cases where you might need to do that for the regular CuDNN input anyway.\r\n\r\n@braingineer The lengths are necessary because whether or not to mask a certain row is dependent on the time step.  I suppose you could provide the full mask ahead of time and slice it, but it would require a lot less memory just to pass the lengths and compute the mask on the fly, especially for extremely long sequences or extremely large batch sizes.   "}