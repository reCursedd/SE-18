{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274264376", "html_url": "https://github.com/pytorch/pytorch/issues/517#issuecomment-274264376", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/517", "id": 274264376, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDI2NDM3Ng==", "user": {"login": "shawnjhenry", "id": 9464836, "node_id": "MDQ6VXNlcjk0NjQ4MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9464836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnjhenry", "html_url": "https://github.com/shawnjhenry", "followers_url": "https://api.github.com/users/shawnjhenry/followers", "following_url": "https://api.github.com/users/shawnjhenry/following{/other_user}", "gists_url": "https://api.github.com/users/shawnjhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnjhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnjhenry/subscriptions", "organizations_url": "https://api.github.com/users/shawnjhenry/orgs", "repos_url": "https://api.github.com/users/shawnjhenry/repos", "events_url": "https://api.github.com/users/shawnjhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnjhenry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-21T14:17:51Z", "updated_at": "2017-01-21T14:17:51Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> While you're contemplating how to add variable length sequences to the non-CuDNN RNN library, would you consider some other changes?  I've profiled a lot of RNN libraries, and it turns out that most of the calculation time is wasted in kernel launches when operating on the GPU.  Pre-allocating buffer tensors and doing the calculations in-place on slices, as in <a href=\"https://github.com/jcjohnson/torch-rnn\">this torch library</a> is considerably faster than having to create new tensors at each time step.  It's a bit more involved to write, but as you can see from the benchmarks there, the speed-up and memory savings is significant, especially for one-layer nets.</p>", "body_text": "@adamlerer While you're contemplating how to add variable length sequences to the non-CuDNN RNN library, would you consider some other changes?  I've profiled a lot of RNN libraries, and it turns out that most of the calculation time is wasted in kernel launches when operating on the GPU.  Pre-allocating buffer tensors and doing the calculations in-place on slices, as in this torch library is considerably faster than having to create new tensors at each time step.  It's a bit more involved to write, but as you can see from the benchmarks there, the speed-up and memory savings is significant, especially for one-layer nets.", "body": "@adamlerer While you're contemplating how to add variable length sequences to the non-CuDNN RNN library, would you consider some other changes?  I've profiled a lot of RNN libraries, and it turns out that most of the calculation time is wasted in kernel launches when operating on the GPU.  Pre-allocating buffer tensors and doing the calculations in-place on slices, as in [this torch library](https://github.com/jcjohnson/torch-rnn) is considerably faster than having to create new tensors at each time step.  It's a bit more involved to write, but as you can see from the benchmarks there, the speed-up and memory savings is significant, especially for one-layer nets."}