{"url": "https://api.github.com/repos/pytorch/pytorch/issues/517", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/517/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/517/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/517/events", "html_url": "https://github.com/pytorch/pytorch/issues/517", "id": 202049586, "node_id": "MDU6SXNzdWUyMDIwNDk1ODY=", "number": 517, "title": "Feature Request: Length Masking for RNNs", "user": {"login": "shawnjhenry", "id": 9464836, "node_id": "MDQ6VXNlcjk0NjQ4MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9464836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnjhenry", "html_url": "https://github.com/shawnjhenry", "followers_url": "https://api.github.com/users/shawnjhenry/followers", "following_url": "https://api.github.com/users/shawnjhenry/following{/other_user}", "gists_url": "https://api.github.com/users/shawnjhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnjhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnjhenry/subscriptions", "organizations_url": "https://api.github.com/users/shawnjhenry/orgs", "repos_url": "https://api.github.com/users/shawnjhenry/repos", "events_url": "https://api.github.com/users/shawnjhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnjhenry/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 20, "created_at": "2017-01-20T04:47:11Z", "updated_at": "2018-03-21T07:44:55Z", "closed_at": "2017-04-18T22:02:47Z", "author_association": "NONE", "body_html": "<p>Some models with RNN components require batching different length inputs by zero padding them to the same length.  The zero padding can interfere with proper training of the network since RNNs with non-zero biases in the linear layers can have non-zero output even with zero input and state.</p>\n<p>One solution to this problem is to pass in an optional parameter of size batch_size containing the lengths of the sequences, allowing the part of the output corresponding to the zero padding to be masked.  The final state should also be modified to contain the last unmasked hidden state.  This can be implemented between RNN layers as a separate module in forward-only RNNs, but for bidirectional RNNs this is an issue for the backward direction.</p>\n<p>Because of the aforementioned non-zero output for zero input and state, the \"initial\" state for a padded sequence can be non-zero and vary according to the padding size.  Alleviating this problem would require masking between time steps for the backward RNN (and therefore might as well be implemented for the forward RNN as well).</p>\n<p>If this isn't possible, would it be possible to have an option to set only some of the gate biases to zero and not update them?  For example, for LSTMs to learn long-distance relations the forget gate bias should be initialized to a large number (1 or 2) but setting the cell gate bias to zero would eliminate the backward problem mentioned above.</p>\n<p>I could implement masking between time steps for the AutogradRNN, but I'm afraid I don't understand the CuDNN backend well enough to modify that, if it's even possible.</p>", "body_text": "Some models with RNN components require batching different length inputs by zero padding them to the same length.  The zero padding can interfere with proper training of the network since RNNs with non-zero biases in the linear layers can have non-zero output even with zero input and state.\nOne solution to this problem is to pass in an optional parameter of size batch_size containing the lengths of the sequences, allowing the part of the output corresponding to the zero padding to be masked.  The final state should also be modified to contain the last unmasked hidden state.  This can be implemented between RNN layers as a separate module in forward-only RNNs, but for bidirectional RNNs this is an issue for the backward direction.\nBecause of the aforementioned non-zero output for zero input and state, the \"initial\" state for a padded sequence can be non-zero and vary according to the padding size.  Alleviating this problem would require masking between time steps for the backward RNN (and therefore might as well be implemented for the forward RNN as well).\nIf this isn't possible, would it be possible to have an option to set only some of the gate biases to zero and not update them?  For example, for LSTMs to learn long-distance relations the forget gate bias should be initialized to a large number (1 or 2) but setting the cell gate bias to zero would eliminate the backward problem mentioned above.\nI could implement masking between time steps for the AutogradRNN, but I'm afraid I don't understand the CuDNN backend well enough to modify that, if it's even possible.", "body": "Some models with RNN components require batching different length inputs by zero padding them to the same length.  The zero padding can interfere with proper training of the network since RNNs with non-zero biases in the linear layers can have non-zero output even with zero input and state.  \r\n\r\nOne solution to this problem is to pass in an optional parameter of size batch_size containing the lengths of the sequences, allowing the part of the output corresponding to the zero padding to be masked.  The final state should also be modified to contain the last unmasked hidden state.  This can be implemented between RNN layers as a separate module in forward-only RNNs, but for bidirectional RNNs this is an issue for the backward direction.  \r\n\r\nBecause of the aforementioned non-zero output for zero input and state, the \"initial\" state for a padded sequence can be non-zero and vary according to the padding size.  Alleviating this problem would require masking between time steps for the backward RNN (and therefore might as well be implemented for the forward RNN as well).  \r\n\r\nIf this isn't possible, would it be possible to have an option to set only some of the gate biases to zero and not update them?  For example, for LSTMs to learn long-distance relations the forget gate bias should be initialized to a large number (1 or 2) but setting the cell gate bias to zero would eliminate the backward problem mentioned above.  \r\n\r\nI could implement masking between time steps for the AutogradRNN, but I'm afraid I don't understand the CuDNN backend well enough to modify that, if it's even possible.  "}