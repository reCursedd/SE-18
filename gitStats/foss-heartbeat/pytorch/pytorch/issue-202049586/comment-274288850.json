{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274288850", "html_url": "https://github.com/pytorch/pytorch/issues/517#issuecomment-274288850", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/517", "id": 274288850, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDI4ODg1MA==", "user": {"login": "shawnjhenry", "id": 9464836, "node_id": "MDQ6VXNlcjk0NjQ4MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9464836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnjhenry", "html_url": "https://github.com/shawnjhenry", "followers_url": "https://api.github.com/users/shawnjhenry/followers", "following_url": "https://api.github.com/users/shawnjhenry/following{/other_user}", "gists_url": "https://api.github.com/users/shawnjhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnjhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnjhenry/subscriptions", "organizations_url": "https://api.github.com/users/shawnjhenry/orgs", "repos_url": "https://api.github.com/users/shawnjhenry/repos", "events_url": "https://api.github.com/users/shawnjhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnjhenry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-21T21:16:45Z", "updated_at": "2017-01-21T21:19:51Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> Good to know about the caching allocator.  I haven't actually looked to see what bottlenecks exist in PyTorch yet - I was thinking about about Lua Torch.  Before I saw the library I linked I built a similar one and saw up to a 5x speedup over the standard Torch RNN library depending on the input size.  The biggest use of time in the standard library seems to be memory allocation, which is why I thought it might be an issue here, but it sounds like that is not the case.</p>\n<p>In case you're interested, I did some testing, and found that indexing is considerably faster than masking, at least for the vanilla tanh rnn.  Indexing also works by passing a <code>length</code> tensor as I mentioned above, but instead of using it to create a mask, I create an index at step <code>i</code>:</p>\n<pre><code>index = torch.squeeze(torch.nonzero(torch.lt(i, length)))\n</code></pre>\n<p>and use it to do an index select:</p>\n<pre><code>step_input = torch.index_select(input[i], 0, index)\nstep_hidden = torch.index_select(hidden, 0, index)\n</code></pre>\n<p>then after computing:</p>\n<pre><code>step_hidden = inner(step_input, step_hidden, *weight)\n</code></pre>\n<p>I scatter <code>step_hidden</code> into a zero tensor:</p>\n<pre><code>hidden = input.new(hidden.size()).fill_(0)\nhidden.scatter_(0, index.unsqueeze(1).expand_as(step_hidden), step_hidden)\n</code></pre>\n<p>and after the output is concatenated after last time step, I get the final state using gather:</p>\n<pre><code>index = (lengths-1).unsqueeze(0).unsqueeze(2).expand(1, output.size(1), output.size(2))\nhidden = torch.gather(output, 0, index)\n</code></pre>\n<p>all inside the <code>forward</code> function in <code>Recurrent</code>.  For inputs of size <code>(100, 50, 1024)</code> and lengths sampled randomly, this method is, on average, about 25% faster than the equivalent method using a binary mask.  Unfortunately the last step won't work for LSTM since nothing is keeping track of the <code>c</code> state, so that needs a bit of a rethink.</p>", "body_text": "@adamlerer Good to know about the caching allocator.  I haven't actually looked to see what bottlenecks exist in PyTorch yet - I was thinking about about Lua Torch.  Before I saw the library I linked I built a similar one and saw up to a 5x speedup over the standard Torch RNN library depending on the input size.  The biggest use of time in the standard library seems to be memory allocation, which is why I thought it might be an issue here, but it sounds like that is not the case.\nIn case you're interested, I did some testing, and found that indexing is considerably faster than masking, at least for the vanilla tanh rnn.  Indexing also works by passing a length tensor as I mentioned above, but instead of using it to create a mask, I create an index at step i:\nindex = torch.squeeze(torch.nonzero(torch.lt(i, length)))\n\nand use it to do an index select:\nstep_input = torch.index_select(input[i], 0, index)\nstep_hidden = torch.index_select(hidden, 0, index)\n\nthen after computing:\nstep_hidden = inner(step_input, step_hidden, *weight)\n\nI scatter step_hidden into a zero tensor:\nhidden = input.new(hidden.size()).fill_(0)\nhidden.scatter_(0, index.unsqueeze(1).expand_as(step_hidden), step_hidden)\n\nand after the output is concatenated after last time step, I get the final state using gather:\nindex = (lengths-1).unsqueeze(0).unsqueeze(2).expand(1, output.size(1), output.size(2))\nhidden = torch.gather(output, 0, index)\n\nall inside the forward function in Recurrent.  For inputs of size (100, 50, 1024) and lengths sampled randomly, this method is, on average, about 25% faster than the equivalent method using a binary mask.  Unfortunately the last step won't work for LSTM since nothing is keeping track of the c state, so that needs a bit of a rethink.", "body": "@adamlerer Good to know about the caching allocator.  I haven't actually looked to see what bottlenecks exist in PyTorch yet - I was thinking about about Lua Torch.  Before I saw the library I linked I built a similar one and saw up to a 5x speedup over the standard Torch RNN library depending on the input size.  The biggest use of time in the standard library seems to be memory allocation, which is why I thought it might be an issue here, but it sounds like that is not the case.\r\n\r\nIn case you're interested, I did some testing, and found that indexing is considerably faster than masking, at least for the vanilla tanh rnn.  Indexing also works by passing a `length` tensor as I mentioned above, but instead of using it to create a mask, I create an index at step `i`: \r\n```\r\nindex = torch.squeeze(torch.nonzero(torch.lt(i, length)))\r\n```\r\nand use it to do an index select: \r\n```\r\nstep_input = torch.index_select(input[i], 0, index)\r\nstep_hidden = torch.index_select(hidden, 0, index)\r\n```\r\nthen after computing:\r\n```\r\nstep_hidden = inner(step_input, step_hidden, *weight)\r\n```\r\nI scatter `step_hidden` into a zero tensor:\r\n```\r\nhidden = input.new(hidden.size()).fill_(0)\r\nhidden.scatter_(0, index.unsqueeze(1).expand_as(step_hidden), step_hidden)\r\n```\r\nand after the output is concatenated after last time step, I get the final state using gather:\r\n```\r\nindex = (lengths-1).unsqueeze(0).unsqueeze(2).expand(1, output.size(1), output.size(2))\r\nhidden = torch.gather(output, 0, index)\r\n```\r\nall inside the `forward` function in `Recurrent`.  For inputs of size `(100, 50, 1024)` and lengths sampled randomly, this method is, on average, about 25% faster than the equivalent method using a binary mask.  Unfortunately the last step won't work for LSTM since nothing is keeping track of the `c` state, so that needs a bit of a rethink.\r\n"}