{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274282660", "html_url": "https://github.com/pytorch/pytorch/issues/517#issuecomment-274282660", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/517", "id": 274282660, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDI4MjY2MA==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-21T19:28:18Z", "updated_at": "2017-01-21T19:28:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9464836\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shawnjhenry\">@shawnjhenry</a></p>\n<blockquote>\n<p>While you're contemplating how to add variable length sequences to the non-CuDNN RNN library, would you consider some other changes? I've profiled a lot of RNN libraries, and it turns out that most of the calculation time is wasted in kernel launches when operating on the GPU.</p>\n</blockquote>\n<p>Yes, unless you use CuDNN, which combines multiple iterations into a single launch call, you spend most of your time doing kernel launches. You can be clever and try to reduce kernel launches (e.g. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> noticed that you can do <code>W_ih * x</code> for the entire sequence in parallel) but these optimizations trade off speed for flexibility. I have been assuming that if you want speed you'll use CuDNN so I haven't yet optimized the non-CuDNN backend much; it's goal was to be simple, flexible, and match the cuDNN RNN for testing. The non-CuDNN backend has similar performance to the Lua-Torch RNN library I compared to, but I'd be curious to compare with torch-rnn to see</p>\n<blockquote>\n<p>Pre-allocating buffer tensors and doing the calculations in-place on slices, as in this torch library is considerably faster than having to create new tensors at each time step.</p>\n</blockquote>\n<p>Are you sure that's still a bottleneck in PyTorch? That certainly used to be the case in Lua Torch, but the new caching CUDA caching allocator is very fast, and we typically don't see memory allocation affecting performance any more.</p>", "body_text": "@shawnjhenry\n\nWhile you're contemplating how to add variable length sequences to the non-CuDNN RNN library, would you consider some other changes? I've profiled a lot of RNN libraries, and it turns out that most of the calculation time is wasted in kernel launches when operating on the GPU.\n\nYes, unless you use CuDNN, which combines multiple iterations into a single launch call, you spend most of your time doing kernel launches. You can be clever and try to reduce kernel launches (e.g. @apaszke noticed that you can do W_ih * x for the entire sequence in parallel) but these optimizations trade off speed for flexibility. I have been assuming that if you want speed you'll use CuDNN so I haven't yet optimized the non-CuDNN backend much; it's goal was to be simple, flexible, and match the cuDNN RNN for testing. The non-CuDNN backend has similar performance to the Lua-Torch RNN library I compared to, but I'd be curious to compare with torch-rnn to see\n\nPre-allocating buffer tensors and doing the calculations in-place on slices, as in this torch library is considerably faster than having to create new tensors at each time step.\n\nAre you sure that's still a bottleneck in PyTorch? That certainly used to be the case in Lua Torch, but the new caching CUDA caching allocator is very fast, and we typically don't see memory allocation affecting performance any more.", "body": "@shawnjhenry \r\n> While you're contemplating how to add variable length sequences to the non-CuDNN RNN library, would you consider some other changes? I've profiled a lot of RNN libraries, and it turns out that most of the calculation time is wasted in kernel launches when operating on the GPU. \r\n\r\nYes, unless you use CuDNN, which combines multiple iterations into a single launch call, you spend most of your time doing kernel launches. You can be clever and try to reduce kernel launches (e.g. @apaszke noticed that you can do `W_ih * x` for the entire sequence in parallel) but these optimizations trade off speed for flexibility. I have been assuming that if you want speed you'll use CuDNN so I haven't yet optimized the non-CuDNN backend much; it's goal was to be simple, flexible, and match the cuDNN RNN for testing. The non-CuDNN backend has similar performance to the Lua-Torch RNN library I compared to, but I'd be curious to compare with torch-rnn to see\r\n\r\n> Pre-allocating buffer tensors and doing the calculations in-place on slices, as in this torch library is considerably faster than having to create new tensors at each time step.\r\n\r\nAre you sure that's still a bottleneck in PyTorch? That certainly used to be the case in Lua Torch, but the new caching CUDA caching allocator is very fast, and we typically don't see memory allocation affecting performance any more."}