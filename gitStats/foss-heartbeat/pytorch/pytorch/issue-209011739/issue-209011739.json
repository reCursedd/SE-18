{"url": "https://api.github.com/repos/pytorch/pytorch/issues/804", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/804/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/804/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/804/events", "html_url": "https://github.com/pytorch/pytorch/issues/804", "id": 209011739, "node_id": "MDU6SXNzdWUyMDkwMTE3Mzk=", "number": 804, "title": "utils.data.DataLoader freezes", "user": {"login": "gcr", "id": 45252, "node_id": "MDQ6VXNlcjQ1MjUy", "avatar_url": "https://avatars0.githubusercontent.com/u/45252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gcr", "html_url": "https://github.com/gcr", "followers_url": "https://api.github.com/users/gcr/followers", "following_url": "https://api.github.com/users/gcr/following{/other_user}", "gists_url": "https://api.github.com/users/gcr/gists{/gist_id}", "starred_url": "https://api.github.com/users/gcr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gcr/subscriptions", "organizations_url": "https://api.github.com/users/gcr/orgs", "repos_url": "https://api.github.com/users/gcr/repos", "events_url": "https://api.github.com/users/gcr/events{/privacy}", "received_events_url": "https://api.github.com/users/gcr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-02-21T00:29:30Z", "updated_at": "2018-02-11T19:58:29Z", "closed_at": "2017-02-21T00:55:25Z", "author_association": "NONE", "body_html": "<p>Hello! I'm having trouble loading data: the multiprocessing batch data loader freezes! Running on Ubuntu 14.10, Python 2.7.6, Pytorch 0.1.7.post2.</p>\n<p>I'm instantiating a DataLoader like this:</p>\n<pre><code>    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(\"data/everything/\", transforms.Compose([\n            transforms.Scale(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])),\n        batch_size=16, shuffle=False,\n        num_workers=3, pin_memory=False)\n</code></pre>\n<ul>\n<li><em>Symptom</em>: After a few batches (but sometimes after just one or two), the DataLoader refuses to yield any more batches. CPU and disk use drop to zero and everything is frozen.</li>\n<li>On my machine, this happens absolutely 100% of the time.</li>\n<li>If I set <code>num_workers=0</code>, then everything works, so the problem lies within the <code>multiprocessing</code> machinery. Ick.</li>\n<li>Using print statement debugging, I can tell that the workers are hanging in the call to <code>data_queue.put((idx, samples))</code> inside the worker loop, <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/4cc11066b267bfc02769cd6124ff01e41405f66b/torch/utils/data/dataloader.py#L36\">pytorch/torch/utils/data/dataloader.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 36\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/4cc11066b267bfc02769cd6124ff01e41405f66b\">4cc1106</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L36\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"36\"></td>\n          <td id=\"LC36\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> data_queue.put((idx, samples)) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</li>\n<li>If I Ctrl+C this, I get the following (very mangled) traceback from the main process:</li>\n</ul>\n<pre><code>^CTraceback (most recent call last):\n  File \"test.py\", line 30, in &lt;module&gt;\nProcess Process-3:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\nProcess Process-2:\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\n 27, in _worker_loop\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    for i, (input, target) in enumerate(val_loader):\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\n 168, in __next__\n    r = index_queue.get()\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n    idx, batch = self.data_queue.get()\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in ge\n    self.run()\n    return recv()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line\n 21, in recv\n    return recv()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line\n 21, in recv\n    buf = self.recv_bytes()\nKeyboardInterrupt\n    self._target(*self._args, **self._kwargs)\n    buf = self.recv_bytes()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\n 40, in _worker_loop\nKeyboardInterrupt\n    data_queue.put((idx, samples))\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 388, in put\n</code></pre>\n<p>Is there any way to help debug?</p>", "body_text": "Hello! I'm having trouble loading data: the multiprocessing batch data loader freezes! Running on Ubuntu 14.10, Python 2.7.6, Pytorch 0.1.7.post2.\nI'm instantiating a DataLoader like this:\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(\"data/everything/\", transforms.Compose([\n            transforms.Scale(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])),\n        batch_size=16, shuffle=False,\n        num_workers=3, pin_memory=False)\n\n\nSymptom: After a few batches (but sometimes after just one or two), the DataLoader refuses to yield any more batches. CPU and disk use drop to zero and everything is frozen.\nOn my machine, this happens absolutely 100% of the time.\nIf I set num_workers=0, then everything works, so the problem lies within the multiprocessing machinery. Ick.\nUsing print statement debugging, I can tell that the workers are hanging in the call to data_queue.put((idx, samples)) inside the worker loop, \n  \n    \n      pytorch/torch/utils/data/dataloader.py\n    \n    \n         Line 36\n      in\n      4cc1106\n    \n    \n    \n    \n\n        \n          \n           data_queue.put((idx, samples)) \n        \n    \n  \n\n\nIf I Ctrl+C this, I get the following (very mangled) traceback from the main process:\n\n^CTraceback (most recent call last):\n  File \"test.py\", line 30, in <module>\nProcess Process-3:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\nProcess Process-2:\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\n 27, in _worker_loop\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    for i, (input, target) in enumerate(val_loader):\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\n 168, in __next__\n    r = index_queue.get()\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n    idx, batch = self.data_queue.get()\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in ge\n    self.run()\n    return recv()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line\n 21, in recv\n    return recv()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line\n 21, in recv\n    buf = self.recv_bytes()\nKeyboardInterrupt\n    self._target(*self._args, **self._kwargs)\n    buf = self.recv_bytes()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\n 40, in _worker_loop\nKeyboardInterrupt\n    data_queue.put((idx, samples))\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 388, in put\n\nIs there any way to help debug?", "body": "Hello! I'm having trouble loading data: the multiprocessing batch data loader freezes! Running on Ubuntu 14.10, Python 2.7.6, Pytorch 0.1.7.post2.\r\n\r\nI'm instantiating a DataLoader like this:\r\n```\r\n    val_loader = torch.utils.data.DataLoader(\r\n        datasets.ImageFolder(\"data/everything/\", transforms.Compose([\r\n            transforms.Scale(256),\r\n            transforms.CenterCrop(224),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                 std=[0.229, 0.224, 0.225])\r\n        ])),\r\n        batch_size=16, shuffle=False,\r\n        num_workers=3, pin_memory=False)\r\n```\r\n\r\n- *Symptom*: After a few batches (but sometimes after just one or two), the DataLoader refuses to yield any more batches. CPU and disk use drop to zero and everything is frozen.\r\n- On my machine, this happens absolutely 100% of the time.\r\n- If I set `num_workers=0`, then everything works, so the problem lies within the `multiprocessing` machinery. Ick.\r\n- Using print statement debugging, I can tell that the workers are hanging in the call to `data_queue.put((idx, samples))` inside the worker loop, https://github.com/pytorch/pytorch/blob/4cc11066b267bfc02769cd6124ff01e41405f66b/torch/utils/data/dataloader.py#L36\r\n- If I Ctrl+C this, I get the following (very mangled) traceback from the main process:\r\n```\r\n^CTraceback (most recent call last):\r\n  File \"test.py\", line 30, in <module>\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\r\nProcess Process-2:\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\r\n 27, in _worker_loop\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\r\n    for i, (input, target) in enumerate(val_loader):\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\r\n 168, in __next__\r\n    r = index_queue.get()\r\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\r\n    idx, batch = self.data_queue.get()\r\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in ge\r\n    self.run()\r\n    return recv()\r\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line\r\n 21, in recv\r\n    return recv()\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line\r\n 21, in recv\r\n    buf = self.recv_bytes()\r\nKeyboardInterrupt\r\n    self._target(*self._args, **self._kwargs)\r\n    buf = self.recv_bytes()\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line\r\n 40, in _worker_loop\r\nKeyboardInterrupt\r\n    data_queue.put((idx, samples))\r\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 388, in put\r\n```\r\n\r\nIs there any way to help debug?"}