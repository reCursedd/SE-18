{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390729076", "html_url": "https://github.com/pytorch/pytorch/issues/7680#issuecomment-390729076", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7680", "id": 390729076, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDcyOTA3Ng==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-21T17:45:03Z", "updated_at": "2018-05-21T17:45:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Adding <strong>launch_bounds</strong> with the max number of threads the kernel is going to be launched with will cause compiler not to overuse registers. We had to do it e.g. for interp kernels when cuda 9 started using more registers<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/4caea64d729728e3304e9c1e97081a6bdd463913/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu#L15\">pytorch/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 15\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/4caea64d729728e3304e9c1e97081a6bdd463913\">4caea64</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L15\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"15\"></td>\n          <td id=\"LC15\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-en\">__launch_bounds__</span>(<span class=\"pl-c1\">1024</span>) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "Adding launch_bounds with the max number of threads the kernel is going to be launched with will cause compiler not to overuse registers. We had to do it e.g. for interp kernels when cuda 9 started using more registers\n\n  \n    \n      pytorch/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu\n    \n    \n         Line 15\n      in\n      4caea64\n    \n    \n    \n    \n\n        \n          \n           __launch_bounds__(1024)", "body": "Adding __launch_bounds__ with the max number of threads the kernel is going to be launched with will cause compiler not to overuse registers. We had to do it e.g. for interp kernels when cuda 9 started using more registers \r\nhttps://github.com/pytorch/pytorch/blob/4caea64d729728e3304e9c1e97081a6bdd463913/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu#L15"}