{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/313071336", "html_url": "https://github.com/pytorch/pytorch/issues/1979#issuecomment-313071336", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1979", "id": 313071336, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzA3MTMzNg==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-05T11:01:03Z", "updated_at": "2017-07-05T11:01:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Dabbled in adding this, got as far as:</p>\n<pre><code>def embedding(input, weight, padding_idx=None,\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n                 sparse=False):\n    assert not sparse, 'sparse not supported in functional embedding for now'\n    assert padding_idx is None, 'non-None padding_idx not supported in functional embedding currently'\n    if padding_idx is None:\n        padding_idx = -1\n    return self._backend.Embedding(\n        padding_idx, self.max_norm, self.norm_type,\n        self.scale_grad_by_freq, self.sparse\n    )(input, weight)\n</code></pre>\n<p>... but not sure what to put in place of <code>self._backend.Embedding</code>?  Presumably this is some property of <code>Module</code>?  Hypothesized that tensors might have this property, but seems not:</p>\n<pre><code>In [12]: a._backend\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-12-03d251838549&gt; in &lt;module&gt;()\n----&gt; 1 a._backend\n\nAttributeError: 'torch.FloatTensor' object has no attribute '_backend'\n</code></pre>", "body_text": "Dabbled in adding this, got as far as:\ndef embedding(input, weight, padding_idx=None,\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n                 sparse=False):\n    assert not sparse, 'sparse not supported in functional embedding for now'\n    assert padding_idx is None, 'non-None padding_idx not supported in functional embedding currently'\n    if padding_idx is None:\n        padding_idx = -1\n    return self._backend.Embedding(\n        padding_idx, self.max_norm, self.norm_type,\n        self.scale_grad_by_freq, self.sparse\n    )(input, weight)\n\n... but not sure what to put in place of self._backend.Embedding?  Presumably this is some property of Module?  Hypothesized that tensors might have this property, but seems not:\nIn [12]: a._backend\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-12-03d251838549> in <module>()\n----> 1 a._backend\n\nAttributeError: 'torch.FloatTensor' object has no attribute '_backend'", "body": "Dabbled in adding this, got as far as:\r\n\r\n```\r\ndef embedding(input, weight, padding_idx=None,\r\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\r\n                 sparse=False):\r\n    assert not sparse, 'sparse not supported in functional embedding for now'\r\n    assert padding_idx is None, 'non-None padding_idx not supported in functional embedding currently'\r\n    if padding_idx is None:\r\n        padding_idx = -1\r\n    return self._backend.Embedding(\r\n        padding_idx, self.max_norm, self.norm_type,\r\n        self.scale_grad_by_freq, self.sparse\r\n    )(input, weight)\r\n```\r\n\r\n... but not sure what to put in place of `self._backend.Embedding`?  Presumably this is some property of `Module`?  Hypothesized that tensors might have this property, but seems not:\r\n\r\n```\r\nIn [12]: a._backend\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-03d251838549> in <module>()\r\n----> 1 a._backend\r\n\r\nAttributeError: 'torch.FloatTensor' object has no attribute '_backend'\r\n```"}