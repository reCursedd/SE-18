{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146565753", "pull_request_review_id": 71524274, "id": 146565753, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjU2NTc1Mw==", "diff_hunk": "@@ -0,0 +1,454 @@\n+#include \"THCUNN.h\"\n+#include \"THCHalf.h\"\n+#include \"THCTensorTypeUtils.cuh\"\n+#include \"THCHalfAutoNumerics.cuh\"\n+#include \"SharedMem.cuh\"\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Spatial kernel (fast with large inner_size and small dim_size)\n+////////////////////////////////////////////////////////////////////////////////\n+\n+// Let's assume that our input has been flattened to have only three dimension:\n+//     outer x dim x inner\n+// The spatial algorithm tries to paralellize along all of them.\n+// Within a 2d block threadIdx.y paralellizes over dim slices, and threads that\n+// share it will speed up reductions over dim (along axis x).\n+// The 2d grid is used to paralellize inner dimension over y axis and outer over x.\n+\n+inline dim3 SpatialSoftMax_getGridSize(\n+    dim3 block, uint32_t max_active_blocks,\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size) {\n+  // First, tile as many blocks as we can over the y axis\n+  uint32_t inner_blocks = (inner_size + block.y - 1) / block.y;\n+  if (inner_blocks > max_active_blocks)\n+    inner_blocks = max_active_blocks;\n+  // Fill the x axis with as many blocks as we can fit (a little more is ok too)\n+  uint32_t outer_blocks = (max_active_blocks + inner_blocks - 1) / inner_blocks;\n+  if (outer_blocks > outer_size)\n+    outer_blocks = outer_size;\n+  return dim3(outer_blocks, inner_blocks);\n+}\n+\n+inline dim3 SpatialSoftMax_getBlockSize(\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size) {\n+  uint32_t inner_threads = inner_size;\n+  inner_threads = std::min(inner_threads, static_cast<uint32_t>(1024));\n+  uint32_t dim_threads = 1;\n+  if (inner_threads <= 64 && dim_size >= 64) {\n+    while (inner_threads * dim_threads <= 1024 && dim_threads <= dim_size)\n+      dim_threads *= 2;\n+    dim_threads /= 2;\n+  }\n+  return dim3(dim_threads, inner_threads);\n+}\n+\n+template<typename AccumT, typename Kernel>\n+void SpatialSoftMax_getLaunchSizes(\n+    THCState *state, Kernel k,\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size,\n+    dim3& grid, dim3& block, uint32_t& smem_size) {\n+  block = SpatialSoftMax_getBlockSize(outer_size, dim_size, inner_size);\n+  uint32_t block_threads = block.x * block.y;\n+  smem_size = block.x == 1 ? 0 : block_threads * sizeof(AccumT);\n+  int max_active_blocks;\n+  cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks,\n+                                                k, block_threads, smem_size);\n+  max_active_blocks *= THCState_getCurrentDeviceProperties(state)->multiProcessorCount;\n+  grid = SpatialSoftMax_getGridSize(block, max_active_blocks, outer_size, dim_size, inner_size);\n+}\n+\n+template<typename T>\n+struct Add {\n+  __device__ __forceinline__ T operator()(T a, T b) const {\n+    return a + b;\n+  }\n+};\n+\n+template<typename T>\n+struct Max {\n+  __device__ __forceinline__ T operator()(T a, T b) const {\n+    return a < b ? b : a;\n+  }\n+};\n+\n+// Note that it's not a complete block-wide reduction.\n+// Only threads that share threadIdx.y reduce values.\n+template<typename T, template<typename> class ReduceOp>\n+__forceinline__ __device__\n+T spatialBlockReduceX(T *shared, T val) {\n+  ReduceOp<T> r;\n+  shared += threadIdx.y * blockDim.x;\n+\n+  __syncthreads();\n+\n+  shared[threadIdx.x] = val;\n+\n+  // NOTE: loop starts with __syncthreads()\n+  int offset = blockDim.x / 2;\n+  while (offset > 0) {\n+    __syncthreads();\n+    if (threadIdx.x < offset)\n+      shared[threadIdx.x] = r(shared[threadIdx.x], shared[threadIdx.x + offset]);\n+    offset /= 2;\n+  }\n+\n+  __syncthreads();\n+\n+  return shared[0];\n+}\n+\n+template <typename T, typename AccumT, template<typename, typename> class Epilogue>\n+__global__ void cunn_SpatialSoftMaxForward(\n+    T *output, T *input,\n+    uint32_t outer_size, uint32_t dim_size, uint32_t inner_size)\n+{\n+  SharedMem<AccumT> smem;\n+  const uint32_t outer_stride = inner_size * dim_size;\n+  const uint32_t dim_stride = inner_size;\n+\n+  for (uint32_t outer_index = blockIdx.x; outer_index < outer_size; outer_index += gridDim.x) {\n+    const uint32_t outer_offset = outer_index * outer_stride;\n+    for (uint32_t inner_index = blockIdx.y * blockDim.y + threadIdx.y; inner_index < inner_size; inner_index += blockDim.y * gridDim.y) {\n+      const uint32_t data_offset = outer_offset + inner_index;\n+      ////////////////////////////////////////////////////////////\n+      // These two blocks are really eqivalent, but specializing on\n+      // blockDim.x == 1 makes the kernel faster when it's unused.\n+      // I didn't want to thread an extra template parameter, and nvcc\n+      // seems to be smart enough to hoist the if outside of the loops.\n+      ////////////////////////////////////////////////////////////\n+      if (blockDim.x > 1) {\n+        T max_input = THCNumerics<T>::min();\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n+          const T value = input[data_offset + d * dim_stride];\n+          max_input = THCNumerics<T>::ge(max_input, value) ? max_input : value;\n+        }\n+        max_input = ScalarConvert<AccumT, T>::to(\n+            spatialBlockReduceX<AccumT, Max>(smem.getPointer(),\n+                                        ScalarConvert<T, AccumT>::to(max_input)));\n+\n+        AccumT sum = 0;\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n+          sum += THCNumerics<T>::exp(input[data_offset + d * dim_stride] - max_input);\n+        sum = spatialBlockReduceX<AccumT, Add>(smem.getPointer(), sum);\n+\n+        Epilogue<T, AccumT> epilogue(max_input, sum);\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n+          output[data_offset + d * dim_stride] = epilogue(input[data_offset + d * dim_stride]);\n+      } else {\n+        T max_input = THCNumerics<T>::min();\n+        for (uint32_t d = 0; d < dim_size; d++) {\n+          const T value = input[data_offset + d * dim_stride];\n+          max_input = THCNumerics<T>::ge(max_input, value) ? max_input : value;\n+        }\n+\n+        AccumT sum = 0;\n+        for (uint32_t d = 0; d < dim_size; d++)\n+          sum += THCNumerics<T>::exp(input[data_offset + d * dim_stride] - max_input);\n+\n+        Epilogue<T, AccumT> epilogue(max_input, sum);\n+        for (uint32_t d = 0; d < dim_size; d++)\n+          output[data_offset + d * dim_stride] = epilogue(input[data_offset + d * dim_stride]);\n+      }\n+    }\n+  }\n+}\n+\n+template <typename T, typename AccumT, template<typename, typename> class Epilogue>\n+__global__ void cunn_SpatialSoftMaxBackward(\n+    T *gradInput, T *output, T *gradOutput,\n+    uint32_t outer_size, uint32_t dim_size, uint32_t inner_size)\n+{\n+  SharedMem<AccumT> smem;\n+  const uint32_t outer_stride = inner_size * dim_size;\n+  const uint32_t dim_stride = inner_size;\n+\n+  for (uint32_t outer_index = blockIdx.x; outer_index < outer_size; outer_index += gridDim.x) {\n+    const uint32_t outer_offset = outer_index * outer_stride;\n+    for (uint32_t inner_index = blockIdx.y * blockDim.y + threadIdx.y; inner_index < inner_size; inner_index += blockDim.y * gridDim.y) {\n+      const uint32_t data_offset = outer_offset + inner_index;\n+      // See the comment in forward kernel\n+      if (blockDim.x > 1) {\n+        AccumT sum = 0;\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n+          sum += gradOutput[data_offset + d * dim_stride];\n+        sum = spatialBlockReduceX<AccumT, Add>(smem.getPointer(), sum);\n+\n+        Epilogue<T, AccumT> epilogue(sum);\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n+          gradInput[data_offset + d * dim_stride] =\n+            epilogue(gradOutput[data_offset + d * dim_stride],\n+                    output[data_offset + d * dim_stride]);\n+        }\n+      } else {\n+        AccumT sum = 0;\n+        for (uint32_t d = 0; d < dim_size; d++)\n+          sum += gradOutput[data_offset + d * dim_stride];\n+\n+        Epilogue<T, AccumT> epilogue(sum);\n+        for (uint32_t d = 0; d < dim_size; d++) {\n+          gradInput[data_offset + d * dim_stride] =\n+            epilogue(gradOutput[data_offset + d * dim_stride],\n+                    output[data_offset + d * dim_stride]);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Regular kernel (fast when dim_size is large; requires inner_size == 1)\n+////////////////////////////////////////////////////////////////////////////////\n+\n+\n+template <typename T, typename AccumT>\n+struct MaxFloat", "path": "torch/lib/THCUNN/SoftMaxCommon.cuh", "position": 204, "original_position": 204, "commit_id": "866f51e158aa33ca810f4c18dfccac15af752d54", "original_commit_id": "866f51e158aa33ca810f4c18dfccac15af752d54", "user": {"login": "killeent", "id": 4529377, "node_id": "MDQ6VXNlcjQ1MjkzNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4529377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/killeent", "html_url": "https://github.com/killeent", "followers_url": "https://api.github.com/users/killeent/followers", "following_url": "https://api.github.com/users/killeent/following{/other_user}", "gists_url": "https://api.github.com/users/killeent/gists{/gist_id}", "starred_url": "https://api.github.com/users/killeent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/killeent/subscriptions", "organizations_url": "https://api.github.com/users/killeent/orgs", "repos_url": "https://api.github.com/users/killeent/repos", "events_url": "https://api.github.com/users/killeent/events{/privacy}", "received_events_url": "https://api.github.com/users/killeent/received_events", "type": "User", "site_admin": false}, "body": "Bikeshed comment: are these supposed to be for Floats only or any type T?", "created_at": "2017-10-24T13:50:44Z", "updated_at": "2018-11-23T15:35:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/3245#discussion_r146565753", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3245", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146565753"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3245#discussion_r146565753"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3245"}}, "body_html": "<p>Bikeshed comment: are these supposed to be for Floats only or any type T?</p>", "body_text": "Bikeshed comment: are these supposed to be for Floats only or any type T?"}