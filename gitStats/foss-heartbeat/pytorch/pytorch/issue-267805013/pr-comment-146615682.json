{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146615682", "pull_request_review_id": 71583178, "id": 146615682, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjYxNTY4Mg==", "diff_hunk": "@@ -0,0 +1,454 @@\n+#include \"THCUNN.h\"\n+#include \"THCHalf.h\"\n+#include \"THCTensorTypeUtils.cuh\"\n+#include \"THCHalfAutoNumerics.cuh\"\n+#include \"SharedMem.cuh\"\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Spatial kernel (fast with large inner_size and small dim_size)\n+////////////////////////////////////////////////////////////////////////////////\n+\n+// Let's assume that our input has been flattened to have only three dimension:\n+//     outer x dim x inner\n+// The spatial algorithm tries to paralellize along all of them.\n+// Within a 2d block threadIdx.y paralellizes over dim slices, and threads that\n+// share it will speed up reductions over dim (along axis x).\n+// The 2d grid is used to paralellize inner dimension over y axis and outer over x.\n+\n+inline dim3 SpatialSoftMax_getGridSize(\n+    dim3 block, uint32_t max_active_blocks,\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size) {\n+  // First, tile as many blocks as we can over the y axis\n+  uint32_t inner_blocks = (inner_size + block.y - 1) / block.y;\n+  if (inner_blocks > max_active_blocks)\n+    inner_blocks = max_active_blocks;\n+  // Fill the x axis with as many blocks as we can fit (a little more is ok too)\n+  uint32_t outer_blocks = (max_active_blocks + inner_blocks - 1) / inner_blocks;\n+  if (outer_blocks > outer_size)\n+    outer_blocks = outer_size;\n+  return dim3(outer_blocks, inner_blocks);\n+}\n+\n+inline dim3 SpatialSoftMax_getBlockSize(\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size) {\n+  uint32_t inner_threads = inner_size;\n+  inner_threads = std::min(inner_threads, static_cast<uint32_t>(1024));\n+  uint32_t dim_threads = 1;\n+  if (inner_threads <= 64 && dim_size >= 64) {\n+    while (inner_threads * dim_threads <= 1024 && dim_threads <= dim_size)\n+      dim_threads *= 2;\n+    dim_threads /= 2;\n+  }\n+  return dim3(dim_threads, inner_threads);\n+}\n+\n+template<typename AccumT, typename Kernel>\n+void SpatialSoftMax_getLaunchSizes(\n+    THCState *state, Kernel k,\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size,\n+    dim3& grid, dim3& block, uint32_t& smem_size) {\n+  block = SpatialSoftMax_getBlockSize(outer_size, dim_size, inner_size);\n+  uint32_t block_threads = block.x * block.y;\n+  smem_size = block.x == 1 ? 0 : block_threads * sizeof(AccumT);\n+  int max_active_blocks;\n+  cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks,\n+                                                k, block_threads, smem_size);\n+  max_active_blocks *= THCState_getCurrentDeviceProperties(state)->multiProcessorCount;\n+  grid = SpatialSoftMax_getGridSize(block, max_active_blocks, outer_size, dim_size, inner_size);\n+}\n+\n+template<typename T>\n+struct Add {\n+  __device__ __forceinline__ T operator()(T a, T b) const {\n+    return a + b;\n+  }\n+};\n+\n+template<typename T>\n+struct Max {\n+  __device__ __forceinline__ T operator()(T a, T b) const {\n+    return a < b ? b : a;\n+  }\n+};\n+\n+// Note that it's not a complete block-wide reduction.\n+// Only threads that share threadIdx.y reduce values.\n+template<typename T, template<typename> class ReduceOp>\n+__forceinline__ __device__\n+T spatialBlockReduceX(T *shared, T val) {\n+  ReduceOp<T> r;\n+  shared += threadIdx.y * blockDim.x;\n+\n+  __syncthreads();", "path": "torch/lib/THCUNN/SoftMaxCommon.cuh", "position": 82, "original_position": 82, "commit_id": "866f51e158aa33ca810f4c18dfccac15af752d54", "original_commit_id": "866f51e158aa33ca810f4c18dfccac15af752d54", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "It's just safer to have it. If you had two calls to `blockReduceX` one after another, it would be possible that some threads wouldn't read the return value in time before the threads that are already in the next call start writing their values", "created_at": "2017-10-24T16:20:50Z", "updated_at": "2018-11-23T15:35:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/3245#discussion_r146615682", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3245", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146615682"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3245#discussion_r146615682"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3245"}}, "body_html": "<p>It's just safer to have it. If you had two calls to <code>blockReduceX</code> one after another, it would be possible that some threads wouldn't read the return value in time before the threads that are already in the next call start writing their values</p>", "body_text": "It's just safer to have it. If you had two calls to blockReduceX one after another, it would be possible that some threads wouldn't read the return value in time before the threads that are already in the next call start writing their values", "in_reply_to_id": 146564834}