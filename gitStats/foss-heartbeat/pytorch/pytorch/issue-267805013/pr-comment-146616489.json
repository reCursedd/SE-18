{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146616489", "pull_request_review_id": 71584155, "id": 146616489, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjYxNjQ4OQ==", "diff_hunk": "@@ -0,0 +1,454 @@\n+#include \"THCUNN.h\"\n+#include \"THCHalf.h\"\n+#include \"THCTensorTypeUtils.cuh\"\n+#include \"THCHalfAutoNumerics.cuh\"\n+#include \"SharedMem.cuh\"\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Spatial kernel (fast with large inner_size and small dim_size)\n+////////////////////////////////////////////////////////////////////////////////\n+\n+// Let's assume that our input has been flattened to have only three dimension:\n+//     outer x dim x inner\n+// The spatial algorithm tries to paralellize along all of them.\n+// Within a 2d block threadIdx.y paralellizes over dim slices, and threads that\n+// share it will speed up reductions over dim (along axis x).\n+// The 2d grid is used to paralellize inner dimension over y axis and outer over x.\n+\n+inline dim3 SpatialSoftMax_getGridSize(\n+    dim3 block, uint32_t max_active_blocks,\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size) {\n+  // First, tile as many blocks as we can over the y axis\n+  uint32_t inner_blocks = (inner_size + block.y - 1) / block.y;\n+  if (inner_blocks > max_active_blocks)\n+    inner_blocks = max_active_blocks;\n+  // Fill the x axis with as many blocks as we can fit (a little more is ok too)\n+  uint32_t outer_blocks = (max_active_blocks + inner_blocks - 1) / inner_blocks;\n+  if (outer_blocks > outer_size)\n+    outer_blocks = outer_size;\n+  return dim3(outer_blocks, inner_blocks);\n+}\n+\n+inline dim3 SpatialSoftMax_getBlockSize(\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size) {\n+  uint32_t inner_threads = inner_size;\n+  inner_threads = std::min(inner_threads, static_cast<uint32_t>(1024));\n+  uint32_t dim_threads = 1;\n+  if (inner_threads <= 64 && dim_size >= 64) {\n+    while (inner_threads * dim_threads <= 1024 && dim_threads <= dim_size)\n+      dim_threads *= 2;\n+    dim_threads /= 2;\n+  }\n+  return dim3(dim_threads, inner_threads);\n+}\n+\n+template<typename AccumT, typename Kernel>\n+void SpatialSoftMax_getLaunchSizes(\n+    THCState *state, Kernel k,\n+    uint64_t outer_size, uint64_t dim_size, uint64_t inner_size,\n+    dim3& grid, dim3& block, uint32_t& smem_size) {\n+  block = SpatialSoftMax_getBlockSize(outer_size, dim_size, inner_size);\n+  uint32_t block_threads = block.x * block.y;\n+  smem_size = block.x == 1 ? 0 : block_threads * sizeof(AccumT);\n+  int max_active_blocks;\n+  cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks,\n+                                                k, block_threads, smem_size);\n+  max_active_blocks *= THCState_getCurrentDeviceProperties(state)->multiProcessorCount;\n+  grid = SpatialSoftMax_getGridSize(block, max_active_blocks, outer_size, dim_size, inner_size);\n+}\n+\n+template<typename T>\n+struct Add {\n+  __device__ __forceinline__ T operator()(T a, T b) const {\n+    return a + b;\n+  }\n+};\n+\n+template<typename T>\n+struct Max {\n+  __device__ __forceinline__ T operator()(T a, T b) const {\n+    return a < b ? b : a;\n+  }\n+};\n+\n+// Note that it's not a complete block-wide reduction.\n+// Only threads that share threadIdx.y reduce values.\n+template<typename T, template<typename> class ReduceOp>\n+__forceinline__ __device__\n+T spatialBlockReduceX(T *shared, T val) {\n+  ReduceOp<T> r;\n+  shared += threadIdx.y * blockDim.x;\n+\n+  __syncthreads();\n+\n+  shared[threadIdx.x] = val;\n+\n+  // NOTE: loop starts with __syncthreads()\n+  int offset = blockDim.x / 2;\n+  while (offset > 0) {\n+    __syncthreads();\n+    if (threadIdx.x < offset)\n+      shared[threadIdx.x] = r(shared[threadIdx.x], shared[threadIdx.x + offset]);\n+    offset /= 2;\n+  }\n+\n+  __syncthreads();\n+\n+  return shared[0];\n+}\n+\n+template <typename T, typename AccumT, template<typename, typename> class Epilogue>\n+__global__ void cunn_SpatialSoftMaxForward(\n+    T *output, T *input,\n+    uint32_t outer_size, uint32_t dim_size, uint32_t inner_size)\n+{\n+  SharedMem<AccumT> smem;\n+  const uint32_t outer_stride = inner_size * dim_size;\n+  const uint32_t dim_stride = inner_size;\n+\n+  for (uint32_t outer_index = blockIdx.x; outer_index < outer_size; outer_index += gridDim.x) {\n+    const uint32_t outer_offset = outer_index * outer_stride;\n+    for (uint32_t inner_index = blockIdx.y * blockDim.y + threadIdx.y; inner_index < inner_size; inner_index += blockDim.y * gridDim.y) {\n+      const uint32_t data_offset = outer_offset + inner_index;\n+      ////////////////////////////////////////////////////////////\n+      // These two blocks are really eqivalent, but specializing on\n+      // blockDim.x == 1 makes the kernel faster when it's unused.\n+      // I didn't want to thread an extra template parameter, and nvcc\n+      // seems to be smart enough to hoist the if outside of the loops.\n+      ////////////////////////////////////////////////////////////\n+      if (blockDim.x > 1) {\n+        T max_input = THCNumerics<T>::min();\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n+          const T value = input[data_offset + d * dim_stride];\n+          max_input = THCNumerics<T>::ge(max_input, value) ? max_input : value;\n+        }\n+        max_input = ScalarConvert<AccumT, T>::to(\n+            spatialBlockReduceX<AccumT, Max>(smem.getPointer(),\n+                                        ScalarConvert<T, AccumT>::to(max_input)));\n+\n+        AccumT sum = 0;\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n+          sum += THCNumerics<T>::exp(input[data_offset + d * dim_stride] - max_input);\n+        sum = spatialBlockReduceX<AccumT, Add>(smem.getPointer(), sum);\n+\n+        Epilogue<T, AccumT> epilogue(max_input, sum);\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n+          output[data_offset + d * dim_stride] = epilogue(input[data_offset + d * dim_stride]);\n+      } else {\n+        T max_input = THCNumerics<T>::min();\n+        for (uint32_t d = 0; d < dim_size; d++) {\n+          const T value = input[data_offset + d * dim_stride];\n+          max_input = THCNumerics<T>::ge(max_input, value) ? max_input : value;\n+        }\n+\n+        AccumT sum = 0;\n+        for (uint32_t d = 0; d < dim_size; d++)\n+          sum += THCNumerics<T>::exp(input[data_offset + d * dim_stride] - max_input);\n+\n+        Epilogue<T, AccumT> epilogue(max_input, sum);\n+        for (uint32_t d = 0; d < dim_size; d++)\n+          output[data_offset + d * dim_stride] = epilogue(input[data_offset + d * dim_stride]);\n+      }\n+    }\n+  }\n+}\n+\n+template <typename T, typename AccumT, template<typename, typename> class Epilogue>\n+__global__ void cunn_SpatialSoftMaxBackward(\n+    T *gradInput, T *output, T *gradOutput,\n+    uint32_t outer_size, uint32_t dim_size, uint32_t inner_size)\n+{\n+  SharedMem<AccumT> smem;\n+  const uint32_t outer_stride = inner_size * dim_size;\n+  const uint32_t dim_stride = inner_size;\n+\n+  for (uint32_t outer_index = blockIdx.x; outer_index < outer_size; outer_index += gridDim.x) {\n+    const uint32_t outer_offset = outer_index * outer_stride;\n+    for (uint32_t inner_index = blockIdx.y * blockDim.y + threadIdx.y; inner_index < inner_size; inner_index += blockDim.y * gridDim.y) {\n+      const uint32_t data_offset = outer_offset + inner_index;\n+      // See the comment in forward kernel\n+      if (blockDim.x > 1) {\n+        AccumT sum = 0;\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n+          sum += gradOutput[data_offset + d * dim_stride];\n+        sum = spatialBlockReduceX<AccumT, Add>(smem.getPointer(), sum);\n+\n+        Epilogue<T, AccumT> epilogue(sum);\n+        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n+          gradInput[data_offset + d * dim_stride] =\n+            epilogue(gradOutput[data_offset + d * dim_stride],\n+                    output[data_offset + d * dim_stride]);\n+        }\n+      } else {\n+        AccumT sum = 0;\n+        for (uint32_t d = 0; d < dim_size; d++)\n+          sum += gradOutput[data_offset + d * dim_stride];\n+\n+        Epilogue<T, AccumT> epilogue(sum);\n+        for (uint32_t d = 0; d < dim_size; d++) {\n+          gradInput[data_offset + d * dim_stride] =\n+            epilogue(gradOutput[data_offset + d * dim_stride],\n+                    output[data_offset + d * dim_stride]);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Regular kernel (fast when dim_size is large; requires inner_size == 1)\n+////////////////////////////////////////////////////////////////////////////////\n+\n+\n+template <typename T, typename AccumT>\n+struct MaxFloat\n+{\n+  __device__ __forceinline__ AccumT operator()(AccumT max, T v) const {\n+    return fmaxType(max, v);\n+  }\n+};\n+\n+template<typename T, typename AccumT>\n+struct AddFloat\n+{\n+  __device__ __forceinline__ AccumT operator()(AccumT sum, T v) const {\n+    return sum + v;\n+  }\n+};\n+\n+template<typename T, typename AccumT>\n+struct SumExpFloat\n+{\n+  __device__ __forceinline__ SumExpFloat(T v)\n+    : max_k(v) {}\n+\n+  __device__ __forceinline__ AccumT operator()(AccumT sum, T v) const {\n+    return sum + THCNumerics<T>::exp(v - max_k);\n+  }\n+\n+  const T max_k;\n+};\n+\n+template <template<typename> class Reduction, typename AccumT>\n+__device__ __forceinline__ AccumT\n+blockReduce(AccumT* smem, AccumT val,\n+            const Reduction<AccumT>& r,\n+            AccumT defaultVal)\n+{\n+  // To avoid RaW races from chaining blockReduce calls together, we need a sync here\n+  __syncthreads();\n+\n+  smem[threadIdx.x] = val;\n+\n+  __syncthreads();\n+\n+  AccumT warpVal = defaultVal;\n+\n+  // First warp will perform per-warp reductions for the remaining warps\n+  if (threadIdx.x < 32) {\n+    int lane = threadIdx.x % 32;\n+    if (lane < blockDim.x / 32) {\n+#pragma unroll\n+      for (int i = 0; i < 32; ++i) {\n+        warpVal = r(warpVal, smem[lane * 32 + i]);\n+      }\n+      smem[lane] = warpVal;\n+    }\n+  }\n+\n+  __syncthreads();\n+\n+  // First thread will perform a reduction of the above per-warp reductions\n+  AccumT blockVal = defaultVal;\n+\n+  if (threadIdx.x == 0) {\n+    for (int i = 0; i < blockDim.x / 32; ++i) {\n+      blockVal = r(blockVal, smem[i]);\n+    }\n+    smem[0] = blockVal;\n+  }\n+\n+  // Sync and broadcast\n+  __syncthreads();\n+  return smem[0];\n+}\n+\n+template <template<typename, typename> class Reduction, int ILP, typename T, typename AccumT>\n+__device__ __forceinline__ AccumT\n+ilpReduce(T* data,\n+          int size,\n+          const Reduction<T, AccumT>& r,\n+          AccumT defaultVal)\n+{\n+  AccumT threadVal = defaultVal;\n+  int offset = threadIdx.x;\n+\n+  int last = size % (ILP * blockDim.x);\n+\n+  // Body (unroll by ILP times)\n+  for (; offset < size - last; offset += blockDim.x * ILP) {", "path": "torch/lib/THCUNN/SoftMaxCommon.cuh", "position": 289, "original_position": 289, "commit_id": "866f51e158aa33ca810f4c18dfccac15af752d54", "original_commit_id": "866f51e158aa33ca810f4c18dfccac15af752d54", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I'm sorry, I'm not sure what are you referring to. BTW this is also only a part of the old kernel that I left untouched", "created_at": "2017-10-24T16:23:54Z", "updated_at": "2018-11-23T15:35:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/3245#discussion_r146616489", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3245", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146616489"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3245#discussion_r146616489"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3245"}}, "body_html": "<p>I'm sorry, I'm not sure what are you referring to. BTW this is also only a part of the old kernel that I left untouched</p>", "body_text": "I'm sorry, I'm not sure what are you referring to. BTW this is also only a part of the old kernel that I left untouched", "in_reply_to_id": 146566471}