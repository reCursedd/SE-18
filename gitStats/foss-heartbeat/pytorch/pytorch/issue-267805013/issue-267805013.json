{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3245", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3245/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3245/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3245/events", "html_url": "https://github.com/pytorch/pytorch/pull/3245", "id": 267805013, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQ4MjMxMjcw", "number": 3245, "title": "Softmax/LogSoftMax refactor (wrapped up)", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-23T20:23:55Z", "updated_at": "2018-11-23T15:35:40Z", "closed_at": "2017-10-25T12:47:56Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3245", "html_url": "https://github.com/pytorch/pytorch/pull/3245", "diff_url": "https://github.com/pytorch/pytorch/pull/3245.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3245.patch"}, "body_html": "<p>These commits wrap up the previous Softmax refactor. All the important changes are on the CUDA side. Once I unified the code I also added a special instantiation that made the kernels faster in certain cases (small inner dim, large softmax dim - might be useful in NLP for short sequences?)</p>\n<p><strong>tl;dr</strong> CUDA Softmax now supports a <code>dim</code> argument, and is usually 4x-256x faster than the previous implementation (it didn't have a spatial implementation before). Now, it also shares kernels with LogSoftmax, and certain optimizations benefited the log case giving up to 64x speedup in certain cases as well.</p>\n<hr>\n<p>Here are the plots that show old / new timing ratios for different sizes of <code>dim</code> and size of the innermost dimensions (on the left of dim). Paralellizing over batch is easy, so it is fixed as 64 in all plots. Red dots are better, blue are regressions. Note that the plot is log in all axis (so z of 8 means 2^8x faster)</p>\n<h4>Softmax</h4>\n<p>Benefits from this diff all over the place. The old kernel was written in a quite archaic way.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/4583066/31911090-d728ffe2-b83f-11e7-92a4-3a4c2cc6516c.png\"><img src=\"https://user-images.githubusercontent.com/4583066/31911090-d728ffe2-b83f-11e7-92a4-3a4c2cc6516c.png\" alt=\"softmax\" style=\"max-width:100%;\"></a></p>\n<h4>LogSoftmax</h4>\n<p>Benefits from adding a custom kernel for the cases when <code>inner_size</code> is no longer 1, so we can't use the super fast kernel, but the <code>dim_size</code> is large, so using a single thread to reduce values is slow. It is only enabled for a subset of the space where it provided speedups.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/4583066/31911085-d259995e-b83f-11e7-98b4-1eae37691065.png\"><img src=\"https://user-images.githubusercontent.com/4583066/31911085-d259995e-b83f-11e7-98b4-1eae37691065.png\" alt=\"log\" style=\"max-width:100%;\"></a></p>\n<h4>Overall times</h4>\n<p>These are the log plots (in all axes) of the time (no more ratios) for the new algorithm. Softmax on the left, LogSoftmax on the right:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/4583066/31911403-ce52591c-b840-11e7-8a29-ef575beb4126.png\"><img src=\"https://user-images.githubusercontent.com/4583066/31911403-ce52591c-b840-11e7-8a29-ef575beb4126.png\" alt=\"times\" style=\"max-width:100%;\"></a></p>", "body_text": "These commits wrap up the previous Softmax refactor. All the important changes are on the CUDA side. Once I unified the code I also added a special instantiation that made the kernels faster in certain cases (small inner dim, large softmax dim - might be useful in NLP for short sequences?)\ntl;dr CUDA Softmax now supports a dim argument, and is usually 4x-256x faster than the previous implementation (it didn't have a spatial implementation before). Now, it also shares kernels with LogSoftmax, and certain optimizations benefited the log case giving up to 64x speedup in certain cases as well.\n\nHere are the plots that show old / new timing ratios for different sizes of dim and size of the innermost dimensions (on the left of dim). Paralellizing over batch is easy, so it is fixed as 64 in all plots. Red dots are better, blue are regressions. Note that the plot is log in all axis (so z of 8 means 2^8x faster)\nSoftmax\nBenefits from this diff all over the place. The old kernel was written in a quite archaic way.\n\nLogSoftmax\nBenefits from adding a custom kernel for the cases when inner_size is no longer 1, so we can't use the super fast kernel, but the dim_size is large, so using a single thread to reduce values is slow. It is only enabled for a subset of the space where it provided speedups.\n\nOverall times\nThese are the log plots (in all axes) of the time (no more ratios) for the new algorithm. Softmax on the left, LogSoftmax on the right:", "body": "These commits wrap up the previous Softmax refactor. All the important changes are on the CUDA side. Once I unified the code I also added a special instantiation that made the kernels faster in certain cases (small inner dim, large softmax dim - might be useful in NLP for short sequences?)\r\n\r\n**tl;dr** CUDA Softmax now supports a `dim` argument, and is usually 4x-256x faster than the previous implementation (it didn't have a spatial implementation before). Now, it also shares kernels with LogSoftmax, and certain optimizations benefited the log case giving up to 64x speedup in certain cases as well.\r\n\r\n---\r\n\r\nHere are the plots that show old / new timing ratios for different sizes of `dim` and size of the innermost dimensions (on the left of dim). Paralellizing over batch is easy, so it is fixed as 64 in all plots. Red dots are better, blue are regressions. Note that the plot is log in all axis (so z of 8 means 2^8x faster)\r\n\r\n#### Softmax\r\n\r\nBenefits from this diff all over the place. The old kernel was written in a quite archaic way.\r\n\r\n![softmax](https://user-images.githubusercontent.com/4583066/31911090-d728ffe2-b83f-11e7-92a4-3a4c2cc6516c.png)\r\n\r\n#### LogSoftmax\r\n\r\nBenefits from adding a custom kernel for the cases when `inner_size` is no longer 1, so we can't use the super fast kernel, but the `dim_size` is large, so using a single thread to reduce values is slow. It is only enabled for a subset of the space where it provided speedups.\r\n\r\n![log](https://user-images.githubusercontent.com/4583066/31911085-d259995e-b83f-11e7-98b4-1eae37691065.png)\r\n\r\n#### Overall times\r\n\r\nThese are the log plots (in all axes) of the time (no more ratios) for the new algorithm. Softmax on the left, LogSoftmax on the right:\r\n\r\n![times](https://user-images.githubusercontent.com/4583066/31911403-ce52591c-b840-11e7-8a29-ef575beb4126.png)"}