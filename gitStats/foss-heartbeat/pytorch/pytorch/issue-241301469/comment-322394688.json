{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/322394688", "html_url": "https://github.com/pytorch/pytorch/issues/2006#issuecomment-322394688", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2006", "id": 322394688, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjM5NDY4OA==", "user": {"login": "t-ae", "id": 12446914, "node_id": "MDQ6VXNlcjEyNDQ2OTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/12446914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-ae", "html_url": "https://github.com/t-ae", "followers_url": "https://api.github.com/users/t-ae/followers", "following_url": "https://api.github.com/users/t-ae/following{/other_user}", "gists_url": "https://api.github.com/users/t-ae/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-ae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-ae/subscriptions", "organizations_url": "https://api.github.com/users/t-ae/orgs", "repos_url": "https://api.github.com/users/t-ae/repos", "events_url": "https://api.github.com/users/t-ae/events{/privacy}", "received_events_url": "https://api.github.com/users/t-ae/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-15T07:17:49Z", "updated_at": "2017-08-15T08:26:29Z", "author_association": "NONE", "body_html": "<p><del>We don't need whole tensor to be contiguous.</del><br>\n<del>If we perform 3D <code>tensor.sum((1, 2))</code>, can reduce <code>sum</code> calls like</del></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> tensor.stride(<span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span>tensor.size(<span class=\"pl-c1\">2</span>) <span class=\"pl-k\">==</span> tensor.stride(<span class=\"pl-c1\">1</span>):\n    output <span class=\"pl-k\">=</span> tensor.view(tensor.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>).sum(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">else</span>:\n    output <span class=\"pl-k\">=</span> tensor.sum(<span class=\"pl-c1\">2</span>).sum(<span class=\"pl-c1\">1</span>)</pre></div>\n<p>And I think <code>mean</code>, <code>median</code>, <code>mode</code>, <code>min</code>, <code>max</code>, <code>std</code>, <code>var</code>, (and <code>norm</code>?) should have same signature for consistency.</p>\n<p><strong>Edit:</strong> I didn't know <code>view</code> can be performed only on contiguous tensor...<br>\n<code>contiguous</code> call is needed as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> mentioned.</p>\n<p>While we can avoid <code>contiguous</code> call like below.</p>\n<div class=\"highlight highlight-source-python\"><pre>order <span class=\"pl-k\">=</span> np.argsort(tensor.stride())[::<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>].tolist()\npermuted <span class=\"pl-k\">=</span> tensor.permute(<span class=\"pl-k\">*</span>order)\n\n<span class=\"pl-k\">if</span> permuted.is_contiguous():\n    axes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: order[x], axes))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> contiguous version</span>\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> not contiguous version</span></pre></div>", "body_text": "We don't need whole tensor to be contiguous.\nIf we perform 3D tensor.sum((1, 2)), can reduce sum calls like\nif tensor.stride(2)*tensor.size(2) == tensor.stride(1):\n    output = tensor.view(tensor.size(0), -1).sum(1)\nelse:\n    output = tensor.sum(2).sum(1)\nAnd I think mean, median, mode, min, max, std, var, (and norm?) should have same signature for consistency.\nEdit: I didn't know view can be performed only on contiguous tensor...\ncontiguous call is needed as @fmassa mentioned.\nWhile we can avoid contiguous call like below.\norder = np.argsort(tensor.stride())[::-1].tolist()\npermuted = tensor.permute(*order)\n\nif permuted.is_contiguous():\n    axes = tuple(map(lambda x: order[x], axes))\n    # contiguous version\nelse:\n    # not contiguous version", "body": "~We don't need whole tensor to be contiguous.~\r\n~If we perform 3D `tensor.sum((1, 2))`, can reduce `sum` calls like~\r\n```python\r\nif tensor.stride(2)*tensor.size(2) == tensor.stride(1):\r\n    output = tensor.view(tensor.size(0), -1).sum(1)\r\nelse:\r\n    output = tensor.sum(2).sum(1)\r\n```\r\n\r\nAnd I think `mean`, `median`, `mode`, `min`, `max`, `std`, `var`, (and `norm`?) should have same signature for consistency.\r\n\r\n**Edit:** I didn't know `view` can be performed only on contiguous tensor...\r\n`contiguous` call is needed as @fmassa mentioned.\r\n\r\nWhile we can avoid `contiguous` call like below.\r\n```python\r\norder = np.argsort(tensor.stride())[::-1].tolist()\r\npermuted = tensor.permute(*order)\r\n\r\nif permuted.is_contiguous():\r\n    axes = tuple(map(lambda x: order[x], axes))\r\n    # contiguous version\r\nelse:\r\n    # not contiguous version\r\n```"}