{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12531", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12531/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12531/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12531/events", "html_url": "https://github.com/pytorch/pytorch/issues/12531", "id": 368677062, "node_id": "MDU6SXNzdWUzNjg2NzcwNjI=", "number": 12531, "title": "Pytorch Convolution Layer Blocking Process in Linux", "user": {"login": "Olloxan", "id": 24493577, "node_id": "MDQ6VXNlcjI0NDkzNTc3", "avatar_url": "https://avatars0.githubusercontent.com/u/24493577?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Olloxan", "html_url": "https://github.com/Olloxan", "followers_url": "https://api.github.com/users/Olloxan/followers", "following_url": "https://api.github.com/users/Olloxan/following{/other_user}", "gists_url": "https://api.github.com/users/Olloxan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Olloxan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Olloxan/subscriptions", "organizations_url": "https://api.github.com/users/Olloxan/orgs", "repos_url": "https://api.github.com/users/Olloxan/repos", "events_url": "https://api.github.com/users/Olloxan/events{/privacy}", "received_events_url": "https://api.github.com/users/Olloxan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-10T13:52:27Z", "updated_at": "2018-10-15T17:43:33Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I am trying to implement an A3C algorithm in pytorch with some convolution layers. Therefore I start multiple processes in my Python programm each with a local neuronal network. The Network gets an image as input witch is processed by a convolution layer.<br>\nWhen I run this programm on a windows machine it has no problems and works fine but on a linux machine the process is somehow blocked when it trys to use the forward method of the network.</p>\n<p>Here is a minimal example:</p>\n<p>`<br>\nimport numpy as np<br>\nimport torch<br>\nimport torch.nn as nn<br>\nimport torch.nn.functional as F<br>\nimport torch.multiprocessing as mp</p>\n<pre><code>state_shape = (3,10,10)\nnum_actions = 5\n\nclass Net(nn.Module):\n    def __init__(self, in_shape, num_actions):\n        super(Net, self).__init__()\n        self.in_shape = in_shape\n        self.num_actions = num_actions\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n            nn.ReLU(),\n        )\n                             \n        self.fc = nn.Sequential(\n            nn.Linear(self.feature_size(), 256), # &lt;-- problematic\n            nn.ReLU(),\n        )\n                   \n    def forward(self, x):  \n        x = self.features(x)   \n            \n    def feature_size(self):\n        return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)\n\nclass Worker(mp.Process):\n    def __init__(self):\n        super(Worker, self).__init__()       \n        self.local_network = Net(state_shape, num_actions)           # local network\n            \n    def run(self):        \n        s = np.zeros((3,10,10), dtype=np.float32)                           \n        for i in range(10):\n            print(\"{0}_before\".format(i))                \n            self.local_network.forward(torch.FloatTensor(s).unsqueeze(0))\n            print(\"{0}_after\".format(i))\n            time.sleep(0.1)  \n                    \nif __name__ == \"__main__\":              \n    # parallel training\n    workers = [Worker() for i in range(1)]\n    [w.start() for w in workers]\n    \n    [w.join(timeout=10) for w in workers]\n\n    [w.terminate() for w in workers]\n</code></pre>\n<p>`<br>\nI am using:<br>\ntorch                 0.4.1 and<br>\ntorchvision           0.2.1</p>\n<p>It seems that the initialisation process is somehow faulty. When initializing the Linear layer of the notwork the convolution output must be flattened to know the inputsize of the linear layer. Therefore</p>\n<pre><code>`self.feature_size()`\n</code></pre>\n<p>is called. The computation of the sequential in the initialization process seems to cause the problem. Unfortunately I have no idea why. Has anyone encountered the same issue?</p>", "body_text": "I am trying to implement an A3C algorithm in pytorch with some convolution layers. Therefore I start multiple processes in my Python programm each with a local neuronal network. The Network gets an image as input witch is processed by a convolution layer.\nWhen I run this programm on a windows machine it has no problems and works fine but on a linux machine the process is somehow blocked when it trys to use the forward method of the network.\nHere is a minimal example:\n`\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\nstate_shape = (3,10,10)\nnum_actions = 5\n\nclass Net(nn.Module):\n    def __init__(self, in_shape, num_actions):\n        super(Net, self).__init__()\n        self.in_shape = in_shape\n        self.num_actions = num_actions\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=2),\n            nn.ReLU(),\n        )\n                             \n        self.fc = nn.Sequential(\n            nn.Linear(self.feature_size(), 256), # <-- problematic\n            nn.ReLU(),\n        )\n                   \n    def forward(self, x):  \n        x = self.features(x)   \n            \n    def feature_size(self):\n        return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)\n\nclass Worker(mp.Process):\n    def __init__(self):\n        super(Worker, self).__init__()       \n        self.local_network = Net(state_shape, num_actions)           # local network\n            \n    def run(self):        \n        s = np.zeros((3,10,10), dtype=np.float32)                           \n        for i in range(10):\n            print(\"{0}_before\".format(i))                \n            self.local_network.forward(torch.FloatTensor(s).unsqueeze(0))\n            print(\"{0}_after\".format(i))\n            time.sleep(0.1)  \n                    \nif __name__ == \"__main__\":              \n    # parallel training\n    workers = [Worker() for i in range(1)]\n    [w.start() for w in workers]\n    \n    [w.join(timeout=10) for w in workers]\n\n    [w.terminate() for w in workers]\n\n`\nI am using:\ntorch                 0.4.1 and\ntorchvision           0.2.1\nIt seems that the initialisation process is somehow faulty. When initializing the Linear layer of the notwork the convolution output must be flattened to know the inputsize of the linear layer. Therefore\n`self.feature_size()`\n\nis called. The computation of the sequential in the initialization process seems to cause the problem. Unfortunately I have no idea why. Has anyone encountered the same issue?", "body": "I am trying to implement an A3C algorithm in pytorch with some convolution layers. Therefore I start multiple processes in my Python programm each with a local neuronal network. The Network gets an image as input witch is processed by a convolution layer.\r\nWhen I run this programm on a windows machine it has no problems and works fine but on a linux machine the process is somehow blocked when it trys to use the forward method of the network.\r\n\r\nHere is a minimal example:\r\n\r\n`\r\n    import numpy as np\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    import torch.multiprocessing as mp\r\n    \r\n    \r\n    state_shape = (3,10,10)\r\n    num_actions = 5\r\n    \r\n    class Net(nn.Module):\r\n        def __init__(self, in_shape, num_actions):\r\n            super(Net, self).__init__()\r\n            self.in_shape = in_shape\r\n            self.num_actions = num_actions\r\n            \r\n            self.features = nn.Sequential(\r\n                nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),\r\n                nn.ReLU(),\r\n                nn.Conv2d(16, 16, kernel_size=3, stride=2),\r\n                nn.ReLU(),\r\n            )\r\n                                 \r\n            self.fc = nn.Sequential(\r\n                nn.Linear(self.feature_size(), 256), # <-- problematic\r\n                nn.ReLU(),\r\n            )\r\n                       \r\n        def forward(self, x):  \r\n            x = self.features(x)   \r\n                \r\n        def feature_size(self):\r\n            return self.features(torch.zeros(1, *self.in_shape)).view(1, -1).size(1)\r\n    \r\n    class Worker(mp.Process):\r\n        def __init__(self):\r\n            super(Worker, self).__init__()       \r\n            self.local_network = Net(state_shape, num_actions)           # local network\r\n                \r\n        def run(self):        \r\n            s = np.zeros((3,10,10), dtype=np.float32)                           \r\n            for i in range(10):\r\n                print(\"{0}_before\".format(i))                \r\n                self.local_network.forward(torch.FloatTensor(s).unsqueeze(0))\r\n                print(\"{0}_after\".format(i))\r\n                time.sleep(0.1)  \r\n                        \r\n    if __name__ == \"__main__\":              \r\n        # parallel training\r\n        workers = [Worker() for i in range(1)]\r\n        [w.start() for w in workers]\r\n        \r\n        [w.join(timeout=10) for w in workers]\r\n\r\n        [w.terminate() for w in workers]\r\n`\r\nI am using:\r\ntorch                 0.4.1 and\r\ntorchvision           0.2.1\r\n\r\nIt seems that the initialisation process is somehow faulty. When initializing the Linear layer of the notwork the convolution output must be flattened to know the inputsize of the linear layer. Therefore \r\n\r\n    `self.feature_size()`\r\n\r\nis called. The computation of the sequential in the initialization process seems to cause the problem. Unfortunately I have no idea why. Has anyone encountered the same issue?"}