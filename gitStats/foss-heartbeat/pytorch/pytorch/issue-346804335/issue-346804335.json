{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10148", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10148/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10148/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10148/events", "html_url": "https://github.com/pytorch/pytorch/issues/10148", "id": 346804335, "node_id": "MDU6SXNzdWUzNDY4MDQzMzU=", "number": 10148, "title": "MSELoss wrongly sums instead of averages when reduction='elementwise_mean'", "user": {"login": "catalys1", "id": 11340846, "node_id": "MDQ6VXNlcjExMzQwODQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/11340846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/catalys1", "html_url": "https://github.com/catalys1", "followers_url": "https://api.github.com/users/catalys1/followers", "following_url": "https://api.github.com/users/catalys1/following{/other_user}", "gists_url": "https://api.github.com/users/catalys1/gists{/gist_id}", "starred_url": "https://api.github.com/users/catalys1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/catalys1/subscriptions", "organizations_url": "https://api.github.com/users/catalys1/orgs", "repos_url": "https://api.github.com/users/catalys1/repos", "events_url": "https://api.github.com/users/catalys1/events{/privacy}", "received_events_url": "https://api.github.com/users/catalys1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-08-01T23:19:10Z", "updated_at": "2018-09-19T14:15:16Z", "closed_at": "2018-08-02T02:44:01Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I was training an autoencoder with MSELoss, and the loss values on the training data were huge but the loss values on the validation data were small. It appeared as if the loss was not being averaged on the training pass, but it was on the validation pass. A little poking around in the debugger revealed this to be the case.</p>\n<p>The problem is in the <code>_pointwise_loss</code> loss function in <code>torch/nn/functional.py</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_pointwise_loss</span>(<span class=\"pl-smi\">lambd</span>, <span class=\"pl-smi\">lambd_optimized</span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>elementwise_mean<span class=\"pl-pds\">'</span></span>):\n    <span class=\"pl-k\">if</span> target.requires_grad:\n        d <span class=\"pl-k\">=</span> lambd(<span class=\"pl-c1\">input</span>, target)\n        <span class=\"pl-k\">if</span> reduction <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">return</span> d\n        <span class=\"pl-k\">return</span> torch.mean(d) <span class=\"pl-k\">if</span> reduction <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>elementwise_mean<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> torch.sum(d)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> lambd_optimized(<span class=\"pl-c1\">input</span>, target, _Reduction.get_enum(reduction))</pre></div>\n<p>In the line <code>return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)</code>, the variable <code>reduction</code> is actually an integer with value <code>1</code>, not a string with value <code>'elementwise_mean'</code>, and so the loss is summed instead of averaged, even though the 'elementwise_mean` option was chosen.</p>\n<h2>Code example</h2>\n<p>This code reproduces the problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torchvision <span class=\"pl-k\">as</span> tv\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.c1 <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.c2 <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.relu <span class=\"pl-k\">=</span> torch.nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.c2(<span class=\"pl-c1\">self</span>.relu(<span class=\"pl-c1\">self</span>.c1(x)))\n        <span class=\"pl-k\">return</span> x\n\nnet <span class=\"pl-k\">=</span> Net()\ndata <span class=\"pl-k\">=</span> torchvision.datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MNIST<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">True</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>tv.transforms.ToTensor())\nmse <span class=\"pl-k\">=</span> torch.nn.MSELoss(<span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>elementwise_mean<span class=\"pl-pds\">'</span></span>)\n\nimg <span class=\"pl-k\">=</span> data[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>].view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>).requires_grad_()\npred <span class=\"pl-k\">=</span> net(img)\n\nloss1 <span class=\"pl-k\">=</span> mse(pred, img).item()\n\nimg.requires_grad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\nloss2 <span class=\"pl-k\">=</span> mse(pred, img).item()\n\n<span class=\"pl-c1\">print</span>(loss1, loss2)</pre></div>\n<p>And the output is:<br>\n<code>155.7478  0.1987</code></p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.2 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.10.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 7.5.17<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080 Ti<br>\nGPU 1: GeForce GTX 1080 Ti<br>\nGPU 2: GeForce GTX 1080 Ti<br>\nGPU 3: GeForce GTX 1080 Ti<br>\nGPU 4: GeForce GTX 1080 Ti<br>\nGPU 5: GeForce GTX 1080 Ti<br>\nGPU 6: GeForce GTX 1080 Ti<br>\nGPU 7: GeForce GTX 1080 Ti</p>\n<p>Nvidia driver version: 387.34<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.0.5<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so.7<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.3<br>\n/usr/local/cuda-9.0/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nI was training an autoencoder with MSELoss, and the loss values on the training data were huge but the loss values on the validation data were small. It appeared as if the loss was not being averaged on the training pass, but it was on the validation pass. A little poking around in the debugger revealed this to be the case.\nThe problem is in the _pointwise_loss loss function in torch/nn/functional.py:\ndef _pointwise_loss(lambd, lambd_optimized, input, target, reduction='elementwise_mean'):\n    if target.requires_grad:\n        d = lambd(input, target)\n        if reduction == 'none':\n            return d\n        return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)\n    else:\n        return lambd_optimized(input, target, _Reduction.get_enum(reduction))\nIn the line return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d), the variable reduction is actually an integer with value 1, not a string with value 'elementwise_mean', and so the loss is summed instead of averaged, even though the 'elementwise_mean` option was chosen.\nCode example\nThis code reproduces the problem:\nimport torch\nimport torchvision as tv\n\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.c1 = torch.nn.Conv2d(1, 16, 3, padding=1)\n        self.c2 = torch.nn.Conv2d(16, 1, 3, padding=1)\n        self.relu = torch.nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.c2(self.relu(self.c1(x)))\n        return x\n\nnet = Net()\ndata = torchvision.datasets.MNIST('MNIST', True, download=True, transform=tv.transforms.ToTensor())\nmse = torch.nn.MSELoss(reduction='elementwise_mean')\n\nimg = data[0][0].view(1, 1, 28, 28).requires_grad_()\npred = net(img)\n\nloss1 = mse(pred, img).item()\n\nimg.requires_grad = False\nloss2 = mse(pred, img).item()\n\nprint(loss1, loss2)\nAnd the output is:\n155.7478  0.1987\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.2 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.10.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 7.5.17\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\nGPU 4: GeForce GTX 1080 Ti\nGPU 5: GeForce GTX 1080 Ti\nGPU 6: GeForce GTX 1080 Ti\nGPU 7: GeForce GTX 1080 Ti\nNvidia driver version: 387.34\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.0.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n/usr/local/cuda-9.0/lib64/libcudnn.so\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.3\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "## Issue description\r\n\r\nI was training an autoencoder with MSELoss, and the loss values on the training data were huge but the loss values on the validation data were small. It appeared as if the loss was not being averaged on the training pass, but it was on the validation pass. A little poking around in the debugger revealed this to be the case.\r\n\r\nThe problem is in the `_pointwise_loss` loss function in `torch/nn/functional.py`:\r\n```python\r\ndef _pointwise_loss(lambd, lambd_optimized, input, target, reduction='elementwise_mean'):\r\n    if target.requires_grad:\r\n        d = lambd(input, target)\r\n        if reduction == 'none':\r\n            return d\r\n        return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)\r\n    else:\r\n        return lambd_optimized(input, target, _Reduction.get_enum(reduction))\r\n```\r\nIn the line `return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)`, the variable `reduction` is actually an integer with value `1`, not a string with value `'elementwise_mean'`, and so the loss is summed instead of averaged, even though the 'elementwise_mean` option was chosen.\r\n\r\n## Code example\r\n\r\nThis code reproduces the problem:\r\n```python\r\nimport torch\r\nimport torchvision as tv\r\n\r\n\r\nclass Net(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.c1 = torch.nn.Conv2d(1, 16, 3, padding=1)\r\n        self.c2 = torch.nn.Conv2d(16, 1, 3, padding=1)\r\n        self.relu = torch.nn.ReLU(inplace=True)\r\n    \r\n    def forward(self, x):\r\n        x = self.c2(self.relu(self.c1(x)))\r\n        return x\r\n\r\nnet = Net()\r\ndata = torchvision.datasets.MNIST('MNIST', True, download=True, transform=tv.transforms.ToTensor())\r\nmse = torch.nn.MSELoss(reduction='elementwise_mean')\r\n\r\nimg = data[0][0].view(1, 1, 28, 28).requires_grad_()\r\npred = net(img)\r\n\r\nloss1 = mse(pred, img).item()\r\n\r\nimg.requires_grad = False\r\nloss2 = mse(pred, img).item()\r\n\r\nprint(loss1, loss2)\r\n```\r\nAnd the output is:\r\n`155.7478  0.1987`\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.2 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 7.5.17\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 387.34\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.0/lib64/libcudnn.so\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\n"}