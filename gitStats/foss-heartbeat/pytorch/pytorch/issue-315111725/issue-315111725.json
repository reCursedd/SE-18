{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6663", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6663/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6663/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6663/events", "html_url": "https://github.com/pytorch/pytorch/issues/6663", "id": 315111725, "node_id": "MDU6SXNzdWUzMTUxMTE3MjU=", "number": 6663, "title": "IntList parsing is inconsistent wrt tensors and integers", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-17T15:26:04Z", "updated_at": "2018-04-17T15:26:04Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>We generally allow items that are convertible to Ints as elements in IntLists, e.g.:</p>\n<pre><code>&gt;&gt;&gt; torch.zeros((torch.tensor(2), np.int64(3)))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n</code></pre>\n<p>Although it's not clear we always want to:</p>\n<pre><code>&gt;&gt;&gt; torch.zeros((torch.tensor(2.3), np.float(3.7)))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n\n&gt;&gt;&gt; np.zeros((2, 3.7))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'float' object cannot be interpreted as an integer\n</code></pre>\n<p>This is also inconsistent with how we bind to Ints/Scalars in a number of ways:</p>\n<ol>\n<li>We allow variables with <code>requires_grad=True</code>:</li>\n</ol>\n<pre><code>&gt;&gt;&gt; torch.zeros((torch.tensor(2, requires_grad=True), 3))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n</code></pre>\n<ol start=\"2\">\n<li>We don't allow non-tuple convertible elements (if they are first) in IntList, e.g.:</li>\n</ol>\n<pre><code>&gt;&gt;&gt; torch.zeros(2, torch.tensor(3))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; torch.zeros(torch.tensor(2), 3)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: zeros(): argument 'size' (position 1) must be tuple of ints, not Tensor\n</code></pre>\n<ol start=\"3\">\n<li>We don't bind (non-variable) convertible elements to Ints</li>\n</ol>\n<pre><code>&gt;&gt;&gt; torch.cumsum(torch.randn(2,3), dim=np.int64(0))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: cumsum(): argument 'dim' must be int, not numpy.int64\n</code></pre>\n<p>There are probably more cases as well.  The main issue comes from the two-stage parse: the binding stage is not the same as the read-out stage.  For example, at the parsing stage for Int64, we call <code>THPUtils_checkLong</code>, but in the read-out stage we call <code>  long long value = PyLong_AsLongLongAndOverflow(obj, &amp;overflow);</code> which has the convert to int behavior.  So essentially, we have both implicit conversions and function overloading, which gets very complicated.</p>\n<p>We could fix this by making the passes the same, e.g. storing the results in the binding stage and then just reading them out, however this could be slower, particularly in cases where e.g. only the last element doesn't bind.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a>.</p>", "body_text": "We generally allow items that are convertible to Ints as elements in IntLists, e.g.:\n>>> torch.zeros((torch.tensor(2), np.int64(3)))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n\nAlthough it's not clear we always want to:\n>>> torch.zeros((torch.tensor(2.3), np.float(3.7)))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n\n>>> np.zeros((2, 3.7))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'float' object cannot be interpreted as an integer\n\nThis is also inconsistent with how we bind to Ints/Scalars in a number of ways:\n\nWe allow variables with requires_grad=True:\n\n>>> torch.zeros((torch.tensor(2, requires_grad=True), 3))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n\n\nWe don't allow non-tuple convertible elements (if they are first) in IntList, e.g.:\n\n>>> torch.zeros(2, torch.tensor(3))\n\n 0  0  0\n 0  0  0\n[torch.FloatTensor of size (2,3)]\n\n>>> torch.zeros(torch.tensor(2), 3)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: zeros(): argument 'size' (position 1) must be tuple of ints, not Tensor\n\n\nWe don't bind (non-variable) convertible elements to Ints\n\n>>> torch.cumsum(torch.randn(2,3), dim=np.int64(0))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: cumsum(): argument 'dim' must be int, not numpy.int64\n\nThere are probably more cases as well.  The main issue comes from the two-stage parse: the binding stage is not the same as the read-out stage.  For example, at the parsing stage for Int64, we call THPUtils_checkLong, but in the read-out stage we call   long long value = PyLong_AsLongLongAndOverflow(obj, &overflow); which has the convert to int behavior.  So essentially, we have both implicit conversions and function overloading, which gets very complicated.\nWe could fix this by making the passes the same, e.g. storing the results in the binding stage and then just reading them out, however this could be slower, particularly in cases where e.g. only the last element doesn't bind.\nCC @ezyang @colesbury.", "body": "We generally allow items that are convertible to Ints as elements in IntLists, e.g.:\r\n\r\n```\r\n>>> torch.zeros((torch.tensor(2), np.int64(3)))\r\n\r\n 0  0  0\r\n 0  0  0\r\n[torch.FloatTensor of size (2,3)]\r\n```\r\n\r\nAlthough it's not clear we always want to:\r\n```\r\n>>> torch.zeros((torch.tensor(2.3), np.float(3.7)))\r\n\r\n 0  0  0\r\n 0  0  0\r\n[torch.FloatTensor of size (2,3)]\r\n\r\n>>> np.zeros((2, 3.7))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: 'float' object cannot be interpreted as an integer\r\n```\r\n\r\nThis is also inconsistent with how we bind to Ints/Scalars in a number of ways:\r\n\r\n1) We allow variables with `requires_grad=True`:\r\n```\r\n>>> torch.zeros((torch.tensor(2, requires_grad=True), 3))\r\n\r\n 0  0  0\r\n 0  0  0\r\n[torch.FloatTensor of size (2,3)]\r\n```\r\n\r\n2) We don't allow non-tuple convertible elements (if they are first) in IntList, e.g.:\r\n```\r\n>>> torch.zeros(2, torch.tensor(3))\r\n\r\n 0  0  0\r\n 0  0  0\r\n[torch.FloatTensor of size (2,3)]\r\n```\r\n\r\n```\r\n>>> torch.zeros(torch.tensor(2), 3)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: zeros(): argument 'size' (position 1) must be tuple of ints, not Tensor\r\n```\r\n\r\n3) We don't bind (non-variable) convertible elements to Ints\r\n```\r\n>>> torch.cumsum(torch.randn(2,3), dim=np.int64(0))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: cumsum(): argument 'dim' must be int, not numpy.int64\r\n```\r\n\r\nThere are probably more cases as well.  The main issue comes from the two-stage parse: the binding stage is not the same as the read-out stage.  For example, at the parsing stage for Int64, we call `THPUtils_checkLong`, but in the read-out stage we call `  long long value = PyLong_AsLongLongAndOverflow(obj, &overflow);` which has the convert to int behavior.  So essentially, we have both implicit conversions and function overloading, which gets very complicated.\r\n\r\nWe could fix this by making the passes the same, e.g. storing the results in the binding stage and then just reading them out, however this could be slower, particularly in cases where e.g. only the last element doesn't bind.\r\n\r\nCC @ezyang @colesbury."}