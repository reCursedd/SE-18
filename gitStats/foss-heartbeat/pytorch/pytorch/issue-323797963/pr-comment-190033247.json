{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190033247", "pull_request_review_id": 122328556, "id": 190033247, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDAzMzI0Nw==", "diff_hunk": "@@ -3,18 +3,124 @@\n #include <sys/socket.h>\n #include <sys/types.h>\n \n-#include <cstdlib>\n+#include <chrono>\n #include <cstdint>\n+#include <cstdlib>\n #include <functional>\n #include <limits>\n #include <string>\n #include <system_error>\n #include <tuple>\n #include <vector>\n-#include <chrono>\n+\n+#include <ATen/ATen.h>\n+\n+#include \"Types.hpp\"\n+\n+inline void hash_combine(std::size_t& seed) {}\n+\n+template <typename T, typename... Rest>\n+inline void hash_combine(std::size_t& seed, const T& v, Rest... rest) {\n+  std::hash<T> hasher;\n+  seed ^= hasher(v) + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n+  hash_combine(seed, rest...);\n+}\n+\n+#define MAKE_HASHABLE(type, ...)                  \\\n+  namespace std {                                 \\\n+  template <>                                     \\\n+  struct hash<type> {                             \\\n+    std::size_t operator()(const type& t) const { \\\n+      std::size_t ret = 0;                        \\\n+      hash_combine(ret, __VA_ARGS__);             \\\n+      return ret;                                 \\\n+    }                                             \\\n+  };                                              \\\n+  }\n+\n+// Make std::vector<T> hashable\n+namespace std {\n+template <typename T>\n+struct hash<vector<T>> {\n+  std::size_t operator()(const vector<T>& v) const {\n+    std::size_t ret = v.size();\n+    for (const auto& e : v) {\n+      hash_combine(ret, e);\n+    }\n+    return ret;\n+  }\n+};\n+} // namespace std\n+\n+// Make at::ArrayRef<T> hashable\n+namespace std {\n+template <typename T>\n+struct hash<at::ArrayRef<T>> {\n+  std::size_t operator()(const at::ArrayRef<T>& v) const {\n+    std::size_t ret = v.size();\n+    for (const auto& e : v) {\n+      hash_combine(ret, e);\n+    }\n+    return ret;\n+  }\n+};\n+} // namespace std\n+\n+MAKE_HASHABLE(::c10d::CollectiveType, static_cast<std::uint8_t>(t));\n+MAKE_HASHABLE(::c10d::DeviceType, static_cast<std::uint8_t>(t));\n+MAKE_HASHABLE(::c10d::ReduceOp, static_cast<std::uint8_t>(t));\n \n namespace c10d {\n \n+inline void assertSameSizeAndType(const std::vector<at::Tensor>& tensors) {\n+  // Ensure we have at least one tensor\n+  if (tensors.size() == 0) {\n+    throw std::invalid_argument(\"argument is empty\");\n+  }\n+\n+  // Ensure all tensors have identical type and shape\n+  auto& type = tensors[0].type();\n+  auto sizes = tensors[0].sizes();\n+  for (auto i = 1; i < tensors.size(); i++) {\n+    if (tensors[i].type() != type) {\n+      throw std::invalid_argument(\"argument contains mixed types\");\n+    }\n+    if (!tensors[i].sizes().equals(sizes)) {\n+      throw std::invalid_argument(\"argument contains mixed sizes\");\n+    }\n+  }\n+}\n+\n+inline std::vector<std::vector<int64_t>> getSizes(\n+    const std::vector<at::Tensor>& tensors) {\n+  std::vector<std::vector<int64_t>> sizes(tensors.size());\n+  for (auto i = 0; i < tensors.size(); i++) {\n+    sizes[i] = tensors[i].sizes();\n+  }\n+  return sizes;\n+}", "path": "torch/lib/c10d/Utils.hpp", "position": 68, "original_position": 101, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9eae42435952a9e0c723aff63234ca783460d467", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "I'll keep it in mind. I don't think it is applicable here because this function gets a vector of sizes. For the assignment operator to work we need a vector<ArrayRef> to be convertible to vector<vector<int64_t>>, which I don't think exists.", "created_at": "2018-05-22T19:58:26Z", "updated_at": "2018-11-23T15:44:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190033247", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190033247"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190033247"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>I'll keep it in mind. I don't think it is applicable here because this function gets a vector of sizes. For the assignment operator to work we need a vector to be convertible to vector&lt;vector&lt;int64_t&gt;&gt;, which I don't think exists.</p>", "body_text": "I'll keep it in mind. I don't think it is applicable here because this function gets a vector of sizes. For the assignment operator to work we need a vector to be convertible to vector<vector<int64_t>>, which I don't think exists.", "in_reply_to_id": 189441436}