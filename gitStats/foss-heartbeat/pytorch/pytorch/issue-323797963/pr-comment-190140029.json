{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190140029", "pull_request_review_id": 122452974, "id": 190140029, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDE0MDAyOQ==", "diff_hunk": "@@ -0,0 +1,384 @@\n+#include \"ProcessGroupGloo.hpp\"\n+\n+#include <gloo/allreduce_halving_doubling.h>\n+#include <gloo/broadcast_one_to_all.h>\n+#include <gloo/rendezvous/context.h>\n+#include <gloo/transport/tcp/device.h>\n+\n+#define GENERATE_ALL_TYPES(type, func, args...)        \\\n+  switch (type) {                                      \\\n+    case ::at::ScalarType::Float:                      \\\n+      func<float>(args);                               \\\n+      break;                                           \\\n+    case ::at::ScalarType::Double:                     \\\n+      func<double>(args);                              \\\n+      break;                                           \\\n+    case ::at::ScalarType::Half:                       \\\n+      func<gloo::float16>(args);                       \\\n+      break;                                           \\\n+    case ::at::ScalarType::Char:                       \\\n+      func<int8_t>(args);                              \\\n+      break;                                           \\\n+    case ::at::ScalarType::Byte:                       \\\n+      func<uint8_t>(args);                             \\\n+      break;                                           \\\n+    case ::at::ScalarType::Int:                        \\\n+      func<int32_t>(args);                             \\\n+      break;                                           \\\n+    case ::at::ScalarType::Long:                       \\\n+      func<int64_t>(args);                             \\\n+      break;                                           \\\n+    default:                                           \\\n+      throw std::runtime_error(\"Invalid scalar type\"); \\\n+  }\n+\n+namespace c10d {\n+\n+using KeyType = AlgorithmKey;\n+using EntryType = std::unique_ptr<AlgorithmEntry>;\n+\n+namespace {\n+\n+// Wrap c10d store as Gloo store\n+class GlooStore : public ::gloo::rendezvous::Store {\n+ public:\n+  GlooStore(const std::shared_ptr<::c10d::Store>& store) : store_(store) {}\n+\n+  void set(const std::string& key, const std::vector<char>& value) override {\n+    std::vector<uint8_t> tmp(value.begin(), value.end());\n+    store_->set(key, tmp);\n+  }\n+\n+  std::vector<char> get(const std::string& key) override {\n+    auto value = store_->get(key);\n+    return std::vector<char>(value.begin(), value.end());\n+  }\n+\n+  void wait(const std::vector<std::string>& keys) override {\n+    store_->wait(keys, Store::kDefaultTimeout);\n+  }\n+\n+  void wait(\n+      const std::vector<std::string>& keys,\n+      const std::chrono::milliseconds& timeout) override {\n+    store_->wait(keys, timeout);\n+  }\n+\n+ protected:\n+  std::shared_ptr<::c10d::Store> store_;\n+};\n+\n+template <typename T>\n+const ::gloo::ReductionFunction<T>* reductionFunction(const ReduceOp& r) {\n+  switch (r) {\n+    case ReduceOp::SUM:\n+      return ::gloo::ReductionFunction<T>::sum;\n+    case ReduceOp::PRODUCT:\n+      return ::gloo::ReductionFunction<T>::product;\n+    case ReduceOp::MIN:\n+      return ::gloo::ReductionFunction<T>::min;\n+    case ReduceOp::MAX:\n+      return ::gloo::ReductionFunction<T>::max;\n+  }\n+\n+  throw std::runtime_error(\"Unhandled ReduceOp\");\n+}\n+\n+} // namespace\n+\n+ProcessGroupGloo::WorkGloo::WorkGloo() : completed_(false) {}\n+\n+ProcessGroupGloo::WorkGloo::~WorkGloo() {}\n+\n+bool ProcessGroupGloo::WorkGloo::isCompleted() const {\n+  return completed_;\n+}\n+\n+bool ProcessGroupGloo::WorkGloo::isSuccess() const {\n+  return !ex_;\n+}\n+\n+bool ProcessGroupGloo::WorkGloo::wait() {\n+  std::unique_lock<std::mutex> lock(m_);\n+  while (!completed_) {\n+    cv_.wait(lock);\n+  }\n+  return isSuccess();\n+}\n+\n+const std::exception& ProcessGroupGloo::WorkGloo::exception() const {\n+  return *ex_;\n+}\n+\n+void ProcessGroupGloo::WorkGloo::finish() {\n+  {\n+    std::unique_lock<std::mutex> lock(m_);\n+    completed_ = true;\n+  }\n+  cv_.notify_all();\n+}\n+\n+void ProcessGroupGloo::WorkGloo::finishWithException(\n+    const ::gloo::Exception& ex) {\n+  {\n+    std::unique_lock<std::mutex> lock(m_);\n+    completed_ = true;\n+    ex_ = std::unique_ptr<::gloo::Exception>(new ::gloo::Exception(ex));\n+  }\n+  cv_.notify_all();\n+}\n+\n+ProcessGroupGloo::Options::Options()\n+    : timeout(std::chrono::milliseconds(10 * 1000)), threads(2) {}\n+\n+ProcessGroupGloo::ProcessGroupGloo(\n+    const std::shared_ptr<Store>& store,\n+    int rank,\n+    int size,\n+    Options options)\n+    : ProcessGroup(rank, size), store_(new GlooStore(store)), stop_(false) {\n+  auto& devices = options.devices;\n+  if (devices.empty()) {\n+    devices.push_back(::gloo::transport::tcp::CreateDevice(\"localhost\"));", "path": "torch/lib/c10d/ProcessGroupGloo.cpp", "position": 142, "original_position": 142, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "Can we also add Infiniband detection here and automatically use that device if available? Like what we did in THD?", "created_at": "2018-05-23T06:57:59Z", "updated_at": "2018-11-23T15:44:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190140029", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190140029"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190140029"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>Can we also add Infiniband detection here and automatically use that device if available? Like what we did in THD?</p>", "body_text": "Can we also add Infiniband detection here and automatically use that device if available? Like what we did in THD?"}