{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190116620", "pull_request_review_id": 122425742, "id": 190116620, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDExNjYyMA==", "diff_hunk": "@@ -0,0 +1,390 @@\n+#include \"ProcessGroupGloo.hpp\"\n+\n+#include <gloo/allreduce_halving_doubling.h>\n+#include <gloo/rendezvous/context.h>\n+\n+#define GENERATE_ALL_TYPES(type, func, args...)                         \\\n+  switch (type) {                                                       \\\n+    case ::at::ScalarType::Float: func<float>(args); break;             \\\n+    case ::at::ScalarType::Double: func<double>(args); break;           \\\n+    case ::at::ScalarType::Half: func<gloo::float16>(args); break;      \\\n+    case ::at::ScalarType::Char: func<int8_t>(args); break;             \\\n+    case ::at::ScalarType::Byte: func<uint8_t>(args); break;            \\\n+    case ::at::ScalarType::Int: func<int32_t>(args); break;             \\\n+    case ::at::ScalarType::Long: func<int64_t>(args); break;            \\\n+    default:                                                            \\\n+      throw std::runtime_error(\"Invalid scalar type\");                  \\\n+  }\n+\n+namespace c10d {\n+\n+using KeyType = AlgorithmKey;\n+using EntryType = std::unique_ptr<AlgorithmEntry>;\n+\n+namespace {\n+\n+// Wrap c10d store as Gloo store\n+class GlooStore : public ::gloo::rendezvous::Store {\n+ public:\n+  GlooStore(const std::shared_ptr<::c10d::Store>& store)\n+      : store_(store) {\n+  }\n+\n+  void set(const std::string& key, const std::vector<char>& value) override {\n+    // @pietern was unable to figure out a way to cast the value of\n+    // type std::vector<char> to an std::vector<unsigned char>,\n+    // so going with a copy here (it's not a hot path so it's fine).\n+    std::vector<uint8_t> tmp(value.size());\n+    memcpy(tmp.data(), value.data(), value.size());\n+    store_->set(key, tmp);\n+  }\n+\n+  std::vector<char> get(const std::string& key) override {\n+    // @pietern was unable to figure out a way to cast the value of\n+    // type std::vector<unsigned char> to an std::vector<char>,\n+    // so going with a copy here (it's not a hot path so it's fine).\n+    auto value = store_->get(key);\n+    std::vector<char> tmp(value.size());\n+    memcpy(tmp.data(), value.data(), value.size());\n+    return tmp;\n+  }\n+\n+  void wait(const std::vector<std::string>& keys) override {\n+    store_->wait(keys, Store::kDefaultTimeout);\n+  }\n+\n+  void wait(\n+      const std::vector<std::string>& keys,\n+      const std::chrono::milliseconds& timeout) override {\n+    store_->wait(keys, timeout);\n+  }\n+\n+ protected:\n+  std::shared_ptr<::c10d::Store> store_;\n+};\n+\n+template <typename T>\n+const ::gloo::ReductionFunction<T>* reductionFunction(const ReduceOp& r) {\n+  switch (r) {\n+    case ReduceOp::SUM:\n+      return ::gloo::ReductionFunction<T>::sum;\n+    case ReduceOp::PRODUCT:\n+      return ::gloo::ReductionFunction<T>::product;\n+    case ReduceOp::MIN:\n+      return ::gloo::ReductionFunction<T>::min;\n+    case ReduceOp::MAX:\n+      return ::gloo::ReductionFunction<T>::max;\n+  }\n+\n+  throw std::runtime_error(\"Unhandled ReduceOp\");\n+}\n+\n+} // namespace\n+\n+ProcessGroupGloo::WorkGloo::WorkGloo() {\n+  completed_ = false;\n+}\n+\n+ProcessGroupGloo::WorkGloo::~WorkGloo() {\n+}\n+\n+bool ProcessGroupGloo::WorkGloo::isCompleted() {\n+  std::unique_lock<std::mutex> lock(m_);\n+  return completed_;\n+}\n+\n+bool ProcessGroupGloo::WorkGloo::wait() {\n+  std::unique_lock<std::mutex> lock(m_);\n+  while (!completed_) {\n+    cv_.wait(lock);\n+  }\n+  return !ex_;\n+}\n+\n+const std::exception& ProcessGroupGloo::WorkGloo::exception() const {\n+  return *ex_;\n+}\n+\n+void ProcessGroupGloo::WorkGloo::finish() {\n+  {\n+    std::unique_lock<std::mutex> lock(m_);\n+    completed_ = true;\n+  }\n+  cv_.notify_all();\n+}\n+\n+void ProcessGroupGloo::WorkGloo::finishWithException(\n+    const ::gloo::Exception& ex) {\n+  {\n+    std::unique_lock<std::mutex> lock(m_);\n+    completed_ = true;\n+    ex_ = std::unique_ptr<::gloo::Exception>(new ::gloo::Exception(ex));\n+  }\n+  cv_.notify_all();\n+}\n+\n+ProcessGroupGloo::ProcessGroupGloo(\n+    const std::shared_ptr<Store>& store,\n+    const std::vector<std::shared_ptr<::gloo::transport::Device>>& devices,\n+    int rank,\n+    int size)\n+    : ProcessGroup(rank, size),\n+      store_(new GlooStore(store)),\n+      devices_(devices) {\n+  stop_ = false;\n+}\n+\n+ProcessGroupGloo::~ProcessGroupGloo() {\n+  // Require this process group to be explicitly shut down prior to being\n+  // destructed. This is the easiest way to guarantee clean shutdown\n+  // and avoid blocking/throwing in a destructor.\n+}\n+\n+void ProcessGroupGloo::initialize() {\n+  contexts_.clear();\n+  for (auto& device : devices_) {\n+    auto context = std::shared_ptr<::gloo::rendezvous::Context>(\n+        new ::gloo::rendezvous::Context(rank_, size_));\n+    context->setTimeout(std::chrono::milliseconds(100));\n+    context->connectFullMesh(*store_, device);\n+    contexts_.push_back(std::move(context));\n+  }\n+\n+  // TODO(@pietern): Make number of threads configurable\n+  threads_.resize(2);\n+  for (int i = 0; i < threads_.size(); i++) {\n+    threads_[i] = std::thread(&ProcessGroupGloo::runLoop, this);\n+  }\n+}\n+\n+void ProcessGroupGloo::destroy() {\n+  std::unique_lock<std::mutex> lock(m_);\n+  while (!queue_.empty()) {\n+    queueConsumeCV_.wait(lock);\n+  }\n+\n+  // Queue is empty, signal stop\n+  stop_ = true;\n+\n+  // Release lock to allow threads to terminate\n+  queueProduceCV_.notify_all();\n+  lock.unlock();\n+\n+  // Wait for worker threads to terminate\n+  for (auto& thread : threads_) {\n+    thread.join();\n+  }\n+}\n+\n+void ProcessGroupGloo::runLoop(void) {\n+  std::unique_lock<std::mutex> lock(m_);\n+\n+  while (!stop_) {\n+    if (queue_.empty()) {\n+      queueProduceCV_.wait(lock);\n+      continue;\n+    }\n+\n+    auto tuple = std::move(queue_.front());", "path": "torch/lib/c10d/ProcessGroupGloo.cpp", "position": 186, "original_position": 188, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "eed8b1e7d7febb5d1c6cc9eef757290ff957bd7f", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "That's fine, I actually used the same :P", "created_at": "2018-05-23T03:56:24Z", "updated_at": "2018-11-23T15:44:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190116620", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190116620"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190116620"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>That's fine, I actually used the same :P</p>", "body_text": "That's fine, I actually used the same :P", "in_reply_to_id": 188817053}