{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189780492", "pull_request_review_id": 122024055, "id": 189780492, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTc4MDQ5Mg==", "diff_hunk": "@@ -0,0 +1,96 @@\n+#pragma once\n+\n+#include <memory>\n+#include <stdexcept>\n+#include <vector>\n+\n+#include <ATen/ATen.h>\n+\n+#include \"Types.hpp\"\n+\n+namespace c10d {\n+\n+// ProcessGroup is a base class that captures collective and point to\n+// point communication in a fixed set of processes.\n+//\n+// The functions specified in the class below describe the API alone;\n+// implementations are provided in subclasses.\n+//\n+// Every function that performs I/O is executed asynchronously by a\n+// thread pool owned by the ProcessGroup (by default). They return an\n+// object that can be used to wait for completion or error.\n+//\n+// The ProcessGroup can instantiate subgroups with fewer or an equal\n+// number of members. Implementations must take care that multiple\n+// process groups can be used in parallel and synchronize accordingly.\n+//\n+// The ProcessGroup assumes a fixed set of processes. If the set\n+// changes, existing instances must be destructed and instantiation\n+// and initialization must start from scratch. For members of the\n+// process group to find each other (referred to as rendezvous from\n+// hereon)\n+//\n+// Note on usage with CUDA tensors:\n+//\n+// Operations on CUDA tensors are assumed to be executed\n+// asynchronously. Therefore they may have not yet executed when the\n+// tensors are passed to a collective function. The collective\n+// functions themselves should not block, so access to these tensors\n+// must be done asynchronously, as well as signaling completion of the\n+// collective function. We want to enable the following pattern:\n+//\n+//   z = at::sum(x, y);\n+//   work = pg.allreduce(z)\n+//   // Do something with z\n+//\n+// To do so, we execute the work associated with the collective\n+// function on a separate CUDA stream. Upon completing, this stream\n+// notifies the stream that produced the tensor in z\n+// (cudaEventRecord). Before returning from the collective function,\n+// it adds an asychronous wait for the internal stream\n+// (cudaEventSynchronize). This way we retain the ability to write\n+// sequential code that executes asynchronously, without requiring the\n+// caller to perform explicit synchronization.\n+//\n+class ProcessGroup {\n+ public:\n+  class Work {\n+   public:\n+    virtual ~Work();\n+\n+    // Checks if request has completed. Non-blocking operation.\n+    virtual bool isCompleted() const = 0;\n+\n+    // Waits until request completes. Blocking operation.\n+    // Returns false if the work completed with an exception.\n+    virtual bool wait() = 0;\n+\n+    // Returns exception if wait() returned false.\n+    virtual const std::exception& exception() const = 0;", "path": "torch/lib/c10d/ProcessGroup.hpp", "position": 51, "original_position": 69, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9eae42435952a9e0c723aff63234ca783460d467", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "Added an `isSuccess` function. Having to call wait to figure out whether or not the work completed successfully or not is a smell.", "created_at": "2018-05-22T05:28:10Z", "updated_at": "2018-11-23T15:44:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189780492", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189780492"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189780492"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>Added an <code>isSuccess</code> function. Having to call wait to figure out whether or not the work completed successfully or not is a smell.</p>", "body_text": "Added an isSuccess function. Having to call wait to figure out whether or not the work completed successfully or not is a smell.", "in_reply_to_id": 189441116}