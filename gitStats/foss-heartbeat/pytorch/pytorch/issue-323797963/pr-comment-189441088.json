{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189441088", "pull_request_review_id": 121625607, "id": 189441088, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQ0MTA4OA==", "diff_hunk": "@@ -0,0 +1,96 @@\n+#pragma once\n+\n+#include <memory>\n+#include <stdexcept>\n+#include <vector>\n+\n+#include <ATen/ATen.h>\n+\n+#include \"Types.hpp\"\n+\n+namespace c10d {\n+\n+// ProcessGroup is a base class that captures collective and point to\n+// point communication in a fixed set of processes.\n+//\n+// The functions specified in the class below describe the API alone;\n+// implementations are provided in subclasses.\n+//\n+// Every function that performs I/O is executed asynchronously by a\n+// thread pool owned by the ProcessGroup (by default). They return an\n+// object that can be used to wait for completion or error.\n+//\n+// The ProcessGroup can instantiate subgroups with fewer or an equal\n+// number of members. Implementations must take care that multiple\n+// process groups can be used in parallel and synchronize accordingly.\n+//\n+// The ProcessGroup assumes a fixed set of processes. If the set\n+// changes, existing instances must be destructed and instantiation\n+// and initialization must start from scratch. For members of the\n+// process group to find each other (referred to as rendezvous from\n+// hereon)\n+//\n+// Note on usage with CUDA tensors:\n+//\n+// Operations on CUDA tensors are assumed to be executed\n+// asynchronously. Therefore they may have not yet executed when the\n+// tensors are passed to a collective function. The collective\n+// functions themselves should not block, so access to these tensors\n+// must be done asynchronously, as well as signaling completion of the\n+// collective function. We want to enable the following pattern:\n+//\n+//   z = at::sum(x, y);\n+//   work = pg.allreduce(z)\n+//   // Do something with z\n+//\n+// To do so, we execute the work associated with the collective\n+// function on a separate CUDA stream. Upon completing, this stream\n+// notifies the stream that produced the tensor in z\n+// (cudaEventRecord). Before returning from the collective function,\n+// it adds an asychronous wait for the internal stream\n+// (cudaEventSynchronize). This way we retain the ability to write\n+// sequential code that executes asynchronously, without requiring the\n+// caller to perform explicit synchronization.", "path": "torch/lib/c10d/ProcessGroup.hpp", "position": null, "original_position": 53, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9eae42435952a9e0c723aff63234ca783460d467", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This part is quite confusing for me. How does `cudaEventRecord` notify a different stream? How can you tell which stream has produced `z`/will use `z` next? Shouldn't we sync the internal stream on the outer stream instead of doing it in the other direction?\r\n\r\nCan you please elaborate on this? Alternatively, since the CUDA support is coming in a different PR, can we just remove it from here and discuss it there?", "created_at": "2018-05-19T18:10:28Z", "updated_at": "2018-11-23T15:44:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189441088", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189441088"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189441088"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>This part is quite confusing for me. How does <code>cudaEventRecord</code> notify a different stream? How can you tell which stream has produced <code>z</code>/will use <code>z</code> next? Shouldn't we sync the internal stream on the outer stream instead of doing it in the other direction?</p>\n<p>Can you please elaborate on this? Alternatively, since the CUDA support is coming in a different PR, can we just remove it from here and discuss it there?</p>", "body_text": "This part is quite confusing for me. How does cudaEventRecord notify a different stream? How can you tell which stream has produced z/will use z next? Shouldn't we sync the internal stream on the outer stream instead of doing it in the other direction?\nCan you please elaborate on this? Alternatively, since the CUDA support is coming in a different PR, can we just remove it from here and discuss it there?"}