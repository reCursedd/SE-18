{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189441436", "pull_request_review_id": 121625607, "id": 189441436, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQ0MTQzNg==", "diff_hunk": "@@ -3,18 +3,124 @@\n #include <sys/socket.h>\n #include <sys/types.h>\n \n-#include <cstdlib>\n+#include <chrono>\n #include <cstdint>\n+#include <cstdlib>\n #include <functional>\n #include <limits>\n #include <string>\n #include <system_error>\n #include <tuple>\n #include <vector>\n-#include <chrono>\n+\n+#include <ATen/ATen.h>\n+\n+#include \"Types.hpp\"\n+\n+inline void hash_combine(std::size_t& seed) {}\n+\n+template <typename T, typename... Rest>\n+inline void hash_combine(std::size_t& seed, const T& v, Rest... rest) {\n+  std::hash<T> hasher;\n+  seed ^= hasher(v) + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n+  hash_combine(seed, rest...);\n+}\n+\n+#define MAKE_HASHABLE(type, ...)                  \\\n+  namespace std {                                 \\\n+  template <>                                     \\\n+  struct hash<type> {                             \\\n+    std::size_t operator()(const type& t) const { \\\n+      std::size_t ret = 0;                        \\\n+      hash_combine(ret, __VA_ARGS__);             \\\n+      return ret;                                 \\\n+    }                                             \\\n+  };                                              \\\n+  }\n+\n+// Make std::vector<T> hashable\n+namespace std {\n+template <typename T>\n+struct hash<vector<T>> {\n+  std::size_t operator()(const vector<T>& v) const {\n+    std::size_t ret = v.size();\n+    for (const auto& e : v) {\n+      hash_combine(ret, e);\n+    }\n+    return ret;\n+  }\n+};\n+} // namespace std\n+\n+// Make at::ArrayRef<T> hashable\n+namespace std {\n+template <typename T>\n+struct hash<at::ArrayRef<T>> {\n+  std::size_t operator()(const at::ArrayRef<T>& v) const {\n+    std::size_t ret = v.size();\n+    for (const auto& e : v) {\n+      hash_combine(ret, e);\n+    }\n+    return ret;\n+  }\n+};\n+} // namespace std\n+\n+MAKE_HASHABLE(::c10d::CollectiveType, static_cast<std::uint8_t>(t));\n+MAKE_HASHABLE(::c10d::DeviceType, static_cast<std::uint8_t>(t));\n+MAKE_HASHABLE(::c10d::ReduceOp, static_cast<std::uint8_t>(t));\n \n namespace c10d {\n \n+inline void assertSameSizeAndType(const std::vector<at::Tensor>& tensors) {\n+  // Ensure we have at least one tensor\n+  if (tensors.size() == 0) {\n+    throw std::invalid_argument(\"argument is empty\");\n+  }\n+\n+  // Ensure all tensors have identical type and shape\n+  auto& type = tensors[0].type();\n+  auto sizes = tensors[0].sizes();\n+  for (auto i = 1; i < tensors.size(); i++) {\n+    if (tensors[i].type() != type) {\n+      throw std::invalid_argument(\"argument contains mixed types\");\n+    }\n+    if (!tensors[i].sizes().equals(sizes)) {\n+      throw std::invalid_argument(\"argument contains mixed sizes\");\n+    }\n+  }\n+}\n+\n+inline std::vector<std::vector<int64_t>> getSizes(\n+    const std::vector<at::Tensor>& tensors) {\n+  std::vector<std::vector<int64_t>> sizes(tensors.size());\n+  for (auto i = 0; i < tensors.size(); i++) {\n+    sizes[i] = tensors[i].sizes();\n+  }\n+  return sizes;\n+}", "path": "torch/lib/c10d/Utils.hpp", "position": 68, "original_position": 101, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9eae42435952a9e0c723aff63234ca783460d467", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "1. It might be better to return a vector of `ArrayRef`s to prevent forcing an allocation for every tensor.\r\n2. Some time ago I've implemented a few utilities for mapping/filtering vectors in PyTorch C++ code. You can find it in `torch/csrc/utils/functional.h`. The thing you wrote here could then become\r\n```cpp\r\nfmap(tensors, [](const at::Tensor& t) { return t.sizes(); });\r\n```\r\nand in fact it would be even more efficient, because it makes sure to `.reserve` the memory and avoid multiple allocations. It's something that everyone always forgets about.\r\n\r\nThis also lets you easily handle functions like `getDevices`, `getDataPointers`, and any future cases where you basically want to map a simple function over a vector.", "created_at": "2018-05-19T18:26:37Z", "updated_at": "2018-11-23T15:44:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189441436", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189441436"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189441436"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<ol>\n<li>It might be better to return a vector of <code>ArrayRef</code>s to prevent forcing an allocation for every tensor.</li>\n<li>Some time ago I've implemented a few utilities for mapping/filtering vectors in PyTorch C++ code. You can find it in <code>torch/csrc/utils/functional.h</code>. The thing you wrote here could then become</li>\n</ol>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-en\">fmap</span>(tensors, [](<span class=\"pl-k\">const</span> at::Tensor&amp; t) { <span class=\"pl-k\">return</span> t.<span class=\"pl-c1\">sizes</span>(); });</pre></div>\n<p>and in fact it would be even more efficient, because it makes sure to <code>.reserve</code> the memory and avoid multiple allocations. It's something that everyone always forgets about.</p>\n<p>This also lets you easily handle functions like <code>getDevices</code>, <code>getDataPointers</code>, and any future cases where you basically want to map a simple function over a vector.</p>", "body_text": "It might be better to return a vector of ArrayRefs to prevent forcing an allocation for every tensor.\nSome time ago I've implemented a few utilities for mapping/filtering vectors in PyTorch C++ code. You can find it in torch/csrc/utils/functional.h. The thing you wrote here could then become\n\nfmap(tensors, [](const at::Tensor& t) { return t.sizes(); });\nand in fact it would be even more efficient, because it makes sure to .reserve the memory and avoid multiple allocations. It's something that everyone always forgets about.\nThis also lets you easily handle functions like getDevices, getDataPointers, and any future cases where you basically want to map a simple function over a vector."}