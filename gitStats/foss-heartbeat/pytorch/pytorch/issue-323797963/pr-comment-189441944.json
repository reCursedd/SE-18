{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189441944", "pull_request_review_id": 121625607, "id": 189441944, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQ0MTk0NA==", "diff_hunk": "@@ -0,0 +1,406 @@\n+#include \"ProcessGroupGloo.hpp\"\n+\n+#include <gloo/allreduce_halving_doubling.h>\n+#include <gloo/broadcast_one_to_all.h>\n+#include <gloo/rendezvous/context.h>\n+#include <gloo/transport/tcp/device.h>\n+\n+#define GENERATE_ALL_TYPES(type, func, args...)        \\\n+  switch (type) {                                      \\\n+    case ::at::ScalarType::Float:                      \\\n+      func<float>(args);                               \\\n+      break;                                           \\\n+    case ::at::ScalarType::Double:                     \\\n+      func<double>(args);                              \\\n+      break;                                           \\\n+    case ::at::ScalarType::Half:                       \\\n+      func<gloo::float16>(args);                       \\\n+      break;                                           \\\n+    case ::at::ScalarType::Char:                       \\\n+      func<int8_t>(args);                              \\\n+      break;                                           \\\n+    case ::at::ScalarType::Byte:                       \\\n+      func<uint8_t>(args);                             \\\n+      break;                                           \\\n+    case ::at::ScalarType::Int:                        \\\n+      func<int32_t>(args);                             \\\n+      break;                                           \\\n+    case ::at::ScalarType::Long:                       \\\n+      func<int64_t>(args);                             \\\n+      break;                                           \\\n+    default:                                           \\\n+      throw std::runtime_error(\"Invalid scalar type\"); \\\n+  }\n+\n+namespace c10d {\n+\n+using KeyType = AlgorithmKey;\n+using EntryType = std::unique_ptr<AlgorithmEntry>;\n+\n+namespace {\n+\n+// Wrap c10d store as Gloo store\n+class GlooStore : public ::gloo::rendezvous::Store {\n+ public:\n+  GlooStore(const std::shared_ptr<::c10d::Store>& store) : store_(store) {}\n+\n+  void set(const std::string& key, const std::vector<char>& value) override {\n+    std::vector<unsigned char> tmp(value.begin(), value.end());\n+    store_->set(key, tmp);\n+  }\n+\n+  std::vector<char> get(const std::string& key) override {\n+    auto value = store_->get(key);\n+    return std::vector<char>(value.begin(), value.end());\n+  }\n+\n+  void wait(const std::vector<std::string>& keys) override {\n+    store_->wait(keys, Store::kDefaultTimeout);\n+  }\n+\n+  void wait(\n+      const std::vector<std::string>& keys,\n+      const std::chrono::milliseconds& timeout) override {\n+    store_->wait(keys, timeout);\n+  }\n+\n+ protected:\n+  std::shared_ptr<::c10d::Store> store_;\n+};\n+\n+template <typename T>\n+const ::gloo::ReductionFunction<T>* reductionFunction(const ReduceOp& r) {\n+  switch (r) {\n+    case ReduceOp::SUM:\n+      return ::gloo::ReductionFunction<T>::sum;\n+    case ReduceOp::PRODUCT:\n+      return ::gloo::ReductionFunction<T>::product;\n+    case ReduceOp::MIN:\n+      return ::gloo::ReductionFunction<T>::min;\n+    case ReduceOp::MAX:\n+      return ::gloo::ReductionFunction<T>::max;\n+  }\n+\n+  throw std::runtime_error(\"Unhandled ReduceOp\");\n+}\n+\n+} // namespace\n+\n+ProcessGroupGloo::WorkGloo::WorkGloo() : completed_(false) {}\n+\n+ProcessGroupGloo::WorkGloo::~WorkGloo() {}\n+\n+bool ProcessGroupGloo::WorkGloo::isCompleted() const {\n+  return completed_;\n+}\n+\n+bool ProcessGroupGloo::WorkGloo::wait() {\n+  std::unique_lock<std::mutex> lock(m_);\n+  while (!completed_) {\n+    cv_.wait(lock);\n+  }\n+  return !ex_;\n+}\n+\n+const std::exception& ProcessGroupGloo::WorkGloo::exception() const {\n+  return *ex_;\n+}\n+\n+void ProcessGroupGloo::WorkGloo::finish() {\n+  {\n+    std::unique_lock<std::mutex> lock(m_);\n+    completed_ = true;\n+  }\n+  cv_.notify_all();\n+}\n+\n+void ProcessGroupGloo::WorkGloo::finishWithException(\n+    const ::gloo::Exception& ex) {\n+  {\n+    std::unique_lock<std::mutex> lock(m_);\n+    completed_ = true;\n+    ex_ = std::unique_ptr<::gloo::Exception>(new ::gloo::Exception(ex));\n+  }\n+  cv_.notify_all();\n+}\n+\n+ProcessGroupGloo::Options::Options()\n+    : timeout(std::chrono::milliseconds(10 * 1000)), threads(1) {}\n+\n+ProcessGroupGloo::ProcessGroupGloo(\n+    const std::shared_ptr<Store>& store,\n+    int rank,\n+    int size)\n+    : ProcessGroup(rank, size), store_(new GlooStore(store)), stop_(false) {}\n+\n+ProcessGroupGloo::~ProcessGroupGloo() {\n+  // Require this process group to be explicitly shut down prior to being\n+  // destructed. This is the easiest way to guarantee clean shutdown\n+  // and avoid blocking/throwing in a destructor.\n+}\n+\n+void ProcessGroupGloo::initialize() {\n+  Options options;\n+  options.timeout = std::chrono::milliseconds(100);\n+  options.threads = 2;\n+  initialize(options);\n+}\n+\n+void ProcessGroupGloo::initialize(Options& options) {\n+  auto& devices = options.devices;\n+  if (devices.empty()) {\n+    devices.push_back(::gloo::transport::tcp::CreateDevice(\"localhost\"));\n+  }\n+\n+  for (auto& device : options.devices) {\n+    auto context = std::make_shared<::gloo::rendezvous::Context>(rank_, size_);\n+    context->setTimeout(options.timeout);\n+    context->connectFullMesh(*store_, device);\n+    contexts_.push_back(std::move(context));\n+  }\n+\n+  threads_.resize(options.threads);\n+  for (int i = 0; i < threads_.size(); i++) {\n+    threads_[i] = std::thread(&ProcessGroupGloo::runLoop, this);\n+  }\n+}\n+\n+void ProcessGroupGloo::destroy() {\n+  std::unique_lock<std::mutex> lock(m_);\n+  while (!queue_.empty()) {\n+    queueConsumeCV_.wait(lock);\n+  }\n+\n+  // Queue is empty, signal stop\n+  stop_ = true;\n+\n+  // Release lock to allow threads to terminate\n+  queueProduceCV_.notify_all();\n+  lock.unlock();\n+\n+  // Wait for worker threads to terminate\n+  for (auto& thread : threads_) {\n+    thread.join();\n+  }\n+}\n+\n+void ProcessGroupGloo::runLoop(void) {\n+  std::unique_lock<std::mutex> lock(m_);\n+\n+  while (!stop_) {\n+    if (queue_.empty()) {\n+      queueProduceCV_.wait(lock);\n+      continue;\n+    }\n+\n+    auto tuple = std::move(queue_.front());\n+    queue_.pop_front();\n+    queueConsumeCV_.notify_one();\n+\n+    auto& entry = std::get<0>(tuple);\n+    auto& work = std::get<1>(tuple);\n+\n+    // Continue holding onto the lock; this ensures that we serialize\n+    // creation of Gloo algorithm instances for the context associated\n+    // with this process group.\n+    if (!entry->algorithm) {\n+      createAlgorithm(*entry);\n+    }\n+\n+    lock.unlock();\n+    runSingle(std::move(tuple));\n+    lock.lock();\n+  }\n+}\n+\n+void ProcessGroupGloo::runSingle(WorkType tuple) {\n+  auto& entry = std::get<0>(tuple);\n+  auto& work = std::get<1>(tuple);\n+\n+  try {\n+    entry->run(entry);\n+    work->finish();\n+  } catch (const ::gloo::Exception& ex) {\n+    work->finishWithException(ex);\n+  }\n+\n+  // Return entry to cache\n+  std::unique_lock<std::mutex> lock(m_);\n+  cache_[entry->key] = std::move(entry);\n+  cacheCV_.notify_all();\n+}\n+\n+void ProcessGroupGloo::createAlgorithm(AlgorithmEntry& entry) {\n+  const auto& key = entry.key;\n+  switch (key.collectiveType) {\n+    case CollectiveType::ALLREDUCE:\n+      GENERATE_ALL_TYPES(key.type->scalarType(), createAllreduce, entry);\n+      return;\n+    case CollectiveType::BROADCAST:\n+      GENERATE_ALL_TYPES(key.type->scalarType(), createBroadcast, entry);\n+      return;\n+  }\n+\n+  throw std::runtime_error(\"Unhandled collective type\");\n+}\n+\n+template <typename T>\n+void ProcessGroupGloo::createAllreduce(AlgorithmEntry& entry) {\n+  const auto& key = entry.key;\n+\n+  // Create algorithm against first context\n+  auto& context = contexts_[0];\n+  entry.algorithm = std::unique_ptr<::gloo::Algorithm>(\n+      new ::gloo::AllreduceHalvingDoubling<T>(\n+          context,\n+          getDataPointers<T>(entry.src),\n+          entry.src[0].numel(),\n+          reductionFunction<T>(key.reduceOp)));\n+}\n+\n+template <typename T>\n+void ProcessGroupGloo::createBroadcast(AlgorithmEntry& entry) {\n+  const auto& key = entry.key;\n+\n+  // Create algorithm against first context\n+  auto& context = contexts_[0];\n+  entry.algorithm =\n+      std::unique_ptr<::gloo::Algorithm>(new ::gloo::BroadcastOneToAll<T>(\n+          context,\n+          getDataPointers<T>(entry.src),\n+          entry.src[0].numel(),\n+          key.srcRank,\n+          key.srcTensor));\n+}\n+\n+// Constructs an AlgorithmEntry instance, except for the algorithm\n+// itself. It allocates temporary input/output tensors, CUDA streams\n+// (if applicable), etcetera. These are lazily allocated and reused\n+// for collective calls with the same signature.\n+//\n+// They cannot be allocated asynchronously because the collective\n+// functions need them before queueing their asynchronous work. For\n+// example, to work with asynchronous CUDA code, the collective call\n+// needs to issue an asynchronous memory copy, and a call to\n+// cudaStreamWaitEvent to make it wait for the asynchronous execution\n+// of the collective call to complete.\n+//\n+// Construction of the Gloo algorithm itself it delayed until a thread\n+// picks up the work, because it performs I/O and can fail. Any I/O\n+// failure must be signaled through the Work future.\n+//\n+EntryType ProcessGroupGloo::construct(const AlgorithmKey& key) {\n+  auto entry = std::unique_ptr<AlgorithmEntry>(new AlgorithmEntry);\n+  entry->key = key;\n+\n+  // Allocate source tensors for this entry\n+  auto& srcSizes = key.srcSizes;\n+  entry->src.resize(srcSizes.size());\n+  for (int i = 0; i < srcSizes.size(); i++) {\n+    entry->src[i] = at::zeros(*key.type, at::IntList(srcSizes[i]));", "path": "torch/lib/c10d/ProcessGroupGloo.cpp", "position": null, "original_position": 300, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9eae42435952a9e0c723aff63234ca783460d467", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Do we really need them to be zeros? You can do `key.type->tensor(srcSizes[i])` to allocate a tensor without forcing zero init.", "created_at": "2018-05-19T18:45:04Z", "updated_at": "2018-11-23T15:44:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189441944", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189441944"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189441944"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>Do we really need them to be zeros? You can do <code>key.type-&gt;tensor(srcSizes[i])</code> to allocate a tensor without forcing zero init.</p>", "body_text": "Do we really need them to be zeros? You can do key.type->tensor(srcSizes[i]) to allocate a tensor without forcing zero init."}