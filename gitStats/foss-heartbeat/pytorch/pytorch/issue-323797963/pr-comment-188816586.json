{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/188816586", "pull_request_review_id": 120864014, "id": 188816586, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4ODgxNjU4Ng==", "diff_hunk": "@@ -0,0 +1,184 @@\n+#pragma once\n+\n+#include <condition_variable>\n+#include <deque>\n+#include <mutex>\n+#include <thread>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include <gloo/algorithm.h>\n+#include <gloo/common/error.h>\n+#include <gloo/context.h>\n+#include <gloo/rendezvous/store.h>\n+#include <gloo/transport/device.h>\n+\n+#include \"ProcessGroup.hpp\"\n+#include \"Store.hpp\"\n+#include \"Types.hpp\"\n+#include \"Utils.hpp\"\n+\n+namespace c10d {\n+\n+// AlgorithmKey is a const identifier for a Gloo algorithm.\n+//\n+// It captures the set of participating devices, the source device,\n+// destination device, source rank, destination rank, reduction type\n+// (if applicable), etcetera. This key is used to cache instances of a\n+// Gloo algorithm for reuse. The number of cached instances can vary\n+// over time and is agreed upon between all processes in the group. It\n+// is also used in identifying the algorithm type for which to change\n+// the maximum alotted number of instances.\n+//\n+struct AlgorithmKey {\n+  bool operator==(const AlgorithmKey &other) const {\n+    return\n+      (collectiveType == other.collectiveType) &&\n+      (type == other.type) &&\n+      (devices == other.devices) &&\n+      (srcSizes == other.srcSizes) &&\n+      (dstSizes == other.dstSizes) &&\n+      (srcDevice == other.srcDevice) &&\n+      (dstDevice == other.dstDevice) &&\n+      (srcRank == other.srcRank) &&\n+      (dstRank == other.dstRank) &&\n+      (reduceOp == other.reduceOp);\n+  }\n+\n+  CollectiveType collectiveType = CollectiveType::UNUSED;\n+  at::Type* type = nullptr;\n+  std::vector<int> devices;\n+  std::vector<std::vector<int64_t>> srcSizes;\n+  std::vector<std::vector<int64_t>> dstSizes;\n+  int srcDevice = -1;\n+  int dstDevice = -1;\n+  int srcRank = -1;\n+  int dstRank = -1;\n+  ReduceOp reduceOp = ReduceOp::UNUSED;\n+};\n+\n+struct AlgorithmEntry {\n+  AlgorithmKey key;\n+  std::unique_ptr<::gloo::Algorithm> algorithm;\n+  std::vector<at::Tensor> src;\n+  std::vector<at::Tensor> dst;\n+  std::function<void(std::unique_ptr<AlgorithmEntry>&)> run;\n+\n+  explicit AlgorithmEntry() {\n+  }\n+\n+  // Must not be copied\n+  AlgorithmEntry & operator=(const AlgorithmEntry&) = delete;\n+  AlgorithmEntry(const AlgorithmEntry&) = delete;\n+\n+  // May be moved\n+  AlgorithmEntry(AlgorithmEntry&& other) {\n+    key = std::move(other.key);\n+    algorithm = std::move(other.algorithm);\n+    src = std::move(other.src);\n+    dst = std::move(other.dst);\n+  }\n+};\n+\n+} // namespace c10d\n+\n+MAKE_HASHABLE(\n+    ::c10d::AlgorithmKey,\n+    t.collectiveType,\n+    t.type,\n+    t.devices,\n+    t.srcSizes,\n+    t.dstSizes,\n+    t.srcDevice,\n+    t.dstDevice,\n+    t.srcRank,\n+    t.dstRank,\n+    t.reduceOp);\n+\n+namespace c10d {\n+\n+class ProcessGroupGloo : public ProcessGroup {\n+ public:\n+  class WorkGloo : public Work {\n+   public:\n+    explicit WorkGloo();\n+    virtual ~WorkGloo();\n+\n+    // Checks if request has completed. Non-blocking operation.\n+    bool isCompleted() override;\n+\n+    // Waits until request completes. Blocking operation.\n+    // Returns false if the work completed with an exception.\n+    bool wait() override;\n+\n+    // Returns exception if wait() returned false.\n+    const std::exception& exception() const override;\n+\n+   protected:\n+    void finish();\n+    void finishWithException(const ::gloo::Exception& ex);\n+\n+    std::mutex m_;\n+    std::condition_variable cv_;\n+    bool completed_;\n+\n+    // Use pointer to ::gloo::Exception because it doesn't have a\n+    // default constructor and constructing an empty std::unique_ptr\n+    // is probably cheaper (this is highly speculative).\n+    std::unique_ptr<::gloo::Exception> ex_;\n+\n+    friend class ProcessGroupGloo;\n+  };\n+\n+  explicit ProcessGroupGloo(\n+      const std::shared_ptr<Store>& store,\n+      const std::vector<std::shared_ptr<::gloo::transport::Device>>& devices,\n+      int rank,\n+      int size);\n+\n+  virtual ~ProcessGroupGloo();\n+\n+  void initialize();\n+  void destroy();\n+\n+  std::shared_ptr<Work> broadcast(\n+      std::vector<at::Tensor>& data,\n+      const BroadcastOptions& opts = BroadcastOptions()) override;\n+\n+  std::shared_ptr<Work> allreduce(\n+      std::vector<at::Tensor>& tensors,\n+      const AllreduceOptions& opts = AllreduceOptions()) override;\n+\n+ protected:\n+  std::unique_ptr<::gloo::rendezvous::Store> store_;\n+  std::vector<std::shared_ptr<::gloo::transport::Device>> devices_;\n+  std::vector<std::shared_ptr<::gloo::Context>> contexts_;\n+\n+  using KeyType = AlgorithmKey;\n+  using EntryType = std::unique_ptr<AlgorithmEntry>;\n+\n+  EntryType construct(const KeyType& key);\n+  EntryType checkout(const KeyType& key);\n+\n+  std::mutex m_;", "path": "torch/lib/c10d/ProcessGroupGloo.hpp", "position": null, "original_position": 163, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "eed8b1e7d7febb5d1c6cc9eef757290ff957bd7f", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "consider renaming this mutex, we have two mutex in two classes with the same name m_?", "created_at": "2018-05-17T01:22:05Z", "updated_at": "2018-11-23T15:44:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r188816586", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/188816586"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r188816586"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>consider renaming this mutex, we have two mutex in two classes with the same name m_?</p>", "body_text": "consider renaming this mutex, we have two mutex in two classes with the same name m_?"}