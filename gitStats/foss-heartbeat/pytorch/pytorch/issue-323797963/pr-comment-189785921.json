{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189785921", "pull_request_review_id": 122030256, "id": 189785921, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTc4NTkyMQ==", "diff_hunk": "@@ -3,18 +3,124 @@\n #include <sys/socket.h>\n #include <sys/types.h>\n \n-#include <cstdlib>\n+#include <chrono>\n #include <cstdint>\n+#include <cstdlib>\n #include <functional>\n #include <limits>\n #include <string>\n #include <system_error>\n #include <tuple>\n #include <vector>\n-#include <chrono>\n+\n+#include <ATen/ATen.h>\n+\n+#include \"Types.hpp\"\n+\n+inline void hash_combine(std::size_t& seed) {}\n+\n+template <typename T, typename... Rest>\n+inline void hash_combine(std::size_t& seed, const T& v, Rest... rest) {\n+  std::hash<T> hasher;\n+  seed ^= hasher(v) + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n+  hash_combine(seed, rest...);\n+}\n+\n+#define MAKE_HASHABLE(type, ...)                  \\\n+  namespace std {                                 \\\n+  template <>                                     \\\n+  struct hash<type> {                             \\\n+    std::size_t operator()(const type& t) const { \\\n+      std::size_t ret = 0;                        \\\n+      hash_combine(ret, __VA_ARGS__);             \\\n+      return ret;                                 \\\n+    }                                             \\\n+  };                                              \\\n+  }\n+\n+// Make std::vector<T> hashable\n+namespace std {\n+template <typename T>\n+struct hash<vector<T>> {\n+  std::size_t operator()(const vector<T>& v) const {\n+    std::size_t ret = v.size();\n+    for (const auto& e : v) {\n+      hash_combine(ret, e);\n+    }\n+    return ret;\n+  }\n+};\n+} // namespace std\n+\n+// Make at::ArrayRef<T> hashable\n+namespace std {\n+template <typename T>\n+struct hash<at::ArrayRef<T>> {\n+  std::size_t operator()(const at::ArrayRef<T>& v) const {\n+    std::size_t ret = v.size();\n+    for (const auto& e : v) {\n+      hash_combine(ret, e);\n+    }\n+    return ret;\n+  }\n+};\n+} // namespace std\n+\n+MAKE_HASHABLE(::c10d::CollectiveType, static_cast<std::uint8_t>(t));\n+MAKE_HASHABLE(::c10d::DeviceType, static_cast<std::uint8_t>(t));\n+MAKE_HASHABLE(::c10d::ReduceOp, static_cast<std::uint8_t>(t));\n \n namespace c10d {\n \n+inline void assertSameSizeAndType(const std::vector<at::Tensor>& tensors) {\n+  // Ensure we have at least one tensor\n+  if (tensors.size() == 0) {\n+    throw std::invalid_argument(\"argument is empty\");\n+  }\n+\n+  // Ensure all tensors have identical type and shape\n+  auto& type = tensors[0].type();\n+  auto sizes = tensors[0].sizes();\n+  for (auto i = 1; i < tensors.size(); i++) {\n+    if (tensors[i].type() != type) {\n+      throw std::invalid_argument(\"argument contains mixed types\");\n+    }\n+    if (!tensors[i].sizes().equals(sizes)) {\n+      throw std::invalid_argument(\"argument contains mixed sizes\");\n+    }\n+  }\n+}\n+\n+inline std::vector<std::vector<int64_t>> getSizes(\n+    const std::vector<at::Tensor>& tensors) {\n+  std::vector<std::vector<int64_t>> sizes(tensors.size());\n+  for (auto i = 0; i < tensors.size(); i++) {\n+    sizes[i] = tensors[i].sizes();\n+  }\n+  return sizes;\n+}", "path": "torch/lib/c10d/Utils.hpp", "position": 68, "original_position": 101, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9eae42435952a9e0c723aff63234ca783460d467", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "Nice, we should pull it into aten/utils or something.\r\n\r\nRegarding return a vector of ArrayRefs: this would only work if the tensors they are retrieved from are guaranteed to stick around. This is not the case. This vector of sizes is used in the algorithm key, which has a lifecycle that is different from the tensors for which it was first created.", "created_at": "2018-05-22T06:11:57Z", "updated_at": "2018-11-23T15:44:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189785921", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189785921"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r189785921"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>Nice, we should pull it into aten/utils or something.</p>\n<p>Regarding return a vector of ArrayRefs: this would only work if the tensors they are retrieved from are guaranteed to stick around. This is not the case. This vector of sizes is used in the algorithm key, which has a lifecycle that is different from the tensors for which it was first created.</p>", "body_text": "Nice, we should pull it into aten/utils or something.\nRegarding return a vector of ArrayRefs: this would only work if the tensors they are retrieved from are guaranteed to stick around. This is not the case. This vector of sizes is used in the algorithm key, which has a lifecycle that is different from the tensors for which it was first created.", "in_reply_to_id": 189441436}