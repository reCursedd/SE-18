{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190303060", "pull_request_review_id": 122652414, "id": 190303060, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDMwMzA2MA==", "diff_hunk": "@@ -297,45 +293,40 @@ EntryType ProcessGroupGloo::construct(const AlgorithmKey& key) {\n   auto& srcSizes = key.srcSizes;\n   entry->src.resize(srcSizes.size());\n   for (int i = 0; i < srcSizes.size(); i++) {\n-    entry->src[i] = at::zeros(*key.type, at::IntList(srcSizes[i]));\n+    entry->src[i] = key.type->tensor(srcSizes[i]);\n   }\n \n   return entry;\n }\n \n-EntryType ProcessGroupGloo::checkout(const AlgorithmKey& key) {\n-  std::unique_lock<std::mutex> lock(m_);\n-\n-  // Initialize number of entries for this key.\n-  if (cacheCreated_.count(key) == 0) {\n-    cacheCreated_.emplace(key, 0);\n-  }\n+AlgorithmEntry* ProcessGroupGloo::checkout(const AlgorithmKey& key) {\n+  auto it = cache_.find(key);\n \n   // If there is no entry for this key yet, it must be the first time\n   // we see and can create a new entry. Use hard limit of 1 instance\n   // per key until we add support for a dynamic limit.\n-  if (cacheCreated_[key] < 1) {\n-    cacheCreated_[key]++;\n-    return construct(key);\n+  if (it == cache_.end()) {\n+    cache_[key] = construct(key);\n+    it = cache_.find(key);\n   }\n \n-  // Optionally wait for entry to be returned to the cache.\n-  auto it = cache_.find(key);\n-  while (it == cache_.end()) {\n-    cacheCV_.wait(lock);\n-    it = cache_.find(key);\n+  auto& entry = it->second;\n+\n+  // Ensure entry is not in use\n+  std::unique_lock<std::mutex> lock(entry->m);\n+  while (entry->busy) {\n+      entry->cv.wait(lock);\n   }\n \n-  // Grab entry from the cache and return it.\n-  auto entry = std::move(it->second);\n-  cache_.erase(key);\n-  return entry;\n+  // Mark entry in use\n+  entry->busy = true;", "path": "torch/lib/c10d/ProcessGroupGloo.cpp", "position": null, "original_position": 161, "commit_id": "f81bd359f6b3550c6c311ff39b58bf62535e43cc", "original_commit_id": "9a6399fbaffd9d8bb24bdbd87d41069b950be75e", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "That is probably true... Thought I like this for being more explicit. The entry is not locked while it is queued and this poses a possible problem of double use. Right now this is caught by having the flag.", "created_at": "2018-05-23T15:52:37Z", "updated_at": "2018-11-23T15:44:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190303060", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190303060"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7628#discussion_r190303060"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7628"}}, "body_html": "<p>That is probably true... Thought I like this for being more explicit. The entry is not locked while it is queued and this poses a possible problem of double use. Right now this is caught by having the flag.</p>", "body_text": "That is probably true... Thought I like this for being more explicit. The entry is not locked while it is queued and this poses a possible problem of double use. Right now this is caught by having the flag.", "in_reply_to_id": 190194937}