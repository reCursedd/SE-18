{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145817204", "pull_request_review_id": 70674702, "id": 145817204, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTgxNzIwNA==", "diff_hunk": "@@ -25,20 +32,45 @@ Tensor maybe_multiply(const Tensor & t, const Scalar & s) {\n \n Tensor norm_backward(const Tensor & grad, const Tensor & self, const Scalar & p_) {\n   auto p = p_.toDouble();\n+  auto norm = self.norm(p_);\n+\n+  if (norm.toDouble() == 0.0) {\n+    // handle case at 0 where we return a subgradient containing 0\n+    return zeros_like(self);\n+  }\n+\n   if (p == 2.0) {\n-    return self * (grad / self.norm(2));\n+    return self * (grad / norm);\n   } else {\n     auto pow_ = self.abs().pow(p - 2);\n-    auto scale_v = grad / self.norm(p).toTensor().pow(p - 1);\n+    auto scale_v = grad / norm.toTensor().pow(p - 1);", "path": "tools/autograd/templates/Functions.cpp", "position": 33, "original_position": 31, "commit_id": "937212b9a919339d4afaede4b277e3fd8ae015d0", "original_commit_id": "cca8b80fd3e352fb955f416bee80b010797bf102", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Saving the result helps, but we don't want to use `std::pow` because then we'd break double-backwards for this function.  (Tensor * Scalar would have the same problem; currently (no gradient for the Scalar).\r\n\r\nWe'll want to fix these as we get true 0-dim tensor support. (It's not an issue now because Scalars are never exposed to Python; we wrap them in 1-dim Variables).", "created_at": "2017-10-19T20:39:20Z", "updated_at": "2018-11-23T15:35:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145817204", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3136", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145817204"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145817204"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3136"}}, "body_html": "<p>Saving the result helps, but we don't want to use <code>std::pow</code> because then we'd break double-backwards for this function.  (Tensor * Scalar would have the same problem; currently (no gradient for the Scalar).</p>\n<p>We'll want to fix these as we get true 0-dim tensor support. (It's not an issue now because Scalars are never exposed to Python; we wrap them in 1-dim Variables).</p>", "body_text": "Saving the result helps, but we don't want to use std::pow because then we'd break double-backwards for this function.  (Tensor * Scalar would have the same problem; currently (no gradient for the Scalar).\nWe'll want to fix these as we get true 0-dim tensor support. (It's not an issue now because Scalars are never exposed to Python; we wrap them in 1-dim Variables).", "in_reply_to_id": 145194056}