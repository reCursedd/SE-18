{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145748054", "pull_request_review_id": 70574035, "id": 145748054, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTc0ODA1NA==", "diff_hunk": "@@ -391,44 +500,105 @@ def create_variable_type(top_env, aten_declarations):\n     type_declarations = top_env['type_derived_method_declarations']\n     type_definitions = top_env['type_derived_method_definitions']\n \n-    def save_variables(option, derivative):\n+    def skip_function(name):\n+        return (name.endswith('_out') or name.endswith('_forward'))\n+\n+    def differentiable_args(declaration, autograd_function):\n+        names = set(name for d in autograd_function['derivatives'] for name in d['var_names'])\n+        args = [arg for arg in declaration['arguments'] if arg['name'] in names]\n+        if len(args) != len(names):\n+            missing = names - set(arg['name'] for arg in args)\n+            raise RuntimeError('Missing arguments for derivatives: {}'.format(missing))\n+        return args\n+\n+    def save_variables(option, saved_variables, is_output):\n         # assign the saved variables to the generated grad_fn\n         stmts = []\n-        for arg in derivative['saved']:\n+        for arg in saved_variables:\n             name = arg['name']\n             expr = arg['name']\n+            if is_output and not option['inplace']:\n+                if len(option['returns']) > 1:\n+                    # unpack multiple outputs\n+                    return_names = [r['name'] for r in option['returns']]\n+                    idx = return_names.index(name)\n+                    stmts.append('auto& {} = std::get<{}>(ret);'.format(name, idx))\n+                elif name != 'input':\n+                    stmts.append('auto& {} = ret;'.format(name))\n             if '_sizes' in name:\n                 expr = name.replace('_sizes', '.sizes()')\n+            elif name.endswith('_info'):\n+                expr = name.replace('_info', '')\n             elif '_argsize_' in name:\n-                # turn x_argsizes_y into to_arg_sizes(x, y)\n+                # turn x_argsize_y into x.size(y)\n                 expr = re.sub(r\"(\\w+)_argsize_(\\w+)\", r\"\\1.size(\\2)\", name)\n             elif '_argsizes_' in name:\n                 # turn x_argsizes_y into to_arg_sizes(x, y)\n                 expr = re.sub(r\"(\\w+)_argsizes_(\\w+)\", r\"to_arg_sizes(\\1, \\2)\", name)\n-            elif arg['type'] == 'Tensor':\n+            elif arg['type'] == 'Tensor' or (is_output and arg['type'] == 'Scalar'):\n                 name += '_'\n                 var = arg['name']\n                 if var == 'self' and option['inplace']:\n                     var = 'self.clone()'\n-                expr = 'SavedVariable({}, nullptr)'.format(var)\n+                    assert not is_output\n+                if option['inplace'] and is_output:\n+                    var = 'self'\n+                ptr = 'grad_fn.get()' if is_output else 'nullptr'\n+                expr = 'SavedVariable({}, {})'.format(var, ptr)", "path": "tools/autograd/gen_variable_type.py", "position": 703, "original_position": 693, "commit_id": "937212b9a919339d4afaede4b277e3fd8ae015d0", "original_commit_id": "cca8b80fd3e352fb955f416bee80b010797bf102", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "The `else` succeeds and leaves the `expr` as-is. For example, for `transpose`:\r\n\r\n```\r\n- name: transpose(Tensor self, int64_t dim0, int64_t dim1)\r\n  self: grad.transpose(dim0, dim1)\r\n```\r\n\r\nWe save `dim0` and `dim1` as-is without any munging.", "created_at": "2017-10-19T16:08:43Z", "updated_at": "2018-11-23T15:35:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145748054", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3136", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145748054"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145748054"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3136"}}, "body_html": "<p>The <code>else</code> succeeds and leaves the <code>expr</code> as-is. For example, for <code>transpose</code>:</p>\n<pre><code>- name: transpose(Tensor self, int64_t dim0, int64_t dim1)\n  self: grad.transpose(dim0, dim1)\n</code></pre>\n<p>We save <code>dim0</code> and <code>dim1</code> as-is without any munging.</p>", "body_text": "The else succeeds and leaves the expr as-is. For example, for transpose:\n- name: transpose(Tensor self, int64_t dim0, int64_t dim1)\n  self: grad.transpose(dim0, dim1)\n\nWe save dim0 and dim1 as-is without any munging.", "in_reply_to_id": 145619840}