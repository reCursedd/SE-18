{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144975776", "pull_request_review_id": 69708170, "id": 144975776, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NDk3NTc3Ng==", "diff_hunk": "@@ -129,73 +119,197 @@\n   self: grad * tensor\n   tensor: grad * self\n \n-- name: eig\n-- name: eq\n-- name: equal\n-- name: exp\n+- name: eig(Tensor self, bool eigenvectors)\n+  self: not_implemented(\"eig\")\n+\n+- name: eq(Tensor self, Scalar value)\n+  self: zeros_like(self)\n+\n+- name: eq(Tensor self, Tensor other)\n+  self: zeros_like(self)\n+  other: zeros_like(other)\n+\n+- name: equal  # fallthrough\n+\n+- name: erf(Tensor self)\n+  self: 2.0 / sqrt(M_PI) * exp(-(self.pow(2))) * grad\n+\n+- name: erfinv(Tensor self)\n+  self: 0.5 * sqrt(M_PI) * exp(self.erfinv().pow(2)) * grad\n+\n+- name: exp(Tensor self)\n+  self: grad * result\n \n - name: expand(Tensor self, IntList size)\n   self: reduce_to(grad, self.sizes())\n+  __view__: True\n+\n+- name: eye  # fallthrough\n \n-- name: eye\n-- name: fill\n-- name: floor\n-- name: fmod\n-- name: frac\n-- name: gather\n-- name: ge\n-- name: gels\n-- name: geometric\n-- name: geqrf\n+- name: fill(Tensor self, Scalar value)  # FIXME\n+\n+- name: floor(Tensor self)\n+  self: zeros_like(grad)\n+\n+- name: fmod(Tensor self, Scalar value)\n+  self: grad\n+\n+- name: fmod(Tensor self, Tensor other)\n+  self: grad\n+  other: 'not_implemented(\"fmod: other\")'\n+\n+- name: frac(Tensor self)\n+  self: grad\n+\n+- name: gather(Tensor self, int64_t dim, Tensor index)\n+  self: grad.type().zeros(self.sizes()).scatter_add_(dim, index, grad)\n+\n+- name: ge(Tensor self, Scalar value)\n+  self: zeros_like(self)\n+\n+- name: ge(Tensor self, Tensor other)\n+  self: zeros_like(self)\n+  other: zeros_like(other)\n+\n+- name: gels(Tensor self, Tensor A)\n+  self: not_implemented(\"gels\")\n+  A: not_implemented(\"gels\")\n+\n+- name: geometric(Tensor self, double p, Generator Generator)\n+  self: zeros_like(grad)\n+\n+- name: geqrf(Tensor self)\n+  self: not_implemented(\"geqrf\")\n \n - name: ger(Tensor self, Tensor vec2)\n   self: grad.mv(vec2)\n   vec2: grad.t().mv(self)\n \n-- name: gesv\n+- name: gesv(Tensor self, Tensor A)\n+  self: std::get<0>(gesv(grad, A.t()))\n+  A: -at::mm(std::get<0>(gesv(grad, A.t())), solution.t())", "path": "tools/autograd/derivatives.yaml", "position": 175, "original_position": 175, "commit_id": "937212b9a919339d4afaede4b277e3fd8ae015d0", "original_commit_id": "8d2acdc6ee302936cc5c344e849bc4573c379a6c", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "nit: if we want to reuse intermediate computations (like the `gesv(grad, A.t())`), the recommended way will be to write the derivative in C++ and not rely on the yaml to express the computations, right?", "created_at": "2017-10-16T21:37:22Z", "updated_at": "2018-11-23T15:35:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/3136#discussion_r144975776", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3136", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144975776"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3136#discussion_r144975776"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3136"}}, "body_html": "<p>nit: if we want to reuse intermediate computations (like the <code>gesv(grad, A.t())</code>), the recommended way will be to write the derivative in C++ and not rely on the yaml to express the computations, right?</p>", "body_text": "nit: if we want to reuse intermediate computations (like the gesv(grad, A.t())), the recommended way will be to write the derivative in C++ and not rely on the yaml to express the computations, right?"}