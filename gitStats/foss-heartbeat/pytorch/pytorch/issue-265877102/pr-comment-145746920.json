{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145746920", "pull_request_review_id": 70574035, "id": 145746920, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTc0NjkyMA==", "diff_hunk": "@@ -309,74 +417,75 @@ def create_autograd_functions(top_env, declarations):\n     function_declarations = top_env['autograd_function_declarations']\n     py_function_initializers = top_env['py_function_initializers']\n \n-    def process_function(op):\n+    def process_function(func):\n+        env = {}\n         saved_variables = []\n         release_variables = []\n-        for arg in op['saved']:\n+        unpack = []\n+\n+        def save_arg(arg, is_output):\n             name = arg['name']\n-            if arg['type'] == 'Tensor':\n+            if arg['type'] == 'Tensor' or (arg['type'] == 'Scalar' and is_output):\n                 saved_variables.append('SavedVariable {}_;'.format(name))\n                 release_variables.append('{}_.data.reset();'.format(name))\n+                ptr = 'shared_from_this()' if is_output else ''\n+                unpack.append('auto {} = {}_.unpack({});'.format(name, name, ptr))\n             elif arg['type'] == 'IntList':\n                 saved_variables.append('std::vector<int64_t> {};'.format(name))\n             else:\n                 saved_variables.append('{} {};'.format(arg['type'], name))\n-        op['saved_variables'] = saved_variables\n-        op['release_variables'] = release_variables\n+\n+        for arg in func['saved_inputs']:\n+            save_arg(arg, is_output=False)\n+        for arg in func['saved_outputs']:\n+            save_arg(arg, is_output=True)\n+        env['saved_variables'] = saved_variables\n+        env['release_variables'] = release_variables\n+\n+        def uses_grad(func):\n+            for derivative in func['derivatives']:\n+                formula = derivative['formula']\n+                if re.search(IDENT_REGEX.format('grad'), formula):\n+                    return True\n+            return False\n \n         body = []\n-        body.append('auto& grad = inputs[0];')\n-\n-        def unpack_args():\n-            unpack = []\n-            for arg in op['saved']:\n-                if arg['type'] == 'Tensor':\n-                    name = arg['name']\n-                    unpack.append('auto {} = {}_.unpack();'.format(name, name))\n-            return unpack\n-\n-        body.extend(unpack_args())\n-\n-        i = 0\n-        added_derivative_tensor = False\n-        added_derivative_tensorlist = False\n-        for arg in op['python_arguments']:\n-            derivative = arg.get('derivative')\n-            if derivative is None:\n-                continue\n \n-            if arg['type'] == 'TensorList':\n-                if added_derivative_tensor:\n-                    raise RuntimeError(\"derivatives don't support specifying both a TensorList \"\n-                                       \"and non-TensorList derivative yet\")\n-                added_derivative_tensorlist = True\n-                body.append(DERIVATIVE_TENSORLIST.substitute({\n-                    'i': i,\n-                    'derivative': derivative,\n-                }))\n+        if uses_grad(func):\n+            body.append('auto& grad = inputs[0];')\n+\n+        def emit_derivative(derivative):\n+            formula = derivative['formula']\n+            idxs = derivative['output_indices']\n+            if idxs == '*':\n+                return DERIVATIVE_TENSORLIST.substitute(derivative=formula)\n+            elif len(idxs) == 1:\n+                return DERIVATIVE_TENSOR.substitute(idx=idxs[0], derivative=formula)\n             else:\n-                if added_derivative_tensorlist:\n-                    raise RuntimeError(\"derivatives don't support specifying both a TensorList \"\n-                                       \"and non-TensorList derivative yet\")\n-                added_derivative_tensor = True\n-                body.append(DERIVATIVE_TENSOR.substitute({\n-                    'i': i,\n-                    'derivative': derivative,\n-                }))\n-            i += 1\n+                grad_inputs = ', '.join(['grad_inputs[{}]'.format(i) for i in idxs])\n+                masks = ['should_compute_output({}),'.format(i) for i in idxs]\n+                return DERIVATIVE_MULTI.substitute(\n+                    idxs=idxs, derivative=formula, grad_inputs=grad_inputs,\n+                    masks=masks, n=len(idxs))", "path": "tools/autograd/gen_variable_type.py", "position": 621, "original_position": 611, "commit_id": "937212b9a919339d4afaede4b277e3fd8ae015d0", "original_commit_id": "cca8b80fd3e352fb955f416bee80b010797bf102", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "It's the index in `grad_input` for each derivative formula. It has to correspond to the ordering of the Tensors passed to Function::flags(...).\r\n\r\nSo for `addcmul` we have:\r\n\r\n```\r\n- name: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1)\r\n  self: grad\r\n  tensor1: grad * tensor2 * value\r\n  tensor2: grad * tensor1 * value\r\n```\r\n\r\nThere's 3 derivative formulas (self, tensor1, tensor2). The ordering is defined by the ordering of the arguments to the ATen function (self, tensor1, tensor2). In this case the output_indices is `[0]` for self, `[1]` for tensor1 and `[2]` for tensor2.", "created_at": "2017-10-19T16:04:50Z", "updated_at": "2018-11-23T15:35:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145746920", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3136", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145746920"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145746920"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3136"}}, "body_html": "<p>It's the index in <code>grad_input</code> for each derivative formula. It has to correspond to the ordering of the Tensors passed to Function::flags(...).</p>\n<p>So for <code>addcmul</code> we have:</p>\n<pre><code>- name: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1)\n  self: grad\n  tensor1: grad * tensor2 * value\n  tensor2: grad * tensor1 * value\n</code></pre>\n<p>There's 3 derivative formulas (self, tensor1, tensor2). The ordering is defined by the ordering of the arguments to the ATen function (self, tensor1, tensor2). In this case the output_indices is <code>[0]</code> for self, <code>[1]</code> for tensor1 and <code>[2]</code> for tensor2.</p>", "body_text": "It's the index in grad_input for each derivative formula. It has to correspond to the ordering of the Tensors passed to Function::flags(...).\nSo for addcmul we have:\n- name: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1)\n  self: grad\n  tensor1: grad * tensor2 * value\n  tensor2: grad * tensor1 * value\n\nThere's 3 derivative formulas (self, tensor1, tensor2). The ordering is defined by the ordering of the arguments to the ATen function (self, tensor1, tensor2). In this case the output_indices is [0] for self, [1] for tensor1 and [2] for tensor2.", "in_reply_to_id": 145619408}