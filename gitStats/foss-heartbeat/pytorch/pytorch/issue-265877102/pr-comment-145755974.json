{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145755974", "pull_request_review_id": 70574035, "id": 145755974, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTc1NTk3NA==", "diff_hunk": "@@ -139,19 +196,59 @@ void wrap_output(VariableImpl& pImpl, FunctionFlags flags, std::shared_ptr<Funct\n   }\n }\n \n+static void wrap_output(Tensor& t, FunctionFlags flags, std::shared_ptr<Function> grad_fn) {\n+  auto pImpl = static_cast<VariableImpl*>(t.get());\n+  wrap_output(*pImpl, std::move(flags), std::move(grad_fn));\n+}\n+\n+static void wrap_output(std::tuple<Variable, Variable>& t, FunctionFlags flags, std::shared_ptr<Function> grad_fn) {\n+  wrap_output(std::get<0>(t), std::move(flags), std::move(grad_fn));\n+}\n+\n+static void wrap_output(std::tuple<Variable, Variable, Variable>& t, FunctionFlags flags, std::shared_ptr<Function> grad_fn) {\n+  wrap_output(std::get<0>(t), std::move(flags), std::move(grad_fn));\n+}\n+\n+static void increment_version(const Tensor & t) {\n+  auto pImpl = static_cast<VariableImpl*>(t.get());\n+  (*pImpl->version_counter)++;\n+}\n+\n+static void take_version_counter(Tensor & dst, const Tensor & src) {\n+  // replaces the version counter in dst with the one in src\n+  // call when dst is a view of src\n+  auto srcImpl = static_cast<VariableImpl*>(src.get());\n+  auto dstImpl = static_cast<VariableImpl*>(dst.get());\n+  dstImpl->version_counter->join_with(*srcImpl->version_counter);\n+}\n+\n+static bool isFloatingPoint(ScalarType s) {\n+  return s == kFloat || s == kDouble || s == kHalf;\n+}\n+\n void VariableType::s_copy(const Tensor & src, Tensor & dst) const {\n-  auto& src_ = checked_unpack(src, \"src\", 0);\n-  auto& dst_ = checked_unpack(dst, \"dst\", 1);\n+  // TODO: once copy is exposed in Declarations.yaml we may be able to bind\n+  // it automatically\n+  auto& src_ = unpack_var(src, \"src\", 0);\n+  auto& dst_ = unpack(dst, \"dst\", 1);\n   auto& pImpl = static_cast<VariableImpl&>(*dst.get());\n   check_inplace(pImpl);\n   auto flags = Function::flags({ src });\n   baseType->s_copy(src_, dst_);\n   (*pImpl.version_counter)++;\n-  wrap_output(pImpl, std::move(flags), std::make_shared<Identity>());\n+  if (isFloatingPoint(dst.type().scalarType())) {\n+    if (isFloatingPoint(src.type().scalarType())) {", "path": "tools/autograd/templates/VariableType.cpp", "position": 217, "original_position": 221, "commit_id": "937212b9a919339d4afaede4b277e3fd8ae015d0", "original_commit_id": "cca8b80fd3e352fb955f416bee80b010797bf102", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "float -> int conversion does an implicit `floor()` operation so the gradient going back should be zero. We're not handling this correctly in the current Python implementation.", "created_at": "2017-10-19T16:39:37Z", "updated_at": "2018-11-23T15:35:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145755974", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3136", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145755974"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145755974"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3136"}}, "body_html": "<p>float -&gt; int conversion does an implicit <code>floor()</code> operation so the gradient going back should be zero. We're not handling this correctly in the current Python implementation.</p>", "body_text": "float -> int conversion does an implicit floor() operation so the gradient going back should be zero. We're not handling this correctly in the current Python implementation.", "in_reply_to_id": 145387567}