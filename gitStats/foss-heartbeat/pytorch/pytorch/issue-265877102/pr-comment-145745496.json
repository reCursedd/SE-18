{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145745496", "pull_request_review_id": 70574035, "id": 145745496, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTc0NTQ5Ng==", "diff_hunk": "@@ -160,146 +194,220 @@ def write(dirname, name, template, env):\n         f.write(template.substitute(env))\n \n \n-def load_derivatives(path):\n-    with open(path, 'r') as f:\n-        definitions = yaml.load(f, Loader=Loader)\n+def saved_variables(formula, args):\n+    # find which arguments need to be saved\n+    saved = []\n \n-    # Matches \"foo\" in \"foo, bar\" but not \"foobar\". The name is substituted for\n-    # the {} characters.\n-    name_regex = r'(^|\\W){}($|\\W)'\n+    for arg in args:\n+        if 'name' not in arg:\n+            # some returned arguments do not have names\n+            continue\n+        name = arg['name']\n+\n+        def replace_sizes(m):\n+            res = name + '_sizes'\n+            saved.append({'name': res, 'type': 'IntList'})\n+            return res\n+\n+        def replace_zeros(m):\n+            r = name + '_info'\n+            saved.append({'name': r, 'type': 'TypeAndSize'})\n+            return r + '.zeros()'\n+\n+        def replace_size_n(m):\n+            res = name + '_argsize_{}'.format(*m.groups())\n+            saved.append({'name': res, 'type': 'int64_t'})\n+            return res\n+\n+        def replace_to_arg_sizes(m):\n+            res = name + '_argsizes_{}'.format(*m.groups())\n+            saved.append({'name': res, 'type': 'IntList'})\n+            return res\n+\n+        # replace self.sizes() with self_sizes\n+        formula = re.sub(r'{}.sizes\\(\\)'.format(name), replace_sizes, formula)\n+        # replace zeros_like(self) with self_info\n+        formula = re.sub(r'zeros_like\\({}\\)'.format(name), replace_zeros, formula)\n+        # replace self.size(2) with self_size_2\n+        formula = re.sub(r'{}.size\\((\\w+)\\)'.format(name), replace_size_n, formula)\n+        # replace to_arg_sizes(self, 2) with self_argsizes_2\n+        formula = re.sub(r'to_arg_sizes\\({}, (\\w+)\\)'.format(name), replace_to_arg_sizes, formula)\n+\n+        if re.search(IDENT_REGEX.format(name), formula):\n+            arg = copy.deepcopy(arg)\n+            arg['type'] = arg['type'].replace('const ', '').replace(' &', '')\n+            saved.append(arg)\n+    return formula, saved\n+\n+\n+def create_derivative(declaration, formula, output_indices, var_names):\n+    returns = [r for r in declaration['returns'] if r.get('name') != 'self']\n+    arguments = [arg for arg in declaration['arguments']]\n+    if any(arg['name'] == 'inplace' for arg in arguments):\n+        for arg in arguments:\n+            if arg['name'] == 'input':\n+                returns += [arg]\n+        arguments = [arg for arg in arguments if arg['name'] != 'input']\n+    formula, saved_inputs = saved_variables(formula, arguments)\n+    formula, saved_outputs = saved_variables(formula, returns)\n+\n+    return {\n+        'formula': formula,\n+        'output_indices': output_indices,\n+        'saved_inputs': saved_inputs,\n+        'saved_outputs': saved_outputs,\n+        'var_names': var_names,\n+    }\n+\n+\n+def create_autograd_function(name, derivatives, num_inputs, buffers=None):\n+    return {\n+        'name': name,\n+        'op': to_camel_case(name) + 'Backward',\n+        'num_inputs': num_inputs,\n+        'derivatives': derivatives,\n+        'buffers': [] if buffers is None else buffers,\n+        'saved_inputs': all_saved_variables(derivatives, 'saved_inputs'),\n+        'saved_outputs': all_saved_variables(derivatives, 'saved_outputs'),\n+    }\n+\n+\n+def all_saved_variables(derivatives, key):\n+    seen = set()\n+    saved = []\n+    for d in derivatives:\n+        for saved_arg in d[key]:\n+            if saved_arg['name'] in seen:\n+                continue\n+            seen.add(saved_arg['name'])\n+            saved.append(saved_arg)\n+    return saved\n \n-    def split_name_params(prototype):\n-        name, params = re.match('(\\w+)\\((.*)\\)', prototype).groups()\n-        return name, params.split(', ')\n \n-    def get_signature(option):\n-        arguments = option['python_arguments']\n-        arg_types = [arg['type'] for arg in arguments]\n-        if option['aten'] is not None:\n-            call_args = split_name_params(option['aten'])[1]\n-            arg_indices = {arg['name']: i for i, arg in enumerate(arguments)}\n+def to_camel_case(name):\n+    return ''.join([p.title() for p in name.split('_')])\n \n-            def get_type(arg_name):\n-                if arg_name not in arg_indices:\n-                    # if the name is not an argument, assume it's a literal\n-                    # number, with type 'Scalar'\n-                    return 'Scalar'\n-                return arg_types[arg_indices[arg_name]]\n \n-            arg_types = [get_type(arg_name) for arg_name in call_args]\n-        return '{}({})'.format(option['name'], ', '.join(arg_types))\n+def split_name_params(prototype):\n+    name, params = re.match('(\\w+)\\((.*)\\)', prototype).groups()\n+    return name, params.split(', ')\n+\n+\n+def load_derivatives(path, declarations_by_signature):\n+    with open(path, 'r') as f:\n+        definitions = yaml.load(f, Loader=Loader)\n+\n+    def canonical_declaration(declarations, name):\n+        for declaration in declarations:\n+            if declaration['name'] == name:\n+                return declaration\n+        return declarations[0]\n \n     # Parse each entry from derivatives.yaml\n-    options = []\n+    autograd_functions = []\n     for defn in definitions:\n-        option = {}\n         if '(' not in defn['name']:\n             continue\n+\n         name, params = split_name_params(defn['name'])\n-        num_tensor_inputs = 0\n-        option['name'] = name\n-        option['aten'] = defn.get('aten')\n-        option['python_arguments'] = []\n-        option['prototype'] = defn['name']  # with default\n-        option['fallthrough'] = defn.get('fallthrough', False)\n-        option['op'] = name[0].upper() + name[1:] + 'Backward'\n-\n-        arg_sizes_found = []\n+        param_types = [p.split(' ')[0] for p in params if p != '*']\n+        signature = '{}({})'.format(name, ', '.join(param_types))\n+\n+        declarations = declarations_by_signature[signature]\n+        if len(declarations) == 0:\n+            raise RuntimeError('no ATen declaration found for: {}'.format(signature))\n+        canonical = canonical_declaration(declarations, name)\n+\n+        num_inputs = 0\n         derivatives = []\n-        for param in params:\n-            if param == '' or param == '*':\n+        for arg in canonical['arguments']:\n+            if arg['name'] not in defn:\n                 continue\n-            arg = {}\n-            arg['type'], name = param.split(' ')\n-            if '=' in name:\n-                name, default = name.split('=')\n-                arg['optional'] = True\n-                arg['default'] = default\n-            arg['name'] = name\n-            option['python_arguments'].append(arg)\n-\n-            if name in defn:\n-                saved = []\n-                formula = defn[name]\n-                for arg in option['python_arguments']:\n-                    size_str = arg['name'] + '.sizes()'\n-                    if size_str in formula:\n-                        sizes_name = arg['name'] + '_sizes'\n-                        formula = formula.replace(size_str, sizes_name)\n-\n-                    # If x is a TensorList, turn x.sizes(y) into x_argsizes_y\n-                    def argsizes_repl(matchobj):\n-                        if arg['type'] != 'TensorList':\n-                            raise RuntimeError(\"sizes(argument) only supported on TensorList\")\n-                        argsizes_name = arg['name'] + \"_argsizes_\" + matchobj.group(1)\n-                        arg_sizes_found.append(argsizes_name + \".size()\")\n-                        return argsizes_name\n-                    formula = re.sub(arg['name'] + r\".sizes\\((\\w+)\\)\", argsizes_repl, formula)\n-\n-                    # If x is a Tensor, turn x.size(y) into x_argsize_y\n-                    def argsize_repl(matchobj):\n-                        if arg['type'] != 'Tensor':\n-                            raise RuntimeError(\"size(argument) only supported on Tensor\")\n-                        argsize_name = arg['name'] + \"_argsize_\" + matchobj.group(1)\n-                        return argsize_name\n-                    formula = re.sub(arg['name'] + r\".size\\((\\w+)\\)\", argsize_repl, formula)\n-\n-                derivatives.append(formula)\n-                arg['derivative'] = formula\n-                if arg['type'] != \"TensorList\":\n-                    num_tensor_inputs += 1\n-\n-        if arg_sizes_found:\n-            option['num_inputs'] = (\"+\".join(arg_sizes_found) +\n-                                    \"\" if num_tensor_inputs == 0 else \" + \" + str(num_tensor_inputs))\n-        else:\n-            option['num_inputs'] = str(num_tensor_inputs)\n+            formula = defn[arg['name']]\n+            if arg['type'] == 'TensorList':\n+                num_inputs = ''\n+                output_indices = '*'\n+            else:\n+                output_indices = [num_inputs]\n+                num_inputs += 1\n+            derivatives.append(create_derivative(canonical, formula, output_indices, [arg['name']]))\n \n-        if option['aten'] is not None:\n-            option['call_args'] = split_name_params(option['aten'])[1]\n-        else:\n-            option['call_args'] = [arg['name'] for arg in option['python_arguments']]\n-        option['signature'] = get_signature(option)\n+        func = create_autograd_function(name, derivatives, num_inputs)\n+        func['__view__'] = defn.get('__view__', False)\n+        autograd_functions.append(func)\n+        for declaration in declarations:\n+            declaration['derivative'] = func\n+\n+    return autograd_functions\n+\n+\n+def ensure_unique_names(autograd_functions):\n+    # de-duplicate operation names\n+    functions_by_name = defaultdict(list)\n+    for func in autograd_functions:\n+        functions_by_name[func['op']].append(func)\n+    for op in sorted(functions_by_name.keys()):\n+        overloads = functions_by_name[op]\n+        if len(overloads) > 1:\n+            for i, func in enumerate(overloads):\n+                func['op'] += str(i)\n+\n+\n+def preprocess_nn_functions(declarations):\n+    declarations_by_name = defaultdict(list)\n+    for d in declarations:\n+        declarations_by_name[d['name']].append(d)\n+\n+    autograd_functions = []\n+    for declaration in declarations:\n+        name = declaration['name']\n+        if name == 'batch_norm' or 'conv' in name:\n+            continue\n+\n+        fwd_name = name + '_forward'\n+        if fwd_name not in declarations_by_name:\n+            continue\n+        declaration['base_name'] = fwd_name\n \n-        saved = []\n-        for arg in option['python_arguments']:\n+        fwd = declarations_by_name[name + '_forward'][0]\n+\n+        input_num = 0\n+        bwd_name = name + '_backward'\n+        assert len(declarations_by_name[bwd_name]) == 1\n+        bwd = declarations_by_name[bwd_name][0]\n+\n+        def actual(arg):\n             name = arg['name']\n-            sizes_name = name + '_sizes'\n-            if any(re.search(name_regex.format(name), f) for f in derivatives):\n-                saved.append(arg)\n-            if any(sizes_name in f for f in derivatives):\n-                saved.append({\n-                    'name': sizes_name,\n-                    'type': 'IntList',\n-                })\n-            for f in derivatives:\n-                for match_name in re.findall(r\"{}_argsize_\\w+\".format(name), f):\n-                    saved.append({\n-                        'name': match_name,\n-                        'type': 'int64_t',\n-                    })\n-                for match_name in re.findall(r\"{}_argsizes_\\w+\".format(name), f):\n-                    saved.append({\n-                        'name': match_name,\n-                        'type': 'IntList',\n-                    })\n-        option['saved'] = saved\n-\n-        options.append(option)\n-\n-    options = sorted(options, key=lambda o: o['name'])\n-    for name, overloads in groupby(options, lambda o: o['name']):\n-        overloads = list(overloads)\n-        for i, option in enumerate(overloads):\n-            name = option['name']\n-            option['op'] = name[0].upper() + name[1:] + 'Backward'\n-            if len(overloads) > 1:\n-                option['op'] += str(i)\n-\n-    return options\n-\n-\n-def create_autograd_functions(top_env, declarations):\n+            return name if name != 'inplace' else 'false'\n+\n+        actuals = [actual(arg) for arg in bwd['arguments']]\n+        formula = '{}({})'.format(bwd_name, ', '.join(actuals))\n+        formula = formula.replace('grad_output', 'grad')\n+        if not re.search(IDENT_REGEX.format('grad'), formula):\n+            formula = '({}).mul_(grad)'.format(formula)", "path": "tools/autograd/gen_variable_type.py", "position": 495, "original_position": 489, "commit_id": "937212b9a919339d4afaede4b277e3fd8ae015d0", "original_commit_id": "cca8b80fd3e352fb955f416bee80b010797bf102", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "The NN criterion functions don't take in a `grad_output` so we have to multiply by `grad` here.", "created_at": "2017-10-19T15:59:46Z", "updated_at": "2018-11-23T15:35:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145745496", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3136", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145745496"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3136#discussion_r145745496"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3136"}}, "body_html": "<p>The NN criterion functions don't take in a <code>grad_output</code> so we have to multiply by <code>grad</code> here.</p>", "body_text": "The NN criterion functions don't take in a grad_output so we have to multiply by grad here.", "in_reply_to_id": 145618187}