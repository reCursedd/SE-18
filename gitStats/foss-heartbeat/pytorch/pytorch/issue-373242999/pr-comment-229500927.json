{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229500927", "pull_request_review_id": 170018355, "id": 229500927, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTUwMDkyNw==", "diff_hunk": "@@ -0,0 +1,600 @@\n+#pragma once\n+\n+#include <unordered_map>\n+#include <stack>\n+#include <string>\n+\n+#include <ATen/core/Allocator.h>\n+\n+#include \"caffe2/core/common.h\"\n+#include \"caffe2/proto/caffe2_pb.h\"\n+#include \"caffe2/proto/torch_pb.h\"\n+#include \"caffe2/serialize/inline_container.h\"\n+\n+namespace at {\n+namespace serialize {\n+\n+// multiple tensor may share the same content\n+// SharedData contains:\n+//    1) record id (i.e., the offset in the inline container)\n+//    2) size, the size of the content\n+//    3) data, in serialize, IntermediateModel does NOT own the data,\n+//       in deserialize, the data pointer is returned by PyTorchFileReader,\n+//       and IntermediateModel owns the data. The ownership later will be\n+//       transferred to Tensor\n+class SharedData {\n+ public:\n+  // constructor\n+  explicit SharedData(uint64_t record_id, at::DataPtr&& data_ptr, uint64_t size)\n+    : recordId_(record_id), dataPtr_(std::move(data_ptr)), size_(size){}\n+\n+  // getters\n+  void* rawData() {\n+    return dataPtr_.get();\n+  }\n+\n+  uint64_t recordId() const {\n+    return recordId_;\n+  }\n+\n+  uint64_t size() const {\n+    return size_;\n+  }\n+\n+  // setters\n+  void setDataPtr(at::DataPtr&& data_ptr) {\n+    dataPtr_ = std::move(data_ptr);\n+  }\n+\n+  void setRecordId(uint64_t record_id) {\n+    recordId_ = record_id;\n+  }\n+\n+  void setSize(uint64_t size) {\n+    size_ = size;\n+  }\n+\n+ private:\n+  uint64_t recordId_;\n+  at::DataPtr dataPtr_;\n+  uint64_t size_;\n+};\n+\n+// IntermediateDeviceOption stores device related information\n+struct IntermediateDeviceOption {\n+  int32_t deviceType = 0;\n+  int32_t deviceId;\n+  bool hasDeviceId = false;\n+};\n+\n+// IntermediateTensor contains\n+//   1) element type information\n+//   2) shape information\n+//   3) pointer to the data (including offset and strides)\n+class IntermediateTensor final {\n+ public:\n+  // constructor\n+  IntermediateTensor() = default;\n+\n+  // extract data from TensorProto, called in deserialize\n+  // assume record id to data mapping is complete\n+  void update(caffe2::TensorProto* tensor_proto,\n+      std::unordered_map<uint64_t, std::shared_ptr<SharedData>>* id_data) {\n+    AT_ASSERTM(tensor_proto->has_data_type(), \"no data_type in TensorProto!\");\n+    dataType_ = tensor_proto->data_type();\n+    for (int i = 0; i < tensor_proto->dims_size(); ++i) {\n+      dims_.push_back(tensor_proto->dims(i));\n+    }\n+    if (tensor_proto->has_name()) {\n+      // TODO: TensorProto's name is not used, we just keep it here for now\n+      // later we will deprecate it.\n+      name_ = tensor_proto->name();\n+    }\n+    if (tensor_proto->has_device_detail()) {\n+      const auto& device_detail = tensor_proto->device_detail();\n+      deviceDetail_.deviceType = device_detail.device_type();\n+      if (device_detail.has_device_id()) {\n+        deviceDetail_.hasDeviceId = true;\n+        deviceDetail_.deviceId = device_detail.device_id();\n+      }\n+      if (device_detail.has_random_seed()) {\n+        AT_ERROR(\"DeviceOption contains random seed, not supported!\");\n+      }\n+      if (device_detail.has_node_name()) {\n+        AT_ERROR(\"DeviceOption contains node name, not supported!\");\n+      }\n+      if (device_detail.extra_info_size() > 0) {\n+        AT_ERROR(\"DeviceOption contains extra info, not supported!\");\n+      }\n+    }\n+    AT_ASSERTM(tensor_proto->has_storage_type(), \"no storage_type in TensorProto!\");\n+    int64_t storage_type = tensor_proto->storage_type();\n+    switch (storage_type) {\n+      case caffe2::TensorProto_StorageType_TYPED:\n+        // TODO\n+        AT_ERROR(\"Storing data in typed field is not suppored yet!\");\n+      case caffe2::TensorProto_StorageType_RAW:\n+        // TODO\n+        AT_ERROR(\"Storing data in raw field is not supported yet!\");\n+      case caffe2::TensorProto_StorageType_EXTERNAL:\n+        {\n+          AT_ASSERTM(tensor_proto->has_external_data(), \"storage type is EXTERNAL, \"\n+              \"but no external_data in TensorProto!\");\n+          auto& external_data = tensor_proto->external_data();\n+          offset_ = external_data.offset();\n+          for (int i = 0; i < external_data.strides_size(); ++i) {\n+            strides_.push_back(external_data.strides(i));\n+          }\n+          int64_t source_type = external_data.source_type();\n+          if (source_type == caffe2::ExternalDataProto_SourceType_INLINE_CONTAINER) {\n+            AT_ASSERTM(external_data.has_record_id(), \"no record_id in ExternalDataProto and source_type is INLINE_CONTAINER!\");\n+            int64_t record_id = std::stoul(external_data.record_id());\n+            auto it = id_data->find(record_id);\n+            if (it == id_data->end()) {\n+              AT_ERROR(\"Tensor's data is missing in id_data, tensor name is %s, and record_id is %lld\", name_, record_id);\n+            }\n+            data_ = it->second;\n+          } else if (source_type == caffe2::ExternalDataProto_SourceType_SIMPLE_FILE) {\n+            // TODO\n+            AT_ERROR(\"Storing data in separate file is not supported yet!\");\n+          } else {\n+            // TODO\n+            AT_ERROR(\"Unknown source_type: %lld!\", source_type);\n+          }\n+          break;\n+        }\n+      case caffe2::TensorProto_StorageType_NO_CONTENT:\n+        {\n+          noContent_ = true;\n+          break;\n+        }\n+      default:\n+        AT_ERROR(\"Uknown storage_type %lld\", storage_type);\n+    }\n+  }\n+\n+  // update the data pointer, invoked in serialize\n+  void updateData(std::shared_ptr<SharedData> data) {\n+    data_ = data;\n+  }\n+\n+  // dump data to TensorProto, called in serialize\n+  // assume the data is already saved\n+  void dump(caffe2::TensorProto* tensor_proto) {\n+    for (auto dim : dims_) {\n+      tensor_proto->add_dims(dim);\n+    }\n+    tensor_proto->set_data_type(static_cast<caffe2::TensorProto_DataType>(dataType_));\n+    // NB: maybe later we support RAW\n+    tensor_proto->set_storage_type(caffe2::TensorProto_StorageType::TensorProto_StorageType_EXTERNAL);\n+    caffe2::ExternalDataProto* data_proto = tensor_proto->mutable_external_data();\n+    // NB: maybe later we support SIMPLE_FILE\n+    data_proto->set_source_type(caffe2::ExternalDataProto_SourceType_INLINE_CONTAINER);\n+    data_proto->set_record_id(std::to_string(data_->recordId()));\n+    data_proto->set_offset(offset_);\n+    for (auto stride : strides_) {\n+      data_proto->add_strides(stride);\n+    }\n+    caffe2::DeviceOption* device_detail = tensor_proto->mutable_device_detail();\n+    device_detail->set_device_type(deviceDetail_.deviceType);\n+    if (deviceDetail_.hasDeviceId) {\n+      device_detail->set_device_id(deviceDetail_.deviceId);\n+    }\n+  }\n+\n+  // getters/setters\n+  std::shared_ptr<SharedData> data() {\n+    return data_;\n+  }\n+\n+  const IntermediateDeviceOption& deviceDetail() const {\n+    return deviceDetail_;\n+  }\n+\n+  bool noContent() const {\n+    return noContent_;\n+  }\n+\n+  int64_t dataType() const {\n+    return dataType_;\n+  }\n+\n+  std::vector<int64_t>* mutableDims() {\n+    return &dims_;\n+  }\n+\n+  std::vector<int64_t>* mutableStrides() {\n+    return &strides_;\n+  }\n+\n+  IntermediateDeviceOption* mutableDeviceDetail() {\n+    return &deviceDetail_;\n+  }\n+\n+  void setDataType(int64_t data_type) {\n+    dataType_ = data_type;\n+  }\n+\n+  void setData(std::shared_ptr<SharedData> data) {\n+    data_ = data;\n+  }\n+\n+ private:\n+  std::string name_;\n+  int64_t dataType_;\n+  std::vector<int64_t> dims_;\n+  int64_t offset_;\n+  std::vector<int64_t> strides_;\n+  // TODO: since we still have 2 different Tensor classes in Caffe2 and PyTorch\n+  // right now, let's just store the data pointer, and create Tensors\n+  // while converting IntermediateModel to the JitScriptModule/Predictor/etc.\n+  std::shared_ptr<SharedData> data_;\n+  IntermediateDeviceOption deviceDetail_;\n+  bool noContent_ = false;\n+};\n+\n+class IntermediateParameter final {\n+ public:\n+  // constructors\n+  IntermediateParameter() = default;\n+\n+  explicit IntermediateParameter(torch::ParameterDef* param_def,\n+      std::unordered_map<uint64_t, std::shared_ptr<SharedData>>* id_data) {\n+    AT_ASSERTM(param_def->has_name(), \"ParameterDef has no name! %s\",\n+        param_def->DebugString());\n+    name_ = param_def->name();\n+    isBuffer_ = param_def->is_buffer();\n+    requireGradient_ = param_def->require_gradient();\n+    if (param_def->has_tensor()) {\n+      tensor_.update(param_def->mutable_tensor(), id_data);\n+    } else {\n+      // TODO\n+      AT_ERROR(\"A ParameterDef does not contain any tensor!\");\n+    }\n+  }\n+\n+  // dump data to ParameterDef, invoked in serialize\n+  void dump(torch::ParameterDef* param_def) {\n+    param_def->set_name(name_);\n+    param_def->set_is_buffer(isBuffer_);\n+    param_def->set_require_gradient(requireGradient_);\n+    caffe2::TensorProto* tensor_def = param_def->mutable_tensor();\n+    tensor_.dump(tensor_def);\n+  }\n+\n+  // getters/setters\n+  const std::string& name() const {\n+    return name_;\n+  }\n+\n+  bool isBuffer() const {\n+    return isBuffer_;\n+  }\n+\n+  bool requireGradient() const {\n+    return requireGradient_;\n+  }\n+\n+  IntermediateTensor* mutableTensor() {\n+    return &tensor_;\n+  }\n+\n+  void setName(const std::string& name) {\n+    name_ = name;\n+  }\n+\n+  void setIsBuffer(bool is_buffer) {\n+    isBuffer_ = is_buffer;\n+  }\n+\n+  void setRequireGradient(bool require_gradient) {\n+    requireGradient_ = require_gradient;\n+  }\n+\n+ private:\n+  bool isBuffer_ = false;\n+  bool requireGradient_ = false;\n+  IntermediateTensor tensor_;\n+  std::string name_;\n+};\n+\n+class IntermediateMethod final {\n+ public:\n+  // constructors\n+  IntermediateMethod() = default;\n+\n+  explicit IntermediateMethod(torch::MethodDef* method_def) {\n+    AT_ASSERTM(method_def->has_name(), \"name is required for MethodDef!\");\n+    name_ = method_def->name();\n+    if (method_def->has_torch_script()) {\n+      torchScript_ = method_def->torch_script();\n+    } else if (method_def->has_graph()) {\n+      graph_.reset(method_def->release_graph());\n+    } else {\n+      AT_ERROR(\"No method body is found!\");\n+    }\n+  }\n+\n+  // dump data to MethodDef, called in serialize\n+  void dump(torch::MethodDef* method_def) {\n+    AT_ASSERTM(name_.size() > 0, \"IntermediateMethod's name is invalid. name: %s\", name_.c_str());\n+    method_def->set_name(name_);\n+    if (graph_) {\n+      method_def->set_allocated_graph(graph_.release());\n+    } else {\n+      method_def->set_torch_script(torchScript_);\n+    }\n+  }\n+\n+  // getters\n+  const std::string& name() const {\n+    return name_;\n+  }\n+\n+  const std::string& torchScript() const {\n+    return torchScript_;\n+  }\n+\n+  // setters\n+  void setName(const std::string& name) {\n+    name_ = name;\n+  }\n+\n+  void setTorchScript(const std::string& torch_script) {\n+    torchScript_ = torch_script;\n+  }\n+\n+ private:\n+  std::string name_;\n+  std::unique_ptr<caffe2::NetDef> graph_;\n+  std::string torchScript_;\n+};\n+\n+class IntermediateModule final {\n+ public:\n+  // constructors\n+  IntermediateModule() = default;\n+\n+  explicit IntermediateModule(torch::ModuleDef* module_def,\n+      std::unordered_map<uint64_t, std::shared_ptr<SharedData>>* id_data) {\n+    update(module_def, id_data);\n+  }\n+\n+  // extract data from ModuleDef, invoked in deserialize\n+  // assume record id to data mapping is complete\n+  void update(torch::ModuleDef* module_def,\n+      std::unordered_map<uint64_t, std::shared_ptr<SharedData>>* id_data) {\n+    AT_ASSERTM(module_def->has_name(), \"name is required for ModuleDef!\");\n+    name_ = module_def->name();\n+    for (int i = 0; i < module_def->parameters_size(); ++i) {\n+      auto* param_def = module_def->mutable_parameters(i);\n+      parameters_.emplace_back(param_def, id_data);\n+    }\n+\n+    for (int i = 0; i < module_def->submodules_size(); ++i) {\n+      auto* sub_def = module_def->mutable_submodules(i);\n+      submodules_.emplace_back(sub_def, id_data);\n+    }\n+\n+    for (int i = 0; i < module_def->methods_size(); ++i) {\n+      auto* method_def = module_def->mutable_methods(i);\n+      methods_.emplace_back(method_def);\n+    }\n+  }\n+\n+  // dump data to ModuleDef, called in serialize\n+  void dump(torch::ModuleDef* module_def) {\n+    module_def->set_name(name_);\n+\n+    for (int i = 0; i < parameters_.size(); ++i) {\n+      module_def->add_parameters();\n+      torch::ParameterDef* param_def = module_def->mutable_parameters(module_def->parameters_size() - 1);\n+      parameters_.at(i).dump(param_def);\n+    }\n+\n+    for (int i = 0; i < submodules_.size(); ++i) {\n+      module_def->add_submodules();\n+      torch::ModuleDef* sub_def = module_def->mutable_submodules(module_def->submodules_size() - 1);\n+      submodules_.at(i).dump(sub_def);\n+    }\n+\n+    for (int i = 0; i < methods_.size(); ++i) {\n+      module_def->add_methods();\n+      torch::MethodDef* method_def = module_def->mutable_methods(module_def->methods_size() - 1);\n+      methods_.at(i).dump(method_def);\n+    }\n+  }\n+\n+  // getters/setters\n+  const std::string& name() const {\n+    return name_;\n+  }\n+\n+  std::vector<IntermediateParameter>* mutableParameters() {\n+    return &parameters_;\n+  }\n+\n+  std::vector<IntermediateModule>* mutableSubmodules() {\n+    return &submodules_;\n+  }\n+\n+  std::vector<IntermediateMethod>* mutableMethods() {\n+    return &methods_;\n+  }\n+\n+  void setName(const std::string& name) {\n+    name_ = name;\n+  }\n+\n+ private:\n+  std::string name_;\n+  std::vector<IntermediateParameter> parameters_;\n+  std::vector<IntermediateModule> submodules_;\n+  std::vector<IntermediateMethod> methods_;\n+  // TODO handle cpp_arena\n+  // TODO handle pickle_arena\n+};\n+\n+class IntermediateModel final {\n+ public:\n+  // constructor\n+  IntermediateModel() = default;\n+\n+  // extract data from ModelDef, invoked in deserialize\n+  // assume record id to data mapping is complete\n+  void update(torch::ModelDef* model_def,\n+      std::unordered_map<uint64_t, std::shared_ptr<SharedData>>* id_data) {\n+    AT_ASSERTM(model_def->has_name(), \"name is required for ModelDef.\");\n+    name_ = model_def->name();\n+    AT_ASSERTM(model_def->has_producer_name(), \"producer_name is required for ModelDef.\");\n+    producerName_ = model_def->producer_name();\n+    producerVersion_ = model_def->producer_version();\n+    AT_ASSERTM(model_def->has_proto_version(), \"proto_version is required for ModelDef.\");\n+    protoVersion_ = model_def->proto_version();\n+    AT_ASSERTM(model_def->has_main_module(), \"main_module is required for ModelDef.\");\n+    mainModule_.update(model_def->mutable_main_module(), id_data);\n+  }\n+\n+  // dump data to ModelDef, called in serialize\n+  void dump(torch::ModelDef* model_def) {\n+    model_def->set_name(name_);\n+    model_def->set_producer_name(producerName_);\n+    model_def->set_producer_version(producerVersion_);\n+    model_def->set_proto_version(protoVersion_);\n+    mainModule_.dump(model_def->mutable_main_module());\n+  }\n+\n+  // getters\n+  const std::string& name() const {\n+    return name_;\n+  }\n+\n+  const std::string& producerName() const {\n+    return producerName_;\n+  }\n+\n+  const std::string& producerVersion() const {\n+    return producerVersion_;\n+  }\n+\n+  const IntermediateModule& mainModule() const {\n+    return mainModule_;\n+  }\n+\n+  int64_t protoVersion() const {\n+    return protoVersion_;\n+  }\n+\n+  // setters, most for test purposes\n+  void setName(const std::string& name) {\n+    name_ = name;\n+  }\n+\n+  void setProducerName(const std::string& producer_name) {\n+    producerName_ = producer_name;\n+  }\n+\n+  void setProducerVersion(const std::string& producer_version) {\n+    producerVersion_ = producer_version;\n+  }\n+\n+  void setProtoVersion(int64_t proto_version) {\n+    protoVersion_ = proto_version;\n+  }\n+\n+  IntermediateModule* mutableMainModule() {\n+    return &mainModule_;\n+  }\n+\n+ private:\n+  std::string name_;\n+  std::string producerName_;\n+  std::string producerVersion_;\n+  int64_t protoVersion_;\n+  IntermediateModule mainModule_;\n+\n+};\n+\n+// serialize an IntermediateModel through a PyTorchFileWriter\n+// we always put the model data at the end, so when serializing\n+// model, the we assume the record_id in imodel is already updated\n+void serializeIntermediateModel(IntermediateModel* imodel,\n+    torch::jit::PyTorchFileWriter* writer) {\n+  std::unordered_map<void*, uint64_t> data_id;\n+  std::stack<IntermediateModule*> imodules;\n+  imodules.push(imodel->mutableMainModule());\n+  while (!imodules.empty()) {\n+    IntermediateModule* m = imodules.top();\n+    imodules.pop();\n+    auto* params = m->mutableParameters();\n+    for (int i = 0; i < params->size(); ++i) {\n+      std::shared_ptr<SharedData> dataptr = params->at(i).mutableTensor()->data();\n+      void* data = dataptr->rawData();\n+      size_t size = dataptr->size();\n+      auto it = data_id.find(data);\n+      if (it != data_id.end()) {\n+        dataptr->setRecordId(it->second);\n+      } else {\n+        uint64_t id = writer->writeRecord(data, size);\n+        dataptr->setRecordId(id);\n+        data_id[data] = id;\n+      }\n+    }\n+\n+    auto* subms = m->mutableSubmodules();\n+    for (int i = 0; i < subms->size(); ++i) {\n+      imodules.push(&subms->at(i));\n+    }\n+  }\n+\n+  torch::ModelDef model_def;\n+  imodel->dump(&model_def);\n+  size_t proto_size = model_def.ByteSizeLong();\n+  void* buffer = malloc(proto_size);\n+  model_def.SerializeToArray(buffer, proto_size);\n+  writer->writeRecord(buffer, proto_size);\n+  free(buffer);\n+}\n+\n+// serialize an IntermediateModel to a given file\n+void serializeIntermediateModel(IntermediateModel* imodel, const std::string& filename) {\n+  torch::jit::PyTorchFileWriter writer(filename);\n+  serializeIntermediateModel(imodel, &writer);\n+  writer.writeEndOfFile();\n+}\n+\n+// deserialize an IntermediateModel through a reader,\n+// serialize tensors' data first, and maintain the mappint from\n+// record id to tensor data\n+void deserializeIntermediateModel(IntermediateModel* imodel,", "path": "caffe2/serialize/intermediate_model.h", "position": null, "original_position": 569, "commit_id": "53180b449481169c2c2ff9e23ac18289412c62b3", "original_commit_id": "a01175f558ff676c429944fe0c885483a2a4265c", "user": {"login": "houseroad", "id": 30275821, "node_id": "MDQ6VXNlcjMwMjc1ODIx", "avatar_url": "https://avatars0.githubusercontent.com/u/30275821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/houseroad", "html_url": "https://github.com/houseroad", "followers_url": "https://api.github.com/users/houseroad/followers", "following_url": "https://api.github.com/users/houseroad/following{/other_user}", "gists_url": "https://api.github.com/users/houseroad/gists{/gist_id}", "starred_url": "https://api.github.com/users/houseroad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/houseroad/subscriptions", "organizations_url": "https://api.github.com/users/houseroad/orgs", "repos_url": "https://api.github.com/users/houseroad/repos", "events_url": "https://api.github.com/users/houseroad/events{/privacy}", "received_events_url": "https://api.github.com/users/houseroad/received_events", "type": "User", "site_admin": false}, "body": "Yes, I will add a lazy mode in deserialize, which only load the model metadata, and we can feed the data to the tensor later. Right now we keep the model as the last record, and corresponding functionality in the FileReader.", "created_at": "2018-10-30T21:55:45Z", "updated_at": "2018-11-23T15:53:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/13020#discussion_r229500927", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13020", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229500927"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13020#discussion_r229500927"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13020"}}, "body_html": "<p>Yes, I will add a lazy mode in deserialize, which only load the model metadata, and we can feed the data to the tensor later. Right now we keep the model as the last record, and corresponding functionality in the FileReader.</p>", "body_text": "Yes, I will add a lazy mode in deserialize, which only load the model metadata, and we can feed the data to the tensor later. Right now we keep the model as the last record, and corresponding functionality in the FileReader.", "in_reply_to_id": 229166519}