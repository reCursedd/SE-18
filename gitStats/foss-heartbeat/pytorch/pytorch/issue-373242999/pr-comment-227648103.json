{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227648103", "pull_request_review_id": 167736189, "id": 227648103, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzY0ODEwMw==", "diff_hunk": "@@ -0,0 +1,318 @@\n+#pragma once\n+\n+#include <unordered_map>\n+\n+#include <ATen/core/Allocator.h>\n+\n+#include \"caffe2/proto/caffe2_pb.h\"\n+#include \"caffe2/proto/torch_pb.h\"\n+#include \"caffe2/serialize/inline_container.h\"\n+\n+namespace at {\n+\n+struct IntermediateDeviceOption {\n+  int32_t deviceType = 0;\n+  bool hasDeviceId = false;\n+  bool hasRandomSeed = false;\n+  bool hasNodeName = false;\n+  int32_t deviceId;\n+  uint32_t randomSeed;\n+  std::string nodeName;\n+  std::vector<std::string> extraInfo;\n+};\n+\n+class IntermediateTensor final {\n+ public:\n+  // constructor\n+  IntermediateTensor() = default;\n+\n+  // update functions\n+  void update(caffe2::TensorProto* tensor_proto,\n+      std::unordered_map<int64_t, at::DataPtr>* id_tensor) {\n+    CAFFE_ENFORCE(tensor_proto->has_data_type(), \"no data_type in TensorProto!\");\n+    dataType_ = tensor_proto->data_type();\n+    for (int i = 0; i < tensor_proto->dims_size()) {\n+      dims_.push_back(tensor_proto->dims(i));\n+    }\n+    CAFFE_ENFORCE(tensor_proto->has_name(), \"no name in TensorProto!\");\n+    name_ = tensor_proto->name();\n+    if (tensor_proto->has_device_detail()) {\n+      const auto& device_detail = tensor_proto->device_detail();\n+      deviceDetail_.deviceType = device_detail.device_type;\n+      if (device_detail.has_device_id()) {\n+        deviceDetail_.hasDeviceId = true;\n+        deviceDetail_.devicedId = device_detail.device_id();\n+      }\n+      if (device_detail.has_random_seed()) {\n+        deviceDetail_.hasRandomSeed = true;\n+        deviceDetail_.randomSeed = device_detail.random_seed();\n+      }\n+      if (device_detail.has_node_name()) {\n+        deviceDetail_.hasNodeName = true;\n+        deviceDetail_.nodeName = device_detail.nodeName();\n+      }\n+      for (int i = 0; i < device_detail.extra_info_size(); ++i) {\n+        deviceDetail_.extraInfo.emplace_back(device_detail.extra_info(i));\n+      }\n+    }\n+    CAFFE_ENFORCE(tensor_proto->has_storage_type(), \"no storage_type in TensorProto!\");\n+    int64_t storage_type = tensor_proto->storage_type();\n+    switch (storage_type) {\n+      case TensorProto_StorageType_TYPED:\n+        // TODO\n+        CAFFE_THROW(\"Not implemented!\");\n+      case TensorProto_StorageType_RAW:\n+        // TODO create the data type\n+        CAFFE_THROW(\"Not implemented!\");\n+      case TensorProto_StorageType_EXTERNAL:\n+        CAFFE_ENFORCE(tensor_proto->has_external_data(), \"storage type is EXTERNAL, \"\n+            \"but no external_data in TensorProto!\");\n+        const auto& external_data = tensor_proto->external_data();\n+        if (external_data->has_offset()) {\n+          offset_ = external_data->offset();\n+        }\n+        for (int i = 0; i < external_data.strides_size(); ++i) {\n+          strides_->push_back(external_data.strides(i));\n+        }\n+        CAFFE_ENFORCE(external_data->has_source_type(), \"no source_type in TensorProto!\");\n+        int64_t source_type = external_data->source_type();\n+        if (source_type == ExternalDataProto_SourceType::ExternalDataProto_SourceType_INLINE_CONTAINER) {\n+          CAFFE_ENFORCE(external_data->has_record_id(), \"no record_id in ExternalDataProto!\");\n+          int64_t record_id = external_data->record_id();\n+          auto it = id_tensor.find(record_id);\n+          if (it == id_tensor.end()) {\n+            CAFFE_THROW(\"Tensor's data is missing, tensor name is \", name_);\n+          }\n+          dataPtr_ = std::move(it->second);", "path": "caffe2/serialize/intermediate_model.h", "position": null, "original_position": 86, "commit_id": "53180b449481169c2c2ff9e23ac18289412c62b3", "original_commit_id": "2c3ab75ce5e562993df90bc4b94a6d37d1b7f219", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "how does it work with shared storages? you'd move out the storage first time (not sure whether it was tested or not in the prototype, but we should be able to serialize aliasing of storages when several tensors point to the same storage and we serialize storage once)", "created_at": "2018-10-24T05:44:16Z", "updated_at": "2018-11-23T15:53:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/13020#discussion_r227648103", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13020", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227648103"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13020#discussion_r227648103"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13020"}}, "body_html": "<p>how does it work with shared storages? you'd move out the storage first time (not sure whether it was tested or not in the prototype, but we should be able to serialize aliasing of storages when several tensors point to the same storage and we serialize storage once)</p>", "body_text": "how does it work with shared storages? you'd move out the storage first time (not sure whether it was tested or not in the prototype, but we should be able to serialize aliasing of storages when several tensors point to the same storage and we serialize storage once)"}