{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435102364", "html_url": "https://github.com/pytorch/pytorch/pull/13350#issuecomment-435102364", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13350", "id": 435102364, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTEwMjM2NA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T16:38:40Z", "updated_at": "2018-11-01T16:38:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16191443\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/crcrpar\">@crcrpar</a> Almost! <code>weight_orig</code> is still a parameter, so updates to it should be done on the parallelized module (the one you pass over to <code>DataParallel</code>) via things like optimizers explicitly started by user. Even if it is not updated in-place, it is fine, because the next time <code>DataParallel</code> copies it to the devices, the new <code>weight_orig</code> will be get from the parallelized module, and broadcast over.</p>\n<p><code>u</code> and <code>v</code>, however, are a bit different because they are updated when the module is activated, and not within users' control. So it is our job to automatically update it. And to ensure that such update works with <code>DataParallel</code>, it needs to be done in-place.</p>", "body_text": "@crcrpar Almost! weight_orig is still a parameter, so updates to it should be done on the parallelized module (the one you pass over to DataParallel) via things like optimizers explicitly started by user. Even if it is not updated in-place, it is fine, because the next time DataParallel copies it to the devices, the new weight_orig will be get from the parallelized module, and broadcast over.\nu and v, however, are a bit different because they are updated when the module is activated, and not within users' control. So it is our job to automatically update it. And to ensure that such update works with DataParallel, it needs to be done in-place.", "body": "@crcrpar Almost! `weight_orig` is still a parameter, so updates to it should be done on the parallelized module (the one you pass over to `DataParallel`) via things like optimizers explicitly started by user. Even if it is not updated in-place, it is fine, because the next time `DataParallel` copies it to the devices, the new `weight_orig` will be get from the parallelized module, and broadcast over. \r\n\r\n`u` and `v`, however, are a bit different because they are updated when the module is activated, and not within users' control. So it is our job to automatically update it. And to ensure that such update works with `DataParallel`, it needs to be done in-place."}