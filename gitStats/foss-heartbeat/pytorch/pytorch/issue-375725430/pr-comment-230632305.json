{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230632305", "pull_request_review_id": 171411421, "id": 230632305, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMDYzMjMwNQ==", "diff_hunk": "@@ -1749,58 +1754,173 @@ def test_spectral_norm(self):\n         self.assertTrue(hasattr(m, 'weight'))\n         self.assertTrue('weight' in m._parameters)\n \n-    @unittest.skipIf(not TEST_MULTIGPU, \"multi-GPU not supported\")\n-    def test_spectral_norm_dp(self):\n-        for requires_grad in (True, False):\n-            m = nn.Linear(5, 7).to(torch.device('cuda'))\n-            m.weight.requires_grad_(requires_grad)\n+        with self.assertRaisesRegex(RuntimeError, 'register two spectral_norm hooks'):\n             m = torch.nn.utils.spectral_norm(m)\n-            dpm = torch.nn.DataParallel(m, [0, 1])\n-            self.assertTrue(hasattr(m, 'weight_u'))\n-            u0 = m.weight_u.clone()\n-\n-            # assert that u is updated\n-            input = torch.randn(2, 5, device=torch.device('cuda'))\n-            dpm(input)\n-            self.assertNotEqual(u0, m.weight_u)\n-\n-            # test that eval works\n-            dpm.eval()\n-            eval_out0 = dpm(input)\n-            self.assertEqual(eval_out0, dpm(input))\n-\n-    def test_spectral_norm_eval_remove(self):\n-        inp = torch.randn(3, 5)\n-        m = nn.Linear(5, 7)\n-        m = torch.nn.utils.spectral_norm(m)\n-        x0 = m(inp)\n-        m.eval()\n-        # test that eval mode and removing / adding+removing doesn't change weight and output\n-        x1 = m(inp)\n-        x2 = m(inp)\n-        self.assertEqual(x0, x1)\n-        self.assertEqual(x0, x2)\n-        # test that we can backward several times without running into problems\n-        x1 = m(inp)\n-        x1.sum().backward()\n-        x1 = m(inp)\n-        x1.sum().backward()\n-        # test removing\n-        m = torch.nn.utils.remove_spectral_norm(m)\n-        x3 = m(inp)\n-        self.assertEqual(x0, x3)\n-        m = torch.nn.utils.spectral_norm(m)\n-        m = torch.nn.utils.remove_spectral_norm(m)\n-        x4 = m(inp)\n-        self.assertEqual(x0, x4)\n-        # check that removing after train doesn't change output\n-        m.train()\n-        m = torch.nn.utils.spectral_norm(m)\n-        for i in range(5):\n-            x0 = m(inp)\n-        m = torch.nn.utils.remove_spectral_norm(m)\n-        x1 = m(inp)\n-        self.assertEqual(x0, x1)\n+            m = torch.nn.utils.spectral_norm(m)\n+\n+        # test correctness in training/eval modes and cpu/multi-gpu settings\n+        for apply_dp in (True, False):\n+            if apply_dp:\n+                if not TEST_MULTIGPU:\n+                    continue\n+                device = torch.device('cuda:0')\n+\n+                def maybe_wrap(m):\n+                    return torch.nn.DataParallel(m, [0, 1])\n+            else:\n+                device = torch.device('cpu')\n+\n+                def maybe_wrap(m):\n+                    return m\n+\n+            for requires_grad in (True, False):\n+                m = nn.Linear(3, 4).to(device)\n+                m.weight.requires_grad_(requires_grad)\n+                m = torch.nn.utils.spectral_norm(m)\n+                wrapped_m = maybe_wrap(m)\n+                self.assertTrue(hasattr(m, 'weight_u'))\n+                u0 = m.weight_u.clone()\n+                v0 = m.weight_v.clone()\n+\n+                # TEST TRAINING BEHAVIOR\n+\n+                # assert that u and v are updated\n+                input = torch.randn(2, 3, device=device)\n+                out = wrapped_m(input)\n+                self.assertNotEqual(u0, m.weight_u)\n+                self.assertNotEqual(v0, m.weight_v)\n+\n+                # assert that backprop reaches weight_orig\n+                # can't use gradcheck because the function changes as we\n+                # activate through it in training mode\n+                if requires_grad:\n+                    torch.autograd.grad(out.sum(), m.weight_orig)\n+\n+                # test backward works with multiple forwards\n+                # it uses training mode so we need to reset `u` and `v` vectors\n+                # to same value at beginning for finite difference test to pass\n+                saved_u = m.weight_u.clone()\n+                saved_v = m.weight_v.clone()\n+\n+                def fn(input):\n+                    m.weight_u.data.copy_(saved_u)\n+                    m.weight_v.data.copy_(saved_v)\n+                    out0 = wrapped_m(input)\n+                    out1 = wrapped_m(input)\n+                    return out0 + out1\n+\n+                torch.autograd.gradcheck(fn, (input.clone().requires_grad_(),))\n+\n+                # test removing\n+                pre_remove_out = wrapped_m(input)\n+                m = torch.nn.utils.remove_spectral_norm(m)\n+                self.assertEqual(wrapped_m(input), pre_remove_out)\n+\n+                m = torch.nn.utils.spectral_norm(m)\n+                m = torch.nn.utils.remove_spectral_norm(m)\n+                self.assertEqual(wrapped_m(input), pre_remove_out)\n+\n+                m = torch.nn.utils.spectral_norm(m)\n+                for i in range(3):", "path": "test/test_nn.py", "position": 146, "original_position": 144, "commit_id": "63a22fb52251a24a16f5e07d3633b625a5e87ad3", "original_commit_id": "63b20bdea2231713a375556d1df134325d5a979c", "user": {"login": "crcrpar", "id": 16191443, "node_id": "MDQ6VXNlcjE2MTkxNDQz", "avatar_url": "https://avatars2.githubusercontent.com/u/16191443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crcrpar", "html_url": "https://github.com/crcrpar", "followers_url": "https://api.github.com/users/crcrpar/followers", "following_url": "https://api.github.com/users/crcrpar/following{/other_user}", "gists_url": "https://api.github.com/users/crcrpar/gists{/gist_id}", "starred_url": "https://api.github.com/users/crcrpar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crcrpar/subscriptions", "organizations_url": "https://api.github.com/users/crcrpar/orgs", "repos_url": "https://api.github.com/users/crcrpar/repos", "events_url": "https://api.github.com/users/crcrpar/events{/privacy}", "received_events_url": "https://api.github.com/users/crcrpar/received_events", "type": "User", "site_admin": false}, "body": "thanks", "created_at": "2018-11-05T04:32:14Z", "updated_at": "2018-11-23T15:54:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/13350#discussion_r230632305", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13350", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230632305"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13350#discussion_r230632305"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13350"}}, "body_html": "<p>thanks</p>", "body_text": "thanks", "in_reply_to_id": 230619471}