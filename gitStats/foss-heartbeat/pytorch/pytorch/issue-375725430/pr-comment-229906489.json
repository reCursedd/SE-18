{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229906489", "pull_request_review_id": 170518396, "id": 229906489, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTkwNjQ4OQ==", "diff_hunk": "@@ -17,90 +25,168 @@ def __init__(self, name='weight', n_power_iterations=1, dim=0, eps=1e-12):\n         self.n_power_iterations = n_power_iterations\n         self.eps = eps\n \n-    def compute_weight_and_update_u(self, module):\n-        # NB: This updates the _u vector **in-place**. This is very important\n-        #     because in DataParallel forward, the _u vector (being a buffer) is\n+    def reshape_weight_to_matrix(self, weight):\n+        weight_mat = weight\n+        if self.dim != 0:\n+            # permute dim to front\n+            weight_mat = weight_mat.permute(self.dim,\n+                                            *[d for d in range(weight_mat.dim()) if d != self.dim])\n+        height = weight_mat.size(0)\n+        return weight_mat.reshape(height, -1)\n+\n+    def compute_weight(self, module, do_power_iteration):\n+        # NB: If `do_power_iteration` is set, the `u` and `v` vectors are\n+        #     updated in power iteration **in-place**. This is very important\n+        #     because in `DataParallel` forward, the vectors (being buffers) are\n         #     broadcast from the parallelized module to each module replica,\n-        #     which is a new module object on the fly. And each replica runs its\n-        #     own spectral norm power iteration. So simply assigning the updated\n-        #     _u vector to module this runs on will cause the update to be lost\n-        #     forever. And the next time the parallelized module is replicated,\n-        #     the same randomly initialized _u vector is broadcast!\n+        #     which is a new module object created on the fly. And each replica\n+        #     runs its own spectral norm power iteration. So simply assigning\n+        #     the updated vectors to the module this function runs on will cause\n+        #     the update to be lost forever. And the next time the parallelized\n+        #     module is replicated, the same randomly initialized vectors are\n+        #     broadcast and used!\n         #\n         #     Therefore, to make the change propagate back, we rely on two\n         #     important bahaviors (also enforced via tests):\n-        #       1. DataParallel doesn't clone storage if the broadcast tensor is\n-        #          alreay on correct device; and it makes sure that the\n-        #          parallelized module is already on device[0].\n-        #       2. If the out tensor in out= kwarg has correct shape, it will\n+        #       1. `DataParallel` doesn't clone storage if the broadcast tensor\n+        #          is alreay on correct device; and it makes sure that the\n+        #          parallelized module is already on `device[0]`.\n+        #       2. If the out tensor in `out=` kwarg has correct shape, it will\n         #          just fill in the values.\n         #     Therefore, since the same power iteration is performed on all\n-        #     devices, simply updating the _u tensor in-place will make sure\n-        #     that the module replica on device[0] will update the _u vector on\n-        #     the parallized module (by shared storage).\n+        #     devices, simply updating the tensors in-place will make sure that\n+        #     the module replica on `device[0]` will update the _u vector on the\n+        #     parallized module (by shared storage).\n+        #\n+        #    However, after we update `u` and `v` in-place, we need to **clone**\n+        #    them before using them to normalize the weight. This is to support\n+        #    backproping through two forward passes, e.g., the common pattern in\n+        #    GAN training: loss = D(real) - D(fake). Otherwise, engine will\n+        #    complain that variables needed to do backward for the first forward\n+        #    (i.e., the `u` and `v` vectors) are changed in the second forward.\n         weight = getattr(module, self.name + '_orig')\n         u = getattr(module, self.name + '_u')\n-        weight_mat = weight\n-        if self.dim != 0:\n-            # permute dim to front\n-            weight_mat = weight_mat.permute(self.dim,\n-                                            *[d for d in range(weight_mat.dim()) if d != self.dim])\n-        height = weight_mat.size(0)\n-        weight_mat = weight_mat.reshape(height, -1)\n-        with torch.no_grad():\n-            for _ in range(self.n_power_iterations):\n-                # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n-                # are the first left and right singular vectors.\n-                # This power iteration produces approximations of `u` and `v`.\n-                v = normalize(torch.matmul(weight_mat.t(), u), dim=0, eps=self.eps)\n-                u = normalize(torch.matmul(weight_mat, v), dim=0, eps=self.eps, out=u)\n-\n-        sigma = torch.dot(u, torch.matmul(weight_mat, v))\n+        v = getattr(module, self.name + '_v')\n+        weight_mat = self.reshape_weight_to_matrix(weight)\n+\n+        if do_power_iteration:\n+            with torch.no_grad():\n+                for _ in range(self.n_power_iterations):\n+                    # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n+                    # are the first left and right singular vectors.\n+                    # This power iteration produces approximations of `u` and `v`.\n+                    v = normalize(torch.mv(weight_mat.t(), u), dim=0, eps=self.eps, out=v)\n+                    u = normalize(torch.mv(weight_mat, v), dim=0, eps=self.eps, out=u)\n+                if self.n_power_iterations > 0:\n+                    # See above on why we need to clone\n+                    u = u.clone()\n+                    v = v.clone()\n+\n+        sigma = torch.dot(u, torch.mv(weight_mat, v))\n         weight = weight / sigma\n         return weight\n \n     def remove(self, module):\n-        weight = getattr(module, self.name)\n+        with torch.no_grad():\n+            weight = self.compute_weight(module, do_power_iteration=False)\n         delattr(module, self.name)\n         delattr(module, self.name + '_u')\n+        delattr(module, self.name + '_v')\n         delattr(module, self.name + '_orig')\n-        module.register_parameter(self.name, torch.nn.Parameter(weight))\n+        module.register_parameter(self.name, torch.nn.Parameter(weight.detach()))\n \n     def __call__(self, module, inputs):\n-        if module.training:\n-            weight = self.compute_weight_and_update_u(module)\n-            setattr(module, self.name, weight)\n-        else:\n-            r_g = getattr(module, self.name + '_orig').requires_grad\n-            weight = getattr(module, self.name).detach()\n-            # NB: Cannot detach weight in-place here because if this is used\n-            #     DataParallel, the buffers are broadcast using\n-            #     `broadacast_coalesced` and `weight` here is actually a view,\n-            #     and you can't detach views in-place.\n-            setattr(module, self.name, weight.requires_grad_(r_g))\n+        setattr(module, self.name, self.compute_weight(module, do_power_iteration=module.training))\n+\n+    def _solve_v_and_rescale(self, weight_mat, u, target_sigma):\n+        # Tries to returns a vector `v` s.t. `u = normalize(W @ v)`\n+        # (the invariant at top of this class) and `u @ W @ v = sigma`.\n+        # This uses pinverse in case W^T W is not invertible.\n+        v = torch.chain_matmul(weight_mat.t().mm(weight_mat).pinverse(), weight_mat.t(), u.unsqueeze(1)).squeeze(1)\n+        return v.mul_(target_sigma / torch.dot(u, torch.mv(weight_mat, v)))\n \n     @staticmethod\n     def apply(module, name, n_power_iterations, dim, eps):\n+        for k, hook in module._forward_pre_hooks.items():\n+            if isinstance(hook, SpectralNorm) and hook.name == name:\n+                raise RuntimeError(\"Cannot register two spectral_norm hooks on \"", "path": "torch/nn/utils/spectral_norm.py", "position": 148, "original_position": 147, "commit_id": "63a22fb52251a24a16f5e07d3633b625a5e87ad3", "original_commit_id": "e36b925ed8531791c8bb1a192d015a49bf514066", "user": {"login": "crcrpar", "id": 16191443, "node_id": "MDQ6VXNlcjE2MTkxNDQz", "avatar_url": "https://avatars2.githubusercontent.com/u/16191443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crcrpar", "html_url": "https://github.com/crcrpar", "followers_url": "https://api.github.com/users/crcrpar/followers", "following_url": "https://api.github.com/users/crcrpar/following{/other_user}", "gists_url": "https://api.github.com/users/crcrpar/gists{/gist_id}", "starred_url": "https://api.github.com/users/crcrpar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crcrpar/subscriptions", "organizations_url": "https://api.github.com/users/crcrpar/orgs", "repos_url": "https://api.github.com/users/crcrpar/repos", "events_url": "https://api.github.com/users/crcrpar/events{/privacy}", "received_events_url": "https://api.github.com/users/crcrpar/received_events", "type": "User", "site_admin": false}, "body": "Thanks for adding this.", "created_at": "2018-11-01T00:08:07Z", "updated_at": "2018-11-23T15:53:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/13350#discussion_r229906489", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13350", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229906489"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13350#discussion_r229906489"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13350"}}, "body_html": "<p>Thanks for adding this.</p>", "body_text": "Thanks for adding this."}