{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386713658", "html_url": "https://github.com/pytorch/pytorch/issues/7271#issuecomment-386713658", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7271", "id": 386713658, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjcxMzY1OA==", "user": {"login": "jrwalsh1", "id": 9166900, "node_id": "MDQ6VXNlcjkxNjY5MDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9166900?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrwalsh1", "html_url": "https://github.com/jrwalsh1", "followers_url": "https://api.github.com/users/jrwalsh1/followers", "following_url": "https://api.github.com/users/jrwalsh1/following{/other_user}", "gists_url": "https://api.github.com/users/jrwalsh1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrwalsh1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrwalsh1/subscriptions", "organizations_url": "https://api.github.com/users/jrwalsh1/orgs", "repos_url": "https://api.github.com/users/jrwalsh1/repos", "events_url": "https://api.github.com/users/jrwalsh1/events{/privacy}", "received_events_url": "https://api.github.com/users/jrwalsh1/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-04T19:48:34Z", "updated_at": "2018-05-04T19:48:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I believe I understand more thoroughly what is going on:</p>\n<ul>\n<li>The <code>numpy</code> array coming from reading the <code>HDFStore</code> does not have contiguous memory.</li>\n<li>The strides are not multiples of the byte size of single elements, which I think leads to <code>PyTorch</code> reading the wrong memory block <em>beyond the first row</em>.</li>\n</ul>\n<p>Here is what I get when printing <code>.array.flags</code> and <code>.strides</code> on the <code>numpy</code> arrays (using <code>float32</code> arrays):</p>\n<p><code>x</code> (direct array creation):</p>\n<pre><code>  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n\nstrides: (12, 4)\n</code></pre>\n<p><code>y</code> (the loaded array):</p>\n<pre><code>  C_CONTIGUOUS : False\n  F_CONTIGUOUS : False\n  OWNDATA : False\n  WRITEABLE : True\n  ALIGNED : False\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n\nstrides: (13, 4)\n</code></pre>\n<p>You can see the loaded array is neither <code>C_CONTIGUOUS</code> nor <code>F_CONTIGUOUS</code>, and it's just a reference (presumably to the <code>pandas.DataFrame</code> loaded from the store with <code>PyTables</code>).</p>\n<p>In <a href=\"https://github.com/pytorch/pytorch/blob/db53389761fa87af09358bc1afa2157be0468130/torch/csrc/utils/tensor_numpy.cpp#L99\">from_numpy</a>, it looks like there's an implicit assumption that the memory is contiguous and in steps that are a multiple of the byte size of single elements.  I assume that making a copy of the array in the <code>FloatTensor</code> constructor is needed.</p>", "body_text": "I believe I understand more thoroughly what is going on:\n\nThe numpy array coming from reading the HDFStore does not have contiguous memory.\nThe strides are not multiples of the byte size of single elements, which I think leads to PyTorch reading the wrong memory block beyond the first row.\n\nHere is what I get when printing .array.flags and .strides on the numpy arrays (using float32 arrays):\nx (direct array creation):\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n\nstrides: (12, 4)\n\ny (the loaded array):\n  C_CONTIGUOUS : False\n  F_CONTIGUOUS : False\n  OWNDATA : False\n  WRITEABLE : True\n  ALIGNED : False\n  WRITEBACKIFCOPY : False\n  UPDATEIFCOPY : False\n\nstrides: (13, 4)\n\nYou can see the loaded array is neither C_CONTIGUOUS nor F_CONTIGUOUS, and it's just a reference (presumably to the pandas.DataFrame loaded from the store with PyTables).\nIn from_numpy, it looks like there's an implicit assumption that the memory is contiguous and in steps that are a multiple of the byte size of single elements.  I assume that making a copy of the array in the FloatTensor constructor is needed.", "body": "I believe I understand more thoroughly what is going on:  \r\n- The `numpy` array coming from reading the `HDFStore` does not have contiguous memory.\r\n- The strides are not multiples of the byte size of single elements, which I think leads to `PyTorch` reading the wrong memory block *beyond the first row*.\r\n\r\nHere is what I get when printing `.array.flags` and `.strides` on the `numpy` arrays (using `float32` arrays):\r\n\r\n`x` (direct array creation):\r\n```\r\n  C_CONTIGUOUS : True\r\n  F_CONTIGUOUS : False\r\n  OWNDATA : True\r\n  WRITEABLE : True\r\n  ALIGNED : True\r\n  WRITEBACKIFCOPY : False\r\n  UPDATEIFCOPY : False\r\n\r\nstrides: (12, 4)\r\n```\r\n\r\n`y` (the loaded array):\r\n```\r\n  C_CONTIGUOUS : False\r\n  F_CONTIGUOUS : False\r\n  OWNDATA : False\r\n  WRITEABLE : True\r\n  ALIGNED : False\r\n  WRITEBACKIFCOPY : False\r\n  UPDATEIFCOPY : False\r\n\r\nstrides: (13, 4)\r\n```\r\nYou can see the loaded array is neither `C_CONTIGUOUS` nor `F_CONTIGUOUS`, and it's just a reference (presumably to the `pandas.DataFrame` loaded from the store with `PyTables`).\r\n\r\nIn [from_numpy](https://github.com/pytorch/pytorch/blob/db53389761fa87af09358bc1afa2157be0468130/torch/csrc/utils/tensor_numpy.cpp#L99), it looks like there's an implicit assumption that the memory is contiguous and in steps that are a multiple of the byte size of single elements.  I assume that making a copy of the array in the `FloatTensor` constructor is needed."}