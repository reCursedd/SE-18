{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13728", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13728/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13728/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13728/events", "html_url": "https://github.com/pytorch/pytorch/issues/13728", "id": 378860230, "node_id": "MDU6SXNzdWUzNzg4NjAyMzA=", "number": 13728, "title": "RuntimeError: param_from.type() == param_to.type() ASSERT FAILED", "user": {"login": "ZhengDeQuan", "id": 24805539, "node_id": "MDQ6VXNlcjI0ODA1NTM5", "avatar_url": "https://avatars1.githubusercontent.com/u/24805539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhengDeQuan", "html_url": "https://github.com/ZhengDeQuan", "followers_url": "https://api.github.com/users/ZhengDeQuan/followers", "following_url": "https://api.github.com/users/ZhengDeQuan/following{/other_user}", "gists_url": "https://api.github.com/users/ZhengDeQuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhengDeQuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhengDeQuan/subscriptions", "organizations_url": "https://api.github.com/users/ZhengDeQuan/orgs", "repos_url": "https://api.github.com/users/ZhengDeQuan/repos", "events_url": "https://api.github.com/users/ZhengDeQuan/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhengDeQuan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-08T18:26:14Z", "updated_at": "2018-11-16T19:54:19Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>RuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch</p>\n\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<p>1.i am using pytorch 0.4.1 with anaconda3<br>\n1.run my code I got the following error:<br>\nTraceback (most recent call last):<br>\nFile \"rc_model.py\", line 849, in <br>\ntrainIters()<br>\nFile \"rc_model.py\", line 811, in trainIters<br>\nloss = train(flowQA_model , OtherInpus , start_label , end_label , criterion , flowQA_optimizer)<br>\nFile \"rc_model.py\", line 737, in train<br>\nP_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(torch.LongTensor(Q) ,torch.LongTensor(C) ,LongTensor(Q_len) ,LongTensor(Sess_len) ,LongTensor (C_len))<br>\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"rc_model.py\", line 701, in forward<br>\nC_one = self.IF1(C_zero , C_zero_len , Sess_C_len) ##[batch , sess_len , con_len , hidden * 2 + 1]<br>\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"rc_model.py\", line 376, in forward<br>\nC_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden * 2]<br>\nFile \"rc_model.py\", line 347, in Context_Integration<br>\nC_h_1, (h_n, c_n) = self.lstm(input = C_h_1)<br>\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\", line 192, in forward<br>\noutput, hidden = func(input, self.all_weights, hx, batch_sizes)<br>\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/_functions/rnn.py\", line 324, in forward<br>\nreturn func(input, *fargs, **fkwargs)<br>\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/_functions/rnn.py\", line 288, in forward<br>\ndropout_ts)<br>\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch</p>\n<ol>\n<li></li>\n</ol>\n\n<h2>Expected behavior</h2>\n\n<h2>Environment</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch Version (e.g., 1.0):</li>\n<li>OS (e.g., Linux):</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source):</li>\n<li>Build command you used (if compiling from source):</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:CUDA Version 9.0.176<br>\n#define CUDNN_MAJOR 7<br>\n#define CUDNN_MINOR 0<br>\n#define CUDNN_PATCHLEVEL 3<br>\n--<br>\n#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)</li>\n</ul>\n<p>#include \"driver_types.h\"</p>\n<ul>\n<li>GPU models and configuration:</li>\n<li>Any other relevant information:</li>\n</ul>\n<h2>Additional context</h2>\n<p>my all code is as follows</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> -*- coding:utf-8 -*-</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span>\"</span>\n<span class=\"pl-s\">seq2seq_translation_tutorial.py -&gt; seq2seq_add_batch_dimension.py \u5c06\u539f\u6765\u7684\u4ee3\u7801\u4e2dbatch\u7ef4\u5ea6\u4ece1\uff0c\u589e\u52a0\u5230n&gt;1,\u540c\u65f6\u9047\u5230\u7684\u9ebb\u70e6\uff0cpadding\u4e0d\u540c\u957f\u5ea6\u5230\u540c\u610f\u957f\u5ea6\uff0cnn.GRU\u4e2d\u6709\u4e24\u4e2a\u548cpad_pack\u76f8\u5173\u7684\u51fd\u6570</span>\n<span class=\"pl-s\">            embedded_input = torch.nn.utils.rnn.pack_padded_sequence(embedded_input,input_length,batch_first=True)#lengths' array has to be sorted in decreasing order</span>\n<span class=\"pl-s\">            \u653e\u5165RNN\u4e4b\u524d\u5bf9\u6570\u636e\u8fdb\u884c\u5c01\u88c5\uff0c</span>\n<span class=\"pl-s\">            output , _ = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)</span>\n<span class=\"pl-s\">            \u5bf9rnn\u7684\u8f93\u51fa\u8fdb\u884c\u89e3\u5c01\u88c5\u3002</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">seq2seq_add_batch_dimension.py -&gt; seq2seq_add_hierarchy_rnn_encoder.py \u589e\u52a0\u4e86\u5c42\u7ea7rnn\u7684\u529f\u80fd\u5bf9dialogue\u53ef\u4ee5\u7f16\u7801 \u300c\u300d \u8fd9\u6b21\u5148\u4ece\u6570\u636e\u5f00\u59cb\u641e</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">\"\"\"</span>\n<span class=\"pl-s\">from __future__ import unicode_literals, print_function, division</span>\n<span class=\"pl-s\">import os</span>\n<span class=\"pl-s\">import torch</span>\n<span class=\"pl-s\">import torch.nn as nn</span>\n<span class=\"pl-s\">import numpy as np</span>\n<span class=\"pl-s\">import torch.nn.functional as F</span>\n<span class=\"pl-s\">from torch import optim</span>\n<span class=\"pl-s\">import time</span>\n<span class=\"pl-s\">import math</span>\n<span class=\"pl-s\">from getData import getData , getBatchData</span>\n<span class=\"pl-s\">import matplotlib.pyplot as plt</span>\n<span class=\"pl-s\">plt.switch_backend('agg')</span>\n<span class=\"pl-s\">import matplotlib.ticker as ticker</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">USE_CUDA = torch.cuda.is_available()</span>\n<span class=\"pl-s\">if USE_CUDA:</span>\n<span class=\"pl-s\">    device = torch.device(\"cuda\")</span>\n<span class=\"pl-s\">else:</span>\n<span class=\"pl-s\">    device = torch.device(\"cpu\")</span>\n<span class=\"pl-s\">print(\"device = \",device)</span>\n<span class=\"pl-s\">gpus = [0]</span>\n<span class=\"pl-s\"># torch.cuda.set_device(gpus[0])</span>\n<span class=\"pl-s\">FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor</span>\n<span class=\"pl-s\">LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor</span>\n<span class=\"pl-s\">ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def asMinutes(seconds):</span>\n<span class=\"pl-s\">    minutes = math.floor(seconds/60)</span>\n<span class=\"pl-s\">    seconds -= minutes * 60</span>\n<span class=\"pl-s\">    return '%d minutes %d seconds'%(minutes , seconds)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def timeSince(since, percent):</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    :param percent <span class=\"pl-k\">=</span> iter_right_now <span class=\"pl-k\">/</span> total_iter\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    now = time.time()</span>\n<span class=\"pl-s\">    seconds = now - since</span>\n<span class=\"pl-s\">    estimate_seconds = seconds / (percent)</span>\n<span class=\"pl-s\">    remain_seconds = estimate_seconds - seconds</span>\n<span class=\"pl-s\">    return '%s (-%s)' %(asMinutes(seconds) , asMinutes())</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def showPlot(points):</span>\n<span class=\"pl-s\">    plt.figure()</span>\n<span class=\"pl-s\">    fig, ax = plt.subplots()</span>\n<span class=\"pl-s\">    # this locator puts ticks at regular intervals</span>\n<span class=\"pl-s\">    loc = ticker.MultipleLocator(base=0.2)</span>\n<span class=\"pl-s\">    ax.yaxis.set_major_locator(loc)</span>\n<span class=\"pl-s\">    plt.plot(points)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">PAD_token = 0</span>\n<span class=\"pl-s\">SOS_token = 1</span>\n<span class=\"pl-s\">EOS_token = 2</span>\n<span class=\"pl-s\">MAX_LENGTH = 10</span>\n<span class=\"pl-s\">DIR_TO_SAVE = \"/data1/zhengquan/data/CoQA/saved_models\"</span>\n<span class=\"pl-s\">if not os.path.exists(DIR_TO_SAVE):</span>\n<span class=\"pl-s\">    os.makedirs(DIR_TO_SAVE)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class Embed(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self,hidden_size = 100,pad_token_index = 0):</span>\n<span class=\"pl-s\">        super(Embed,self).__init__()</span>\n<span class=\"pl-s\">        self.hidden_size = hidden_size</span>\n<span class=\"pl-s\">        self.embedding = nn.Embedding(</span>\n<span class=\"pl-s\">            num_embeddings=1000,</span>\n<span class=\"pl-s\">            embedding_dim=self.hidden_size,</span>\n<span class=\"pl-s\">            padding_idx=pad_token_index</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , input):</span>\n<span class=\"pl-s\">        return self.embedding(input)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class DotAtt(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):</span>\n<span class=\"pl-s\">        super(DotAtt, self).__init__()</span>\n<span class=\"pl-s\">        self.in_hidden_size = in_hidden_size</span>\n<span class=\"pl-s\">        self.out_hidden_size = out_hidden_size</span>\n<span class=\"pl-s\">        self.W = nn.Linear(self.in_hidden_size, self.out_hidden_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , qi, c , qi_length , c_length):</span>\n<span class=\"pl-s\">        #qi [batch , seq_len , in_hidden]</span>\n<span class=\"pl-s\">        #c [batch , con_len , in_hidden]</span>\n<span class=\"pl-s\">        #qi_length = [batch]</span>\n<span class=\"pl-s\">        #c_length = [batch]</span>\n<span class=\"pl-s\">        batch_size , seq_len  = qi.size()[:2]</span>\n<span class=\"pl-s\">        con_len = c.size(1)</span>\n<span class=\"pl-s\">        mask_qi = torch.ones(batch_size , seq_len)</span>\n<span class=\"pl-s\">        mask_c = torch.ones(batch_size , con_len)</span>\n<span class=\"pl-s\">        for index , (qi_l , ci_l) in range(zip(qi_length , c_length)):</span>\n<span class=\"pl-s\">            mask_qi[index , :qi_l] = 0</span>\n<span class=\"pl-s\">            mask_c[index , :ci_l] = 0</span>\n<span class=\"pl-s\">        mask_c *= (-1e10)</span>\n<span class=\"pl-s\">        mask_qi *= -1e10</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        mask_c = mask_c.view(1,batch_size,con_len).repeat(seq_len,1,1)</span>\n<span class=\"pl-s\">        mask_qi = mask_qi.view(1,batch_size ,seq_len).repeat(con_len,1,1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        qi = F.relu(self.W(qi))</span>\n<span class=\"pl-s\">        c = F.relu(self.W(c))</span>\n<span class=\"pl-s\">        attn = torch.matmul(c ,qi.transpose(-2,-1)) #[batch , con_len , seq_len]</span>\n<span class=\"pl-s\">        attn = attn.permute(1,0,2) #[con_len , batch , seq_len]</span>\n<span class=\"pl-s\">        attn = attn + mask_qi</span>\n<span class=\"pl-s\">        attn = attn.permute(1,0,2).permute(2,0,1) #[seq_len , batch , con_len]</span>\n<span class=\"pl-s\">        attn = attn + mask_c</span>\n<span class=\"pl-s\">        attn = attn.permute(1,2,0)#[batch , con_len , seq_len]</span>\n<span class=\"pl-s\">        attn = F.softmax(attn ,dim = -1)</span>\n<span class=\"pl-s\">        qi_hat = torch.matmul(attn , qi) #[batch , con_len , out_hidden]</span>\n<span class=\"pl-s\">        return qi_hat</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class DotAttHier(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):</span>\n<span class=\"pl-s\">        super(DotAttHier , self).__init__()</span>\n<span class=\"pl-s\">        self.in_hidden_size = in_hidden_size</span>\n<span class=\"pl-s\">        self.out_hidden_size = out_hidden_size</span>\n<span class=\"pl-s\">        self.dotatt = DotAtt(in_hidden_size , out_hidden_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self, Q , C , Q_len , C_len):</span>\n<span class=\"pl-s\">        #Q[batch , sess_len , seq_len , in_hidden]</span>\n<span class=\"pl-s\">        #C[batch , con_len , in_hidden]</span>\n<span class=\"pl-s\">        #Q_len[batch , sess_len]</span>\n<span class=\"pl-s\">        #C_len[batch]</span>\n<span class=\"pl-s\">        Q = Q.permute(1,0,2,3) #[sess_len , batch , seq_len , in_hidden]</span>\n<span class=\"pl-s\">        Q_len = Q_len.permute(1,0)#[sess_len , batch]</span>\n<span class=\"pl-s\">        sess_len ,batch_size , seq_len , in_hidden = Q.size()</span>\n<span class=\"pl-s\">        con_len = C.size(1)</span>\n<span class=\"pl-s\">        Q_hat = []</span>\n<span class=\"pl-s\">        for index in range(sess_len):</span>\n<span class=\"pl-s\">            Q_hat.append(self.dotatt(Q[index],C , Q_len[index] , C_len).view(1 , con_len , seq_len , -1))</span>\n<span class=\"pl-s\">            # [1 , batch , con_len , out_hidden]</span>\n<span class=\"pl-s\">        Q_hat = torch.cat(Q_hat , dim = 0).permute(1,0,2,3) #[batch ,sess_len , con_len , in_hidden]</span>\n<span class=\"pl-s\">        return Q_hat</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class DotAttHier2(nn.Module):</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\u76f4\u63a5\u8fdb\u884cHierattn\uff0c\u4e0d\u4f1a\u5728sess_len\u7ef4\u5ea6\u4e0a\u5faa\u73af<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):</span>\n<span class=\"pl-s\">        super(DotAttHier2 , self).__init__()</span>\n<span class=\"pl-s\">        self.in_hidden_size = in_hidden_size</span>\n<span class=\"pl-s\">        self.out_hidden_size = out_hidden_size</span>\n<span class=\"pl-s\">        self.W = nn.Linear(self.in_hidden_size, self.out_hidden_size).to(device)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , Q , C , Q_mask , C_mask):</span>\n<span class=\"pl-s\">        #Q_len[batch , sess_len]</span>\n<span class=\"pl-s\">        batch_size , sess_len , seq_len , in_hidden = Q.size()</span>\n<span class=\"pl-s\">        con_len = C.size(1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Q = Q.view(batch_size * sess_len , seq_len , in_hidden)</span>\n<span class=\"pl-s\">        C = C.repeat(sess_len , 1, 1)</span>\n<span class=\"pl-s\">        Q = F.relu(self.W(Q))</span>\n<span class=\"pl-s\">        C = F.relu(self.W(C))</span>\n<span class=\"pl-s\">        attn = torch.matmul(C, Q.transpose(-2, -1))  # [batch * sess_len , con_len , seq_len]</span>\n<span class=\"pl-s\">        Q_mask = Q_mask * (-1e10) #[batch , sess_len , seq_len]</span>\n<span class=\"pl-s\">        Q_mask = Q_mask.view(-1 , seq_len).unsqueeze(1).repeat(1 ,con_len , 1) #[batch * sess_len , con_len , seq_len]</span>\n<span class=\"pl-s\">        C_mask = C_mask * (-1e10) #[batch , con_len]</span>\n<span class=\"pl-s\">        C_mask = C_mask.unsqueeze(-1).repeat(sess_len , 1 , seq_len) #[batch * sess_len , con_len , seq_len]</span>\n<span class=\"pl-s\">        attn = attn + C_mask + Q_mask</span>\n<span class=\"pl-s\">        attn = F.softmax(attn, dim=-1) #[batch * sess_len , con_len , seq_len(\u5f52\u4e00\u5316)]</span>\n<span class=\"pl-s\">        Q_hat = torch.matmul(attn , Q) #[batch * sess_len , con_len , in_hidden]</span>\n<span class=\"pl-s\">        Q_hat = Q_hat.view(batch_size , sess_len , con_len , in_hidden)</span>\n<span class=\"pl-s\">        return Q_hat</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class NaiveLSTM(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , input_size , hidden_size , batch_first = True,</span>\n<span class=\"pl-s\">                 num_layer = 1, num_dir = 2,</span>\n<span class=\"pl-s\">                 batch_size = 10 , dropout_p = 0.5,</span>\n<span class=\"pl-s\">                 max_length = MAX_LENGTH):</span>\n<span class=\"pl-s\">        super(NaiveLSTM,self).__init__()</span>\n<span class=\"pl-s\">        self.input_size = input_size</span>\n<span class=\"pl-s\">        self.hidden_size = hidden_size</span>\n<span class=\"pl-s\">        self.batch_first = batch_first</span>\n<span class=\"pl-s\">        self.num_layer = num_layer</span>\n<span class=\"pl-s\">        self.num_dir = num_dir</span>\n<span class=\"pl-s\">        self.batch_size = batch_size</span>\n<span class=\"pl-s\">        self.dropout_p = dropout_p</span>\n<span class=\"pl-s\">        self.lstm  = nn.LSTM(</span>\n<span class=\"pl-s\">            input_size=input_size,</span>\n<span class=\"pl-s\">            hidden_size=hidden_size,</span>\n<span class=\"pl-s\">            num_layers=num_layer,</span>\n<span class=\"pl-s\">            bidirectional=num_dir &gt; 1,</span>\n<span class=\"pl-s\">            batch_first=batch_first,</span>\n<span class=\"pl-s\">            dropout=dropout_p</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self, input , input_length):</span>\n<span class=\"pl-s\">        #input = [batch * sess_len , seq_len , hidden_size]</span>\n<span class=\"pl-s\">        #input_length = [batch * sess_len]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-input_length)</span>\n<span class=\"pl-s\">        input = input[sorted_idx]</span>\n<span class=\"pl-s\">        input_length = input_length[sorted_idx]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        input = torch.nn.utils.rnn.pack_padded_sequence(input, input_length, batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        out, (h_n, c_n) = self.lstm(input=input)</span>\n<span class=\"pl-s\">        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        out = out[unsorted_idx]  # [batch_size * sess_len, seq_len , num_dir * hidden_size]</span>\n<span class=\"pl-s\">        return out</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class HierEncoderLSTM(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self ,input_size, hidden_size , batch_first = True,</span>\n<span class=\"pl-s\">                 num_layer = 2 , num_dir = 2,</span>\n<span class=\"pl-s\">                 batch_size = 10 , dropout_p = 0.5,</span>\n<span class=\"pl-s\">                 max_length = MAX_LENGTH):</span>\n<span class=\"pl-s\">        super(HierEncoderLSTM,self).__init__()</span>\n<span class=\"pl-s\">        self.input_size = input_size</span>\n<span class=\"pl-s\">        self.hidden_size = hidden_size</span>\n<span class=\"pl-s\">        self.batch_first = batch_first</span>\n<span class=\"pl-s\">        self.num_layer = num_layer</span>\n<span class=\"pl-s\">        self.num_dir = num_dir</span>\n<span class=\"pl-s\">        self.batch_size = batch_size</span>\n<span class=\"pl-s\">        self.dropout_p = dropout_p</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.lstm = nn.LSTM(</span>\n<span class=\"pl-s\">            input_size = input_size,</span>\n<span class=\"pl-s\">            hidden_size = hidden_size,</span>\n<span class=\"pl-s\">            num_layers = num_layer,</span>\n<span class=\"pl-s\">            bidirectional= num_dir &gt; 1,</span>\n<span class=\"pl-s\">            batch_first = batch_first,</span>\n<span class=\"pl-s\">            dropout = dropout_p</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.w  = nn.Linear(</span>\n<span class=\"pl-s\">            num_dir * hidden_size,</span>\n<span class=\"pl-s\">            1</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.high_lstm = nn.LSTM(</span>\n<span class=\"pl-s\">            input_size = hidden_size * num_dir,</span>\n<span class=\"pl-s\">            hidden_size = hidden_size * num_dir,</span>\n<span class=\"pl-s\">            num_layers = 1,</span>\n<span class=\"pl-s\">            batch_first = batch_first,</span>\n<span class=\"pl-s\">            dropout = dropout_p</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , Q , Q_len , Sess_len , Q_mask0):</span>\n<span class=\"pl-s\">        batch_size , sess_len , seq_len , input_size = Q.size()</span>\n<span class=\"pl-s\">        #Q_len = [batch , sess_len]</span>\n<span class=\"pl-s\">        #Q_mask0 = [batch , sess_len , seq_len]</span>\n<span class=\"pl-s\">        Q = Q.view(-1 , seq_len , input_size)</span>\n<span class=\"pl-s\">        Q_len = Q_len.view(-1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-Q_len)</span>\n<span class=\"pl-s\">        out = Q[sorted_idx]</span>\n<span class=\"pl-s\">        out_len = Q_len[sorted_idx]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        out = torch.nn.utils.rnn.pack_padded_sequence(out,out_len , batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        out, (h_n, c_n) = self.lstm(input=out)</span>\n<span class=\"pl-s\">        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        out = out[unsorted_idx] #[batch_size * sess_len, seq_len , self.num_dir * self.hidden_size]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        attn = self.w(out) #[batch * sess_len , seq_len , 1]</span>\n<span class=\"pl-s\">        Q_mask0 = Q_mask0 * (-1e10)</span>\n<span class=\"pl-s\">        Q_mask0 = Q_mask0.view(-1,seq_len).unsqueeze(-1) #[batch * sess_len , seq_len , 1]</span>\n<span class=\"pl-s\">        attn = (attn + Q_mask0).transpose(-1,-2) #[batch * sess_len , 1 , seq_len]</span>\n<span class=\"pl-s\">        high_out = torch.matmul(attn , out) #[batch * sess_len  , 1 , num_dir * hidden]</span>\n<span class=\"pl-s\">        high_out = high_out.squeeze(1).view(batch_size , sess_len , -1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-Sess_len)</span>\n<span class=\"pl-s\">        high_out = high_out[sorted_idx]</span>\n<span class=\"pl-s\">        Sess_len = Sess_len[sorted_idx]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        high_out = torch.nn.utils.rnn.pack_padded_sequence(high_out , Sess_len, batch_first = self.batch_first)</span>\n<span class=\"pl-s\">        high_out , (h_n , c_n) = self.high_lstm(input = high_out)</span>\n<span class=\"pl-s\">        high_out , _ = torch.nn.utils.rnn.pad_packed_sequence(high_out , batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        high_out = high_out[unsorted_idx] #[batch , sess_len , num_dir * hidden]</span>\n<span class=\"pl-s\">        return out , high_out</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def get_mask(input_length , max_seq_len , mask_value = 0):</span>\n<span class=\"pl-s\">    #[batch ] or [batch , sess_len]</span>\n<span class=\"pl-s\">    #mask_value = 0 ,\u6709\u503c\u7684\u5730\u65b9\u4e3a0 \uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a1</span>\n<span class=\"pl-s\">    #mask_value = 1 ,\u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0</span>\n<span class=\"pl-s\">    #for situation 1</span>\n<span class=\"pl-s\">    siz = []</span>\n<span class=\"pl-s\">    for i in range(len(input_length.size())):</span>\n<span class=\"pl-s\">        siz.append(input_length.size(i))</span>\n<span class=\"pl-s\">    siz.append(max_seq_len)</span>\n<span class=\"pl-s\">    if mask_value == 0:</span>\n<span class=\"pl-s\">        mask = torch.zeros(siz).to(device) #[batch, max_seq_len] or[batch , sess_len ,max_seq_len]</span>\n<span class=\"pl-s\">    if mask_value == 1:</span>\n<span class=\"pl-s\">        mask = torch.ones(siz).to(device)</span>\n<span class=\"pl-s\">    input_length = input_length.view(-1) #[batch] or [batch * sess_len]</span>\n<span class=\"pl-s\">    mask = mask.view(-1 , max_seq_len)</span>\n<span class=\"pl-s\">    for index ,l in enumerate(input_length):</span>\n<span class=\"pl-s\">        if mask_value == 0:</span>\n<span class=\"pl-s\">            mask[index , l:] = 1</span>\n<span class=\"pl-s\">        elif mask_value == 1:</span>\n<span class=\"pl-s\">            mask[index , l:] = 0</span>\n<span class=\"pl-s\">    mask = mask.view(siz)</span>\n<span class=\"pl-s\">    return mask</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class IF(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , in_hidden_size , hidden_size ,</span>\n<span class=\"pl-s\">                 num_dir =2 , batch_first = True,</span>\n<span class=\"pl-s\">                 dropout_p = 0.5 , num_layer = 1):</span>\n<span class=\"pl-s\">        super(IF,self).__init__()</span>\n<span class=\"pl-s\">        # print(\"in_IF_Class\")</span>\n<span class=\"pl-s\">        # print(\"in_hidden_size= \",in_hidden_size)</span>\n<span class=\"pl-s\">        # print(\"hidden_size = \",hidden_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.in_hidden_size = in_hidden_size</span>\n<span class=\"pl-s\">        self.hidden_size = hidden_size</span>\n<span class=\"pl-s\">        self.batch_first = batch_first</span>\n<span class=\"pl-s\">        self.lstm = nn.LSTM(</span>\n<span class=\"pl-s\">            input_size=in_hidden_size,</span>\n<span class=\"pl-s\">            hidden_size=hidden_size//2,</span>\n<span class=\"pl-s\">            num_layers=num_layer,</span>\n<span class=\"pl-s\">            bidirectional=num_dir &gt; 1,</span>\n<span class=\"pl-s\">            batch_first=batch_first,</span>\n<span class=\"pl-s\">            dropout=dropout_p</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\">        try :</span>\n<span class=\"pl-s\">            assert in_hidden_size == hidden_size * 2 + 1</span>\n<span class=\"pl-s\">        except:</span>\n<span class=\"pl-s\">            print(\"in_hidden_size = \",in_hidden_size)</span>\n<span class=\"pl-s\">            print(\"hidden_size = \",hidden_size)</span>\n<span class=\"pl-s\">            exit(87909)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.gru = nn.GRU(</span>\n<span class=\"pl-s\">            input_size = hidden_size * num_dir,</span>\n<span class=\"pl-s\">            hidden_size = in_hidden_size - hidden_size // 2 * 2,</span>\n<span class=\"pl-s\">            batch_first= batch_first,</span>\n<span class=\"pl-s\">            dropout= dropout_p,</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def Context_Integration(self , C_h_1 , C_h_len):</span>\n<span class=\"pl-s\">        #C_h_1 [batch , sess_len , con_len , in_hidden]</span>\n<span class=\"pl-s\">        #C_h_len [batch , sess_len]</span>\n<span class=\"pl-s\">        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1.view(-1 , con_len , in_hidden_size)</span>\n<span class=\"pl-s\">        C_h_len = C_h_len.view(-1)</span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-C_h_len)</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1[sorted_idx]</span>\n<span class=\"pl-s\">        C_h_len = C_h_len[sorted_idx]</span>\n<span class=\"pl-s\">        print(\"C_h_len = \",C_h_len)</span>\n<span class=\"pl-s\">        C_h_1 = torch.nn.utils.rnn.pack_padded_sequence(C_h_1 , C_h_len , batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)</span>\n<span class=\"pl-s\">        C_h_1 , _  = torch.nn.utils.rnn.pad_packed_sequence(C_h_1,batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1[unsorted_idx]</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1.view(batch_size,sess_len,con_len,-1)</span>\n<span class=\"pl-s\">        return C_h_1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def Flow(self , C_h , Sess_C_len):</span>\n<span class=\"pl-s\">        #Sess_C_len [batch , con_len]</span>\n<span class=\"pl-s\">        batch_size , con_len , sess_len , hidden = C_h.size()</span>\n<span class=\"pl-s\">        C_h = C_h.view(-1,sess_len,hidden)</span>\n<span class=\"pl-s\">        Sess_C_len = Sess_C_len.view(-1)</span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-Sess_C_len)</span>\n<span class=\"pl-s\">        C_hat = C_h[sorted_idx]</span>\n<span class=\"pl-s\">        Sess_C_len = Sess_C_len[sorted_idx]</span>\n<span class=\"pl-s\">        C_hat = torch.nn.utils.rnn.pack_padded_sequence(C_hat, Sess_C_len, batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        C_hat , (h_n , c_n) = self.gru(input = C_hat)</span>\n<span class=\"pl-s\">        C_hat , _ = torch.nn.utils.rnn.pad_packed_sequence(C_hat , batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        C_hat = C_hat[unsorted_idx] #[batch * con_len , sess_len , hidden]</span>\n<span class=\"pl-s\">        C_h_plus_1 = torch.cat((C_h,C_hat) , dim = -1) #[batch * con_len , sess_len , hidden]</span>\n<span class=\"pl-s\">        C_h_plus_1 = C_h_plus_1.view(batch_size , con_len , sess_len , -1).permute(0,2,1,3)</span>\n<span class=\"pl-s\">        return C_h_plus_1 #[batch , sess_len , con_len , in_hidden_size]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , C_h_1 , C_h_len , Sess_C_len):</span>\n<span class=\"pl-s\">        #C_h_len [batch , sess_len]</span>\n<span class=\"pl-s\">        #Sess_C_len [batch , con_len]</span>\n<span class=\"pl-s\">        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()</span>\n<span class=\"pl-s\">        # C_h_1 = C_h_1.view( -1, con_len , in_hidden_size)</span>\n<span class=\"pl-s\">        C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden * 2]</span>\n<span class=\"pl-s\">        C_hat = C_hat.permute(0,2,1,3)#[batch , con_len , sess_len , 2 * hidden]</span>\n<span class=\"pl-s\">        C_h = self.Flow(C_hat , Sess_C_len) #[batch , sess_len , con_len , hidden * 2 + 1]</span>\n<span class=\"pl-s\">        return C_h</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class IF3(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , in_hidden_size , hidden_size ,</span>\n<span class=\"pl-s\">                 num_dir =2 , batch_first = True,</span>\n<span class=\"pl-s\">                 dropout_p = 0.5 , num_layer = 1):</span>\n<span class=\"pl-s\">        super(IF3,self).__init__()</span>\n<span class=\"pl-s\">        # print(\"in_IF3_Class\")</span>\n<span class=\"pl-s\">        # print(\"in_hidden_size= \",in_hidden_size)</span>\n<span class=\"pl-s\">        # print(\"hidden_size = \",hidden_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.in_hidden_size = in_hidden_size</span>\n<span class=\"pl-s\">        self.hidden_size = hidden_size</span>\n<span class=\"pl-s\">        self.batch_first = batch_first</span>\n<span class=\"pl-s\">        self.lstm = nn.LSTM(</span>\n<span class=\"pl-s\">            input_size=in_hidden_size,</span>\n<span class=\"pl-s\">            hidden_size=hidden_size//2,</span>\n<span class=\"pl-s\">            num_layers=num_layer,</span>\n<span class=\"pl-s\">            bidirectional=num_dir &gt; 1,</span>\n<span class=\"pl-s\">            batch_first=batch_first,</span>\n<span class=\"pl-s\">            dropout=dropout_p</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\">        try :</span>\n<span class=\"pl-s\">            assert in_hidden_size == hidden_size * 2 + 1 + hidden_size * 2</span>\n<span class=\"pl-s\">        except:</span>\n<span class=\"pl-s\">            print(\"in_hidden = \",in_hidden_size)</span>\n<span class=\"pl-s\">            print(\"hidden = \",hidden_size)</span>\n<span class=\"pl-s\">            exit(6789)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.gru = nn.GRU(</span>\n<span class=\"pl-s\">            input_size = hidden_size * num_dir,</span>\n<span class=\"pl-s\">            hidden_size = in_hidden_size - hidden_size // 2 * 2,</span>\n<span class=\"pl-s\">            batch_first= batch_first,</span>\n<span class=\"pl-s\">            dropout= dropout_p,</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def Context_Integration(self , C_h_1 , C_h_len):</span>\n<span class=\"pl-s\">        #C_h_1 [batch , sess_len , con_len , in_hidden]</span>\n<span class=\"pl-s\">        #C_h_len [batch , sess_len]</span>\n<span class=\"pl-s\">        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1.view(-1 , con_len , in_hidden_size)</span>\n<span class=\"pl-s\">        C_h_len = C_h_len.view(-1)</span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-C_h_len)</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1[sorted_idx]</span>\n<span class=\"pl-s\">        C_h_len = C_h_len[sorted_idx]</span>\n<span class=\"pl-s\">        C_h_1 = torch.nn.utils.rnn.pack_padded_sequence(C_h_1 , C_h_len , batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)</span>\n<span class=\"pl-s\">        C_h_1 , _  = torch.nn.utils.rnn.pad_packed_sequence(C_h_1,batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1[unsorted_idx]</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1.view(batch_size,sess_len,con_len,-1)</span>\n<span class=\"pl-s\">        return C_h_1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def Flow(self , C_h , Sess_C_len):</span>\n<span class=\"pl-s\">        #Sess_C_len [batch , con_len]</span>\n<span class=\"pl-s\">        batch_size , con_len , sess_len , hidden = C_h.size()</span>\n<span class=\"pl-s\">        C_h = C_h.view(-1,sess_len,hidden)</span>\n<span class=\"pl-s\">        Sess_C_len = Sess_C_len.view(-1)</span>\n<span class=\"pl-s\">        sorted_idx = np.argsort(-Sess_C_len)</span>\n<span class=\"pl-s\">        C_hat = C_h[sorted_idx]</span>\n<span class=\"pl-s\">        Sess_C_len = Sess_C_len[sorted_idx]</span>\n<span class=\"pl-s\">        C_hat = torch.nn.utils.rnn.pack_padded_sequence(C_hat, Sess_C_len, batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        C_hat , (h_n , c_n) = self.gru(input = C_hat)</span>\n<span class=\"pl-s\">        C_hat , _ = torch.nn.utils.rnn.pad_packed_sequence(C_hat , batch_first=self.batch_first)</span>\n<span class=\"pl-s\">        unsorted_idx = np.argsort(sorted_idx)</span>\n<span class=\"pl-s\">        C_hat = C_hat[unsorted_idx] #[batch * con_len , sess_len ,3 *  hidden + 1]</span>\n<span class=\"pl-s\">        C_h_plus_1 = torch.cat((C_h,C_hat) , dim = -1) #[batch * con_len , sess_len ,  4 * hidden + 1]</span>\n<span class=\"pl-s\">        C_h_plus_1 = C_h_plus_1.view(batch_size , con_len , sess_len , -1).permute(0,2,1,3)</span>\n<span class=\"pl-s\">        return C_h_plus_1 #[batch , sess_len , con_len , in_hidden_size]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , C_h_1 , C_h_len , Sess_C_len):</span>\n<span class=\"pl-s\">        #C_h_len [batch , sess_len]</span>\n<span class=\"pl-s\">        #Sess_C_len [batch , con_len]</span>\n<span class=\"pl-s\">        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()</span>\n<span class=\"pl-s\">        C_h_1 = C_h_1.view( -1, con_len , in_hidden_size)</span>\n<span class=\"pl-s\">        C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden]</span>\n<span class=\"pl-s\">        C_hat = C_hat.permute(0,2,1,3)#[batch , con_len , sess_len , hidden]</span>\n<span class=\"pl-s\">        C_h = self.Flow(C_hat , Sess_C_len) #[batch , sess_len , con_len , hidden * 2 + 1]</span>\n<span class=\"pl-s\">        return C_h</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class FullyAwareAttn(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , hidden_size , x_hidden_size , y_hidden_size):</span>\n<span class=\"pl-s\">        super(FullyAwareAttn , self).__init__()</span>\n<span class=\"pl-s\">        self.hidden_size = hidden_size</span>\n<span class=\"pl-s\">        self.U_x = nn.Linear(in_features=x_hidden_size,out_features=hidden_size)</span>\n<span class=\"pl-s\">        self.U_y = nn.Linear(in_features=y_hidden_size,out_features=hidden_size)</span>\n<span class=\"pl-s\">        self.D = torch.diag(torch.rand(hidden_size)) #[hidden,hidden]\u7684\u5bf9\u89d2\u9635,torch.rand(10),size\u662f10\u7684\u5411\u91cf, \u30100\uff0c1)\u4e4b\u95f4\u7684\u5747\u5300\u5206\u5e03</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def S(self ,C ,Q):</span>\n<span class=\"pl-s\">        #C [batch , sess_len , con_len , hidden]</span>\n<span class=\"pl-s\">        #Q [batch , sess_len , seq_len , hidden]</span>\n<span class=\"pl-s\">        #a[i,j,k] = C[j][i] * Q[j][k] \u7b2cj\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u4e2d,C\u4e2d\u7684\u7b2ci\u4e2a\u5355\u8bcd\u4e0eQ\u4e2d\u7684\u7b2ck\u4e2a\u5355\u8bcd\u8ba1\u7b97\u7684\u76f8\u4f3c\u5ea6</span>\n<span class=\"pl-s\">        #a = torch.matmul(C , Q.transpose(-2 , -1)) #[batch , sess_len , con_len , seq_len] \u662f\u8fd9\u4e2a\u610f\u601d\uff0c\u4f46\u662f\u4e0d\u662f\u7b80\u5355\u7684\u70b9\u4e58\u7b97\u76f8\u4f3c\u5ea6</span>\n<span class=\"pl-s\">        C = F.relu(self.U_x(C))</span>\n<span class=\"pl-s\">        Q = F.relu(self.U_y(Q))</span>\n<span class=\"pl-s\">        temp = torch.matmul(C,self.D)</span>\n<span class=\"pl-s\">        a = torch.matmul(temp , Q.transpose(-2,-1)) #[batch , sess_len , con_len , seq_len]</span>\n<span class=\"pl-s\">        return a</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self , C , Q , C_mask0 , Q_mask0):</span>\n<span class=\"pl-s\">        # C = ( C_zero, C_one, C_two )</span>\n<span class=\"pl-s\">        # Q = ( Q_naive , Q_lstm1 , Q_lstm2)</span>\n<span class=\"pl-s\">        C_zero, C_one, C_two = C</span>\n<span class=\"pl-s\">        embed_Q, Q_lstm_1, Q_lstm_2 = Q</span>\n<span class=\"pl-s\">        Q = torch.cat((embed_Q, Q_lstm_1, Q_lstm_2),dim = -1)</span>\n<span class=\"pl-s\">        C = torch.cat((C_zero, C_one, C_two), dim=-1)</span>\n<span class=\"pl-s\">        batch_size , sess_len , con_len , hidden_size = C.size()</span>\n<span class=\"pl-s\">        seq_len = Q.size(2)</span>\n<span class=\"pl-s\">        #C_mask0 [batch , con_len]</span>\n<span class=\"pl-s\">        #Q_mask0 [batch , sess_len , seq_len]</span>\n<span class=\"pl-s\">        a = self.S(C,Q) #[batch , sess_len , con_len , seq_len]</span>\n<span class=\"pl-s\">        C_mask0 = C_mask0 * (-1e10)</span>\n<span class=\"pl-s\">        Q_mask0 = Q_mask0 * (-1e10)</span>\n<span class=\"pl-s\">        C_mask0 = C_mask0.unsqueeze(1).unsqueeze(-1).repeat(1 , sess_len , 1 , seq_len)</span>\n<span class=\"pl-s\">        Q_mask0 = Q_mask0.unsqueeze(-2).repeat(1,1,con_len,1)</span>\n<span class=\"pl-s\">        a = a + Q_mask0 + C_mask0</span>\n<span class=\"pl-s\">        a = F.softmax(a , dim = -1) #[batch , sess_len , con_len , seq_len(\u5f52\u4e00\u5316)]</span>\n<span class=\"pl-s\">        Q_hat = torch.matmul(a , Q_lstm_2) #[batch , sess_len , con_len , hidden]</span>\n<span class=\"pl-s\">        return Q_hat</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">class FlowQA(nn.Module):</span>\n<span class=\"pl-s\">    def __init__(self , vocab_size , hidden_size , pad_token_index):</span>\n<span class=\"pl-s\">        super(FlowQA,self).__init__()</span>\n<span class=\"pl-s\">        self.vocab_size = vocab_size</span>\n<span class=\"pl-s\">        self.hidden_size= hidden_size</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.embedding = nn.Embedding(</span>\n<span class=\"pl-s\">            num_embeddings=self.vocab_size,</span>\n<span class=\"pl-s\">            embedding_dim=self.hidden_size,</span>\n<span class=\"pl-s\">            padding_idx=pad_token_index</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.dotatthier = DotAttHier2(</span>\n<span class=\"pl-s\">            in_hidden_size=hidden_size,</span>\n<span class=\"pl-s\">            out_hidden_size = hidden_size</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.lstm_Q = NaiveLSTM(</span>\n<span class=\"pl-s\">            input_size = hidden_size,</span>\n<span class=\"pl-s\">            hidden_size = hidden_size,</span>\n<span class=\"pl-s\">            num_layer= 1,</span>\n<span class=\"pl-s\">            num_dir=2,</span>\n<span class=\"pl-s\">            batch_first=True</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.hierQueEncoder = HierEncoderLSTM(</span>\n<span class=\"pl-s\">            input_size = hidden_size,</span>\n<span class=\"pl-s\">            hidden_size = hidden_size,</span>\n<span class=\"pl-s\">            num_layer=1</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.IF1 = IF(</span>\n<span class=\"pl-s\">            in_hidden_size = 2 * hidden_size + 1,</span>\n<span class=\"pl-s\">            hidden_size = hidden_size</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        # print(\"IF1.hidden = \",self.IF1.hidden_size , \"IF1.in_hidden = \",self.IF1.in_hidden_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.IF2 = IF(</span>\n<span class=\"pl-s\">            in_hidden_size= 2 * hidden_size + 1,</span>\n<span class=\"pl-s\">            hidden_size=hidden_size</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        # print(\"IF2.hidden = \", self.IF2.hidden_size, \"IF2.in_hidden = \", self.IF2.in_hidden_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.FAT_On_Que = FullyAwareAttn(</span>\n<span class=\"pl-s\">            hidden_size = hidden_size * 3 ,</span>\n<span class=\"pl-s\">            x_hidden_size = (2 * hidden_size + 1) * 3,</span>\n<span class=\"pl-s\">            y_hidden_size = (2 * 2 + 1) * hidden_size</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.IF3 = IF3(</span>\n<span class=\"pl-s\">            in_hidden_size= 2 * hidden_size + 1 + 2 * hidden_size,</span>\n<span class=\"pl-s\">            hidden_size = hidden_size</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.FAT_C_self = FullyAwareAttn(</span>\n<span class=\"pl-s\">            hidden_size = hidden_size * 3,</span>\n<span class=\"pl-s\">            x_hidden_size = (2 * hidden_size + 1) * 3,</span>\n<span class=\"pl-s\">            y_hidden_size = (2 * hidden_size + 1) * 3</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.lstm_C = NaiveLSTM(</span>\n<span class=\"pl-s\">            input_size=(2 * hidden_size + 1) * 2,</span>\n<span class=\"pl-s\">            hidden_size=hidden_size,</span>\n<span class=\"pl-s\">            num_layer=1,</span>\n<span class=\"pl-s\">            num_dir=2,</span>\n<span class=\"pl-s\">            batch_first=True</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.predAnsable_W = nn.Bilinear(</span>\n<span class=\"pl-s\">            in1_features= 2 * (2 * hidden_size + 1),</span>\n<span class=\"pl-s\">            in2_features= 2 * hidden_size,</span>\n<span class=\"pl-s\">            out_features= 1</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.Ws = nn.Bilinear(</span>\n<span class=\"pl-s\">            in1_features= 2 * hidden_size + 1,</span>\n<span class=\"pl-s\">            in2_features= 2 * hidden_size,</span>\n<span class=\"pl-s\">            out_features= 1</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.ptr_gru = nn.GRUCell(</span>\n<span class=\"pl-s\">            input_size= 2 * hidden_size + 1,</span>\n<span class=\"pl-s\">            hidden_size = 2 * hidden_size</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        self.We = nn.Bilinear(</span>\n<span class=\"pl-s\">            in1_features= 2 * hidden_size + 1,</span>\n<span class=\"pl-s\">            in2_features= 2 * hidden_size,</span>\n<span class=\"pl-s\">            out_features= 1</span>\n<span class=\"pl-s\">        )</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def get_common(self, Q , C):</span>\n<span class=\"pl-s\">        em = []</span>\n<span class=\"pl-s\">        batch_size , sess_len , seq_len = Q.size()</span>\n<span class=\"pl-s\">        con_len = C.size(1)</span>\n<span class=\"pl-s\">        Q = Q.transpose(0,1) #[sess_len , batch , seq_len]</span>\n<span class=\"pl-s\">        for index in range(sess_len):</span>\n<span class=\"pl-s\">            temp = []</span>\n<span class=\"pl-s\">            for i in range(batch_size):</span>\n<span class=\"pl-s\">                tmp = [ 1 if ele in Q[index][i] else 0 for ele in C[i]]</span>\n<span class=\"pl-s\">                tmp = FloatTensor(tmp)</span>\n<span class=\"pl-s\">                tmp = temp.view(1, -1)</span>\n<span class=\"pl-s\">                temp.append(tmp)</span>\n<span class=\"pl-s\">            temp = torch.cat(tmp,dim=0)</span>\n<span class=\"pl-s\">            temp = temp.view(1,batch_size,con_len)</span>\n<span class=\"pl-s\">            em.append(temp)</span>\n<span class=\"pl-s\">        em = torch.cat(em,dim=0) #[sess_len ,batch_size , con_len]</span>\n<span class=\"pl-s\">        em = em.permute(1,0,2)</span>\n<span class=\"pl-s\">        return em</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def get_common2(self, Q ,C):</span>\n<span class=\"pl-s\">        em = []</span>\n<span class=\"pl-s\">        batch_size, sess_len, seq_len = Q.size()</span>\n<span class=\"pl-s\">        con_len = C.size(1)</span>\n<span class=\"pl-s\">        Q = Q.transpose(0, 1)  # [sess_len , batch , seq_len]</span>\n<span class=\"pl-s\">        for index in range(sess_len):</span>\n<span class=\"pl-s\">            temp = [[1 if ele in Q[index][i] else 0 for ele in C[i]] for i in range(batch_size)]</span>\n<span class=\"pl-s\">            em.append(FloatTensor(temp).view(1,batch_size,con_len))</span>\n<span class=\"pl-s\">        em = torch.cat(em, dim=0) #[sess_len ,batch_size , con_len]</span>\n<span class=\"pl-s\">        em = em.permute(1,0,2)</span>\n<span class=\"pl-s\">        return em</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def predictAnswerable(self , C_fou , C_mask1 , Q_lstm_high , Sess_len_mask1):</span>\n<span class=\"pl-s\">        #C_fou [batch , sess_len , con_len , 2 * hidden + 1]</span>\n<span class=\"pl-s\">        #C_mask1 [batch , con_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0</span>\n<span class=\"pl-s\">        #Q_lstm_high [batch , sess_len , 2 * hidden]</span>\n<span class=\"pl-s\">        #Sess_len_mask1 [batch , sess_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0</span>\n<span class=\"pl-s\">        batch_size , sess_len , con_len , _ = C_fou.size()</span>\n<span class=\"pl-s\">        C_mask1 = C_mask1.unsqueeze(1).unsqueeze(-1).repeat(1,sess_len,1)#[batch , sess_len , con_len , 1]</span>\n<span class=\"pl-s\">        C_fou = C_fou * C_mask1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Sess_len_mask1 = Sess_len_mask1.unsqueeze(-1) #[batch , sess_len , 1]</span>\n<span class=\"pl-s\">        Q_lstm_high = Q_lstm_high * Sess_len_mask1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        C_max = C_fou.max(-2)[0] #[batch , sess_len , 2 * hidden + 1]</span>\n<span class=\"pl-s\">        C_sum = C_fou.sum(-2) #[batch , sess_len , 2 * hidden + 1]</span>\n<span class=\"pl-s\">        C = torch.cat((C_max , C_sum) , dim = -1)  #[batch , sess_len , 2 * (2 * hidden + 1)]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        pred = self.predAnsable_W(C , Q_lstm_high) #[batch , sess_len , 1]</span>\n<span class=\"pl-s\">        Sess_len_mask0 = 1 - Sess_len_mask1</span>\n<span class=\"pl-s\">        Sess_len_mask0 = Sess_len_mask0 * (-1e10)</span>\n<span class=\"pl-s\">        pred = pred + Sess_len_mask0</span>\n<span class=\"pl-s\">        pred = pred.view(batch_size , sess_len)</span>\n<span class=\"pl-s\">        pred = F.softmax(pred,dim=-1)</span>\n<span class=\"pl-s\">        return pred , Sess_len_mask1.squeeze(-1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def predictSpan(self , C_fou , C_mask1 , Q_lstm_high , Sess_len_mask1 , max_ans_len = 15):</span>\n<span class=\"pl-s\">        # C_fou [batch , sess_len , con_len , 2 * hidden + 1]</span>\n<span class=\"pl-s\">        # C_mask1 [batch , con_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0</span>\n<span class=\"pl-s\">        # Q_lstm_high [batch , sess_len , 2 * hidden]</span>\n<span class=\"pl-s\">        # Sess_len_mask1 [batch , sess_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0</span>\n<span class=\"pl-s\">        batch_size, sess_len, con_len, _ = C_fou.size()</span>\n<span class=\"pl-s\">        C_mask1 = C_mask1.unsqueeze(1).unsqueeze(-1).repeat(1, sess_len, 1)  # [batch , sess_len , con_len , 1]</span>\n<span class=\"pl-s\">        C_fou = C_fou * C_mask1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Sess_len_mask1 = Sess_len_mask1.unsqueeze(-1)  # [batch , sess_len , 1]</span>\n<span class=\"pl-s\">        Q_lstm_high = Q_lstm_high * Sess_len_mask1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Q_lstm_high = Q_lstm_high.unsqueeze(2).repeat(1,1,con_len,1) #[batch , sess_len , con_len , 2 * hidden]</span>\n<span class=\"pl-s\">        P_S = self.Ws(C_fou,Q_lstm_high) #[batch , sess_len , con_len , 1]</span>\n<span class=\"pl-s\">        P_S = (P_S + (1 - C_mask1) * (-1e10)).squeeze(-1) #[batch , sess_len , con_len]</span>\n<span class=\"pl-s\">        P_S = F.softmax(P_S , -1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        C_fou_i = (C_fou * P_S.unsqueeze(-1)).view(batch_size * sess_len ,con_len , -1).sum(1) #[batch * sess_len , 2 * hidden + 1]</span>\n<span class=\"pl-s\">        Q_lstm_high_h_hat = self.ptr_gru(input = C_fou_i , hidden = Q_lstm_high[:,:,0,:].view(batch_size * sess_len , -1))</span>\n<span class=\"pl-s\">        #[batch * sess_len , 2 * hidden]</span>\n<span class=\"pl-s\">        Q_lstm_high_h_hat = Q_lstm_high_h_hat.view(batch_size , sess_len , -1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Q_lstm_high_h_hat = Q_lstm_high_h_hat.unsqueeze(2).repeat(1,1,con_len,1)</span>\n<span class=\"pl-s\">        P_E = self.We(C_fou , Q_lstm_high_h_hat) #[batch , sess_len , con_len , 1]</span>\n<span class=\"pl-s\">        P_E = P_E + ((1 - C_mask1) * (-1e10)).squeeze(-1) #[batch , sess_len , con_len]</span>\n<span class=\"pl-s\">        P_E = F.softmax(P_E , -1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        return  P_S, P_E , C_mask1.squeeeze(-1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    def forward(self, Q , C , Q_len , Sess_len , C_len , threadhold = 0.5, max_ans_len = 15):</span>\n<span class=\"pl-s\">        #Q [batch , sess_len , seq_len]</span>\n<span class=\"pl-s\">        #C [batch , context_len]</span>\n<span class=\"pl-s\">        #Q_len [batch , sess_len]</span>\n<span class=\"pl-s\">        #Sess_len [batch]</span>\n<span class=\"pl-s\">        #C_len [batch]</span>\n<span class=\"pl-s\">        embed_Q = self.embedding(Q).to(device=device)</span>\n<span class=\"pl-s\">        embed_C = self.embedding(C).to(device=device)</span>\n<span class=\"pl-s\">        print(\"device = \",device)</span>\n<span class=\"pl-s\">        batch_size , sess_len , seq_len , in_hidden_size = embed_Q.size()</span>\n<span class=\"pl-s\">        con_len = C.size(1)</span>\n<span class=\"pl-s\">        Q_mask0 = get_mask(Q_len, seq_len, mask_value=0) #[batch , sess_len ,seq_len]</span>\n<span class=\"pl-s\">        C_mask0 = get_mask(C_len, con_len, mask_value=0) #[batch , con_len]</span>\n<span class=\"pl-s\">        Q_hat = self.dotatthier(embed_Q,embed_C,Q_mask0,C_mask0)#[batch ,sess_len , con_len , in_hidden]</span>\n<span class=\"pl-s\">        em = self.get_common2(Q , C) #[batch , sess_len , con_len]</span>\n<span class=\"pl-s\">        em = em.view(batch_size,sess_len,con_len,1)</span>\n<span class=\"pl-s\">        C_zero = torch.cat((embed_C.view(batch_size,1,con_len,in_hidden_size).repeat(1,sess_len,1,1), em , Q_hat) , dim = -1)</span>\n<span class=\"pl-s\">        #[batch , sess_len , con_len , in_hidden + 1 + in_hidden]</span>\n<span class=\"pl-s\">        print(\"C_zero.shape = \",C_zero.shape)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        C_zero_len = C_len.unsqueeze(1).repeat(1 , sess_len)</span>\n<span class=\"pl-s\">        #[batch , sess_len]</span>\n<span class=\"pl-s\">        Sess_C_len = Sess_len.unsqueeze(1).repeat(1 , con_len)</span>\n<span class=\"pl-s\">        #[batch , con_len]</span>\n<span class=\"pl-s\">        C_one = self.IF1(C_zero , C_zero_len , Sess_C_len) ##[batch , sess_len , con_len , hidden * 2 + 1]</span>\n<span class=\"pl-s\">        C_two = self.IF2(C_one , C_zero_len , Sess_C_len)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Q_lstm_1, (h_n, c_n) = self.lstm_Q(input = embed_Q.view(-1 , seq_len ,in_hidden_size) , input_length = Q_len.view(-1))</span>\n<span class=\"pl-s\">        #[batch * sess_len , seq_len , num_dir * hidden]</span>\n<span class=\"pl-s\">        Q_lstm_1 = Q_lstm_1.view(batch_size , sess_len , seq_len , -1)</span>\n<span class=\"pl-s\">        Q_lstm_2 , Q_lstm_high = self.hierQueEncoder(Q = Q_lstm_1 , Q_len = Q_len , Sess_len = Sess_len , Q_mask0 = Q_mask0)</span>\n<span class=\"pl-s\">        #[batch , sess_len , seq_len , 2 * hidden]  [batch , sess_len , 2 * hidden]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Q_attned =  self.FAT_On_Que( (C_zero, C_one, C_two) , (embed_Q, Q_lstm_1, Q_lstm_2) , C_mask0 , Q_mask0)</span>\n<span class=\"pl-s\">        # [batch , sess_len , con_len , 2 * hidden]</span>\n<span class=\"pl-s\">        temp = torch.cat([C_two , Q_attned] , dim = -1)</span>\n<span class=\"pl-s\">        C_thr = self.IF3(temp , C_zero_len , Sess_C_len)</span>\n<span class=\"pl-s\">        C_attend =  self.FAT_C_self( (C_one , C_two , C_thr) , (C_one , C_two , C_thr) , C_mask0 , C_mask0)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        C_fou = torch.cat([C_thr , C_attend] , dim = -1) # [batch , sess_len , con_len , 2 * (2 * hidden + 1)]</span>\n<span class=\"pl-s\">        C_fou = C_fou.view(batch_size * sess_len , con_len , -1)</span>\n<span class=\"pl-s\">        C_fou = self.lstm_C(C_fou, C_zero_len.view(-1))</span>\n<span class=\"pl-s\">        C_fou = C_fou.view(batch_size , sess_len , con_len , -1)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        P_answerable , Sess_len_mask1 = self.predictAnswerable( C_fou , (1 - C_mask0) , Q_lstm_high , get_mask(Sess_len , sess_len ,mask_value=1))</span>\n<span class=\"pl-s\">        #[batch , sess_len]</span>\n<span class=\"pl-s\">        P_answerable_yes_no = P_answerable &gt;= threadhold #\u4e0d\u4e00\u5b9a\u8981\u4e0d\u8981\u6ce8\u91ca\u6389\u5b83</span>\n<span class=\"pl-s\">        P_start , P_end , P_mask1 = self.predictSpan(C_fou,(1 - C_mask0) , Q_lstm_high , Sess_len_mask1 , max_ans_len)</span>\n<span class=\"pl-s\">        #[batch , sess_len , con_len]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        return P_answerable , Sess_len_mask1 , P_start , P_end , P_mask1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def train(model , otherInputs , start_label , end_label , criterion , optimizer):</span>\n<span class=\"pl-s\">    #start_label = [batch * sess_len] tensor \u7c7b\u578b\u7684</span>\n<span class=\"pl-s\">    #criterion = nn.NLLLoss()</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    \u5728\u8fd9\u91cc\u8c03\u7528model\u6a21\u5757\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    (Q, C, Q_len, Sess_len, C_len) = otherInputs</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(torch.LongTensor(Q) ,torch.LongTensor(C) ,LongTensor(Q_len) ,LongTensor(Sess_len) ,LongTensor (C_len))</span>\n<span class=\"pl-s\">    # [batch , sess_len]          #[batch , sess_len , con_len]</span>\n<span class=\"pl-s\">    loss = 0</span>\n<span class=\"pl-s\">    ones = torch.ones(len(start_label))</span>\n<span class=\"pl-s\">    batch_size , sess_len , con_len = P_start.size()</span>\n<span class=\"pl-s\">    P_start = P_start.view(-1 , con_len)</span>\n<span class=\"pl-s\">    P_end = P_end.view(-1,con_len)</span>\n<span class=\"pl-s\">    #\u5bf9\u4e8eloss\u6709\u4e24\u79cd\u64cd\u4f5c\uff0c1\uff0c\u53ea\u9009\u62e9\u6b63\u786e\u7684\u4f4d\u7f6e\u7684\u6982\u7387\uff0c2\uff0csubsample\u4e00\u4e9b\u9519\u8bef\u7684\u4f4d\u7f6e\u7684\u6982\u7387\u6765\u8fdb\u884c\u8ba1\u7b97\u3002\u8fdb\u884c2\u7684\u65f6\u5019\u600e\u4e48\u6837\u4fdd\u8bc11\u4e2d\u7684\u4e0d\u88ab\u518d\u9009\u4e2d\u5462 \uff0c  \u53c8\u600e\u4e48\u4fdd\u8bc1mask\u7684\u4e1c\u897f\u4e0d\u88ab\u9009\u4e2d\u5462\u3002</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    #\u5b8c\u62101</span>\n<span class=\"pl-s\">    start = FloatTensor( [ P_start[i][start_label[i]] for i in range(batch_size * sess_len) ] )</span>\n<span class=\"pl-s\">    start_neg = 1 - start</span>\n<span class=\"pl-s\">    start = start.view(-1,1)</span>\n<span class=\"pl-s\">    start_neg = start_neg.view(-1,1)</span>\n<span class=\"pl-s\">    start_p = torch.cat((start_neg , start) , dim = -1)</span>\n<span class=\"pl-s\">    loss += criterion(start_p , torch.ones(batch_size * sess_len))</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    end = FloatTensor([P_end[i][end_label[i]] for i in range(batch_size * sess_len)])</span>\n<span class=\"pl-s\">    end_neg = 1 - end</span>\n<span class=\"pl-s\">    end = end.view(-1, 1)</span>\n<span class=\"pl-s\">    end_neg = end_neg.view(-1, 1)</span>\n<span class=\"pl-s\">    end_p = torch.cat((end_neg, end), dim=-1)</span>\n<span class=\"pl-s\">    loss += criterion(end_p, torch.ones(batch_size * sess_len))</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    loss.backward()</span>\n<span class=\"pl-s\">    optimizer.step()</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    return loss.item / batch_size</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def reload(args,PATH):</span>\n<span class=\"pl-s\">    flowQA_model = FlowQA()</span>\n<span class=\"pl-s\">    flowQA_optimizer = optim.Adamax(flowQA_model.parameters(), lr=0.1)</span>\n<span class=\"pl-s\">    checkpoint = torch.load(PATH)</span>\n<span class=\"pl-s\">    flowQA_model.load_state_dict(checkpoint['model_state_dict'])</span>\n<span class=\"pl-s\">    flowQA_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])</span>\n<span class=\"pl-s\">    epoch = checkpoint['epoch']</span>\n<span class=\"pl-s\">    loss = checkpoint['loss']</span>\n<span class=\"pl-s\">    # flowQA_model.eval()</span>\n<span class=\"pl-s\">    # flowQA_model.train() \u4e3a\u7684\u662f\u8ba9\u6a21\u578b\u53c2\u6570\u53ef\u4ee5\u88ab\u8bad\u7ec3</span>\n<span class=\"pl-s\">    return flowQA_model,flowQA_optimizer</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def trainIters(in_file = \"/data1/zhengquan/data/CoQA/subTrain_preproed.json\", n_epochs = 20,  print_every=1000, plot_every=100, save_every = 100,learning_rate=0.01, batch_size = 2):</span>\n<span class=\"pl-s\">    Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label, lang = getData(in_file)</span>\n<span class=\"pl-s\">    print(\"data load finished\")</span>\n<span class=\"pl-s\">    flowQA_model = FlowQA(vocab_size = lang.n_words , hidden_size = 100, pad_token_index = lang.word2id['&lt;PAD&gt;'] )</span>\n<span class=\"pl-s\">    criterion = nn.CrossEntropyLoss(size_average=True, reduce=False)</span>\n<span class=\"pl-s\">    flowQA_optimizer = optim.Adamax(flowQA_model.parameters(), lr=0.1)</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    OtherInputs \u5305\u62ec\n    Q , C , Q_len , Sess_len , C_len \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Q [batch , sess_len , seq_len]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>C [batch , context_len]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Q_len [batch , sess_len]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Sess_len [batch]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>C_len [batch]</span>\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    start = time.time()</span>\n<span class=\"pl-s\">    plot_losses = []</span>\n<span class=\"pl-s\">    print_loss_total = 0  # Reset every print_every</span>\n<span class=\"pl-s\">    plot_loss_total = 0  # Reset every plot_every</span>\n<span class=\"pl-s\">    inner_loop_iter = 0</span>\n<span class=\"pl-s\">    for iter in range(1, n_epochs + 1):</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">'''</span></span>\n        \u8fd9\u91cc\u7684otherInputs\u548cstart_label,end_label,\u624d\u662f\u771f\u6b63\u6309\u7167batch\u7f51\u91cc\u9762\u653e\u7684\uff0c\u73b0\u5728\u624d\u771f\u7684\u9700\u8981\u5728sess_len, seq_len\u6216context_len\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8865\u9f50,\u4fdd\u8bc1\u6bcf\u4e2abatch\u5185\u7684\u6570\u636e\u683c\u5f0f\u4e00\u81f4\uff0c\n        <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">        data = getBatchData((Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label))</span>\n<span class=\"pl-s\">        while True:</span>\n<span class=\"pl-s\">            temp_data = next(data ,None)</span>\n<span class=\"pl-s\">            if temp_data is None:</span>\n<span class=\"pl-s\">                break</span>\n<span class=\"pl-s\">            #inner_loop_iter += 1</span>\n<span class=\"pl-s\">            inner_loop_iter += batch_size</span>\n<span class=\"pl-s\">            Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label = temp_data</span>\n<span class=\"pl-s\">            OtherInpus = (Q , C , Q_len , Sess_len , C_len)</span>\n<span class=\"pl-s\">            loss = train(flowQA_model , OtherInpus , start_label , end_label , criterion , flowQA_optimizer)</span>\n<span class=\"pl-s\">            print_loss_total += loss</span>\n<span class=\"pl-s\">            plot_loss_total += loss</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">            if inner_loop_iter % print_every == 0:</span>\n<span class=\"pl-s\">                print_loss_avg = print_loss_total / print_every</span>\n<span class=\"pl-s\">                print_loss_total = 0</span>\n<span class=\"pl-s\">                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),</span>\n<span class=\"pl-s\">                                             iter, iter / n_epochs * 100, print_loss_avg))</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">            if inner_loop_iter % plot_every == 0:</span>\n<span class=\"pl-s\">                plot_loss_avg = plot_loss_total / plot_every</span>\n<span class=\"pl-s\">                plot_losses.append(plot_loss_avg)</span>\n<span class=\"pl-s\">                plot_loss_total = 0</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">            if inner_loop_iter % save_every == 0:</span>\n<span class=\"pl-s\">                temp_num = inner_loop_iter</span>\n<span class=\"pl-s\">                path_to_save = os.path.join(DIR_TO_SAVE,\"%d\"%(temp_num))</span>\n<span class=\"pl-s\">                while os.path.exists(path_to_save):</span>\n<span class=\"pl-s\">                    temp_num += 1</span>\n<span class=\"pl-s\">                    path_to_save = os.path.join(DIR_TO_SAVE, \"%d\" % (temp_num))</span>\n<span class=\"pl-s\">                torch.save({'epoch':iter,</span>\n<span class=\"pl-s\">                            'model_state_dict':flowQA_model.state.dict(),</span>\n<span class=\"pl-s\">                            'optimizer_state_dict':flowQA_optimizer.state_dict(),</span>\n<span class=\"pl-s\">                            'loss':loss}</span>\n<span class=\"pl-s\">                           ,path_to_save)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    showPlot(plot_losses)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">def evaluate(model , otherInputs , start_label , end_label):</span>\n<span class=\"pl-s\">    with torch.no_grad():</span>\n<span class=\"pl-s\">        P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(otherInputs)</span>\n<span class=\"pl-s\">    return P_start , P_end</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">if __name__ == \"__main__\":</span>\n<span class=\"pl-s\">    # in_file  = \"/data1/zhengquan/data/coqa-train-v1.0.json\"</span>\n<span class=\"pl-s\">    # Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label, lang = getData(in_file)</span>\n<span class=\"pl-s\">    # print(lang.n_words)</span>\n<span class=\"pl-s\">    trainIters()</span></pre></div>", "body_text": "\ud83d\udc1b Bug\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch\n\nTo Reproduce\nSteps to reproduce the behavior:\n1.i am using pytorch 0.4.1 with anaconda3\n1.run my code I got the following error:\nTraceback (most recent call last):\nFile \"rc_model.py\", line 849, in \ntrainIters()\nFile \"rc_model.py\", line 811, in trainIters\nloss = train(flowQA_model , OtherInpus , start_label , end_label , criterion , flowQA_optimizer)\nFile \"rc_model.py\", line 737, in train\nP_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(torch.LongTensor(Q) ,torch.LongTensor(C) ,LongTensor(Q_len) ,LongTensor(Sess_len) ,LongTensor (C_len))\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in call\nresult = self.forward(*input, **kwargs)\nFile \"rc_model.py\", line 701, in forward\nC_one = self.IF1(C_zero , C_zero_len , Sess_C_len) ##[batch , sess_len , con_len , hidden * 2 + 1]\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in call\nresult = self.forward(*input, **kwargs)\nFile \"rc_model.py\", line 376, in forward\nC_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden * 2]\nFile \"rc_model.py\", line 347, in Context_Integration\nC_h_1, (h_n, c_n) = self.lstm(input = C_h_1)\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in call\nresult = self.forward(*input, **kwargs)\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\", line 192, in forward\noutput, hidden = func(input, self.all_weights, hx, batch_sizes)\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/_functions/rnn.py\", line 324, in forward\nreturn func(input, *fargs, **fkwargs)\nFile \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/_functions/rnn.py\", line 288, in forward\ndropout_ts)\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch\n\n\n\n\nExpected behavior\n\nEnvironment\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch Version (e.g., 1.0):\nOS (e.g., Linux):\nHow you installed PyTorch (conda, pip, source):\nBuild command you used (if compiling from source):\nPython version:\nCUDA/cuDNN version:CUDA Version 9.0.176\n#define CUDNN_MAJOR 7\n#define CUDNN_MINOR 0\n#define CUDNN_PATCHLEVEL 3\n--\n#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n\n#include \"driver_types.h\"\n\nGPU models and configuration:\nAny other relevant information:\n\nAdditional context\nmy all code is as follows\n# -*- coding:utf-8 -*-\n'''\"\nseq2seq_translation_tutorial.py -> seq2seq_add_batch_dimension.py \u5c06\u539f\u6765\u7684\u4ee3\u7801\u4e2dbatch\u7ef4\u5ea6\u4ece1\uff0c\u589e\u52a0\u5230n>1,\u540c\u65f6\u9047\u5230\u7684\u9ebb\u70e6\uff0cpadding\u4e0d\u540c\u957f\u5ea6\u5230\u540c\u610f\u957f\u5ea6\uff0cnn.GRU\u4e2d\u6709\u4e24\u4e2a\u548cpad_pack\u76f8\u5173\u7684\u51fd\u6570\n            embedded_input = torch.nn.utils.rnn.pack_padded_sequence(embedded_input,input_length,batch_first=True)#lengths' array has to be sorted in decreasing order\n            \u653e\u5165RNN\u4e4b\u524d\u5bf9\u6570\u636e\u8fdb\u884c\u5c01\u88c5\uff0c\n            output , _ = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n            \u5bf9rnn\u7684\u8f93\u51fa\u8fdb\u884c\u89e3\u5c01\u88c5\u3002\n\nseq2seq_add_batch_dimension.py -> seq2seq_add_hierarchy_rnn_encoder.py \u589e\u52a0\u4e86\u5c42\u7ea7rnn\u7684\u529f\u80fd\u5bf9dialogue\u53ef\u4ee5\u7f16\u7801 \u300c\u300d \u8fd9\u6b21\u5148\u4ece\u6570\u636e\u5f00\u59cb\u641e\n\n\"\"\"\nfrom __future__ import unicode_literals, print_function, division\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import optim\nimport time\nimport math\nfrom getData import getData , getBatchData\nimport matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\n\nUSE_CUDA = torch.cuda.is_available()\nif USE_CUDA:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nprint(\"device = \",device)\ngpus = [0]\n# torch.cuda.set_device(gpus[0])\nFloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\nByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n\ndef asMinutes(seconds):\n    minutes = math.floor(seconds/60)\n    seconds -= minutes * 60\n    return '%d minutes %d seconds'%(minutes , seconds)\n\ndef timeSince(since, percent):\n    '''\n    :param percent = iter_right_now / total_iter\n    '''\n    now = time.time()\n    seconds = now - since\n    estimate_seconds = seconds / (percent)\n    remain_seconds = estimate_seconds - seconds\n    return '%s (-%s)' %(asMinutes(seconds) , asMinutes())\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)\n\nPAD_token = 0\nSOS_token = 1\nEOS_token = 2\nMAX_LENGTH = 10\nDIR_TO_SAVE = \"/data1/zhengquan/data/CoQA/saved_models\"\nif not os.path.exists(DIR_TO_SAVE):\n    os.makedirs(DIR_TO_SAVE)\n\nclass Embed(nn.Module):\n    def __init__(self,hidden_size = 100,pad_token_index = 0):\n        super(Embed,self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(\n            num_embeddings=1000,\n            embedding_dim=self.hidden_size,\n            padding_idx=pad_token_index\n        )\n\n    def forward(self , input):\n        return self.embedding(input)\n\nclass DotAtt(nn.Module):\n    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):\n        super(DotAtt, self).__init__()\n        self.in_hidden_size = in_hidden_size\n        self.out_hidden_size = out_hidden_size\n        self.W = nn.Linear(self.in_hidden_size, self.out_hidden_size)\n\n    def forward(self , qi, c , qi_length , c_length):\n        #qi [batch , seq_len , in_hidden]\n        #c [batch , con_len , in_hidden]\n        #qi_length = [batch]\n        #c_length = [batch]\n        batch_size , seq_len  = qi.size()[:2]\n        con_len = c.size(1)\n        mask_qi = torch.ones(batch_size , seq_len)\n        mask_c = torch.ones(batch_size , con_len)\n        for index , (qi_l , ci_l) in range(zip(qi_length , c_length)):\n            mask_qi[index , :qi_l] = 0\n            mask_c[index , :ci_l] = 0\n        mask_c *= (-1e10)\n        mask_qi *= -1e10\n\n        mask_c = mask_c.view(1,batch_size,con_len).repeat(seq_len,1,1)\n        mask_qi = mask_qi.view(1,batch_size ,seq_len).repeat(con_len,1,1)\n\n\n        qi = F.relu(self.W(qi))\n        c = F.relu(self.W(c))\n        attn = torch.matmul(c ,qi.transpose(-2,-1)) #[batch , con_len , seq_len]\n        attn = attn.permute(1,0,2) #[con_len , batch , seq_len]\n        attn = attn + mask_qi\n        attn = attn.permute(1,0,2).permute(2,0,1) #[seq_len , batch , con_len]\n        attn = attn + mask_c\n        attn = attn.permute(1,2,0)#[batch , con_len , seq_len]\n        attn = F.softmax(attn ,dim = -1)\n        qi_hat = torch.matmul(attn , qi) #[batch , con_len , out_hidden]\n        return qi_hat\n\nclass DotAttHier(nn.Module):\n    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):\n        super(DotAttHier , self).__init__()\n        self.in_hidden_size = in_hidden_size\n        self.out_hidden_size = out_hidden_size\n        self.dotatt = DotAtt(in_hidden_size , out_hidden_size)\n\n    def forward(self, Q , C , Q_len , C_len):\n        #Q[batch , sess_len , seq_len , in_hidden]\n        #C[batch , con_len , in_hidden]\n        #Q_len[batch , sess_len]\n        #C_len[batch]\n        Q = Q.permute(1,0,2,3) #[sess_len , batch , seq_len , in_hidden]\n        Q_len = Q_len.permute(1,0)#[sess_len , batch]\n        sess_len ,batch_size , seq_len , in_hidden = Q.size()\n        con_len = C.size(1)\n        Q_hat = []\n        for index in range(sess_len):\n            Q_hat.append(self.dotatt(Q[index],C , Q_len[index] , C_len).view(1 , con_len , seq_len , -1))\n            # [1 , batch , con_len , out_hidden]\n        Q_hat = torch.cat(Q_hat , dim = 0).permute(1,0,2,3) #[batch ,sess_len , con_len , in_hidden]\n        return Q_hat\n\nclass DotAttHier2(nn.Module):\n    '''\u76f4\u63a5\u8fdb\u884cHierattn\uff0c\u4e0d\u4f1a\u5728sess_len\u7ef4\u5ea6\u4e0a\u5faa\u73af'''\n    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):\n        super(DotAttHier2 , self).__init__()\n        self.in_hidden_size = in_hidden_size\n        self.out_hidden_size = out_hidden_size\n        self.W = nn.Linear(self.in_hidden_size, self.out_hidden_size).to(device)\n\n    def forward(self , Q , C , Q_mask , C_mask):\n        #Q_len[batch , sess_len]\n        batch_size , sess_len , seq_len , in_hidden = Q.size()\n        con_len = C.size(1)\n\n        Q = Q.view(batch_size * sess_len , seq_len , in_hidden)\n        C = C.repeat(sess_len , 1, 1)\n        Q = F.relu(self.W(Q))\n        C = F.relu(self.W(C))\n        attn = torch.matmul(C, Q.transpose(-2, -1))  # [batch * sess_len , con_len , seq_len]\n        Q_mask = Q_mask * (-1e10) #[batch , sess_len , seq_len]\n        Q_mask = Q_mask.view(-1 , seq_len).unsqueeze(1).repeat(1 ,con_len , 1) #[batch * sess_len , con_len , seq_len]\n        C_mask = C_mask * (-1e10) #[batch , con_len]\n        C_mask = C_mask.unsqueeze(-1).repeat(sess_len , 1 , seq_len) #[batch * sess_len , con_len , seq_len]\n        attn = attn + C_mask + Q_mask\n        attn = F.softmax(attn, dim=-1) #[batch * sess_len , con_len , seq_len(\u5f52\u4e00\u5316)]\n        Q_hat = torch.matmul(attn , Q) #[batch * sess_len , con_len , in_hidden]\n        Q_hat = Q_hat.view(batch_size , sess_len , con_len , in_hidden)\n        return Q_hat\n\nclass NaiveLSTM(nn.Module):\n    def __init__(self , input_size , hidden_size , batch_first = True,\n                 num_layer = 1, num_dir = 2,\n                 batch_size = 10 , dropout_p = 0.5,\n                 max_length = MAX_LENGTH):\n        super(NaiveLSTM,self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.num_layer = num_layer\n        self.num_dir = num_dir\n        self.batch_size = batch_size\n        self.dropout_p = dropout_p\n        self.lstm  = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layer,\n            bidirectional=num_dir > 1,\n            batch_first=batch_first,\n            dropout=dropout_p\n        )\n\n    def forward(self, input , input_length):\n        #input = [batch * sess_len , seq_len , hidden_size]\n        #input_length = [batch * sess_len]\n\n        sorted_idx = np.argsort(-input_length)\n        input = input[sorted_idx]\n        input_length = input_length[sorted_idx]\n\n        input = torch.nn.utils.rnn.pack_padded_sequence(input, input_length, batch_first=self.batch_first)\n        out, (h_n, c_n) = self.lstm(input=input)\n        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        out = out[unsorted_idx]  # [batch_size * sess_len, seq_len , num_dir * hidden_size]\n        return out\n\nclass HierEncoderLSTM(nn.Module):\n    def __init__(self ,input_size, hidden_size , batch_first = True,\n                 num_layer = 2 , num_dir = 2,\n                 batch_size = 10 , dropout_p = 0.5,\n                 max_length = MAX_LENGTH):\n        super(HierEncoderLSTM,self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.num_layer = num_layer\n        self.num_dir = num_dir\n        self.batch_size = batch_size\n        self.dropout_p = dropout_p\n\n        self.lstm = nn.LSTM(\n            input_size = input_size,\n            hidden_size = hidden_size,\n            num_layers = num_layer,\n            bidirectional= num_dir > 1,\n            batch_first = batch_first,\n            dropout = dropout_p\n        )\n\n        self.w  = nn.Linear(\n            num_dir * hidden_size,\n            1\n        )\n\n        self.high_lstm = nn.LSTM(\n            input_size = hidden_size * num_dir,\n            hidden_size = hidden_size * num_dir,\n            num_layers = 1,\n            batch_first = batch_first,\n            dropout = dropout_p\n        )\n\n    def forward(self , Q , Q_len , Sess_len , Q_mask0):\n        batch_size , sess_len , seq_len , input_size = Q.size()\n        #Q_len = [batch , sess_len]\n        #Q_mask0 = [batch , sess_len , seq_len]\n        Q = Q.view(-1 , seq_len , input_size)\n        Q_len = Q_len.view(-1)\n\n        sorted_idx = np.argsort(-Q_len)\n        out = Q[sorted_idx]\n        out_len = Q_len[sorted_idx]\n\n        out = torch.nn.utils.rnn.pack_padded_sequence(out,out_len , batch_first=self.batch_first)\n        out, (h_n, c_n) = self.lstm(input=out)\n        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        out = out[unsorted_idx] #[batch_size * sess_len, seq_len , self.num_dir * self.hidden_size]\n\n        attn = self.w(out) #[batch * sess_len , seq_len , 1]\n        Q_mask0 = Q_mask0 * (-1e10)\n        Q_mask0 = Q_mask0.view(-1,seq_len).unsqueeze(-1) #[batch * sess_len , seq_len , 1]\n        attn = (attn + Q_mask0).transpose(-1,-2) #[batch * sess_len , 1 , seq_len]\n        high_out = torch.matmul(attn , out) #[batch * sess_len  , 1 , num_dir * hidden]\n        high_out = high_out.squeeze(1).view(batch_size , sess_len , -1)\n\n        sorted_idx = np.argsort(-Sess_len)\n        high_out = high_out[sorted_idx]\n        Sess_len = Sess_len[sorted_idx]\n\n        high_out = torch.nn.utils.rnn.pack_padded_sequence(high_out , Sess_len, batch_first = self.batch_first)\n        high_out , (h_n , c_n) = self.high_lstm(input = high_out)\n        high_out , _ = torch.nn.utils.rnn.pad_packed_sequence(high_out , batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        high_out = high_out[unsorted_idx] #[batch , sess_len , num_dir * hidden]\n        return out , high_out\n\ndef get_mask(input_length , max_seq_len , mask_value = 0):\n    #[batch ] or [batch , sess_len]\n    #mask_value = 0 ,\u6709\u503c\u7684\u5730\u65b9\u4e3a0 \uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a1\n    #mask_value = 1 ,\u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\n    #for situation 1\n    siz = []\n    for i in range(len(input_length.size())):\n        siz.append(input_length.size(i))\n    siz.append(max_seq_len)\n    if mask_value == 0:\n        mask = torch.zeros(siz).to(device) #[batch, max_seq_len] or[batch , sess_len ,max_seq_len]\n    if mask_value == 1:\n        mask = torch.ones(siz).to(device)\n    input_length = input_length.view(-1) #[batch] or [batch * sess_len]\n    mask = mask.view(-1 , max_seq_len)\n    for index ,l in enumerate(input_length):\n        if mask_value == 0:\n            mask[index , l:] = 1\n        elif mask_value == 1:\n            mask[index , l:] = 0\n    mask = mask.view(siz)\n    return mask\n\nclass IF(nn.Module):\n    def __init__(self , in_hidden_size , hidden_size ,\n                 num_dir =2 , batch_first = True,\n                 dropout_p = 0.5 , num_layer = 1):\n        super(IF,self).__init__()\n        # print(\"in_IF_Class\")\n        # print(\"in_hidden_size= \",in_hidden_size)\n        # print(\"hidden_size = \",hidden_size)\n\n        self.in_hidden_size = in_hidden_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.lstm = nn.LSTM(\n            input_size=in_hidden_size,\n            hidden_size=hidden_size//2,\n            num_layers=num_layer,\n            bidirectional=num_dir > 1,\n            batch_first=batch_first,\n            dropout=dropout_p\n        )\n        try :\n            assert in_hidden_size == hidden_size * 2 + 1\n        except:\n            print(\"in_hidden_size = \",in_hidden_size)\n            print(\"hidden_size = \",hidden_size)\n            exit(87909)\n\n        self.gru = nn.GRU(\n            input_size = hidden_size * num_dir,\n            hidden_size = in_hidden_size - hidden_size // 2 * 2,\n            batch_first= batch_first,\n            dropout= dropout_p,\n        )\n\n    def Context_Integration(self , C_h_1 , C_h_len):\n        #C_h_1 [batch , sess_len , con_len , in_hidden]\n        #C_h_len [batch , sess_len]\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\n        C_h_1 = C_h_1.view(-1 , con_len , in_hidden_size)\n        C_h_len = C_h_len.view(-1)\n        sorted_idx = np.argsort(-C_h_len)\n        C_h_1 = C_h_1[sorted_idx]\n        C_h_len = C_h_len[sorted_idx]\n        print(\"C_h_len = \",C_h_len)\n        C_h_1 = torch.nn.utils.rnn.pack_padded_sequence(C_h_1 , C_h_len , batch_first=self.batch_first)\n        C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)\n        C_h_1 , _  = torch.nn.utils.rnn.pad_packed_sequence(C_h_1,batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        C_h_1 = C_h_1[unsorted_idx]\n        C_h_1 = C_h_1.view(batch_size,sess_len,con_len,-1)\n        return C_h_1\n\n    def Flow(self , C_h , Sess_C_len):\n        #Sess_C_len [batch , con_len]\n        batch_size , con_len , sess_len , hidden = C_h.size()\n        C_h = C_h.view(-1,sess_len,hidden)\n        Sess_C_len = Sess_C_len.view(-1)\n        sorted_idx = np.argsort(-Sess_C_len)\n        C_hat = C_h[sorted_idx]\n        Sess_C_len = Sess_C_len[sorted_idx]\n        C_hat = torch.nn.utils.rnn.pack_padded_sequence(C_hat, Sess_C_len, batch_first=self.batch_first)\n        C_hat , (h_n , c_n) = self.gru(input = C_hat)\n        C_hat , _ = torch.nn.utils.rnn.pad_packed_sequence(C_hat , batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        C_hat = C_hat[unsorted_idx] #[batch * con_len , sess_len , hidden]\n        C_h_plus_1 = torch.cat((C_h,C_hat) , dim = -1) #[batch * con_len , sess_len , hidden]\n        C_h_plus_1 = C_h_plus_1.view(batch_size , con_len , sess_len , -1).permute(0,2,1,3)\n        return C_h_plus_1 #[batch , sess_len , con_len , in_hidden_size]\n\n    def forward(self , C_h_1 , C_h_len , Sess_C_len):\n        #C_h_len [batch , sess_len]\n        #Sess_C_len [batch , con_len]\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\n        # C_h_1 = C_h_1.view( -1, con_len , in_hidden_size)\n        C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden * 2]\n        C_hat = C_hat.permute(0,2,1,3)#[batch , con_len , sess_len , 2 * hidden]\n        C_h = self.Flow(C_hat , Sess_C_len) #[batch , sess_len , con_len , hidden * 2 + 1]\n        return C_h\n\nclass IF3(nn.Module):\n    def __init__(self , in_hidden_size , hidden_size ,\n                 num_dir =2 , batch_first = True,\n                 dropout_p = 0.5 , num_layer = 1):\n        super(IF3,self).__init__()\n        # print(\"in_IF3_Class\")\n        # print(\"in_hidden_size= \",in_hidden_size)\n        # print(\"hidden_size = \",hidden_size)\n\n        self.in_hidden_size = in_hidden_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n        self.lstm = nn.LSTM(\n            input_size=in_hidden_size,\n            hidden_size=hidden_size//2,\n            num_layers=num_layer,\n            bidirectional=num_dir > 1,\n            batch_first=batch_first,\n            dropout=dropout_p\n        )\n        try :\n            assert in_hidden_size == hidden_size * 2 + 1 + hidden_size * 2\n        except:\n            print(\"in_hidden = \",in_hidden_size)\n            print(\"hidden = \",hidden_size)\n            exit(6789)\n\n        self.gru = nn.GRU(\n            input_size = hidden_size * num_dir,\n            hidden_size = in_hidden_size - hidden_size // 2 * 2,\n            batch_first= batch_first,\n            dropout= dropout_p,\n        )\n\n    def Context_Integration(self , C_h_1 , C_h_len):\n        #C_h_1 [batch , sess_len , con_len , in_hidden]\n        #C_h_len [batch , sess_len]\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\n        C_h_1 = C_h_1.view(-1 , con_len , in_hidden_size)\n        C_h_len = C_h_len.view(-1)\n        sorted_idx = np.argsort(-C_h_len)\n        C_h_1 = C_h_1[sorted_idx]\n        C_h_len = C_h_len[sorted_idx]\n        C_h_1 = torch.nn.utils.rnn.pack_padded_sequence(C_h_1 , C_h_len , batch_first=self.batch_first)\n        C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)\n        C_h_1 , _  = torch.nn.utils.rnn.pad_packed_sequence(C_h_1,batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        C_h_1 = C_h_1[unsorted_idx]\n        C_h_1 = C_h_1.view(batch_size,sess_len,con_len,-1)\n        return C_h_1\n\n    def Flow(self , C_h , Sess_C_len):\n        #Sess_C_len [batch , con_len]\n        batch_size , con_len , sess_len , hidden = C_h.size()\n        C_h = C_h.view(-1,sess_len,hidden)\n        Sess_C_len = Sess_C_len.view(-1)\n        sorted_idx = np.argsort(-Sess_C_len)\n        C_hat = C_h[sorted_idx]\n        Sess_C_len = Sess_C_len[sorted_idx]\n        C_hat = torch.nn.utils.rnn.pack_padded_sequence(C_hat, Sess_C_len, batch_first=self.batch_first)\n        C_hat , (h_n , c_n) = self.gru(input = C_hat)\n        C_hat , _ = torch.nn.utils.rnn.pad_packed_sequence(C_hat , batch_first=self.batch_first)\n        unsorted_idx = np.argsort(sorted_idx)\n        C_hat = C_hat[unsorted_idx] #[batch * con_len , sess_len ,3 *  hidden + 1]\n        C_h_plus_1 = torch.cat((C_h,C_hat) , dim = -1) #[batch * con_len , sess_len ,  4 * hidden + 1]\n        C_h_plus_1 = C_h_plus_1.view(batch_size , con_len , sess_len , -1).permute(0,2,1,3)\n        return C_h_plus_1 #[batch , sess_len , con_len , in_hidden_size]\n\n    def forward(self , C_h_1 , C_h_len , Sess_C_len):\n        #C_h_len [batch , sess_len]\n        #Sess_C_len [batch , con_len]\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\n        C_h_1 = C_h_1.view( -1, con_len , in_hidden_size)\n        C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden]\n        C_hat = C_hat.permute(0,2,1,3)#[batch , con_len , sess_len , hidden]\n        C_h = self.Flow(C_hat , Sess_C_len) #[batch , sess_len , con_len , hidden * 2 + 1]\n        return C_h\n\nclass FullyAwareAttn(nn.Module):\n    def __init__(self , hidden_size , x_hidden_size , y_hidden_size):\n        super(FullyAwareAttn , self).__init__()\n        self.hidden_size = hidden_size\n        self.U_x = nn.Linear(in_features=x_hidden_size,out_features=hidden_size)\n        self.U_y = nn.Linear(in_features=y_hidden_size,out_features=hidden_size)\n        self.D = torch.diag(torch.rand(hidden_size)) #[hidden,hidden]\u7684\u5bf9\u89d2\u9635,torch.rand(10),size\u662f10\u7684\u5411\u91cf, \u30100\uff0c1)\u4e4b\u95f4\u7684\u5747\u5300\u5206\u5e03\n\n    def S(self ,C ,Q):\n        #C [batch , sess_len , con_len , hidden]\n        #Q [batch , sess_len , seq_len , hidden]\n        #a[i,j,k] = C[j][i] * Q[j][k] \u7b2cj\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u4e2d,C\u4e2d\u7684\u7b2ci\u4e2a\u5355\u8bcd\u4e0eQ\u4e2d\u7684\u7b2ck\u4e2a\u5355\u8bcd\u8ba1\u7b97\u7684\u76f8\u4f3c\u5ea6\n        #a = torch.matmul(C , Q.transpose(-2 , -1)) #[batch , sess_len , con_len , seq_len] \u662f\u8fd9\u4e2a\u610f\u601d\uff0c\u4f46\u662f\u4e0d\u662f\u7b80\u5355\u7684\u70b9\u4e58\u7b97\u76f8\u4f3c\u5ea6\n        C = F.relu(self.U_x(C))\n        Q = F.relu(self.U_y(Q))\n        temp = torch.matmul(C,self.D)\n        a = torch.matmul(temp , Q.transpose(-2,-1)) #[batch , sess_len , con_len , seq_len]\n        return a\n\n    def forward(self , C , Q , C_mask0 , Q_mask0):\n        # C = ( C_zero, C_one, C_two )\n        # Q = ( Q_naive , Q_lstm1 , Q_lstm2)\n        C_zero, C_one, C_two = C\n        embed_Q, Q_lstm_1, Q_lstm_2 = Q\n        Q = torch.cat((embed_Q, Q_lstm_1, Q_lstm_2),dim = -1)\n        C = torch.cat((C_zero, C_one, C_two), dim=-1)\n        batch_size , sess_len , con_len , hidden_size = C.size()\n        seq_len = Q.size(2)\n        #C_mask0 [batch , con_len]\n        #Q_mask0 [batch , sess_len , seq_len]\n        a = self.S(C,Q) #[batch , sess_len , con_len , seq_len]\n        C_mask0 = C_mask0 * (-1e10)\n        Q_mask0 = Q_mask0 * (-1e10)\n        C_mask0 = C_mask0.unsqueeze(1).unsqueeze(-1).repeat(1 , sess_len , 1 , seq_len)\n        Q_mask0 = Q_mask0.unsqueeze(-2).repeat(1,1,con_len,1)\n        a = a + Q_mask0 + C_mask0\n        a = F.softmax(a , dim = -1) #[batch , sess_len , con_len , seq_len(\u5f52\u4e00\u5316)]\n        Q_hat = torch.matmul(a , Q_lstm_2) #[batch , sess_len , con_len , hidden]\n        return Q_hat\n\nclass FlowQA(nn.Module):\n    def __init__(self , vocab_size , hidden_size , pad_token_index):\n        super(FlowQA,self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size= hidden_size\n\n        self.embedding = nn.Embedding(\n            num_embeddings=self.vocab_size,\n            embedding_dim=self.hidden_size,\n            padding_idx=pad_token_index\n        )\n\n        self.dotatthier = DotAttHier2(\n            in_hidden_size=hidden_size,\n            out_hidden_size = hidden_size\n        )\n\n        self.lstm_Q = NaiveLSTM(\n            input_size = hidden_size,\n            hidden_size = hidden_size,\n            num_layer= 1,\n            num_dir=2,\n            batch_first=True\n        )\n\n        self.hierQueEncoder = HierEncoderLSTM(\n            input_size = hidden_size,\n            hidden_size = hidden_size,\n            num_layer=1\n        )\n\n        self.IF1 = IF(\n            in_hidden_size = 2 * hidden_size + 1,\n            hidden_size = hidden_size\n        )\n\n        # print(\"IF1.hidden = \",self.IF1.hidden_size , \"IF1.in_hidden = \",self.IF1.in_hidden_size)\n\n        self.IF2 = IF(\n            in_hidden_size= 2 * hidden_size + 1,\n            hidden_size=hidden_size\n        )\n\n        # print(\"IF2.hidden = \", self.IF2.hidden_size, \"IF2.in_hidden = \", self.IF2.in_hidden_size)\n\n        self.FAT_On_Que = FullyAwareAttn(\n            hidden_size = hidden_size * 3 ,\n            x_hidden_size = (2 * hidden_size + 1) * 3,\n            y_hidden_size = (2 * 2 + 1) * hidden_size\n        )\n\n        self.IF3 = IF3(\n            in_hidden_size= 2 * hidden_size + 1 + 2 * hidden_size,\n            hidden_size = hidden_size\n        )\n\n        self.FAT_C_self = FullyAwareAttn(\n            hidden_size = hidden_size * 3,\n            x_hidden_size = (2 * hidden_size + 1) * 3,\n            y_hidden_size = (2 * hidden_size + 1) * 3\n        )\n\n        self.lstm_C = NaiveLSTM(\n            input_size=(2 * hidden_size + 1) * 2,\n            hidden_size=hidden_size,\n            num_layer=1,\n            num_dir=2,\n            batch_first=True\n        )\n\n        self.predAnsable_W = nn.Bilinear(\n            in1_features= 2 * (2 * hidden_size + 1),\n            in2_features= 2 * hidden_size,\n            out_features= 1\n        )\n\n        self.Ws = nn.Bilinear(\n            in1_features= 2 * hidden_size + 1,\n            in2_features= 2 * hidden_size,\n            out_features= 1\n        )\n\n        self.ptr_gru = nn.GRUCell(\n            input_size= 2 * hidden_size + 1,\n            hidden_size = 2 * hidden_size\n        )\n\n        self.We = nn.Bilinear(\n            in1_features= 2 * hidden_size + 1,\n            in2_features= 2 * hidden_size,\n            out_features= 1\n        )\n\n    def get_common(self, Q , C):\n        em = []\n        batch_size , sess_len , seq_len = Q.size()\n        con_len = C.size(1)\n        Q = Q.transpose(0,1) #[sess_len , batch , seq_len]\n        for index in range(sess_len):\n            temp = []\n            for i in range(batch_size):\n                tmp = [ 1 if ele in Q[index][i] else 0 for ele in C[i]]\n                tmp = FloatTensor(tmp)\n                tmp = temp.view(1, -1)\n                temp.append(tmp)\n            temp = torch.cat(tmp,dim=0)\n            temp = temp.view(1,batch_size,con_len)\n            em.append(temp)\n        em = torch.cat(em,dim=0) #[sess_len ,batch_size , con_len]\n        em = em.permute(1,0,2)\n        return em\n\n    def get_common2(self, Q ,C):\n        em = []\n        batch_size, sess_len, seq_len = Q.size()\n        con_len = C.size(1)\n        Q = Q.transpose(0, 1)  # [sess_len , batch , seq_len]\n        for index in range(sess_len):\n            temp = [[1 if ele in Q[index][i] else 0 for ele in C[i]] for i in range(batch_size)]\n            em.append(FloatTensor(temp).view(1,batch_size,con_len))\n        em = torch.cat(em, dim=0) #[sess_len ,batch_size , con_len]\n        em = em.permute(1,0,2)\n        return em\n\n    def predictAnswerable(self , C_fou , C_mask1 , Q_lstm_high , Sess_len_mask1):\n        #C_fou [batch , sess_len , con_len , 2 * hidden + 1]\n        #C_mask1 [batch , con_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\n        #Q_lstm_high [batch , sess_len , 2 * hidden]\n        #Sess_len_mask1 [batch , sess_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\n        batch_size , sess_len , con_len , _ = C_fou.size()\n        C_mask1 = C_mask1.unsqueeze(1).unsqueeze(-1).repeat(1,sess_len,1)#[batch , sess_len , con_len , 1]\n        C_fou = C_fou * C_mask1\n\n        Sess_len_mask1 = Sess_len_mask1.unsqueeze(-1) #[batch , sess_len , 1]\n        Q_lstm_high = Q_lstm_high * Sess_len_mask1\n\n        C_max = C_fou.max(-2)[0] #[batch , sess_len , 2 * hidden + 1]\n        C_sum = C_fou.sum(-2) #[batch , sess_len , 2 * hidden + 1]\n        C = torch.cat((C_max , C_sum) , dim = -1)  #[batch , sess_len , 2 * (2 * hidden + 1)]\n\n        pred = self.predAnsable_W(C , Q_lstm_high) #[batch , sess_len , 1]\n        Sess_len_mask0 = 1 - Sess_len_mask1\n        Sess_len_mask0 = Sess_len_mask0 * (-1e10)\n        pred = pred + Sess_len_mask0\n        pred = pred.view(batch_size , sess_len)\n        pred = F.softmax(pred,dim=-1)\n        return pred , Sess_len_mask1.squeeze(-1)\n\n    def predictSpan(self , C_fou , C_mask1 , Q_lstm_high , Sess_len_mask1 , max_ans_len = 15):\n        # C_fou [batch , sess_len , con_len , 2 * hidden + 1]\n        # C_mask1 [batch , con_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\n        # Q_lstm_high [batch , sess_len , 2 * hidden]\n        # Sess_len_mask1 [batch , sess_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\n        batch_size, sess_len, con_len, _ = C_fou.size()\n        C_mask1 = C_mask1.unsqueeze(1).unsqueeze(-1).repeat(1, sess_len, 1)  # [batch , sess_len , con_len , 1]\n        C_fou = C_fou * C_mask1\n\n        Sess_len_mask1 = Sess_len_mask1.unsqueeze(-1)  # [batch , sess_len , 1]\n        Q_lstm_high = Q_lstm_high * Sess_len_mask1\n\n        Q_lstm_high = Q_lstm_high.unsqueeze(2).repeat(1,1,con_len,1) #[batch , sess_len , con_len , 2 * hidden]\n        P_S = self.Ws(C_fou,Q_lstm_high) #[batch , sess_len , con_len , 1]\n        P_S = (P_S + (1 - C_mask1) * (-1e10)).squeeze(-1) #[batch , sess_len , con_len]\n        P_S = F.softmax(P_S , -1)\n\n        C_fou_i = (C_fou * P_S.unsqueeze(-1)).view(batch_size * sess_len ,con_len , -1).sum(1) #[batch * sess_len , 2 * hidden + 1]\n        Q_lstm_high_h_hat = self.ptr_gru(input = C_fou_i , hidden = Q_lstm_high[:,:,0,:].view(batch_size * sess_len , -1))\n        #[batch * sess_len , 2 * hidden]\n        Q_lstm_high_h_hat = Q_lstm_high_h_hat.view(batch_size , sess_len , -1)\n\n        Q_lstm_high_h_hat = Q_lstm_high_h_hat.unsqueeze(2).repeat(1,1,con_len,1)\n        P_E = self.We(C_fou , Q_lstm_high_h_hat) #[batch , sess_len , con_len , 1]\n        P_E = P_E + ((1 - C_mask1) * (-1e10)).squeeze(-1) #[batch , sess_len , con_len]\n        P_E = F.softmax(P_E , -1)\n\n        return  P_S, P_E , C_mask1.squeeeze(-1)\n\n\n    def forward(self, Q , C , Q_len , Sess_len , C_len , threadhold = 0.5, max_ans_len = 15):\n        #Q [batch , sess_len , seq_len]\n        #C [batch , context_len]\n        #Q_len [batch , sess_len]\n        #Sess_len [batch]\n        #C_len [batch]\n        embed_Q = self.embedding(Q).to(device=device)\n        embed_C = self.embedding(C).to(device=device)\n        print(\"device = \",device)\n        batch_size , sess_len , seq_len , in_hidden_size = embed_Q.size()\n        con_len = C.size(1)\n        Q_mask0 = get_mask(Q_len, seq_len, mask_value=0) #[batch , sess_len ,seq_len]\n        C_mask0 = get_mask(C_len, con_len, mask_value=0) #[batch , con_len]\n        Q_hat = self.dotatthier(embed_Q,embed_C,Q_mask0,C_mask0)#[batch ,sess_len , con_len , in_hidden]\n        em = self.get_common2(Q , C) #[batch , sess_len , con_len]\n        em = em.view(batch_size,sess_len,con_len,1)\n        C_zero = torch.cat((embed_C.view(batch_size,1,con_len,in_hidden_size).repeat(1,sess_len,1,1), em , Q_hat) , dim = -1)\n        #[batch , sess_len , con_len , in_hidden + 1 + in_hidden]\n        print(\"C_zero.shape = \",C_zero.shape)\n\n        C_zero_len = C_len.unsqueeze(1).repeat(1 , sess_len)\n        #[batch , sess_len]\n        Sess_C_len = Sess_len.unsqueeze(1).repeat(1 , con_len)\n        #[batch , con_len]\n        C_one = self.IF1(C_zero , C_zero_len , Sess_C_len) ##[batch , sess_len , con_len , hidden * 2 + 1]\n        C_two = self.IF2(C_one , C_zero_len , Sess_C_len)\n\n        Q_lstm_1, (h_n, c_n) = self.lstm_Q(input = embed_Q.view(-1 , seq_len ,in_hidden_size) , input_length = Q_len.view(-1))\n        #[batch * sess_len , seq_len , num_dir * hidden]\n        Q_lstm_1 = Q_lstm_1.view(batch_size , sess_len , seq_len , -1)\n        Q_lstm_2 , Q_lstm_high = self.hierQueEncoder(Q = Q_lstm_1 , Q_len = Q_len , Sess_len = Sess_len , Q_mask0 = Q_mask0)\n        #[batch , sess_len , seq_len , 2 * hidden]  [batch , sess_len , 2 * hidden]\n\n        Q_attned =  self.FAT_On_Que( (C_zero, C_one, C_two) , (embed_Q, Q_lstm_1, Q_lstm_2) , C_mask0 , Q_mask0)\n        # [batch , sess_len , con_len , 2 * hidden]\n        temp = torch.cat([C_two , Q_attned] , dim = -1)\n        C_thr = self.IF3(temp , C_zero_len , Sess_C_len)\n        C_attend =  self.FAT_C_self( (C_one , C_two , C_thr) , (C_one , C_two , C_thr) , C_mask0 , C_mask0)\n\n        C_fou = torch.cat([C_thr , C_attend] , dim = -1) # [batch , sess_len , con_len , 2 * (2 * hidden + 1)]\n        C_fou = C_fou.view(batch_size * sess_len , con_len , -1)\n        C_fou = self.lstm_C(C_fou, C_zero_len.view(-1))\n        C_fou = C_fou.view(batch_size , sess_len , con_len , -1)\n\n        P_answerable , Sess_len_mask1 = self.predictAnswerable( C_fou , (1 - C_mask0) , Q_lstm_high , get_mask(Sess_len , sess_len ,mask_value=1))\n        #[batch , sess_len]\n        P_answerable_yes_no = P_answerable >= threadhold #\u4e0d\u4e00\u5b9a\u8981\u4e0d\u8981\u6ce8\u91ca\u6389\u5b83\n        P_start , P_end , P_mask1 = self.predictSpan(C_fou,(1 - C_mask0) , Q_lstm_high , Sess_len_mask1 , max_ans_len)\n        #[batch , sess_len , con_len]\n\n        return P_answerable , Sess_len_mask1 , P_start , P_end , P_mask1\n\ndef train(model , otherInputs , start_label , end_label , criterion , optimizer):\n    #start_label = [batch * sess_len] tensor \u7c7b\u578b\u7684\n    #criterion = nn.NLLLoss()\n    '''\n    \u5728\u8fd9\u91cc\u8c03\u7528model\u6a21\u5757\n    '''\n    (Q, C, Q_len, Sess_len, C_len) = otherInputs\n\n    P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(torch.LongTensor(Q) ,torch.LongTensor(C) ,LongTensor(Q_len) ,LongTensor(Sess_len) ,LongTensor (C_len))\n    # [batch , sess_len]          #[batch , sess_len , con_len]\n    loss = 0\n    ones = torch.ones(len(start_label))\n    batch_size , sess_len , con_len = P_start.size()\n    P_start = P_start.view(-1 , con_len)\n    P_end = P_end.view(-1,con_len)\n    #\u5bf9\u4e8eloss\u6709\u4e24\u79cd\u64cd\u4f5c\uff0c1\uff0c\u53ea\u9009\u62e9\u6b63\u786e\u7684\u4f4d\u7f6e\u7684\u6982\u7387\uff0c2\uff0csubsample\u4e00\u4e9b\u9519\u8bef\u7684\u4f4d\u7f6e\u7684\u6982\u7387\u6765\u8fdb\u884c\u8ba1\u7b97\u3002\u8fdb\u884c2\u7684\u65f6\u5019\u600e\u4e48\u6837\u4fdd\u8bc11\u4e2d\u7684\u4e0d\u88ab\u518d\u9009\u4e2d\u5462 \uff0c  \u53c8\u600e\u4e48\u4fdd\u8bc1mask\u7684\u4e1c\u897f\u4e0d\u88ab\u9009\u4e2d\u5462\u3002\n\n    #\u5b8c\u62101\n    start = FloatTensor( [ P_start[i][start_label[i]] for i in range(batch_size * sess_len) ] )\n    start_neg = 1 - start\n    start = start.view(-1,1)\n    start_neg = start_neg.view(-1,1)\n    start_p = torch.cat((start_neg , start) , dim = -1)\n    loss += criterion(start_p , torch.ones(batch_size * sess_len))\n\n    end = FloatTensor([P_end[i][end_label[i]] for i in range(batch_size * sess_len)])\n    end_neg = 1 - end\n    end = end.view(-1, 1)\n    end_neg = end_neg.view(-1, 1)\n    end_p = torch.cat((end_neg, end), dim=-1)\n    loss += criterion(end_p, torch.ones(batch_size * sess_len))\n\n    loss.backward()\n    optimizer.step()\n\n    return loss.item / batch_size\n\ndef reload(args,PATH):\n    flowQA_model = FlowQA()\n    flowQA_optimizer = optim.Adamax(flowQA_model.parameters(), lr=0.1)\n    checkpoint = torch.load(PATH)\n    flowQA_model.load_state_dict(checkpoint['model_state_dict'])\n    flowQA_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n    # flowQA_model.eval()\n    # flowQA_model.train() \u4e3a\u7684\u662f\u8ba9\u6a21\u578b\u53c2\u6570\u53ef\u4ee5\u88ab\u8bad\u7ec3\n    return flowQA_model,flowQA_optimizer\n\ndef trainIters(in_file = \"/data1/zhengquan/data/CoQA/subTrain_preproed.json\", n_epochs = 20,  print_every=1000, plot_every=100, save_every = 100,learning_rate=0.01, batch_size = 2):\n    Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label, lang = getData(in_file)\n    print(\"data load finished\")\n    flowQA_model = FlowQA(vocab_size = lang.n_words , hidden_size = 100, pad_token_index = lang.word2id['<PAD>'] )\n    criterion = nn.CrossEntropyLoss(size_average=True, reduce=False)\n    flowQA_optimizer = optim.Adamax(flowQA_model.parameters(), lr=0.1)\n    '''\n    OtherInputs \u5305\u62ec\n    Q , C , Q_len , Sess_len , C_len \n    #Q [batch , sess_len , seq_len]\n    #C [batch , context_len]\n    #Q_len [batch , sess_len]\n    #Sess_len [batch]\n    #C_len [batch]\n    '''\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n    inner_loop_iter = 0\n    for iter in range(1, n_epochs + 1):\n        '''\n        \u8fd9\u91cc\u7684otherInputs\u548cstart_label,end_label,\u624d\u662f\u771f\u6b63\u6309\u7167batch\u7f51\u91cc\u9762\u653e\u7684\uff0c\u73b0\u5728\u624d\u771f\u7684\u9700\u8981\u5728sess_len, seq_len\u6216context_len\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8865\u9f50,\u4fdd\u8bc1\u6bcf\u4e2abatch\u5185\u7684\u6570\u636e\u683c\u5f0f\u4e00\u81f4\uff0c\n        '''\n        data = getBatchData((Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label))\n        while True:\n            temp_data = next(data ,None)\n            if temp_data is None:\n                break\n            #inner_loop_iter += 1\n            inner_loop_iter += batch_size\n            Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label = temp_data\n            OtherInpus = (Q , C , Q_len , Sess_len , C_len)\n            loss = train(flowQA_model , OtherInpus , start_label , end_label , criterion , flowQA_optimizer)\n            print_loss_total += loss\n            plot_loss_total += loss\n\n            if inner_loop_iter % print_every == 0:\n                print_loss_avg = print_loss_total / print_every\n                print_loss_total = 0\n                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\n                                             iter, iter / n_epochs * 100, print_loss_avg))\n\n            if inner_loop_iter % plot_every == 0:\n                plot_loss_avg = plot_loss_total / plot_every\n                plot_losses.append(plot_loss_avg)\n                plot_loss_total = 0\n\n            if inner_loop_iter % save_every == 0:\n                temp_num = inner_loop_iter\n                path_to_save = os.path.join(DIR_TO_SAVE,\"%d\"%(temp_num))\n                while os.path.exists(path_to_save):\n                    temp_num += 1\n                    path_to_save = os.path.join(DIR_TO_SAVE, \"%d\" % (temp_num))\n                torch.save({'epoch':iter,\n                            'model_state_dict':flowQA_model.state.dict(),\n                            'optimizer_state_dict':flowQA_optimizer.state_dict(),\n                            'loss':loss}\n                           ,path_to_save)\n\n    showPlot(plot_losses)\n\ndef evaluate(model , otherInputs , start_label , end_label):\n    with torch.no_grad():\n        P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(otherInputs)\n    return P_start , P_end\n\nif __name__ == \"__main__\":\n    # in_file  = \"/data1/zhengquan/data/coqa-train-v1.0.json\"\n    # Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label, lang = getData(in_file)\n    # print(lang.n_words)\n    trainIters()", "body": "## \ud83d\udc1b Bug\r\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.i am using pytorch 0.4.1 with anaconda3\r\n1.run my code I got the following error:\r\nTraceback (most recent call last):\r\n  File \"rc_model.py\", line 849, in <module>\r\n    trainIters()\r\n  File \"rc_model.py\", line 811, in trainIters\r\n    loss = train(flowQA_model , OtherInpus , start_label , end_label , criterion , flowQA_optimizer)\r\n  File \"rc_model.py\", line 737, in train\r\n    P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(torch.LongTensor(Q) ,torch.LongTensor(C) ,LongTensor(Q_len) ,LongTensor(Sess_len) ,LongTensor (C_len))\r\n  File \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"rc_model.py\", line 701, in forward\r\n    C_one = self.IF1(C_zero , C_zero_len , Sess_C_len) ##[batch , sess_len , con_len , hidden * 2 + 1]\r\n  File \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"rc_model.py\", line 376, in forward\r\n    C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden * 2]\r\n  File \"rc_model.py\", line 347, in Context_Integration\r\n    C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)\r\n  File \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\", line 192, in forward\r\n    output, hidden = func(input, self.all_weights, hx, batch_sizes)\r\n  File \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/_functions/rnn.py\", line 324, in forward\r\n    return func(input, *fargs, **fkwargs)\r\n  File \"/home/zhengquan/anaconda3/lib/python3.7/site-packages/torch/nn/_functions/rnn.py\", line 288, in forward\r\n    dropout_ts)\r\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch\r\n\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:CUDA Version 9.0.176\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 0\r\n#define CUDNN_PATCHLEVEL 3\r\n--\r\n#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n\r\n#include \"driver_types.h\"\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nmy all code is as follows\r\n\r\n```python\r\n# -*- coding:utf-8 -*-\r\n'''\"\r\nseq2seq_translation_tutorial.py -> seq2seq_add_batch_dimension.py \u5c06\u539f\u6765\u7684\u4ee3\u7801\u4e2dbatch\u7ef4\u5ea6\u4ece1\uff0c\u589e\u52a0\u5230n>1,\u540c\u65f6\u9047\u5230\u7684\u9ebb\u70e6\uff0cpadding\u4e0d\u540c\u957f\u5ea6\u5230\u540c\u610f\u957f\u5ea6\uff0cnn.GRU\u4e2d\u6709\u4e24\u4e2a\u548cpad_pack\u76f8\u5173\u7684\u51fd\u6570\r\n            embedded_input = torch.nn.utils.rnn.pack_padded_sequence(embedded_input,input_length,batch_first=True)#lengths' array has to be sorted in decreasing order\r\n            \u653e\u5165RNN\u4e4b\u524d\u5bf9\u6570\u636e\u8fdb\u884c\u5c01\u88c5\uff0c\r\n            output , _ = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\r\n            \u5bf9rnn\u7684\u8f93\u51fa\u8fdb\u884c\u89e3\u5c01\u88c5\u3002\r\n\r\nseq2seq_add_batch_dimension.py -> seq2seq_add_hierarchy_rnn_encoder.py \u589e\u52a0\u4e86\u5c42\u7ea7rnn\u7684\u529f\u80fd\u5bf9dialogue\u53ef\u4ee5\u7f16\u7801 \u300c\u300d \u8fd9\u6b21\u5148\u4ece\u6570\u636e\u5f00\u59cb\u641e\r\n\r\n\"\"\"\r\nfrom __future__ import unicode_literals, print_function, division\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport torch.nn.functional as F\r\nfrom torch import optim\r\nimport time\r\nimport math\r\nfrom getData import getData , getBatchData\r\nimport matplotlib.pyplot as plt\r\nplt.switch_backend('agg')\r\nimport matplotlib.ticker as ticker\r\n\r\nUSE_CUDA = torch.cuda.is_available()\r\nif USE_CUDA:\r\n    device = torch.device(\"cuda\")\r\nelse:\r\n    device = torch.device(\"cpu\")\r\nprint(\"device = \",device)\r\ngpus = [0]\r\n# torch.cuda.set_device(gpus[0])\r\nFloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\r\nLongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\r\nByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\r\n\r\ndef asMinutes(seconds):\r\n    minutes = math.floor(seconds/60)\r\n    seconds -= minutes * 60\r\n    return '%d minutes %d seconds'%(minutes , seconds)\r\n\r\ndef timeSince(since, percent):\r\n    '''\r\n    :param percent = iter_right_now / total_iter\r\n    '''\r\n    now = time.time()\r\n    seconds = now - since\r\n    estimate_seconds = seconds / (percent)\r\n    remain_seconds = estimate_seconds - seconds\r\n    return '%s (-%s)' %(asMinutes(seconds) , asMinutes())\r\n\r\ndef showPlot(points):\r\n    plt.figure()\r\n    fig, ax = plt.subplots()\r\n    # this locator puts ticks at regular intervals\r\n    loc = ticker.MultipleLocator(base=0.2)\r\n    ax.yaxis.set_major_locator(loc)\r\n    plt.plot(points)\r\n\r\nPAD_token = 0\r\nSOS_token = 1\r\nEOS_token = 2\r\nMAX_LENGTH = 10\r\nDIR_TO_SAVE = \"/data1/zhengquan/data/CoQA/saved_models\"\r\nif not os.path.exists(DIR_TO_SAVE):\r\n    os.makedirs(DIR_TO_SAVE)\r\n\r\nclass Embed(nn.Module):\r\n    def __init__(self,hidden_size = 100,pad_token_index = 0):\r\n        super(Embed,self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.embedding = nn.Embedding(\r\n            num_embeddings=1000,\r\n            embedding_dim=self.hidden_size,\r\n            padding_idx=pad_token_index\r\n        )\r\n\r\n    def forward(self , input):\r\n        return self.embedding(input)\r\n\r\nclass DotAtt(nn.Module):\r\n    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):\r\n        super(DotAtt, self).__init__()\r\n        self.in_hidden_size = in_hidden_size\r\n        self.out_hidden_size = out_hidden_size\r\n        self.W = nn.Linear(self.in_hidden_size, self.out_hidden_size)\r\n\r\n    def forward(self , qi, c , qi_length , c_length):\r\n        #qi [batch , seq_len , in_hidden]\r\n        #c [batch , con_len , in_hidden]\r\n        #qi_length = [batch]\r\n        #c_length = [batch]\r\n        batch_size , seq_len  = qi.size()[:2]\r\n        con_len = c.size(1)\r\n        mask_qi = torch.ones(batch_size , seq_len)\r\n        mask_c = torch.ones(batch_size , con_len)\r\n        for index , (qi_l , ci_l) in range(zip(qi_length , c_length)):\r\n            mask_qi[index , :qi_l] = 0\r\n            mask_c[index , :ci_l] = 0\r\n        mask_c *= (-1e10)\r\n        mask_qi *= -1e10\r\n\r\n        mask_c = mask_c.view(1,batch_size,con_len).repeat(seq_len,1,1)\r\n        mask_qi = mask_qi.view(1,batch_size ,seq_len).repeat(con_len,1,1)\r\n\r\n\r\n        qi = F.relu(self.W(qi))\r\n        c = F.relu(self.W(c))\r\n        attn = torch.matmul(c ,qi.transpose(-2,-1)) #[batch , con_len , seq_len]\r\n        attn = attn.permute(1,0,2) #[con_len , batch , seq_len]\r\n        attn = attn + mask_qi\r\n        attn = attn.permute(1,0,2).permute(2,0,1) #[seq_len , batch , con_len]\r\n        attn = attn + mask_c\r\n        attn = attn.permute(1,2,0)#[batch , con_len , seq_len]\r\n        attn = F.softmax(attn ,dim = -1)\r\n        qi_hat = torch.matmul(attn , qi) #[batch , con_len , out_hidden]\r\n        return qi_hat\r\n\r\nclass DotAttHier(nn.Module):\r\n    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):\r\n        super(DotAttHier , self).__init__()\r\n        self.in_hidden_size = in_hidden_size\r\n        self.out_hidden_size = out_hidden_size\r\n        self.dotatt = DotAtt(in_hidden_size , out_hidden_size)\r\n\r\n    def forward(self, Q , C , Q_len , C_len):\r\n        #Q[batch , sess_len , seq_len , in_hidden]\r\n        #C[batch , con_len , in_hidden]\r\n        #Q_len[batch , sess_len]\r\n        #C_len[batch]\r\n        Q = Q.permute(1,0,2,3) #[sess_len , batch , seq_len , in_hidden]\r\n        Q_len = Q_len.permute(1,0)#[sess_len , batch]\r\n        sess_len ,batch_size , seq_len , in_hidden = Q.size()\r\n        con_len = C.size(1)\r\n        Q_hat = []\r\n        for index in range(sess_len):\r\n            Q_hat.append(self.dotatt(Q[index],C , Q_len[index] , C_len).view(1 , con_len , seq_len , -1))\r\n            # [1 , batch , con_len , out_hidden]\r\n        Q_hat = torch.cat(Q_hat , dim = 0).permute(1,0,2,3) #[batch ,sess_len , con_len , in_hidden]\r\n        return Q_hat\r\n\r\nclass DotAttHier2(nn.Module):\r\n    '''\u76f4\u63a5\u8fdb\u884cHierattn\uff0c\u4e0d\u4f1a\u5728sess_len\u7ef4\u5ea6\u4e0a\u5faa\u73af'''\r\n    def __init__(self , in_hidden_size = 100 , out_hidden_size = 100):\r\n        super(DotAttHier2 , self).__init__()\r\n        self.in_hidden_size = in_hidden_size\r\n        self.out_hidden_size = out_hidden_size\r\n        self.W = nn.Linear(self.in_hidden_size, self.out_hidden_size).to(device)\r\n\r\n    def forward(self , Q , C , Q_mask , C_mask):\r\n        #Q_len[batch , sess_len]\r\n        batch_size , sess_len , seq_len , in_hidden = Q.size()\r\n        con_len = C.size(1)\r\n\r\n        Q = Q.view(batch_size * sess_len , seq_len , in_hidden)\r\n        C = C.repeat(sess_len , 1, 1)\r\n        Q = F.relu(self.W(Q))\r\n        C = F.relu(self.W(C))\r\n        attn = torch.matmul(C, Q.transpose(-2, -1))  # [batch * sess_len , con_len , seq_len]\r\n        Q_mask = Q_mask * (-1e10) #[batch , sess_len , seq_len]\r\n        Q_mask = Q_mask.view(-1 , seq_len).unsqueeze(1).repeat(1 ,con_len , 1) #[batch * sess_len , con_len , seq_len]\r\n        C_mask = C_mask * (-1e10) #[batch , con_len]\r\n        C_mask = C_mask.unsqueeze(-1).repeat(sess_len , 1 , seq_len) #[batch * sess_len , con_len , seq_len]\r\n        attn = attn + C_mask + Q_mask\r\n        attn = F.softmax(attn, dim=-1) #[batch * sess_len , con_len , seq_len(\u5f52\u4e00\u5316)]\r\n        Q_hat = torch.matmul(attn , Q) #[batch * sess_len , con_len , in_hidden]\r\n        Q_hat = Q_hat.view(batch_size , sess_len , con_len , in_hidden)\r\n        return Q_hat\r\n\r\nclass NaiveLSTM(nn.Module):\r\n    def __init__(self , input_size , hidden_size , batch_first = True,\r\n                 num_layer = 1, num_dir = 2,\r\n                 batch_size = 10 , dropout_p = 0.5,\r\n                 max_length = MAX_LENGTH):\r\n        super(NaiveLSTM,self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.batch_first = batch_first\r\n        self.num_layer = num_layer\r\n        self.num_dir = num_dir\r\n        self.batch_size = batch_size\r\n        self.dropout_p = dropout_p\r\n        self.lstm  = nn.LSTM(\r\n            input_size=input_size,\r\n            hidden_size=hidden_size,\r\n            num_layers=num_layer,\r\n            bidirectional=num_dir > 1,\r\n            batch_first=batch_first,\r\n            dropout=dropout_p\r\n        )\r\n\r\n    def forward(self, input , input_length):\r\n        #input = [batch * sess_len , seq_len , hidden_size]\r\n        #input_length = [batch * sess_len]\r\n\r\n        sorted_idx = np.argsort(-input_length)\r\n        input = input[sorted_idx]\r\n        input_length = input_length[sorted_idx]\r\n\r\n        input = torch.nn.utils.rnn.pack_padded_sequence(input, input_length, batch_first=self.batch_first)\r\n        out, (h_n, c_n) = self.lstm(input=input)\r\n        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        out = out[unsorted_idx]  # [batch_size * sess_len, seq_len , num_dir * hidden_size]\r\n        return out\r\n\r\nclass HierEncoderLSTM(nn.Module):\r\n    def __init__(self ,input_size, hidden_size , batch_first = True,\r\n                 num_layer = 2 , num_dir = 2,\r\n                 batch_size = 10 , dropout_p = 0.5,\r\n                 max_length = MAX_LENGTH):\r\n        super(HierEncoderLSTM,self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.batch_first = batch_first\r\n        self.num_layer = num_layer\r\n        self.num_dir = num_dir\r\n        self.batch_size = batch_size\r\n        self.dropout_p = dropout_p\r\n\r\n        self.lstm = nn.LSTM(\r\n            input_size = input_size,\r\n            hidden_size = hidden_size,\r\n            num_layers = num_layer,\r\n            bidirectional= num_dir > 1,\r\n            batch_first = batch_first,\r\n            dropout = dropout_p\r\n        )\r\n\r\n        self.w  = nn.Linear(\r\n            num_dir * hidden_size,\r\n            1\r\n        )\r\n\r\n        self.high_lstm = nn.LSTM(\r\n            input_size = hidden_size * num_dir,\r\n            hidden_size = hidden_size * num_dir,\r\n            num_layers = 1,\r\n            batch_first = batch_first,\r\n            dropout = dropout_p\r\n        )\r\n\r\n    def forward(self , Q , Q_len , Sess_len , Q_mask0):\r\n        batch_size , sess_len , seq_len , input_size = Q.size()\r\n        #Q_len = [batch , sess_len]\r\n        #Q_mask0 = [batch , sess_len , seq_len]\r\n        Q = Q.view(-1 , seq_len , input_size)\r\n        Q_len = Q_len.view(-1)\r\n\r\n        sorted_idx = np.argsort(-Q_len)\r\n        out = Q[sorted_idx]\r\n        out_len = Q_len[sorted_idx]\r\n\r\n        out = torch.nn.utils.rnn.pack_padded_sequence(out,out_len , batch_first=self.batch_first)\r\n        out, (h_n, c_n) = self.lstm(input=out)\r\n        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        out = out[unsorted_idx] #[batch_size * sess_len, seq_len , self.num_dir * self.hidden_size]\r\n\r\n        attn = self.w(out) #[batch * sess_len , seq_len , 1]\r\n        Q_mask0 = Q_mask0 * (-1e10)\r\n        Q_mask0 = Q_mask0.view(-1,seq_len).unsqueeze(-1) #[batch * sess_len , seq_len , 1]\r\n        attn = (attn + Q_mask0).transpose(-1,-2) #[batch * sess_len , 1 , seq_len]\r\n        high_out = torch.matmul(attn , out) #[batch * sess_len  , 1 , num_dir * hidden]\r\n        high_out = high_out.squeeze(1).view(batch_size , sess_len , -1)\r\n\r\n        sorted_idx = np.argsort(-Sess_len)\r\n        high_out = high_out[sorted_idx]\r\n        Sess_len = Sess_len[sorted_idx]\r\n\r\n        high_out = torch.nn.utils.rnn.pack_padded_sequence(high_out , Sess_len, batch_first = self.batch_first)\r\n        high_out , (h_n , c_n) = self.high_lstm(input = high_out)\r\n        high_out , _ = torch.nn.utils.rnn.pad_packed_sequence(high_out , batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        high_out = high_out[unsorted_idx] #[batch , sess_len , num_dir * hidden]\r\n        return out , high_out\r\n\r\ndef get_mask(input_length , max_seq_len , mask_value = 0):\r\n    #[batch ] or [batch , sess_len]\r\n    #mask_value = 0 ,\u6709\u503c\u7684\u5730\u65b9\u4e3a0 \uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a1\r\n    #mask_value = 1 ,\u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\r\n    #for situation 1\r\n    siz = []\r\n    for i in range(len(input_length.size())):\r\n        siz.append(input_length.size(i))\r\n    siz.append(max_seq_len)\r\n    if mask_value == 0:\r\n        mask = torch.zeros(siz).to(device) #[batch, max_seq_len] or[batch , sess_len ,max_seq_len]\r\n    if mask_value == 1:\r\n        mask = torch.ones(siz).to(device)\r\n    input_length = input_length.view(-1) #[batch] or [batch * sess_len]\r\n    mask = mask.view(-1 , max_seq_len)\r\n    for index ,l in enumerate(input_length):\r\n        if mask_value == 0:\r\n            mask[index , l:] = 1\r\n        elif mask_value == 1:\r\n            mask[index , l:] = 0\r\n    mask = mask.view(siz)\r\n    return mask\r\n\r\nclass IF(nn.Module):\r\n    def __init__(self , in_hidden_size , hidden_size ,\r\n                 num_dir =2 , batch_first = True,\r\n                 dropout_p = 0.5 , num_layer = 1):\r\n        super(IF,self).__init__()\r\n        # print(\"in_IF_Class\")\r\n        # print(\"in_hidden_size= \",in_hidden_size)\r\n        # print(\"hidden_size = \",hidden_size)\r\n\r\n        self.in_hidden_size = in_hidden_size\r\n        self.hidden_size = hidden_size\r\n        self.batch_first = batch_first\r\n        self.lstm = nn.LSTM(\r\n            input_size=in_hidden_size,\r\n            hidden_size=hidden_size//2,\r\n            num_layers=num_layer,\r\n            bidirectional=num_dir > 1,\r\n            batch_first=batch_first,\r\n            dropout=dropout_p\r\n        )\r\n        try :\r\n            assert in_hidden_size == hidden_size * 2 + 1\r\n        except:\r\n            print(\"in_hidden_size = \",in_hidden_size)\r\n            print(\"hidden_size = \",hidden_size)\r\n            exit(87909)\r\n\r\n        self.gru = nn.GRU(\r\n            input_size = hidden_size * num_dir,\r\n            hidden_size = in_hidden_size - hidden_size // 2 * 2,\r\n            batch_first= batch_first,\r\n            dropout= dropout_p,\r\n        )\r\n\r\n    def Context_Integration(self , C_h_1 , C_h_len):\r\n        #C_h_1 [batch , sess_len , con_len , in_hidden]\r\n        #C_h_len [batch , sess_len]\r\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\r\n        C_h_1 = C_h_1.view(-1 , con_len , in_hidden_size)\r\n        C_h_len = C_h_len.view(-1)\r\n        sorted_idx = np.argsort(-C_h_len)\r\n        C_h_1 = C_h_1[sorted_idx]\r\n        C_h_len = C_h_len[sorted_idx]\r\n        print(\"C_h_len = \",C_h_len)\r\n        C_h_1 = torch.nn.utils.rnn.pack_padded_sequence(C_h_1 , C_h_len , batch_first=self.batch_first)\r\n        C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)\r\n        C_h_1 , _  = torch.nn.utils.rnn.pad_packed_sequence(C_h_1,batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        C_h_1 = C_h_1[unsorted_idx]\r\n        C_h_1 = C_h_1.view(batch_size,sess_len,con_len,-1)\r\n        return C_h_1\r\n\r\n    def Flow(self , C_h , Sess_C_len):\r\n        #Sess_C_len [batch , con_len]\r\n        batch_size , con_len , sess_len , hidden = C_h.size()\r\n        C_h = C_h.view(-1,sess_len,hidden)\r\n        Sess_C_len = Sess_C_len.view(-1)\r\n        sorted_idx = np.argsort(-Sess_C_len)\r\n        C_hat = C_h[sorted_idx]\r\n        Sess_C_len = Sess_C_len[sorted_idx]\r\n        C_hat = torch.nn.utils.rnn.pack_padded_sequence(C_hat, Sess_C_len, batch_first=self.batch_first)\r\n        C_hat , (h_n , c_n) = self.gru(input = C_hat)\r\n        C_hat , _ = torch.nn.utils.rnn.pad_packed_sequence(C_hat , batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        C_hat = C_hat[unsorted_idx] #[batch * con_len , sess_len , hidden]\r\n        C_h_plus_1 = torch.cat((C_h,C_hat) , dim = -1) #[batch * con_len , sess_len , hidden]\r\n        C_h_plus_1 = C_h_plus_1.view(batch_size , con_len , sess_len , -1).permute(0,2,1,3)\r\n        return C_h_plus_1 #[batch , sess_len , con_len , in_hidden_size]\r\n\r\n    def forward(self , C_h_1 , C_h_len , Sess_C_len):\r\n        #C_h_len [batch , sess_len]\r\n        #Sess_C_len [batch , con_len]\r\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\r\n        # C_h_1 = C_h_1.view( -1, con_len , in_hidden_size)\r\n        C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden * 2]\r\n        C_hat = C_hat.permute(0,2,1,3)#[batch , con_len , sess_len , 2 * hidden]\r\n        C_h = self.Flow(C_hat , Sess_C_len) #[batch , sess_len , con_len , hidden * 2 + 1]\r\n        return C_h\r\n\r\nclass IF3(nn.Module):\r\n    def __init__(self , in_hidden_size , hidden_size ,\r\n                 num_dir =2 , batch_first = True,\r\n                 dropout_p = 0.5 , num_layer = 1):\r\n        super(IF3,self).__init__()\r\n        # print(\"in_IF3_Class\")\r\n        # print(\"in_hidden_size= \",in_hidden_size)\r\n        # print(\"hidden_size = \",hidden_size)\r\n\r\n        self.in_hidden_size = in_hidden_size\r\n        self.hidden_size = hidden_size\r\n        self.batch_first = batch_first\r\n        self.lstm = nn.LSTM(\r\n            input_size=in_hidden_size,\r\n            hidden_size=hidden_size//2,\r\n            num_layers=num_layer,\r\n            bidirectional=num_dir > 1,\r\n            batch_first=batch_first,\r\n            dropout=dropout_p\r\n        )\r\n        try :\r\n            assert in_hidden_size == hidden_size * 2 + 1 + hidden_size * 2\r\n        except:\r\n            print(\"in_hidden = \",in_hidden_size)\r\n            print(\"hidden = \",hidden_size)\r\n            exit(6789)\r\n\r\n        self.gru = nn.GRU(\r\n            input_size = hidden_size * num_dir,\r\n            hidden_size = in_hidden_size - hidden_size // 2 * 2,\r\n            batch_first= batch_first,\r\n            dropout= dropout_p,\r\n        )\r\n\r\n    def Context_Integration(self , C_h_1 , C_h_len):\r\n        #C_h_1 [batch , sess_len , con_len , in_hidden]\r\n        #C_h_len [batch , sess_len]\r\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\r\n        C_h_1 = C_h_1.view(-1 , con_len , in_hidden_size)\r\n        C_h_len = C_h_len.view(-1)\r\n        sorted_idx = np.argsort(-C_h_len)\r\n        C_h_1 = C_h_1[sorted_idx]\r\n        C_h_len = C_h_len[sorted_idx]\r\n        C_h_1 = torch.nn.utils.rnn.pack_padded_sequence(C_h_1 , C_h_len , batch_first=self.batch_first)\r\n        C_h_1, (h_n, c_n) = self.lstm(input = C_h_1)\r\n        C_h_1 , _  = torch.nn.utils.rnn.pad_packed_sequence(C_h_1,batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        C_h_1 = C_h_1[unsorted_idx]\r\n        C_h_1 = C_h_1.view(batch_size,sess_len,con_len,-1)\r\n        return C_h_1\r\n\r\n    def Flow(self , C_h , Sess_C_len):\r\n        #Sess_C_len [batch , con_len]\r\n        batch_size , con_len , sess_len , hidden = C_h.size()\r\n        C_h = C_h.view(-1,sess_len,hidden)\r\n        Sess_C_len = Sess_C_len.view(-1)\r\n        sorted_idx = np.argsort(-Sess_C_len)\r\n        C_hat = C_h[sorted_idx]\r\n        Sess_C_len = Sess_C_len[sorted_idx]\r\n        C_hat = torch.nn.utils.rnn.pack_padded_sequence(C_hat, Sess_C_len, batch_first=self.batch_first)\r\n        C_hat , (h_n , c_n) = self.gru(input = C_hat)\r\n        C_hat , _ = torch.nn.utils.rnn.pad_packed_sequence(C_hat , batch_first=self.batch_first)\r\n        unsorted_idx = np.argsort(sorted_idx)\r\n        C_hat = C_hat[unsorted_idx] #[batch * con_len , sess_len ,3 *  hidden + 1]\r\n        C_h_plus_1 = torch.cat((C_h,C_hat) , dim = -1) #[batch * con_len , sess_len ,  4 * hidden + 1]\r\n        C_h_plus_1 = C_h_plus_1.view(batch_size , con_len , sess_len , -1).permute(0,2,1,3)\r\n        return C_h_plus_1 #[batch , sess_len , con_len , in_hidden_size]\r\n\r\n    def forward(self , C_h_1 , C_h_len , Sess_C_len):\r\n        #C_h_len [batch , sess_len]\r\n        #Sess_C_len [batch , con_len]\r\n        batch_size , sess_len , con_len , in_hidden_size = C_h_1.size()\r\n        C_h_1 = C_h_1.view( -1, con_len , in_hidden_size)\r\n        C_hat = self.Context_Integration(C_h_1 , C_h_len) #[batch , sess_len , con_len , hidden]\r\n        C_hat = C_hat.permute(0,2,1,3)#[batch , con_len , sess_len , hidden]\r\n        C_h = self.Flow(C_hat , Sess_C_len) #[batch , sess_len , con_len , hidden * 2 + 1]\r\n        return C_h\r\n\r\nclass FullyAwareAttn(nn.Module):\r\n    def __init__(self , hidden_size , x_hidden_size , y_hidden_size):\r\n        super(FullyAwareAttn , self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.U_x = nn.Linear(in_features=x_hidden_size,out_features=hidden_size)\r\n        self.U_y = nn.Linear(in_features=y_hidden_size,out_features=hidden_size)\r\n        self.D = torch.diag(torch.rand(hidden_size)) #[hidden,hidden]\u7684\u5bf9\u89d2\u9635,torch.rand(10),size\u662f10\u7684\u5411\u91cf, \u30100\uff0c1)\u4e4b\u95f4\u7684\u5747\u5300\u5206\u5e03\r\n\r\n    def S(self ,C ,Q):\r\n        #C [batch , sess_len , con_len , hidden]\r\n        #Q [batch , sess_len , seq_len , hidden]\r\n        #a[i,j,k] = C[j][i] * Q[j][k] \u7b2cj\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u4e2d,C\u4e2d\u7684\u7b2ci\u4e2a\u5355\u8bcd\u4e0eQ\u4e2d\u7684\u7b2ck\u4e2a\u5355\u8bcd\u8ba1\u7b97\u7684\u76f8\u4f3c\u5ea6\r\n        #a = torch.matmul(C , Q.transpose(-2 , -1)) #[batch , sess_len , con_len , seq_len] \u662f\u8fd9\u4e2a\u610f\u601d\uff0c\u4f46\u662f\u4e0d\u662f\u7b80\u5355\u7684\u70b9\u4e58\u7b97\u76f8\u4f3c\u5ea6\r\n        C = F.relu(self.U_x(C))\r\n        Q = F.relu(self.U_y(Q))\r\n        temp = torch.matmul(C,self.D)\r\n        a = torch.matmul(temp , Q.transpose(-2,-1)) #[batch , sess_len , con_len , seq_len]\r\n        return a\r\n\r\n    def forward(self , C , Q , C_mask0 , Q_mask0):\r\n        # C = ( C_zero, C_one, C_two )\r\n        # Q = ( Q_naive , Q_lstm1 , Q_lstm2)\r\n        C_zero, C_one, C_two = C\r\n        embed_Q, Q_lstm_1, Q_lstm_2 = Q\r\n        Q = torch.cat((embed_Q, Q_lstm_1, Q_lstm_2),dim = -1)\r\n        C = torch.cat((C_zero, C_one, C_two), dim=-1)\r\n        batch_size , sess_len , con_len , hidden_size = C.size()\r\n        seq_len = Q.size(2)\r\n        #C_mask0 [batch , con_len]\r\n        #Q_mask0 [batch , sess_len , seq_len]\r\n        a = self.S(C,Q) #[batch , sess_len , con_len , seq_len]\r\n        C_mask0 = C_mask0 * (-1e10)\r\n        Q_mask0 = Q_mask0 * (-1e10)\r\n        C_mask0 = C_mask0.unsqueeze(1).unsqueeze(-1).repeat(1 , sess_len , 1 , seq_len)\r\n        Q_mask0 = Q_mask0.unsqueeze(-2).repeat(1,1,con_len,1)\r\n        a = a + Q_mask0 + C_mask0\r\n        a = F.softmax(a , dim = -1) #[batch , sess_len , con_len , seq_len(\u5f52\u4e00\u5316)]\r\n        Q_hat = torch.matmul(a , Q_lstm_2) #[batch , sess_len , con_len , hidden]\r\n        return Q_hat\r\n\r\nclass FlowQA(nn.Module):\r\n    def __init__(self , vocab_size , hidden_size , pad_token_index):\r\n        super(FlowQA,self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size= hidden_size\r\n\r\n        self.embedding = nn.Embedding(\r\n            num_embeddings=self.vocab_size,\r\n            embedding_dim=self.hidden_size,\r\n            padding_idx=pad_token_index\r\n        )\r\n\r\n        self.dotatthier = DotAttHier2(\r\n            in_hidden_size=hidden_size,\r\n            out_hidden_size = hidden_size\r\n        )\r\n\r\n        self.lstm_Q = NaiveLSTM(\r\n            input_size = hidden_size,\r\n            hidden_size = hidden_size,\r\n            num_layer= 1,\r\n            num_dir=2,\r\n            batch_first=True\r\n        )\r\n\r\n        self.hierQueEncoder = HierEncoderLSTM(\r\n            input_size = hidden_size,\r\n            hidden_size = hidden_size,\r\n            num_layer=1\r\n        )\r\n\r\n        self.IF1 = IF(\r\n            in_hidden_size = 2 * hidden_size + 1,\r\n            hidden_size = hidden_size\r\n        )\r\n\r\n        # print(\"IF1.hidden = \",self.IF1.hidden_size , \"IF1.in_hidden = \",self.IF1.in_hidden_size)\r\n\r\n        self.IF2 = IF(\r\n            in_hidden_size= 2 * hidden_size + 1,\r\n            hidden_size=hidden_size\r\n        )\r\n\r\n        # print(\"IF2.hidden = \", self.IF2.hidden_size, \"IF2.in_hidden = \", self.IF2.in_hidden_size)\r\n\r\n        self.FAT_On_Que = FullyAwareAttn(\r\n            hidden_size = hidden_size * 3 ,\r\n            x_hidden_size = (2 * hidden_size + 1) * 3,\r\n            y_hidden_size = (2 * 2 + 1) * hidden_size\r\n        )\r\n\r\n        self.IF3 = IF3(\r\n            in_hidden_size= 2 * hidden_size + 1 + 2 * hidden_size,\r\n            hidden_size = hidden_size\r\n        )\r\n\r\n        self.FAT_C_self = FullyAwareAttn(\r\n            hidden_size = hidden_size * 3,\r\n            x_hidden_size = (2 * hidden_size + 1) * 3,\r\n            y_hidden_size = (2 * hidden_size + 1) * 3\r\n        )\r\n\r\n        self.lstm_C = NaiveLSTM(\r\n            input_size=(2 * hidden_size + 1) * 2,\r\n            hidden_size=hidden_size,\r\n            num_layer=1,\r\n            num_dir=2,\r\n            batch_first=True\r\n        )\r\n\r\n        self.predAnsable_W = nn.Bilinear(\r\n            in1_features= 2 * (2 * hidden_size + 1),\r\n            in2_features= 2 * hidden_size,\r\n            out_features= 1\r\n        )\r\n\r\n        self.Ws = nn.Bilinear(\r\n            in1_features= 2 * hidden_size + 1,\r\n            in2_features= 2 * hidden_size,\r\n            out_features= 1\r\n        )\r\n\r\n        self.ptr_gru = nn.GRUCell(\r\n            input_size= 2 * hidden_size + 1,\r\n            hidden_size = 2 * hidden_size\r\n        )\r\n\r\n        self.We = nn.Bilinear(\r\n            in1_features= 2 * hidden_size + 1,\r\n            in2_features= 2 * hidden_size,\r\n            out_features= 1\r\n        )\r\n\r\n    def get_common(self, Q , C):\r\n        em = []\r\n        batch_size , sess_len , seq_len = Q.size()\r\n        con_len = C.size(1)\r\n        Q = Q.transpose(0,1) #[sess_len , batch , seq_len]\r\n        for index in range(sess_len):\r\n            temp = []\r\n            for i in range(batch_size):\r\n                tmp = [ 1 if ele in Q[index][i] else 0 for ele in C[i]]\r\n                tmp = FloatTensor(tmp)\r\n                tmp = temp.view(1, -1)\r\n                temp.append(tmp)\r\n            temp = torch.cat(tmp,dim=0)\r\n            temp = temp.view(1,batch_size,con_len)\r\n            em.append(temp)\r\n        em = torch.cat(em,dim=0) #[sess_len ,batch_size , con_len]\r\n        em = em.permute(1,0,2)\r\n        return em\r\n\r\n    def get_common2(self, Q ,C):\r\n        em = []\r\n        batch_size, sess_len, seq_len = Q.size()\r\n        con_len = C.size(1)\r\n        Q = Q.transpose(0, 1)  # [sess_len , batch , seq_len]\r\n        for index in range(sess_len):\r\n            temp = [[1 if ele in Q[index][i] else 0 for ele in C[i]] for i in range(batch_size)]\r\n            em.append(FloatTensor(temp).view(1,batch_size,con_len))\r\n        em = torch.cat(em, dim=0) #[sess_len ,batch_size , con_len]\r\n        em = em.permute(1,0,2)\r\n        return em\r\n\r\n    def predictAnswerable(self , C_fou , C_mask1 , Q_lstm_high , Sess_len_mask1):\r\n        #C_fou [batch , sess_len , con_len , 2 * hidden + 1]\r\n        #C_mask1 [batch , con_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\r\n        #Q_lstm_high [batch , sess_len , 2 * hidden]\r\n        #Sess_len_mask1 [batch , sess_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\r\n        batch_size , sess_len , con_len , _ = C_fou.size()\r\n        C_mask1 = C_mask1.unsqueeze(1).unsqueeze(-1).repeat(1,sess_len,1)#[batch , sess_len , con_len , 1]\r\n        C_fou = C_fou * C_mask1\r\n\r\n        Sess_len_mask1 = Sess_len_mask1.unsqueeze(-1) #[batch , sess_len , 1]\r\n        Q_lstm_high = Q_lstm_high * Sess_len_mask1\r\n\r\n        C_max = C_fou.max(-2)[0] #[batch , sess_len , 2 * hidden + 1]\r\n        C_sum = C_fou.sum(-2) #[batch , sess_len , 2 * hidden + 1]\r\n        C = torch.cat((C_max , C_sum) , dim = -1)  #[batch , sess_len , 2 * (2 * hidden + 1)]\r\n\r\n        pred = self.predAnsable_W(C , Q_lstm_high) #[batch , sess_len , 1]\r\n        Sess_len_mask0 = 1 - Sess_len_mask1\r\n        Sess_len_mask0 = Sess_len_mask0 * (-1e10)\r\n        pred = pred + Sess_len_mask0\r\n        pred = pred.view(batch_size , sess_len)\r\n        pred = F.softmax(pred,dim=-1)\r\n        return pred , Sess_len_mask1.squeeze(-1)\r\n\r\n    def predictSpan(self , C_fou , C_mask1 , Q_lstm_high , Sess_len_mask1 , max_ans_len = 15):\r\n        # C_fou [batch , sess_len , con_len , 2 * hidden + 1]\r\n        # C_mask1 [batch , con_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\r\n        # Q_lstm_high [batch , sess_len , 2 * hidden]\r\n        # Sess_len_mask1 [batch , sess_len] \u6709\u503c\u7684\u5730\u65b9\u4e3a1\uff0c\u65e0\u503c\u7684\u5730\u65b9\u4e3a0\r\n        batch_size, sess_len, con_len, _ = C_fou.size()\r\n        C_mask1 = C_mask1.unsqueeze(1).unsqueeze(-1).repeat(1, sess_len, 1)  # [batch , sess_len , con_len , 1]\r\n        C_fou = C_fou * C_mask1\r\n\r\n        Sess_len_mask1 = Sess_len_mask1.unsqueeze(-1)  # [batch , sess_len , 1]\r\n        Q_lstm_high = Q_lstm_high * Sess_len_mask1\r\n\r\n        Q_lstm_high = Q_lstm_high.unsqueeze(2).repeat(1,1,con_len,1) #[batch , sess_len , con_len , 2 * hidden]\r\n        P_S = self.Ws(C_fou,Q_lstm_high) #[batch , sess_len , con_len , 1]\r\n        P_S = (P_S + (1 - C_mask1) * (-1e10)).squeeze(-1) #[batch , sess_len , con_len]\r\n        P_S = F.softmax(P_S , -1)\r\n\r\n        C_fou_i = (C_fou * P_S.unsqueeze(-1)).view(batch_size * sess_len ,con_len , -1).sum(1) #[batch * sess_len , 2 * hidden + 1]\r\n        Q_lstm_high_h_hat = self.ptr_gru(input = C_fou_i , hidden = Q_lstm_high[:,:,0,:].view(batch_size * sess_len , -1))\r\n        #[batch * sess_len , 2 * hidden]\r\n        Q_lstm_high_h_hat = Q_lstm_high_h_hat.view(batch_size , sess_len , -1)\r\n\r\n        Q_lstm_high_h_hat = Q_lstm_high_h_hat.unsqueeze(2).repeat(1,1,con_len,1)\r\n        P_E = self.We(C_fou , Q_lstm_high_h_hat) #[batch , sess_len , con_len , 1]\r\n        P_E = P_E + ((1 - C_mask1) * (-1e10)).squeeze(-1) #[batch , sess_len , con_len]\r\n        P_E = F.softmax(P_E , -1)\r\n\r\n        return  P_S, P_E , C_mask1.squeeeze(-1)\r\n\r\n\r\n    def forward(self, Q , C , Q_len , Sess_len , C_len , threadhold = 0.5, max_ans_len = 15):\r\n        #Q [batch , sess_len , seq_len]\r\n        #C [batch , context_len]\r\n        #Q_len [batch , sess_len]\r\n        #Sess_len [batch]\r\n        #C_len [batch]\r\n        embed_Q = self.embedding(Q).to(device=device)\r\n        embed_C = self.embedding(C).to(device=device)\r\n        print(\"device = \",device)\r\n        batch_size , sess_len , seq_len , in_hidden_size = embed_Q.size()\r\n        con_len = C.size(1)\r\n        Q_mask0 = get_mask(Q_len, seq_len, mask_value=0) #[batch , sess_len ,seq_len]\r\n        C_mask0 = get_mask(C_len, con_len, mask_value=0) #[batch , con_len]\r\n        Q_hat = self.dotatthier(embed_Q,embed_C,Q_mask0,C_mask0)#[batch ,sess_len , con_len , in_hidden]\r\n        em = self.get_common2(Q , C) #[batch , sess_len , con_len]\r\n        em = em.view(batch_size,sess_len,con_len,1)\r\n        C_zero = torch.cat((embed_C.view(batch_size,1,con_len,in_hidden_size).repeat(1,sess_len,1,1), em , Q_hat) , dim = -1)\r\n        #[batch , sess_len , con_len , in_hidden + 1 + in_hidden]\r\n        print(\"C_zero.shape = \",C_zero.shape)\r\n\r\n        C_zero_len = C_len.unsqueeze(1).repeat(1 , sess_len)\r\n        #[batch , sess_len]\r\n        Sess_C_len = Sess_len.unsqueeze(1).repeat(1 , con_len)\r\n        #[batch , con_len]\r\n        C_one = self.IF1(C_zero , C_zero_len , Sess_C_len) ##[batch , sess_len , con_len , hidden * 2 + 1]\r\n        C_two = self.IF2(C_one , C_zero_len , Sess_C_len)\r\n\r\n        Q_lstm_1, (h_n, c_n) = self.lstm_Q(input = embed_Q.view(-1 , seq_len ,in_hidden_size) , input_length = Q_len.view(-1))\r\n        #[batch * sess_len , seq_len , num_dir * hidden]\r\n        Q_lstm_1 = Q_lstm_1.view(batch_size , sess_len , seq_len , -1)\r\n        Q_lstm_2 , Q_lstm_high = self.hierQueEncoder(Q = Q_lstm_1 , Q_len = Q_len , Sess_len = Sess_len , Q_mask0 = Q_mask0)\r\n        #[batch , sess_len , seq_len , 2 * hidden]  [batch , sess_len , 2 * hidden]\r\n\r\n        Q_attned =  self.FAT_On_Que( (C_zero, C_one, C_two) , (embed_Q, Q_lstm_1, Q_lstm_2) , C_mask0 , Q_mask0)\r\n        # [batch , sess_len , con_len , 2 * hidden]\r\n        temp = torch.cat([C_two , Q_attned] , dim = -1)\r\n        C_thr = self.IF3(temp , C_zero_len , Sess_C_len)\r\n        C_attend =  self.FAT_C_self( (C_one , C_two , C_thr) , (C_one , C_two , C_thr) , C_mask0 , C_mask0)\r\n\r\n        C_fou = torch.cat([C_thr , C_attend] , dim = -1) # [batch , sess_len , con_len , 2 * (2 * hidden + 1)]\r\n        C_fou = C_fou.view(batch_size * sess_len , con_len , -1)\r\n        C_fou = self.lstm_C(C_fou, C_zero_len.view(-1))\r\n        C_fou = C_fou.view(batch_size , sess_len , con_len , -1)\r\n\r\n        P_answerable , Sess_len_mask1 = self.predictAnswerable( C_fou , (1 - C_mask0) , Q_lstm_high , get_mask(Sess_len , sess_len ,mask_value=1))\r\n        #[batch , sess_len]\r\n        P_answerable_yes_no = P_answerable >= threadhold #\u4e0d\u4e00\u5b9a\u8981\u4e0d\u8981\u6ce8\u91ca\u6389\u5b83\r\n        P_start , P_end , P_mask1 = self.predictSpan(C_fou,(1 - C_mask0) , Q_lstm_high , Sess_len_mask1 , max_ans_len)\r\n        #[batch , sess_len , con_len]\r\n\r\n        return P_answerable , Sess_len_mask1 , P_start , P_end , P_mask1\r\n\r\ndef train(model , otherInputs , start_label , end_label , criterion , optimizer):\r\n    #start_label = [batch * sess_len] tensor \u7c7b\u578b\u7684\r\n    #criterion = nn.NLLLoss()\r\n    '''\r\n    \u5728\u8fd9\u91cc\u8c03\u7528model\u6a21\u5757\r\n    '''\r\n    (Q, C, Q_len, Sess_len, C_len) = otherInputs\r\n\r\n    P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(torch.LongTensor(Q) ,torch.LongTensor(C) ,LongTensor(Q_len) ,LongTensor(Sess_len) ,LongTensor (C_len))\r\n    # [batch , sess_len]          #[batch , sess_len , con_len]\r\n    loss = 0\r\n    ones = torch.ones(len(start_label))\r\n    batch_size , sess_len , con_len = P_start.size()\r\n    P_start = P_start.view(-1 , con_len)\r\n    P_end = P_end.view(-1,con_len)\r\n    #\u5bf9\u4e8eloss\u6709\u4e24\u79cd\u64cd\u4f5c\uff0c1\uff0c\u53ea\u9009\u62e9\u6b63\u786e\u7684\u4f4d\u7f6e\u7684\u6982\u7387\uff0c2\uff0csubsample\u4e00\u4e9b\u9519\u8bef\u7684\u4f4d\u7f6e\u7684\u6982\u7387\u6765\u8fdb\u884c\u8ba1\u7b97\u3002\u8fdb\u884c2\u7684\u65f6\u5019\u600e\u4e48\u6837\u4fdd\u8bc11\u4e2d\u7684\u4e0d\u88ab\u518d\u9009\u4e2d\u5462 \uff0c  \u53c8\u600e\u4e48\u4fdd\u8bc1mask\u7684\u4e1c\u897f\u4e0d\u88ab\u9009\u4e2d\u5462\u3002\r\n\r\n    #\u5b8c\u62101\r\n    start = FloatTensor( [ P_start[i][start_label[i]] for i in range(batch_size * sess_len) ] )\r\n    start_neg = 1 - start\r\n    start = start.view(-1,1)\r\n    start_neg = start_neg.view(-1,1)\r\n    start_p = torch.cat((start_neg , start) , dim = -1)\r\n    loss += criterion(start_p , torch.ones(batch_size * sess_len))\r\n\r\n    end = FloatTensor([P_end[i][end_label[i]] for i in range(batch_size * sess_len)])\r\n    end_neg = 1 - end\r\n    end = end.view(-1, 1)\r\n    end_neg = end_neg.view(-1, 1)\r\n    end_p = torch.cat((end_neg, end), dim=-1)\r\n    loss += criterion(end_p, torch.ones(batch_size * sess_len))\r\n\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    return loss.item / batch_size\r\n\r\ndef reload(args,PATH):\r\n    flowQA_model = FlowQA()\r\n    flowQA_optimizer = optim.Adamax(flowQA_model.parameters(), lr=0.1)\r\n    checkpoint = torch.load(PATH)\r\n    flowQA_model.load_state_dict(checkpoint['model_state_dict'])\r\n    flowQA_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n    epoch = checkpoint['epoch']\r\n    loss = checkpoint['loss']\r\n    # flowQA_model.eval()\r\n    # flowQA_model.train() \u4e3a\u7684\u662f\u8ba9\u6a21\u578b\u53c2\u6570\u53ef\u4ee5\u88ab\u8bad\u7ec3\r\n    return flowQA_model,flowQA_optimizer\r\n\r\ndef trainIters(in_file = \"/data1/zhengquan/data/CoQA/subTrain_preproed.json\", n_epochs = 20,  print_every=1000, plot_every=100, save_every = 100,learning_rate=0.01, batch_size = 2):\r\n    Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label, lang = getData(in_file)\r\n    print(\"data load finished\")\r\n    flowQA_model = FlowQA(vocab_size = lang.n_words , hidden_size = 100, pad_token_index = lang.word2id['<PAD>'] )\r\n    criterion = nn.CrossEntropyLoss(size_average=True, reduce=False)\r\n    flowQA_optimizer = optim.Adamax(flowQA_model.parameters(), lr=0.1)\r\n    '''\r\n    OtherInputs \u5305\u62ec\r\n    Q , C , Q_len , Sess_len , C_len \r\n    #Q [batch , sess_len , seq_len]\r\n    #C [batch , context_len]\r\n    #Q_len [batch , sess_len]\r\n    #Sess_len [batch]\r\n    #C_len [batch]\r\n    '''\r\n    start = time.time()\r\n    plot_losses = []\r\n    print_loss_total = 0  # Reset every print_every\r\n    plot_loss_total = 0  # Reset every plot_every\r\n    inner_loop_iter = 0\r\n    for iter in range(1, n_epochs + 1):\r\n        '''\r\n        \u8fd9\u91cc\u7684otherInputs\u548cstart_label,end_label,\u624d\u662f\u771f\u6b63\u6309\u7167batch\u7f51\u91cc\u9762\u653e\u7684\uff0c\u73b0\u5728\u624d\u771f\u7684\u9700\u8981\u5728sess_len, seq_len\u6216context_len\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8865\u9f50,\u4fdd\u8bc1\u6bcf\u4e2abatch\u5185\u7684\u6570\u636e\u683c\u5f0f\u4e00\u81f4\uff0c\r\n        '''\r\n        data = getBatchData((Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label))\r\n        while True:\r\n            temp_data = next(data ,None)\r\n            if temp_data is None:\r\n                break\r\n            #inner_loop_iter += 1\r\n            inner_loop_iter += batch_size\r\n            Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label = temp_data\r\n            OtherInpus = (Q , C , Q_len , Sess_len , C_len)\r\n            loss = train(flowQA_model , OtherInpus , start_label , end_label , criterion , flowQA_optimizer)\r\n            print_loss_total += loss\r\n            plot_loss_total += loss\r\n\r\n            if inner_loop_iter % print_every == 0:\r\n                print_loss_avg = print_loss_total / print_every\r\n                print_loss_total = 0\r\n                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\r\n                                             iter, iter / n_epochs * 100, print_loss_avg))\r\n\r\n            if inner_loop_iter % plot_every == 0:\r\n                plot_loss_avg = plot_loss_total / plot_every\r\n                plot_losses.append(plot_loss_avg)\r\n                plot_loss_total = 0\r\n\r\n            if inner_loop_iter % save_every == 0:\r\n                temp_num = inner_loop_iter\r\n                path_to_save = os.path.join(DIR_TO_SAVE,\"%d\"%(temp_num))\r\n                while os.path.exists(path_to_save):\r\n                    temp_num += 1\r\n                    path_to_save = os.path.join(DIR_TO_SAVE, \"%d\" % (temp_num))\r\n                torch.save({'epoch':iter,\r\n                            'model_state_dict':flowQA_model.state.dict(),\r\n                            'optimizer_state_dict':flowQA_optimizer.state_dict(),\r\n                            'loss':loss}\r\n                           ,path_to_save)\r\n\r\n    showPlot(plot_losses)\r\n\r\ndef evaluate(model , otherInputs , start_label , end_label):\r\n    with torch.no_grad():\r\n        P_answerable, Sess_len_mask1, P_start, P_end, P_mask1 = model(otherInputs)\r\n    return P_start , P_end\r\n\r\nif __name__ == \"__main__\":\r\n    # in_file  = \"/data1/zhengquan/data/coqa-train-v1.0.json\"\r\n    # Q, Q_len, A, A_len, C, C_len, Sess_len, start_label, end_label, lang = getData(in_file)\r\n    # print(lang.n_words)\r\n    trainIters()\r\n```\r\n"}