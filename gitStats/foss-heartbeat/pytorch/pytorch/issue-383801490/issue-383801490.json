{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14336", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14336/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14336/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14336/events", "html_url": "https://github.com/pytorch/pytorch/pull/14336", "id": 383801490, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMzMTgyMzQ4", "number": 14336, "title": "[JIT] Add Type support to the fuser, fuse more", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-23T12:06:51Z", "updated_at": "2018-11-23T15:31:17Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14336", "html_url": "https://github.com/pytorch/pytorch/pull/14336", "diff_url": "https://github.com/pytorch/pytorch/pull/14336.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14336.patch"}, "body_html": "<p>This adds scalar type support to the fuser, both internally (instead of auto / assuming float) and for the inputs/outputs.<br>\nWe can now fuse things with input / output of arbitrary scalar type, in particular comparisons and where work well. So it <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #13384.\">fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"375896332\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13384\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13384/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13384\">#13384</a> by returning the right type tensor (and adds a test where byte and double tensors are returned).<br>\nThe type inference is done by re-calling PropagateTensorShapeOnNode in the compilation, I would venture that it isn't prohibitively expensive compared to the actual compilation. (Propagation was fixed for where to return the second argument's type and amended to handle FusedConcat.)<br>\nI'm not sure how to add a check for the code generated by the fuser, but I am not sure we absolutely need to (we'd see if it is invalid / produces wrong results).</p>\n<p>Thanks in particular to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> for advice and encouragement! All the errors are my own.</p>\n<p>I have discussed order of PRs briefly with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a>, if this goes in before he submits the PR, he graciously agreed to rebasing his, but I'd happily rebase, too.</p>", "body_text": "This adds scalar type support to the fuser, both internally (instead of auto / assuming float) and for the inputs/outputs.\nWe can now fuse things with input / output of arbitrary scalar type, in particular comparisons and where work well. So it fixes #13384 by returning the right type tensor (and adds a test where byte and double tensors are returned).\nThe type inference is done by re-calling PropagateTensorShapeOnNode in the compilation, I would venture that it isn't prohibitively expensive compared to the actual compilation. (Propagation was fixed for where to return the second argument's type and amended to handle FusedConcat.)\nI'm not sure how to add a check for the code generated by the fuser, but I am not sure we absolutely need to (we'd see if it is invalid / produces wrong results).\nThanks in particular to @apaszke, @fmassa, @mruberry for advice and encouragement! All the errors are my own.\nI have discussed order of PRs briefly with @mruberry, if this goes in before he submits the PR, he graciously agreed to rebasing his, but I'd happily rebase, too.", "body": "This adds scalar type support to the fuser, both internally (instead of auto / assuming float) and for the inputs/outputs.\r\nWe can now fuse things with input / output of arbitrary scalar type, in particular comparisons and where work well. So it fixes #13384 by returning the right type tensor (and adds a test where byte and double tensors are returned).\r\nThe type inference is done by re-calling PropagateTensorShapeOnNode in the compilation, I would venture that it isn't prohibitively expensive compared to the actual compilation. (Propagation was fixed for where to return the second argument's type and amended to handle FusedConcat.)\r\nI'm not sure how to add a check for the code generated by the fuser, but I am not sure we absolutely need to (we'd see if it is invalid / produces wrong results).\r\n\r\nThanks in particular to @apaszke, @fmassa, @mruberry for advice and encouragement! All the errors are my own.\r\n\r\nI have discussed order of PRs briefly with @mruberry, if this goes in before he submits the PR, he graciously agreed to rebasing his, but I'd happily rebase, too.\r\n"}