{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10178", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10178/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10178/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10178/events", "html_url": "https://github.com/pytorch/pytorch/pull/10178", "id": 347166125, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA1ODU0NjUy", "number": 10178, "title": "Move at::chunk into the graph fuser", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-02T20:39:52Z", "updated_at": "2018-11-23T15:49:36Z", "closed_at": "2018-08-18T23:11:23Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10178", "html_url": "https://github.com/pytorch/pytorch/pull/10178", "diff_url": "https://github.com/pytorch/pytorch/pull/10178.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10178.patch"}, "body_html": "<p>... to avoid slow at::chunk (it is slow due to tensor initialization). Picking up from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345927350\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10026\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10026/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10026\">#10026</a></p>\n<p>This is done through the following:</p>\n<ol>\n<li>Absorb starting chunks into FusionGroup as a part of the graph fuser<br>\npass.</li>\n<li>When compiling a kernel, emit a <code>std::vector&lt;ConcatDesc&gt;</code> that describes if an input (of the original graph) will be chunked.</li>\n<li>When launching a kernel, <code>use std::vector&lt;ConcatDesc&gt;</code> to chunk an<br>\ninput tensor on the CPU. This chunk directly takes in an at::Tensor and creates<br>\nfour TensorInfo structs in-place in the argument list, bypassing the creation of intermediate Tensors.</li>\n</ol>\n<h2>Test Plan</h2>\n<ul>\n<li>Expect test and correctness test to see if a single chunk is fused<br>\nby the graph fuser</li>\n<li>Correctness test for a variety of chunks (dimension = beginning,<br>\nmiddle, end) and tensors (contiguous, non-contiguous, edge case<br>\n(splitSize = 1) for both CPU/CUDA</li>\n<li>Expect test for multiple chunks fused into the same kernel and<br>\ncorrectness test.</li>\n</ul>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>\n<h2>Perf</h2>\n<p>LSTM forward pass, 1 layer, 512 hidden size and input size, 100 seq length, requires_grad=False on all inputs and weights.</p>\n<p>After changes:</p>\n<pre><code>thnn    cudnn   jit\n8.8468  6.5797  9.3470\n</code></pre>\n<p>Before changes:</p>\n<pre><code>thnn    cudnn   jit\n9.9221  6.6539  11.2550\n</code></pre>", "body_text": "... to avoid slow at::chunk (it is slow due to tensor initialization). Picking up from #10026\nThis is done through the following:\n\nAbsorb starting chunks into FusionGroup as a part of the graph fuser\npass.\nWhen compiling a kernel, emit a std::vector<ConcatDesc> that describes if an input (of the original graph) will be chunked.\nWhen launching a kernel, use std::vector<ConcatDesc> to chunk an\ninput tensor on the CPU. This chunk directly takes in an at::Tensor and creates\nfour TensorInfo structs in-place in the argument list, bypassing the creation of intermediate Tensors.\n\nTest Plan\n\nExpect test and correctness test to see if a single chunk is fused\nby the graph fuser\nCorrectness test for a variety of chunks (dimension = beginning,\nmiddle, end) and tensors (contiguous, non-contiguous, edge case\n(splitSize = 1) for both CPU/CUDA\nExpect test for multiple chunks fused into the same kernel and\ncorrectness test.\n\ncc @zdevito @apaszke\nPerf\nLSTM forward pass, 1 layer, 512 hidden size and input size, 100 seq length, requires_grad=False on all inputs and weights.\nAfter changes:\nthnn    cudnn   jit\n8.8468  6.5797  9.3470\n\nBefore changes:\nthnn    cudnn   jit\n9.9221  6.6539  11.2550", "body": "... to avoid slow at::chunk (it is slow due to tensor initialization). Picking up from #10026\r\n\r\nThis is done through the following:\r\n\r\n1) Absorb starting chunks into FusionGroup as a part of the graph fuser\r\npass.\r\n2) When compiling a kernel, emit a `std::vector<ConcatDesc>` that describes if an input (of the original graph) will be chunked.\r\n3) When launching a kernel, `use std::vector<ConcatDesc>` to chunk an\r\ninput tensor on the CPU. This chunk directly takes in an at::Tensor and creates\r\nfour TensorInfo structs in-place in the argument list, bypassing the creation of intermediate Tensors.\r\n\r\n## Test Plan\r\n\r\n- Expect test and correctness test to see if a single chunk is fused\r\n  by the graph fuser\r\n- Correctness test for a variety of chunks (dimension = beginning,\r\n  middle, end) and tensors (contiguous, non-contiguous, edge case\r\n  (splitSize = 1) for both CPU/CUDA\r\n- Expect test for multiple chunks fused into the same kernel and\r\n  correctness test.\r\n\r\ncc @zdevito @apaszke \r\n\r\n## Perf\r\nLSTM forward pass, 1 layer, 512 hidden size and input size, 100 seq length, requires_grad=False on all inputs and weights.\r\n\r\nAfter changes:\r\n```\r\nthnn    cudnn   jit\r\n8.8468  6.5797  9.3470\r\n```\r\n\r\nBefore changes:\r\n```\r\nthnn    cudnn   jit\r\n9.9221  6.6539  11.2550\r\n```\r\n"}