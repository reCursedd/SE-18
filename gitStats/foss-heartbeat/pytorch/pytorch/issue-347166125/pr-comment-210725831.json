{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210725831", "pull_request_review_id": 147001456, "id": 210725831, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDcyNTgzMQ==", "diff_hunk": "@@ -60,24 +60,37 @@ struct AnnotatedGraph {\n   std::vector<TensorDesc> output_desc;\n };\n \n-struct ConcatDesc {\n-  size_t nSubtensors; // == 1 for outputs that are not concats, otherwise it is the number tensors concatenated\n-  size_t dim; // dimension along which the concat occurs\n+// Descriptor for chunk-ing an input tensor into subtensors\n+// OR concat-ing an output tensor from subtensors\n+struct PartitionDesc {\n+  size_t nSubtensors; // == 1 for tensors that should not be operated on via chunk/cat\n+  size_t dim; // dimension along which the chunk/concat occurs\n   std::unique_ptr<TensorDesc> subtensorDesc; // descriptor for the subtensor, if it exists\n-  ConcatDesc()\n+  PartitionDesc()\n   : nSubtensors(1), dim(0) {}\n-  ConcatDesc(const TensorDesc & desc, size_t nSubtensors, size_t dim)\n+  // PartitionDesc is used as metadata for either fused chunk or FusedConcat ops.\n+  // Depending on which one it is used for, the contiguity calc is different.\n+  PartitionDesc(const TensorDesc & desc, size_t nSubtensors, size_t dim, bool is_chunk)\n   : nSubtensors(nSubtensors), dim(dim) {\n     JIT_ASSERT(nSubtensors > 1);\n     std::vector<bool> cont = desc.contiguity;\n-    if(dim > 0) {\n-      // when we narrow the concatenated output\n-      // we make the size[dim] smaller while keeping the stride[dim] the same,\n-      // meaning: stride[dim - 1] != stride[dim]*size[dim]\n-      // so dim - 1 is no longer contiguous\n-      cont[dim - 1] = false;\n+    if (is_chunk) {\n+      subtensorDesc.reset(new TensorDesc(desc.scalar_type, desc.contiguity));", "path": "torch/csrc/jit/fusion_compiler.h", "position": null, "original_position": 30, "commit_id": "b00a76e1588f34aa2c1d1a9e55fe76b1ad7cf71d", "original_commit_id": "60b58f153f0325eeb585d030da9af61f124b4aa7", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Are you sure that we don't need to modify the contiguity similarly to what we do below? Chunking along any dimension other than the first one will change a contiguous input to non-contiguous chunks.\r\n\r\n```python\r\nIn [2]: import torch\r\n\r\nIn [3]: x = torch.randn(10, 10)\r\n\r\nIn [4]: x.chunk(10, 0)[0].is_contiguous()\r\nOut[4]: True\r\n\r\nIn [5]: x.chunk(10, 1)[0].is_contiguous()\r\nOut[5]: False\r\n```", "created_at": "2018-08-16T20:08:01Z", "updated_at": "2018-11-23T15:49:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/10178#discussion_r210725831", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10178", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210725831"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10178#discussion_r210725831"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10178"}}, "body_html": "<p>Are you sure that we don't need to modify the contiguity similarly to what we do below? Chunking along any dimension other than the first one will change a contiguous input to non-contiguous chunks.</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">3</span>]: x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>)\n\nIn [<span class=\"pl-c1\">4</span>]: x.chunk(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">0</span>)[<span class=\"pl-c1\">0</span>].is_contiguous()\nOut[<span class=\"pl-c1\">4</span>]: <span class=\"pl-c1\">True</span>\n\nIn [<span class=\"pl-c1\">5</span>]: x.chunk(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>)[<span class=\"pl-c1\">0</span>].is_contiguous()\nOut[<span class=\"pl-c1\">5</span>]: <span class=\"pl-c1\">False</span></pre></div>", "body_text": "Are you sure that we don't need to modify the contiguity similarly to what we do below? Chunking along any dimension other than the first one will change a contiguous input to non-contiguous chunks.\nIn [2]: import torch\n\nIn [3]: x = torch.randn(10, 10)\n\nIn [4]: x.chunk(10, 0)[0].is_contiguous()\nOut[4]: True\n\nIn [5]: x.chunk(10, 1)[0].is_contiguous()\nOut[5]: False"}