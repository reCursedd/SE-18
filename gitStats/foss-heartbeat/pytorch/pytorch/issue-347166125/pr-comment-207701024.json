{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207701024", "pull_request_review_id": 143372876, "id": 207701024, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNzcwMTAyNA==", "diff_hunk": "@@ -624,27 +641,179 @@ void compressContiguous(\n \n } // anonymous namespace\n \n+// XXX: assumes graph inputs are 32-bit indexable\n+// assumes v is an input to the graph.\n+static size_t toInputIndex(Value * v, Graph & subgraph) {\n+  auto start = subgraph.inputs().begin();\n+  auto it = std::find(start, subgraph.inputs().end(), v);\n+  JIT_ASSERT(it != subgraph.inputs().end());\n+  return it - start;\n+}\n+\n+// If there are fused Chunk nodes in the graph, returns a new Graph replacing", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": null, "original_position": 51, "commit_id": "b00a76e1588f34aa2c1d1a9e55fe76b1ad7cf71d", "original_commit_id": "b7c32375b90410b074fd47b6daacd5d72bd25ae2", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "If you look at how this is handled for concat, it takes a more lightweight approach. By leaving the Concat operators in and just skipping them in code generation we can still generate the correct kernel, and make sure the input list has the 'flattened' inputs added, without having to copy/do surgery on the fusion graph. It is possible that something similar might work here (see later comment about how chunk and concat are basically the same exact operator).", "created_at": "2018-08-04T06:12:43Z", "updated_at": "2018-11-23T15:48:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/10178#discussion_r207701024", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10178", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207701024"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10178#discussion_r207701024"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10178"}}, "body_html": "<p>If you look at how this is handled for concat, it takes a more lightweight approach. By leaving the Concat operators in and just skipping them in code generation we can still generate the correct kernel, and make sure the input list has the 'flattened' inputs added, without having to copy/do surgery on the fusion graph. It is possible that something similar might work here (see later comment about how chunk and concat are basically the same exact operator).</p>", "body_text": "If you look at how this is handled for concat, it takes a more lightweight approach. By leaving the Concat operators in and just skipping them in code generation we can still generate the correct kernel, and make sure the input list has the 'flattened' inputs added, without having to copy/do surgery on the fusion graph. It is possible that something similar might work here (see later comment about how chunk and concat are basically the same exact operator)."}