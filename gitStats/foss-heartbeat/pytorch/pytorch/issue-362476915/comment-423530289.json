{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423530289", "html_url": "https://github.com/pytorch/pytorch/issues/11929#issuecomment-423530289", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11929", "id": 423530289, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzUzMDI4OQ==", "user": {"login": "yunyundong", "id": 24907398, "node_id": "MDQ6VXNlcjI0OTA3Mzk4", "avatar_url": "https://avatars2.githubusercontent.com/u/24907398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yunyundong", "html_url": "https://github.com/yunyundong", "followers_url": "https://api.github.com/users/yunyundong/followers", "following_url": "https://api.github.com/users/yunyundong/following{/other_user}", "gists_url": "https://api.github.com/users/yunyundong/gists{/gist_id}", "starred_url": "https://api.github.com/users/yunyundong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yunyundong/subscriptions", "organizations_url": "https://api.github.com/users/yunyundong/orgs", "repos_url": "https://api.github.com/users/yunyundong/repos", "events_url": "https://api.github.com/users/yunyundong/events{/privacy}", "received_events_url": "https://api.github.com/users/yunyundong/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-21T13:26:26Z", "updated_at": "2018-09-21T13:26:50Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>I encountered the very same issue, and after spending a day trying to marry PyTorch DataParallel loader wrapper with HDF5 via h5py, I discovered that it is crucial to open h5py.File inside the new process, rather than having it opened in the main process and hope it gets inherited by the underlying multiprocessing implementation.</p>\n</blockquote>\n<blockquote>\n<p>Since PyTorch seems to adopt lazy way of initializing workers, this means that the actual file opening has to happen inside of the<code>__getitem__</code>function of the Dataset wrapper. refer to <a href=\"url\">https://stackoverflow.com/questions/46045512/h5py-hdf5-database-randomly-returning-nans-and-near-very-small-data-with-multi/52438133#52438133</a></p>\n</blockquote>\n<p>This is the answer to the problem.  I modify the code, and it works well. Can you explain more about it? Thank you in advance. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "I encountered the very same issue, and after spending a day trying to marry PyTorch DataParallel loader wrapper with HDF5 via h5py, I discovered that it is crucial to open h5py.File inside the new process, rather than having it opened in the main process and hope it gets inherited by the underlying multiprocessing implementation.\n\n\nSince PyTorch seems to adopt lazy way of initializing workers, this means that the actual file opening has to happen inside of the__getitem__function of the Dataset wrapper. refer to https://stackoverflow.com/questions/46045512/h5py-hdf5-database-randomly-returning-nans-and-near-very-small-data-with-multi/52438133#52438133\n\nThis is the answer to the problem.  I modify the code, and it works well. Can you explain more about it? Thank you in advance. @soumith", "body": ">I encountered the very same issue, and after spending a day trying to marry PyTorch DataParallel loader wrapper with HDF5 via h5py, I discovered that it is crucial to open h5py.File inside the new process, rather than having it opened in the main process and hope it gets inherited by the underlying multiprocessing implementation.\r\n\r\n>Since PyTorch seems to adopt lazy way of initializing workers, this means that the actual file opening has to happen inside of the` __getitem__ `function of the Dataset wrapper. refer to [https://stackoverflow.com/questions/46045512/h5py-hdf5-database-randomly-returning-nans-and-near-very-small-data-with-multi/52438133#52438133](url)\r\n\r\nThis is the answer to the problem.  I modify the code, and it works well. Can you explain more about it? Thank you in advance. @soumith "}