{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6168/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6168/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6168/events", "html_url": "https://github.com/pytorch/pytorch/issues/6168", "id": 310302056, "node_id": "MDU6SXNzdWUzMTAzMDIwNTY=", "number": 6168, "title": "DataLoader loads whole dataset when it got non-int batch_size", "user": {"login": "dandelin", "id": 3676247, "node_id": "MDQ6VXNlcjM2NzYyNDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3676247?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dandelin", "html_url": "https://github.com/dandelin", "followers_url": "https://api.github.com/users/dandelin/followers", "following_url": "https://api.github.com/users/dandelin/following{/other_user}", "gists_url": "https://api.github.com/users/dandelin/gists{/gist_id}", "starred_url": "https://api.github.com/users/dandelin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dandelin/subscriptions", "organizations_url": "https://api.github.com/users/dandelin/orgs", "repos_url": "https://api.github.com/users/dandelin/repos", "events_url": "https://api.github.com/users/dandelin/events{/privacy}", "received_events_url": "https://api.github.com/users/dandelin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-01T06:35:05Z", "updated_at": "2018-04-05T03:07:31Z", "closed_at": "2018-04-05T03:07:31Z", "author_association": "NONE", "body_html": "<p>I've noticed this malfunction while implementing DataLoader for loading enormous images from directory.  As you can see in below code snippet, the loader could blow up the memory if the dataset is large enough.</p>\n<div class=\"highlight highlight-source-python\"><pre>Type <span class=\"pl-s\"><span class=\"pl-pds\">'</span>copyright<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>credits<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>license<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> more information\nIPython <span class=\"pl-c1\">6.2</span>.1 <span class=\"pl-ii\">--</span> An enhanced Interactive Python. Type <span class=\"pl-s\"><span class=\"pl-pds\">'</span>?<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> <span class=\"pl-c1\">help</span>.\n\nIn [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> Dataset, DataLoader\n   <span class=\"pl-c1\">...</span>: \n   <span class=\"pl-c1\">...</span>: <span class=\"pl-k\">class</span> <span class=\"pl-en\">A</span>(<span class=\"pl-e\">Dataset</span>):\n   <span class=\"pl-c1\">...</span>:     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n   <span class=\"pl-c1\">...</span>:         <span class=\"pl-c1\">self</span>.data <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>\n   <span class=\"pl-c1\">...</span>:     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">index</span>):\n   <span class=\"pl-c1\">...</span>:         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.data[index <span class=\"pl-k\">%</span> <span class=\"pl-c1\">3</span>]\n   <span class=\"pl-c1\">...</span>:     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n   <span class=\"pl-c1\">...</span>:         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.data)\n   <span class=\"pl-c1\">...</span>: \n   <span class=\"pl-c1\">...</span>: a <span class=\"pl-k\">=</span> A()\n   <span class=\"pl-c1\">...</span>: \n   <span class=\"pl-c1\">...</span>: loader <span class=\"pl-k\">=</span> DataLoader(a, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>hello world!<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n   <span class=\"pl-c1\">...</span>: \n   <span class=\"pl-c1\">...</span>: <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> loader:\n   <span class=\"pl-c1\">...</span>:     <span class=\"pl-c1\">print</span>(batch)\n   <span class=\"pl-c1\">...</span>:     <span class=\"pl-k\">break</span>\n   <span class=\"pl-c1\">...</span>: \n\n <span class=\"pl-c1\">2</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n\u22ee \n <span class=\"pl-c1\">2</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">3</span>\n[torch.LongTensor of size <span class=\"pl-c1\">3000</span>]<span class=\"pl-bu\">```</span></pre></div>", "body_text": "I've noticed this malfunction while implementing DataLoader for loading enormous images from directory.  As you can see in below code snippet, the loader could blow up the memory if the dataset is large enough.\nType 'copyright', 'credits' or 'license' for more information\nIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: from torch.utils.data import Dataset, DataLoader\n   ...: \n   ...: class A(Dataset):\n   ...:     def __init__(self):\n   ...:         self.data = [1, 2, 3] * 1000\n   ...:     def __getitem__(self, index):\n   ...:         return self.data[index % 3]\n   ...:     def __len__(self):\n   ...:         return len(self.data)\n   ...: \n   ...: a = A()\n   ...: \n   ...: loader = DataLoader(a, batch_size='hello world!', shuffle=True)\n   ...: \n   ...: for batch in loader:\n   ...:     print(batch)\n   ...:     break\n   ...: \n\n 2\n 1\n 1\n\u22ee \n 2\n 1\n 3\n[torch.LongTensor of size 3000]```", "body": "I've noticed this malfunction while implementing DataLoader for loading enormous images from directory.  As you can see in below code snippet, the loader could blow up the memory if the dataset is large enough.\r\n\r\n```Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: from torch.utils.data import Dataset, DataLoader\r\n   ...: \r\n   ...: class A(Dataset):\r\n   ...:     def __init__(self):\r\n   ...:         self.data = [1, 2, 3] * 1000\r\n   ...:     def __getitem__(self, index):\r\n   ...:         return self.data[index % 3]\r\n   ...:     def __len__(self):\r\n   ...:         return len(self.data)\r\n   ...: \r\n   ...: a = A()\r\n   ...: \r\n   ...: loader = DataLoader(a, batch_size='hello world!', shuffle=True)\r\n   ...: \r\n   ...: for batch in loader:\r\n   ...:     print(batch)\r\n   ...:     break\r\n   ...: \r\n\r\n 2\r\n 1\r\n 1\r\n\u22ee \r\n 2\r\n 1\r\n 3\r\n[torch.LongTensor of size 3000]```"}