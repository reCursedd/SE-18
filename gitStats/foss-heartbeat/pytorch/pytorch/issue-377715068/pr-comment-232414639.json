{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232414639", "pull_request_review_id": 173604923, "id": 232414639, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjQxNDYzOQ==", "diff_hunk": "@@ -0,0 +1,588 @@\n+#include \"torch/csrc/jit/passes/python_print.h\"\n+#include \"torch/csrc/jit/attributes.h\"\n+#include \"torch/csrc/jit/generic_if.h\"\n+#include \"torch/csrc/jit/ir.h\"\n+#include \"torch/csrc/jit/resource_guard.h\"\n+#include \"torch/csrc/jit/script/error_report.h\"\n+\n+namespace torch {\n+namespace jit {\n+\n+\n+class PythonPrintPass {\n+  std::ostream& out;\n+\n+  // constants are written to this table, and given then name CONSTANTS.cN\n+  // where N is the index into this table.\n+\n+  std::vector<at::Tensor> tensor_constants;\n+  // When printing this node, is it safe to write it inline (i.e. without\n+  // assigning a temporary variable\n+  std::unordered_set<const Node*> output_inline_;\n+\n+  // when we print this, should we error if the resulting output would\n+  // not be able to be reparsed?\n+  bool enforce_importable_;\n+\n+  // what valid identifiers are in use for the current function\n+  std::unordered_set<std::string> used_names_;\n+\n+  // for fork,\n+  // subgraphs get added to the worklist, and will be printed later\n+  std::vector<std::pair<const Graph*, std::string>> worklist;\n+\n+  // scanValue, scanNode, scanBlock:\n+  // decide if it is safe to omit the output of a temporary variable,\n+  // and inline the expression into its use\n+  // we only do this if\n+  // (1) it is a constant, or\n+  // (2) the temporary is unnamed, is single output, is used once,\n+  //     and would appear in the same order when the expression tree is reparsed.\n+  // The last case can be checked\n+  // becuase when we emit a expresion tree in the parser,\n+  // we do a left-to-right postorder traversal of the expression tree (emit children, then emit op).\n+  // The reverse of this is a right-to-left preorder traversal of the tree.\n+  // By doing a right-to-left preorder traversal of the inputs of a node,\n+  // while also scanning the list of emitted nodes backward, we can see if\n+  // they line up with what would happen when parsed the node as an expression. While they line\n+  // up we collapse them into an inline expression.\n+\n+  // The inductive step is that the right-most input should be produced by the node\n+  // immediatly before the current node if it is in tree order.\n+\n+  // block_point is the current node in the reverse linear scan of the emitted nodes\n+  // v is the current value in the tree traversal that may match with block_point's output.\n+  const Node* scanValue(const Node* block_point, const Value* v) {\n+    const Node* n = v->node();\n+    JIT_ASSERT(n->kind() == prim::Constant || output_inline_.count(n) == 0);\n+\n+    if (n == block_point && // the node must be at the expected point of the typical tree traversal\n+        n->outputs().size() == 1 && // there must be only 1 values, otherwise we need an assignment to handle the multiple outout values\n+        v->uses().size() == 1 && // if it is used more than once, then we need a variable\n+        n->blocks().size() == 0 && // don't try to inline control blocks,\n+        !v->hasUniqueName() && // if it has a name set, then it was written as a variable so preserve that\n+        (v->uses().at(0).user->kind() != prim::Loop // if it is a loop-carried input, we need a variable\n+         || v->uses().at(0).offset < 2)) {          // otherwise the condition or trip count may be emitted in the wrong order w.r.t. to it\n+      // recursively see if we can inline the inputs to this input\n+      block_point = scanNode(block_point);\n+      output_inline_.insert(n);\n+    } else if (n->kind() == prim::Constant) {\n+      // constant nodes can always be inlined, we will de-dup them on parsing\n+      // and put them at the top of the function regardless\n+      output_inline_.insert(n);\n+    }\n+    return block_point;\n+  }\n+\n+  const Node* scanNode(const Node* n) {\n+    // don't bother to scan nodes we have already determined to be inline\n+    if(output_inline_.count(n)) {\n+      return n;\n+    }\n+    for(auto b : n->blocks()) {\n+      scanBlock(b);\n+    }\n+    const Node* block_point = n->prev();\n+    for(auto it = n->inputs().rbegin(),\n+             end = n->inputs().rend(); it != end; ++it) {\n+      block_point = scanValue(block_point, *it);\n+    }\n+    return block_point;\n+  }\n+\n+  void scanBlock(const Block* b) {\n+    scanNode(b->return_node());\n+    for(auto node : b->nodes().reverse()) {\n+      scanNode(node);\n+    }\n+  }\n+\n+  // get a new name unique across calls to uniqueName() and\n+  // anything we have used.\n+  size_t next_id = 0;\n+  std::string genName(const std::string& candidate) {\n+    // some names are valid identifiers but off limits because\n+    // they are keywords or namespaces used in the output\n+    const static std::unordered_set<std::string> reserved_names = {\n+      \"aten\",\n+      \"prim\",\n+      \"CONSTANTS\",\n+      \"int\",\n+      \"float\",\n+      \"bool\",\n+      \"print\",\n+      \"return\",\n+      \"while\",\n+      \"for\",\n+      \"if\",\n+      \"True\",\n+      \"False\",\n+      \"fork\",\n+    };\n+\n+    std::string name = candidate;\n+    while(used_names_.count(name) || reserved_names.count(name)) {\n+      name = candidate + std::to_string(next_id++);\n+    }\n+    used_names_.insert(name);\n+    return name;\n+  }\n+\n+  // unique names might not be valid identifiers,\n+  // force them to be by rewriting them\n+  static std::string makeValidIdentifier(const std::string& candidate) {\n+    std::stringstream ss;\n+    if (candidate.size() == 0 || isdigit(candidate[0]))\n+      ss << \"_\";\n+    for(char c : candidate) {\n+      if (isupper(c) || islower(c) || isdigit(c) || c == '_')\n+        ss << c;\n+      else\n+        ss << '_';\n+    }\n+    return ss.str();\n+  }\n+  // if we have to assign 'v' a name, what should it be?\n+  // use the uniqueName if it was set, otherwise generate a name.\n+  std::string genUniqueNameFor(const Value* v) {\n+    return genName(\n+        v->hasUniqueName() ? makeValidIdentifier(v->uniqueName()) : \"t\");\n+  }\n+\n+  // map from Value to how it should be printed at each use\n+  std::unordered_map<const Value*, std::string> value_names_;\n+\n+  std::string useOf(const Value* v) const {\n+    return value_names_.at(v);\n+  }\n+  void assignValue(const Value* v, const std::string& s) {\n+    value_names_[v] = s;\n+  }\n+  void assignValue(const Value* v, const Value* w) {\n+    assignValue(v, useOf(w));\n+  }\n+  void assignValuesToTheirUniqueNames(at::ArrayRef<const Value*> values) {\n+    for(auto v : values) {\n+      assignValue(v, genUniqueNameFor(v));\n+    }\n+  }\n+\n+  size_t level = 0;\n+  // indent to the current indent level\n+  std::ostream& indent() {\n+    for (size_t i = 0; i < level; ++i) {\n+      out << \"  \";\n+    }\n+    return out;\n+  }\n+\n+  ResourceGuard WithIndented() {\n+    level++;\n+    return ResourceGuard([this]{\n+      level--;\n+    });\n+  }\n+\n+  template <class T0, class T1, class F>\n+  void zipWith(\n+      at::ArrayRef<T0> list_a,\n+      at::ArrayRef<T1> list_b,\n+      F action) const {\n+    auto it_a = list_a.begin();\n+    auto it_b = list_b.begin();\n+\n+    if (list_a.size() != list_b.size()) {\n+      AT_ERROR(\"Pretty printer expected 2 lists of same size\");\n+    }\n+\n+    for (; it_a != list_a.end(); ++it_a, ++it_b) {\n+      action(*it_a, *it_b);\n+    }\n+  }\n+\n+  void printValueList(std::ostream& stmt, at::ArrayRef<const Value*> list, const char* begin = \"\", const char* end = \"\") {\n+    stmt << begin;\n+    auto delimiter = \"\";\n+    for (const auto* value : list) {\n+      stmt << delimiter;\n+      stmt << useOf(value);\n+      delimiter = \", \";\n+    }\n+    stmt << end;\n+  }\n+\n+  void printAssignment(\n+      at::ArrayRef<const Value*> lhs,\n+      at::ArrayRef<const Value*> rhs) {\n+    if(lhs.size() > 0) {\n+      indent();\n+      printValueList(out, lhs);\n+      out << \" = \";\n+      printValueList(out, rhs);\n+      out << \"\\n\";\n+    }\n+  }\n+\n+  void printIf(\n+      const Node* node) {\n+    assignValuesToTheirUniqueNames(node->outputs());\n+    auto cond = node->inputs()[0];\n+    const auto if_block = node->blocks()[0];\n+    const auto else_block = node->blocks()[1];\n+    indent() << \"if \" << useOf(cond) << \":\\n\";\n+    {\n+      auto guard = WithIndented();\n+      // Print node contents\n+      printBlock(if_block);\n+      printAssignment(node->outputs(), if_block->outputs());\n+    }\n+    indent() << \"else:\\n\";\n+    {\n+      auto guard = WithIndented();\n+      printBlock(else_block);\n+      printAssignment(node->outputs(), else_block->outputs());\n+    }\n+  }\n+\n+  // our way of encoding loops makes them difficult to turn back into python syntax.\n+  // we have to check properties of the condition and trip count inputs to\n+  // figure out which one it initially was\n+  static bool shouldEmitAsForLoop(const Node* node) {\n+      const auto body_block = node->blocks()[0];\n+      auto trip_count = toIValue(node->inputs().at(0));\n+      auto cond_input = toIValue(node->inputs().at(1));\n+      auto cond_next = toIValue(body_block->outputs().at(0));\n+\n+      bool condition_is_always_true = cond_input && cond_input->toBool() && cond_next &&\n+        cond_next->toBool();\n+      bool trip_count_is_specified = !trip_count || // trip is not a constant\n+          trip_count->toInt() != std::numeric_limits<int64_t>::max() || // it is a constant but not the default one\n+          body_block->inputs().at(0)->uses().size() > 0; // it is actually being used in the body.\n+\n+      if (condition_is_always_true) {\n+        // if the trip count was not specified this was a user-written while True:\n+        return trip_count_is_specified;\n+      } else {\n+        // this must be a while loop, but check that there isn't _also_ a trip count\n+        if (trip_count_is_specified) {\n+          throw script::ErrorReport(node->getSourceLocation())\n+              << \"loop cannot be printed as python because it has gone through an optimization \"\n+              << \"that combined while and for loops. File a bug.\";\n+        }\n+        return false;\n+      }\n+  }\n+\n+  void printLoop(const Node* node) {\n+\n+    // Loop carried dependencies are handled by assigning their initial\n+    // values to the node->outputs() before the loop,\n+    // and assign node->outputs() to the new values at the end of each trip.\n+\n+\n+    bool emit_as_for_loop = shouldEmitAsForLoop(node);\n+    const auto body_block = node->blocks()[0];\n+\n+    assignValuesToTheirUniqueNames(node->outputs());\n+    // Add aliases for loop-carried dependencies\n+    zipWith(\n+        body_block->inputs().slice(1), // Start at 1 to ignore trip count\n+        node->outputs(),\n+        [&](const Value* block_input, const Value* node_output) {\n+          assignValue(block_input, node_output);\n+        });\n+\n+    // Print initial assignments of loop node outputs = loop node inputs\n+    printAssignment(node->outputs(), node->inputs().slice(2));\n+\n+    auto trip_count_in_block = body_block->inputs().at(0);\n+    assignValuesToTheirUniqueNames(trip_count_in_block);\n+    // Loop header\n+    if (emit_as_for_loop) {\n+      indent();\n+      out << \"for \" << useOf(trip_count_in_block) << \" in range(\"\n+          << useOf(node->inputs().at(0)) << \"):\\n\";\n+    } else {\n+      // note: trip_count_in_block is unused because this is a while loop,\n+      // so we reuse the Value* as a stand-in for the loop condition\n+      printAssignment(trip_count_in_block, node->inputs().at(1));\n+      indent();\n+      out << \"while \" << useOf(trip_count_in_block) << \":\\n\";\n+    }\n+    // Loop body\n+    {\n+      ResourceGuard indent = WithIndented();\n+      printBlock(body_block);\n+      // Update block outputs to block inputs for next loop iteration\n+      // skip the assignment to the new condition in for loops because\n+      // the condition is always True\n+      size_t offset = emit_as_for_loop ? 1 : 0;\n+      printAssignment(body_block->inputs().slice(offset), body_block->outputs().slice(offset));\n+    }\n+  }\n+\n+  void printNode(const Node* node) {\n+    switch (node->kind()) {\n+      case prim::Return:\n+        if (node->inputs().size() > 0) {\n+          indent();\n+          out << \"return \";\n+          printValueList(out, node->inputs());\n+          out << \"\\n\";\n+        }\n+        break;\n+      case prim::Loop:\n+        printLoop(node);\n+        break;\n+      case prim::If:\n+        printIf(node);\n+        break;\n+      case prim::TupleUnpack:\n+      case prim::ListUnpack:\n+        assignValuesToTheirUniqueNames(node->outputs());\n+        indent();\n+        // TupleUnpack(unpacked) turns into an assignment op that forces\n+        // the unpack to be inserted when parsed back in:\n+        // a, b, = unpacked\n+        // a, = unpacked # trailing comma forces an unpack to happen\n+        if (node->outputs().size() > 0) {\n+          printValueList(out, node->outputs(), \"\", \", = \");\n+        }\n+        out << useOf(node->input()) << \"\\n\";\n+        break;\n+      default:\n+\n+        std::stringstream ss;\n+        printRHS(ss, node);\n+\n+        // this node is safe to inline, so assign the output value\n+        // to that expression directly\n+        // guard against really long lines\n+        if (output_inline_.count(node) > 0 && ss.str().size() + level * 2 < 40) {\n+          assignValue(node->output(), ss.str());\n+          return;\n+        }\n+        assignValuesToTheirUniqueNames(node->outputs());\n+        indent();\n+        // Print outputs\n+        if (node->outputs().size() > 0) {\n+          printValueList(out, node->outputs());\n+          out << \" = \";\n+        }\n+        out << ss.str() << \"\\n\";\n+    }\n+  }\n+\n+  size_t addTensorConstant(at::Tensor t) {\n+    tensor_constants.emplace_back(std::move(t));\n+    return tensor_constants.size() - 1;\n+  }\n+\n+  // Prints the RHS value of a Node, e.g. `aten.add(x, y)`\n+  void printRHS(std::ostream& stmt, const Node* node) {\n+    switch(node->kind()) {\n+      case PythonOp::Kind: {\n+        auto value = static_cast<const PythonOp*>(node);\n+        if (enforce_importable_) {\n+          throw script::ErrorReport(node->getSourceLocation())\n+              << \"could not export python function call \" << value->name()\n+              << \". Remove calls to python functions before export.\";\n+        }\n+\n+        stmt << \"^\" << value->name();\n+        value->writeScalars(stmt);\n+        printValueList(stmt, node->inputs(), \"(\", \")\");\n+      } break;\n+      case prim::Constant: {\n+        IValue v = toIValue(node->output()).value();\n+        if(v.isTensor()) {\n+          stmt << \"CONSTANTS.c\" << addTensorConstant(std::move(v).toTensor());\n+        } else if(v.isString()) {\n+          // TODO: escape the string correctly by implementing a subset of\n+          // string escapes in both printing and parsing.\n+          stmt << \"\\\"\" << v.toStringRef() << \"\\\"\";\n+        } else if(v.isTensorList()) {\n+          auto tl = v.toTensorListRef();\n+          stmt << \"[\";\n+          const char* delim = \"\";\n+          for(at::Tensor t : tl) {\n+            stmt << delim << \"CONSTANTS.c\" << addTensorConstant(std::move(t));\n+            delim = \", \";\n+          }\n+          stmt << \"]\";\n+        } else {\n+          // TODO: ensure floats always print with their periods\n+          stmt << v;\n+        }\n+      } break;\n+      case prim::None:\n+      case prim::NoneGenerator:\n+      case prim::Undefined: {\n+        stmt << \"None\";\n+      } break;\n+      case prim::FloatToInt: {\n+        printValueList(stmt, node->inputs(), \"int(\", \")\");\n+      } break;\n+      case prim::StringToFloat:\n+      case prim::IntToFloat: {\n+        printValueList(stmt, node->inputs(), \"float(\", \")\");\n+      } break;\n+      case prim::TensorToBool: {\n+        printValueList(stmt, node->inputs(), \"bool(\", \")\");\n+      } break;\n+      case prim::Print: {\n+        printValueList(stmt, node->inputs(), \"print(\",\")\");\n+      } break;\n+      case prim::TupleConstruct: {\n+        printValueList(\n+            stmt, node->inputs(), \"(\", node->inputs().size() == 1 ? \",)\" : \")\");\n+      } break;\n+      case prim::TupleIndex: {\n+        stmt << \"(\" << useOf(node->input()) << \")[\" << node->i(attr::index) << \"]\";\n+      } break;\n+      case prim::TupleSlice: {\n+        stmt << \"(\" << useOf(node->input()) << \")[\" << node->i(attr::beg) << \":\"\n+             << node->i(attr::end) << \"]\";\n+      } break;\n+      case prim::ListConstruct: {\n+        // TODO: when the list is empty and is not a list of tensors,\n+        // we need to annotate it, otherwise it won't be possible\n+        // to infer the type on import\n+        printValueList(stmt, node->inputs(), \"[\", \"]\");\n+      } break;\n+      case prim::fork: {\n+        // the subgraph gets emitted as another function\n+        auto name = genName(\"__forked_function\");\n+        std::shared_ptr<Graph> graph = node->g(attr::Subgraph);\n+        worklist.emplace_back(graph.get(), name);\n+        // and we put a call to fork which invokes that function.\n+        stmt << \"fork(\" << name;\n+        for(const Value* v : node->inputs()) {\n+          stmt << \", \" << useOf(v);\n+        }\n+        stmt << \")\";\n+      } break;\n+      default: {\n+        Symbol kind = node->kind();\n+        stmt << kind.ns().toUnqualString() << \".\" << kind.toUnqualString();\n+        printValueList(stmt, node->inputs(), \"(\", \")\");\n+      } break;\n+    }\n+  }\n+\n+  std::ostream& printBlock(\n+      const Block* root) {\n+    for (const auto* node : root->nodes()) {\n+      printNode(node);\n+    }\n+    return out;\n+  }\n+\n+  void printOneFunction(const Graph& graph, const std::string& name) {\n+    used_names_.clear(); // each graph can reuse local names\n+    // current graph is used to de-dup names within a single graph\n+    scanBlock(graph.block());\n+    assignValuesToTheirUniqueNames(graph.inputs());\n+    out << \"def \" << name << \"(\\n\";\n+    const char * delim = \"    \";\n+    for(auto input : graph.inputs()) {\n+      out << delim << useOf(input) << \": \" << input->type()->python_str();\n+      delim = \",\\n    \";\n+    }\n+    out << \") -> \" << resultType(graph)->python_str() << \":\\n\";\n+    {\n+      auto guard = WithIndented();\n+      // Print body\n+      printBlock(graph.block());\n+      printNode(graph.block()->return_node());\n+    }\n+  }\n+\n+ public:\n+  PythonPrintPass(\n+      std::ostream& out_,\n+      bool enforce_importable = false)\n+      : out(out_), enforce_importable_(enforce_importable) {}\n+\n+  // TODO: we should consider forcing functions to return a single value\n+  // instead of handling this tuple logic both in the compiler and the printer\n+  TypePtr resultType(const Graph& graph) {\n+    if (graph.outputs().size() == 1) {\n+      return graph.outputs().at(0)->type();\n+    } else {\n+      return TupleType::create(\n+          fmap(graph.outputs(), [&](const Value* v) { return v->type(); }));\n+    }\n+  }\n+\n+  void printFunction(const Graph& graph, const std::string& name) {\n+    printOneFunction(graph, name);\n+    while(!worklist.empty()) {\n+      out << \"\\n\\n\";\n+      auto work = worklist.back();\n+      worklist.pop_back();\n+      printOneFunction(*work.first, work.second);\n+    }\n+  }\n+};\n+\n+TORCH_API std::ostream& PythonPrint(std::ostream& out, const Graph& graph) {\n+  PythonPrintPass(out).printFunction(graph, \"script\");\n+  return out;\n+}\n+\n+TORCH_API bool printerHasSpecialCaseFor(Symbol sym) {\n+  // WARNING: by adding a value to this set, you are asserting\n+  // that you have also added special handling of this symbol to\n+  // the printer above. Not adding handling will cause import and export\n+  // of modules with this new operator to fail. This is only required\n+  // for operators without schema. Prefer registering your operator with\n+  // schema to editing this list here. These cases should only be things\n+  // that require special handling because they do not fit normal schema\n+  const static std::unordered_set<Symbol> handled = {\n+    prim::BoolToTensor,", "path": "torch/csrc/jit/passes/python_print.cpp", "position": 543, "original_position": 543, "commit_id": "4cec9824789e15709f325f807220bbb8aef5e49a", "original_commit_id": "4a89c375c91b6ab5d74375a931eac35726a1f5b8", "user": {"login": "wanchaol", "id": 9443650, "node_id": "MDQ6VXNlcjk0NDM2NTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9443650?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wanchaol", "html_url": "https://github.com/wanchaol", "followers_url": "https://api.github.com/users/wanchaol/followers", "following_url": "https://api.github.com/users/wanchaol/following{/other_user}", "gists_url": "https://api.github.com/users/wanchaol/gists{/gist_id}", "starred_url": "https://api.github.com/users/wanchaol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wanchaol/subscriptions", "organizations_url": "https://api.github.com/users/wanchaol/orgs", "repos_url": "https://api.github.com/users/wanchaol/repos", "events_url": "https://api.github.com/users/wanchaol/events{/privacy}", "received_events_url": "https://api.github.com/users/wanchaol/received_events", "type": "User", "site_admin": false}, "body": "Missing ImplicitTensorToNum", "created_at": "2018-11-09T22:40:18Z", "updated_at": "2018-11-23T15:54:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/13616#discussion_r232414639", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13616", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232414639"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13616#discussion_r232414639"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13616"}}, "body_html": "<p>Missing ImplicitTensorToNum</p>", "body_text": "Missing ImplicitTensorToNum"}