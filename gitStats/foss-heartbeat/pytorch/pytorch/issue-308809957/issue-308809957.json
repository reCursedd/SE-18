{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6035", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6035/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6035/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6035/events", "html_url": "https://github.com/pytorch/pytorch/pull/6035", "id": 308809957, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc3NjIzMDQz", "number": 6035, "title": "[distributions] Support python floats in AffineTransform", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-27T03:32:33Z", "updated_at": "2018-04-02T03:56:35Z", "closed_at": "2018-04-02T03:56:35Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6035", "html_url": "https://github.com/pytorch/pytorch/pull/6035", "diff_url": "https://github.com/pytorch/pytorch/pull/6035.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6035.patch"}, "body_html": "<p>This avoids promotion from python float to <code>torch.Tensor</code> for <code>AffineTransform</code>. This appears to be needed so that constraint registration works across CPU and all GPUs.</p>\n<p>Previous discussion at <a href=\"https://github.com/pytorch/pytorch/pull/5931/files/3a25db73c8fa4cf5fe53705bb392c711dd95b980#r176361909\">https://github.com/pytorch/pytorch/pull/5931/files/3a25db73c8fa4cf5fe53705bb392c711dd95b980#r176361909</a></p>\n<h2>Background</h2>\n<p>There are three basic types of objects in torch.distributions:</p>\n<ul>\n<li><a href=\"http://pytorch.org/docs/master/distributions.html#distribution\" rel=\"nofollow\">Distributions</a> are flyweight objects constructed from tensor or float args. They always promote float args to tensors.</li>\n<li><a href=\"http://pytorch.org/docs/master/distributions.html#module-torch.distributions.transforms\" rel=\"nofollow\">Transforms</a> are longer-lived objects (sometimes cached; some are static globals). They can take float arguments. This PR makes <code>AffineTransform</code> avoid promoting float args to tensors.</li>\n<li><a href=\"http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraints\" rel=\"nofollow\">Constraints</a> are long-lived objects. They can take either float or tensor arguments. They do not promote floats to tensors. These are relatively symbolic and are not much more than partially evaluated comparisons, e.g. <code>constraints.positive</code> is basically a symbolic version of <code>lambda x: x &gt; 0</code> that can be stored in a <a href=\"http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraint_registry\" rel=\"nofollow\">ConstraintRegistry</a> table.</li>\n</ul>\n<h2>The Problem</h2>\n<p>Sometimes we want to apply <code>transform_to(constraints.positive)</code> to a <code>torch.Cuda.FloatTensor</code>. This is fine since</p>\n<div class=\"highlight highlight-source-python\"><pre>transform_to(constraints.positive)(x)\n    <span class=\"pl-k\">=</span> ExpTransform()(x)\n    <span class=\"pl-k\">=</span> x.exp()</pre></div>\n<p>which works with any tensor type.</p>\n<p>Other times we want to apply <code>transform_to(constraints.greater_than(1.5))</code> to a <code>torch.cuda.FloatTensor</code>. This is problematic before this PR since</p>\n<div class=\"highlight highlight-source-python\"><pre>transform_to(constraints.greater_than(<span class=\"pl-c1\">1.5</span>))(x)\n    <span class=\"pl-k\">=</span> ComposeTransform([ExpTransform(), AffineTransform(<span class=\"pl-c1\">1.5</span>, <span class=\"pl-c1\">1</span>)])(x)\n    <span class=\"pl-k\">=</span> AffineTransform(<span class=\"pl-c1\">1.5</span>, <span class=\"pl-c1\">1</span>)(x.exp())\n    <span class=\"pl-k\">=</span> t.loc <span class=\"pl-k\">+</span> t.scale <span class=\"pl-k\">*</span> x.exp()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> where t = AffineTransform(1.5, 1)</span></pre></div>\n<p>Before this PR, <code>AffineTransform</code> would promote <code>t.loc</code> and <code>t.scale</code> to tensors. This promotion can happen as early as library load time for some transforms, e.g. <code>transform_to(constraints.unit_interval)</code>. Therefore before this PR, the second example would error at <code>t.scale * x.exp()</code> because <code>t.scale</code> is a [default] <code>torch.FloatTensor</code> whereas <code>x.exp()</code> is a <code>torch.cuda.FloatTensor</code>.</p>\n<h2>Proposed solution</h2>\n<p>This PR merely adds support for python floats as the <code>.loc</code> and <code>.scale</code> parameters of <code>AffineTransform</code>. This should suffice for most purposes since only <code>AffineTransform</code> and a handful of parameter-free transforms are ever stored in the global <code>transform_to</code> and <code>biject_to</code> registries.</p>\n<p>Alternative solutions include:</p>\n<ul>\n<li>allowing promotion from <code>torch.FloatTensor</code> to all other tensor types, e.g. <code>torch.cuda.FloatTensor</code>.</li>\n<li>adding a handful of specific parameter-free transforms like <code>NegateTransform()</code> in lieu of <code>AffineTransform(0, -1)</code>.</li>\n</ul>\n<h2>Tested</h2>\n<ul>\n<li>added a regression test</li>\n</ul>", "body_text": "This avoids promotion from python float to torch.Tensor for AffineTransform. This appears to be needed so that constraint registration works across CPU and all GPUs.\nPrevious discussion at https://github.com/pytorch/pytorch/pull/5931/files/3a25db73c8fa4cf5fe53705bb392c711dd95b980#r176361909\nBackground\nThere are three basic types of objects in torch.distributions:\n\nDistributions are flyweight objects constructed from tensor or float args. They always promote float args to tensors.\nTransforms are longer-lived objects (sometimes cached; some are static globals). They can take float arguments. This PR makes AffineTransform avoid promoting float args to tensors.\nConstraints are long-lived objects. They can take either float or tensor arguments. They do not promote floats to tensors. These are relatively symbolic and are not much more than partially evaluated comparisons, e.g. constraints.positive is basically a symbolic version of lambda x: x > 0 that can be stored in a ConstraintRegistry table.\n\nThe Problem\nSometimes we want to apply transform_to(constraints.positive) to a torch.Cuda.FloatTensor. This is fine since\ntransform_to(constraints.positive)(x)\n    = ExpTransform()(x)\n    = x.exp()\nwhich works with any tensor type.\nOther times we want to apply transform_to(constraints.greater_than(1.5)) to a torch.cuda.FloatTensor. This is problematic before this PR since\ntransform_to(constraints.greater_than(1.5))(x)\n    = ComposeTransform([ExpTransform(), AffineTransform(1.5, 1)])(x)\n    = AffineTransform(1.5, 1)(x.exp())\n    = t.loc + t.scale * x.exp()  # where t = AffineTransform(1.5, 1)\nBefore this PR, AffineTransform would promote t.loc and t.scale to tensors. This promotion can happen as early as library load time for some transforms, e.g. transform_to(constraints.unit_interval). Therefore before this PR, the second example would error at t.scale * x.exp() because t.scale is a [default] torch.FloatTensor whereas x.exp() is a torch.cuda.FloatTensor.\nProposed solution\nThis PR merely adds support for python floats as the .loc and .scale parameters of AffineTransform. This should suffice for most purposes since only AffineTransform and a handful of parameter-free transforms are ever stored in the global transform_to and biject_to registries.\nAlternative solutions include:\n\nallowing promotion from torch.FloatTensor to all other tensor types, e.g. torch.cuda.FloatTensor.\nadding a handful of specific parameter-free transforms like NegateTransform() in lieu of AffineTransform(0, -1).\n\nTested\n\nadded a regression test", "body": "This avoids promotion from python float to `torch.Tensor` for `AffineTransform`. This appears to be needed so that constraint registration works across CPU and all GPUs.\r\n\r\nPrevious discussion at https://github.com/pytorch/pytorch/pull/5931/files/3a25db73c8fa4cf5fe53705bb392c711dd95b980#r176361909\r\n\r\n## Background\r\n\r\nThere are three basic types of objects in torch.distributions:\r\n- [Distributions](http://pytorch.org/docs/master/distributions.html#distribution) are flyweight objects constructed from tensor or float args. They always promote float args to tensors.\r\n- [Transforms](http://pytorch.org/docs/master/distributions.html#module-torch.distributions.transforms) are longer-lived objects (sometimes cached; some are static globals). They can take float arguments. This PR makes `AffineTransform` avoid promoting float args to tensors.\r\n- [Constraints](http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraints) are long-lived objects. They can take either float or tensor arguments. They do not promote floats to tensors. These are relatively symbolic and are not much more than partially evaluated comparisons, e.g. `constraints.positive` is basically a symbolic version of `lambda x: x > 0` that can be stored in a [ConstraintRegistry](http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraint_registry) table.\r\n\r\n## The Problem\r\n\r\nSometimes we want to apply `transform_to(constraints.positive)` to a `torch.Cuda.FloatTensor`. This is fine since\r\n```py\r\ntransform_to(constraints.positive)(x)\r\n    = ExpTransform()(x)\r\n    = x.exp()\r\n```\r\nwhich works with any tensor type.\r\n\r\nOther times we want to apply `transform_to(constraints.greater_than(1.5))` to a `torch.cuda.FloatTensor`. This is problematic before this PR since\r\n```py\r\ntransform_to(constraints.greater_than(1.5))(x)\r\n    = ComposeTransform([ExpTransform(), AffineTransform(1.5, 1)])(x)\r\n    = AffineTransform(1.5, 1)(x.exp())\r\n    = t.loc + t.scale * x.exp()  # where t = AffineTransform(1.5, 1)\r\n```\r\nBefore this PR, `AffineTransform` would promote `t.loc` and `t.scale` to tensors. This promotion can happen as early as library load time for some transforms, e.g. `transform_to(constraints.unit_interval)`. Therefore before this PR, the second example would error at `t.scale * x.exp()` because `t.scale` is a [default] `torch.FloatTensor` whereas `x.exp()` is a `torch.cuda.FloatTensor`.\r\n\r\n## Proposed solution\r\n\r\nThis PR merely adds support for python floats as the `.loc` and `.scale` parameters of `AffineTransform`. This should suffice for most purposes since only `AffineTransform` and a handful of parameter-free transforms are ever stored in the global `transform_to` and `biject_to` registries.\r\n\r\nAlternative solutions include:\r\n- allowing promotion from `torch.FloatTensor` to all other tensor types, e.g. `torch.cuda.FloatTensor`.\r\n- adding a handful of specific parameter-free transforms like `NegateTransform()` in lieu of `AffineTransform(0, -1)`.\r\n\r\n## Tested\r\n\r\n- added a regression test"}