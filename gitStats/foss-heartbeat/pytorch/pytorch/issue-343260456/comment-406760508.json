{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406760508", "html_url": "https://github.com/pytorch/pytorch/pull/9661#issuecomment-406760508", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9661", "id": 406760508, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjc2MDUwOA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-21T01:21:40Z", "updated_at": "2018-07-21T03:46:57Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Nice! Is the second benchmark using the new kernel?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Jul 20, 2018 at 18:03 Ailing ***@***.***&gt; wrote:\n This PR fix <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335055825\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8817\" href=\"https://github.com/pytorch/pytorch/issues/8817\">#8817</a> &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335055825\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8817\" href=\"https://github.com/pytorch/pytorch/issues/8817\">#8817</a>&gt; the\n perf gap from GPU argmax/argmin with numpy argmax/argmin.\n\n I will explain using argmax, where argmin is almost the same. There were\n two issues:\n\n    1. The way kernel was designed was not friendly to tensors with very\n    long rows. The old logic was designed to use a kernel with\n    BlockDim(16,32,1), all 16 threads in the block handles the reduction\n    over one row. Given an edge case of [1, 1440000], we actually only\n    have 1 thread doing the reduction over 1440000 elements which is slow. We\n    used BlockDim.y=32 to coordinate handling different rows, which is\n    actually not necessary. A better logic is to have BlockDim(512,1,1),\n    let 512 threads working on a single row. And let GridDim(min(1024,\n    num_rows)) to handle reduction for different rows.\n    2. When dim=None(argmax over the whole tensor), we squeeze the tensor\n    to a 1-d tensor first and then reduce over dim=0. This actually always put\n    us in an edge case that our old kernel fails to handle efficiently. To fix,\n    it actually makes sense to do it multiple passes over each dimension, and\n    then translates the index back.\n\n This PR implements the logic above and here's a brief perf comparison.\n\n Comparison for kernel: argmax(dim=1) of shape\n [1, 1440000] [14400, 14400]\n old kernel [16,32]: 37.5 4.49\n new kernel [512, 1]: 1.96 3.58\n numpy: 2.25 446\n cpu: 2.01 448\n\n Comparing for multiple pass: argmax() of shape\n [14400, 14400] [1, 1440000]\n gpu with multi pass: 3.59 2.35\n gpu without multi pass: 175 1.97\n numpy: 458 2.17\n cpu with multi pass: 733 22\n cpu without multi pass: 411 1.98\n\n We can see that with a regular large tensor shape, multiple pass could\n help a lot in GPU perf.\n But doing so is not good from cpu side. On the other side, even without\n this multiple pass, our GPU perf with new kernel is still better than CPU\n and numpy. So I'm not sure if we would like to aggressively optimize for\n this. Attaching some perf numbers for our reference. IIRC we do this\n multiple pass logic in kernels of torch.max() so that we could keep CPU\n separate. We could also do that if it's preferred.\n\n cc: <a class=\"user-mention\" href=\"https://github.com/soumith\">@soumith</a> &lt;<a href=\"https://github.com/soumith\">https://github.com/soumith</a>&gt; <a class=\"user-mention\" href=\"https://github.com/SsnL\">@SsnL</a> &lt;<a href=\"https://github.com/SsnL\">https://github.com/SsnL</a>&gt;\n ------------------------------\n You can view, comment on, or merge this pull request online at:\n\n   <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"343260456\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9661\" href=\"https://github.com/pytorch/pytorch/pull/9661\">#9661</a>\n Commit Summary\n\n    - fix argmax/min perf gap with numpy\n\n File Changes\n\n    - *M* aten/src/THC/THCReduceAll.cuh\n    &lt;<a href=\"https://github.com/pytorch/pytorch/pull/9661/files#diff-0\">https://github.com/pytorch/pytorch/pull/9661/files#diff-0</a>&gt; (2)\n    - *M* aten/src/THC/THCTensorMathReduce.cuh\n    &lt;<a href=\"https://github.com/pytorch/pytorch/pull/9661/files#diff-1\">https://github.com/pytorch/pytorch/pull/9661/files#diff-1</a>&gt; (44)\n    - *M* aten/src/THC/generic/THCTensorMathReduce.cu\n    &lt;<a href=\"https://github.com/pytorch/pytorch/pull/9661/files#diff-2\">https://github.com/pytorch/pytorch/pull/9661/files#diff-2</a>&gt; (2)\n    - *M* torch/functional.py\n    &lt;<a href=\"https://github.com/pytorch/pytorch/pull/9661/files#diff-3\">https://github.com/pytorch/pytorch/pull/9661/files#diff-3</a>&gt; (38)\n\n Patch Links:\n\n    - <a href=\"https://github.com/pytorch/pytorch/pull/9661.patch\">https://github.com/pytorch/pytorch/pull/9661.patch</a>\n    - <a href=\"https://github.com/pytorch/pytorch/pull/9661.diff\">https://github.com/pytorch/pytorch/pull/9661.diff</a>\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"343260456\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9661\" href=\"https://github.com/pytorch/pytorch/pull/9661\">#9661</a>&gt;, or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AFaWZcsTU6Et_PnlvDjSdz7xrb2mZtqVks5uIlO8gaJpZM4VZU60\">https://github.com/notifications/unsubscribe-auth/AFaWZcsTU6Et_PnlvDjSdz7xrb2mZtqVks5uIlO8gaJpZM4VZU60</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Nice! Is the second benchmark using the new kernel?\n\u2026\nOn Fri, Jul 20, 2018 at 18:03 Ailing ***@***.***> wrote:\n This PR fix #8817 <#8817> the\n perf gap from GPU argmax/argmin with numpy argmax/argmin.\n\n I will explain using argmax, where argmin is almost the same. There were\n two issues:\n\n    1. The way kernel was designed was not friendly to tensors with very\n    long rows. The old logic was designed to use a kernel with\n    BlockDim(16,32,1), all 16 threads in the block handles the reduction\n    over one row. Given an edge case of [1, 1440000], we actually only\n    have 1 thread doing the reduction over 1440000 elements which is slow. We\n    used BlockDim.y=32 to coordinate handling different rows, which is\n    actually not necessary. A better logic is to have BlockDim(512,1,1),\n    let 512 threads working on a single row. And let GridDim(min(1024,\n    num_rows)) to handle reduction for different rows.\n    2. When dim=None(argmax over the whole tensor), we squeeze the tensor\n    to a 1-d tensor first and then reduce over dim=0. This actually always put\n    us in an edge case that our old kernel fails to handle efficiently. To fix,\n    it actually makes sense to do it multiple passes over each dimension, and\n    then translates the index back.\n\n This PR implements the logic above and here's a brief perf comparison.\n\n Comparison for kernel: argmax(dim=1) of shape\n [1, 1440000] [14400, 14400]\n old kernel [16,32]: 37.5 4.49\n new kernel [512, 1]: 1.96 3.58\n numpy: 2.25 446\n cpu: 2.01 448\n\n Comparing for multiple pass: argmax() of shape\n [14400, 14400] [1, 1440000]\n gpu with multi pass: 3.59 2.35\n gpu without multi pass: 175 1.97\n numpy: 458 2.17\n cpu with multi pass: 733 22\n cpu without multi pass: 411 1.98\n\n We can see that with a regular large tensor shape, multiple pass could\n help a lot in GPU perf.\n But doing so is not good from cpu side. On the other side, even without\n this multiple pass, our GPU perf with new kernel is still better than CPU\n and numpy. So I'm not sure if we would like to aggressively optimize for\n this. Attaching some perf numbers for our reference. IIRC we do this\n multiple pass logic in kernels of torch.max() so that we could keep CPU\n separate. We could also do that if it's preferred.\n\n cc: @soumith <https://github.com/soumith> @SsnL <https://github.com/SsnL>\n ------------------------------\n You can view, comment on, or merge this pull request online at:\n\n   #9661\n Commit Summary\n\n    - fix argmax/min perf gap with numpy\n\n File Changes\n\n    - *M* aten/src/THC/THCReduceAll.cuh\n    <https://github.com/pytorch/pytorch/pull/9661/files#diff-0> (2)\n    - *M* aten/src/THC/THCTensorMathReduce.cuh\n    <https://github.com/pytorch/pytorch/pull/9661/files#diff-1> (44)\n    - *M* aten/src/THC/generic/THCTensorMathReduce.cu\n    <https://github.com/pytorch/pytorch/pull/9661/files#diff-2> (2)\n    - *M* torch/functional.py\n    <https://github.com/pytorch/pytorch/pull/9661/files#diff-3> (38)\n\n Patch Links:\n\n    - https://github.com/pytorch/pytorch/pull/9661.patch\n    - https://github.com/pytorch/pytorch/pull/9661.diff\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#9661>, or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AFaWZcsTU6Et_PnlvDjSdz7xrb2mZtqVks5uIlO8gaJpZM4VZU60>\n .", "body": "Nice! Is the second benchmark using the new kernel?\r\n\r\nOn Fri, Jul 20, 2018 at 18:03 Ailing <notifications@github.com> wrote:\r\n\r\n> This PR fix #8817 <https://github.com/pytorch/pytorch/issues/8817> the\r\n> perf gap from GPU argmax/argmin with numpy argmax/argmin.\r\n>\r\n> I will explain using argmax, where argmin is almost the same. There were\r\n> two issues:\r\n>\r\n>    1. The way kernel was designed was not friendly to tensors with very\r\n>    long rows. The old logic was designed to use a kernel with\r\n>    BlockDim(16,32,1), all 16 threads in the block handles the reduction\r\n>    over one row. Given an edge case of [1, 1440000], we actually only\r\n>    have 1 thread doing the reduction over 1440000 elements which is slow. We\r\n>    used BlockDim.y=32 to coordinate handling different rows, which is\r\n>    actually not necessary. A better logic is to have BlockDim(512,1,1),\r\n>    let 512 threads working on a single row. And let GridDim(min(1024,\r\n>    num_rows)) to handle reduction for different rows.\r\n>    2. When dim=None(argmax over the whole tensor), we squeeze the tensor\r\n>    to a 1-d tensor first and then reduce over dim=0. This actually always put\r\n>    us in an edge case that our old kernel fails to handle efficiently. To fix,\r\n>    it actually makes sense to do it multiple passes over each dimension, and\r\n>    then translates the index back.\r\n>\r\n> This PR implements the logic above and here's a brief perf comparison.\r\n>\r\n> Comparison for kernel: argmax(dim=1) of shape\r\n> [1, 1440000] [14400, 14400]\r\n> old kernel [16,32]: 37.5 4.49\r\n> new kernel [512, 1]: 1.96 3.58\r\n> numpy: 2.25 446\r\n> cpu: 2.01 448\r\n>\r\n> Comparing for multiple pass: argmax() of shape\r\n> [14400, 14400] [1, 1440000]\r\n> gpu with multi pass: 3.59 2.35\r\n> gpu without multi pass: 175 1.97\r\n> numpy: 458 2.17\r\n> cpu with multi pass: 733 22\r\n> cpu without multi pass: 411 1.98\r\n>\r\n> We can see that with a regular large tensor shape, multiple pass could\r\n> help a lot in GPU perf.\r\n> But doing so is not good from cpu side. On the other side, even without\r\n> this multiple pass, our GPU perf with new kernel is still better than CPU\r\n> and numpy. So I'm not sure if we would like to aggressively optimize for\r\n> this. Attaching some perf numbers for our reference. IIRC we do this\r\n> multiple pass logic in kernels of torch.max() so that we could keep CPU\r\n> separate. We could also do that if it's preferred.\r\n>\r\n> cc: @soumith <https://github.com/soumith> @SsnL <https://github.com/SsnL>\r\n> ------------------------------\r\n> You can view, comment on, or merge this pull request online at:\r\n>\r\n>   https://github.com/pytorch/pytorch/pull/9661\r\n> Commit Summary\r\n>\r\n>    - fix argmax/min perf gap with numpy\r\n>\r\n> File Changes\r\n>\r\n>    - *M* aten/src/THC/THCReduceAll.cuh\r\n>    <https://github.com/pytorch/pytorch/pull/9661/files#diff-0> (2)\r\n>    - *M* aten/src/THC/THCTensorMathReduce.cuh\r\n>    <https://github.com/pytorch/pytorch/pull/9661/files#diff-1> (44)\r\n>    - *M* aten/src/THC/generic/THCTensorMathReduce.cu\r\n>    <https://github.com/pytorch/pytorch/pull/9661/files#diff-2> (2)\r\n>    - *M* torch/functional.py\r\n>    <https://github.com/pytorch/pytorch/pull/9661/files#diff-3> (38)\r\n>\r\n> Patch Links:\r\n>\r\n>    - https://github.com/pytorch/pytorch/pull/9661.patch\r\n>    - https://github.com/pytorch/pytorch/pull/9661.diff\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/pytorch/pytorch/pull/9661>, or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AFaWZcsTU6Et_PnlvDjSdz7xrb2mZtqVks5uIlO8gaJpZM4VZU60>\r\n> .\r\n>\r\n"}