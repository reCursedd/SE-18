{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9661", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9661/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9661/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9661/events", "html_url": "https://github.com/pytorch/pytorch/pull/9661", "id": 343260456, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAyOTY2Mjg2", "number": 9661, "title": "fix argmax/min perf gap with numpy", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-07-20T22:03:29Z", "updated_at": "2018-11-23T15:47:55Z", "closed_at": "2018-08-06T21:48:40Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9661", "html_url": "https://github.com/pytorch/pytorch/pull/9661", "diff_url": "https://github.com/pytorch/pytorch/pull/9661.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9661.patch"}, "body_html": "<p>This PR <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #8817.\">fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335055825\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8817\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8817/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8817\">#8817</a> (also master track of 0.4.1 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"339582935\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9280\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9280/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9280\">#9280</a>) the perf gap from GPU argmax/argmin with numpy argmax/argmin.</p>\n<p>I will explain using argmax, where argmin is almost the same. There were two issues:</p>\n<ol>\n<li>The way kernel was designed was not friendly to tensors with very long rows. The old logic was designed to use a kernel with <code>BlockDim(16,32,1)</code>, all 16 threads in the same column of block handles the reduction over one row. Given an edge case of <code>[1, 1440000]</code>, we actually only have 16 thread doing the reduction over 1440000 elements which is slow. We used <code>BlockDim.y=32</code> to coordinate handling different rows, which is actually not necessary. A better logic is to have <code>BlockDim(512,1,1)</code>, let 512 threads working on a single row. And let <code>GridDim(min(1024, num_rows))</code> to handle reduction for different rows.</li>\n<li>When dim=None(argmax over the  whole tensor), we squeeze the tensor to a 1-d tensor first and then reduce over dim=0. This actually always put us in an edge case that our old kernel fails to handle efficiently. To fix, it actually makes sense to do it multiple passes over each dimension, and then translates the index back.</li>\n</ol>\n<p>This PR implements the logic above and here's a brief perf comparison.</p>\n<p>Comparison for kernel: <code>argmax(dim=1)</code> of shape</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>[1, 1440000]</th>\n<th>[14400, 14400]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>old kernel [16,32]:</td>\n<td>37.5</td>\n<td>4.49</td>\n</tr>\n<tr>\n<td>new kernel [512, 1]:</td>\n<td>1.96</td>\n<td>3.58</td>\n</tr>\n<tr>\n<td>numpy:</td>\n<td>2.25</td>\n<td>446</td>\n</tr>\n<tr>\n<td>cpu:</td>\n<td>2.01</td>\n<td>448</td>\n</tr>\n</tbody>\n</table>\n<p>Comparing for multiple pass: <code>argmax()</code> of shape</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>[14400, 14400]</th>\n<th>[1, 1440000]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>gpu with multi pass:</td>\n<td>3.59</td>\n<td>2.35</td>\n</tr>\n<tr>\n<td>gpu without multi pass:</td>\n<td>175</td>\n<td>1.97</td>\n</tr>\n<tr>\n<td>numpy:</td>\n<td>458</td>\n<td>2.17</td>\n</tr>\n<tr>\n<td>cpu with multi pass:</td>\n<td>733</td>\n<td>22</td>\n</tr>\n<tr>\n<td>cpu without multi pass:</td>\n<td>411</td>\n<td>1.98</td>\n</tr>\n</tbody>\n</table>\n<p>We can see that with a regular large tensor shape, multiple pass could help a lot in GPU perf.<br>\nBut doing so is not good from cpu side. On the other side, even without this multiple pass, our GPU perf with new kernel is still better than CPU and numpy. So I'm not sure if we would like to aggressively optimize for this. Attaching some perf numbers for our reference.  IIRC we do this multiple pass logic in kernels of torch.max() so that we could keep CPU separate. We could also do that if it's preferred.</p>\n<p>cc: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a></p>", "body_text": "This PR fix #8817 (also master track of 0.4.1 #9280) the perf gap from GPU argmax/argmin with numpy argmax/argmin.\nI will explain using argmax, where argmin is almost the same. There were two issues:\n\nThe way kernel was designed was not friendly to tensors with very long rows. The old logic was designed to use a kernel with BlockDim(16,32,1), all 16 threads in the same column of block handles the reduction over one row. Given an edge case of [1, 1440000], we actually only have 16 thread doing the reduction over 1440000 elements which is slow. We used BlockDim.y=32 to coordinate handling different rows, which is actually not necessary. A better logic is to have BlockDim(512,1,1), let 512 threads working on a single row. And let GridDim(min(1024, num_rows)) to handle reduction for different rows.\nWhen dim=None(argmax over the  whole tensor), we squeeze the tensor to a 1-d tensor first and then reduce over dim=0. This actually always put us in an edge case that our old kernel fails to handle efficiently. To fix, it actually makes sense to do it multiple passes over each dimension, and then translates the index back.\n\nThis PR implements the logic above and here's a brief perf comparison.\nComparison for kernel: argmax(dim=1) of shape\n\n\n\n\n[1, 1440000]\n[14400, 14400]\n\n\n\n\nold kernel [16,32]:\n37.5\n4.49\n\n\nnew kernel [512, 1]:\n1.96\n3.58\n\n\nnumpy:\n2.25\n446\n\n\ncpu:\n2.01\n448\n\n\n\nComparing for multiple pass: argmax() of shape\n\n\n\n\n[14400, 14400]\n[1, 1440000]\n\n\n\n\ngpu with multi pass:\n3.59\n2.35\n\n\ngpu without multi pass:\n175\n1.97\n\n\nnumpy:\n458\n2.17\n\n\ncpu with multi pass:\n733\n22\n\n\ncpu without multi pass:\n411\n1.98\n\n\n\nWe can see that with a regular large tensor shape, multiple pass could help a lot in GPU perf.\nBut doing so is not good from cpu side. On the other side, even without this multiple pass, our GPU perf with new kernel is still better than CPU and numpy. So I'm not sure if we would like to aggressively optimize for this. Attaching some perf numbers for our reference.  IIRC we do this multiple pass logic in kernels of torch.max() so that we could keep CPU separate. We could also do that if it's preferred.\ncc: @soumith  @SsnL", "body": "This PR fix #8817 (also master track of 0.4.1 #9280) the perf gap from GPU argmax/argmin with numpy argmax/argmin.\r\n\r\n\r\nI will explain using argmax, where argmin is almost the same. There were two issues:\r\n1. The way kernel was designed was not friendly to tensors with very long rows. The old logic was designed to use a kernel with `BlockDim(16,32,1)`, all 16 threads in the same column of block handles the reduction over one row. Given an edge case of `[1, 1440000]`, we actually only have 16 thread doing the reduction over 1440000 elements which is slow. We used `BlockDim.y=32` to coordinate handling different rows, which is actually not necessary. A better logic is to have `BlockDim(512,1,1)`, let 512 threads working on a single row. And let `GridDim(min(1024, num_rows))` to handle reduction for different rows.\r\n2. When dim=None(argmax over the  whole tensor), we squeeze the tensor to a 1-d tensor first and then reduce over dim=0. This actually always put us in an edge case that our old kernel fails to handle efficiently. To fix, it actually makes sense to do it multiple passes over each dimension, and then translates the index back. \r\n\r\n\r\n\r\nThis PR implements the logic above and here's a brief perf comparison.\r\n\r\nComparison for kernel: `argmax(dim=1)` of shape \r\n\r\n|  | [1, 1440000]  | [14400, 14400]|\r\n|---|---|---|\r\n|old kernel [16,32]:|37.5|4.49|\r\n|new kernel [512, 1]:|1.96| 3.58|\r\n|numpy:\t|2.25|446|\r\n|cpu:|2.01|448|\r\n\r\nComparing for multiple pass: `argmax()` of shape \r\n\r\n| |[14400, 14400] | [1, 1440000]|\r\n|---|---|---|\r\n|gpu with multi pass:|  3.59|\t2.35|\r\n|gpu without multi pass:|  175 | 1.97|\r\n|numpy:| 458  |2.17|\r\n|cpu with multi pass:\t|  733 | 22|\r\n|cpu without multi pass: |  411| 1.98|\r\n\r\nWe can see that with a regular large tensor shape, multiple pass could help a lot in GPU perf.\r\nBut doing so is not good from cpu side. On the other side, even without this multiple pass, our GPU perf with new kernel is still better than CPU and numpy. So I'm not sure if we would like to aggressively optimize for this. Attaching some perf numbers for our reference.  IIRC we do this multiple pass logic in kernels of torch.max() so that we could keep CPU separate. We could also do that if it's preferred.\r\n\r\ncc: @soumith  @SsnL \r\n\r\n\r\n\r\n"}