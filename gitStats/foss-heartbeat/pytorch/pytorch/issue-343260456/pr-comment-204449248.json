{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204449248", "pull_request_review_id": 139530220, "id": 204449248, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDQ0OTI0OA==", "diff_hunk": "@@ -234,7 +257,13 @@ def argmax(input, dim=None, keepdim=False):\n         tensor([ 0,  2,  0,  1])\n     \"\"\"\n     if dim is None:\n-        return torch._argmax(input.contiguous().view(-1), dim=0, keepdim=False)\n+        # Instead of squeeze to 1 dimension, do it multiple passes for better perf.", "path": "torch/functional.py", "position": 35, "original_position": 35, "commit_id": "492bce85d4859c74db558b1189b98181c8a948ae", "original_commit_id": "492bce85d4859c74db558b1189b98181c8a948ae", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I'm not 100% convinced that having multiple passes is a clear win. There are different considerations we have to take into account here:\r\n1. You did this, because it exposes more paralellism on the GPU, but this is only because of how our kernel is written. This will likely be slower on the CPU.\r\n2. If the input is actually very small (think of cases when the kernel launch overhead is larger than the actual cost of the whole compute), then this will make the op multiple times slower (because we'll have to launch multiple kernels).", "created_at": "2018-07-23T15:36:51Z", "updated_at": "2018-11-23T15:47:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/9661#discussion_r204449248", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9661", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204449248"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9661#discussion_r204449248"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9661"}}, "body_html": "<p>I'm not 100% convinced that having multiple passes is a clear win. There are different considerations we have to take into account here:</p>\n<ol>\n<li>You did this, because it exposes more paralellism on the GPU, but this is only because of how our kernel is written. This will likely be slower on the CPU.</li>\n<li>If the input is actually very small (think of cases when the kernel launch overhead is larger than the actual cost of the whole compute), then this will make the op multiple times slower (because we'll have to launch multiple kernels).</li>\n</ol>", "body_text": "I'm not 100% convinced that having multiple passes is a clear win. There are different considerations we have to take into account here:\n\nYou did this, because it exposes more paralellism on the GPU, but this is only because of how our kernel is written. This will likely be slower on the CPU.\nIf the input is actually very small (think of cases when the kernel launch overhead is larger than the actual cost of the whole compute), then this will make the op multiple times slower (because we'll have to launch multiple kernels)."}