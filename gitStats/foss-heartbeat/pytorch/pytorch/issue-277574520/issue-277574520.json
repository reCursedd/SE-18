{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3930", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3930/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3930/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3930/events", "html_url": "https://github.com/pytorch/pytorch/issues/3930", "id": 277574520, "node_id": "MDU6SXNzdWUyNzc1NzQ1MjA=", "number": 3930, "title": "Move replicate from DataParallel to C", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-11-28T23:23:53Z", "updated_at": "2017-11-29T11:55:21Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>It can take as long as 180ms for networks as large as ResNet1001, mostly because it has a lot of tiny parameters, and Python loops take forever. The broadcasts itself are quite fast. The snippet below prints 34ms for the first loop and 4ms for the second one.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time                                                 \n<span class=\"pl-k\">import</span> torch                                                \n<span class=\"pl-k\">from</span> torch.cuda.comm <span class=\"pl-k\">import</span> broadcast_coalesced             \n                                                            \nparams <span class=\"pl-k\">=</span> [torch.cuda.FloatTensor(<span class=\"pl-c1\">3500</span>) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3000</span>)]\n                                                            \n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):                                         \n    start <span class=\"pl-k\">=</span> time.time()                                     \n    broadcast_coalesced(params, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])                     \n    torch.cuda.synchronize()                                \n    end <span class=\"pl-k\">=</span> time.time()                                       \n    <span class=\"pl-c1\">print</span>(((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ms<span class=\"pl-pds\">'</span></span>)                     \n                                                            \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">80</span>)                                             \nflat_param <span class=\"pl-k\">=</span> torch.cat(params)                              \n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):                                         \n    start <span class=\"pl-k\">=</span> time.time()                                     \n    broadcast_coalesced([flat_param], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])               \n    torch.cuda.synchronize()                                \n    end <span class=\"pl-k\">=</span> time.time()                                       \n    <span class=\"pl-c1\">print</span>(((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ms<span class=\"pl-pds\">'</span></span>)                     </pre></div>", "body_text": "It can take as long as 180ms for networks as large as ResNet1001, mostly because it has a lot of tiny parameters, and Python loops take forever. The broadcasts itself are quite fast. The snippet below prints 34ms for the first loop and 4ms for the second one.\nimport time                                                 \nimport torch                                                \nfrom torch.cuda.comm import broadcast_coalesced             \n                                                            \nparams = [torch.cuda.FloatTensor(3500) for i in range(3000)]\n                                                            \nfor i in range(10):                                         \n    start = time.time()                                     \n    broadcast_coalesced(params, [0, 1])                     \n    torch.cuda.synchronize()                                \n    end = time.time()                                       \n    print(((end - start) * 1000), 'ms')                     \n                                                            \nprint('-' * 80)                                             \nflat_param = torch.cat(params)                              \nfor i in range(10):                                         \n    start = time.time()                                     \n    broadcast_coalesced([flat_param], [0, 1])               \n    torch.cuda.synchronize()                                \n    end = time.time()                                       \n    print(((end - start) * 1000), 'ms')", "body": "It can take as long as 180ms for networks as large as ResNet1001, mostly because it has a lot of tiny parameters, and Python loops take forever. The broadcasts itself are quite fast. The snippet below prints 34ms for the first loop and 4ms for the second one.\r\n\r\n```python\r\nimport time                                                 \r\nimport torch                                                \r\nfrom torch.cuda.comm import broadcast_coalesced             \r\n                                                            \r\nparams = [torch.cuda.FloatTensor(3500) for i in range(3000)]\r\n                                                            \r\nfor i in range(10):                                         \r\n    start = time.time()                                     \r\n    broadcast_coalesced(params, [0, 1])                     \r\n    torch.cuda.synchronize()                                \r\n    end = time.time()                                       \r\n    print(((end - start) * 1000), 'ms')                     \r\n                                                            \r\nprint('-' * 80)                                             \r\nflat_param = torch.cat(params)                              \r\nfor i in range(10):                                         \r\n    start = time.time()                                     \r\n    broadcast_coalesced([flat_param], [0, 1])               \r\n    torch.cuda.synchronize()                                \r\n    end = time.time()                                       \r\n    print(((end - start) * 1000), 'ms')                     \r\n```"}