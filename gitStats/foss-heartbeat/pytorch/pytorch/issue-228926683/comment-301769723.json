{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/301769723", "html_url": "https://github.com/pytorch/pytorch/issues/1567#issuecomment-301769723", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1567", "id": 301769723, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTc2OTcyMw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-16T12:41:32Z", "updated_at": "2017-05-16T12:41:32Z", "author_association": "MEMBER", "body_html": "<p>I'm not sure how would you like to do this. If you want to guarantee consistency of snapshots you have to either clone all parameters and objects or stop the training while the serialization is performed, and both these options will have fairly high overhead (with the first one also significantly increasing memory pressure). If you're worried about time spent in pickling you can only save the <code>state_dict</code> which should have a very small <code>pickle</code> overhead (there's only a dict, a few strings and tensors). Additionally, <code>torch.save</code> accepts a <code>file</code> object and asynchronous operation could result in it being closed in the main thread before all the data is flushed e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>my_file<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n    torch.save(model, f)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.save will return immediately, f will be closed and the thread will be unable to write to the file</span></pre></div>", "body_text": "I'm not sure how would you like to do this. If you want to guarantee consistency of snapshots you have to either clone all parameters and objects or stop the training while the serialization is performed, and both these options will have fairly high overhead (with the first one also significantly increasing memory pressure). If you're worried about time spent in pickling you can only save the state_dict which should have a very small pickle overhead (there's only a dict, a few strings and tensors). Additionally, torch.save accepts a file object and asynchronous operation could result in it being closed in the main thread before all the data is flushed e.g.\nwith open('my_file', 'wb') as f:\n    torch.save(model, f)\n# torch.save will return immediately, f will be closed and the thread will be unable to write to the file", "body": "I'm not sure how would you like to do this. If you want to guarantee consistency of snapshots you have to either clone all parameters and objects or stop the training while the serialization is performed, and both these options will have fairly high overhead (with the first one also significantly increasing memory pressure). If you're worried about time spent in pickling you can only save the `state_dict` which should have a very small `pickle` overhead (there's only a dict, a few strings and tensors). Additionally, `torch.save` accepts a `file` object and asynchronous operation could result in it being closed in the main thread before all the data is flushed e.g.\r\n```python\r\nwith open('my_file', 'wb') as f:\r\n    torch.save(model, f)\r\n# torch.save will return immediately, f will be closed and the thread will be unable to write to the file\r\n```"}