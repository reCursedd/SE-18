{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/346412197", "html_url": "https://github.com/pytorch/pytorch/pull/3838#issuecomment-346412197", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3838", "id": 346412197, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjQxMjE5Nw==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-22T16:59:49Z", "updated_at": "2017-11-22T16:59:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Note that cpu() tried to change density even before the change to use the ATen version, i.e. you would get:</p>\n<pre><code>&gt;&gt;&gt; a=Variable(torch.sparse.FloatTensor(2,3), requires_grad=True).cpu()\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/variable.py\", line 282, in cpu\n    return self.type(getattr(torch, type(self.data).__name__))\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/variable.py\", line 266, in type\n    return Type.apply(self, t)\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/_functions/tensor.py\", line 135, in forward\n    return i.type(dest_type)\n  File \"/data/users/gchanan/pytorch9g/torch/_utils.py\", line 28, in _type\n    raise RuntimeError(\"Cannot cast sparse tensor to dense tensor\")\nRuntimeError: Cannot cast sparse tensor to dense tensor\n</code></pre>\n<p>(whether coming from a cpu or cuda sparse tensor)</p>\n<p>Now this works for the cpu case:</p>\n<pre><code>&gt;&gt;&gt; a=Variable(torch.sparse.FloatTensor(2,3), requires_grad=True).cpu()\n&gt;&gt;&gt; a\nVariable containing:FloatTensor of size 2x3 with indices:\n[torch.LongTensor with no dimension]\nand values:\n[torch.FloatTensor with no dimension]\n</code></pre>\n<p>but not for the cuda transfer case:</p>\n<pre><code>&gt;&gt;&gt; a=Variable(torch.cuda.sparse.FloatTensor(2,3), requires_grad=True).cpu()\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nRuntimeError: tensor is not implemented for type torch.sparse.FloatTensor\n&gt;&gt;&gt;\n</code></pre>\n<p>Implementing <code>tensor</code> should come with moving sparse Embedding to ATen.</p>", "body_text": "Note that cpu() tried to change density even before the change to use the ATen version, i.e. you would get:\n>>> a=Variable(torch.sparse.FloatTensor(2,3), requires_grad=True).cpu()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/variable.py\", line 282, in cpu\n    return self.type(getattr(torch, type(self.data).__name__))\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/variable.py\", line 266, in type\n    return Type.apply(self, t)\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/_functions/tensor.py\", line 135, in forward\n    return i.type(dest_type)\n  File \"/data/users/gchanan/pytorch9g/torch/_utils.py\", line 28, in _type\n    raise RuntimeError(\"Cannot cast sparse tensor to dense tensor\")\nRuntimeError: Cannot cast sparse tensor to dense tensor\n\n(whether coming from a cpu or cuda sparse tensor)\nNow this works for the cpu case:\n>>> a=Variable(torch.sparse.FloatTensor(2,3), requires_grad=True).cpu()\n>>> a\nVariable containing:FloatTensor of size 2x3 with indices:\n[torch.LongTensor with no dimension]\nand values:\n[torch.FloatTensor with no dimension]\n\nbut not for the cuda transfer case:\n>>> a=Variable(torch.cuda.sparse.FloatTensor(2,3), requires_grad=True).cpu()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: tensor is not implemented for type torch.sparse.FloatTensor\n>>>\n\nImplementing tensor should come with moving sparse Embedding to ATen.", "body": "Note that cpu() tried to change density even before the change to use the ATen version, i.e. you would get:\r\n```\r\n>>> a=Variable(torch.sparse.FloatTensor(2,3), requires_grad=True).cpu()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/variable.py\", line 282, in cpu\r\n    return self.type(getattr(torch, type(self.data).__name__))\r\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/variable.py\", line 266, in type\r\n    return Type.apply(self, t)\r\n  File \"/data/users/gchanan/pytorch9g/torch/autograd/_functions/tensor.py\", line 135, in forward\r\n    return i.type(dest_type)\r\n  File \"/data/users/gchanan/pytorch9g/torch/_utils.py\", line 28, in _type\r\n    raise RuntimeError(\"Cannot cast sparse tensor to dense tensor\")\r\nRuntimeError: Cannot cast sparse tensor to dense tensor\r\n```\r\n(whether coming from a cpu or cuda sparse tensor)\r\n\r\nNow this works for the cpu case:\r\n```\r\n>>> a=Variable(torch.sparse.FloatTensor(2,3), requires_grad=True).cpu()\r\n>>> a\r\nVariable containing:FloatTensor of size 2x3 with indices:\r\n[torch.LongTensor with no dimension]\r\nand values:\r\n[torch.FloatTensor with no dimension]\r\n```\r\n\r\nbut not for the cuda transfer case:\r\n```\r\n>>> a=Variable(torch.cuda.sparse.FloatTensor(2,3), requires_grad=True).cpu()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: tensor is not implemented for type torch.sparse.FloatTensor\r\n>>>\r\n```\r\n\r\nImplementing `tensor` should come with moving sparse Embedding to ATen."}