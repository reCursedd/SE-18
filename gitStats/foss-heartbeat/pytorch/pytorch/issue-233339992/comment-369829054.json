{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369829054", "html_url": "https://github.com/pytorch/pytorch/issues/1708#issuecomment-369829054", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1708", "id": 369829054, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTgyOTA1NA==", "user": {"login": "tstandley", "id": 17170654, "node_id": "MDQ6VXNlcjE3MTcwNjU0", "avatar_url": "https://avatars2.githubusercontent.com/u/17170654?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tstandley", "html_url": "https://github.com/tstandley", "followers_url": "https://api.github.com/users/tstandley/followers", "following_url": "https://api.github.com/users/tstandley/following{/other_user}", "gists_url": "https://api.github.com/users/tstandley/gists{/gist_id}", "starred_url": "https://api.github.com/users/tstandley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tstandley/subscriptions", "organizations_url": "https://api.github.com/users/tstandley/orgs", "repos_url": "https://api.github.com/users/tstandley/repos", "events_url": "https://api.github.com/users/tstandley/events{/privacy}", "received_events_url": "https://api.github.com/users/tstandley/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-02T05:58:16Z", "updated_at": "2018-03-02T05:58:16Z", "author_association": "NONE", "body_html": "<p>I'm not sure I buy the \"bandwidth bound\" explanation either. If you increase the kernal size from 3x3 to say 5x5, your operation needs the same amount of non-cache memory, but takes way longer.</p>\n<p>I don't know how to program cuda, but here's how I see the operation working:<br>\nLet's say we run a Conv2d(1024, 1024, kernel_size=3, groups=1024) on a shape 20x20x1024 tensor:</p>\n<p>Read a single channel from a single layer into cache. That's 400 floats (1600 bytes). It will fit into cache.<br>\nRead that channels worth of parameters into cache (that's just 9 floats)</p>\n<p>Then we do the following 18x18 times:<br>\nMultiply those 9 floats (weights) by the appropriate 9 floats in the channel from cache.<br>\nStore the output in the appropriate cell in a 18x18 matrix in main memory.</p>\n<p>Here we have one main memory read and one main memory write per channel per spacial location.</p>\n<p>If we do the same operation with Conv2d(1024, 1024, kernel_size=5, groups=1024), we should still be doing only one read and one write per spacial location per channel from main memory.</p>\n<p>The amount of time this takes should be insignificant next to the pointwise convolution that follows the channel wise convolution:<br>\nConv2d(1024, 1024, kernel_size=1, groups=1)</p>\n<p>Here we need to read a million floats from main memory and do 400 1024x1024 matrix multiplies.</p>\n<p>The channel-wise operation shouldn't even register next to that.</p>\n<p>I really wish we could get this working. We could have blazing fast convolutions with arbitrarily large kernels.</p>", "body_text": "I'm not sure I buy the \"bandwidth bound\" explanation either. If you increase the kernal size from 3x3 to say 5x5, your operation needs the same amount of non-cache memory, but takes way longer.\nI don't know how to program cuda, but here's how I see the operation working:\nLet's say we run a Conv2d(1024, 1024, kernel_size=3, groups=1024) on a shape 20x20x1024 tensor:\nRead a single channel from a single layer into cache. That's 400 floats (1600 bytes). It will fit into cache.\nRead that channels worth of parameters into cache (that's just 9 floats)\nThen we do the following 18x18 times:\nMultiply those 9 floats (weights) by the appropriate 9 floats in the channel from cache.\nStore the output in the appropriate cell in a 18x18 matrix in main memory.\nHere we have one main memory read and one main memory write per channel per spacial location.\nIf we do the same operation with Conv2d(1024, 1024, kernel_size=5, groups=1024), we should still be doing only one read and one write per spacial location per channel from main memory.\nThe amount of time this takes should be insignificant next to the pointwise convolution that follows the channel wise convolution:\nConv2d(1024, 1024, kernel_size=1, groups=1)\nHere we need to read a million floats from main memory and do 400 1024x1024 matrix multiplies.\nThe channel-wise operation shouldn't even register next to that.\nI really wish we could get this working. We could have blazing fast convolutions with arbitrarily large kernels.", "body": "I'm not sure I buy the \"bandwidth bound\" explanation either. If you increase the kernal size from 3x3 to say 5x5, your operation needs the same amount of non-cache memory, but takes way longer.\r\n\r\nI don't know how to program cuda, but here's how I see the operation working:\r\nLet's say we run a Conv2d(1024, 1024, kernel_size=3, groups=1024) on a shape 20x20x1024 tensor:\r\n\r\nRead a single channel from a single layer into cache. That's 400 floats (1600 bytes). It will fit into cache.\r\nRead that channels worth of parameters into cache (that's just 9 floats)\r\n\r\nThen we do the following 18x18 times:\r\n  Multiply those 9 floats (weights) by the appropriate 9 floats in the channel from cache.\r\n  Store the output in the appropriate cell in a 18x18 matrix in main memory.\r\n\r\nHere we have one main memory read and one main memory write per channel per spacial location.\r\n\r\nIf we do the same operation with Conv2d(1024, 1024, kernel_size=5, groups=1024), we should still be doing only one read and one write per spacial location per channel from main memory.\r\n\r\nThe amount of time this takes should be insignificant next to the pointwise convolution that follows the channel wise convolution:\r\nConv2d(1024, 1024, kernel_size=1, groups=1)\r\n\r\nHere we need to read a million floats from main memory and do 400 1024x1024 matrix multiplies.\r\n\r\nThe channel-wise operation shouldn't even register next to that.\r\n\r\nI really wish we could get this working. We could have blazing fast convolutions with arbitrarily large kernels."}