{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6165", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6165/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6165/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6165/events", "html_url": "https://github.com/pytorch/pytorch/issues/6165", "id": 310287775, "node_id": "MDU6SXNzdWUzMTAyODc3NzU=", "number": 6165, "title": "Note about unusual stride situations in dev docs / make it easier to test for this in the library", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-01T02:31:35Z", "updated_at": "2018-04-01T02:32:04Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>On its face, strides are pretty simple: given strides <code>s</code>, the memory location of the indices <code>x</code> is <code>sum(s[i] * x[i] for i in dim)</code>. It's also fairly simple to say when strides are contiguous: that is when <code>s[i] == product(s[j] for j in range(0, i))</code>. And then the standard application of strides is to let you get non-contiguous views into an underlying contiguous tensor.</p>\n<p>But there are also some weirder, less obvious stride situations:</p>\n<p>Broadcasting is achieved by having zero strides. So, e.g.,</p>\n<pre><code>&gt;&gt;&gt; torch.tensor(range(0,3)).expand(2,3)\n\n 0  1  2\n 0  1  2\n[torch.LongTensor of size (2,3)]\n&gt;&gt;&gt; torch.tensor(range(0,3)).as_strided(size=(2,3), stride=(0,1))\n\n 0  1  2\n 0  1  2\n[torch.LongTensor of size (2,3)]\n</code></pre>\n<p>And the technical definition of strides permits weird, overlapping striding, e.g.,</p>\n<pre><code>&gt;&gt;&gt; torch.tensor(range(0,7)).as_strided(size=(2,2,2), stride=(3,2,1))\n\n(0 ,.,.) = \n  0  1\n  2  3\n\n(1 ,.,.) = \n  3  4\n  5  6\n[torch.LongTensor of size (2,2,2)]\n</code></pre>\n<p>It would be good to have a developer doc describing all of this, as well as adding functions making it easier to test for certain stride situations, to make it easier to work with external libraries and specialized kernels. An example is in the cuFFT interface <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"309901813\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6118\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6118/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6118\">#6118</a> where only non-overlapping striding is supported.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a></p>", "body_text": "On its face, strides are pretty simple: given strides s, the memory location of the indices x is sum(s[i] * x[i] for i in dim). It's also fairly simple to say when strides are contiguous: that is when s[i] == product(s[j] for j in range(0, i)). And then the standard application of strides is to let you get non-contiguous views into an underlying contiguous tensor.\nBut there are also some weirder, less obvious stride situations:\nBroadcasting is achieved by having zero strides. So, e.g.,\n>>> torch.tensor(range(0,3)).expand(2,3)\n\n 0  1  2\n 0  1  2\n[torch.LongTensor of size (2,3)]\n>>> torch.tensor(range(0,3)).as_strided(size=(2,3), stride=(0,1))\n\n 0  1  2\n 0  1  2\n[torch.LongTensor of size (2,3)]\n\nAnd the technical definition of strides permits weird, overlapping striding, e.g.,\n>>> torch.tensor(range(0,7)).as_strided(size=(2,2,2), stride=(3,2,1))\n\n(0 ,.,.) = \n  0  1\n  2  3\n\n(1 ,.,.) = \n  3  4\n  5  6\n[torch.LongTensor of size (2,2,2)]\n\nIt would be good to have a developer doc describing all of this, as well as adding functions making it easier to test for certain stride situations, to make it easier to work with external libraries and specialized kernels. An example is in the cuFFT interface #6118 where only non-overlapping striding is supported.\nCC @SsnL", "body": "On its face, strides are pretty simple: given strides `s`, the memory location of the indices `x` is `sum(s[i] * x[i] for i in dim)`. It's also fairly simple to say when strides are contiguous: that is when `s[i] == product(s[j] for j in range(0, i))`. And then the standard application of strides is to let you get non-contiguous views into an underlying contiguous tensor.\r\n\r\nBut there are also some weirder, less obvious stride situations:\r\n\r\nBroadcasting is achieved by having zero strides. So, e.g., \r\n\r\n```\r\n>>> torch.tensor(range(0,3)).expand(2,3)\r\n\r\n 0  1  2\r\n 0  1  2\r\n[torch.LongTensor of size (2,3)]\r\n>>> torch.tensor(range(0,3)).as_strided(size=(2,3), stride=(0,1))\r\n\r\n 0  1  2\r\n 0  1  2\r\n[torch.LongTensor of size (2,3)]\r\n```\r\n\r\nAnd the technical definition of strides permits weird, overlapping striding, e.g.,\r\n\r\n```\r\n>>> torch.tensor(range(0,7)).as_strided(size=(2,2,2), stride=(3,2,1))\r\n\r\n(0 ,.,.) = \r\n  0  1\r\n  2  3\r\n\r\n(1 ,.,.) = \r\n  3  4\r\n  5  6\r\n[torch.LongTensor of size (2,2,2)]\r\n```\r\n\r\nIt would be good to have a developer doc describing all of this, as well as adding functions making it easier to test for certain stride situations, to make it easier to work with external libraries and specialized kernels. An example is in the cuFFT interface https://github.com/pytorch/pytorch/pull/6118 where only non-overlapping striding is supported.\r\n\r\nCC @SsnL "}