{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2623", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2623/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2623/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2623/events", "html_url": "https://github.com/pytorch/pytorch/issues/2623", "id": 255169187, "node_id": "MDU6SXNzdWUyNTUxNjkxODc=", "number": 2623, "title": "How to handle exploding/vanishing gradient in Pytorch and negative loss values", "user": {"login": "AdityaAS", "id": 7334811, "node_id": "MDQ6VXNlcjczMzQ4MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/7334811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AdityaAS", "html_url": "https://github.com/AdityaAS", "followers_url": "https://api.github.com/users/AdityaAS/followers", "following_url": "https://api.github.com/users/AdityaAS/following{/other_user}", "gists_url": "https://api.github.com/users/AdityaAS/gists{/gist_id}", "starred_url": "https://api.github.com/users/AdityaAS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AdityaAS/subscriptions", "organizations_url": "https://api.github.com/users/AdityaAS/orgs", "repos_url": "https://api.github.com/users/AdityaAS/repos", "events_url": "https://api.github.com/users/AdityaAS/events{/privacy}", "received_events_url": "https://api.github.com/users/AdityaAS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-05T06:11:45Z", "updated_at": "2017-09-05T12:35:21Z", "closed_at": "2017-09-05T12:35:21Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I'm trying to modify the <a href=\"http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\" rel=\"nofollow\">character level rnn classification</a> code to make it fit for my application. The data set I have is pretty huge (4 lac training instances). The code snippets are shown below (I've shown only the necessary parts, all helper functions are same as the official example)</p>\n<p>I initially faced the problem of exploding / vanishing gradient as described in this issue <a href=\"https://discuss.pytorch.org/t/nan-loss-in-rnn-model/655\" rel=\"nofollow\">issue</a></p>\n<p>I used the solution given there to clip the gradient in the train() function. But now, I seem to get negative values for loss. What is that supposed to mean?</p>\n<p>Also, how is it that in the official example (when I apply it to my dataset) I get loss values that are greater than 1.</p>\n<pre><code>class RNN(nn.Module):\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.Softmax()\n  \n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        output = output.add(1e-8)\n        output = output.log()\n\n        return output, hidden\n        \n\n    def initHidden(self):\n        return Variable(torch.zeros(1, self.hidden_size).cuda())\n</code></pre>\n<p>criterion and the train() function are written as follows:</p>\n<pre><code>criterion = nn.NLLLoss().cuda()\n\nlearning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n\ndef train(category_tensor, line_tensor):\n    hidden = rnn.initHidden()\n\n    rnn.zero_grad()indian\n    # print(len(line_tensor.size()))\n    if(line_tensor.dim() != 0): #I have random new lines in some cases. This condition is to handle those\n        for i in range(line_tensor.size()[0]):\n            output, hidden = rnn(line_tensor[i], hidden)\n\n        loss = criterion(output, category_tensor)\n        loss.backward()\n        \n       # This line is used to prevent the vanishing / exploding gradient problem\n        torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.25)\n        \n        for p in rnn.parameters():\n            p.data.add_(-learning_rate, p.grad.data)\n            \n        return output, loss.data[0]\n    else:\n        return None, -1\n\n</code></pre>\n<p>Training of the model happens here</p>\n<pre><code>n_iters = 40000\nprint_every = 200\nplot_every = 200\n\n# # Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\nstart = time.time()\n\ntp = 0\ntn = 0\nfp = 0\nfn = 0\n\nprecision = 0\nrecall = 0\nfmeasure = 0\n\nfor iter in range(1, n_iters + 1):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    output, loss = train(category_tensor, line_tensor)\n\n    if loss != -1:\n        current_loss += loss\n\n        guess, guess_i = categoryFromOutput(output)\n        if guess == -1 and guess_i == -1:\n            continue\n        else:                \n            correct = '1' if guess == category else '0 (%s)' % category\n            if guess == 'class1' and category == 'class1':\n                tp += 1\n            elif guess == 'class2' and category == 'class2':\n                fn += 1\n            elif guess == 'class1' and category == 'class2':\n                fp += 1\n            else:\n                tn += 1\n            \n            if iter % print_every == 0:\n                loss = current_loss / print_every\n                print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n                all_losses.append(current_loss / plot_every)\n                current_loss = 0\n</code></pre>\n<pre><code>def evaluate(line_tensor):\n    hidden = rnn.initHidden()\n    if(line_tensor.dim() == 0):\n        return line_tensor\n    else:\n        for i in range(line_tensor.size()[0]):\n            output, hidden = rnn(line_tensor[i], hidden)\n        return output\n\ndef predict(input_line, category, n_predictions=1):\n    output = evaluate(Variable(lineToTensor(input_line)).cuda())\n    global total\n    global indian\n    global nonindian\n\n    total += 1\n    if(output.dim() != 0):\n        topv, topi = output.data.topk(1, 1, True)\n\n        for i in range(0, n_predictions):\n            value = topv[0][i]\n            category_index = topi[0][i]\n\n            if category_index &lt;= 1:\n                if all_categories[category_index] == 'indian':\n                    indian += 1\n                else:\n                    nonindian += 1\n                predictions.append([value, all_categories[category_index], category])\n\n</code></pre>", "body_text": "Hi,\nI'm trying to modify the character level rnn classification code to make it fit for my application. The data set I have is pretty huge (4 lac training instances). The code snippets are shown below (I've shown only the necessary parts, all helper functions are same as the official example)\nI initially faced the problem of exploding / vanishing gradient as described in this issue issue\nI used the solution given there to clip the gradient in the train() function. But now, I seem to get negative values for loss. What is that supposed to mean?\nAlso, how is it that in the official example (when I apply it to my dataset) I get loss values that are greater than 1.\nclass RNN(nn.Module):\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.Softmax()\n  \n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        output = output.add(1e-8)\n        output = output.log()\n\n        return output, hidden\n        \n\n    def initHidden(self):\n        return Variable(torch.zeros(1, self.hidden_size).cuda())\n\ncriterion and the train() function are written as follows:\ncriterion = nn.NLLLoss().cuda()\n\nlearning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n\ndef train(category_tensor, line_tensor):\n    hidden = rnn.initHidden()\n\n    rnn.zero_grad()indian\n    # print(len(line_tensor.size()))\n    if(line_tensor.dim() != 0): #I have random new lines in some cases. This condition is to handle those\n        for i in range(line_tensor.size()[0]):\n            output, hidden = rnn(line_tensor[i], hidden)\n\n        loss = criterion(output, category_tensor)\n        loss.backward()\n        \n       # This line is used to prevent the vanishing / exploding gradient problem\n        torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.25)\n        \n        for p in rnn.parameters():\n            p.data.add_(-learning_rate, p.grad.data)\n            \n        return output, loss.data[0]\n    else:\n        return None, -1\n\n\nTraining of the model happens here\nn_iters = 40000\nprint_every = 200\nplot_every = 200\n\n# # Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\nstart = time.time()\n\ntp = 0\ntn = 0\nfp = 0\nfn = 0\n\nprecision = 0\nrecall = 0\nfmeasure = 0\n\nfor iter in range(1, n_iters + 1):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    output, loss = train(category_tensor, line_tensor)\n\n    if loss != -1:\n        current_loss += loss\n\n        guess, guess_i = categoryFromOutput(output)\n        if guess == -1 and guess_i == -1:\n            continue\n        else:                \n            correct = '1' if guess == category else '0 (%s)' % category\n            if guess == 'class1' and category == 'class1':\n                tp += 1\n            elif guess == 'class2' and category == 'class2':\n                fn += 1\n            elif guess == 'class1' and category == 'class2':\n                fp += 1\n            else:\n                tn += 1\n            \n            if iter % print_every == 0:\n                loss = current_loss / print_every\n                print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n                all_losses.append(current_loss / plot_every)\n                current_loss = 0\n\ndef evaluate(line_tensor):\n    hidden = rnn.initHidden()\n    if(line_tensor.dim() == 0):\n        return line_tensor\n    else:\n        for i in range(line_tensor.size()[0]):\n            output, hidden = rnn(line_tensor[i], hidden)\n        return output\n\ndef predict(input_line, category, n_predictions=1):\n    output = evaluate(Variable(lineToTensor(input_line)).cuda())\n    global total\n    global indian\n    global nonindian\n\n    total += 1\n    if(output.dim() != 0):\n        topv, topi = output.data.topk(1, 1, True)\n\n        for i in range(0, n_predictions):\n            value = topv[0][i]\n            category_index = topi[0][i]\n\n            if category_index <= 1:\n                if all_categories[category_index] == 'indian':\n                    indian += 1\n                else:\n                    nonindian += 1\n                predictions.append([value, all_categories[category_index], category])", "body": "Hi,\r\n\r\nI'm trying to modify the [character level rnn classification](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) code to make it fit for my application. The data set I have is pretty huge (4 lac training instances). The code snippets are shown below (I've shown only the necessary parts, all helper functions are same as the official example) \r\n\r\nI initially faced the problem of exploding / vanishing gradient as described in this issue [issue](https://discuss.pytorch.org/t/nan-loss-in-rnn-model/655)\r\n\r\nI used the solution given there to clip the gradient in the train() function. But now, I seem to get negative values for loss. What is that supposed to mean?\r\n\r\nAlso, how is it that in the official example (when I apply it to my dataset) I get loss values that are greater than 1. \r\n\r\n```\r\nclass RNN(nn.Module):\r\n\r\n    def __init__(self, input_size, hidden_size, output_size):\r\n        super(RNN, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\r\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\r\n        self.softmax = nn.Softmax()\r\n  \r\n    def forward(self, input, hidden):\r\n        combined = torch.cat((input, hidden), 1)\r\n        hidden = self.i2h(combined)\r\n        output = self.i2o(combined)\r\n        output = self.softmax(output)\r\n        output = output.add(1e-8)\r\n        output = output.log()\r\n\r\n        return output, hidden\r\n        \r\n\r\n    def initHidden(self):\r\n        return Variable(torch.zeros(1, self.hidden_size).cuda())\r\n```\r\ncriterion and the train() function are written as follows:\r\n\r\n```\r\ncriterion = nn.NLLLoss().cuda()\r\n\r\nlearning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\r\n\r\ndef train(category_tensor, line_tensor):\r\n    hidden = rnn.initHidden()\r\n\r\n    rnn.zero_grad()indian\r\n    # print(len(line_tensor.size()))\r\n    if(line_tensor.dim() != 0): #I have random new lines in some cases. This condition is to handle those\r\n        for i in range(line_tensor.size()[0]):\r\n            output, hidden = rnn(line_tensor[i], hidden)\r\n\r\n        loss = criterion(output, category_tensor)\r\n        loss.backward()\r\n        \r\n       # This line is used to prevent the vanishing / exploding gradient problem\r\n        torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.25)\r\n        \r\n        for p in rnn.parameters():\r\n            p.data.add_(-learning_rate, p.grad.data)\r\n            \r\n        return output, loss.data[0]\r\n    else:\r\n        return None, -1\r\n\r\n```\r\nTraining of the model happens here\r\n\r\n```\r\nn_iters = 40000\r\nprint_every = 200\r\nplot_every = 200\r\n\r\n# # Keep track of losses for plotting\r\ncurrent_loss = 0\r\nall_losses = []\r\n\r\ndef timeSince(since):\r\n    now = time.time()\r\n    s = now - since\r\n    m = math.floor(s / 60)\r\n    s -= m * 60\r\n    return '%dm %ds' % (m, s)\r\n\r\nstart = time.time()\r\n\r\ntp = 0\r\ntn = 0\r\nfp = 0\r\nfn = 0\r\n\r\nprecision = 0\r\nrecall = 0\r\nfmeasure = 0\r\n\r\nfor iter in range(1, n_iters + 1):\r\n    category, line, category_tensor, line_tensor = randomTrainingExample()\r\n    output, loss = train(category_tensor, line_tensor)\r\n\r\n    if loss != -1:\r\n        current_loss += loss\r\n\r\n        guess, guess_i = categoryFromOutput(output)\r\n        if guess == -1 and guess_i == -1:\r\n            continue\r\n        else:                \r\n            correct = '1' if guess == category else '0 (%s)' % category\r\n            if guess == 'class1' and category == 'class1':\r\n                tp += 1\r\n            elif guess == 'class2' and category == 'class2':\r\n                fn += 1\r\n            elif guess == 'class1' and category == 'class2':\r\n                fp += 1\r\n            else:\r\n                tn += 1\r\n            \r\n            if iter % print_every == 0:\r\n                loss = current_loss / print_every\r\n                print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\r\n                all_losses.append(current_loss / plot_every)\r\n                current_loss = 0\r\n```\r\n\r\n```\r\ndef evaluate(line_tensor):\r\n    hidden = rnn.initHidden()\r\n    if(line_tensor.dim() == 0):\r\n        return line_tensor\r\n    else:\r\n        for i in range(line_tensor.size()[0]):\r\n            output, hidden = rnn(line_tensor[i], hidden)\r\n        return output\r\n\r\ndef predict(input_line, category, n_predictions=1):\r\n    output = evaluate(Variable(lineToTensor(input_line)).cuda())\r\n    global total\r\n    global indian\r\n    global nonindian\r\n\r\n    total += 1\r\n    if(output.dim() != 0):\r\n        topv, topi = output.data.topk(1, 1, True)\r\n\r\n        for i in range(0, n_predictions):\r\n            value = topv[0][i]\r\n            category_index = topi[0][i]\r\n\r\n            if category_index <= 1:\r\n                if all_categories[category_index] == 'indian':\r\n                    indian += 1\r\n                else:\r\n                    nonindian += 1\r\n                predictions.append([value, all_categories[category_index], category])\r\n\r\n```\r\n\r\n"}