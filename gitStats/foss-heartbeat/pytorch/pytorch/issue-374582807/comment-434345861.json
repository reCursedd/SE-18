{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434345861", "html_url": "https://github.com/pytorch/pytorch/issues/13197#issuecomment-434345861", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13197", "id": 434345861, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDM0NTg2MQ==", "user": {"login": "abaisero", "id": 1794938, "node_id": "MDQ6VXNlcjE3OTQ5Mzg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1794938?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abaisero", "html_url": "https://github.com/abaisero", "followers_url": "https://api.github.com/users/abaisero/followers", "following_url": "https://api.github.com/users/abaisero/following{/other_user}", "gists_url": "https://api.github.com/users/abaisero/gists{/gist_id}", "starred_url": "https://api.github.com/users/abaisero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abaisero/subscriptions", "organizations_url": "https://api.github.com/users/abaisero/orgs", "repos_url": "https://api.github.com/users/abaisero/repos", "events_url": "https://api.github.com/users/abaisero/events{/privacy}", "received_events_url": "https://api.github.com/users/abaisero/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-30T15:24:46Z", "updated_at": "2018-10-30T15:24:46Z", "author_association": "NONE", "body_html": "<p>I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order.  You can use <code>torch.set_num_threads(1)</code> for this.</p>", "body_text": "I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order.  You can use torch.set_num_threads(1) for this.", "body": "I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order.  You can use `torch.set_num_threads(1)` for this."}