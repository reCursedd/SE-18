{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/258422636", "html_url": "https://github.com/pytorch/pytorch/issues/175#issuecomment-258422636", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/175", "id": 258422636, "node_id": "MDEyOklzc3VlQ29tbWVudDI1ODQyMjYzNg==", "user": {"login": "bshillingford", "id": 2326749, "node_id": "MDQ6VXNlcjIzMjY3NDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2326749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bshillingford", "html_url": "https://github.com/bshillingford", "followers_url": "https://api.github.com/users/bshillingford/followers", "following_url": "https://api.github.com/users/bshillingford/following{/other_user}", "gists_url": "https://api.github.com/users/bshillingford/gists{/gist_id}", "starred_url": "https://api.github.com/users/bshillingford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bshillingford/subscriptions", "organizations_url": "https://api.github.com/users/bshillingford/orgs", "repos_url": "https://api.github.com/users/bshillingford/repos", "events_url": "https://api.github.com/users/bshillingford/events{/privacy}", "received_events_url": "https://api.github.com/users/bshillingford/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-04T12:50:52Z", "updated_at": "2016-11-04T12:51:09Z", "author_association": "NONE", "body_html": "<p>Similarly to comment by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8885556\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/glample\">@glample</a> on gradient clipping, is there a way to add generic modifications to the gradient before passing to the optimizer? Or give arbitrary gradients to the optimizer as opposed to those inside the autograd Variables? In lua Torch optim, the closure interface allows this. (But perhaps the latter case is rare, in which case the legacy optim is an acceptable alternative.)</p>\n<p>Also related: since you can optimize arbitrary quantities that are not parameters (e.g. the input of a network), you simply pass these as an iterable to the <code>parameters</code> ctor arg, right? In that case, would a name other than <code>parameters</code> be better?</p>", "body_text": "Similarly to comment by @glample on gradient clipping, is there a way to add generic modifications to the gradient before passing to the optimizer? Or give arbitrary gradients to the optimizer as opposed to those inside the autograd Variables? In lua Torch optim, the closure interface allows this. (But perhaps the latter case is rare, in which case the legacy optim is an acceptable alternative.)\nAlso related: since you can optimize arbitrary quantities that are not parameters (e.g. the input of a network), you simply pass these as an iterable to the parameters ctor arg, right? In that case, would a name other than parameters be better?", "body": "Similarly to comment by @glample on gradient clipping, is there a way to add generic modifications to the gradient before passing to the optimizer? Or give arbitrary gradients to the optimizer as opposed to those inside the autograd Variables? In lua Torch optim, the closure interface allows this. (But perhaps the latter case is rare, in which case the legacy optim is an acceptable alternative.)\n\nAlso related: since you can optimize arbitrary quantities that are not parameters (e.g. the input of a network), you simply pass these as an iterable to the `parameters` ctor arg, right? In that case, would a name other than `parameters` be better?\n"}