{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424719557", "html_url": "https://github.com/pytorch/pytorch/issues/12045#issuecomment-424719557", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12045", "id": 424719557, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDcxOTU1Nw==", "user": {"login": "steplee", "id": 5303539, "node_id": "MDQ6VXNlcjUzMDM1Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5303539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/steplee", "html_url": "https://github.com/steplee", "followers_url": "https://api.github.com/users/steplee/followers", "following_url": "https://api.github.com/users/steplee/following{/other_user}", "gists_url": "https://api.github.com/users/steplee/gists{/gist_id}", "starred_url": "https://api.github.com/users/steplee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/steplee/subscriptions", "organizations_url": "https://api.github.com/users/steplee/orgs", "repos_url": "https://api.github.com/users/steplee/repos", "events_url": "https://api.github.com/users/steplee/events{/privacy}", "received_events_url": "https://api.github.com/users/steplee/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T13:42:02Z", "updated_at": "2018-09-26T13:42:02Z", "author_association": "NONE", "body_html": "<p>The purpose of a deep learning framework is to wire together a computational graph that does forward and backward passes. To compute gradients for the backward pass, you need more than a typical C++ function that only returns the answer during a forward pass.</p>\n<p>The difference between the two is that an operator wraps a function, provides another operator for its gradient, and, crucially, can be used as a node in a net.</p>\n<p>See <a href=\"https://caffe2.ai/docs/custom-operators.html\" rel=\"nofollow\">this link</a>. Also, see the hundreds of ops from <code>pytorch/caffe2/operators</code>. If you want to build the op out of the source tree, I have two repos in my profile that may help you get started (but they are messy and I haven't written gradient ops yet).</p>", "body_text": "The purpose of a deep learning framework is to wire together a computational graph that does forward and backward passes. To compute gradients for the backward pass, you need more than a typical C++ function that only returns the answer during a forward pass.\nThe difference between the two is that an operator wraps a function, provides another operator for its gradient, and, crucially, can be used as a node in a net.\nSee this link. Also, see the hundreds of ops from pytorch/caffe2/operators. If you want to build the op out of the source tree, I have two repos in my profile that may help you get started (but they are messy and I haven't written gradient ops yet).", "body": "The purpose of a deep learning framework is to wire together a computational graph that does forward and backward passes. To compute gradients for the backward pass, you need more than a typical C++ function that only returns the answer during a forward pass.\r\n\r\nThe difference between the two is that an operator wraps a function, provides another operator for its gradient, and, crucially, can be used as a node in a net. \r\n\r\nSee [this link](https://caffe2.ai/docs/custom-operators.html). Also, see the hundreds of ops from `pytorch/caffe2/operators`. If you want to build the op out of the source tree, I have two repos in my profile that may help you get started (but they are messy and I haven't written gradient ops yet)."}