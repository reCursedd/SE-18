{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4845", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4845/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4845/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4845/events", "html_url": "https://github.com/pytorch/pytorch/issues/4845", "id": 291605988, "node_id": "MDU6SXNzdWUyOTE2MDU5ODg=", "number": 4845, "title": "Using CUDNN gives bad results.", "user": {"login": "isabeaups", "id": 12017880, "node_id": "MDQ6VXNlcjEyMDE3ODgw", "avatar_url": "https://avatars2.githubusercontent.com/u/12017880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isabeaups", "html_url": "https://github.com/isabeaups", "followers_url": "https://api.github.com/users/isabeaups/followers", "following_url": "https://api.github.com/users/isabeaups/following{/other_user}", "gists_url": "https://api.github.com/users/isabeaups/gists{/gist_id}", "starred_url": "https://api.github.com/users/isabeaups/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isabeaups/subscriptions", "organizations_url": "https://api.github.com/users/isabeaups/orgs", "repos_url": "https://api.github.com/users/isabeaups/repos", "events_url": "https://api.github.com/users/isabeaups/events{/privacy}", "received_events_url": "https://api.github.com/users/isabeaups/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-25T15:19:29Z", "updated_at": "2018-01-29T08:50:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I have a next step prediction model on times series which is simply a GRU with a fully-connected layer on top of it. When I train it using CPU after 50 epochs I get a loss of 0.10 but when I train it with GPU the loss is 0.15 after 50 epochs. Doing more epochs doesnt really lower the losses in either cases.</p>\n<p>I have tried changing the random seeds for both data and model, and these results are independent of the random seeds.</p>\n<p>What does solve the problem is not using CUDNN:<br>\nIf I add the line<br>\n<code>torch.backends.cudnn.enabled = False</code><br>\nthen training with GPU gives as good results as training with CPU.</p>\n<p>This might be related to the issue that other people have that training with PyTorch gives worse results than Tensorflow: <a href=\"https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380\" rel=\"nofollow\">https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380</a></p>\n<p>I have:<br>\nLinux<br>\nPython 3.6.2<br>\nPyTorch 0.3.0<br>\nCUDNN_MAJOR 7<br>\nCUDNN_MINOR 0<br>\nCUDNN_PATCHLEVEL 5</p>\n<p>GPU: Titan X Maxwell</p>", "body_text": "I have a next step prediction model on times series which is simply a GRU with a fully-connected layer on top of it. When I train it using CPU after 50 epochs I get a loss of 0.10 but when I train it with GPU the loss is 0.15 after 50 epochs. Doing more epochs doesnt really lower the losses in either cases.\nI have tried changing the random seeds for both data and model, and these results are independent of the random seeds.\nWhat does solve the problem is not using CUDNN:\nIf I add the line\ntorch.backends.cudnn.enabled = False\nthen training with GPU gives as good results as training with CPU.\nThis might be related to the issue that other people have that training with PyTorch gives worse results than Tensorflow: https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380\nI have:\nLinux\nPython 3.6.2\nPyTorch 0.3.0\nCUDNN_MAJOR 7\nCUDNN_MINOR 0\nCUDNN_PATCHLEVEL 5\nGPU: Titan X Maxwell", "body": "\r\n\r\nI have a next step prediction model on times series which is simply a GRU with a fully-connected layer on top of it. When I train it using CPU after 50 epochs I get a loss of 0.10 but when I train it with GPU the loss is 0.15 after 50 epochs. Doing more epochs doesnt really lower the losses in either cases.\r\n\r\nI have tried changing the random seeds for both data and model, and these results are independent of the random seeds.\r\n\r\nWhat does solve the problem is not using CUDNN:\r\nIf I add the line\r\n`torch.backends.cudnn.enabled = False`\r\nthen training with GPU gives as good results as training with CPU.\r\n\r\nThis might be related to the issue that other people have that training with PyTorch gives worse results than Tensorflow: https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380\r\n\r\nI have:\r\nLinux\r\nPython 3.6.2\r\nPyTorch 0.3.0\r\nCUDNN_MAJOR 7\r\nCUDNN_MINOR 0\r\nCUDNN_PATCHLEVEL 5\r\n\r\nGPU: Titan X Maxwell\r\n"}