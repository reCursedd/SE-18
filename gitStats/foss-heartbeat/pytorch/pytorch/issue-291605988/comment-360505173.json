{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360505173", "html_url": "https://github.com/pytorch/pytorch/issues/4845#issuecomment-360505173", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4845", "id": 360505173, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDUwNTE3Mw==", "user": {"login": "isabeaups", "id": 12017880, "node_id": "MDQ6VXNlcjEyMDE3ODgw", "avatar_url": "https://avatars2.githubusercontent.com/u/12017880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isabeaups", "html_url": "https://github.com/isabeaups", "followers_url": "https://api.github.com/users/isabeaups/followers", "following_url": "https://api.github.com/users/isabeaups/following{/other_user}", "gists_url": "https://api.github.com/users/isabeaups/gists{/gist_id}", "starred_url": "https://api.github.com/users/isabeaups/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isabeaups/subscriptions", "organizations_url": "https://api.github.com/users/isabeaups/orgs", "repos_url": "https://api.github.com/users/isabeaups/repos", "events_url": "https://api.github.com/users/isabeaups/events{/privacy}", "received_events_url": "https://api.github.com/users/isabeaups/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-25T15:42:37Z", "updated_at": "2018-01-25T15:42:37Z", "author_association": "NONE", "body_html": "<p>It is simply a GRU with a fully-connected output.</p>\n<pre><code>class SimpleRNN(Module):\n    '''\n    Simple RNN with teacher forcing option.\n    '''\n    def __init__(self, input_size, hidden_size, output_size=1, control_vec_size=0, weight_normalize=False):\n        'Known inputs come last'\n        super(SimpleRNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.control_vec_size = control_vec_size\n        self.weight_normalize = weight_normalize\n\n\n        self.rnn = GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1)\n        # Initialize to encourage learning of the slow dynamics\n        for name, param in self.rnn.named_parameters():\n            if 'bias' in name:\n\n                nn.init.normal(param)\n            elif 'weight' in name:\n                nn.init.xavier_normal(param)\n\n        self.fc = Linear(hidden_size, output_size)\n\n        if weight_normalize:\n            rnn_names = [n for n, v in self.rnn.named_parameters()]\n            fc_names = [n for n,v in self.fc.named_parameters()]\n            for n in rnn_names:\n                if 'weight' in n:\n                    weight_norm(self.rnn, n)\n            for n in fc_names:\n                if 'weight' in n:\n                    weight_norm(self.fc, n)\n\n\n    def step(self, input, hidden=None):\n        # input is (batch, inp_size), hidden is tuple\n        rnn_output, hidden = self.rnn(input.unsqueeze(0), hidden) # rnn_output is (1, batch, hidden_size)\n        output = self.fc(rnn_output.squeeze(0)) # (batch, out_size)\n        return output, hidden\n\n    def forward(self, inputs, hidden=None, gen_poisson_lambda=0):\n        n_ahead = 1\n        n_steps = len(inputs)\n        n_batch = inputs.size(1)\n        outputs = Variable(torch.zeros(n_steps-n_ahead, n_batch, self.output_size))\n        left_to_gen = 0\n        for t in range(n_steps-1):\n            if left_to_gen == 0:\n                inp = inputs[t]\n                left_to_gen = np.random.poisson(gen_poisson_lambda)\n            else:\n                inp = torch.cat([output[:, :-self.control_vec_size], inputs[t, :, -self.control_vec_size:]], dim=1)\n                left_to_gen -= 1\n            output, hidden = self.step(inp, hidden)\n            outputs[t] = output\n        return outputs, hidden\n</code></pre>", "body_text": "It is simply a GRU with a fully-connected output.\nclass SimpleRNN(Module):\n    '''\n    Simple RNN with teacher forcing option.\n    '''\n    def __init__(self, input_size, hidden_size, output_size=1, control_vec_size=0, weight_normalize=False):\n        'Known inputs come last'\n        super(SimpleRNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.control_vec_size = control_vec_size\n        self.weight_normalize = weight_normalize\n\n\n        self.rnn = GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1)\n        # Initialize to encourage learning of the slow dynamics\n        for name, param in self.rnn.named_parameters():\n            if 'bias' in name:\n\n                nn.init.normal(param)\n            elif 'weight' in name:\n                nn.init.xavier_normal(param)\n\n        self.fc = Linear(hidden_size, output_size)\n\n        if weight_normalize:\n            rnn_names = [n for n, v in self.rnn.named_parameters()]\n            fc_names = [n for n,v in self.fc.named_parameters()]\n            for n in rnn_names:\n                if 'weight' in n:\n                    weight_norm(self.rnn, n)\n            for n in fc_names:\n                if 'weight' in n:\n                    weight_norm(self.fc, n)\n\n\n    def step(self, input, hidden=None):\n        # input is (batch, inp_size), hidden is tuple\n        rnn_output, hidden = self.rnn(input.unsqueeze(0), hidden) # rnn_output is (1, batch, hidden_size)\n        output = self.fc(rnn_output.squeeze(0)) # (batch, out_size)\n        return output, hidden\n\n    def forward(self, inputs, hidden=None, gen_poisson_lambda=0):\n        n_ahead = 1\n        n_steps = len(inputs)\n        n_batch = inputs.size(1)\n        outputs = Variable(torch.zeros(n_steps-n_ahead, n_batch, self.output_size))\n        left_to_gen = 0\n        for t in range(n_steps-1):\n            if left_to_gen == 0:\n                inp = inputs[t]\n                left_to_gen = np.random.poisson(gen_poisson_lambda)\n            else:\n                inp = torch.cat([output[:, :-self.control_vec_size], inputs[t, :, -self.control_vec_size:]], dim=1)\n                left_to_gen -= 1\n            output, hidden = self.step(inp, hidden)\n            outputs[t] = output\n        return outputs, hidden", "body": "It is simply a GRU with a fully-connected output. \r\n\r\n    class SimpleRNN(Module):\r\n        '''\r\n        Simple RNN with teacher forcing option.\r\n        '''\r\n        def __init__(self, input_size, hidden_size, output_size=1, control_vec_size=0, weight_normalize=False):\r\n            'Known inputs come last'\r\n            super(SimpleRNN, self).__init__()\r\n            self.input_size = input_size\r\n            self.hidden_size = hidden_size\r\n            self.output_size = output_size\r\n            self.control_vec_size = control_vec_size\r\n            self.weight_normalize = weight_normalize\r\n\r\n\r\n            self.rnn = GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1)\r\n            # Initialize to encourage learning of the slow dynamics\r\n            for name, param in self.rnn.named_parameters():\r\n                if 'bias' in name:\r\n\r\n                    nn.init.normal(param)\r\n                elif 'weight' in name:\r\n                    nn.init.xavier_normal(param)\r\n\r\n            self.fc = Linear(hidden_size, output_size)\r\n\r\n            if weight_normalize:\r\n                rnn_names = [n for n, v in self.rnn.named_parameters()]\r\n                fc_names = [n for n,v in self.fc.named_parameters()]\r\n                for n in rnn_names:\r\n                    if 'weight' in n:\r\n                        weight_norm(self.rnn, n)\r\n                for n in fc_names:\r\n                    if 'weight' in n:\r\n                        weight_norm(self.fc, n)\r\n\r\n\r\n        def step(self, input, hidden=None):\r\n            # input is (batch, inp_size), hidden is tuple\r\n            rnn_output, hidden = self.rnn(input.unsqueeze(0), hidden) # rnn_output is (1, batch, hidden_size)\r\n            output = self.fc(rnn_output.squeeze(0)) # (batch, out_size)\r\n            return output, hidden\r\n\r\n        def forward(self, inputs, hidden=None, gen_poisson_lambda=0):\r\n            n_ahead = 1\r\n            n_steps = len(inputs)\r\n            n_batch = inputs.size(1)\r\n            outputs = Variable(torch.zeros(n_steps-n_ahead, n_batch, self.output_size))\r\n            left_to_gen = 0\r\n            for t in range(n_steps-1):\r\n                if left_to_gen == 0:\r\n                    inp = inputs[t]\r\n                    left_to_gen = np.random.poisson(gen_poisson_lambda)\r\n                else:\r\n                    inp = torch.cat([output[:, :-self.control_vec_size], inputs[t, :, -self.control_vec_size:]], dim=1)\r\n                    left_to_gen -= 1\r\n                output, hidden = self.step(inp, hidden)\r\n                outputs[t] = output\r\n            return outputs, hidden"}