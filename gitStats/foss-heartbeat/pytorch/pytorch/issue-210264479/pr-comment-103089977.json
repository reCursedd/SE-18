{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103089977", "pull_request_review_id": 23868411, "id": 103089977, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMzA4OTk3Nw==", "diff_hunk": "@@ -0,0 +1,139 @@\n+import torch\n+from torch.autograd import Variable\n+\n+\n+def iter_gradients(x):\n+    if isinstance(x, Variable):\n+        if x.requires_grad:\n+            yield x.grad.data\n+    else:\n+        for elem in x:\n+            for result in iter_gradients(elem):\n+                yield result\n+\n+\n+def zero_gradients(i):\n+    for t in iter_gradients(i):\n+        t.zero_()\n+\n+\n+def make_jacobian(input, num_out):\n+    if isinstance(input, Variable) and not input.requires_grad:\n+        return None\n+    if torch.is_tensor(input) or isinstance(input, Variable):\n+        return torch.zeros(input.nelement(), num_out)\n+    else:\n+        return type(input)(filter(lambda x: x is not None,\n+                                  (make_jacobian(elem, num_out) for elem in input)))\n+\n+\n+def iter_tensors(x, only_requiring_grad=False):\n+    if torch.is_tensor(x):\n+        yield x\n+    elif isinstance(x, Variable):\n+        if x.requires_grad or not only_requiring_grad:\n+            yield x.data\n+    else:\n+        for elem in x:\n+            for result in iter_tensors(elem, only_requiring_grad):\n+                yield result\n+\n+\n+def contiguous(input):\n+    if torch.is_tensor(input):\n+        return input.contiguous()\n+    elif isinstance(input, Variable):\n+        return input.contiguous()\n+    else:\n+        return type(input)(contiguous(e) for e in input)\n+\n+\n+def get_numerical_jacobian(fn, input, target, eps=1e-3):\n+    # To be able to use .view(-1) input must be contiguous\n+    input = contiguous(input)\n+    output_size = fn(input).numel()\n+    jacobian = make_jacobian(target, output_size)\n+\n+    # It's much easier to iterate over flattened lists of tensors.\n+    # These are reference to the same objects in jacobian, so any changes\n+    # will be reflected in it as well.\n+    x_tensors = [t for t in iter_tensors(target, True)]\n+    j_tensors = [t for t in iter_tensors(jacobian)]\n+\n+    outa = torch.DoubleTensor(output_size)\n+    outb = torch.DoubleTensor(output_size)\n+\n+    # TODO: compare structure\n+    for x_tensor, d_tensor in zip(x_tensors, j_tensors):\n+        flat_tensor = x_tensor.view(-1)\n+        for i in range(flat_tensor.nelement()):\n+            orig = flat_tensor[i]\n+            flat_tensor[i] = orig - eps\n+            outa.copy_(fn(input))\n+            flat_tensor[i] = orig + eps\n+            outb.copy_(fn(input))\n+            flat_tensor[i] = orig\n+\n+            outb.add_(-1, outa).div_(2 * eps)\n+            d_tensor[i] = outb\n+\n+    return jacobian\n+\n+\n+def get_analytical_jacobian(input, output):\n+    jacobian = make_jacobian(input, output.numel())\n+    grad_output = output.data.clone().zero_()\n+    flat_grad_output = grad_output.view(-1)\n+\n+    for i in range(flat_grad_output.numel()):\n+        flat_grad_output.zero_()\n+        flat_grad_output[i] = 1\n+        zero_gradients(input)\n+        output.backward(grad_output, retain_variables=True)\n+        for jacobian_x, d_x in zip(jacobian, iter_gradients(input)):\n+            jacobian_x[:, i] = d_x\n+\n+    return jacobian\n+\n+\n+def _as_tuple(x):\n+    if isinstance(x, tuple):\n+        return x\n+    elif isinstance(x, list):\n+        return tuple(x)\n+    else:\n+        return x,\n+\n+\n+def gradcheck(func, inputs, eps=1e-3, atol=1e-5):\n+    \"\"\"Check gradients computed via small finite differences\n+       against analytical gradients\n+\n+    Args:\n+        func: Python function that takes Variable inputs and returns\n+            a tuple of Variables\n+        inputs: tuple of Variables\n+        eps: perturbation for finite differences\n+        atol: absolute tolerance, if max abs violated return False\n+\n+    Returns:\n+        True if all differences are within atol\n+    \"\"\"\n+    output = func(*inputs)\n+    output = _as_tuple(output)\n+\n+    for i, o in enumerate(output):\n+        if not o.requires_grad:\n+            continue\n+\n+        def fn(input):\n+            tmp = _as_tuple(func(*input))\n+            return tmp[i].data\n+\n+        numerical = get_numerical_jacobian(fn, inputs, inputs, eps)\n+        analytical = get_analytical_jacobian(_as_tuple(inputs), o)\n+\n+        if max(a.add(-1, n).abs().max()", "path": "torch/autograd/gradcheck.py", "position": null, "original_position": 136, "commit_id": "60acd6de66479bf6b6afe1d3d48f15df2f2682ff", "original_commit_id": "0eafa4958342e3719acab559f72f8e7a2261ae39", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "can you remove a newline there? we allow up to 120 chars in line", "created_at": "2017-02-25T22:23:14Z", "updated_at": "2018-11-23T15:32:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/851#discussion_r103089977", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/851", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103089977"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/851#discussion_r103089977"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/851"}}, "body_html": "<p>can you remove a newline there? we allow up to 120 chars in line</p>", "body_text": "can you remove a newline there? we allow up to 120 chars in line"}