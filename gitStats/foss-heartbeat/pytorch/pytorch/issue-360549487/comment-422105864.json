{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422105864", "html_url": "https://github.com/pytorch/pytorch/issues/11730#issuecomment-422105864", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11730", "id": 422105864, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjEwNTg2NA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-17T17:39:56Z", "updated_at": "2018-09-17T17:39:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Not really, but it should be very straightforward to do that. You can just move it in the <code>forward()</code> definition and return the loss. DataParallel will gather the losses to main GPU, and you can do backward on that loss on main GPU.</p>", "body_text": "Not really, but it should be very straightforward to do that. You can just move it in the forward() definition and return the loss. DataParallel will gather the losses to main GPU, and you can do backward on that loss on main GPU.", "body": "Not really, but it should be very straightforward to do that. You can just move it in the `forward()` definition and return the loss. DataParallel will gather the losses to main GPU, and you can do backward on that loss on main GPU."}