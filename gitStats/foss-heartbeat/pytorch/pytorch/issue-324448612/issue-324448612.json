{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7675", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7675/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7675/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7675/events", "html_url": "https://github.com/pytorch/pytorch/issues/7675", "id": 324448612, "node_id": "MDU6SXNzdWUzMjQ0NDg2MTI=", "number": 7675, "title": "when test , loss can not change!!", "user": {"login": "qlwang25", "id": 38132016, "node_id": "MDQ6VXNlcjM4MTMyMDE2", "avatar_url": "https://avatars2.githubusercontent.com/u/38132016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qlwang25", "html_url": "https://github.com/qlwang25", "followers_url": "https://api.github.com/users/qlwang25/followers", "following_url": "https://api.github.com/users/qlwang25/following{/other_user}", "gists_url": "https://api.github.com/users/qlwang25/gists{/gist_id}", "starred_url": "https://api.github.com/users/qlwang25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qlwang25/subscriptions", "organizations_url": "https://api.github.com/users/qlwang25/orgs", "repos_url": "https://api.github.com/users/qlwang25/repos", "events_url": "https://api.github.com/users/qlwang25/events{/privacy}", "received_events_url": "https://api.github.com/users/qlwang25/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-18T15:09:27Z", "updated_at": "2018-05-18T15:12:34Z", "closed_at": "2018-05-18T15:12:34Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I write a model about sequence label problem.<br>\nonly use three layers cnn.<br>\nwhen it train, loss is decrease and f1 is increase.<br>\nbut when test and epoch is about 10, loss and f1 is not change .<br>\nIs it overfitting?<br>\nHow to solve it ?</p>\n<h2>Code example</h2>\n<p>train:</p>\n<pre><code>train---epoch : 51 ,  global step : 24356\nloss :  0.016644377261400223\naccuracy : 0.999849\nprecision : 0.998770\nrecall : 0.998679\nf1 : 0.998725\n------------------------------------\ntrain---epoch : 51 ,  global step : 24357\nloss :  0.043941885232925415\naccuracy : 0.999844\nprecision : 0.998727\nrecall : 0.998636\nf1 : 0.998682\n------------------------------------\ntrain---epoch : 51 ,  global step : 24358\nloss :  0.0024001500569283962\naccuracy : 0.999844\nprecision : 0.998729\nrecall : 0.998638\nf1 : 0.998684\n</code></pre>\n<p>can see that model is run well  on the train data when epoch is about 50.<br>\nhowever, when opch is about 50, model is not desirable  on test data.<br>\neval information ( txt file save all run information on the test data, then sorted by f1 value , this is before the twentieth) :<br>\ncan see f1 less change from epoch 4 to epoch 50</p>\n<pre><code>epoch   loss    precision       recall  f1\n4       11.766307       0.198263        0.254603        0.222928\n2       10.437247       0.241509        0.203966        0.221156\n3       10.858424       0.199627        0.246282        0.220514\n1       9.906065        0.195629        0.225035        0.209304\n6       16.741554       0.167704        0.205205        0.184569\n12      18.872616       0.189472        0.166962        0.177506\n16      19.437512       0.179588        0.169795        0.174554\n5       14.804908       0.148019        0.211048        0.174002\n13      20.942182       0.203333        0.151204        0.173436\n7       16.623977       0.152858        0.195999        0.171761\n9       17.268815       0.159771        0.177762        0.168287\n10      19.142138       0.172041        0.157755        0.164589\n8       17.819491       0.155651        0.166785        0.161026\n15      21.475587       0.184768        0.142174        0.160696\n11      19.324749       0.170243        0.150319        0.159661\n19      22.313276       0.197936        0.132436        0.158693\n28      25.159835       0.209786        0.126771        0.158040\n20      23.833431       0.215899        0.123584        0.157190\n38      27.536659       0.200055        0.128187        0.156253\n0       10.829172       0.170155        0.143945        0.155956\n</code></pre>\n<p>Please try to provide a minimal example to repro the bug.<br>\nError messages and stack traces are also helpful.</p>\n<ul>\n<li>PyTorch or Caffe2: pytorch 0.4</li>\n<li>OS:Ubuntu 16</li>\n</ul>", "body_text": "Issue description\nI write a model about sequence label problem.\nonly use three layers cnn.\nwhen it train, loss is decrease and f1 is increase.\nbut when test and epoch is about 10, loss and f1 is not change .\nIs it overfitting?\nHow to solve it ?\nCode example\ntrain:\ntrain---epoch : 51 ,  global step : 24356\nloss :  0.016644377261400223\naccuracy : 0.999849\nprecision : 0.998770\nrecall : 0.998679\nf1 : 0.998725\n------------------------------------\ntrain---epoch : 51 ,  global step : 24357\nloss :  0.043941885232925415\naccuracy : 0.999844\nprecision : 0.998727\nrecall : 0.998636\nf1 : 0.998682\n------------------------------------\ntrain---epoch : 51 ,  global step : 24358\nloss :  0.0024001500569283962\naccuracy : 0.999844\nprecision : 0.998729\nrecall : 0.998638\nf1 : 0.998684\n\ncan see that model is run well  on the train data when epoch is about 50.\nhowever, when opch is about 50, model is not desirable  on test data.\neval information ( txt file save all run information on the test data, then sorted by f1 value , this is before the twentieth) :\ncan see f1 less change from epoch 4 to epoch 50\nepoch   loss    precision       recall  f1\n4       11.766307       0.198263        0.254603        0.222928\n2       10.437247       0.241509        0.203966        0.221156\n3       10.858424       0.199627        0.246282        0.220514\n1       9.906065        0.195629        0.225035        0.209304\n6       16.741554       0.167704        0.205205        0.184569\n12      18.872616       0.189472        0.166962        0.177506\n16      19.437512       0.179588        0.169795        0.174554\n5       14.804908       0.148019        0.211048        0.174002\n13      20.942182       0.203333        0.151204        0.173436\n7       16.623977       0.152858        0.195999        0.171761\n9       17.268815       0.159771        0.177762        0.168287\n10      19.142138       0.172041        0.157755        0.164589\n8       17.819491       0.155651        0.166785        0.161026\n15      21.475587       0.184768        0.142174        0.160696\n11      19.324749       0.170243        0.150319        0.159661\n19      22.313276       0.197936        0.132436        0.158693\n28      25.159835       0.209786        0.126771        0.158040\n20      23.833431       0.215899        0.123584        0.157190\n38      27.536659       0.200055        0.128187        0.156253\n0       10.829172       0.170155        0.143945        0.155956\n\nPlease try to provide a minimal example to repro the bug.\nError messages and stack traces are also helpful.\n\nPyTorch or Caffe2: pytorch 0.4\nOS:Ubuntu 16", "body": "## Issue description\r\nI write a model about sequence label problem.\r\nonly use three layers cnn.\r\nwhen it train, loss is decrease and f1 is increase.\r\nbut when test and epoch is about 10, loss and f1 is not change .\r\nIs it overfitting?\r\nHow to solve it ?  \r\n\r\n## Code example\r\ntrain:\r\n```\r\ntrain---epoch : 51 ,  global step : 24356\r\nloss :  0.016644377261400223\r\naccuracy : 0.999849\r\nprecision : 0.998770\r\nrecall : 0.998679\r\nf1 : 0.998725\r\n------------------------------------\r\ntrain---epoch : 51 ,  global step : 24357\r\nloss :  0.043941885232925415\r\naccuracy : 0.999844\r\nprecision : 0.998727\r\nrecall : 0.998636\r\nf1 : 0.998682\r\n------------------------------------\r\ntrain---epoch : 51 ,  global step : 24358\r\nloss :  0.0024001500569283962\r\naccuracy : 0.999844\r\nprecision : 0.998729\r\nrecall : 0.998638\r\nf1 : 0.998684\r\n```\r\ncan see that model is run well  on the train data when epoch is about 50.\r\nhowever, when opch is about 50, model is not desirable  on test data. \r\neval information ( txt file save all run information on the test data, then sorted by f1 value , this is before the twentieth) :\r\ncan see f1 less change from epoch 4 to epoch 50\r\n```\r\nepoch   loss    precision       recall  f1\r\n4       11.766307       0.198263        0.254603        0.222928\r\n2       10.437247       0.241509        0.203966        0.221156\r\n3       10.858424       0.199627        0.246282        0.220514\r\n1       9.906065        0.195629        0.225035        0.209304\r\n6       16.741554       0.167704        0.205205        0.184569\r\n12      18.872616       0.189472        0.166962        0.177506\r\n16      19.437512       0.179588        0.169795        0.174554\r\n5       14.804908       0.148019        0.211048        0.174002\r\n13      20.942182       0.203333        0.151204        0.173436\r\n7       16.623977       0.152858        0.195999        0.171761\r\n9       17.268815       0.159771        0.177762        0.168287\r\n10      19.142138       0.172041        0.157755        0.164589\r\n8       17.819491       0.155651        0.166785        0.161026\r\n15      21.475587       0.184768        0.142174        0.160696\r\n11      19.324749       0.170243        0.150319        0.159661\r\n19      22.313276       0.197936        0.132436        0.158693\r\n28      25.159835       0.209786        0.126771        0.158040\r\n20      23.833431       0.215899        0.123584        0.157190\r\n38      27.536659       0.200055        0.128187        0.156253\r\n0       10.829172       0.170155        0.143945        0.155956\r\n```\r\n\r\n\r\nPlease try to provide a minimal example to repro the bug.\r\nError messages and stack traces are also helpful.\r\n\r\n\r\n- PyTorch or Caffe2: pytorch 0.4\r\n- OS:Ubuntu 16"}