{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/404232359", "html_url": "https://github.com/pytorch/pytorch/issues/5059#issuecomment-404232359", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5059", "id": 404232359, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDIzMjM1OQ==", "user": {"login": "ltrottier", "id": 14935326, "node_id": "MDQ6VXNlcjE0OTM1MzI2", "avatar_url": "https://avatars0.githubusercontent.com/u/14935326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ltrottier", "html_url": "https://github.com/ltrottier", "followers_url": "https://api.github.com/users/ltrottier/followers", "following_url": "https://api.github.com/users/ltrottier/following{/other_user}", "gists_url": "https://api.github.com/users/ltrottier/gists{/gist_id}", "starred_url": "https://api.github.com/users/ltrottier/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ltrottier/subscriptions", "organizations_url": "https://api.github.com/users/ltrottier/orgs", "repos_url": "https://api.github.com/users/ltrottier/repos", "events_url": "https://api.github.com/users/ltrottier/events{/privacy}", "received_events_url": "https://api.github.com/users/ltrottier/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-11T16:31:00Z", "updated_at": "2018-07-11T16:31:00Z", "author_association": "NONE", "body_html": "<h4>Solution (torch &gt;= 0.4)</h4>\n<p>This is my solution:</p>\n<ol>\n<li>First, reset the numpy seed at the beginning of each epoch:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epoch <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(epoch))\n    np.random.seed() <span class=\"pl-c\"><span class=\"pl-c\">#</span> reset seed</span>\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> ds:\n        <span class=\"pl-c1\">print</span>(batch)</pre></div>\n<ol start=\"2\">\n<li>Use <code>worker_init_fn</code> to set the numpy seed of each worker:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">worker_init_fn</span>(<span class=\"pl-smi\">worker_id</span>):                                                          \n    np.random.seed(np.random.get_state()[<span class=\"pl-c1\">1</span>][<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> worker_id)\n\nds <span class=\"pl-k\">=</span> RandomDataset()\nds <span class=\"pl-k\">=</span> DataLoader(ds, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">worker_init_fn</span><span class=\"pl-k\">=</span>worker_init_fn)</pre></div>\n<h4>Explanations</h4>\n<p>After creating the workers, each worker has an independent seed that is initialized to the curent random seed + the id of the worker. Also, you need to reset the numpy random seed at the beginning of each epoch because all random seed modifications in <code>__getitem__</code> are local to each worker.</p>\n<p>FYI, <code>np.random.get_state()[1][0]</code> allows you to get the seed. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: np.random.seed(<span class=\"pl-c1\">1234</span>)\n   <span class=\"pl-c1\">...</span>: np.random.get_state()[<span class=\"pl-c1\">1</span>][<span class=\"pl-c1\">0</span>]\n   <span class=\"pl-c1\">...</span>: \nOut[<span class=\"pl-c1\">1</span>]: <span class=\"pl-c1\">1234</span></pre></div>\n<h4>Updated Code</h4>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> np.random.seed(0)</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Transform</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">item</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>):\n        <span class=\"pl-k\">return</span> [np.random.randint(<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">20000</span>), random.randint(<span class=\"pl-c1\">20000</span>,<span class=\"pl-c1\">30000</span>)]\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RandomDataset</span>(<span class=\"pl-c1\">object</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">ind</span>):\n        item <span class=\"pl-k\">=</span> [ind, np.random.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10000</span>), random.randint(<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">20000</span>), <span class=\"pl-c1\">0</span>]\n        tsfm <span class=\"pl-k\">=</span>Transform()(item)\n        <span class=\"pl-k\">return</span> np.array(item <span class=\"pl-k\">+</span> tsfm)\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">20</span>\n\n\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">worker_init_fn</span>(<span class=\"pl-smi\">worker_id</span>):                                                          \n    np.random.seed(np.random.get_state()[<span class=\"pl-c1\">1</span>][<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> worker_id)\n\nds <span class=\"pl-k\">=</span> RandomDataset()\nds <span class=\"pl-k\">=</span> DataLoader(ds, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">worker_init_fn</span><span class=\"pl-k\">=</span>worker_init_fn)\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epoch <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(epoch))\n    np.random.seed()\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> ds:\n        <span class=\"pl-c1\">print</span>(batch)</pre></div>", "body_text": "Solution (torch >= 0.4)\nThis is my solution:\n\nFirst, reset the numpy seed at the beginning of each epoch:\n\nfor epoch in range(2):\n    print(\"epoch {}\".format(epoch))\n    np.random.seed() # reset seed\n    for batch in ds:\n        print(batch)\n\nUse worker_init_fn to set the numpy seed of each worker:\n\ndef worker_init_fn(worker_id):                                                          \n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\nds = RandomDataset()\nds = DataLoader(ds, 10, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)\nExplanations\nAfter creating the workers, each worker has an independent seed that is initialized to the curent random seed + the id of the worker. Also, you need to reset the numpy random seed at the beginning of each epoch because all random seed modifications in __getitem__ are local to each worker.\nFYI, np.random.get_state()[1][0] allows you to get the seed. For example:\nIn [1]: np.random.seed(1234)\n   ...: np.random.get_state()[1][0]\n   ...: \nOut[1]: 1234\nUpdated Code\nimport numpy as np\nimport random\n\n# np.random.seed(0)\n\nclass Transform(object):\n    def __init__(self):\n        pass\n\n    def __call__(self, item = None):\n        return [np.random.randint(10000, 20000), random.randint(20000,30000)]\n\nclass RandomDataset(object):\n\n    def __init__(self):\n        pass\n\n    def __getitem__(self, ind):\n        item = [ind, np.random.randint(1, 10000), random.randint(10000, 20000), 0]\n        tsfm =Transform()(item)\n        return np.array(item + tsfm)\n    def __len__(self):\n        return 20\n\n\nfrom torch.utils.data import DataLoader\n\ndef worker_init_fn(worker_id):                                                          \n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\nds = RandomDataset()\nds = DataLoader(ds, 10, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)\n\nfor epoch in range(2):\n    print(\"epoch {}\".format(epoch))\n    np.random.seed()\n    for batch in ds:\n        print(batch)", "body": "#### Solution (torch >= 0.4)\r\n\r\nThis is my solution:\r\n\r\n1. First, reset the numpy seed at the beginning of each epoch:\r\n\r\n```python\r\nfor epoch in range(2):\r\n    print(\"epoch {}\".format(epoch))\r\n    np.random.seed() # reset seed\r\n    for batch in ds:\r\n        print(batch)\r\n```\r\n\r\n2. Use `worker_init_fn` to set the numpy seed of each worker:\r\n```python\r\ndef worker_init_fn(worker_id):                                                          \r\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\r\n\r\nds = RandomDataset()\r\nds = DataLoader(ds, 10, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)\r\n```\r\n\r\n#### Explanations\r\n\r\nAfter creating the workers, each worker has an independent seed that is initialized to the curent random seed + the id of the worker. Also, you need to reset the numpy random seed at the beginning of each epoch because all random seed modifications in `__getitem__` are local to each worker.\r\n\r\nFYI, `np.random.get_state()[1][0]` allows you to get the seed. For example:\r\n```python\r\nIn [1]: np.random.seed(1234)\r\n   ...: np.random.get_state()[1][0]\r\n   ...: \r\nOut[1]: 1234\r\n``` \r\n\r\n#### Updated Code\r\n\r\n```python\r\nimport numpy as np\r\nimport random\r\n\r\n# np.random.seed(0)\r\n\r\nclass Transform(object):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __call__(self, item = None):\r\n        return [np.random.randint(10000, 20000), random.randint(20000,30000)]\r\n\r\nclass RandomDataset(object):\r\n\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __getitem__(self, ind):\r\n        item = [ind, np.random.randint(1, 10000), random.randint(10000, 20000), 0]\r\n        tsfm =Transform()(item)\r\n        return np.array(item + tsfm)\r\n    def __len__(self):\r\n        return 20\r\n\r\n\r\nfrom torch.utils.data import DataLoader\r\n\r\ndef worker_init_fn(worker_id):                                                          \r\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\r\n\r\nds = RandomDataset()\r\nds = DataLoader(ds, 10, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)\r\n\r\nfor epoch in range(2):\r\n    print(\"epoch {}\".format(epoch))\r\n    np.random.seed()\r\n    for batch in ds:\r\n        print(batch)\r\n```"}