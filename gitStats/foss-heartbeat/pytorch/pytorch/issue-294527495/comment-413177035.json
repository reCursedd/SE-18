{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413177035", "html_url": "https://github.com/pytorch/pytorch/issues/5059#issuecomment-413177035", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5059", "id": 413177035, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzE3NzAzNQ==", "user": {"login": "fcalvet", "id": 38648518, "node_id": "MDQ6VXNlcjM4NjQ4NTE4", "avatar_url": "https://avatars0.githubusercontent.com/u/38648518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fcalvet", "html_url": "https://github.com/fcalvet", "followers_url": "https://api.github.com/users/fcalvet/followers", "following_url": "https://api.github.com/users/fcalvet/following{/other_user}", "gists_url": "https://api.github.com/users/fcalvet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fcalvet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fcalvet/subscriptions", "organizations_url": "https://api.github.com/users/fcalvet/orgs", "repos_url": "https://api.github.com/users/fcalvet/repos", "events_url": "https://api.github.com/users/fcalvet/events{/privacy}", "received_events_url": "https://api.github.com/users/fcalvet/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-15T12:01:08Z", "updated_at": "2018-08-15T12:04:38Z", "author_association": "NONE", "body_html": "<p>Despite this being a NumPy issue, I feel the doc doesn't emphasize this strongly enough. Especially:</p>\n<ul>\n<li>the fact that the seed is the same at every epoch for every worker (one could easily assume workers are not epoch dependent and that the issue only affect randomness \"in between workers\" which could be a non issue for certain application)</li>\n<li>as the NumPy issue doesn't seem to happen on Windows, code could be performing fine on Windows and doing nonsense on Linux for example.</li>\n</ul>\n<p>Furthermore, the example doesn't show how bad this can actually affect random generation (as for me it shows the issue for epoch reset and not in between workers).<br>\nHere is one to show how bad it can get</p>\n<pre><code>import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass FalseDataset(Dataset):\n    def __init__(self, length):\n        self.len = length\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, ID):\n        r = np.random.randint(1,10000)\n        return [r, torch.initial_seed()]\n    \nfalse_instance = FalseDataset(8)\ntrain_loader = DataLoader(false_instance, shuffle=False, num_workers=4)\n\ndef train_epoch(loader, epoch):\n    for batch_idx, output in enumerate(loader):\n        print(batch_idx, output)\n    return \"epoch\"+str(epoch)+\" ended\"\n\nfor i in range(2): #simulating epochs\n    print(train_epoch(train_loader, i))\n</code></pre>\n<p>which gives</p>\n<pre><code>0 [tensor([6441]), tensor([4899689041141450080])]\n1 [tensor([6441]), tensor([4899689041141450081])]\n2 [tensor([6441]), tensor([4899689041141450082])]\n3 [tensor([6441]), tensor([4899689041141450083])]\n4 [tensor([3807]), tensor([4899689041141450080])]\n5 [tensor([3807]), tensor([4899689041141450081])]\n6 [tensor([3807]), tensor([4899689041141450082])]\n7 [tensor([3807]), tensor([4899689041141450083])]\nepoch0 ended\n0 [tensor([6441]), tensor([1246574829706935184])]\n1 [tensor([6441]), tensor([1246574829706935185])]\n2 [tensor([6441]), tensor([1246574829706935186])]\n3 [tensor([6441]), tensor([1246574829706935187])]\n4 [tensor([3807]), tensor([1246574829706935184])]\n5 [tensor([3807]), tensor([1246574829706935185])]\n6 [tensor([3807]), tensor([1246574829706935186])]\n7 [tensor([3807]), tensor([1246574829706935187])]\nepoch1 ended\n</code></pre>", "body_text": "Despite this being a NumPy issue, I feel the doc doesn't emphasize this strongly enough. Especially:\n\nthe fact that the seed is the same at every epoch for every worker (one could easily assume workers are not epoch dependent and that the issue only affect randomness \"in between workers\" which could be a non issue for certain application)\nas the NumPy issue doesn't seem to happen on Windows, code could be performing fine on Windows and doing nonsense on Linux for example.\n\nFurthermore, the example doesn't show how bad this can actually affect random generation (as for me it shows the issue for epoch reset and not in between workers).\nHere is one to show how bad it can get\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass FalseDataset(Dataset):\n    def __init__(self, length):\n        self.len = length\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, ID):\n        r = np.random.randint(1,10000)\n        return [r, torch.initial_seed()]\n    \nfalse_instance = FalseDataset(8)\ntrain_loader = DataLoader(false_instance, shuffle=False, num_workers=4)\n\ndef train_epoch(loader, epoch):\n    for batch_idx, output in enumerate(loader):\n        print(batch_idx, output)\n    return \"epoch\"+str(epoch)+\" ended\"\n\nfor i in range(2): #simulating epochs\n    print(train_epoch(train_loader, i))\n\nwhich gives\n0 [tensor([6441]), tensor([4899689041141450080])]\n1 [tensor([6441]), tensor([4899689041141450081])]\n2 [tensor([6441]), tensor([4899689041141450082])]\n3 [tensor([6441]), tensor([4899689041141450083])]\n4 [tensor([3807]), tensor([4899689041141450080])]\n5 [tensor([3807]), tensor([4899689041141450081])]\n6 [tensor([3807]), tensor([4899689041141450082])]\n7 [tensor([3807]), tensor([4899689041141450083])]\nepoch0 ended\n0 [tensor([6441]), tensor([1246574829706935184])]\n1 [tensor([6441]), tensor([1246574829706935185])]\n2 [tensor([6441]), tensor([1246574829706935186])]\n3 [tensor([6441]), tensor([1246574829706935187])]\n4 [tensor([3807]), tensor([1246574829706935184])]\n5 [tensor([3807]), tensor([1246574829706935185])]\n6 [tensor([3807]), tensor([1246574829706935186])]\n7 [tensor([3807]), tensor([1246574829706935187])]\nepoch1 ended", "body": "Despite this being a NumPy issue, I feel the doc doesn't emphasize this strongly enough. Especially:\r\n- the fact that the seed is the same at every epoch for every worker (one could easily assume workers are not epoch dependent and that the issue only affect randomness \"in between workers\" which could be a non issue for certain application)\r\n- as the NumPy issue doesn't seem to happen on Windows, code could be performing fine on Windows and doing nonsense on Linux for example.\r\n\r\nFurthermore, the example doesn't show how bad this can actually affect random generation (as for me it shows the issue for epoch reset and not in between workers).\r\nHere is one to show how bad it can get\r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nclass FalseDataset(Dataset):\r\n    def __init__(self, length):\r\n        self.len = length\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n    def __getitem__(self, ID):\r\n        r = np.random.randint(1,10000)\r\n        return [r, torch.initial_seed()]\r\n    \r\nfalse_instance = FalseDataset(8)\r\ntrain_loader = DataLoader(false_instance, shuffle=False, num_workers=4)\r\n\r\ndef train_epoch(loader, epoch):\r\n    for batch_idx, output in enumerate(loader):\r\n        print(batch_idx, output)\r\n    return \"epoch\"+str(epoch)+\" ended\"\r\n\r\nfor i in range(2): #simulating epochs\r\n    print(train_epoch(train_loader, i))\r\n```\r\n\r\nwhich gives\r\n```\r\n0 [tensor([6441]), tensor([4899689041141450080])]\r\n1 [tensor([6441]), tensor([4899689041141450081])]\r\n2 [tensor([6441]), tensor([4899689041141450082])]\r\n3 [tensor([6441]), tensor([4899689041141450083])]\r\n4 [tensor([3807]), tensor([4899689041141450080])]\r\n5 [tensor([3807]), tensor([4899689041141450081])]\r\n6 [tensor([3807]), tensor([4899689041141450082])]\r\n7 [tensor([3807]), tensor([4899689041141450083])]\r\nepoch0 ended\r\n0 [tensor([6441]), tensor([1246574829706935184])]\r\n1 [tensor([6441]), tensor([1246574829706935185])]\r\n2 [tensor([6441]), tensor([1246574829706935186])]\r\n3 [tensor([6441]), tensor([1246574829706935187])]\r\n4 [tensor([3807]), tensor([1246574829706935184])]\r\n5 [tensor([3807]), tensor([1246574829706935185])]\r\n6 [tensor([3807]), tensor([1246574829706935186])]\r\n7 [tensor([3807]), tensor([1246574829706935187])]\r\nepoch1 ended\r\n```\r\n"}