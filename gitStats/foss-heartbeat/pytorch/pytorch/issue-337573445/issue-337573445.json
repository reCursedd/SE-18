{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9105/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9105/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9105/events", "html_url": "https://github.com/pytorch/pytorch/issues/9105", "id": 337573445, "node_id": "MDU6SXNzdWUzMzc1NzM0NDU=", "number": 9105, "title": "DistributedDataParallel Batch Size", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-07-02T15:52:13Z", "updated_at": "2018-07-05T23:58:24Z", "closed_at": "2018-07-05T23:58:17Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>The documentation for <code>DistributedDataParallel</code> mentions:</p>\n<blockquote>\n<p>The batch size should be larger than the number of GPUs used locally. It<br>\nshould also be an integer multiple of the number of GPUs so that each chunk<br>\nis the same size (so that each GPU processes the same number of samples).</p>\n</blockquote>\n<p>It's not clear if this is a suggestion or requirement. Does the module break if a batch size of one is run?</p>\n<p>Concerning my use case, sometimes during training, an evaluating step is run with a batch size of one to generate a result for tensorboard. It's not clear to me if it's okay for me to do that with  <code>DistributedDataParallel</code> or if I need to use the <code>DistributedDataParallel.module</code> attribute.</p>", "body_text": "Issue description\nThe documentation for DistributedDataParallel mentions:\n\nThe batch size should be larger than the number of GPUs used locally. It\nshould also be an integer multiple of the number of GPUs so that each chunk\nis the same size (so that each GPU processes the same number of samples).\n\nIt's not clear if this is a suggestion or requirement. Does the module break if a batch size of one is run?\nConcerning my use case, sometimes during training, an evaluating step is run with a batch size of one to generate a result for tensorboard. It's not clear to me if it's okay for me to do that with  DistributedDataParallel or if I need to use the DistributedDataParallel.module attribute.", "body": "## Issue description\r\n\r\nThe documentation for ``DistributedDataParallel`` mentions:\r\n\r\n> The batch size should be larger than the number of GPUs used locally. It\r\n> should also be an integer multiple of the number of GPUs so that each chunk\r\n> is the same size (so that each GPU processes the same number of samples).\r\n\r\nIt's not clear if this is a suggestion or requirement. Does the module break if a batch size of one is run? \r\n\r\nConcerning my use case, sometimes during training, an evaluating step is run with a batch size of one to generate a result for tensorboard. It's not clear to me if it's okay for me to do that with  ``DistributedDataParallel`` or if I need to use the ``DistributedDataParallel.module`` attribute.\r\n"}