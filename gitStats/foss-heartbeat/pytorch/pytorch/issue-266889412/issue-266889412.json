{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3179", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3179/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3179/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3179/events", "html_url": "https://github.com/pytorch/pytorch/issues/3179", "id": 266889412, "node_id": "MDU6SXNzdWUyNjY4ODk0MTI=", "number": 3179, "title": "max_norm not supported for nn.Embedding", "user": {"login": "backpropper", "id": 5422228, "node_id": "MDQ6VXNlcjU0MjIyMjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/5422228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/backpropper", "html_url": "https://github.com/backpropper", "followers_url": "https://api.github.com/users/backpropper/followers", "following_url": "https://api.github.com/users/backpropper/following{/other_user}", "gists_url": "https://api.github.com/users/backpropper/gists{/gist_id}", "starred_url": "https://api.github.com/users/backpropper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/backpropper/subscriptions", "organizations_url": "https://api.github.com/users/backpropper/orgs", "repos_url": "https://api.github.com/users/backpropper/repos", "events_url": "https://api.github.com/users/backpropper/events{/privacy}", "received_events_url": "https://api.github.com/users/backpropper/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-19T15:38:00Z", "updated_at": "2017-10-19T15:49:35Z", "closed_at": "2017-10-19T15:49:35Z", "author_association": "NONE", "body_html": "<p>The following code snippet reproduces the error:</p>\n<pre><code>enc = nn.Embedding(10, 200, max_norm=10)\ninput = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\nenc(input)\n</code></pre>\n<p>It gives the following error:<br>\n<code>TypeError: _renorm() missing 1 required positional argument: 'norm_type'</code><br>\nEven when I give some value to the argument norm_type, it gives the same error.<br>\nI saw in the documentation that _renorm() is not supported. Will it be fixed in the next release version?<br>\nPytorch version : 0.2.0_4</p>", "body_text": "The following code snippet reproduces the error:\nenc = nn.Embedding(10, 200, max_norm=10)\ninput = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\nenc(input)\n\nIt gives the following error:\nTypeError: _renorm() missing 1 required positional argument: 'norm_type'\nEven when I give some value to the argument norm_type, it gives the same error.\nI saw in the documentation that _renorm() is not supported. Will it be fixed in the next release version?\nPytorch version : 0.2.0_4", "body": "The following code snippet reproduces the error:\r\n```\r\nenc = nn.Embedding(10, 200, max_norm=10)\r\ninput = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\r\nenc(input)\r\n```\r\nIt gives the following error:\r\n`TypeError: _renorm() missing 1 required positional argument: 'norm_type'`\r\nEven when I give some value to the argument norm_type, it gives the same error.\r\nI saw in the documentation that _renorm() is not supported. Will it be fixed in the next release version?\r\nPytorch version : 0.2.0_4"}