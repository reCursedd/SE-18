{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397173419", "html_url": "https://github.com/pytorch/pytorch/issues/7903#issuecomment-397173419", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7903", "id": 397173419, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzE3MzQxOQ==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T05:16:28Z", "updated_at": "2018-06-14T05:21:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> update on the perf regression issue.<br>\nthe <a href=\"https://github.com/MlWoo/PyTorch-benchmark/blob/master/rnn_benchmark/run.sh\">rnn_benchmark</a> is running inference only by default. By <code>python LSTM.py train daily</code>, you can run training.</p>\n<p>One thing i didn't consider at the beginning is that <code>mkldnn v0.13</code> was compiled against <code>mkl v2018.0.2</code> of which certain gemm size has perf regression. This is fixed on <code>mkl v2018.0.3</code> and i have updated <code>mkldnn v0.14</code>. So make sure you have latest <code>mkl</code> and <code>mkldnn</code> installed.</p>\n<p>Once the distraction is eliminated, i tested different KMP environment setting. I tried <code>KMP_COMPOSABILITY</code> and scale <code>KMP_BLOCKTIME</code> at the range of <strong>[0.1, 10]</strong>, the default value is <code>200</code>.</p>\n<p>tested on Xeon 8180, 28 cores * 2 sockets, 2.5GHz, all tests have set <code>OMP_NUM_THREADS=56</code></p>\n<table>\n<thead>\n<tr>\n<th>SPS</th>\n<th>size</th>\n<th><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/4d2693973e638d5b8a8fe4e11bb4da85d1698596/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/4d2693973e638d5b8a8fe4e11bb4da85d1698596\"><tt>4d26939</tt></a></th>\n<th><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/c9fc7b5da80b7c8f568bb3d5b23d9d843d31efbf/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/c9fc7b5da80b7c8f568bb3d5b23d9d843d31efbf\"><tt>c9fc7b5</tt></a></th>\n<th><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/c9fc7b5da80b7c8f568bb3d5b23d9d843d31efbf/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/c9fc7b5da80b7c8f568bb3d5b23d9d843d31efbf\"><tt>c9fc7b5</tt></a>  + KMP_BLOCKTIME=1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>training</td>\n<td>[64, 50, 500,500]</td>\n<td>518.64</td>\n<td>324.48</td>\n<td>538.37</td>\n</tr>\n<tr>\n<td>training</td>\n<td>[128, 25, 4096, 4096]</td>\n<td>46.21</td>\n<td>45.35</td>\n<td>46.64</td>\n</tr>\n<tr>\n<td>inference</td>\n<td>[64, 50, 500,500]</td>\n<td><strong>2173.17</strong></td>\n<td>1265.75</td>\n<td><strong>1252.68</strong></td>\n</tr>\n<tr>\n<td>inference</td>\n<td>[128, 25, 4096, 4096]</td>\n<td>93.70</td>\n<td>92.22</td>\n<td>92.60</td>\n</tr>\n</tbody>\n</table>\n<p>it turns out with <code>KMP_BLOCKTIME=1</code> has the best perf, but <strong>1~10</strong> actually has very similar perf. Perf regression is pretty much solved except on inference of small LSTM, still has <strong>1.7x</strong> gap.</p>\n<p>i will continue to dig how to solve this.</p>", "body_text": "@cpuhrsch update on the perf regression issue.\nthe rnn_benchmark is running inference only by default. By python LSTM.py train daily, you can run training.\nOne thing i didn't consider at the beginning is that mkldnn v0.13 was compiled against mkl v2018.0.2 of which certain gemm size has perf regression. This is fixed on mkl v2018.0.3 and i have updated mkldnn v0.14. So make sure you have latest mkl and mkldnn installed.\nOnce the distraction is eliminated, i tested different KMP environment setting. I tried KMP_COMPOSABILITY and scale KMP_BLOCKTIME at the range of [0.1, 10], the default value is 200.\ntested on Xeon 8180, 28 cores * 2 sockets, 2.5GHz, all tests have set OMP_NUM_THREADS=56\n\n\n\nSPS\nsize\n4d26939\nc9fc7b5\nc9fc7b5  + KMP_BLOCKTIME=1\n\n\n\n\ntraining\n[64, 50, 500,500]\n518.64\n324.48\n538.37\n\n\ntraining\n[128, 25, 4096, 4096]\n46.21\n45.35\n46.64\n\n\ninference\n[64, 50, 500,500]\n2173.17\n1265.75\n1252.68\n\n\ninference\n[128, 25, 4096, 4096]\n93.70\n92.22\n92.60\n\n\n\nit turns out with KMP_BLOCKTIME=1 has the best perf, but 1~10 actually has very similar perf. Perf regression is pretty much solved except on inference of small LSTM, still has 1.7x gap.\ni will continue to dig how to solve this.", "body": "@cpuhrsch update on the perf regression issue.\r\nthe [rnn_benchmark](https://github.com/MlWoo/PyTorch-benchmark/blob/master/rnn_benchmark/run.sh) is running inference only by default. By `python LSTM.py train daily`, you can run training.\r\n\r\nOne thing i didn't consider at the beginning is that `mkldnn v0.13` was compiled against `mkl v2018.0.2` of which certain gemm size has perf regression. This is fixed on `mkl v2018.0.3` and i have updated `mkldnn v0.14`. So make sure you have latest `mkl` and `mkldnn` installed.\r\n\r\nOnce the distraction is eliminated, i tested different KMP environment setting. I tried `KMP_COMPOSABILITY` and scale `KMP_BLOCKTIME` at the range of **[0.1, 10]**, the default value is `200`.\r\n\r\ntested on Xeon 8180, 28 cores * 2 sockets, 2.5GHz, all tests have set `OMP_NUM_THREADS=56`\r\n\r\nSPS | size | 4d26939 | c9fc7b5 | c9fc7b5  + KMP_BLOCKTIME=1\r\n--|--|--|-- | --\r\ntraining | [64, 50, 500,500] | 518.64 | 324.48 | 538.37\r\ntraining | [128, 25, 4096, 4096] | 46.21 | 45.35 | 46.64\r\ninference | [64, 50, 500,500] | **2173.17** | 1265.75 | **1252.68**\r\ninference | [128, 25, 4096, 4096] | 93.70 | 92.22 | 92.60\r\n\r\nit turns out with `KMP_BLOCKTIME=1` has the best perf, but **1~10** actually has very similar perf. Perf regression is pretty much solved except on inference of small LSTM, still has **1.7x** gap.\r\n\r\ni will continue to dig how to solve this.\r\n\r\n"}