{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7228", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7228/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7228/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7228/events", "html_url": "https://github.com/pytorch/pytorch/issues/7228", "id": 319787960, "node_id": "MDU6SXNzdWUzMTk3ODc5NjA=", "number": 7228, "title": "[feature request] hash of torch.Device", "user": {"login": "mariogeiger", "id": 333780, "node_id": "MDQ6VXNlcjMzMzc4MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/333780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariogeiger", "html_url": "https://github.com/mariogeiger", "followers_url": "https://api.github.com/users/mariogeiger/followers", "following_url": "https://api.github.com/users/mariogeiger/following{/other_user}", "gists_url": "https://api.github.com/users/mariogeiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariogeiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariogeiger/subscriptions", "organizations_url": "https://api.github.com/users/mariogeiger/orgs", "repos_url": "https://api.github.com/users/mariogeiger/repos", "events_url": "https://api.github.com/users/mariogeiger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariogeiger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2018-05-03T04:53:39Z", "updated_at": "2018-07-18T06:57:54Z", "closed_at": "2018-07-18T00:11:18Z", "author_association": "NONE", "body_html": "<p>Do you think it is a good idea that <code>torch.Device</code> support hashing ?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">hash</span>(torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu<span class=\"pl-pds\">\"</span></span>))\n<span class=\"pl-c1\">TypeError</span>: unhashable <span class=\"pl-c1\">type</span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.Device<span class=\"pl-pds\">'</span></span></pre></div>\n<p>On my side, it would be useful for cached functions like</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@lru_cache</span>()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">big_constant_tensor</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">device</span>):\n    <span class=\"pl-k\">return</span> torch.tensor((<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>), math.pi, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device)</pre></div>\n<p>Currently I do this to avoid the problem</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@lru_cache</span>()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">big_constant_tensor</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">device_type</span>, <span class=\"pl-smi\">device_index</span>):\n    <span class=\"pl-k\">return</span> torch.tensor((<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>), math.pi, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>torch.device(device_type, device_index))</pre></div>", "body_text": "Do you think it is a good idea that torch.Device support hashing ?\nhash(torch.device(\"cpu\"))\nTypeError: unhashable type: 'torch.Device'\nOn my side, it would be useful for cached functions like\n@lru_cache()\ndef big_constant_tensor(x, y, device):\n    return torch.tensor((1000, 1000), math.pi, device=device)\nCurrently I do this to avoid the problem\n@lru_cache()\ndef big_constant_tensor(x, y, device_type, device_index):\n    return torch.tensor((1000, 1000), math.pi, device=torch.device(device_type, device_index))", "body": "Do you think it is a good idea that `torch.Device` support hashing ?\r\n```python\r\nhash(torch.device(\"cpu\"))\r\nTypeError: unhashable type: 'torch.Device'\r\n```\r\n\r\nOn my side, it would be useful for cached functions like \r\n```python\r\n@lru_cache()\r\ndef big_constant_tensor(x, y, device):\r\n    return torch.tensor((1000, 1000), math.pi, device=device)\r\n```\r\n\r\nCurrently I do this to avoid the problem\r\n```python\r\n@lru_cache()\r\ndef big_constant_tensor(x, y, device_type, device_index):\r\n    return torch.tensor((1000, 1000), math.pi, device=torch.device(device_type, device_index))\r\n```\r\n"}