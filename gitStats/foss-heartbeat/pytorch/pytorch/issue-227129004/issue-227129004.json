{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1513", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1513/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1513/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1513/events", "html_url": "https://github.com/pytorch/pytorch/issues/1513", "id": 227129004, "node_id": "MDU6SXNzdWUyMjcxMjkwMDQ=", "number": 1513, "title": "list(tensor) seems to disregard tensor device and initialize the current device instead", "user": {"login": "vzhong", "id": 1855260, "node_id": "MDQ6VXNlcjE4NTUyNjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1855260?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vzhong", "html_url": "https://github.com/vzhong", "followers_url": "https://api.github.com/users/vzhong/followers", "following_url": "https://api.github.com/users/vzhong/following{/other_user}", "gists_url": "https://api.github.com/users/vzhong/gists{/gist_id}", "starred_url": "https://api.github.com/users/vzhong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vzhong/subscriptions", "organizations_url": "https://api.github.com/users/vzhong/orgs", "repos_url": "https://api.github.com/users/vzhong/repos", "events_url": "https://api.github.com/users/vzhong/events{/privacy}", "received_events_url": "https://api.github.com/users/vzhong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-05-08T18:14:24Z", "updated_at": "2017-05-09T07:37:18Z", "closed_at": "2017-05-08T19:21:56Z", "author_association": "NONE", "body_html": "<p>The default way of converting a <code>Tensor</code> into a list of native type numbers in Python is this:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">5</span>).gpu(<span class=\"pl-c1\">2</span>)\na.cpu().numpy().tolist()</pre></div>\n<p>However, there is a less verbose way of doing this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">list</span>(a)</pre></div>\n<p>While debugging a memory leak, I found that the latter method of using <code>list</code> actually leaks memory onto a default GPU:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">5</span>).gpu(<span class=\"pl-c1\">2</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> now GPU2 is used</span>\nb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(a)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> now memory has leaked to GPU0</span></pre></div>\n<p>Is this behaviour intended? It seems unintuitive that calling <code>list</code> on a <code>Tensor</code> on GPU 2 would use up storage on GPU 0.</p>", "body_text": "The default way of converting a Tensor into a list of native type numbers in Python is this:\na = torch.Tensor(5).gpu(2)\na.cpu().numpy().tolist()\nHowever, there is a less verbose way of doing this:\nlist(a)\nWhile debugging a memory leak, I found that the latter method of using list actually leaks memory onto a default GPU:\na = torch.Tensor(5).gpu(2)  # now GPU2 is used\nb = list(a)  # now memory has leaked to GPU0\nIs this behaviour intended? It seems unintuitive that calling list on a Tensor on GPU 2 would use up storage on GPU 0.", "body": "The default way of converting a `Tensor` into a list of native type numbers in Python is this:\r\n\r\n```python\r\na = torch.Tensor(5).gpu(2)\r\na.cpu().numpy().tolist()\r\n``` \r\n\r\nHowever, there is a less verbose way of doing this:\r\n\r\n```python\r\nlist(a)\r\n```\r\n\r\nWhile debugging a memory leak, I found that the latter method of using `list` actually leaks memory onto a default GPU:\r\n\r\n```python\r\na = torch.Tensor(5).gpu(2)  # now GPU2 is used\r\nb = list(a)  # now memory has leaked to GPU0\r\n```\r\n\r\nIs this behaviour intended? It seems unintuitive that calling `list` on a `Tensor` on GPU 2 would use up storage on GPU 0."}