{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3223", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3223/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3223/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3223/events", "html_url": "https://github.com/pytorch/pytorch/issues/3223", "id": 267484477, "node_id": "MDU6SXNzdWUyNjc0ODQ0Nzc=", "number": 3223, "title": "[Feature Request] more flexible splitting of Tensors", "user": {"login": "clavichord93", "id": 5952339, "node_id": "MDQ6VXNlcjU5NTIzMzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5952339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clavichord93", "html_url": "https://github.com/clavichord93", "followers_url": "https://api.github.com/users/clavichord93/followers", "following_url": "https://api.github.com/users/clavichord93/following{/other_user}", "gists_url": "https://api.github.com/users/clavichord93/gists{/gist_id}", "starred_url": "https://api.github.com/users/clavichord93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clavichord93/subscriptions", "organizations_url": "https://api.github.com/users/clavichord93/orgs", "repos_url": "https://api.github.com/users/clavichord93/repos", "events_url": "https://api.github.com/users/clavichord93/events{/privacy}", "received_events_url": "https://api.github.com/users/clavichord93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-10-22T17:50:50Z", "updated_at": "2018-05-02T05:48:50Z", "closed_at": "2018-05-02T05:48:50Z", "author_association": "NONE", "body_html": "<p>Currently, <code>torch.split</code> only splits a Tensor into equal-size (if possible) chunks. But sometimes Tensors need to be split into more flexible chunks.</p>\n<p>For example, splitting a Tensor of size (256, 256, 32, 32) into two chunks of sizes (256, 32, 32, 32) and (256, 224, 32, 32) along dim 1, like <code>tf.split</code> in TensorFlow.</p>\n<p>So can this be added into PyTorch standard operators?</p>", "body_text": "Currently, torch.split only splits a Tensor into equal-size (if possible) chunks. But sometimes Tensors need to be split into more flexible chunks.\nFor example, splitting a Tensor of size (256, 256, 32, 32) into two chunks of sizes (256, 32, 32, 32) and (256, 224, 32, 32) along dim 1, like tf.split in TensorFlow.\nSo can this be added into PyTorch standard operators?", "body": "Currently, `torch.split` only splits a Tensor into equal-size (if possible) chunks. But sometimes Tensors need to be split into more flexible chunks.\r\n\r\nFor example, splitting a Tensor of size (256, 256, 32, 32) into two chunks of sizes (256, 32, 32, 32) and (256, 224, 32, 32) along dim 1, like `tf.split` in TensorFlow.\r\n\r\nSo can this be added into PyTorch standard operators?"}