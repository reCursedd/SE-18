{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338613854", "html_url": "https://github.com/pytorch/pytorch/issues/3223#issuecomment-338613854", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3223", "id": 338613854, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODYxMzg1NA==", "user": {"login": "ptrblck", "id": 11662379, "node_id": "MDQ6VXNlcjExNjYyMzc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11662379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptrblck", "html_url": "https://github.com/ptrblck", "followers_url": "https://api.github.com/users/ptrblck/followers", "following_url": "https://api.github.com/users/ptrblck/following{/other_user}", "gists_url": "https://api.github.com/users/ptrblck/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptrblck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptrblck/subscriptions", "organizations_url": "https://api.github.com/users/ptrblck/orgs", "repos_url": "https://api.github.com/users/ptrblck/repos", "events_url": "https://api.github.com/users/ptrblck/events{/privacy}", "received_events_url": "https://api.github.com/users/ptrblck/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T10:17:03Z", "updated_at": "2017-10-23T10:17:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,</p>\n<p>I tried to implement your desired functionality, since I think it might be quite useful.</p>\n<p>Here is the code snippet:</p>\n<pre><code>def size_splits(tensor, split_sizes, dim=0):\n    \"\"\"Splits the tensor according to chunks of split_sizes.\n    \n    Arguments:\n        tensor (Tensor): tensor to split.\n        split_sizes (list(int)): sizes of chunks\n        dim (int): dimension along which to split the tensor.\n    \"\"\"\n    if dim &lt; 0:\n        dim += tensor.dim()\n    \n    dim_size = tensor.size(dim)\n    if dim_size != torch.sum(torch.Tensor(split_sizes)):\n        raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n    \n    splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n\n    return tuple(tensor.narrow(int(dim), int(start), int(length)) \n        for start, length in zip(splits, split_sizes))\n</code></pre>\n<p>Here are some tests:</p>\n<pre><code>a = torch.randn(20, 10, 2, 2)\n\nsplit_sizes = [2, 2, 6]\ndim = 1\nres = size_splits(tensor=a, split_sizes=split_sizes, dim=dim)\n\nfor r, split_size in zip(res, split_sizes):\n    assert r.size(dim) == split_size\n\ntry:\n    res = size_splits(tensor=a, split_sizes=split_sizes, dim=0)\nexcept KeyError as e: \n    print(e)\n\nsplit_sizes = [5, 5, 10]\ndim = 0\nres = size_splits(tensor=a, split_sizes=split_sizes, dim=dim)\n\nfor r, split_size in zip(res, split_sizes):\n    assert r.size(dim) == split_size\n</code></pre>\n<p>I would like to create a pull request, if others find this function useful as well.</p>\n<p>Greets!</p>", "body_text": "Hi,\nI tried to implement your desired functionality, since I think it might be quite useful.\nHere is the code snippet:\ndef size_splits(tensor, split_sizes, dim=0):\n    \"\"\"Splits the tensor according to chunks of split_sizes.\n    \n    Arguments:\n        tensor (Tensor): tensor to split.\n        split_sizes (list(int)): sizes of chunks\n        dim (int): dimension along which to split the tensor.\n    \"\"\"\n    if dim < 0:\n        dim += tensor.dim()\n    \n    dim_size = tensor.size(dim)\n    if dim_size != torch.sum(torch.Tensor(split_sizes)):\n        raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n    \n    splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n\n    return tuple(tensor.narrow(int(dim), int(start), int(length)) \n        for start, length in zip(splits, split_sizes))\n\nHere are some tests:\na = torch.randn(20, 10, 2, 2)\n\nsplit_sizes = [2, 2, 6]\ndim = 1\nres = size_splits(tensor=a, split_sizes=split_sizes, dim=dim)\n\nfor r, split_size in zip(res, split_sizes):\n    assert r.size(dim) == split_size\n\ntry:\n    res = size_splits(tensor=a, split_sizes=split_sizes, dim=0)\nexcept KeyError as e: \n    print(e)\n\nsplit_sizes = [5, 5, 10]\ndim = 0\nres = size_splits(tensor=a, split_sizes=split_sizes, dim=dim)\n\nfor r, split_size in zip(res, split_sizes):\n    assert r.size(dim) == split_size\n\nI would like to create a pull request, if others find this function useful as well.\nGreets!", "body": "Hi,\r\n\r\nI tried to implement your desired functionality, since I think it might be quite useful.\r\n\r\nHere is the code snippet:\r\n\r\n    def size_splits(tensor, split_sizes, dim=0):\r\n        \"\"\"Splits the tensor according to chunks of split_sizes.\r\n        \r\n        Arguments:\r\n            tensor (Tensor): tensor to split.\r\n            split_sizes (list(int)): sizes of chunks\r\n            dim (int): dimension along which to split the tensor.\r\n        \"\"\"\r\n        if dim < 0:\r\n            dim += tensor.dim()\r\n        \r\n        dim_size = tensor.size(dim)\r\n        if dim_size != torch.sum(torch.Tensor(split_sizes)):\r\n            raise KeyError(\"Sum of split sizes exceeds tensor dim\")\r\n        \r\n        splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\r\n    \r\n        return tuple(tensor.narrow(int(dim), int(start), int(length)) \r\n            for start, length in zip(splits, split_sizes))\r\n\r\nHere are some tests:\r\n\r\n    a = torch.randn(20, 10, 2, 2)\r\n    \r\n    split_sizes = [2, 2, 6]\r\n    dim = 1\r\n    res = size_splits(tensor=a, split_sizes=split_sizes, dim=dim)\r\n    \r\n    for r, split_size in zip(res, split_sizes):\r\n        assert r.size(dim) == split_size\r\n    \r\n    try:\r\n        res = size_splits(tensor=a, split_sizes=split_sizes, dim=0)\r\n    except KeyError as e: \r\n        print(e)\r\n    \r\n    split_sizes = [5, 5, 10]\r\n    dim = 0\r\n    res = size_splits(tensor=a, split_sizes=split_sizes, dim=dim)\r\n    \r\n    for r, split_size in zip(res, split_sizes):\r\n        assert r.size(dim) == split_size\r\n\r\nI would like to create a pull request, if others find this function useful as well.\r\n\r\nGreets!"}