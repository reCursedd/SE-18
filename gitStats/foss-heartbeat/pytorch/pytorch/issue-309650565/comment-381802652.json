{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/381802652", "html_url": "https://github.com/pytorch/pytorch/pull/6104#issuecomment-381802652", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6104", "id": 381802652, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTgwMjY1Mg==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-17T01:42:10Z", "updated_at": "2018-04-17T01:50:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> requires some help here! I have trouble in understanding jit logic...</p>\n<p>LSTM and GRU requires caching some forward pass computation results for backward path, this is done via <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/rnnFusedPointwise.py#L52\">ctx in LSTMFused</a>.</p>\n<p>And this ctx is causing the fail of <code>test_legacy_traced_module</code> of test_jit.py, raising a runtime error at <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/interpreter.cpp#L504\">interpreter.cpp#L504</a>, <code>output_tuple.size()</code> equals 2 while <code>num_output</code> equals 3. The extra output refers to <code>ctx</code> and compiled to <code>Handle</code>.</p>\n<p>The test case failure has nothing to do with the file of <code>aten/src/THNN/generic/FusedRNNKernel.c</code> actually. If you hack the code of <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L300\">cudnn path</a> to let the code run into  <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/generic/FusedRNNKernel.cu\">THCUNN/generic/FusedRNNKernel.cu</a>, the case failed in the same way. (GPU LSTM choose cudnn over THCUNN so have to hack it here)</p>\n<p>I tried the following code with nn.Dropout, also failed. Somehow if i compile with <code>torch.jit.compile(dd.forward)</code> instead of <code>@torch.jit.compile(nderivs=0)</code>, it passes.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-en\">@torch.jit.compile</span>(<span class=\"pl-v\">nderivs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">my_dropout</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Dropout</span>):\n    <span class=\"pl-k\">pass</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>))\ndd <span class=\"pl-k\">=</span> my_dropout()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>interpreted run</span>\nout1 <span class=\"pl-k\">=</span> dd(<span class=\"pl-c1\">input</span>)\n<span class=\"pl-c1\">print</span>(out1)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>compiled run</span>\nout2 <span class=\"pl-k\">=</span> dd(<span class=\"pl-c1\">input</span>)\n<span class=\"pl-c1\">print</span>(out2)</pre></div>", "body_text": "@apaszke requires some help here! I have trouble in understanding jit logic...\nLSTM and GRU requires caching some forward pass computation results for backward path, this is done via ctx in LSTMFused.\nAnd this ctx is causing the fail of test_legacy_traced_module of test_jit.py, raising a runtime error at interpreter.cpp#L504, output_tuple.size() equals 2 while num_output equals 3. The extra output refers to ctx and compiled to Handle.\nThe test case failure has nothing to do with the file of aten/src/THNN/generic/FusedRNNKernel.c actually. If you hack the code of cudnn path to let the code run into  THCUNN/generic/FusedRNNKernel.cu, the case failed in the same way. (GPU LSTM choose cudnn over THCUNN so have to hack it here)\nI tried the following code with nn.Dropout, also failed. Somehow if i compile with torch.jit.compile(dd.forward) instead of @torch.jit.compile(nderivs=0), it passes.\nimport torch\nfrom torch.autograd import Variable\n\n@torch.jit.compile(nderivs=0)\nclass my_dropout(torch.nn.Dropout):\n    pass\ninput = Variable(torch.randn(3, 5))\ndd = my_dropout()\n\n#interpreted run\nout1 = dd(input)\nprint(out1)\n#compiled run\nout2 = dd(input)\nprint(out2)", "body": "@apaszke requires some help here! I have trouble in understanding jit logic...\r\n\r\nLSTM and GRU requires caching some forward pass computation results for backward path, this is done via [ctx in LSTMFused](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/rnnFusedPointwise.py#L52). \r\n\r\nAnd this ctx is causing the fail of `test_legacy_traced_module` of test_jit.py, raising a runtime error at [interpreter.cpp#L504](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/interpreter.cpp#L504), `output_tuple.size()` equals 2 while `num_output` equals 3. The extra output refers to `ctx` and compiled to `Handle`.\r\n\r\nThe test case failure has nothing to do with the file of `aten/src/THNN/generic/FusedRNNKernel.c` actually. If you hack the code of [cudnn path](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L300) to let the code run into  [THCUNN/generic/FusedRNNKernel.cu](https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/generic/FusedRNNKernel.cu), the case failed in the same way. (GPU LSTM choose cudnn over THCUNN so have to hack it here)\r\n\r\nI tried the following code with nn.Dropout, also failed. Somehow if i compile with `torch.jit.compile(dd.forward)` instead of `@torch.jit.compile(nderivs=0)`, it passes.\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n@torch.jit.compile(nderivs=0)\r\nclass my_dropout(torch.nn.Dropout):\r\n    pass\r\ninput = Variable(torch.randn(3, 5))\r\ndd = my_dropout()\r\n\r\n#interpreted run\r\nout1 = dd(input)\r\nprint(out1)\r\n#compiled run\r\nout2 = dd(input)\r\nprint(out2)\r\n```"}