{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6104", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6104/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6104/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6104/events", "html_url": "https://github.com/pytorch/pytorch/pull/6104", "id": 309650565, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc4MjQ5NDUx", "number": 6104, "title": "implement fusedRNNKernel for THNN module", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2018-03-29T07:40:35Z", "updated_at": "2018-11-23T15:41:21Z", "closed_at": "2018-09-10T01:52:40Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6104", "html_url": "https://github.com/pytorch/pytorch/pull/6104", "diff_url": "https://github.com/pytorch/pytorch/pull/6104.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6104.patch"}, "body_html": "<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282297290\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4186\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4186/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4186\">#4186</a> this PR provides fused GRU and LSTM implementation in aten/src/THNN/generic/FusedRNNKernel.c<br>\nto speedup CPU rnn performance.<br>\nOn my test machine, Xeon skylake 8180 (56 core @ 2.5GHz), the performance comparison of this PR is listed in the table below:<br>\n(<strong>B</strong>: batch size, <strong>T</strong>: time step, <strong>I</strong>: input size, <strong>H</strong>: hidden size)<br>\nthe <a href=\"https://github.com/xhzhao/pytorch-rnn-benchmark\">benchmark</a> is a pytorch version of baidu's <a href=\"https://github.com/baidu-research/DeepBench\">deep-bench</a>, unit is <strong>SPS</strong> sentences per second, the higher the better.</p>\n<table>\n<thead>\n<tr>\n<th>B</th>\n<th>T</th>\n<th>I</th>\n<th>H</th>\n<th>original (SPS)</th>\n<th>fused (SPS)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>64</td>\n<td>15</td>\n<td>500</td>\n<td>500</td>\n<td>876</td>\n<td>2427</td>\n</tr>\n<tr>\n<td>64</td>\n<td>20</td>\n<td>500</td>\n<td>500</td>\n<td>689</td>\n<td>2140</td>\n</tr>\n<tr>\n<td>64</td>\n<td>25</td>\n<td>500</td>\n<td>500</td>\n<td>479</td>\n<td>1445</td>\n</tr>\n<tr>\n<td>64</td>\n<td>30</td>\n<td>500</td>\n<td>500</td>\n<td>460</td>\n<td>1262</td>\n</tr>\n<tr>\n<td>64</td>\n<td>35</td>\n<td>500</td>\n<td>500</td>\n<td>398</td>\n<td>1308</td>\n</tr>\n<tr>\n<td>64</td>\n<td>40</td>\n<td>500</td>\n<td>500</td>\n<td>350</td>\n<td>1107</td>\n</tr>\n<tr>\n<td>64</td>\n<td>45</td>\n<td>500</td>\n<td>500</td>\n<td>309</td>\n<td>1011</td>\n</tr>\n<tr>\n<td>64</td>\n<td>50</td>\n<td>500</td>\n<td>500</td>\n<td>280</td>\n<td>915</td>\n</tr>\n<tr>\n<td>16</td>\n<td>25</td>\n<td>512</td>\n<td>512</td>\n<td>340</td>\n<td>657</td>\n</tr>\n<tr>\n<td>32</td>\n<td>25</td>\n<td>512</td>\n<td>512</td>\n<td>416</td>\n<td>921</td>\n</tr>\n<tr>\n<td>64</td>\n<td>25</td>\n<td>512</td>\n<td>512</td>\n<td>492</td>\n<td>1236</td>\n</tr>\n<tr>\n<td>128</td>\n<td>25</td>\n<td>512</td>\n<td>512</td>\n<td>582</td>\n<td>1765</td>\n</tr>\n<tr>\n<td>16</td>\n<td>25</td>\n<td>1024</td>\n<td>1024</td>\n<td>93</td>\n<td>149</td>\n</tr>\n<tr>\n<td>32</td>\n<td>25</td>\n<td>1024</td>\n<td>1024</td>\n<td>140</td>\n<td>215</td>\n</tr>\n<tr>\n<td>64</td>\n<td>25</td>\n<td>1024</td>\n<td>1024</td>\n<td>197</td>\n<td>335</td>\n</tr>\n<tr>\n<td>128</td>\n<td>25</td>\n<td>1024</td>\n<td>1024</td>\n<td>532</td>\n<td>606</td>\n</tr>\n<tr>\n<td>16</td>\n<td>25</td>\n<td>2048</td>\n<td>2048</td>\n<td>23</td>\n<td>28</td>\n</tr>\n<tr>\n<td>32</td>\n<td>25</td>\n<td>2048</td>\n<td>2048</td>\n<td>41</td>\n<td>51</td>\n</tr>\n<tr>\n<td>64</td>\n<td>25</td>\n<td>2048</td>\n<td>2048</td>\n<td>88</td>\n<td>97</td>\n</tr>\n<tr>\n<td>128</td>\n<td>25</td>\n<td>2048</td>\n<td>2048</td>\n<td>162</td>\n<td>174</td>\n</tr>\n<tr>\n<td>16</td>\n<td>25</td>\n<td>4096</td>\n<td>4096</td>\n<td>6.7</td>\n<td>7.0</td>\n</tr>\n<tr>\n<td>32</td>\n<td>25</td>\n<td>4096</td>\n<td>4096</td>\n<td>13.5</td>\n<td>13.7</td>\n</tr>\n<tr>\n<td>64</td>\n<td>25</td>\n<td>4096</td>\n<td>4096</td>\n<td>24.9</td>\n<td>25.3</td>\n</tr>\n<tr>\n<td>128</td>\n<td>25</td>\n<td>4096</td>\n<td>4096</td>\n<td>45.1</td>\n<td>47.8</td>\n</tr>\n</tbody>\n</table>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a>, the <a href=\"https://github.com/yf225/examples/tree/benchmark_test/word_language_model\">word_language_model</a> performance pretty much doubles on my machine.</p>\n<p>Some environment settings are needed to regulate OpenMP threads behavior so as to achieve better performance,</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span># take Xeon for example</span>\n<span class=\"pl-k\">export</span> OMP_NUM_THREADS=[number of physical cores]\n<span class=\"pl-k\">export</span> KMP_AFFINITY=granularity=fine,compact,1,0</pre></div>\n<p>generally, fused rnn kernel receives more benefit when input size / hidden size are small, as gemm will donate in case of large input size / hidden size. By comparing the last four rows,  we can see increasing batch size is almost a free launch.</p>\n<p>However, aten/src/THNN/generic/FusedRNNKernel.c is fusing only element wise operation such as sigmoid, add, mul. To achieve more performance speedup, need to further fuse <strong>x*W</strong> gemm accross different time steps, this type of optimization is WIP.</p>", "body_text": "#4186 this PR provides fused GRU and LSTM implementation in aten/src/THNN/generic/FusedRNNKernel.c\nto speedup CPU rnn performance.\nOn my test machine, Xeon skylake 8180 (56 core @ 2.5GHz), the performance comparison of this PR is listed in the table below:\n(B: batch size, T: time step, I: input size, H: hidden size)\nthe benchmark is a pytorch version of baidu's deep-bench, unit is SPS sentences per second, the higher the better.\n\n\n\nB\nT\nI\nH\noriginal (SPS)\nfused (SPS)\n\n\n\n\n64\n15\n500\n500\n876\n2427\n\n\n64\n20\n500\n500\n689\n2140\n\n\n64\n25\n500\n500\n479\n1445\n\n\n64\n30\n500\n500\n460\n1262\n\n\n64\n35\n500\n500\n398\n1308\n\n\n64\n40\n500\n500\n350\n1107\n\n\n64\n45\n500\n500\n309\n1011\n\n\n64\n50\n500\n500\n280\n915\n\n\n16\n25\n512\n512\n340\n657\n\n\n32\n25\n512\n512\n416\n921\n\n\n64\n25\n512\n512\n492\n1236\n\n\n128\n25\n512\n512\n582\n1765\n\n\n16\n25\n1024\n1024\n93\n149\n\n\n32\n25\n1024\n1024\n140\n215\n\n\n64\n25\n1024\n1024\n197\n335\n\n\n128\n25\n1024\n1024\n532\n606\n\n\n16\n25\n2048\n2048\n23\n28\n\n\n32\n25\n2048\n2048\n41\n51\n\n\n64\n25\n2048\n2048\n88\n97\n\n\n128\n25\n2048\n2048\n162\n174\n\n\n16\n25\n4096\n4096\n6.7\n7.0\n\n\n32\n25\n4096\n4096\n13.5\n13.7\n\n\n64\n25\n4096\n4096\n24.9\n25.3\n\n\n128\n25\n4096\n4096\n45.1\n47.8\n\n\n\n@yf225, the word_language_model performance pretty much doubles on my machine.\nSome environment settings are needed to regulate OpenMP threads behavior so as to achieve better performance,\n## take Xeon for example\nexport OMP_NUM_THREADS=[number of physical cores]\nexport KMP_AFFINITY=granularity=fine,compact,1,0\ngenerally, fused rnn kernel receives more benefit when input size / hidden size are small, as gemm will donate in case of large input size / hidden size. By comparing the last four rows,  we can see increasing batch size is almost a free launch.\nHowever, aten/src/THNN/generic/FusedRNNKernel.c is fusing only element wise operation such as sigmoid, add, mul. To achieve more performance speedup, need to further fuse x*W gemm accross different time steps, this type of optimization is WIP.", "body": "#4186 this PR provides fused GRU and LSTM implementation in aten/src/THNN/generic/FusedRNNKernel.c\r\nto speedup CPU rnn performance.\r\nOn my test machine, Xeon skylake 8180 (56 core @ 2.5GHz), the performance comparison of this PR is listed in the table below:\r\n(**B**: batch size, **T**: time step, **I**: input size, **H**: hidden size)\r\nthe [benchmark](https://github.com/xhzhao/pytorch-rnn-benchmark) is a pytorch version of baidu's [deep-bench](https://github.com/baidu-research/DeepBench), unit is **SPS** sentences per second, the higher the better.\r\n\r\nB |T |I |H |original (SPS) | fused (SPS)\r\n---|---|---|---|---|---\r\n64 |15 |500 |500 | 876| 2427\r\n64 | 20 | 500 | 500 | 689 | 2140\r\n64 | 25 | 500 | 500 | 479 | 1445\r\n64 | 30 | 500 | 500 | 460 | 1262\r\n64 | 35 | 500 | 500 | 398 | 1308\r\n64 | 40 | 500 | 500 | 350 | 1107\r\n64 | 45 | 500 | 500 | 309 | 1011\r\n64 | 50 | 500 | 500 | 280 | 915\r\n16 | 25 | 512 | 512 | 340 | 657\r\n32 | 25 | 512 | 512 | 416 | 921\r\n64 | 25 | 512 | 512 | 492 | 1236\r\n128 | 25 | 512 | 512 | 582 | 1765\r\n16 | 25 | 1024 | 1024 | 93 | 149\r\n32 | 25 | 1024 | 1024 | 140 | 215\r\n64 | 25 | 1024 | 1024 | 197 | 335\r\n128 | 25 | 1024 | 1024 | 532 | 606\r\n16 | 25 | 2048 | 2048 | 23 | 28\r\n32 | 25 | 2048 | 2048 | 41 | 51\r\n64 | 25 | 2048 | 2048 | 88 | 97\r\n128 | 25 | 2048 | 2048 | 162 | 174\r\n16 | 25 | 4096 | 4096 | 6.7 | 7.0\r\n32| 25| 4096| 4096 | 13.5 | 13.7\r\n64 | 25 | 4096 | 4096 | 24.9 | 25.3\r\n128 | 25 | 4096 | 4096 | 45.1 | 47.8\r\n\r\n@yf225, the [word_language_model](https://github.com/yf225/examples/tree/benchmark_test/word_language_model) performance pretty much doubles on my machine. \r\n\r\nSome environment settings are needed to regulate OpenMP threads behavior so as to achieve better performance,\r\n```bash\r\n## take Xeon for example\r\nexport OMP_NUM_THREADS=[number of physical cores]\r\nexport KMP_AFFINITY=granularity=fine,compact,1,0\r\n```\r\ngenerally, fused rnn kernel receives more benefit when input size / hidden size are small, as gemm will donate in case of large input size / hidden size. By comparing the last four rows,  we can see increasing batch size is almost a free launch.\r\n\r\nHowever, aten/src/THNN/generic/FusedRNNKernel.c is fusing only element wise operation such as sigmoid, add, mul. To achieve more performance speedup, need to further fuse **x*W** gemm accross different time steps, this type of optimization is WIP."}