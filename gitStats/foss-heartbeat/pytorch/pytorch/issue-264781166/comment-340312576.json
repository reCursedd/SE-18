{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340312576", "html_url": "https://github.com/pytorch/pytorch/issues/3082#issuecomment-340312576", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3082", "id": 340312576, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDMxMjU3Ng==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-29T23:38:11Z", "updated_at": "2017-10-29T23:38:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> , I looked into this problem and imho this is not a pytorch bug.<br>\nThe extra context was created by <code>torch.cuda.synchronize()</code> in the provided script.  This can be verified by the following logic:</p>\n<ol>\n<li>I added another <code>torch.cuda.synchronize()</code> before <code>init_process_group()</code>, I saw there were already 2 processes running on GPU 0 before <code>init_process_group()</code>, which tells us that the extra process was not created by <code>init_process_group()</code>.</li>\n<li>I ran a simple script below</li>\n</ol>\n<pre><code>import torch\ntorch.cuda.synchronize()\n</code></pre>\n<p>It creates a context on GPU 0( default) because nothing was on GPU yet, it needs to do initialization.<br>\n3. Trying to explain what happens in your example, <code>net = Net().cuda(opt.rank)</code> only copies all model parameters and buffers to the specified GPU, without <code> torch.cuda.set_device(opt.rank)</code>, when <code>torch.cuda.synchronize()</code> is called, it actually doesn't know its visible device is limited. So it defaults to create a new context on GPU 0.<br>\nOn the other hand, if <code> torch.cuda.set_device(opt.rank)</code> is called, it initializes a context on the specified device, and kind of set a environment variable for cuda to limit the visible device. In this case, <code>torch.cuda.synchronize()</code> will respect what's already set and there is no need to create context any more.<br>\nHope this explains, please correct me if I'm wrong <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> , thanks!</p>", "body_text": "Hi @ngimel , I looked into this problem and imho this is not a pytorch bug.\nThe extra context was created by torch.cuda.synchronize() in the provided script.  This can be verified by the following logic:\n\nI added another torch.cuda.synchronize() before init_process_group(), I saw there were already 2 processes running on GPU 0 before init_process_group(), which tells us that the extra process was not created by init_process_group().\nI ran a simple script below\n\nimport torch\ntorch.cuda.synchronize()\n\nIt creates a context on GPU 0( default) because nothing was on GPU yet, it needs to do initialization.\n3. Trying to explain what happens in your example, net = Net().cuda(opt.rank) only copies all model parameters and buffers to the specified GPU, without  torch.cuda.set_device(opt.rank), when torch.cuda.synchronize() is called, it actually doesn't know its visible device is limited. So it defaults to create a new context on GPU 0.\nOn the other hand, if  torch.cuda.set_device(opt.rank) is called, it initializes a context on the specified device, and kind of set a environment variable for cuda to limit the visible device. In this case, torch.cuda.synchronize() will respect what's already set and there is no need to create context any more.\nHope this explains, please correct me if I'm wrong @apaszke , thanks!", "body": "Hi @ngimel , I looked into this problem and imho this is not a pytorch bug.\r\nThe extra context was created by `torch.cuda.synchronize()` in the provided script.  This can be verified by the following logic:\r\n1. I added another `torch.cuda.synchronize()` before `init_process_group()`, I saw there were already 2 processes running on GPU 0 before `init_process_group()`, which tells us that the extra process was not created by `init_process_group()`.\r\n2. I ran a simple script below\r\n```\r\nimport torch\r\ntorch.cuda.synchronize()\r\n```\r\nIt creates a context on GPU 0( default) because nothing was on GPU yet, it needs to do initialization.\r\n3. Trying to explain what happens in your example, `net = Net().cuda(opt.rank)` only copies all model parameters and buffers to the specified GPU, without ` torch.cuda.set_device(opt.rank)`, when `torch.cuda.synchronize()` is called, it actually doesn't know its visible device is limited. So it defaults to create a new context on GPU 0. \r\nOn the other hand, if ` torch.cuda.set_device(opt.rank)` is called, it initializes a context on the specified device, and kind of set a environment variable for cuda to limit the visible device. In this case, `torch.cuda.synchronize()` will respect what's already set and there is no need to create context any more. \r\nHope this explains, please correct me if I'm wrong @apaszke , thanks! "}