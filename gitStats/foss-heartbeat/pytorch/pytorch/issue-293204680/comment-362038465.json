{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362038465", "html_url": "https://github.com/pytorch/pytorch/issues/4959#issuecomment-362038465", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4959", "id": 362038465, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjAzODQ2NQ==", "user": {"login": "bheinzerling", "id": 4348795, "node_id": "MDQ6VXNlcjQzNDg3OTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/4348795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bheinzerling", "html_url": "https://github.com/bheinzerling", "followers_url": "https://api.github.com/users/bheinzerling/followers", "following_url": "https://api.github.com/users/bheinzerling/following{/other_user}", "gists_url": "https://api.github.com/users/bheinzerling/gists{/gist_id}", "starred_url": "https://api.github.com/users/bheinzerling/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bheinzerling/subscriptions", "organizations_url": "https://api.github.com/users/bheinzerling/orgs", "repos_url": "https://api.github.com/users/bheinzerling/repos", "events_url": "https://api.github.com/users/bheinzerling/events{/privacy}", "received_events_url": "https://api.github.com/users/bheinzerling/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T19:11:54Z", "updated_at": "2018-01-31T22:01:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>DataLoader is very convenient and offers lots of functionality, but in simple single-process cases I found it much faster to not use DataLoader at all. Just tensorize your data and do the batching yourself, something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_iter</span>(<span class=\"pl-smi\">X</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>)\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    X: feature tensor (shape: num_instances x num_features)</span>\n<span class=\"pl-s\">    y: target tensor (shape: num_instances)</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    idxs <span class=\"pl-k\">=</span> torch.randperm(X.size(<span class=\"pl-c1\">0</span>))\n    <span class=\"pl-k\">if</span> X.is_cuda:\n         idxs <span class=\"pl-k\">=</span> idxs.cuda()\n    <span class=\"pl-k\">for</span> batch_idxs <span class=\"pl-k\">in</span> idxs.split(batch_size):\n        <span class=\"pl-k\">yield</span> X[batch_idxs], y[batch_idxs]</pre></div>", "body_text": "DataLoader is very convenient and offers lots of functionality, but in simple single-process cases I found it much faster to not use DataLoader at all. Just tensorize your data and do the batching yourself, something like:\ndef batch_iter(X, y, batch_size=64)\n    \"\"\"\n    X: feature tensor (shape: num_instances x num_features)\n    y: target tensor (shape: num_instances)\n    \"\"\"\n    idxs = torch.randperm(X.size(0))\n    if X.is_cuda:\n         idxs = idxs.cuda()\n    for batch_idxs in idxs.split(batch_size):\n        yield X[batch_idxs], y[batch_idxs]", "body": "DataLoader is very convenient and offers lots of functionality, but in simple single-process cases I found it much faster to not use DataLoader at all. Just tensorize your data and do the batching yourself, something like:\r\n\r\n```Python\r\ndef batch_iter(X, y, batch_size=64)\r\n    \"\"\"\r\n    X: feature tensor (shape: num_instances x num_features)\r\n    y: target tensor (shape: num_instances)\r\n    \"\"\"\r\n    idxs = torch.randperm(X.size(0))\r\n    if X.is_cuda:\r\n         idxs = idxs.cuda()\r\n    for batch_idxs in idxs.split(batch_size):\r\n        yield X[batch_idxs], y[batch_idxs]\r\n```"}