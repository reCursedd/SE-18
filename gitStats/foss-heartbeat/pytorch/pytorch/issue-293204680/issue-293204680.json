{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4959", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4959/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4959/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4959/events", "html_url": "https://github.com/pytorch/pytorch/issues/4959", "id": 293204680, "node_id": "MDU6SXNzdWUyOTMyMDQ2ODA=", "number": 4959, "title": "Speed up data loading for `TensorDataset` if the underlying dataset supports index by a list of indices", "user": {"login": "colinfang", "id": 1499555, "node_id": "MDQ6VXNlcjE0OTk1NTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1499555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colinfang", "html_url": "https://github.com/colinfang", "followers_url": "https://api.github.com/users/colinfang/followers", "following_url": "https://api.github.com/users/colinfang/following{/other_user}", "gists_url": "https://api.github.com/users/colinfang/gists{/gist_id}", "starred_url": "https://api.github.com/users/colinfang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colinfang/subscriptions", "organizations_url": "https://api.github.com/users/colinfang/orgs", "repos_url": "https://api.github.com/users/colinfang/repos", "events_url": "https://api.github.com/users/colinfang/events{/privacy}", "received_events_url": "https://api.github.com/users/colinfang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-31T15:40:23Z", "updated_at": "2018-10-12T18:28:11Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Looking at <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/5b43c22f73279c67084a2357a489420c705cb84f/torch/utils/data/dataloader.py#L259\">pytorch/torch/utils/data/dataloader.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 259\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/5b43c22f73279c67084a2357a489420c705cb84f\">5b43c22</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L259\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"259\"></td>\n          <td id=\"LC259\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.collate_fn([<span class=\"pl-c1\">self</span>.dataset[i] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> indices]) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>The loader fetches one row at a time of the data set, and then combine them into a minibatch. It is quite inefficient if the underlying data set already supports indexing by a list of indices.</p>\n<p>If there are a lot of elements in a row, e.g. image data, it is relatively OK since it takes more time to process (infer + back-propagate in GPU) about the data. However if the feature dimension is small, say &lt; 100, then data loading becomes the bottleneck.</p>\n<p>An alternative is to do the following</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> batch = self.collate_fn([self.dataset[i] for i in indices])</span>\nbatch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dataset[indices]</pre></div>\n<p>I applied monkey patch for my specific problem.</p>\n<pre><code>def data_loader_next(self):\n    if self.num_workers == 0:  # same-process loading\n        indices = next(self.sample_iter)  # may raise StopIteration\n        # I know that my dataset supports index by indices.\n        # -- batch = self.collate_fn([self.dataset[i] for i in indices])\n        batch = self.dataset[indices]\n        if self.pin_memory:\n            batch = pin_memory_batch(batch)\n        return batch\n\n    # check if the next sample has already been generated\n    if self.rcvd_idx in self.reorder_dict:\n        batch = self.reorder_dict.pop(self.rcvd_idx)\n        return self._process_next_batch(batch)\n\n    if self.batches_outstanding == 0:\n        self._shutdown_workers()\n        raise StopIteration\n\n    while True:\n        assert (not self.shutdown and self.batches_outstanding &gt; 0)\n        idx, batch = self.data_queue.get()\n        self.batches_outstanding -= 1\n        if idx != self.rcvd_idx:\n            # store out-of-order samples\n            self.reorder_dict[idx] = batch\n            continue\n        return self._process_next_batch(batch)\n\nDataLoaderIter.next = data_loader_next\nDataLoaderIter.__next__ = data_loader_next\n</code></pre>\n<p>It speeds up the data loading by 5 times while <code>using num_workers=0</code>.</p>", "body_text": "Looking at \n  \n    \n      pytorch/torch/utils/data/dataloader.py\n    \n    \n         Line 259\n      in\n      5b43c22\n    \n    \n    \n    \n\n        \n          \n           batch = self.collate_fn([self.dataset[i] for i in indices]) \n        \n    \n  \n\n\nThe loader fetches one row at a time of the data set, and then combine them into a minibatch. It is quite inefficient if the underlying data set already supports indexing by a list of indices.\nIf there are a lot of elements in a row, e.g. image data, it is relatively OK since it takes more time to process (infer + back-propagate in GPU) about the data. However if the feature dimension is small, say < 100, then data loading becomes the bottleneck.\nAn alternative is to do the following\n# batch = self.collate_fn([self.dataset[i] for i in indices])\nbatch = self.dataset[indices]\nI applied monkey patch for my specific problem.\ndef data_loader_next(self):\n    if self.num_workers == 0:  # same-process loading\n        indices = next(self.sample_iter)  # may raise StopIteration\n        # I know that my dataset supports index by indices.\n        # -- batch = self.collate_fn([self.dataset[i] for i in indices])\n        batch = self.dataset[indices]\n        if self.pin_memory:\n            batch = pin_memory_batch(batch)\n        return batch\n\n    # check if the next sample has already been generated\n    if self.rcvd_idx in self.reorder_dict:\n        batch = self.reorder_dict.pop(self.rcvd_idx)\n        return self._process_next_batch(batch)\n\n    if self.batches_outstanding == 0:\n        self._shutdown_workers()\n        raise StopIteration\n\n    while True:\n        assert (not self.shutdown and self.batches_outstanding > 0)\n        idx, batch = self.data_queue.get()\n        self.batches_outstanding -= 1\n        if idx != self.rcvd_idx:\n            # store out-of-order samples\n            self.reorder_dict[idx] = batch\n            continue\n        return self._process_next_batch(batch)\n\nDataLoaderIter.next = data_loader_next\nDataLoaderIter.__next__ = data_loader_next\n\nIt speeds up the data loading by 5 times while using num_workers=0.", "body": "Looking at https://github.com/pytorch/pytorch/blob/5b43c22f73279c67084a2357a489420c705cb84f/torch/utils/data/dataloader.py#L259\r\n\r\nThe loader fetches one row at a time of the data set, and then combine them into a minibatch. It is quite inefficient if the underlying data set already supports indexing by a list of indices.\r\n\r\nIf there are a lot of elements in a row, e.g. image data, it is relatively OK since it takes more time to process (infer + back-propagate in GPU) about the data. However if the feature dimension is small, say < 100, then data loading becomes the bottleneck.\r\n\r\nAn alternative is to do the following\r\n\r\n```python\r\n# batch = self.collate_fn([self.dataset[i] for i in indices])\r\nbatch = self.dataset[indices]\r\n```\r\n\r\nI applied monkey patch for my specific problem.\r\n\r\n```\r\ndef data_loader_next(self):\r\n    if self.num_workers == 0:  # same-process loading\r\n        indices = next(self.sample_iter)  # may raise StopIteration\r\n        # I know that my dataset supports index by indices.\r\n        # -- batch = self.collate_fn([self.dataset[i] for i in indices])\r\n        batch = self.dataset[indices]\r\n        if self.pin_memory:\r\n            batch = pin_memory_batch(batch)\r\n        return batch\r\n\r\n    # check if the next sample has already been generated\r\n    if self.rcvd_idx in self.reorder_dict:\r\n        batch = self.reorder_dict.pop(self.rcvd_idx)\r\n        return self._process_next_batch(batch)\r\n\r\n    if self.batches_outstanding == 0:\r\n        self._shutdown_workers()\r\n        raise StopIteration\r\n\r\n    while True:\r\n        assert (not self.shutdown and self.batches_outstanding > 0)\r\n        idx, batch = self.data_queue.get()\r\n        self.batches_outstanding -= 1\r\n        if idx != self.rcvd_idx:\r\n            # store out-of-order samples\r\n            self.reorder_dict[idx] = batch\r\n            continue\r\n        return self._process_next_batch(batch)\r\n\r\nDataLoaderIter.next = data_loader_next\r\nDataLoaderIter.__next__ = data_loader_next\r\n```\r\n\r\nIt speeds up the data loading by 5 times while `using num_workers=0`.\r\n\r\n"}