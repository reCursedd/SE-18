{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/306453313", "html_url": "https://github.com/pytorch/pytorch/issues/1736#issuecomment-306453313", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1736", "id": 306453313, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjQ1MzMxMw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-06T11:04:22Z", "updated_at": "2017-06-06T11:04:22Z", "author_association": "MEMBER", "body_html": "<p>This is because optimizer can have multiple parameters groups with different options, so there's no such thing as an optimizer-wide <code>lr</code>. You can always iterate over <code>optimizer.param_groups</code> and each <code>dict</code> will have an <code>lr</code> parameter that you can modify.</p>\n<p>In fact maybe we could expose these values as attributes when there is only a single param group, as it's not ambiguous in this case.</p>", "body_text": "This is because optimizer can have multiple parameters groups with different options, so there's no such thing as an optimizer-wide lr. You can always iterate over optimizer.param_groups and each dict will have an lr parameter that you can modify.\nIn fact maybe we could expose these values as attributes when there is only a single param group, as it's not ambiguous in this case.", "body": "This is because optimizer can have multiple parameters groups with different options, so there's no such thing as an optimizer-wide `lr`. You can always iterate over `optimizer.param_groups` and each `dict` will have an `lr` parameter that you can modify.\r\n\r\nIn fact maybe we could expose these values as attributes when there is only a single param group, as it's not ambiguous in this case."}