{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7702", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7702/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7702/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7702/events", "html_url": "https://github.com/pytorch/pytorch/issues/7702", "id": 324631070, "node_id": "MDU6SXNzdWUzMjQ2MzEwNzA=", "number": 7702, "title": "[pytorch][bug] memory leaking for gloo backend when `all_reduce` CPU tensor", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-05-19T13:48:52Z", "updated_at": "2018-11-14T22:41:08Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>The <code>dist.all_reduce</code> caused memory leaking when using gloo backend for CPU memory.<br>\nobservation:</p>\n<ol>\n<li>both <code>tcp</code> and <code>gloo</code> backends are tested, <code>tcp</code> is fine.</li>\n<li>if <code>index</code> is of fixed size, it's fine.</li>\n<li>other collective communication methods are not tested yet.</li>\n<li><code>reduce</code> tensors on GPU memory seems fine.</li>\n<li>the memory usage increases linearly with the tensor size.</li>\n</ol>\n<p>I think the leaking may happen in the buffer management of gloo backend.</p>\n<p>reproducing snippets:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test.py</span>\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">print_mem</span>(<span class=\"pl-smi\">extra_str</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>):\n    <span class=\"pl-k\">if</span> dist.get_rank() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">import</span> os\n        <span class=\"pl-k\">import</span> psutil\n        process <span class=\"pl-k\">=</span> psutil.Process(os.getpid())\n        <span class=\"pl-c1\">print</span>(extra_str, process.memory_info().rss <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">20</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>MB<span class=\"pl-pds\">'</span></span>)\n\ndist.init_process_group(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>gloo<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>file:///tmp/shared_file<span class=\"pl-pds\">'</span></span>)\nembedding <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">8000</span>, <span class=\"pl-c1\">200</span>)\n<span class=\"pl-k\">for</span> epo <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n    index <span class=\"pl-k\">=</span> torch.randint(embedding.size(<span class=\"pl-c1\">0</span>), (<span class=\"pl-c1\">30000</span> <span class=\"pl-k\">-</span> epo,)).long()\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ten = torch.randn(numel)</span>\n    ten <span class=\"pl-k\">=</span> embedding[index]\n    print_mem(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before reducing<span class=\"pl-pds\">'</span></span>)\n    dist.all_reduce(ten, <span class=\"pl-v\">op</span><span class=\"pl-k\">=</span>dist.reduce_op.<span class=\"pl-c1\">SUM</span>)\n    print_mem(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>after reducing<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>how to run:</p>\n<div class=\"highlight highlight-source-shell\"><pre>pip install psutil\npython test.py <span class=\"pl-k\">&amp;</span> python test.py</pre></div>\n<p>Envs</p>\n<pre><code>PyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5\nCMake version: version 3.6.3\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\n\nNvidia driver version: 375.26\ncuDNN version: Probably one of the following:\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.1)\n[pip3] torch (0.4.0)\n[conda] magma-cuda80              2.1.0                         5    soumith\n[conda] torch-0.1.12              2                         &lt;pip&gt;\n</code></pre>", "body_text": "The dist.all_reduce caused memory leaking when using gloo backend for CPU memory.\nobservation:\n\nboth tcp and gloo backends are tested, tcp is fine.\nif index is of fixed size, it's fine.\nother collective communication methods are not tested yet.\nreduce tensors on GPU memory seems fine.\nthe memory usage increases linearly with the tensor size.\n\nI think the leaking may happen in the buffer management of gloo backend.\nreproducing snippets:\n# test.py\nimport time\nimport random\n\nimport torch\nimport torch.distributed as dist\ndef print_mem(extra_str=''):\n    if dist.get_rank() == 0:\n        import os\n        import psutil\n        process = psutil.Process(os.getpid())\n        print(extra_str, process.memory_info().rss // 2**20, 'MB')\n\ndist.init_process_group('gloo', world_size=2, init_method='file:///tmp/shared_file')\nembedding = torch.randn(8000, 200)\nfor epo in range(1000):\n    index = torch.randint(embedding.size(0), (30000 - epo,)).long()\n    # ten = torch.randn(numel)\n    ten = embedding[index]\n    print_mem('before reducing')\n    dist.all_reduce(ten, op=dist.reduce_op.SUM)\n    print_mem('after reducing')\nhow to run:\npip install psutil\npython test.py & python test.py\nEnvs\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5\nCMake version: version 3.6.3\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\n\nNvidia driver version: 375.26\ncuDNN version: Probably one of the following:\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.1)\n[pip3] torch (0.4.0)\n[conda] magma-cuda80              2.1.0                         5    soumith\n[conda] torch-0.1.12              2                         <pip>", "body": "The `dist.all_reduce` caused memory leaking when using gloo backend for CPU memory.\r\nobservation:\r\n1. both `tcp` and `gloo` backends are tested, `tcp` is fine.\r\n2. if `index` is of fixed size, it's fine.\r\n3. other collective communication methods are not tested yet.\r\n4. `reduce` tensors on GPU memory seems fine.\r\n5. the memory usage increases linearly with the tensor size.\r\n\r\nI think the leaking may happen in the buffer management of gloo backend.\r\n\r\nreproducing snippets:\r\n```python\r\n# test.py\r\nimport time\r\nimport random\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\ndef print_mem(extra_str=''):\r\n    if dist.get_rank() == 0:\r\n        import os\r\n        import psutil\r\n        process = psutil.Process(os.getpid())\r\n        print(extra_str, process.memory_info().rss // 2**20, 'MB')\r\n\r\ndist.init_process_group('gloo', world_size=2, init_method='file:///tmp/shared_file')\r\nembedding = torch.randn(8000, 200)\r\nfor epo in range(1000):\r\n    index = torch.randint(embedding.size(0), (30000 - epo,)).long()\r\n    # ten = torch.randn(numel)\r\n    ten = embedding[index]\r\n    print_mem('before reducing')\r\n    dist.all_reduce(ten, op=dist.reduce_op.SUM)\r\n    print_mem('after reducing')\r\n```\r\n\r\nhow to run:\r\n```bash\r\npip install psutil\r\npython test.py & python test.py\r\n```\r\n\r\n\r\nEnvs\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5\r\nCMake version: version 3.6.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 375.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.1)\r\n[pip3] torch (0.4.0)\r\n[conda] magma-cuda80              2.1.0                         5    soumith\r\n[conda] torch-0.1.12              2                         <pip>\r\n```"}