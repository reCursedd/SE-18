{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1481", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1481/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1481/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1481/events", "html_url": "https://github.com/pytorch/pytorch/pull/1481", "id": 226491150, "node_id": "MDExOlB1bGxSZXF1ZXN0MTE5MTE5Njg3", "number": 1481, "title": "Fix bug in magma qr decomposition", "user": {"login": "aam-at", "id": 486336, "node_id": "MDQ6VXNlcjQ4NjMzNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/486336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aam-at", "html_url": "https://github.com/aam-at", "followers_url": "https://api.github.com/users/aam-at/followers", "following_url": "https://api.github.com/users/aam-at/following{/other_user}", "gists_url": "https://api.github.com/users/aam-at/gists{/gist_id}", "starred_url": "https://api.github.com/users/aam-at/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aam-at/subscriptions", "organizations_url": "https://api.github.com/users/aam-at/orgs", "repos_url": "https://api.github.com/users/aam-at/repos", "events_url": "https://api.github.com/users/aam-at/events{/privacy}", "received_events_url": "https://api.github.com/users/aam-at/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-05-05T07:11:32Z", "updated_at": "2018-11-23T15:33:21Z", "closed_at": "2017-05-08T23:59:31Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1481", "html_url": "https://github.com/pytorch/pytorch/pull/1481", "diff_url": "https://github.com/pytorch/pytorch/pull/1481.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1481.patch"}, "body_html": "<p><a href=\"http://icl.cs.utk.edu/magma/forum/viewtopic.php?f=2&amp;t=1015&amp;p=2800&amp;hilit=geqrf_gpu#p2800\" rel=\"nofollow\">Documentation</a> for <code>geqrf_gpu</code> is incorrect. Tests pass because for smaller matrices <code>magma</code> falls back on <code>lapack</code> for qr factorization. For large matrices, returned <code>r</code> is incorrect. Bug can be reproduced using the following script (<code>r</code> will contains matrix with ones on diagonal):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).cuda()\nq, r <span class=\"pl-k\">=</span> torch.qr(a)</pre></div>", "body_text": "Documentation for geqrf_gpu is incorrect. Tests pass because for smaller matrices magma falls back on lapack for qr factorization. For large matrices, returned r is incorrect. Bug can be reproduced using the following script (r will contains matrix with ones on diagonal):\nimport torch\na = torch.rand(1000, 1000).cuda()\nq, r = torch.qr(a)", "body": "[Documentation](http://icl.cs.utk.edu/magma/forum/viewtopic.php?f=2&t=1015&p=2800&hilit=geqrf_gpu#p2800) for `geqrf_gpu` is incorrect. Tests pass because for smaller matrices `magma` falls back on `lapack` for qr factorization. For large matrices, returned `r` is incorrect. Bug can be reproduced using the following script (`r` will contains matrix with ones on diagonal):\r\n```python\r\nimport torch\r\na = torch.rand(1000, 1000).cuda()\r\nq, r = torch.qr(a)\r\n```"}