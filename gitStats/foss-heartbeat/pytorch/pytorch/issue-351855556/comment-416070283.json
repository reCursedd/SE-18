{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/416070283", "html_url": "https://github.com/pytorch/pytorch/issues/10661#issuecomment-416070283", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10661", "id": 416070283, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjA3MDI4Mw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-26T20:38:19Z", "updated_at": "2018-08-26T20:38:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So at first sight, we seem to inherit a lot of slowdown from bmm.<br>\nI get</p>\n<pre><code>0.0\ntorch: 5.314521988999331\nnumpy: 0.005557203001444577\ntorch.bmm: 5.034999315999812\n</code></pre>\n<p>from the script below.<br>\nQuite likely, bmm doesn't like large batches of small multiplications. When replacing \"2\" by \"200\" as the smaller dimensions, there is a single digit factor between the speeds (which still is rather large).<br>\nIf possible, I'd like to fix this in bmm, but I haven't tracked down the root cause yet.</p>\n<pre><code>import torch, numpy, timeit\nx = torch.randn((2, 2000))\ny = torch.randn((2, 2, 2000))\nxp = x.permute(1, 0).view(2000, 1, 2)\nyp = y.permute(2, 0, 1)\nequation = 'ac,abc-&gt;cb'\n\ntime0 = timeit.default_timer()\nfor _ in range(1000):\n    _ = torch.einsum(equation, [x, y])\n\ntime1 = timeit.default_timer()\nfor _ in range(1000):\n    _ = numpy.einsum(equation, x.numpy(), y.numpy())\n\ntime2 = timeit.default_timer()\n\nfor _ in range(1000):\n    _ = torch.bmm(xp, yp)\ntime3 = timeit.default_timer()\n\n# check that we calculate the right thing.\nprint((torch.einsum(equation, [x, y])-torch.bmm(xp, yp).squeeze()).abs().max().item())\n\nprint('torch: {}'.format(time1 - time0))\nprint('numpy: {}'.format(time2 - time1))\nprint('torch.bmm: {}'.format(time3 - time2))\n</code></pre>", "body_text": "So at first sight, we seem to inherit a lot of slowdown from bmm.\nI get\n0.0\ntorch: 5.314521988999331\nnumpy: 0.005557203001444577\ntorch.bmm: 5.034999315999812\n\nfrom the script below.\nQuite likely, bmm doesn't like large batches of small multiplications. When replacing \"2\" by \"200\" as the smaller dimensions, there is a single digit factor between the speeds (which still is rather large).\nIf possible, I'd like to fix this in bmm, but I haven't tracked down the root cause yet.\nimport torch, numpy, timeit\nx = torch.randn((2, 2000))\ny = torch.randn((2, 2, 2000))\nxp = x.permute(1, 0).view(2000, 1, 2)\nyp = y.permute(2, 0, 1)\nequation = 'ac,abc->cb'\n\ntime0 = timeit.default_timer()\nfor _ in range(1000):\n    _ = torch.einsum(equation, [x, y])\n\ntime1 = timeit.default_timer()\nfor _ in range(1000):\n    _ = numpy.einsum(equation, x.numpy(), y.numpy())\n\ntime2 = timeit.default_timer()\n\nfor _ in range(1000):\n    _ = torch.bmm(xp, yp)\ntime3 = timeit.default_timer()\n\n# check that we calculate the right thing.\nprint((torch.einsum(equation, [x, y])-torch.bmm(xp, yp).squeeze()).abs().max().item())\n\nprint('torch: {}'.format(time1 - time0))\nprint('numpy: {}'.format(time2 - time1))\nprint('torch.bmm: {}'.format(time3 - time2))", "body": "So at first sight, we seem to inherit a lot of slowdown from bmm.\r\nI get\r\n```\r\n0.0\r\ntorch: 5.314521988999331\r\nnumpy: 0.005557203001444577\r\ntorch.bmm: 5.034999315999812\r\n```\r\nfrom the script below.\r\nQuite likely, bmm doesn't like large batches of small multiplications. When replacing \"2\" by \"200\" as the smaller dimensions, there is a single digit factor between the speeds (which still is rather large).\r\nIf possible, I'd like to fix this in bmm, but I haven't tracked down the root cause yet.\r\n\r\n```\r\nimport torch, numpy, timeit\r\nx = torch.randn((2, 2000))\r\ny = torch.randn((2, 2, 2000))\r\nxp = x.permute(1, 0).view(2000, 1, 2)\r\nyp = y.permute(2, 0, 1)\r\nequation = 'ac,abc->cb'\r\n\r\ntime0 = timeit.default_timer()\r\nfor _ in range(1000):\r\n    _ = torch.einsum(equation, [x, y])\r\n\r\ntime1 = timeit.default_timer()\r\nfor _ in range(1000):\r\n    _ = numpy.einsum(equation, x.numpy(), y.numpy())\r\n\r\ntime2 = timeit.default_timer()\r\n\r\nfor _ in range(1000):\r\n    _ = torch.bmm(xp, yp)\r\ntime3 = timeit.default_timer()\r\n\r\n# check that we calculate the right thing.\r\nprint((torch.einsum(equation, [x, y])-torch.bmm(xp, yp).squeeze()).abs().max().item())\r\n\r\nprint('torch: {}'.format(time1 - time0))\r\nprint('numpy: {}'.format(time2 - time1))\r\nprint('torch.bmm: {}'.format(time3 - time2))\r\n```\r\n"}