{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/416857107", "html_url": "https://github.com/pytorch/pytorch/issues/10661#issuecomment-416857107", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10661", "id": 416857107, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjg1NzEwNw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-29T07:42:19Z", "updated_at": "2018-08-29T07:42:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So the inefficiency comes from <code>bmm</code> on CPU iterating over the batch dimension and relegating to the matrix multiplication.<br>\nI have a tentative CPU fix that just spells out the bmm contraction (currently for <code>batch &gt; 10 * contraction size</code>, but I would need to investigate the tradeoff) and parallelizes over the batch dimension.</p>\n<p>The GPU bmm uses cublas' batch gemm (if available / applicable) so I would leave that alone for the moment unless we identify inferior performance there, too. I don't know how it compares e.g. to magma.</p>\n<p>Fun facts:</p>\n<ul>\n<li>Numpy's <code>bmm</code> actually delegates to numpy's einsum which has all sorts of contraction operations for various setups.</li>\n<li>MKL seems to have a batch gemm, one might use that eventually.</li>\n</ul>", "body_text": "So the inefficiency comes from bmm on CPU iterating over the batch dimension and relegating to the matrix multiplication.\nI have a tentative CPU fix that just spells out the bmm contraction (currently for batch > 10 * contraction size, but I would need to investigate the tradeoff) and parallelizes over the batch dimension.\nThe GPU bmm uses cublas' batch gemm (if available / applicable) so I would leave that alone for the moment unless we identify inferior performance there, too. I don't know how it compares e.g. to magma.\nFun facts:\n\nNumpy's bmm actually delegates to numpy's einsum which has all sorts of contraction operations for various setups.\nMKL seems to have a batch gemm, one might use that eventually.", "body": "So the inefficiency comes from `bmm` on CPU iterating over the batch dimension and relegating to the matrix multiplication.\r\nI have a tentative CPU fix that just spells out the bmm contraction (currently for `batch > 10 * contraction size`, but I would need to investigate the tradeoff) and parallelizes over the batch dimension.\r\n\r\nThe GPU bmm uses cublas' batch gemm (if available / applicable) so I would leave that alone for the moment unless we identify inferior performance there, too. I don't know how it compares e.g. to magma.\r\n\r\nFun facts:\r\n- Numpy's `bmm` actually delegates to numpy's einsum which has all sorts of contraction operations for various setups. \r\n- MKL seems to have a batch gemm, one might use that eventually.\r\n"}