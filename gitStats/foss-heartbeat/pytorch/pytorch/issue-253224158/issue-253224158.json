{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2550", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2550/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2550/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2550/events", "html_url": "https://github.com/pytorch/pytorch/issues/2550", "id": 253224158, "node_id": "MDU6SXNzdWUyNTMyMjQxNTg=", "number": 2550, "title": "5D constant padding", "user": {"login": "sbhaaf", "id": 19878077, "node_id": "MDQ6VXNlcjE5ODc4MDc3", "avatar_url": "https://avatars2.githubusercontent.com/u/19878077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbhaaf", "html_url": "https://github.com/sbhaaf", "followers_url": "https://api.github.com/users/sbhaaf/followers", "following_url": "https://api.github.com/users/sbhaaf/following{/other_user}", "gists_url": "https://api.github.com/users/sbhaaf/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbhaaf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbhaaf/subscriptions", "organizations_url": "https://api.github.com/users/sbhaaf/orgs", "repos_url": "https://api.github.com/users/sbhaaf/repos", "events_url": "https://api.github.com/users/sbhaaf/events{/privacy}", "received_events_url": "https://api.github.com/users/sbhaaf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-28T04:32:17Z", "updated_at": "2017-09-20T17:36:37Z", "closed_at": "2017-09-20T17:36:37Z", "author_association": "NONE", "body_html": "<p>Hey, I'm not sure how to make a push request, but here is the code for 5D padding.</p>\n<p><code>/torch/nn/_functions</code> should get the following class:</p>\n<pre><code>class ConstantPad3d(Function):\n\n    @staticmethod\n    def forward(ctx, input, pad, value=0):\n        assert input.dim() == 5, 'only 5D supported for padding'\n        ctx.pad = pad\n        ctx.value = value\n        pad_lt, pad_rt, pad_tp, pad_bm, pad_ft, pad_bk = ctx.pad\n        d = input.size(2) + pad_ft + pad_bk\n        h = input.size(3) + pad_tp + pad_bm\n        w = input.size(4) + pad_lt + pad_rt\n        assert w &gt; 0 and h &gt; 0 and d &gt; 0, 'input is too small'\n        ctx.input_size = input.size()\n\n        # crop input if necessary\n        output = input.new(input.size(0), input.size(1), d, h, w).fill_(ctx.value)\n        c_input = input\n        if pad_ft &lt; 0:\n            c_input = c_input.narrow(2, -pad_ft, c_input.size(2) + pad_ft)\n        if pad_bk &lt; 0:\n            c_input = c_input.narrow(2, 0, c_input.size(2) + pad_bk)\n        if pad_tp &lt; 0:\n            c_input = c_input.narrow(3, -pad_tp, c_input.size(3) + pad_tp)\n        if pad_bm &lt; 0:\n            c_input = c_input.narrow(3, 0, c_input.size(3) + pad_bm)\n        if pad_lt &lt; 0:\n            c_input = c_input.narrow(4, -pad_lt, c_input.size(4) + pad_lt)\n        if pad_rt &lt; 0:\n            c_input = c_input.narrow(4, 0, c_input.size(4) + pad_rt)\n\n        # crop output if necessary\n        c_output = output\n        if pad_ft &gt; 0:\n            c_output = c_output.narrow(2, pad_ft, c_output.size(2) - pad_ft)\n        if pad_bk &gt; 0:\n            c_output = c_output.narrow(2, 0, c_output.size(2) - pad_bk)\n        if pad_tp &gt; 0:\n            c_output = c_output.narrow(3, pad_tp, c_output.size(3) - pad_tp)\n        if pad_bm &gt; 0:\n            c_output = c_output.narrow(3, 0, c_output.size(3) - pad_bm)\n        if pad_lt &gt; 0:\n            c_output = c_output.narrow(4, pad_lt, c_output.size(4) - pad_lt)\n        if pad_rt &gt; 0:\n            c_output = c_output.narrow(4, 0, c_output.size(4) - pad_rt)\n        c_output.copy_(c_input)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        pad_lt, pad_rt, pad_tp, pad_bm, pad_ft, pad_bk = ctx.pad\n\n        grad_input = Variable(grad_output.data.new(ctx.input_size).zero_())\n        grad_input_slices = [slice(0, x,) for x in ctx.input_size]\n\n        def narrow_slice(dim, start, length):\n            grad_input_slices[dim] = (slice(grad_input_slices[dim].start + start,\n                                            grad_input_slices[dim].start + start + length))\n\n        def slice_length(dim):\n            return grad_input_slices[dim].stop - grad_input_slices[dim].start\n\n        #  crop grad_input if necessary\n        if pad_ft &lt; 0:\n            narrow_slice(2, -pad_ft, slice_length(2) + pad_ft)\n        if pad_bk &lt; 0:\n            narrow_slice(2, 0, slice_length(2) + pad_bk)\n        if pad_tp &lt; 0:\n            narrow_slice(3, -pad_tp, slice_length(3) + pad_tp)\n        if pad_bm &lt; 0:\n            narrow_slice(3, 0, slice_length(3) + pad_bm)\n        if pad_lt &lt; 0:\n            narrow_slice(4, -pad_lt, slice_length(4) + pad_lt)\n        if pad_rt &lt; 0:\n            narrow_slice(4, 0, slice_length(4) + pad_rt)\n\n        # crop grad_output if necessary\n        cg_output = grad_output\n        if pad_ft &gt; 0:\n            cg_output = cg_output.narrow(2, pad_ft, cg_output.size(2) - pad_ft)\n        if pad_bk &gt; 0:\n            cg_output = cg_output.narrow(2, 0, cg_output.size(2) - pad_bk)\n        if pad_tp &gt; 0:\n            cg_output = cg_output.narrow(3, pad_tp, cg_output.size(3) - pad_tp)\n        if pad_bm &gt; 0:\n            cg_output = cg_output.narrow(3, 0, cg_output.size(3) - pad_bm)\n        if pad_lt &gt; 0:\n            cg_output = cg_output.narrow(4, pad_lt, cg_output.size(4) - pad_lt)\n        if pad_rt &gt; 0:\n            cg_output = cg_output.narrow(4, 0, cg_output.size(4) - pad_rt)\n        gis = tuple(grad_input_slices)\n        grad_input[gis] = cg_output\n\n        return grad_input, None, None\n</code></pre>\n<p>and <code>torch/nn/functional.py</code> should be updated with:</p>\n<pre><code>from ._functions.padding import ConstantPad2d, ConstantPad3d\n\n...\n\ndef pad(input, pad, mode='constant', value=0):\n    \"\"\"Pads tensor.\n    Currently only 2D and 3D padding supported.\n    In case of 4D input tensor pad should be in form\n    (pad_l, pad_r, pad_t, pad_b ).\n    In case of 5D pad should be (pleft, pright, ptop, pbottom, pfront, pback)\n    Args:\n        input (Variable): 4D or 5D tensor\n        pad (tuple): 4-elem or 6-elem tuple\n        mode: 'constant', 'reflect' or 'replicate'. Default: 'constant'\n        value: fill value for 'constant' padding. Default: 0\n    \"\"\"\n    if input.dim() == 4:\n        assert len(pad) == 4, '4D tensors expect 4 values for padding'\n        if mode == 'constant':\n            return ConstantPad2d.apply(input, pad, value)\n        elif mode == 'reflect':\n            return _functions.thnn.ReflectionPad2d.apply(input, *pad)\n        elif mode == 'replicate':\n            return _functions.thnn.ReplicationPad2d.apply(input, *pad)\n    elif input.dim() == 5:\n        assert len(pad) == 6, '5D tensors expect 6 values for padding'\n        if mode == 'constant':\n            return ConstantPad3d.apply(input, pad, value)\n        elif mode == 'reflect':\n            raise NotImplementedError\n        elif mode == 'replicate':\n            return _functions.thnn.ReplicationPad3d.apply(input, *pad)\n    else:\n        raise NotImplementedError(\"Only 4D and 5D padding is supported for now\")\n</code></pre>", "body_text": "Hey, I'm not sure how to make a push request, but here is the code for 5D padding.\n/torch/nn/_functions should get the following class:\nclass ConstantPad3d(Function):\n\n    @staticmethod\n    def forward(ctx, input, pad, value=0):\n        assert input.dim() == 5, 'only 5D supported for padding'\n        ctx.pad = pad\n        ctx.value = value\n        pad_lt, pad_rt, pad_tp, pad_bm, pad_ft, pad_bk = ctx.pad\n        d = input.size(2) + pad_ft + pad_bk\n        h = input.size(3) + pad_tp + pad_bm\n        w = input.size(4) + pad_lt + pad_rt\n        assert w > 0 and h > 0 and d > 0, 'input is too small'\n        ctx.input_size = input.size()\n\n        # crop input if necessary\n        output = input.new(input.size(0), input.size(1), d, h, w).fill_(ctx.value)\n        c_input = input\n        if pad_ft < 0:\n            c_input = c_input.narrow(2, -pad_ft, c_input.size(2) + pad_ft)\n        if pad_bk < 0:\n            c_input = c_input.narrow(2, 0, c_input.size(2) + pad_bk)\n        if pad_tp < 0:\n            c_input = c_input.narrow(3, -pad_tp, c_input.size(3) + pad_tp)\n        if pad_bm < 0:\n            c_input = c_input.narrow(3, 0, c_input.size(3) + pad_bm)\n        if pad_lt < 0:\n            c_input = c_input.narrow(4, -pad_lt, c_input.size(4) + pad_lt)\n        if pad_rt < 0:\n            c_input = c_input.narrow(4, 0, c_input.size(4) + pad_rt)\n\n        # crop output if necessary\n        c_output = output\n        if pad_ft > 0:\n            c_output = c_output.narrow(2, pad_ft, c_output.size(2) - pad_ft)\n        if pad_bk > 0:\n            c_output = c_output.narrow(2, 0, c_output.size(2) - pad_bk)\n        if pad_tp > 0:\n            c_output = c_output.narrow(3, pad_tp, c_output.size(3) - pad_tp)\n        if pad_bm > 0:\n            c_output = c_output.narrow(3, 0, c_output.size(3) - pad_bm)\n        if pad_lt > 0:\n            c_output = c_output.narrow(4, pad_lt, c_output.size(4) - pad_lt)\n        if pad_rt > 0:\n            c_output = c_output.narrow(4, 0, c_output.size(4) - pad_rt)\n        c_output.copy_(c_input)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        pad_lt, pad_rt, pad_tp, pad_bm, pad_ft, pad_bk = ctx.pad\n\n        grad_input = Variable(grad_output.data.new(ctx.input_size).zero_())\n        grad_input_slices = [slice(0, x,) for x in ctx.input_size]\n\n        def narrow_slice(dim, start, length):\n            grad_input_slices[dim] = (slice(grad_input_slices[dim].start + start,\n                                            grad_input_slices[dim].start + start + length))\n\n        def slice_length(dim):\n            return grad_input_slices[dim].stop - grad_input_slices[dim].start\n\n        #  crop grad_input if necessary\n        if pad_ft < 0:\n            narrow_slice(2, -pad_ft, slice_length(2) + pad_ft)\n        if pad_bk < 0:\n            narrow_slice(2, 0, slice_length(2) + pad_bk)\n        if pad_tp < 0:\n            narrow_slice(3, -pad_tp, slice_length(3) + pad_tp)\n        if pad_bm < 0:\n            narrow_slice(3, 0, slice_length(3) + pad_bm)\n        if pad_lt < 0:\n            narrow_slice(4, -pad_lt, slice_length(4) + pad_lt)\n        if pad_rt < 0:\n            narrow_slice(4, 0, slice_length(4) + pad_rt)\n\n        # crop grad_output if necessary\n        cg_output = grad_output\n        if pad_ft > 0:\n            cg_output = cg_output.narrow(2, pad_ft, cg_output.size(2) - pad_ft)\n        if pad_bk > 0:\n            cg_output = cg_output.narrow(2, 0, cg_output.size(2) - pad_bk)\n        if pad_tp > 0:\n            cg_output = cg_output.narrow(3, pad_tp, cg_output.size(3) - pad_tp)\n        if pad_bm > 0:\n            cg_output = cg_output.narrow(3, 0, cg_output.size(3) - pad_bm)\n        if pad_lt > 0:\n            cg_output = cg_output.narrow(4, pad_lt, cg_output.size(4) - pad_lt)\n        if pad_rt > 0:\n            cg_output = cg_output.narrow(4, 0, cg_output.size(4) - pad_rt)\n        gis = tuple(grad_input_slices)\n        grad_input[gis] = cg_output\n\n        return grad_input, None, None\n\nand torch/nn/functional.py should be updated with:\nfrom ._functions.padding import ConstantPad2d, ConstantPad3d\n\n...\n\ndef pad(input, pad, mode='constant', value=0):\n    \"\"\"Pads tensor.\n    Currently only 2D and 3D padding supported.\n    In case of 4D input tensor pad should be in form\n    (pad_l, pad_r, pad_t, pad_b ).\n    In case of 5D pad should be (pleft, pright, ptop, pbottom, pfront, pback)\n    Args:\n        input (Variable): 4D or 5D tensor\n        pad (tuple): 4-elem or 6-elem tuple\n        mode: 'constant', 'reflect' or 'replicate'. Default: 'constant'\n        value: fill value for 'constant' padding. Default: 0\n    \"\"\"\n    if input.dim() == 4:\n        assert len(pad) == 4, '4D tensors expect 4 values for padding'\n        if mode == 'constant':\n            return ConstantPad2d.apply(input, pad, value)\n        elif mode == 'reflect':\n            return _functions.thnn.ReflectionPad2d.apply(input, *pad)\n        elif mode == 'replicate':\n            return _functions.thnn.ReplicationPad2d.apply(input, *pad)\n    elif input.dim() == 5:\n        assert len(pad) == 6, '5D tensors expect 6 values for padding'\n        if mode == 'constant':\n            return ConstantPad3d.apply(input, pad, value)\n        elif mode == 'reflect':\n            raise NotImplementedError\n        elif mode == 'replicate':\n            return _functions.thnn.ReplicationPad3d.apply(input, *pad)\n    else:\n        raise NotImplementedError(\"Only 4D and 5D padding is supported for now\")", "body": "Hey, I'm not sure how to make a push request, but here is the code for 5D padding.\r\n\r\n`/torch/nn/_functions` should get the following class:\r\n\r\n```\r\nclass ConstantPad3d(Function):\r\n\r\n    @staticmethod\r\n    def forward(ctx, input, pad, value=0):\r\n        assert input.dim() == 5, 'only 5D supported for padding'\r\n        ctx.pad = pad\r\n        ctx.value = value\r\n        pad_lt, pad_rt, pad_tp, pad_bm, pad_ft, pad_bk = ctx.pad\r\n        d = input.size(2) + pad_ft + pad_bk\r\n        h = input.size(3) + pad_tp + pad_bm\r\n        w = input.size(4) + pad_lt + pad_rt\r\n        assert w > 0 and h > 0 and d > 0, 'input is too small'\r\n        ctx.input_size = input.size()\r\n\r\n        # crop input if necessary\r\n        output = input.new(input.size(0), input.size(1), d, h, w).fill_(ctx.value)\r\n        c_input = input\r\n        if pad_ft < 0:\r\n            c_input = c_input.narrow(2, -pad_ft, c_input.size(2) + pad_ft)\r\n        if pad_bk < 0:\r\n            c_input = c_input.narrow(2, 0, c_input.size(2) + pad_bk)\r\n        if pad_tp < 0:\r\n            c_input = c_input.narrow(3, -pad_tp, c_input.size(3) + pad_tp)\r\n        if pad_bm < 0:\r\n            c_input = c_input.narrow(3, 0, c_input.size(3) + pad_bm)\r\n        if pad_lt < 0:\r\n            c_input = c_input.narrow(4, -pad_lt, c_input.size(4) + pad_lt)\r\n        if pad_rt < 0:\r\n            c_input = c_input.narrow(4, 0, c_input.size(4) + pad_rt)\r\n\r\n        # crop output if necessary\r\n        c_output = output\r\n        if pad_ft > 0:\r\n            c_output = c_output.narrow(2, pad_ft, c_output.size(2) - pad_ft)\r\n        if pad_bk > 0:\r\n            c_output = c_output.narrow(2, 0, c_output.size(2) - pad_bk)\r\n        if pad_tp > 0:\r\n            c_output = c_output.narrow(3, pad_tp, c_output.size(3) - pad_tp)\r\n        if pad_bm > 0:\r\n            c_output = c_output.narrow(3, 0, c_output.size(3) - pad_bm)\r\n        if pad_lt > 0:\r\n            c_output = c_output.narrow(4, pad_lt, c_output.size(4) - pad_lt)\r\n        if pad_rt > 0:\r\n            c_output = c_output.narrow(4, 0, c_output.size(4) - pad_rt)\r\n        c_output.copy_(c_input)\r\n        return output\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        pad_lt, pad_rt, pad_tp, pad_bm, pad_ft, pad_bk = ctx.pad\r\n\r\n        grad_input = Variable(grad_output.data.new(ctx.input_size).zero_())\r\n        grad_input_slices = [slice(0, x,) for x in ctx.input_size]\r\n\r\n        def narrow_slice(dim, start, length):\r\n            grad_input_slices[dim] = (slice(grad_input_slices[dim].start + start,\r\n                                            grad_input_slices[dim].start + start + length))\r\n\r\n        def slice_length(dim):\r\n            return grad_input_slices[dim].stop - grad_input_slices[dim].start\r\n\r\n        #  crop grad_input if necessary\r\n        if pad_ft < 0:\r\n            narrow_slice(2, -pad_ft, slice_length(2) + pad_ft)\r\n        if pad_bk < 0:\r\n            narrow_slice(2, 0, slice_length(2) + pad_bk)\r\n        if pad_tp < 0:\r\n            narrow_slice(3, -pad_tp, slice_length(3) + pad_tp)\r\n        if pad_bm < 0:\r\n            narrow_slice(3, 0, slice_length(3) + pad_bm)\r\n        if pad_lt < 0:\r\n            narrow_slice(4, -pad_lt, slice_length(4) + pad_lt)\r\n        if pad_rt < 0:\r\n            narrow_slice(4, 0, slice_length(4) + pad_rt)\r\n\r\n        # crop grad_output if necessary\r\n        cg_output = grad_output\r\n        if pad_ft > 0:\r\n            cg_output = cg_output.narrow(2, pad_ft, cg_output.size(2) - pad_ft)\r\n        if pad_bk > 0:\r\n            cg_output = cg_output.narrow(2, 0, cg_output.size(2) - pad_bk)\r\n        if pad_tp > 0:\r\n            cg_output = cg_output.narrow(3, pad_tp, cg_output.size(3) - pad_tp)\r\n        if pad_bm > 0:\r\n            cg_output = cg_output.narrow(3, 0, cg_output.size(3) - pad_bm)\r\n        if pad_lt > 0:\r\n            cg_output = cg_output.narrow(4, pad_lt, cg_output.size(4) - pad_lt)\r\n        if pad_rt > 0:\r\n            cg_output = cg_output.narrow(4, 0, cg_output.size(4) - pad_rt)\r\n        gis = tuple(grad_input_slices)\r\n        grad_input[gis] = cg_output\r\n\r\n        return grad_input, None, None\r\n```\r\n\r\nand `torch/nn/functional.py` should be updated with:\r\n\r\n```\r\nfrom ._functions.padding import ConstantPad2d, ConstantPad3d\r\n\r\n...\r\n\r\ndef pad(input, pad, mode='constant', value=0):\r\n    \"\"\"Pads tensor.\r\n    Currently only 2D and 3D padding supported.\r\n    In case of 4D input tensor pad should be in form\r\n    (pad_l, pad_r, pad_t, pad_b ).\r\n    In case of 5D pad should be (pleft, pright, ptop, pbottom, pfront, pback)\r\n    Args:\r\n        input (Variable): 4D or 5D tensor\r\n        pad (tuple): 4-elem or 6-elem tuple\r\n        mode: 'constant', 'reflect' or 'replicate'. Default: 'constant'\r\n        value: fill value for 'constant' padding. Default: 0\r\n    \"\"\"\r\n    if input.dim() == 4:\r\n        assert len(pad) == 4, '4D tensors expect 4 values for padding'\r\n        if mode == 'constant':\r\n            return ConstantPad2d.apply(input, pad, value)\r\n        elif mode == 'reflect':\r\n            return _functions.thnn.ReflectionPad2d.apply(input, *pad)\r\n        elif mode == 'replicate':\r\n            return _functions.thnn.ReplicationPad2d.apply(input, *pad)\r\n    elif input.dim() == 5:\r\n        assert len(pad) == 6, '5D tensors expect 6 values for padding'\r\n        if mode == 'constant':\r\n            return ConstantPad3d.apply(input, pad, value)\r\n        elif mode == 'reflect':\r\n            raise NotImplementedError\r\n        elif mode == 'replicate':\r\n            return _functions.thnn.ReplicationPad3d.apply(input, *pad)\r\n    else:\r\n        raise NotImplementedError(\"Only 4D and 5D padding is supported for now\")\r\n```"}