{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328097072", "html_url": "https://github.com/pytorch/pytorch/issues/2672#issuecomment-328097072", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2672", "id": 328097072, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODA5NzA3Mg==", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-08T13:01:56Z", "updated_at": "2017-09-08T13:01:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Some code is not optimized for the inference at all. Actually I would like to think the <code>requires_grad</code> as an attribute for <code>Optimizer</code> rather than an attribute for forward and backward in <strong>current</strong> version(0.2) of <code>pytorch</code>, but sometimes it helps for error detecting, e.g. the <code>index</code> for <code>index_select</code> can't have <code>requires_grad=True</code>.<br>\nLet's take a look at the sample code for <code>cumprod</code>, the <code>save_for_backward</code> function saves the operands regardless of <code>requires_grad</code>.<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/459cc5a346e25200eca1b921729d10363fac2b1b/torch/autograd/_functions/tensor.py#L735-L742\">pytorch/torch/autograd/_functions/tensor.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 735 to 742\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/459cc5a346e25200eca1b921729d10363fac2b1b\">459cc5a</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L735\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"735\"></td>\n          <td id=\"LC735\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">class</span> <span class=\"pl-en\">Cumprod</span>(<span class=\"pl-e\">Function</span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L736\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"736\"></td>\n          <td id=\"LC736\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L737\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"737\"></td>\n          <td id=\"LC737\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L738\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"738\"></td>\n          <td id=\"LC738\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">dim</span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L739\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"739\"></td>\n          <td id=\"LC739\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         ctx.dim <span class=\"pl-k\">=</span> dim </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L740\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"740\"></td>\n          <td id=\"LC740\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         ctx.save_for_backward(<span class=\"pl-c1\">input</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L741\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"741\"></td>\n          <td id=\"LC741\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         <span class=\"pl-k\">return</span> torch.cumprod(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>ctx.dim) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L742\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"742\"></td>\n          <td id=\"LC742\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>I don't exactly know about how convolution is performed by low level matrix operation, but this should be one problem.</p>", "body_text": "Some code is not optimized for the inference at all. Actually I would like to think the requires_grad as an attribute for Optimizer rather than an attribute for forward and backward in current version(0.2) of pytorch, but sometimes it helps for error detecting, e.g. the index for index_select can't have requires_grad=True.\nLet's take a look at the sample code for cumprod, the save_for_backward function saves the operands regardless of requires_grad.\n\n  \n    \n      pytorch/torch/autograd/_functions/tensor.py\n    \n    \n        Lines 735 to 742\n      in\n      459cc5a\n    \n    \n    \n    \n\n        \n          \n           class Cumprod(Function): \n        \n\n        \n          \n            \n        \n\n        \n          \n               @staticmethod \n        \n\n        \n          \n               def forward(ctx, input, dim): \n        \n\n        \n          \n                   ctx.dim = dim \n        \n\n        \n          \n                   ctx.save_for_backward(input) \n        \n\n        \n          \n                   return torch.cumprod(input, dim=ctx.dim) \n        \n\n        \n          \n            \n        \n    \n  \n\n\nI don't exactly know about how convolution is performed by low level matrix operation, but this should be one problem.", "body": "Some code is not optimized for the inference at all. Actually I would like to think the `requires_grad` as an attribute for `Optimizer` rather than an attribute for forward and backward in **current** version(0.2) of `pytorch`, but sometimes it helps for error detecting, e.g. the `index` for `index_select` can't have `requires_grad=True`.\r\n Let's take a look at the sample code for `cumprod`, the `save_for_backward` function saves the operands regardless of `requires_grad`.\r\nhttps://github.com/pytorch/pytorch/blob/459cc5a346e25200eca1b921729d10363fac2b1b/torch/autograd/_functions/tensor.py#L735-L742\r\n\r\nI don't exactly know about how convolution is performed by low level matrix operation, but this should be one problem."}