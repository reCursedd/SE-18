{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11732", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11732/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11732/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11732/events", "html_url": "https://github.com/pytorch/pytorch/issues/11732", "id": 360563078, "node_id": "MDU6SXNzdWUzNjA1NjMwNzg=", "number": 11732, "title": "Segfault in dataparallel + checkpoint", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-15T18:16:23Z", "updated_at": "2018-11-08T08:30:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Reproducible on master. Reported at <a href=\"https://discuss.pytorch.org/t/segmentation-fault-when-using-checkpoint-and-dataparallel/25247\" rel=\"nofollow\">https://discuss.pytorch.org/t/segmentation-fault-when-using-checkpoint-and-dataparallel/25247</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> optim\n<span class=\"pl-k\">import</span> torch.utils.checkpoint <span class=\"pl-k\">as</span> chk\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">model</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.blocks <span class=\"pl-k\">=</span> nn.ModuleDict()\n\n        <span class=\"pl-c1\">self</span>.conv0 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.blocks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>0<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv0\n\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.blocks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1\n\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.blocks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>2<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2\n\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.blocks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>3<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv3\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.blocks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>0<span class=\"pl-pds\">'</span></span>](x)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>x1 = self.blocks['1'](x)</span>\n        x1 <span class=\"pl-k\">=</span> chk.checkpoint(<span class=\"pl-c1\">self</span>.conv1,x)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>x2 = self.blocks['2'](x)</span>\n        x2 <span class=\"pl-k\">=</span> chk.checkpoint(<span class=\"pl-c1\">self</span>.conv2,x)\n\n        x <span class=\"pl-k\">=</span> torch.cat((x1,x2),<span class=\"pl-c1\">1</span>)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.blocks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>3<span class=\"pl-pds\">'</span></span>](x)\n\n        <span class=\"pl-k\">return</span> x\n    \ntest_model <span class=\"pl-k\">=</span> model()\ntest_model <span class=\"pl-k\">=</span> nn.DataParallel(test_model)\ntest_model <span class=\"pl-k\">=</span> test_model.cuda()\n\nloss <span class=\"pl-k\">=</span> nn.MSELoss()\noptimizer <span class=\"pl-k\">=</span> optim.SGD(test_model.module.parameters(), <span class=\"pl-v\">lr</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.01</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n    <span class=\"pl-c1\">print</span>(i)\n    data <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">15</span>,<span class=\"pl-c1\">15</span>)\n    labels <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">15</span>,<span class=\"pl-c1\">15</span>).cuda()\n    test_preds <span class=\"pl-k\">=</span> test_model(data)\n    optimizer.zero_grad()\n    test_loss <span class=\"pl-k\">=</span> loss(test_preds, labels)\n    test_loss.backward()\n    optimizer.step()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finished<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>Author of the post also provided GDB trace</p>\n<pre><code>Program received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fff649ff700 (LWP 39737)]\nstd::__push_heap&lt;__gnu_cxx::__normal_iterator&lt;torch::autograd::FunctionTask*, std::vector&lt;torch::autograd::FunctionTask&gt; &gt;, long, torch::autograd::FunctionTask, __gnu_cxx::__ops::_Iter_comp_val&lt;torch::autograd::CompareFunctionTaskTime&gt; &gt; (__first=..., __holeIndex=1, __topIndex=__topIndex@entry=0, __value=..., __comp=...) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/stl_heap.h:129\n129     /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/stl_heap.h: No such file or directory.\n</code></pre>", "body_text": "Reproducible on master. Reported at https://discuss.pytorch.org/t/segmentation-fault-when-using-checkpoint-and-dataparallel/25247\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.utils.checkpoint as chk\nimport torch.nn.functional as F\n\nclass model(nn.Module):\n\n    def __init__(self):\n        super(model, self).__init__()\n\n        self.blocks = nn.ModuleDict()\n\n        self.conv0 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=True)\n        self.blocks['0'] = self.conv0\n\n        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n        self.blocks['1'] = self.conv1\n\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n        self.blocks['2'] = self.conv2\n\n        self.conv3 = nn.Conv2d(64, 20, kernel_size=3, stride=1, padding=1, bias=True)\n        self.blocks['3'] = self.conv3\n\n    def forward(self, x):\n\n        x = self.blocks['0'](x)\n\n        #x1 = self.blocks['1'](x)\n        x1 = chk.checkpoint(self.conv1,x)\n\n        #x2 = self.blocks['2'](x)\n        x2 = chk.checkpoint(self.conv2,x)\n\n        x = torch.cat((x1,x2),1)\n\n        x = self.blocks['3'](x)\n\n        return x\n    \ntest_model = model()\ntest_model = nn.DataParallel(test_model)\ntest_model = test_model.cuda()\n\nloss = nn.MSELoss()\noptimizer = optim.SGD(test_model.module.parameters(), lr = 0.01)\n\nfor i in range(100):\n    print(i)\n    data = torch.rand(4, 3, 15,15)\n    labels = torch.rand(4,20, 15,15).cuda()\n    test_preds = test_model(data)\n    optimizer.zero_grad()\n    test_loss = loss(test_preds, labels)\n    test_loss.backward()\n    optimizer.step()\n\nprint('Finished')\nAuthor of the post also provided GDB trace\nProgram received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fff649ff700 (LWP 39737)]\nstd::__push_heap<__gnu_cxx::__normal_iterator<torch::autograd::FunctionTask*, std::vector<torch::autograd::FunctionTask> >, long, torch::autograd::FunctionTask, __gnu_cxx::__ops::_Iter_comp_val<torch::autograd::CompareFunctionTaskTime> > (__first=..., __holeIndex=1, __topIndex=__topIndex@entry=0, __value=..., __comp=...) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/stl_heap.h:129\n129     /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/stl_heap.h: No such file or directory.", "body": "Reproducible on master. Reported at https://discuss.pytorch.org/t/segmentation-fault-when-using-checkpoint-and-dataparallel/25247\r\n\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch import optim\r\nimport torch.utils.checkpoint as chk\r\nimport torch.nn.functional as F\r\n\r\nclass model(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(model, self).__init__()\r\n\r\n        self.blocks = nn.ModuleDict()\r\n\r\n        self.conv0 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=True)\r\n        self.blocks['0'] = self.conv0\r\n\r\n        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\r\n        self.blocks['1'] = self.conv1\r\n\r\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\r\n        self.blocks['2'] = self.conv2\r\n\r\n        self.conv3 = nn.Conv2d(64, 20, kernel_size=3, stride=1, padding=1, bias=True)\r\n        self.blocks['3'] = self.conv3\r\n\r\n    def forward(self, x):\r\n\r\n        x = self.blocks['0'](x)\r\n\r\n        #x1 = self.blocks['1'](x)\r\n        x1 = chk.checkpoint(self.conv1,x)\r\n\r\n        #x2 = self.blocks['2'](x)\r\n        x2 = chk.checkpoint(self.conv2,x)\r\n\r\n        x = torch.cat((x1,x2),1)\r\n\r\n        x = self.blocks['3'](x)\r\n\r\n        return x\r\n    \r\ntest_model = model()\r\ntest_model = nn.DataParallel(test_model)\r\ntest_model = test_model.cuda()\r\n\r\nloss = nn.MSELoss()\r\noptimizer = optim.SGD(test_model.module.parameters(), lr = 0.01)\r\n\r\nfor i in range(100):\r\n    print(i)\r\n    data = torch.rand(4, 3, 15,15)\r\n    labels = torch.rand(4,20, 15,15).cuda()\r\n    test_preds = test_model(data)\r\n    optimizer.zero_grad()\r\n    test_loss = loss(test_preds, labels)\r\n    test_loss.backward()\r\n    optimizer.step()\r\n\r\nprint('Finished')\r\n```\r\n\r\nAuthor of the post also provided GDB trace\r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fff649ff700 (LWP 39737)]\r\nstd::__push_heap<__gnu_cxx::__normal_iterator<torch::autograd::FunctionTask*, std::vector<torch::autograd::FunctionTask> >, long, torch::autograd::FunctionTask, __gnu_cxx::__ops::_Iter_comp_val<torch::autograd::CompareFunctionTaskTime> > (__first=..., __holeIndex=1, __topIndex=__topIndex@entry=0, __value=..., __comp=...) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/stl_heap.h:129\r\n129     /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/stl_heap.h: No such file or directory.\r\n```"}