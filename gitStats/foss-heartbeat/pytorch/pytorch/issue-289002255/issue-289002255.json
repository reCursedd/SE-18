{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4685", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4685/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4685/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4685/events", "html_url": "https://github.com/pytorch/pytorch/issues/4685", "id": 289002255, "node_id": "MDU6SXNzdWUyODkwMDIyNTU=", "number": 4685, "title": "issues with nn.utils.weight_norm at network evaluation", "user": {"login": "adrienchaton", "id": 35500385, "node_id": "MDQ6VXNlcjM1NTAwMzg1", "avatar_url": "https://avatars2.githubusercontent.com/u/35500385?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adrienchaton", "html_url": "https://github.com/adrienchaton", "followers_url": "https://api.github.com/users/adrienchaton/followers", "following_url": "https://api.github.com/users/adrienchaton/following{/other_user}", "gists_url": "https://api.github.com/users/adrienchaton/gists{/gist_id}", "starred_url": "https://api.github.com/users/adrienchaton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adrienchaton/subscriptions", "organizations_url": "https://api.github.com/users/adrienchaton/orgs", "repos_url": "https://api.github.com/users/adrienchaton/repos", "events_url": "https://api.github.com/users/adrienchaton/events{/privacy}", "received_events_url": "https://api.github.com/users/adrienchaton/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-16T18:00:36Z", "updated_at": "2018-02-15T19:25:15Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi Pytorch community,</p>\n<p>Thanks for the amazing library you are putting on, my favorite now.</p>\n<p>About the <strong>nn.utils.weight_norm</strong>, I use it on linear layers, at training it seems working well.<br>\nThen I load my trained model, put it on eval() and run it on my data which created a steadily increasing memory load until the GPU is saturated (both on GTX Titan or 980) ...<br>\nI wondered what was the reason and eventually used nn.utils.remove_weight_norm on the corresponding layers after loading the trained weight and then the memory kept stable at a normal load ...</p>\n<p>Is there an issue here ?</p>\n<p>I mean, when not computing gradients and optimization, it should simply output the same values with or without the weight decomposition. Then removing it shouldn't affect the generation after the end of the training and it's only a minor issue as it can be alleviated with the remove_weight_norm. Or maybe there's something I don't understand here ?</p>\n<p>Two other minor points:<br>\n_ I also had an issue when using <strong>torch.nn.ConvTranspose1d</strong> which returned 'RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.'<br>\nIt is similar to: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"271583717\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3512\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3512/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3512\">#3512</a> and when setting torch.backends.cudnn.enabled = False it doesn't appear (and the input was contiguous in both cases). I wanted to add that it seems to depend on the input tensor size as the error was returned when my Lin was only 1 (with stride = 4 and kernel size = 4). Can be fixed with changing the backend but maybe it is worth noting it ..<br>\n_ Lastly, I noted a weird behavior with <strong>F.log_softmax</strong> that I am using at the output of a MLP to train on uint8 samples with F.nll_loss. In this case it is with a sampleRNN model that learns to predict 8bit audio samples. Training loss behaves well, then at generation I take the argmax of the prediction in order to retrieve the sample. If I take it on the logsoftmax or softmax outputs it behaves badly whereas it works better on the raw MLP output .. Whereas it should be consistent at generation to use any and the argmax position should remain the same ?</p>\n<p>Thanks everyone here !</p>", "body_text": "Hi Pytorch community,\nThanks for the amazing library you are putting on, my favorite now.\nAbout the nn.utils.weight_norm, I use it on linear layers, at training it seems working well.\nThen I load my trained model, put it on eval() and run it on my data which created a steadily increasing memory load until the GPU is saturated (both on GTX Titan or 980) ...\nI wondered what was the reason and eventually used nn.utils.remove_weight_norm on the corresponding layers after loading the trained weight and then the memory kept stable at a normal load ...\nIs there an issue here ?\nI mean, when not computing gradients and optimization, it should simply output the same values with or without the weight decomposition. Then removing it shouldn't affect the generation after the end of the training and it's only a minor issue as it can be alleviated with the remove_weight_norm. Or maybe there's something I don't understand here ?\nTwo other minor points:\n_ I also had an issue when using torch.nn.ConvTranspose1d which returned 'RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.'\nIt is similar to: #3512 and when setting torch.backends.cudnn.enabled = False it doesn't appear (and the input was contiguous in both cases). I wanted to add that it seems to depend on the input tensor size as the error was returned when my Lin was only 1 (with stride = 4 and kernel size = 4). Can be fixed with changing the backend but maybe it is worth noting it ..\n_ Lastly, I noted a weird behavior with F.log_softmax that I am using at the output of a MLP to train on uint8 samples with F.nll_loss. In this case it is with a sampleRNN model that learns to predict 8bit audio samples. Training loss behaves well, then at generation I take the argmax of the prediction in order to retrieve the sample. If I take it on the logsoftmax or softmax outputs it behaves badly whereas it works better on the raw MLP output .. Whereas it should be consistent at generation to use any and the argmax position should remain the same ?\nThanks everyone here !", "body": "Hi Pytorch community,\r\n\r\nThanks for the amazing library you are putting on, my favorite now.\r\n\r\nAbout the **nn.utils.weight_norm**, I use it on linear layers, at training it seems working well.\r\nThen I load my trained model, put it on eval() and run it on my data which created a steadily increasing memory load until the GPU is saturated (both on GTX Titan or 980) ...\r\nI wondered what was the reason and eventually used nn.utils.remove_weight_norm on the corresponding layers after loading the trained weight and then the memory kept stable at a normal load ...\r\n\r\nIs there an issue here ?\r\n\r\nI mean, when not computing gradients and optimization, it should simply output the same values with or without the weight decomposition. Then removing it shouldn't affect the generation after the end of the training and it's only a minor issue as it can be alleviated with the remove_weight_norm. Or maybe there's something I don't understand here ?\r\n\r\nTwo other minor points:\r\n_ I also had an issue when using **torch.nn.ConvTranspose1d** which returned 'RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.'\r\nIt is similar to: https://github.com/pytorch/pytorch/issues/3512 and when setting torch.backends.cudnn.enabled = False it doesn't appear (and the input was contiguous in both cases). I wanted to add that it seems to depend on the input tensor size as the error was returned when my Lin was only 1 (with stride = 4 and kernel size = 4). Can be fixed with changing the backend but maybe it is worth noting it ..\r\n_ Lastly, I noted a weird behavior with **F.log_softmax** that I am using at the output of a MLP to train on uint8 samples with F.nll_loss. In this case it is with a sampleRNN model that learns to predict 8bit audio samples. Training loss behaves well, then at generation I take the argmax of the prediction in order to retrieve the sample. If I take it on the logsoftmax or softmax outputs it behaves badly whereas it works better on the raw MLP output .. Whereas it should be consistent at generation to use any and the argmax position should remain the same ?\r\n\r\nThanks everyone here !"}