{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113825535", "pull_request_review_id": 35248243, "id": 113825535, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMzgyNTUzNQ==", "diff_hunk": "@@ -0,0 +1,181 @@\n+import numpy as np\n+import warnings\n+from bisect import bisect_right\n+from torch.optim.optimizer import Optimizer\n+\n+\n+class LambdaLR(object):\n+    def __init__(self, optimizer, base_lr, lr_lambda):\n+        self.optimizer = optimizer\n+        self.lr_lambda = lr_lambda\n+        self.base_lr = base_lr\n+\n+    def step(self, epoch):\n+        for param_group in self.optimizer.param_groups:\n+            param_group['lr'] = self.base_lr * self.lr_lambda(epoch)\n+\n+\n+class GroupLambdaLR(object):\n+    def __init__(self, optimizer, base_lrs, lr_lambdas):\n+        self.zip = zip(optimizer.param_groups, base_lrs, lr_lambdas)\n+\n+    def step(self, epoch):\n+        for param_group, base_lr, lr_lambda in self.zip:\n+            param_group['lr'] = base_lr * lr_lambda(epoch)\n+\n+\n+class StepLR(LambdaLR):\n+    \"\"\"Set the learning rate to the base_lr decayed by gamma \n+    every step_size epochs.\n+    \n+    \n+    Example:\n+        >>> # lr = 0.05     if epoch < 30\n+        >>> # lr = 0.005    if 30 <= epoch < 60\n+        >>> # lr = 0.0005   if 60 <= epoch < 90\n+        >>> # ...\n+        >>> scheduler = StepLR(optimizer, base_lr=0.05, gamma=0.1, step_size=30)\n+        >>> for epoch in range(100):\n+        >>>     scheduler.step(epoch)\n+        >>>     train(...)\n+        >>>     validate(...)\n+    \"\"\"\n+\n+    def __init__(self, optimizer, base_lr=0.1, gamma=0.1, step_size=30):\n+        super(StepLR, self).__init__(optimizer, base_lr,\n+                                     lambda epoch: gamma ** (epoch // step_size))\n+\n+\n+class MultiStepLR(LambdaLR):\n+    \"\"\"Set the learning rate to the base_lr decayed by gamma \n+    once the number of epoch reaches one of the milestones.\n+    \n+    \n+    Example:\n+        >>> # lr = 0.05     if epoch < 30\n+        >>> # lr = 0.005    if 30 <= epoch < 80\n+        >>> # lr = 0.0005   if epoch >=80\n+        >>> scheduler = MultiStepLR(optimizer, base_lr=0.05, gamma=0.1, milestones=[30,80])\n+        >>> for epoch in range(100):\n+        >>>     scheduler.step(epoch)\n+        >>>     train(...)\n+        >>>     validate(...)\n+    \"\"\"\n+\n+    def __init__(self, optimizer, base_lr=0.1, gamma=0.1, milestones=(10, 20, 30)):\n+        milestones = sorted(milestones)\n+        super(MultiStepLR, self).__init__(optimizer, base_lr,\n+                                          lambda epoch: gamma ** bisect_right(milestones, epoch))\n+\n+\n+class ExponentialLR(LambdaLR):\n+    \"\"\"Set the learning rate to the initial LR decayed by gamma in\n+    every epoch.\"\"\"\n+\n+    def __init__(self, optimizer, base_lr, gamma):\n+        super(ExponentialLR, self).__init__(optimizer, base_lr,\n+                                            lambda epoch: gamma ** epoch)\n+\n+\n+class ReduceLROnPlateau(object):\n+    \"\"\"Reduce learning rate when a metric has stopped improving.\n+    Models often benefit from reducing the learning rate by a factor\n+    of 2-10 once learning stagnates. This scheduler reads a metrics\n+    quantity and if no improvement is seen for a 'patience' number\n+    of epochs, the learning rate is reduced.\n+    \n+    Args:\n+        factor: factor by which the learning rate will\n+            be reduced. new_lr = lr * factor\n+        patience: number of epochs with no improvement\n+            after which learning rate will be reduced.\n+        verbose: int. 0: quiet, 1: update messages.\n+        mode: one of {min, max}. In `min` mode,\n+            lr will be reduced when the quantity\n+            monitored has stopped decreasing; in `max`\n+            mode it will be reduced when the quantity\n+            monitored has stopped increasing.\n+        epsilon: threshold for measuring the new optimum,\n+            to only focus on significant changes.\n+        cooldown: number of epochs to wait before resuming\n+            normal operation after lr has been reduced.\n+        min_lr: lower bound on the learning rate.\n+        \n+        \n+    Example:\n+        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n+        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n+        >>> for epoch in range(10):\n+        >>>     train(...)\n+        >>>     val_loss = validate(...)\n+        >>>     # different from LambdaLR, step should be called after validate()\n+        >>>     scheduler.step(epoch, val_loss) \n+    \"\"\"\n+\n+    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,\n+                 verbose=0, epsilon=1e-4, cooldown=0, min_lr=0):\n+\n+        if factor >= 1.0:\n+            raise ValueError('ReduceLROnPlateau '\n+                             'does not support a factor >= 1.0.')\n+        self.factor = factor\n+        self.min_lr = min_lr\n+        self.epsilon = epsilon\n+        self.patience = patience\n+        self.verbose = verbose\n+        self.cooldown = cooldown\n+        self.cooldown_counter = 0  # Cooldown counter.\n+        self.monitor_op = None\n+        self.wait = 0\n+        self.best = 0\n+        self.mode = mode\n+        assert isinstance(optimizer, Optimizer)\n+        self.optimizer = optimizer\n+        self._reset()\n+\n+    def _reset(self):\n+        \"\"\"Resets wait counter and cooldown counter.\n+        \"\"\"\n+        if self.mode not in ['min', 'max']:\n+            raise RuntimeError('Learning Rate Plateau Reducing mode %s is unknown!')\n+        if self.mode == 'min':\n+            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)", "path": "torch/optim/lr_scheduler.py", "position": null, "original_position": 142, "commit_id": "db59187201d18c203f084c8b288e07f7bd2a43aa", "original_commit_id": "a74ec0d5aa8a3b5c4b5e78a411b67c9706befdbc", "user": {"login": "Jiaming-Liu", "id": 16099575, "node_id": "MDQ6VXNlcjE2MDk5NTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/16099575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jiaming-Liu", "html_url": "https://github.com/Jiaming-Liu", "followers_url": "https://api.github.com/users/Jiaming-Liu/followers", "following_url": "https://api.github.com/users/Jiaming-Liu/following{/other_user}", "gists_url": "https://api.github.com/users/Jiaming-Liu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jiaming-Liu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jiaming-Liu/subscriptions", "organizations_url": "https://api.github.com/users/Jiaming-Liu/orgs", "repos_url": "https://api.github.com/users/Jiaming-Liu/repos", "events_url": "https://api.github.com/users/Jiaming-Liu/events{/privacy}", "received_events_url": "https://api.github.com/users/Jiaming-Liu/received_events", "type": "User", "site_admin": false}, "body": "@colesbury I am not sure about this one. ~~Epsilon should be used for handling numerical error. If we really need a threshold, we might use something else.~~", "created_at": "2017-04-27T23:35:38Z", "updated_at": "2018-11-23T15:33:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/1370#discussion_r113825535", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1370", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113825535"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1370#discussion_r113825535"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1370"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> I am not sure about this one. <del>Epsilon should be used for handling numerical error. If we really need a threshold, we might use something else.</del></p>", "body_text": "@colesbury I am not sure about this one. Epsilon should be used for handling numerical error. If we really need a threshold, we might use something else.", "in_reply_to_id": 113589052}