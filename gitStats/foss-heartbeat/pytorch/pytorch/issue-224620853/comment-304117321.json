{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/304117321", "html_url": "https://github.com/pytorch/pytorch/pull/1370#issuecomment-304117321", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1370", "id": 304117321, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDExNzMyMQ==", "user": {"login": "szagoruyko", "id": 4953728, "node_id": "MDQ6VXNlcjQ5NTM3Mjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/4953728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szagoruyko", "html_url": "https://github.com/szagoruyko", "followers_url": "https://api.github.com/users/szagoruyko/followers", "following_url": "https://api.github.com/users/szagoruyko/following{/other_user}", "gists_url": "https://api.github.com/users/szagoruyko/gists{/gist_id}", "starred_url": "https://api.github.com/users/szagoruyko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szagoruyko/subscriptions", "organizations_url": "https://api.github.com/users/szagoruyko/orgs", "repos_url": "https://api.github.com/users/szagoruyko/repos", "events_url": "https://api.github.com/users/szagoruyko/events{/privacy}", "received_events_url": "https://api.github.com/users/szagoruyko/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-25T20:37:27Z", "updated_at": "2017-05-25T20:37:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As far as I see there is only one optimizer being kept, so on learning rate drop all other parameters are also kept. How would one add momentum resetting on learning rate drops in SGD?</p>", "body_text": "As far as I see there is only one optimizer being kept, so on learning rate drop all other parameters are also kept. How would one add momentum resetting on learning rate drops in SGD?", "body": "As far as I see there is only one optimizer being kept, so on learning rate drop all other parameters are also kept. How would one add momentum resetting on learning rate drops in SGD?"}