{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/154342917", "pull_request_review_id": 80493660, "id": 154342917, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDM0MjkxNw==", "diff_hunk": "@@ -0,0 +1,305 @@\n+from bisect import bisect_right\n+from .optimizer import Optimizer\n+\n+\n+class _LRScheduler(object):\n+    def __init__(self, optimizer, last_epoch=-1):\n+        if not isinstance(optimizer, Optimizer):\n+            raise TypeError('{} is not an Optimizer'.format(\n+                type(optimizer).__name__))\n+        self.optimizer = optimizer\n+        if last_epoch == -1:\n+            for group in optimizer.param_groups:\n+                group.setdefault('initial_lr', group['lr'])\n+        else:\n+            for i, group in enumerate(optimizer.param_groups):\n+                if 'initial_lr' not in group:\n+                    raise KeyError(\"param 'initial_lr' is not specified \"\n+                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n+        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n+        self.step(last_epoch + 1)\n+        self.last_epoch = last_epoch\n+\n+    def get_lr(self):\n+        raise NotImplementedError\n+\n+    def step(self, epoch=None):\n+        if epoch is None:\n+            epoch = self.last_epoch + 1\n+        self.last_epoch = epoch\n+        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n+            param_group['lr'] = lr\n+\n+\n+class LambdaLR(_LRScheduler):\n+    \"\"\"Sets the learning rate of each parameter group to the initial lr\n+    times a given function. When last_epoch=-1, sets initial lr as lr.\n+\n+    Args:\n+        optimizer (Optimizer): Wrapped optimizer.\n+        lr_lambda (function or list): A function which computes a multiplicative\n+            factor given an integer parameter epoch, or a list of such functions,\n+            one for each group in optimizer.param_groups.\n+        last_epoch (int): The index of last epoch. Default: -1.\n+\n+    Example:\n+        >>> # Assuming optimizer has two groups.\n+        >>> lambda1 = lambda epoch: epoch // 30\n+        >>> lambda2 = lambda epoch: 0.95 ** epoch\n+        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n+        >>> for epoch in range(100):\n+        >>>     scheduler.step()", "path": "torch/optim/lr_scheduler.py", "position": 51, "original_position": 51, "commit_id": "db59187201d18c203f084c8b288e07f7bd2a43aa", "original_commit_id": "db59187201d18c203f084c8b288e07f7bd2a43aa", "user": {"login": "victorhcm", "id": 3802222, "node_id": "MDQ6VXNlcjM4MDIyMjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3802222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/victorhcm", "html_url": "https://github.com/victorhcm", "followers_url": "https://api.github.com/users/victorhcm/followers", "following_url": "https://api.github.com/users/victorhcm/following{/other_user}", "gists_url": "https://api.github.com/users/victorhcm/gists{/gist_id}", "starred_url": "https://api.github.com/users/victorhcm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/victorhcm/subscriptions", "organizations_url": "https://api.github.com/users/victorhcm/orgs", "repos_url": "https://api.github.com/users/victorhcm/repos", "events_url": "https://api.github.com/users/victorhcm/events{/privacy}", "received_events_url": "https://api.github.com/users/victorhcm/received_events", "type": "User", "site_admin": false}, "body": "Sorry for resurrecting this, @Jiaming-Liu, I just wanted to check with you first. In the docstrings, should we change `scheduler.step()` to `scheduler.step(epoch)`, such as in the tests? The current example will update the learning rate for every epoch, which might be misleading in the documentation. ", "created_at": "2017-12-01T13:19:57Z", "updated_at": "2018-11-23T15:37:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/1370#discussion_r154342917", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1370", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/154342917"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1370#discussion_r154342917"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1370"}}, "body_html": "<p>Sorry for resurrecting this, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16099575\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Jiaming-Liu\">@Jiaming-Liu</a>, I just wanted to check with you first. In the docstrings, should we change <code>scheduler.step()</code> to <code>scheduler.step(epoch)</code>, such as in the tests? The current example will update the learning rate for every epoch, which might be misleading in the documentation.</p>", "body_text": "Sorry for resurrecting this, @Jiaming-Liu, I just wanted to check with you first. In the docstrings, should we change scheduler.step() to scheduler.step(epoch), such as in the tests? The current example will update the learning rate for every epoch, which might be misleading in the documentation."}