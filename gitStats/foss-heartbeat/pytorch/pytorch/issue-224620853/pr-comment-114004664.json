{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/114004664", "pull_request_review_id": 35443808, "id": 114004664, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNDAwNDY2NA==", "diff_hunk": "@@ -0,0 +1,206 @@\n+import numpy as np\n+from bisect import bisect_right\n+from torch.optim.optimizer import Optimizer\n+\n+\n+class LambdaLR(object):\n+    def __init__(self, optimizer, base_lr, lr_lambda):\n+        self.optimizer = optimizer\n+        self.base_lr = base_lr\n+        self.lr_lambda = lr_lambda\n+\n+    def step(self, epoch):\n+        for param_group in self.optimizer.param_groups:\n+            param_group['lr'] = self.base_lr * self.lr_lambda(epoch)\n+\n+\n+class GroupLambdaLR(object):\n+    def __init__(self, optimizer, base_lrs, lr_lambdas):\n+        self.optimizer = optimizer\n+        self.base_lrs = base_lrs\n+        self.lr_lambdas = lr_lambdas\n+\n+    def step(self, epoch):\n+        for param_group, base_lr, lr_lambda in zip(\n+                self.optimizer.param_groups,\n+                self.base_lrs, self.lr_lambdas):\n+            param_group['lr'] = base_lr * lr_lambda(epoch)\n+\n+\n+class StepLR(LambdaLR):\n+    \"\"\"Set the learning rate to the base_lr decayed by gamma\n+    every step_size epochs.\n+\n+\n+    Example:\n+        >>> # lr = 0.05     if epoch < 30\n+        >>> # lr = 0.005    if 30 <= epoch < 60\n+        >>> # lr = 0.0005   if 60 <= epoch < 90\n+        >>> # ...\n+        >>> scheduler = StepLR(optimizer, base_lr=0.05, gamma=0.1, step_size=30)\n+        >>> for epoch in range(100):\n+        >>>     scheduler.step(epoch)\n+        >>>     train(...)\n+        >>>     validate(...)\n+    \"\"\"\n+\n+    def __init__(self, optimizer, base_lr=0.1, gamma=0.1, step_size=30):\n+        super(StepLR, self).__init__(optimizer, base_lr,\n+                                     lambda epoch: gamma ** (epoch // step_size))\n+\n+\n+class MultiStepLR(LambdaLR):\n+    \"\"\"Set the learning rate to the base_lr decayed by gamma\n+    once the number of epoch reaches one of the milestones.\n+\n+\n+    Example:\n+        >>> # lr = 0.05     if epoch < 30\n+        >>> # lr = 0.005    if 30 <= epoch < 80\n+        >>> # lr = 0.0005   if epoch >=80\n+        >>> scheduler = MultiStepLR(optimizer, base_lr=0.05, gamma=0.1, milestones=[30,80])\n+        >>> for epoch in range(100):\n+        >>>     scheduler.step(epoch)\n+        >>>     train(...)\n+        >>>     validate(...)\n+    \"\"\"\n+\n+    def __init__(self, optimizer, base_lr=0.1, gamma=0.1, milestones=(10, 20, 30)):", "path": "torch/optim/lr_scheduler.py", "position": null, "original_position": 68, "commit_id": "db59187201d18c203f084c8b288e07f7bd2a43aa", "original_commit_id": "eca2266556481e3c02427a27c3758b5af079786a", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why not merge this class with the above one? They could accept a `schedule` argument, and it would behave like `StepLR` when given a scalar, but like this class when given a list of epoch numbers. Also, I think it's better to verify that the list is sorted instead of sorting it yourself.\r\n\r\nAlso, remove the default for lr.", "created_at": "2017-04-28T19:35:42Z", "updated_at": "2018-11-23T15:33:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/1370#discussion_r114004664", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1370", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/114004664"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1370#discussion_r114004664"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1370"}}, "body_html": "<p>Why not merge this class with the above one? They could accept a <code>schedule</code> argument, and it would behave like <code>StepLR</code> when given a scalar, but like this class when given a list of epoch numbers. Also, I think it's better to verify that the list is sorted instead of sorting it yourself.</p>\n<p>Also, remove the default for lr.</p>", "body_text": "Why not merge this class with the above one? They could accept a schedule argument, and it would behave like StepLR when given a scalar, but like this class when given a list of epoch numbers. Also, I think it's better to verify that the list is sorted instead of sorting it yourself.\nAlso, remove the default for lr."}