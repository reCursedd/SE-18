{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342355581", "html_url": "https://github.com/pytorch/pytorch/issues/2312#issuecomment-342355581", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2312", "id": 342355581, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjM1NTU4MQ==", "user": {"login": "jgc128", "id": 669142, "node_id": "MDQ6VXNlcjY2OTE0Mg==", "avatar_url": "https://avatars3.githubusercontent.com/u/669142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgc128", "html_url": "https://github.com/jgc128", "followers_url": "https://api.github.com/users/jgc128/followers", "following_url": "https://api.github.com/users/jgc128/following{/other_user}", "gists_url": "https://api.github.com/users/jgc128/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgc128/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgc128/subscriptions", "organizations_url": "https://api.github.com/users/jgc128/orgs", "repos_url": "https://api.github.com/users/jgc128/repos", "events_url": "https://api.github.com/users/jgc128/events{/privacy}", "received_events_url": "https://api.github.com/users/jgc128/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-07T02:26:47Z", "updated_at": "2017-11-07T02:32:38Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have the same error (there's also this issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"229837263\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1591\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1591/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1591\">#1591</a>). The code below works on one GPU (<code>CUDA_VISIBLE_DEVICES=0 python pack_padded_sequence_data_parallel.py</code>), but fails with \"ValueError: lengths array has incorrect size\" on two GPUs (<code>CUDA_VISIBLE_DEVICES=0,1 python pack_padded_sequence_data_parallel.py</code>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RNNDataParallel</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(RNNDataParallel, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">lengths</span>):\n        packed <span class=\"pl-k\">=</span> torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        <span class=\"pl-k\">return</span> packed\n\n\nmodel <span class=\"pl-k\">=</span> RNNDataParallel()\nmodel <span class=\"pl-k\">=</span> torch.nn.DataParallel(model)\nmodel <span class=\"pl-k\">=</span> model.cuda()\n\ninputs <span class=\"pl-k\">=</span> Variable(torch.from_numpy(np.array([\n    [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>],\n    [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">0</span>],\n])))\nlengths <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>]\n\npacked <span class=\"pl-k\">=</span> model(inputs, lengths)\n\n<span class=\"pl-c1\">print</span>(packed)</pre></div>\n<p>My PyTorch version is 0.2.0+e02f7bf</p>", "body_text": "Hi,\nI have the same error (there's also this issue #1591). The code below works on one GPU (CUDA_VISIBLE_DEVICES=0 python pack_padded_sequence_data_parallel.py), but fails with \"ValueError: lengths array has incorrect size\" on two GPUs (CUDA_VISIBLE_DEVICES=0,1 python pack_padded_sequence_data_parallel.py):\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\nclass RNNDataParallel(torch.nn.Module):\n    def __init__(self):\n        super(RNNDataParallel, self).__init__()\n\n    def forward(self, inputs, lengths):\n        packed = torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=True)\n\n        return packed\n\n\nmodel = RNNDataParallel()\nmodel = torch.nn.DataParallel(model)\nmodel = model.cuda()\n\ninputs = Variable(torch.from_numpy(np.array([\n    [1, 2, 3],\n    [4, 5, 0],\n])))\nlengths = [3, 2]\n\npacked = model(inputs, lengths)\n\nprint(packed)\nMy PyTorch version is 0.2.0+e02f7bf", "body": "Hi, \r\n\r\nI have the same error (there's also this issue #1591). The code below works on one GPU (`CUDA_VISIBLE_DEVICES=0 python pack_padded_sequence_data_parallel.py`), but fails with \"ValueError: lengths array has incorrect size\" on two GPUs (`CUDA_VISIBLE_DEVICES=0,1 python pack_padded_sequence_data_parallel.py`):\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass RNNDataParallel(torch.nn.Module):\r\n    def __init__(self):\r\n        super(RNNDataParallel, self).__init__()\r\n\r\n    def forward(self, inputs, lengths):\r\n        packed = torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=True)\r\n\r\n        return packed\r\n\r\n\r\nmodel = RNNDataParallel()\r\nmodel = torch.nn.DataParallel(model)\r\nmodel = model.cuda()\r\n\r\ninputs = Variable(torch.from_numpy(np.array([\r\n    [1, 2, 3],\r\n    [4, 5, 0],\r\n])))\r\nlengths = [3, 2]\r\n\r\npacked = model(inputs, lengths)\r\n\r\nprint(packed)\r\n```\r\nMy PyTorch version is 0.2.0+e02f7bf"}