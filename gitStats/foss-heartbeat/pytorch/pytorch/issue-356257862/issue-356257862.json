{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11182", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11182/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11182/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11182/events", "html_url": "https://github.com/pytorch/pytorch/issues/11182", "id": 356257862, "node_id": "MDU6SXNzdWUzNTYyNTc4NjI=", "number": 11182, "title": "[jit] comparison op does not get fused if input requires grad", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-02T06:50:11Z", "updated_at": "2018-09-02T06:50:12Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>The following function</p>\n<pre><code>def maskrelu(x):\n    mask = (x&lt;0.5).type_as(x)\n    return F.relu(mask*x)\n\n</code></pre>\n<p>if input requires grad, produces the following graph:</p>\n<pre><code>graph(%0 : Float(10240, 1024)) {\n  %1 : float = prim::Constant[value=0.5]()\n  %2 : Byte(10240, 1024) = aten::lt(%0, %1)\n  %3 : Float(10240, 1024) = aten::type_as(%2, %0)\n  %4 : Float(10240, 1024) = aten::mul(%3, %0)\n  %5 : Float(10240, 1024) = aten::relu(%4)\n  return (%5);\n}\n\ngraph(%0 : Float(*, *)) {\n  %1 : float = prim::Constant[value=0.5]()\n  %2 : Byte(*, *) = aten::lt(%0, %1)\n  %6 : Float(*, *) = prim::DifferentiableGraph_0(%2, %0)\n  return (%6);\n}\nwith prim::DifferentiableGraph_0 = graph(%1 : Byte(*, *)\n      %2 : Float(*, *)) {\n  %18 : Float(*, *), %19 : Float(*, *) = prim::FusionGroup_0[device=0](%2, %1)\n  return (%18, %19);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)\n      %5 : Byte(*, *)) {\n  %6 : Float(*, *) = aten::type_as(%5, %3)\n  %4 : Float(*, *) = aten::mul(%6, %3)\n  %1 : Float(*, *) = aten::relu(%4)\n  return (%1, %6);\n}\n</code></pre>\n<p>Note that aten::lt is outside of DifferentiableGraph and FusionGroup<br>\nIf input does not requires grad, the whole function is fused:</p>\n<pre lang=\"graph(%0\" data-meta=\": Float(10240, 1024)) {\"><code>  %1 : float = prim::Constant[value=0.5]()\n  %2 : Byte(10240, 1024) = aten::lt(%0, %1)\n  %3 : Float(10240, 1024) = aten::type_as(%2, %0)\n  %4 : Float(10240, 1024) = aten::mul(%3, %0)\n  %5 : Float(10240, 1024) = aten::relu(%4)\n  return (%5);\n}\n\ngraph(%0 : Float(*, *)) {\n  %6 : Float(*, *) = prim::FusionGroup_0[device=0](%0)\n  return (%6);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)) {\n  %7 : float = prim::Constant[value=0.5]()\n  %8 : Byte(*, *) = aten::lt(%3, %7)\n  %6 : Float(*, *) = aten::type_as(%8, %3)\n  %4 : Float(*, *) = aten::mul(%6, %3)\n  %1 : Float(*, *) = aten::relu(%4)\n  return (%1);\n}\n</code></pre>", "body_text": "The following function\ndef maskrelu(x):\n    mask = (x<0.5).type_as(x)\n    return F.relu(mask*x)\n\n\nif input requires grad, produces the following graph:\ngraph(%0 : Float(10240, 1024)) {\n  %1 : float = prim::Constant[value=0.5]()\n  %2 : Byte(10240, 1024) = aten::lt(%0, %1)\n  %3 : Float(10240, 1024) = aten::type_as(%2, %0)\n  %4 : Float(10240, 1024) = aten::mul(%3, %0)\n  %5 : Float(10240, 1024) = aten::relu(%4)\n  return (%5);\n}\n\ngraph(%0 : Float(*, *)) {\n  %1 : float = prim::Constant[value=0.5]()\n  %2 : Byte(*, *) = aten::lt(%0, %1)\n  %6 : Float(*, *) = prim::DifferentiableGraph_0(%2, %0)\n  return (%6);\n}\nwith prim::DifferentiableGraph_0 = graph(%1 : Byte(*, *)\n      %2 : Float(*, *)) {\n  %18 : Float(*, *), %19 : Float(*, *) = prim::FusionGroup_0[device=0](%2, %1)\n  return (%18, %19);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)\n      %5 : Byte(*, *)) {\n  %6 : Float(*, *) = aten::type_as(%5, %3)\n  %4 : Float(*, *) = aten::mul(%6, %3)\n  %1 : Float(*, *) = aten::relu(%4)\n  return (%1, %6);\n}\n\nNote that aten::lt is outside of DifferentiableGraph and FusionGroup\nIf input does not requires grad, the whole function is fused:\n  %1 : float = prim::Constant[value=0.5]()\n  %2 : Byte(10240, 1024) = aten::lt(%0, %1)\n  %3 : Float(10240, 1024) = aten::type_as(%2, %0)\n  %4 : Float(10240, 1024) = aten::mul(%3, %0)\n  %5 : Float(10240, 1024) = aten::relu(%4)\n  return (%5);\n}\n\ngraph(%0 : Float(*, *)) {\n  %6 : Float(*, *) = prim::FusionGroup_0[device=0](%0)\n  return (%6);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)) {\n  %7 : float = prim::Constant[value=0.5]()\n  %8 : Byte(*, *) = aten::lt(%3, %7)\n  %6 : Float(*, *) = aten::type_as(%8, %3)\n  %4 : Float(*, *) = aten::mul(%6, %3)\n  %1 : Float(*, *) = aten::relu(%4)\n  return (%1);\n}", "body": "The following function\r\n```\r\ndef maskrelu(x):\r\n    mask = (x<0.5).type_as(x)\r\n    return F.relu(mask*x)\r\n\r\n```\r\nif input requires grad, produces the following graph:\r\n```\r\ngraph(%0 : Float(10240, 1024)) {\r\n  %1 : float = prim::Constant[value=0.5]()\r\n  %2 : Byte(10240, 1024) = aten::lt(%0, %1)\r\n  %3 : Float(10240, 1024) = aten::type_as(%2, %0)\r\n  %4 : Float(10240, 1024) = aten::mul(%3, %0)\r\n  %5 : Float(10240, 1024) = aten::relu(%4)\r\n  return (%5);\r\n}\r\n\r\ngraph(%0 : Float(*, *)) {\r\n  %1 : float = prim::Constant[value=0.5]()\r\n  %2 : Byte(*, *) = aten::lt(%0, %1)\r\n  %6 : Float(*, *) = prim::DifferentiableGraph_0(%2, %0)\r\n  return (%6);\r\n}\r\nwith prim::DifferentiableGraph_0 = graph(%1 : Byte(*, *)\r\n      %2 : Float(*, *)) {\r\n  %18 : Float(*, *), %19 : Float(*, *) = prim::FusionGroup_0[device=0](%2, %1)\r\n  return (%18, %19);\r\n}\r\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)\r\n      %5 : Byte(*, *)) {\r\n  %6 : Float(*, *) = aten::type_as(%5, %3)\r\n  %4 : Float(*, *) = aten::mul(%6, %3)\r\n  %1 : Float(*, *) = aten::relu(%4)\r\n  return (%1, %6);\r\n}\r\n```\r\nNote that aten::lt is outside of DifferentiableGraph and FusionGroup\r\nIf input does not requires grad, the whole function is fused:\r\n```graph(%0 : Float(10240, 1024)) {\r\n  %1 : float = prim::Constant[value=0.5]()\r\n  %2 : Byte(10240, 1024) = aten::lt(%0, %1)\r\n  %3 : Float(10240, 1024) = aten::type_as(%2, %0)\r\n  %4 : Float(10240, 1024) = aten::mul(%3, %0)\r\n  %5 : Float(10240, 1024) = aten::relu(%4)\r\n  return (%5);\r\n}\r\n\r\ngraph(%0 : Float(*, *)) {\r\n  %6 : Float(*, *) = prim::FusionGroup_0[device=0](%0)\r\n  return (%6);\r\n}\r\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)) {\r\n  %7 : float = prim::Constant[value=0.5]()\r\n  %8 : Byte(*, *) = aten::lt(%3, %7)\r\n  %6 : Float(*, *) = aten::type_as(%8, %3)\r\n  %4 : Float(*, *) = aten::mul(%6, %3)\r\n  %1 : Float(*, *) = aten::relu(%4)\r\n  return (%1);\r\n}\r\n```\r\n"}