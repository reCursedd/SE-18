{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207432383", "pull_request_review_id": 143043998, "id": 207432383, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNzQzMjM4Mw==", "diff_hunk": "@@ -1872,6 +1872,15 @@ Tensor log1p_backward(const Tensor& grad, const Tensor& self) {\n   return grad / (self + 1);\n }\n \n+std::tuple<Tensor, Tensor> copy_sparse_to_sparse_backward(const Tensor& grad, const Tensor& self, const Tensor& src, bool non_blocking) {\n+  auto self_grad = at::native::sparse_coo_tensor( self._indices().type().tensor({0}), self._values().type().tensor({0}), self.sizes());\n+  if (src.is_cuda() != grad.is_cuda()) { // cuda -> cpu or cpu -> cuda", "path": "tools/autograd/templates/Functions.cpp", "position": null, "original_position": 6, "commit_id": "73754a9bebe746a9c9ae429841f3e2f6a5aee0c2", "original_commit_id": "8fb8fba69d154f36c7a2f73c8c41f313e2d0c8de", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "This breaks if the copy is from one CUDA device to another. Also, IIRC, in autograd engine, backward of `gpu0_tensor.copy(gpu1_tensor)` is executed on thread of `gpu1`, which has `DeviceGuard(1)`. So this should actually do something in `CopyBackward`.\r\n\r\nActually, I suspect that the code of `CopyBackward` could just work (or with little changes). Can you try move the special casing on `s_copy_` in `VariableType.cpp` to be on `copy_`.", "created_at": "2018-08-03T03:30:26Z", "updated_at": "2018-11-23T15:48:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/9005#discussion_r207432383", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9005", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207432383"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9005#discussion_r207432383"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9005"}}, "body_html": "<p>This breaks if the copy is from one CUDA device to another. Also, IIRC, in autograd engine, backward of <code>gpu0_tensor.copy(gpu1_tensor)</code> is executed on thread of <code>gpu1</code>, which has <code>DeviceGuard(1)</code>. So this should actually do something in <code>CopyBackward</code>.</p>\n<p>Actually, I suspect that the code of <code>CopyBackward</code> could just work (or with little changes). Can you try move the special casing on <code>s_copy_</code> in <code>VariableType.cpp</code> to be on <code>copy_</code>.</p>", "body_text": "This breaks if the copy is from one CUDA device to another. Also, IIRC, in autograd engine, backward of gpu0_tensor.copy(gpu1_tensor) is executed on thread of gpu1, which has DeviceGuard(1). So this should actually do something in CopyBackward.\nActually, I suspect that the code of CopyBackward could just work (or with little changes). Can you try move the special casing on s_copy_ in VariableType.cpp to be on copy_."}