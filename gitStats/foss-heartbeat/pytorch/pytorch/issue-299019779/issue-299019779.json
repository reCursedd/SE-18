{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5332", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5332/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5332/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5332/events", "html_url": "https://github.com/pytorch/pytorch/issues/5332", "id": 299019779, "node_id": "MDU6SXNzdWUyOTkwMTk3Nzk=", "number": 5332, "title": "torch.cat behaves weirdly with empty variables", "user": {"login": "baldassarreFe", "id": 13353649, "node_id": "MDQ6VXNlcjEzMzUzNjQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/13353649?v=4", "gravatar_id": "", "url": "https://api.github.com/users/baldassarreFe", "html_url": "https://github.com/baldassarreFe", "followers_url": "https://api.github.com/users/baldassarreFe/followers", "following_url": "https://api.github.com/users/baldassarreFe/following{/other_user}", "gists_url": "https://api.github.com/users/baldassarreFe/gists{/gist_id}", "starred_url": "https://api.github.com/users/baldassarreFe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/baldassarreFe/subscriptions", "organizations_url": "https://api.github.com/users/baldassarreFe/orgs", "repos_url": "https://api.github.com/users/baldassarreFe/repos", "events_url": "https://api.github.com/users/baldassarreFe/events{/privacy}", "received_events_url": "https://api.github.com/users/baldassarreFe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-21T15:43:02Z", "updated_at": "2018-03-23T15:53:32Z", "closed_at": "2018-03-23T15:53:32Z", "author_association": "NONE", "body_html": "<h2>Context</h2>\n<p>I'm experimenting with dense blocks like the ones from DenseNet, where the input of a layer in a dense block is the concatenation of the input and the output of the previous layer, except of course for the very first layer, where only the block input is available.</p>\n<p>Ideally, I would write the forward method of a dense block like this, taking advantage of the fact that the first time <code>cat</code> is invoked the empty tensor is simply discarded.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">block_input</span>):\n        layer_input <span class=\"pl-k\">=</span> block_input\n        layer_output <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.FloatTensor())  <span class=\"pl-c\"><span class=\"pl-c\">#</span> empty</span>\n\n        all_outputs <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> layer <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>._modules.values():\n            layer_input <span class=\"pl-k\">=</span> torch.cat([layer_input, layer_output], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n            layer_output <span class=\"pl-k\">=</span> layer(layer_input)\n            all_outputs.append(layer_output)\n\n        <span class=\"pl-k\">return</span> torch.cat(all_outputs, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)</pre></div>\n<h2>Issue</h2>\n<p>I discovered that <code>torch.cat</code> behaves differently when dealing with empty Tensors and Variables, and in the latter case, it is influenced by the order of the inputs (even weirder)</p>\n<h2>To reproduce</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> FloatTensor\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c1\">print</span>(torch.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-c1\">print</span>(sys.version)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span><span class=\"pl-c1\">0.3</span>.0.post4\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">3.6</span>.4 <span class=\"pl-k\">|</span>Anaconda, Inc.<span class=\"pl-k\">|</span> (default, Jan <span class=\"pl-c1\">16</span> <span class=\"pl-c1\">2018</span>, <span class=\"pl-c1\">18</span>:<span class=\"pl-c1\">10</span>:<span class=\"pl-c1\">19</span>) \n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> [<span class=\"pl-c1\">GCC</span> <span class=\"pl-c1\">7.2</span>.0]</pre></div>\n<p>With Tensors it ignores the empty inputs:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> FloatTensor(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)\nempty <span class=\"pl-k\">=</span> FloatTensor()\n\nres <span class=\"pl-k\">=</span> torch.cat([x, empty], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.Size([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>])\n\nres <span class=\"pl-k\">=</span> torch.cat([empty, x], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.Size([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>])\n\nres <span class=\"pl-k\">=</span> torch.cat([empty, empty], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.Size([])</pre></div>\n<p>With Variables I would expect it to work the same, but...</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> Variable(x)\nempty <span class=\"pl-k\">=</span> Variable(empty)</pre></div>\n<p>If the first element in the concat list is not empty it's ok:</p>\n<div class=\"highlight highlight-source-python\"><pre>res <span class=\"pl-k\">=</span> torch.cat([x, empty], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.Size([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>])</pre></div>\n<p>Unless the first element is the result of an operation, and not a manually created tensor:</p>\n<div class=\"highlight highlight-source-python\"><pre>conv <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nres <span class=\"pl-k\">=</span> torch.cat([conv(x), empty], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">RuntimeError</span>: dimension specified <span class=\"pl-k\">as</span> <span class=\"pl-c1\">1</span> but tensor has no dimensions</pre></div>\n<p>Also fails if the empty Variable is first, or with two empty Variables</p>\n<div class=\"highlight highlight-source-python\"><pre>res <span class=\"pl-k\">=</span> torch.cat([empty, x], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">RuntimeError</span>: dimension specified <span class=\"pl-k\">as</span> <span class=\"pl-c1\">1</span> but tensor has no dimensions\n\nres <span class=\"pl-k\">=</span> torch.cat([empty, empty], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(res.shape)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">RuntimeError</span>: dimension specified <span class=\"pl-k\">as</span> <span class=\"pl-c1\">1</span> but tensor has no dimensions</pre></div>\n<h2>Expected behavior</h2>\n<p><code>torch.cat</code> should behave identically with Tensors and Variables, or the difference should be stated in the documentation.</p>\n<p>Haven't tried on a GPU, so I don't know what the behavior is with CUDA operations.</p>\n<h2>Environment:</h2>\n<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.3.0.post4</li>\n<li>How you installed PyTorch: conda</li>\n<li>Python version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)</li>\n<li>CUDA/cuDNN version: No</li>\n</ul>", "body_text": "Context\nI'm experimenting with dense blocks like the ones from DenseNet, where the input of a layer in a dense block is the concatenation of the input and the output of the previous layer, except of course for the very first layer, where only the block input is available.\nIdeally, I would write the forward method of a dense block like this, taking advantage of the fact that the first time cat is invoked the empty tensor is simply discarded.\ndef forward(self, block_input):\n        layer_input = block_input\n        layer_output = torch.autograd.Variable(torch.FloatTensor())  # empty\n\n        all_outputs = []\n        for layer in self._modules.values():\n            layer_input = torch.cat([layer_input, layer_output], dim=1)\n            layer_output = layer(layer_input)\n            all_outputs.append(layer_output)\n\n        return torch.cat(all_outputs, dim=1)\nIssue\nI discovered that torch.cat behaves differently when dealing with empty Tensors and Variables, and in the latter case, it is influenced by the order of the inputs (even weirder)\nTo reproduce\nimport sys\nimport torch\nfrom torch import FloatTensor\nfrom torch.autograd import Variable\n\nprint(torch.__version__)\nprint(sys.version)\n>>>0.3.0.post4\n>>> 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \n>>> [GCC 7.2.0]\nWith Tensors it ignores the empty inputs:\nx = FloatTensor(4, 3, 32, 32)\nempty = FloatTensor()\n\nres = torch.cat([x, empty], dim=1)\nprint(res.shape)\n>>> torch.Size([4, 3, 32, 32])\n\nres = torch.cat([empty, x], dim=1)\nprint(res.shape)\n>>> torch.Size([4, 3, 32, 32])\n\nres = torch.cat([empty, empty], dim=1)\nprint(res.shape)\n>>> torch.Size([])\nWith Variables I would expect it to work the same, but...\nx = Variable(x)\nempty = Variable(empty)\nIf the first element in the concat list is not empty it's ok:\nres = torch.cat([x, empty], dim=1)\nprint(res.shape)\n>>> torch.Size([4, 3, 32, 32])\nUnless the first element is the result of an operation, and not a manually created tensor:\nconv = torch.nn.Conv2d(3, 3, kernel_size=1)\nres = torch.cat([conv(x), empty], dim=1)\nprint(res.shape)\n>>> RuntimeError: dimension specified as 1 but tensor has no dimensions\nAlso fails if the empty Variable is first, or with two empty Variables\nres = torch.cat([empty, x], dim=1)\nprint(res.shape)\n>>> RuntimeError: dimension specified as 1 but tensor has no dimensions\n\nres = torch.cat([empty, empty], dim=1)\nprint(res.shape)\n>>> RuntimeError: dimension specified as 1 but tensor has no dimensions\nExpected behavior\ntorch.cat should behave identically with Tensors and Variables, or the difference should be stated in the documentation.\nHaven't tried on a GPU, so I don't know what the behavior is with CUDA operations.\nEnvironment:\n\nOS: Ubuntu 16.04\nPyTorch version: 0.3.0.post4\nHow you installed PyTorch: conda\nPython version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)\nCUDA/cuDNN version: No", "body": "## Context\r\n\r\nI'm experimenting with dense blocks like the ones from DenseNet, where the input of a layer in a dense block is the concatenation of the input and the output of the previous layer, except of course for the very first layer, where only the block input is available.\r\n\r\nIdeally, I would write the forward method of a dense block like this, taking advantage of the fact that the first time `cat` is invoked the empty tensor is simply discarded.\r\n\r\n```python\r\ndef forward(self, block_input):\r\n        layer_input = block_input\r\n        layer_output = torch.autograd.Variable(torch.FloatTensor())  # empty\r\n\r\n        all_outputs = []\r\n        for layer in self._modules.values():\r\n            layer_input = torch.cat([layer_input, layer_output], dim=1)\r\n            layer_output = layer(layer_input)\r\n            all_outputs.append(layer_output)\r\n\r\n        return torch.cat(all_outputs, dim=1)\r\n```\r\n\r\n## Issue\r\n\r\nI discovered that `torch.cat` behaves differently when dealing with empty Tensors and Variables, and in the latter case, it is influenced by the order of the inputs (even weirder)\r\n\r\n## To reproduce\r\n\r\n```python\r\nimport sys\r\nimport torch\r\nfrom torch import FloatTensor\r\nfrom torch.autograd import Variable\r\n\r\nprint(torch.__version__)\r\nprint(sys.version)\r\n>>>0.3.0.post4\r\n>>> 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \r\n>>> [GCC 7.2.0]\r\n```\r\n\r\nWith Tensors it ignores the empty inputs:\r\n\r\n```python\r\nx = FloatTensor(4, 3, 32, 32)\r\nempty = FloatTensor()\r\n\r\nres = torch.cat([x, empty], dim=1)\r\nprint(res.shape)\r\n>>> torch.Size([4, 3, 32, 32])\r\n\r\nres = torch.cat([empty, x], dim=1)\r\nprint(res.shape)\r\n>>> torch.Size([4, 3, 32, 32])\r\n\r\nres = torch.cat([empty, empty], dim=1)\r\nprint(res.shape)\r\n>>> torch.Size([])\r\n```\r\n\r\nWith Variables I would expect it to work the same, but... \r\n```python\r\nx = Variable(x)\r\nempty = Variable(empty)\r\n```\r\n\r\nIf the first element in the concat list is not empty it's ok:\r\n```python\r\nres = torch.cat([x, empty], dim=1)\r\nprint(res.shape)\r\n>>> torch.Size([4, 3, 32, 32])\r\n```\r\n\r\nUnless the first element is the result of an operation, and not a manually created tensor:\r\n```python\r\nconv = torch.nn.Conv2d(3, 3, kernel_size=1)\r\nres = torch.cat([conv(x), empty], dim=1)\r\nprint(res.shape)\r\n>>> RuntimeError: dimension specified as 1 but tensor has no dimensions\r\n```\r\n\r\nAlso fails if the empty Variable is first, or with two empty Variables\r\n```python\r\nres = torch.cat([empty, x], dim=1)\r\nprint(res.shape)\r\n>>> RuntimeError: dimension specified as 1 but tensor has no dimensions\r\n\r\nres = torch.cat([empty, empty], dim=1)\r\nprint(res.shape)\r\n>>> RuntimeError: dimension specified as 1 but tensor has no dimensions\r\n```\r\n\r\n## Expected behavior\r\n\r\n`torch.cat` should behave identically with Tensors and Variables, or the difference should be stated in the documentation.\r\n\r\nHaven't tried on a GPU, so I don't know what the behavior is with CUDA operations.\r\n\r\n## Environment:\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.0.post4\r\n- How you installed PyTorch: conda\r\n- Python version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)\r\n- CUDA/cuDNN version: No"}