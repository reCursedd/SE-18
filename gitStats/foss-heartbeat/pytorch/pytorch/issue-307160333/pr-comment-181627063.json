{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181627063", "pull_request_review_id": 112284953, "id": 181627063, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTYyNzA2Mw==", "diff_hunk": "@@ -0,0 +1,106 @@\n+import torch\n+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n+import torch.distributed as dist\n+from torch.nn.modules import Module\n+from collections import defaultdict\n+from torch.autograd import Variable\n+\n+\n+class DistributedDataParallelCPU(Module):\n+    r\"\"\"Implements distributed data parallelism for CPU at the module level.\n+\n+    This module support the ``mpi``, ``gloo``, ``tcp`` backends.\n+\n+    This container parallelizes the application of the given module by\n+    splitting the input across the specified devices by chunking in the batch\n+    dimension. The module is replicated on each machine, and each such replica\n+    handles a portion of the input. During the backwards pass, gradients from\n+    each node are averaged.\n+\n+    This module could be used in conjunction with the DistributedSampler,\n+    (see :class `torch.utils.data.distributed.DistributedSampler`)\n+    which will load a subset of the original datset for each node with the same\n+    batch size. So strong scaling should be configured like this:\n+        n = 1, batch size = 128\n+        n = 2, batch size = 64\n+        n = 4, batch size = 32\n+        n = 8, batch size = 16\n+\n+    Creation of this class requires the distributed package to be already\n+    initialized in the process group mode\n+    (see :func:`torch.distributed.init_process_group`).\n+\n+    .. warning::\n+        Constructor, forward method, and differentiation of the output (or a\n+        function of the output of this module) is a distributed synchronization\n+        point. Take that into account in case different node might be\n+        executing different code.\n+\n+    .. warning::\n+        This module assumes all parameters are registered in the model by the\n+        time it is created. No parameters should be added nor removed later.\n+\n+    .. warning::\n+        This module assumes all gradients are dense.\n+\n+    .. warning::\n+        This module doesn't work with :func:`torch.autograd.grad` (i.e. it will\n+        only work if gradients are to be accumulated in ``.grad`` attributes of\n+        parameters).\n+\n+    .. note::\n+        Parameters are broadcast between nodes in the __init__() function. The\n+        module performs an all-reduce step on gradients and assumes that they\n+        will be modified by the optimizer in all nodes in the same way.\n+\n+    .. warning::\n+        Forward and backward hooks defined on :attr:`module` and its submodules\n+        won't be invoked anymore, unless the hooks are initialized in the\n+        :meth:`forward` method.\n+\n+    Args:\n+        module: module to be parallelized\n+\n+    Example::\n+\n+        >>> torch.distributed.init_process_group(world_size=4, init_method='...')\n+        >>> net = torch.nn.DistributedDataParallelCPU(model)\n+    \"\"\"\n+\n+    def __init__(self, module):\n+        super(DistributedDataParallelCPU, self).__init__()\n+        self.module = module\n+        self.weight_broadcast()\n+\n+        def allreduce_params():\n+            if self.needs_reduction:\n+                self.needs_reduction = False\n+                buckets = defaultdict(list)\n+                for param in self.module.parameters():\n+                    if param.requires_grad and param.grad is not None:\n+                        tp = type(param.data)\n+                        buckets[tp].append(param)\n+\n+                for bucket in buckets.values():\n+                    grads = [param.grad.data for param in bucket]\n+                    coalesced = _flatten_dense_tensors(grads)\n+                    dist.all_reduce(coalesced)\n+                    coalesced /= dist.get_world_size()\n+                    for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):\n+                        buf.copy_(synced)\n+\n+        for param in list(self.module.parameters()):\n+            def allreduce_hook(*unused):\n+                Variable._execution_engine.queue_callback(allreduce_params)\n+\n+            if param.requires_grad:\n+                param.register_hook(allreduce_hook)\n+\n+    def weight_broadcast(self):", "path": "torch/nn/parallel/distributed_cpu.py", "position": null, "original_position": 99, "commit_id": "d53427571c5d5fce78bf543b58de8a904e93efe3", "original_commit_id": "9235815b3e48231df7f210e8f168e3c1ed646a66", "user": {"login": "xhzhao", "id": 17486215, "node_id": "MDQ6VXNlcjE3NDg2MjE1", "avatar_url": "https://avatars1.githubusercontent.com/u/17486215?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhzhao", "html_url": "https://github.com/xhzhao", "followers_url": "https://api.github.com/users/xhzhao/followers", "following_url": "https://api.github.com/users/xhzhao/following{/other_user}", "gists_url": "https://api.github.com/users/xhzhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhzhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhzhao/subscriptions", "organizations_url": "https://api.github.com/users/xhzhao/orgs", "repos_url": "https://api.github.com/users/xhzhao/repos", "events_url": "https://api.github.com/users/xhzhao/events{/privacy}", "received_events_url": "https://api.github.com/users/xhzhao/received_events", "type": "User", "site_admin": false}, "body": "sure", "created_at": "2018-04-16T05:47:31Z", "updated_at": "2018-11-23T15:42:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/5919#discussion_r181627063", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5919", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181627063"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5919#discussion_r181627063"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5919"}}, "body_html": "<p>sure</p>", "body_text": "sure", "in_reply_to_id": 181596162}