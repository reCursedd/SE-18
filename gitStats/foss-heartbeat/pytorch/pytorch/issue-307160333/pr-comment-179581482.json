{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179581482", "pull_request_review_id": 109852733, "id": 179581482, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU4MTQ4Mg==", "diff_hunk": "@@ -0,0 +1,112 @@\n+import torch\n+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n+import torch.distributed as dist\n+from torch.nn.modules import Module\n+\n+\n+class DistributedDataParallelCPU(Module):\n+    r\"\"\"Implements distributed data parallelism for CPU at the module level.\n+\n+    This container parallelizes the application of the given module by\n+    splitting the input across the specified devices by chunking in the batch\n+    dimension. The module is replicated on each machine, and each such replica\n+    handles a portion of the input. During the backwards pass, gradients from\n+    each node are averaged.\n+\n+    This module should be used in conjunction with the DistributedSampler,\n+    (see :class `torch.utils.data.distributed.DistributedSampler`)\n+    which will load a subset of the original datset for each node with the same\n+    batch size. So weak scaling should be configed like this:\n+        n = 1, batch size = 128\n+        n = 2, batch size = 64\n+        n = 4, batch size = 32\n+        n = 8, batch size = 16\n+\n+    Creation of this class requires the distributed package to be already\n+    initialized in the process group mode\n+    (see :func:`torch.distributed.init_process_group`).\n+\n+    .. warning::\n+        This module works only with the ``mpi`` backends.\n+        The other backends like ``gloo``, ``tcp`` are not tested yet.\n+\n+    .. warning::\n+        Constructor, forward method, and differentiation of the output (or a\n+        function of the output of this module) is a distributed synchronization\n+        point. Take that into account in case different processes might be\n+        executing different code.\n+\n+    .. warning::\n+        This module assumes all parameters are registered in the model by the\n+        time it is created. No parameters should be added nor removed later.\n+\n+    .. warning::\n+        This module assumes all gradients are dense.\n+\n+    .. warning::\n+        This module doesn't work with :func:`torch.autograd.grad` (i.e. it will\n+        only work if gradients are to be accumulated in ``.grad`` attributes of\n+        parameters).\n+\n+    .. note::\n+        Parameters are broadcast between nodes in the first forward. The\n+        module performs an all-reduce step on gradients and assumes that they\n+        will be modified by the optimizer in all processes in the same way.\n+\n+    .. warning::\n+        Forward and backward hooks defined on :attr:`module` and its submodules\n+        won't be invoked anymore, unless the hooks are initialized in the\n+        :meth:`forward` method.\n+\n+    Args:\n+        module: module to be parallelized\n+\n+    Example::\n+\n+        >>> torch.distributed.init_process_group(world_size=4, init_method='...')\n+        >>> net = torch.nn.DistributedDataParallelCPU(model)\n+    \"\"\"\n+\n+    def __init__(self, module):\n+        super(DistributedDataParallelCPU, self).__init__()\n+        self.module = module\n+        self.first_call = True\n+\n+        def allreduce_params():\n+            if (self.needs_reduction):\n+                self.needs_reduction = False\n+                buckets = {}\n+                for param in self.module.parameters():\n+                    if param.requires_grad and param.grad is not None:\n+                        tp = type(param.data)\n+                        if tp not in buckets:\n+                            buckets[tp] = []", "path": "torch/nn/parallel/distributed_cpu.py", "position": null, "original_position": 83, "commit_id": "d53427571c5d5fce78bf543b58de8a904e93efe3", "original_commit_id": "ef3645c554659be3153a9ca89281946714442089", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "nit: please make `buckets` a `defaultdict(list)`", "created_at": "2018-04-05T19:50:50Z", "updated_at": "2018-11-23T15:41:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/5919#discussion_r179581482", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5919", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179581482"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5919#discussion_r179581482"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5919"}}, "body_html": "<p>nit: please make <code>buckets</code> a <code>defaultdict(list)</code></p>", "body_text": "nit: please make buckets a defaultdict(list)"}