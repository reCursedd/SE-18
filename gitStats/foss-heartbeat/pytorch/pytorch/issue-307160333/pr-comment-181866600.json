{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181866600", "pull_request_review_id": 112566129, "id": 181866600, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTg2NjYwMA==", "diff_hunk": "@@ -843,6 +843,78 @@ def assert_equal_param(param_gpu, param_DDP):\n \n         self._barrier()\n \n+    @unittest.skipIf(BACKEND == 'nccl', \"nccl does not support DistributedDataParallelCPU\")\n+    def test_DistributedDataParallelCPU(self):\n+        # Run a simple end to end DDP-CPU model, use result of single node\n+        # model as baseline\n+        group, group_id, rank = self._init_global_test()\n+\n+        class Net(nn.Module):\n+            def __init__(self):\n+                super(Net, self).__init__()\n+                self.fc1 = nn.Linear(2, 10, bias=False)\n+                self.fc2 = nn.Linear(10, 50, bias=False)\n+                self.fc3 = nn.Linear(50, 4, bias=False)\n+                self.relu = nn.ReLU()\n+\n+            def forward(self, x):\n+                x = self.relu(self.fc1(x))\n+                x = self.relu(self.fc2(x))\n+                x = self.fc3(x)\n+                return F.softmax(x, dim=1)\n+\n+        def model_step(model):\n+            for param in model.parameters():\n+                param.data += param.grad\n+                param.grad = None\n+\n+        def assert_equal_param(param_gpu, param_DDP):\n+            self.assertEqual(len(param_gpu), len(param_DDP))\n+            for p_gpu, p_DDP in zip(param_gpu, param_DDP):\n+                self.assertEqual(p_gpu, p_DDP)\n+\n+        # cpu training setup\n+        model = Net()\n+\n+        # single cpu training setup\n+        model_single = copy.deepcopy(model)\n+\n+        # DDP-MPI training setup\n+        model_DDP = copy.deepcopy(model)\n+        model_DDP = nn.parallel.DistributedDataParallelCPU(model_DDP)\n+\n+        # batch_size for DDP should be divisible by WORLD_SIZE\n+        local_bs = 10\n+        global_bs = int(WORLD_SIZE) * local_bs\n+        input_cpu = torch.randn(global_bs, 2)\n+        target = torch.randn(global_bs, 4)\n+        loss = nn.MSELoss()\n+\n+        for i in range(2):\n+            # single cpu training\n+            self._test_DDP_helper(model_single,\n+                                  input_cpu,\n+                                  target,\n+                                  loss)\n+\n+            # DDP training, DDP scatters subsets of input_cpu to nodes/GPUs\n+            self._test_DDP_helper(model_DDP,\n+                                  input_cpu[rank * local_bs: (rank + 1) * local_bs],\n+                                  target[rank * local_bs: (rank + 1) * local_bs],\n+                                  loss)\n+\n+            # Update weights and run a second iteration to shake out errors\n+            model_step(model_single)\n+            model_step(model_DDP)\n+\n+            assert_equal_param(list(model_single.parameters()), list(model_DDP.module.parameters()))\n+\n+            # Shuffle the input so that DDP input is different\n+            input_cpu = input_cpu[torch.randperm(global_bs)]\n+\n+        self._barrier()", "path": "test/test_distributed.py", "position": null, "original_position": 73, "commit_id": "d53427571c5d5fce78bf543b58de8a904e93efe3", "original_commit_id": "9235815b3e48231df7f210e8f168e3c1ed646a66", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "That's a step in a good direction, but I think there's still a ton of shared code. The loop is exactly the same, except that you replaced `len(gpu_subset)` with `local_bs`. You even have a comment where you mention GPUs.", "created_at": "2018-04-16T20:00:30Z", "updated_at": "2018-11-23T15:42:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/5919#discussion_r181866600", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5919", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181866600"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5919#discussion_r181866600"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5919"}}, "body_html": "<p>That's a step in a good direction, but I think there's still a ton of shared code. The loop is exactly the same, except that you replaced <code>len(gpu_subset)</code> with <code>local_bs</code>. You even have a comment where you mention GPUs.</p>", "body_text": "That's a step in a good direction, but I think there's still a ton of shared code. The loop is exactly the same, except that you replaced len(gpu_subset) with local_bs. You even have a comment where you mention GPUs.", "in_reply_to_id": 181596051}