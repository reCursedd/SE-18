{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/401658437", "html_url": "https://github.com/pytorch/pytorch/issues/9068#issuecomment-401658437", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9068", "id": 401658437, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTY1ODQzNw==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-02T02:59:29Z", "updated_at": "2018-07-02T02:59:29Z", "author_association": "MEMBER", "body_html": "<p>In 1.0, for server-side deployment, we will skip the middleman, any operators that are not supported in ONNX will still be exported via PyTorch's IR. And we can read the model from <code>libtorch</code> (PyTorch's C++ bits). This gives full coverage of all models that are defined via the PyTorch API.</p>\n<p>For mobile, the standardized backends are becoming CoreML and NNAPI, at the moment it didn't make sense to compete with the standard for the platform for 1.0. The bottleneck (from my cursory understanding) isn't whether it's supported in ONNX, but whether it's supported in CoreML. That being said, dont get me wrong, there's actually quite some work being done to make the PyTorch -&gt; ONNX exporting have much wider coverage (to cover many more model definitions), and we plan to add convenience utilities, boilerplate and showcase repositories to go from one end to another (without needing to be aware of the onnx middleman)</p>", "body_text": "In 1.0, for server-side deployment, we will skip the middleman, any operators that are not supported in ONNX will still be exported via PyTorch's IR. And we can read the model from libtorch (PyTorch's C++ bits). This gives full coverage of all models that are defined via the PyTorch API.\nFor mobile, the standardized backends are becoming CoreML and NNAPI, at the moment it didn't make sense to compete with the standard for the platform for 1.0. The bottleneck (from my cursory understanding) isn't whether it's supported in ONNX, but whether it's supported in CoreML. That being said, dont get me wrong, there's actually quite some work being done to make the PyTorch -> ONNX exporting have much wider coverage (to cover many more model definitions), and we plan to add convenience utilities, boilerplate and showcase repositories to go from one end to another (without needing to be aware of the onnx middleman)", "body": "In 1.0, for server-side deployment, we will skip the middleman, any operators that are not supported in ONNX will still be exported via PyTorch's IR. And we can read the model from `libtorch` (PyTorch's C++ bits). This gives full coverage of all models that are defined via the PyTorch API.\r\n\r\nFor mobile, the standardized backends are becoming CoreML and NNAPI, at the moment it didn't make sense to compete with the standard for the platform for 1.0. The bottleneck (from my cursory understanding) isn't whether it's supported in ONNX, but whether it's supported in CoreML. That being said, dont get me wrong, there's actually quite some work being done to make the PyTorch -> ONNX exporting have much wider coverage (to cover many more model definitions), and we plan to add convenience utilities, boilerplate and showcase repositories to go from one end to another (without needing to be aware of the onnx middleman)"}