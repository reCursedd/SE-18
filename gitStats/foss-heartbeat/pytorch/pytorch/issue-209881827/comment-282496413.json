{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/282496413", "html_url": "https://github.com/pytorch/pytorch/issues/839#issuecomment-282496413", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/839", "id": 282496413, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjQ5NjQxMw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-25T16:54:36Z", "updated_at": "2017-02-25T16:54:43Z", "author_association": "MEMBER", "body_html": "<p>Ok, so it seems that it's not really a memory leak, but a reference cycle. You're both modifying the leaf Variable in-place so the graph ends up looking like <code>init &lt;- MulConstant &lt;- init</code>, and nothing will be freed until the gc kicks in. If you only modify the loop to look like that:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000</span>):\n    init <span class=\"pl-k\">=</span> Variable(torch.cuda.FloatTensor(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">512</span>).zero_())\n    init <span class=\"pl-k\">=</span> init <span class=\"pl-k\">+</span> <span class=\"pl-c1\">0</span>\n    init <span class=\"pl-k\">*=</span> <span class=\"pl-c1\">2</span></pre></div>\n<p>the memory usage will remain constant.</p>", "body_text": "Ok, so it seems that it's not really a memory leak, but a reference cycle. You're both modifying the leaf Variable in-place so the graph ends up looking like init <- MulConstant <- init, and nothing will be freed until the gc kicks in. If you only modify the loop to look like that:\nfor _ in range(100000):\n    init = Variable(torch.cuda.FloatTensor(2, 32, 512).zero_())\n    init = init + 0\n    init *= 2\nthe memory usage will remain constant.", "body": "Ok, so it seems that it's not really a memory leak, but a reference cycle. You're both modifying the leaf Variable in-place so the graph ends up looking like `init <- MulConstant <- init`, and nothing will be freed until the gc kicks in. If you only modify the loop to look like that:\r\n```python\r\nfor _ in range(100000):\r\n    init = Variable(torch.cuda.FloatTensor(2, 32, 512).zero_())\r\n    init = init + 0\r\n    init *= 2\r\n```\r\nthe memory usage will remain constant."}