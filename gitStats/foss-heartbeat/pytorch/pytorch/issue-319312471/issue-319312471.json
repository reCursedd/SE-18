{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7151", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7151/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7151/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7151/events", "html_url": "https://github.com/pytorch/pytorch/pull/7151", "id": 319312471, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg1MjY0OTQz", "number": 7151, "title": "Fix ReduceLROnPlateau's patience mechanic.", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-01T20:15:45Z", "updated_at": "2018-05-03T15:06:35Z", "closed_at": "2018-05-03T15:06:35Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7151", "html_url": "https://github.com/pytorch/pytorch/pull/7151", "diff_url": "https://github.com/pytorch/pytorch/pull/7151.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7151.patch"}, "body_html": "<p>Let's say patience = 1 and the losses are:<br>\n[1.0, 1.1, 1.2, ..., ] (we've hit a plateau). Let's also say the losses<br>\neach come at one unit per time (at the <code>t = 0</code>, we receive losses[0], etc).</p>\n<p><code>self.patience</code> is 1.</p>\n<p>at <code>t=0</code>:</p>\n<ul>\n<li><code>self.num_bad_epochs</code> is 0.</li>\n<li>The epoch is better than the last one (which has a \"loss value\" of<br>\n<code>self.mode_worst</code>. <code>self.num_bad_epochs</code> gets set to 0.</li>\n</ul>\n<p>at <code>t=1</code>:</p>\n<ul>\n<li><code>self.num_bad_epochs</code> is 0.</li>\n<li>The epoch is not better than the last one (which had a \"loss value\" of<br>\n<code>1.0</code>). <code>self.num_bad_epochs</code> gets set to 1.</li>\n<li>The LR isn't reduced, because the check<br>\n(<code>self.num_bad_epochs &gt; self.patience</code>) is false</li>\n</ul>\n<p>at <code>t=2</code></p>\n<ul>\n<li><code>self.num_bad_epochs</code> is 0.</li>\n<li>The epoch is not better than the last one (which had a \"loss value\" of<br>\n<code>1.1</code>). <code>self.num_bad_epochs</code> gets set to 2.</li>\n<li>The LR is reduced, because the check (<code>self.num_bad_epochs &gt; self.patience</code>)<br>\nis true (2 &gt; 1)</li>\n</ul>\n<p>However, I would expect that at <code>t=1</code> the LR is reduced because, in the<br>\nwords of the documentation, \"Number of epochs with no improvement after<br>\nwhich learning rate will be reduced\" implies that with <code>patience = 1</code>,<br>\nafter one epoch with no improvement, the LR should be reduced.</p>\n<p>Bug was reported by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5520155\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/W4ngatang\">@W4ngatang</a>.</p>\n<p>If we thing this fix is good (this might break user code, so it might be better to just change our documentation?) I'll add a test. I'm not sure what the best way to test this is; I could try mocking the optimizer.</p>", "body_text": "Let's say patience = 1 and the losses are:\n[1.0, 1.1, 1.2, ..., ] (we've hit a plateau). Let's also say the losses\neach come at one unit per time (at the t = 0, we receive losses[0], etc).\nself.patience is 1.\nat t=0:\n\nself.num_bad_epochs is 0.\nThe epoch is better than the last one (which has a \"loss value\" of\nself.mode_worst. self.num_bad_epochs gets set to 0.\n\nat t=1:\n\nself.num_bad_epochs is 0.\nThe epoch is not better than the last one (which had a \"loss value\" of\n1.0). self.num_bad_epochs gets set to 1.\nThe LR isn't reduced, because the check\n(self.num_bad_epochs > self.patience) is false\n\nat t=2\n\nself.num_bad_epochs is 0.\nThe epoch is not better than the last one (which had a \"loss value\" of\n1.1). self.num_bad_epochs gets set to 2.\nThe LR is reduced, because the check (self.num_bad_epochs > self.patience)\nis true (2 > 1)\n\nHowever, I would expect that at t=1 the LR is reduced because, in the\nwords of the documentation, \"Number of epochs with no improvement after\nwhich learning rate will be reduced\" implies that with patience = 1,\nafter one epoch with no improvement, the LR should be reduced.\nBug was reported by @W4ngatang.\nIf we thing this fix is good (this might break user code, so it might be better to just change our documentation?) I'll add a test. I'm not sure what the best way to test this is; I could try mocking the optimizer.", "body": "Let's say patience = 1 and the losses are:\r\n[1.0, 1.1, 1.2, ..., ] (we've hit a plateau). Let's also say the losses\r\neach come at one unit per time (at the `t = 0`, we receive losses[0], etc).\r\n\r\n`self.patience` is 1.\r\n\r\nat `t=0`:\r\n- `self.num_bad_epochs` is 0.\r\n- The epoch is better than the last one (which has a \"loss value\" of\r\n  `self.mode_worst`. `self.num_bad_epochs` gets set to 0.\r\n\r\nat `t=1`:\r\n- `self.num_bad_epochs` is 0.\r\n- The epoch is not better than the last one (which had a \"loss value\" of\r\n  `1.0`). `self.num_bad_epochs` gets set to 1.\r\n- The LR isn't reduced, because the check\r\n  (`self.num_bad_epochs > self.patience`) is false\r\n\r\nat `t=2`\r\n- `self.num_bad_epochs` is 0.\r\n- The epoch is not better than the last one (which had a \"loss value\" of\r\n  `1.1`). `self.num_bad_epochs` gets set to 2.\r\n- The LR is reduced, because the check (`self.num_bad_epochs > self.patience`)\r\n  is true (2 > 1)\r\n\r\nHowever, I would expect that at `t=1` the LR is reduced because, in the\r\nwords of the documentation, \"Number of epochs with no improvement after\r\nwhich learning rate will be reduced\" implies that with `patience = 1`,\r\nafter one epoch with no improvement, the LR should be reduced.\r\n\r\nBug was reported by @W4ngatang.\r\n\r\nIf we thing this fix is good (this might break user code, so it might be better to just change our documentation?) I'll add a test. I'm not sure what the best way to test this is; I could try mocking the optimizer.\r\n\r\n"}