{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210427929", "pull_request_review_id": 146641898, "id": 210427929, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDQyNzkyOQ==", "diff_hunk": "@@ -47,6 +47,81 @@ std::tuple<Tensor, Tensor> _unique_cpu_template(\n   }\n   return std::make_tuple(output, inverse_indices);\n }\n+\n+template <typename scalar_t>\n+std::tuple<Tensor, Tensor> _unique_dim_cpu_template(\n+    const Tensor& self,\n+    const int64_t dim,\n+    const bool return_inverse) {\n+  // reshape tensor as [dim, -1]\n+  Tensor input_flat = self.transpose(dim, 0);\n+  std::vector<int64_t> orig_sizes(input_flat.sizes().begin(), input_flat.sizes().end());\n+  input_flat = input_flat.contiguous().view({input_flat.size(0), -1});\n+\n+  std::vector<int64_t> indices(input_flat.size(0));\n+  std::iota(indices.begin(), indices.end(), 0);\n+  int64_t numel = input_flat.size(1);\n+  scalar_t* input_flat_ptr = ((scalar_t*)input_flat.data_ptr());\n+\n+  // sort indices using data\n+  std::sort(indices.begin(), indices.end(),\n+    [&](int64_t a, int64_t b) -> bool {\n+      for (int64_t i = 0; i < numel; ++i) {\n+        scalar_t lhs = input_flat_ptr[i + a * numel];\n+        scalar_t rhs = input_flat_ptr[i + b * numel];\n+        if (lhs < rhs) {\n+          return true;\n+        } else if (lhs > rhs) {\n+          return false;\n+        }\n+      }\n+      return false;\n+    });\n+\n+  Tensor input_sorted = at::empty(input_flat.sizes(), input_flat.type());\n+  for (int i = 0; i < indices.size(); ++i) {\n+    input_sorted[i] = input_flat[indices[i]];\n+  }\n+ \n+  // pre-calculate mask for inverse_indices\n+  Tensor mask = at::empty(input_sorted.size(0), self.type().toScalarType(kLong));\n+  mask[0] = 1;\n+  int mask_idx = 1;\n+\n+  std::vector<Tensor> input_unbind = at::unbind(input_sorted, 0);\n+  auto last = std::unique(input_unbind.begin(), input_unbind.end(), [&](Tensor a, Tensor b) {", "path": "aten/src/ATen/native/Unique.cpp", "position": null, "original_position": 46, "commit_id": "5a8ecf2e7c59e42c089cd3d41a66041990730b1b", "original_commit_id": "cbafa5b2b3fcaa704fd43c92a57d00a2e23114cb", "user": {"login": "ptrblck", "id": 11662379, "node_id": "MDQ6VXNlcjExNjYyMzc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11662379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptrblck", "html_url": "https://github.com/ptrblck", "followers_url": "https://api.github.com/users/ptrblck/followers", "following_url": "https://api.github.com/users/ptrblck/following{/other_user}", "gists_url": "https://api.github.com/users/ptrblck/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptrblck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptrblck/subscriptions", "organizations_url": "https://api.github.com/users/ptrblck/orgs", "repos_url": "https://api.github.com/users/ptrblck/repos", "events_url": "https://api.github.com/users/ptrblck/events{/privacy}", "received_events_url": "https://api.github.com/users/ptrblck/received_events", "type": "User", "site_admin": false}, "body": "Makes sense!\r\n\r\nWhat do you think about the CUDA version? We are using a lambda function there as well.\r\nShould it be refactored hand in hand with the CPU version?", "created_at": "2018-08-15T22:20:52Z", "updated_at": "2018-11-23T15:49:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/10423#discussion_r210427929", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10423", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210427929"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10423#discussion_r210427929"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10423"}}, "body_html": "<p>Makes sense!</p>\n<p>What do you think about the CUDA version? We are using a lambda function there as well.<br>\nShould it be refactored hand in hand with the CPU version?</p>", "body_text": "Makes sense!\nWhat do you think about the CUDA version? We are using a lambda function there as well.\nShould it be refactored hand in hand with the CPU version?", "in_reply_to_id": 209964206}