{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210427100", "pull_request_review_id": 146640981, "id": 210427100, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDQyNzEwMA==", "diff_hunk": "@@ -69,6 +70,92 @@ template <typename scalar_t>\n     return std::tuple<Tensor, Tensor>(output, inverse_indices);\n \n   }\n+\n+template <typename scalar_t>\n+  std::tuple<Tensor, Tensor> _unique_dim_cuda_template(\n+    const Tensor& self,\n+    const int64_t dim,\n+    const bool return_inverse) {\n+\n+    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+    auto allocator = THCThrustAllocator(globalContext().lazyInitCUDA());\n+    auto policy = thrust::cuda::par(allocator).on(stream);\n+\n+    Tensor input_flat = self.transpose(dim, 0);\n+    auto orig_sizes = input_flat.sizes().vec();\n+    input_flat = input_flat.contiguous().view({input_flat.size(0), -1});\n+\n+    scalar_t* input_flat_ptr = input_flat.data<scalar_t>();\n+\n+    Tensor indices = at::arange(0, input_flat.size(0), self.type().toScalarType(kLong));\n+    int64_t* indices_ptr = indices.data<int64_t>();\n+    int64_t numel = input_flat.size(1);\n+\n+    // sort indices using data\n+    thrust::sort(policy, indices_ptr, indices_ptr + indices.numel(),\n+      [=] __device__ (int64_t a, int64_t b) -> bool {\n+        for (int64_t i = 0; i < numel; ++i) {\n+          scalar_t lhs = input_flat_ptr[i + a * numel];\n+          scalar_t rhs = input_flat_ptr[i + b * numel];\n+          if (lhs < rhs) {\n+            return true;\n+          } else if (lhs > rhs) {\n+            return false;\n+          }\n+        }\n+        return false;\n+      });\n+\n+    Tensor input_sorted = input_flat.index_select(0, indices);\n+\n+    // get unique tensors\n+    scalar_t* input_sorted_ptr = input_sorted.data<scalar_t>();    \n+    Tensor input_sorted_indices = at::arange(0, input_sorted.size(0), self.type().toScalarType(kLong));\n+    int64_t* input_sorted_indices_ptr = input_sorted_indices.data<int64_t>();\n+    auto last = thrust::unique(policy, input_sorted_indices_ptr, input_sorted_indices_ptr + input_sorted_indices.numel(),\n+      [=] __device__ (int64_t a, int64_t b) -> bool {\n+        for (int64_t i = 0; i < numel; ++i) {\n+          scalar_t lhs = input_sorted_ptr[i + a * numel];\n+          scalar_t rhs = input_sorted_ptr[i + b * numel];\n+          if (lhs != rhs) {\n+            return false;\n+          }\n+        }\n+        return true;\n+      });\n+    input_sorted_indices.resize_(last - input_sorted_indices_ptr);\n+    Tensor output = input_sorted.index_select(0, input_sorted_indices);\n+\n+    // // reshape back\n+    std::vector<int64_t> new_sizes(orig_sizes.begin(), orig_sizes.end());", "path": "aten/src/ATen/native/cuda/Unique.cu", "position": null, "original_position": 69, "commit_id": "5a8ecf2e7c59e42c089cd3d41a66041990730b1b", "original_commit_id": "cbafa5b2b3fcaa704fd43c92a57d00a2e23114cb", "user": {"login": "ptrblck", "id": 11662379, "node_id": "MDQ6VXNlcjExNjYyMzc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11662379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptrblck", "html_url": "https://github.com/ptrblck", "followers_url": "https://api.github.com/users/ptrblck/followers", "following_url": "https://api.github.com/users/ptrblck/following{/other_user}", "gists_url": "https://api.github.com/users/ptrblck/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptrblck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptrblck/subscriptions", "organizations_url": "https://api.github.com/users/ptrblck/orgs", "repos_url": "https://api.github.com/users/ptrblck/repos", "events_url": "https://api.github.com/users/ptrblck/events{/privacy}", "received_events_url": "https://api.github.com/users/ptrblck/received_events", "type": "User", "site_admin": false}, "body": "That won't probably work, as `orig_sizes` is already a `std::vector`. \r\nHowever that's a good catch nonetheless, since I can just manipulate `orig_sizes` and use it to resize `output`.\r\nI will just drop `new_sizes` completely in the CPU and CUDA version.", "created_at": "2018-08-15T22:17:08Z", "updated_at": "2018-11-23T15:49:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/10423#discussion_r210427100", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10423", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210427100"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10423#discussion_r210427100"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10423"}}, "body_html": "<p>That won't probably work, as <code>orig_sizes</code> is already a <code>std::vector</code>.<br>\nHowever that's a good catch nonetheless, since I can just manipulate <code>orig_sizes</code> and use it to resize <code>output</code>.<br>\nI will just drop <code>new_sizes</code> completely in the CPU and CUDA version.</p>", "body_text": "That won't probably work, as orig_sizes is already a std::vector.\nHowever that's a good catch nonetheless, since I can just manipulate orig_sizes and use it to resize output.\nI will just drop new_sizes completely in the CPU and CUDA version.", "in_reply_to_id": 209757401}