{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355716511", "html_url": "https://github.com/pytorch/pytorch/issues/4468#issuecomment-355716511", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4468", "id": 355716511, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTcxNjUxMQ==", "user": {"login": "deepbrain", "id": 10003025, "node_id": "MDQ6VXNlcjEwMDAzMDI1", "avatar_url": "https://avatars3.githubusercontent.com/u/10003025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepbrain", "html_url": "https://github.com/deepbrain", "followers_url": "https://api.github.com/users/deepbrain/followers", "following_url": "https://api.github.com/users/deepbrain/following{/other_user}", "gists_url": "https://api.github.com/users/deepbrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepbrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepbrain/subscriptions", "organizations_url": "https://api.github.com/users/deepbrain/orgs", "repos_url": "https://api.github.com/users/deepbrain/repos", "events_url": "https://api.github.com/users/deepbrain/events{/privacy}", "received_events_url": "https://api.github.com/users/deepbrain/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-06T02:27:12Z", "updated_at": "2018-01-06T02:27:12Z", "author_association": "NONE", "body_html": "<p>I am still getting the same error with the new code. Here is a simple test case, if you set the GPU=0 all works fine, when GPU=1 I get the error:</p>\n<p>RuntimeError: arguments are located on different GPUs at ../pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:236</p>\n<p>if I comment out @torch.jit.compile(nderivs=1), no error is thrown.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> jit\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\n<span class=\"pl-en\">@torch.jit.compile</span>(<span class=\"pl-v\">nderivs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TestNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(TestNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.net1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">200</span>)\n        <span class=\"pl-c1\">self</span>.net2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.sigmoid <span class=\"pl-k\">=</span> nn.Sigmoid()\n        <span class=\"pl-c1\">self</span>.ReLU <span class=\"pl-k\">=</span> nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.drop <span class=\"pl-k\">=</span> nn.Dropout(<span class=\"pl-c1\">0.5</span>)\n<span class=\"pl-c1\">...............</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">V</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.sigmoid(<span class=\"pl-c1\">self</span>.net2(<span class=\"pl-c1\">self</span>.drop(<span class=\"pl-c1\">self</span>.ReLU(<span class=\"pl-c1\">self</span>.net1(V))))).squeeze().\n\n\nuse_cuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n<span class=\"pl-c1\">GPU</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nnet <span class=\"pl-k\">=</span> TestNet()\ncriterion <span class=\"pl-k\">=</span> nn.BCELoss()\n<span class=\"pl-k\">if</span> use_cuda:\n    net.cuda(<span class=\"pl-c1\">GPU</span>)\n    criterion.cuda(<span class=\"pl-c1\">GPU</span>)\n    V <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)).cuda(<span class=\"pl-c1\">GPU</span>)\n    label <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>)).cuda(<span class=\"pl-c1\">GPU</span>)\n<span class=\"pl-k\">else</span>:\n    V <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>))\n    label <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>))\n\nnet.train()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1000000</span>):\n    r <span class=\"pl-k\">=</span> net(V)\n    err <span class=\"pl-k\">=</span> criterion(r, label)\n    err.backward()<span class=\"pl-c1\">...</span>\n</pre></div>", "body_text": "I am still getting the same error with the new code. Here is a simple test case, if you set the GPU=0 all works fine, when GPU=1 I get the error:\nRuntimeError: arguments are located on different GPUs at ../pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:236\nif I comment out @torch.jit.compile(nderivs=1), no error is thrown.\nimport torch\nfrom torch import jit\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n@torch.jit.compile(nderivs=1)\nclass TestNet(nn.Module):\n    def __init__(self):\n        super(TestNet, self).__init__()\n        self.net1 = nn.Linear(100, 200)\n        self.net2 = nn.Linear(200, 1)\n        self.sigmoid = nn.Sigmoid()\n        self.ReLU = nn.ReLU(inplace=False)\n        self.drop = nn.Dropout(0.5)\n...............\n    def forward(self, V):\n        return self.sigmoid(self.net2(self.drop(self.ReLU(self.net1(V))))).squeeze().\n\n\nuse_cuda = True\nGPU = 1\nnet = TestNet()\ncriterion = nn.BCELoss()\nif use_cuda:\n    net.cuda(GPU)\n    criterion.cuda(GPU)\n    V = Variable(torch.randn(100, 100)).cuda(GPU)\n    label = Variable(torch.randn(100)).cuda(GPU)\nelse:\n    V = Variable(torch.randn(100, 100))\n    label = Variable(torch.randn(100))\n\nnet.train()\nfor i in range(0,1000000):\n    r = net(V)\n    err = criterion(r, label)\n    err.backward()...", "body": "I am still getting the same error with the new code. Here is a simple test case, if you set the GPU=0 all works fine, when GPU=1 I get the error:\r\n\r\nRuntimeError: arguments are located on different GPUs at ../pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:236\r\n\r\nif I comment out @torch.jit.compile(nderivs=1), no error is thrown.\r\n\r\n```python\r\nimport torch\r\nfrom torch import jit\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\n@torch.jit.compile(nderivs=1)\r\nclass TestNet(nn.Module):\r\n    def __init__(self):\r\n        super(TestNet, self).__init__()\r\n        self.net1 = nn.Linear(100, 200)\r\n        self.net2 = nn.Linear(200, 1)\r\n        self.sigmoid = nn.Sigmoid()\r\n        self.ReLU = nn.ReLU(inplace=False)\r\n        self.drop = nn.Dropout(0.5)\r\n...............\r\n    def forward(self, V):\r\n        return self.sigmoid(self.net2(self.drop(self.ReLU(self.net1(V))))).squeeze().\r\n\r\n\r\nuse_cuda = True\r\nGPU = 1\r\nnet = TestNet()\r\ncriterion = nn.BCELoss()\r\nif use_cuda:\r\n    net.cuda(GPU)\r\n    criterion.cuda(GPU)\r\n    V = Variable(torch.randn(100, 100)).cuda(GPU)\r\n    label = Variable(torch.randn(100)).cuda(GPU)\r\nelse:\r\n    V = Variable(torch.randn(100, 100))\r\n    label = Variable(torch.randn(100))\r\n\r\nnet.train()\r\nfor i in range(0,1000000):\r\n    r = net(V)\r\n    err = criterion(r, label)\r\n    err.backward()...\r\n\r\n```\r\n"}