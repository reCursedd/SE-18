{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7944", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7944/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7944/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7944/events", "html_url": "https://github.com/pytorch/pytorch/issues/7944", "id": 327569088, "node_id": "MDU6SXNzdWUzMjc1NjkwODg=", "number": 7944, "title": "Better error message in DataChannelTCP::_receive", "user": {"login": "sethah", "id": 7275795, "node_id": "MDQ6VXNlcjcyNzU3OTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7275795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sethah", "html_url": "https://github.com/sethah", "followers_url": "https://api.github.com/users/sethah/followers", "following_url": "https://api.github.com/users/sethah/following{/other_user}", "gists_url": "https://api.github.com/users/sethah/gists{/gist_id}", "starred_url": "https://api.github.com/users/sethah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sethah/subscriptions", "organizations_url": "https://api.github.com/users/sethah/orgs", "repos_url": "https://api.github.com/users/sethah/repos", "events_url": "https://api.github.com/users/sethah/events{/privacy}", "received_events_url": "https://api.github.com/users/sethah/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-30T04:21:34Z", "updated_at": "2018-05-30T08:49:09Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>The following code will produce an error from the TCP distributed backend:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> sys\n\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MASTER_ADDR<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>127.0.0.1<span class=\"pl-pds\">'</span></span>\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MASTER_PORT<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>29500<span class=\"pl-pds\">'</span></span>\ndist.init_process_group(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tcp<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">rank</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(sys.argv[<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\n\n<span class=\"pl-k\">if</span> dist.get_rank() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n\tt <span class=\"pl-k\">=</span> torch.arange(<span class=\"pl-c1\">9</span>)\n\tdist.send(<span class=\"pl-v\">tensor</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">dst</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">else</span>:\n\tt <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">9</span>)\n\tdist.recv(<span class=\"pl-v\">tensor</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">src</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)</pre></div>\n<p>Error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/Users/shendrickson/pytorch_dist.py\", line 19, in &lt;module&gt;\n    dist.recv(tensor=t, src=0)\n  File \"/Users/shendrickson/anaconda2/envs/torchdev/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 230, in recv\n    return torch._C._dist_recv(tensor, src)\nRuntimeError: Tensor sizes do not match\n</code></pre>\n<p>This is a bit misleading, since the sizes of the tensors are the same but the types are different. I think it would be better to give a message along the lines of:</p>\n<pre><code>RuntimeError: Expected to receive 72 bytes, but got 36 bytes instead. Are tensors of same size and type?\n</code></pre>\n<p>I'd be happy to submit a PR for this if others agree that the original message should be changed.</p>", "body_text": "Issue description\nThe following code will produce an error from the TCP distributed backend:\nimport os\nimport torch\nimport torch.distributed as dist\nimport sys\n\nos.environ['MASTER_ADDR'] = '127.0.0.1'\nos.environ['MASTER_PORT'] = '29500'\ndist.init_process_group('tcp', rank=int(sys.argv[1]), world_size=2)\n\n\nif dist.get_rank() == 0:\n\tt = torch.arange(9)\n\tdist.send(tensor=t, dst=1)\nelse:\n\tt = torch.zeros(9)\n\tdist.recv(tensor=t, src=0)\nError:\nTraceback (most recent call last):\n  File \"/Users/shendrickson/pytorch_dist.py\", line 19, in <module>\n    dist.recv(tensor=t, src=0)\n  File \"/Users/shendrickson/anaconda2/envs/torchdev/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 230, in recv\n    return torch._C._dist_recv(tensor, src)\nRuntimeError: Tensor sizes do not match\n\nThis is a bit misleading, since the sizes of the tensors are the same but the types are different. I think it would be better to give a message along the lines of:\nRuntimeError: Expected to receive 72 bytes, but got 36 bytes instead. Are tensors of same size and type?\n\nI'd be happy to submit a PR for this if others agree that the original message should be changed.", "body": "## Issue description\r\n\r\nThe following code will produce an error from the TCP distributed backend:\r\n\r\n```python\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nimport sys\r\n\r\nos.environ['MASTER_ADDR'] = '127.0.0.1'\r\nos.environ['MASTER_PORT'] = '29500'\r\ndist.init_process_group('tcp', rank=int(sys.argv[1]), world_size=2)\r\n\r\n\r\nif dist.get_rank() == 0:\r\n\tt = torch.arange(9)\r\n\tdist.send(tensor=t, dst=1)\r\nelse:\r\n\tt = torch.zeros(9)\r\n\tdist.recv(tensor=t, src=0)\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/shendrickson/pytorch_dist.py\", line 19, in <module>\r\n    dist.recv(tensor=t, src=0)\r\n  File \"/Users/shendrickson/anaconda2/envs/torchdev/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 230, in recv\r\n    return torch._C._dist_recv(tensor, src)\r\nRuntimeError: Tensor sizes do not match\r\n```\r\n\r\nThis is a bit misleading, since the sizes of the tensors are the same but the types are different. I think it would be better to give a message along the lines of:\r\n\r\n```\r\nRuntimeError: Expected to receive 72 bytes, but got 36 bytes instead. Are tensors of same size and type?\r\n```\r\n\r\nI'd be happy to submit a PR for this if others agree that the original message should be changed."}