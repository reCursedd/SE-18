{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1546", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1546/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1546/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1546/events", "html_url": "https://github.com/pytorch/pytorch/issues/1546", "id": 228322088, "node_id": "MDU6SXNzdWUyMjgzMjIwODg=", "number": 1546, "title": "Numerical instability in log_softmax", "user": {"login": "aosokin", "id": 2099291, "node_id": "MDQ6VXNlcjIwOTkyOTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2099291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aosokin", "html_url": "https://github.com/aosokin", "followers_url": "https://api.github.com/users/aosokin/followers", "following_url": "https://api.github.com/users/aosokin/following{/other_user}", "gists_url": "https://api.github.com/users/aosokin/gists{/gist_id}", "starred_url": "https://api.github.com/users/aosokin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aosokin/subscriptions", "organizations_url": "https://api.github.com/users/aosokin/orgs", "repos_url": "https://api.github.com/users/aosokin/repos", "events_url": "https://api.github.com/users/aosokin/events{/privacy}", "received_events_url": "https://api.github.com/users/aosokin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-12T15:11:02Z", "updated_at": "2017-05-12T15:44:45Z", "closed_at": "2017-05-12T15:44:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, it looks like the current GPU implementation of log_softmax is not very numerically stable.<br>\nCode</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\ninput = torch.cuda.FloatTensor([0.1232, 0.0869, -0.0047, 0.1211, 0.0513, 0.0545,\n                                0.0151, -0.0354, 0.0453, -0.0031, 0.0412, -0.0250,\n                                -0.0522, -0.0889, -0.0904, -0.0834, 0.0259, -0.0785,\n                                0.0016, 0.0152, 0.1067, -0.0039, 0.0144, -0.0634,\n                                0.0839, 0.0226, 0.0547,\n                                ]).unsqueeze(0)\ndef compute_log_softmax(input):\n    max_vals, max_pos = torch.max(input, 1)\n    input = input - max_vals.expand_as(input)\n    input_exp = torch.exp(input)\n    norm_vals = input_exp.sum(1)\n    norm_vals = torch.log(norm_vals)\n    # subtract sum\n    input = input - norm_vals.expand_as(input)\n    return input\nbuiltin = torch.nn.functional.log_softmax(Variable(input))\nmine = compute_log_softmax(input)\nprint(builtin.data - mine)\n</code></pre>\n<p>produces</p>\n<pre><code>Columns 0 to 9 \n1.00000e-07 *\n  0.0000  0.0000  0.0000  0.0000  0.0000  2.3842  0.0000  2.3842  0.0000  2.3842\n\nColumns 10 to 19 \n1.00000e-07 *\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 20 to 26 \n1.00000e-07 *\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.cuda.FloatTensor of size 1x27 (GPU 0)]\n</code></pre>\n<p>In my application, this discrepancy costed me 1% of final accuracy.<br>\nIs it possible to implement the max trick for stability on low level?</p>", "body_text": "Hi, it looks like the current GPU implementation of log_softmax is not very numerically stable.\nCode\nimport torch\nfrom torch.autograd import Variable\ninput = torch.cuda.FloatTensor([0.1232, 0.0869, -0.0047, 0.1211, 0.0513, 0.0545,\n                                0.0151, -0.0354, 0.0453, -0.0031, 0.0412, -0.0250,\n                                -0.0522, -0.0889, -0.0904, -0.0834, 0.0259, -0.0785,\n                                0.0016, 0.0152, 0.1067, -0.0039, 0.0144, -0.0634,\n                                0.0839, 0.0226, 0.0547,\n                                ]).unsqueeze(0)\ndef compute_log_softmax(input):\n    max_vals, max_pos = torch.max(input, 1)\n    input = input - max_vals.expand_as(input)\n    input_exp = torch.exp(input)\n    norm_vals = input_exp.sum(1)\n    norm_vals = torch.log(norm_vals)\n    # subtract sum\n    input = input - norm_vals.expand_as(input)\n    return input\nbuiltin = torch.nn.functional.log_softmax(Variable(input))\nmine = compute_log_softmax(input)\nprint(builtin.data - mine)\n\nproduces\nColumns 0 to 9 \n1.00000e-07 *\n  0.0000  0.0000  0.0000  0.0000  0.0000  2.3842  0.0000  2.3842  0.0000  2.3842\n\nColumns 10 to 19 \n1.00000e-07 *\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\nColumns 20 to 26 \n1.00000e-07 *\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.cuda.FloatTensor of size 1x27 (GPU 0)]\n\nIn my application, this discrepancy costed me 1% of final accuracy.\nIs it possible to implement the max trick for stability on low level?", "body": "Hi, it looks like the current GPU implementation of log_softmax is not very numerically stable.\r\nCode\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\ninput = torch.cuda.FloatTensor([0.1232, 0.0869, -0.0047, 0.1211, 0.0513, 0.0545,\r\n                                0.0151, -0.0354, 0.0453, -0.0031, 0.0412, -0.0250,\r\n                                -0.0522, -0.0889, -0.0904, -0.0834, 0.0259, -0.0785,\r\n                                0.0016, 0.0152, 0.1067, -0.0039, 0.0144, -0.0634,\r\n                                0.0839, 0.0226, 0.0547,\r\n                                ]).unsqueeze(0)\r\ndef compute_log_softmax(input):\r\n    max_vals, max_pos = torch.max(input, 1)\r\n    input = input - max_vals.expand_as(input)\r\n    input_exp = torch.exp(input)\r\n    norm_vals = input_exp.sum(1)\r\n    norm_vals = torch.log(norm_vals)\r\n    # subtract sum\r\n    input = input - norm_vals.expand_as(input)\r\n    return input\r\nbuiltin = torch.nn.functional.log_softmax(Variable(input))\r\nmine = compute_log_softmax(input)\r\nprint(builtin.data - mine)\r\n```\r\nproduces \r\n```\r\nColumns 0 to 9 \r\n1.00000e-07 *\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  2.3842  0.0000  2.3842  0.0000  2.3842\r\n\r\nColumns 10 to 19 \r\n1.00000e-07 *\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\r\n\r\nColumns 20 to 26 \r\n1.00000e-07 *\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\r\n[torch.cuda.FloatTensor of size 1x27 (GPU 0)]\r\n```\r\nIn my application, this discrepancy costed me 1% of final accuracy.\r\nIs it possible to implement the max trick for stability on low level?\r\n"}