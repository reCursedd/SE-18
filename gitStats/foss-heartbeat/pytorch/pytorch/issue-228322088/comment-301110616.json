{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/301110616", "html_url": "https://github.com/pytorch/pytorch/issues/1546#issuecomment-301110616", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1546", "id": 301110616, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTExMDYxNg==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-12T15:36:57Z", "updated_at": "2017-05-12T15:36:57Z", "author_association": "MEMBER", "body_html": "<p>The implementation is already using the max trick in all LogSoftmax implementations (see e.g. <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/LogSoftMax.cu#L258-L269\">here</a>). The differences seem small (max 2.38e-7) and may be a result of performing the operations in a different order. From what I see the CUDA kernel does <code>input - (max + log(exp_sum))</code> while you do <code>input - max - log(exp_sum)</code>.</p>", "body_text": "The implementation is already using the max trick in all LogSoftmax implementations (see e.g. here). The differences seem small (max 2.38e-7) and may be a result of performing the operations in a different order. From what I see the CUDA kernel does input - (max + log(exp_sum)) while you do input - max - log(exp_sum).", "body": "The implementation is already using the max trick in all LogSoftmax implementations (see e.g. [here](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/LogSoftMax.cu#L258-L269)). The differences seem small (max 2.38e-7) and may be a result of performing the operations in a different order. From what I see the CUDA kernel does `input - (max + log(exp_sum))` while you do `input - max - log(exp_sum)`."}