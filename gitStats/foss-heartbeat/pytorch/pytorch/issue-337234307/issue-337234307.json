{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9069", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9069/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9069/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9069/events", "html_url": "https://github.com/pytorch/pytorch/issues/9069", "id": 337234307, "node_id": "MDU6SXNzdWUzMzcyMzQzMDc=", "number": 9069, "title": "[jit] cannot trace tensor factory methods", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-06-30T18:33:26Z", "updated_at": "2018-09-12T18:43:08Z", "closed_at": "2018-09-12T18:43:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Tracing newly created tensors is not too much a problem..... until we hit random numbers.</p>\n<p>Things like <code>torch.randn(3)</code> are traced as <code>prim::Constant</code>, e.g.,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">x</span>): <span class=\"pl-k\">return</span> torch.randn(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)<span class=\"pl-k\">+</span>x\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.jit.get_trace_graph(fn, torch.ones(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">nderivs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)[<span class=\"pl-c1\">0</span>].graph()\ngraph(<span class=\"pl-k\">%</span><span class=\"pl-c1\">0</span> : Float(<span class=\"pl-c1\">1</span>)) {\n  <span class=\"pl-k\">%</span><span class=\"pl-c1\">1</span> : Float(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>!) = aten::expand[size=[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], implicit=<span class=\"pl-c1\">1</span>](<span class=\"pl-k\">%</span><span class=\"pl-c1\">0</span>)\n  <span class=\"pl-k\">%</span><span class=\"pl-c1\">2</span> : Float(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>) = prim::Constant[value= <span class=\"pl-c1\">1.0159</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.3672</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.7744</span>  <span class=\"pl-c1\">2.4991</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6615</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.9963</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1653</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6848</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.5436</span> [ CPUFloatTensor{<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>} ]]()\n  <span class=\"pl-k\">%</span><span class=\"pl-c1\">3</span> : Float(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>) = aten::add[alpha={<span class=\"pl-c1\">1</span>}](<span class=\"pl-k\">%</span><span class=\"pl-c1\">2</span>, <span class=\"pl-k\">%</span><span class=\"pl-c1\">1</span>)\n  <span class=\"pl-k\">return</span> (<span class=\"pl-k\">%</span><span class=\"pl-c1\">3</span>);\n}</pre></div>\n<p>The <code>_like</code> variant is traced similarly (requires_grad or not):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">x</span>): <span class=\"pl-k\">return</span> torch.randn_like(x,<span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)<span class=\"pl-k\">+</span>x\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.jit.get_trace_graph(fn, torch.ones(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">nderivs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)[<span class=\"pl-c1\">0</span>].graph()\ngraph(<span class=\"pl-k\">%</span><span class=\"pl-c1\">0</span> : Float(<span class=\"pl-c1\">1</span>)) {\n  <span class=\"pl-k\">%</span><span class=\"pl-c1\">1</span> : Float(<span class=\"pl-c1\">1</span>) = prim::Constant[value={<span class=\"pl-c1\">0.299761</span>}]()\n  <span class=\"pl-k\">%</span><span class=\"pl-c1\">2</span> : Float(<span class=\"pl-c1\">1</span>) = aten::add[alpha={<span class=\"pl-c1\">1</span>}](<span class=\"pl-k\">%</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">%</span><span class=\"pl-c1\">0</span>)\n  <span class=\"pl-k\">return</span> (<span class=\"pl-k\">%</span><span class=\"pl-c1\">2</span>);\n}</pre></div>\n<p>In-place sampling methods like <code>normal_</code> aren't supported either.</p>\n<p>This is blocking moving dropout into ATen <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336817565\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9008\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9008/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9008\">#9008</a> . Because, assuming we don't have custom backward for dropout (we really shouldn't since it's just sampling and multiplication), then we need to trace creation of the stochastic mask tensor.</p>\n<p>The workaround that I really really want to avoid is to have a custom backward. Then, because we need the stochastic mask in the backward, we need to write a <code>dropout_with_mask</code> that returns both the output and the mask, and write <code>dropout</code> as a wrapper. This shouldn't be necessary for such a simple op and makes the mask depending on input in graph.</p>\n<p>The fundamental solution is to fix the jit tracer on random numbers.</p>\n<p>Relevant <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"332170392\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8450\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8450/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8450\">#8450</a> .</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4685384\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jamesr66a\">@jamesr66a</a></p>", "body_text": "Tracing newly created tensors is not too much a problem..... until we hit random numbers.\nThings like torch.randn(3) are traced as prim::Constant, e.g.,\n>>> def fn(x): return torch.randn(3,3)+x\n...\n>>>\n>>> torch.jit.get_trace_graph(fn, torch.ones(1), nderivs=0)[0].graph()\ngraph(%0 : Float(1)) {\n  %1 : Float(3, 3!) = aten::expand[size=[3, 3], implicit=1](%0)\n  %2 : Float(3, 3) = prim::Constant[value= 1.0159 -0.3672 -0.7744  2.4991 -0.6615 -1.9963 -0.1653 -0.6848 -0.5436 [ CPUFloatTensor{3,3} ]]()\n  %3 : Float(3, 3) = aten::add[alpha={1}](%2, %1)\n  return (%3);\n}\nThe _like variant is traced similarly (requires_grad or not):\n>>> def fn(x): return torch.randn_like(x,requires_grad=True)+x\n...\n>>> torch.jit.get_trace_graph(fn, torch.ones(1), nderivs=0)[0].graph()\ngraph(%0 : Float(1)) {\n  %1 : Float(1) = prim::Constant[value={0.299761}]()\n  %2 : Float(1) = aten::add[alpha={1}](%1, %0)\n  return (%2);\n}\nIn-place sampling methods like normal_ aren't supported either.\nThis is blocking moving dropout into ATen #9008 . Because, assuming we don't have custom backward for dropout (we really shouldn't since it's just sampling and multiplication), then we need to trace creation of the stochastic mask tensor.\nThe workaround that I really really want to avoid is to have a custom backward. Then, because we need the stochastic mask in the backward, we need to write a dropout_with_mask that returns both the output and the mask, and write dropout as a wrapper. This shouldn't be necessary for such a simple op and makes the mask depending on input in graph.\nThe fundamental solution is to fix the jit tracer on random numbers.\nRelevant #8450 .\ncc @apaszke  @jamesr66a", "body": "Tracing newly created tensors is not too much a problem..... until we hit random numbers.\r\n\r\nThings like `torch.randn(3)` are traced as `prim::Constant`, e.g.,\r\n```py\r\n>>> def fn(x): return torch.randn(3,3)+x\r\n...\r\n>>>\r\n>>> torch.jit.get_trace_graph(fn, torch.ones(1), nderivs=0)[0].graph()\r\ngraph(%0 : Float(1)) {\r\n  %1 : Float(3, 3!) = aten::expand[size=[3, 3], implicit=1](%0)\r\n  %2 : Float(3, 3) = prim::Constant[value= 1.0159 -0.3672 -0.7744  2.4991 -0.6615 -1.9963 -0.1653 -0.6848 -0.5436 [ CPUFloatTensor{3,3} ]]()\r\n  %3 : Float(3, 3) = aten::add[alpha={1}](%2, %1)\r\n  return (%3);\r\n}\r\n```\r\n\r\nThe `_like` variant is traced similarly (requires_grad or not):\r\n```py\r\n>>> def fn(x): return torch.randn_like(x,requires_grad=True)+x\r\n...\r\n>>> torch.jit.get_trace_graph(fn, torch.ones(1), nderivs=0)[0].graph()\r\ngraph(%0 : Float(1)) {\r\n  %1 : Float(1) = prim::Constant[value={0.299761}]()\r\n  %2 : Float(1) = aten::add[alpha={1}](%1, %0)\r\n  return (%2);\r\n}\r\n```\r\n\r\nIn-place sampling methods like `normal_` aren't supported either.\r\n\r\n\r\nThis is blocking moving dropout into ATen #9008 . Because, assuming we don't have custom backward for dropout (we really shouldn't since it's just sampling and multiplication), then we need to trace creation of the stochastic mask tensor.\r\n\r\nThe workaround that I really really want to avoid is to have a custom backward. Then, because we need the stochastic mask in the backward, we need to write a `dropout_with_mask` that returns both the output and the mask, and write `dropout` as a wrapper. This shouldn't be necessary for such a simple op and makes the mask depending on input in graph.\r\n\r\nThe fundamental solution is to fix the jit tracer on random numbers.\r\n\r\nRelevant #8450 .\r\n\r\ncc @apaszke  @jamesr66a \r\n"}