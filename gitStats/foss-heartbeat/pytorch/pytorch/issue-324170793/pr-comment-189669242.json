{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189669242", "pull_request_review_id": 121893743, "id": 189669242, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTY2OTI0Mg==", "diff_hunk": "@@ -32,116 +31,191 @@ Tensor& fill_(Tensor& self, const Tensor& value) {\n   return self._fill_(value);\n }\n \n+Tensor& zero_(Tensor& self) {\n+  self._th_zero_();\n+  return self;\n+}\n+\n+Tensor sign(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return native::sign_out(result, self);\n+}\n+\n+Tensor& sign_(Tensor& self) {\n+  return native::sign_out(self, self);\n+}\n+\n+Tensor& sign_out(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  result.copy_(_sign(self));\n+  return result;\n+}\n+\n+Tensor clamp(const Tensor& self, Scalar value1, Scalar value2) {\n+  Tensor result = self.type().tensor();\n+  return native::clamp_out(result, self, value1, value2);\n+}\n+\n+Tensor& clamp_(Tensor& self, Scalar value1, Scalar value2) {\n+  return self._clamp_(value1, value2);\n+}\n+\n+Tensor& clamp_out(Tensor& result, const Tensor& self, Scalar value1, Scalar value2) {\n+  result.resize_(self.sizes());\n+  result.copy_(self);\n+  return result._clamp_(value1, value2);\n+}\n+\n+Tensor clamp_max(const Tensor& self, Scalar value) {\n+  Tensor result = self.type().tensor();\n+  return native::clamp_max_out(result, self, value);\n+}\n+\n+Tensor& clamp_max_(Tensor& self, Scalar value) {\n+  return self._clamp_max_(value);\n+}\n+\n+Tensor& clamp_max_out(Tensor& result, const Tensor& self, Scalar value1) {\n+  result.resize_(self.sizes());\n+  result.copy_(self);\n+  return result._clamp_max_(value1);\n+}\n+\n+Tensor clamp_min(const Tensor& self, Scalar value) {\n+  Tensor result = self.type().tensor();\n+  return native::clamp_min_out(result, self, value);\n+}\n+\n+Tensor& clamp_min_(Tensor& self, Scalar value) {\n+  return self._clamp_min_(value);\n+}\n+\n+Tensor& clamp_min_out(Tensor& result, const Tensor& self, Scalar value1) {\n+  result.resize_(self.sizes());\n+  result.copy_(self);\n+  return result._clamp_min_(value1);\n+}\n+\n+Tensor frac(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return frac_out(result, self);\n+}\n+\n+Tensor& frac_(Tensor& self) {\n+  return frac_out(self, self);\n+}\n+\n+Tensor& _frac_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_frac_out(result, self);\n+}\n+\n+Tensor erfinv(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return erfinv_out(result, self);\n+}\n+\n+Tensor& _erfinv__cpu(Tensor& self) {\n+  return _erfinv_out_cpu(self, self);\n+}\n+\n+Tensor& _erfinv_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_erfinv_out(result, self);\n+}\n+\n+Tensor sigmoid(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return sigmoid_out(result, self);\n+}\n+\n+Tensor& sigmoid_(Tensor& self) {\n+  return sigmoid_out(self, self);\n+}\n+\n+Tensor& _sigmoid_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_sigmoid_out(result, self);\n+}\n+\n+Tensor clone(const Tensor& self) {\n+  return self.type()._th_clone(self);\n+}\n+\n+Tensor _contiguous_cpu(const Tensor& self) {\n+  return self.type()._th_contiguous(self);\n+}\n+\n+Tensor neg(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return neg_out(result, self);\n+}\n+\n+Tensor& neg_(Tensor& self) {\n+  return neg_out(self, self);\n+}\n+\n+Tensor& _neg_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_neg_out(result, self);\n+}\n+\n+Tensor reciprocal(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return reciprocal_out(result, self);\n+}\n+\n+Tensor& reciprocal_(Tensor& self) {\n+  return reciprocal_out(self, self);\n+}\n+\n+Tensor& _reciprocal_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_reciprocal_out(result, self);\n+}", "path": "aten/src/ATen/native/UnaryOps.cpp", "position": null, "original_position": 155, "commit_id": "7d71bee290679800bae475a261ac70f97ad1b472", "original_commit_id": "17685fff7430239ff7014dc3d81d94095ff8bea2", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "body": "I'm just writing out all the boilerplate code and will then vectorize them one by one.", "created_at": "2018-05-21T18:11:05Z", "updated_at": "2018-11-23T15:44:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/7655#discussion_r189669242", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7655", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189669242"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7655#discussion_r189669242"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7655"}}, "body_html": "<p>I'm just writing out all the boilerplate code and will then vectorize them one by one.</p>", "body_text": "I'm just writing out all the boilerplate code and will then vectorize them one by one.", "in_reply_to_id": 189442568}