{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189442736", "pull_request_review_id": 121627132, "id": 189442736, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQ0MjczNg==", "diff_hunk": "@@ -2,84 +2,164 @@\n \n #include <cmath>\n #include <iostream>\n+#include \"ATen/CPUApplyUtils.h\"\n #include \"ATen/Dispatch.h\"\n #include \"ATen/Parallel.h\"\n-#include \"ATen/cpu/vec256/vec256.h\"\n #include \"ATen/cpu/vec256/functional.h\"\n+#include \"ATen/cpu/vec256/vec256.h\"\n #include \"ATen/native/cpu/CapabilityDispatch.h\"\n \n namespace at { namespace native {\n namespace {\n \n using namespace vec256;\n \n-template <class scalar_t, class F>\n-static void parallel_apply(Tensor& result, const Tensor& self, F f) {\n-  internal::init_tbb_num_threads();\n-\n-  static tbb::affinity_partitioner ap;\n-\n-  auto arr_out = result.data<scalar_t>();\n-  auto arr_in = self.data<scalar_t>();\n-  int64_t size = self.numel();\n-  if (size < internal::TBB_GRAIN_SIZE) {\n-    map(f, arr_out, arr_in, size);\n-  } else {\n-    tbb::parallel_for(\n-        tbb::blocked_range<int64_t>(0, size, internal::TBB_GRAIN_SIZE),\n-        [&](const tbb::blocked_range<int64_t>& r) {\n-          map(f, arr_out + r.begin(), arr_in + r.begin(), r.end() - r.begin());\n-        },\n-        ap);\n-  }\n-}\n-\n static void abs_kernel(Tensor& result, const Tensor& self) {\n   AT_DISPATCH_ALL_TYPES(self.type(), \"abs\", [&] {\n-    parallel_apply<scalar_t>(\n+    CPU_tensor_parallel_kernel_apply2<scalar_t, scalar_t>(\n         result,\n         self,\n-        [](const Vec256<scalar_t>& x) { return x.abs(); });  });\n+        [](int64_t size,\n+           scalar_t* x,\n+           scalar_t* y,\n+           int64_t stridex,\n+           int64_t stridey) {\n+          if (stridex == 1 && stridey == 1) {\n+            map([](const Vec256<scalar_t>& x) { return x.abs(); }, x, y, size);\n+          } else {\n+            for (int64_t i = 0; i < size; i++) {\n+              x[stridex * i] = std::abs(y[stridey * i]);\n+            }\n+          }\n+        });\n+  });", "path": "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "position": null, "original_position": 58, "commit_id": "7d71bee290679800bae475a261ac70f97ad1b472", "original_commit_id": "17685fff7430239ff7014dc3d81d94095ff8bea2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why can't abs use `IMPLEMENT_FLOAT_KERNEL`?", "created_at": "2018-05-19T19:24:41Z", "updated_at": "2018-11-23T15:44:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/7655#discussion_r189442736", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7655", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189442736"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7655#discussion_r189442736"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7655"}}, "body_html": "<p>Why can't abs use <code>IMPLEMENT_FLOAT_KERNEL</code>?</p>", "body_text": "Why can't abs use IMPLEMENT_FLOAT_KERNEL?"}