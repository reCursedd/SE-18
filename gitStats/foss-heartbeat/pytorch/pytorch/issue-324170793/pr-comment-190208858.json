{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190208858", "pull_request_review_id": 122535176, "id": 190208858, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDIwODg1OA==", "diff_hunk": "@@ -2,84 +2,263 @@\n \n #include <cmath>\n #include <iostream>\n+#include \"ATen/CPUApplyUtils.h\"\n #include \"ATen/Dispatch.h\"\n #include \"ATen/Parallel.h\"\n-#include \"ATen/cpu/vec256/vec256.h\"\n #include \"ATen/cpu/vec256/functional.h\"\n+#include \"ATen/cpu/vec256/vec256.h\"\n #include \"ATen/native/cpu/CapabilityDispatch.h\"\n \n-namespace at { namespace native {\n+// [Note AVX-SSE transitions] In general we avoid calls into cmath for code\n+// compiled with AVX/AVX2 This is because of SSE-AVX transitions and a bug in\n+// Glibc2.23 See https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280\n+// Calling zeroupper when using AVX/AVX2 code resolves this.\n+#if defined(__AVX__) && defined(__GLIBC__) && __GLIBC_MINOR__ == 23\n+#define ZEROUPPER _mm256_zeroupper();\n+#else\n+#define ZEROUPPER\n+#endif\n+\n+namespace at {\n+namespace native {\n namespace {\n \n-using namespace vec256;\n+template <typename scalar_t, typename VecOp, typename ScalarOp>\n+inline void _parallel_vector_map(\n+    VecOp vec_op,\n+    ScalarOp scalar_op,\n+    Tensor& result,\n+    const Tensor& self) {\n+  CPU_tensor_parallel_kernel_apply2<scalar_t, scalar_t>(\n+      result,\n+      self,\n+      [vec_op, scalar_op](int64_t size,\n+         scalar_t* x,\n+         scalar_t* y,\n+         int64_t stridex,\n+         int64_t stridey) {\n+        if (stridex == 1 && stridey == 1) {\n+          vec256::map(vec_op, x, y, size);\n+        } else {\n+          for (int64_t i = 0; i < size; i++) {\n+            ZEROUPPER\n+            x[stridex * i] = scalar_op(y[i * stridey]);\n+          }\n+        }\n+      });\n+}\n \n-template <class scalar_t, class F>\n-static void parallel_apply(Tensor& result, const Tensor& self, F f) {\n-  internal::init_tbb_num_threads();\n+static void clamp_max_kernel(Tensor& result, const Tensor& self, Scalar& max_) {\n+  AT_DISPATCH_ALL_TYPES(self.type(), \"clamp_max\", [&] {\n+    const scalar_t max_val = max_.to<scalar_t>();\n+    using Vec = vec256::Vec256<scalar_t>;\n+    _parallel_vector_map<scalar_t>(\n+        [max_val](const Vec& x) { return vec256::min(Vec(max_val), x); },\n+        [max_val](const scalar_t y) { return std::min(max_val, y); },\n+        result,\n+        self);\n+  });\n+}\n \n-  static tbb::affinity_partitioner ap;\n+static void clamp_min_kernel(Tensor& result, const Tensor& self, Scalar& min_) {\n+  AT_DISPATCH_ALL_TYPES(self.type(), \"clamp_min\", [&] {\n+    const scalar_t min_val = min_.to<scalar_t>();\n+    using Vec = vec256::Vec256<scalar_t>;\n+    _parallel_vector_map<scalar_t>(\n+        [min_val](const Vec& x) { return vec256::max(Vec(min_val), x); },\n+        [min_val](const scalar_t y) { return std::max(min_val, y); },\n+        result,\n+        self);});\n+}\n \n-  auto arr_out = result.data<scalar_t>();\n-  auto arr_in = self.data<scalar_t>();\n-  int64_t size = self.numel();\n-  if (size < internal::TBB_GRAIN_SIZE) {\n-    map(f, arr_out, arr_in, size);\n-  } else {\n-    tbb::parallel_for(\n-        tbb::blocked_range<int64_t>(0, size, internal::TBB_GRAIN_SIZE),\n-        [&](const tbb::blocked_range<int64_t>& r) {\n-          map(f, arr_out + r.begin(), arr_in + r.begin(), r.end() - r.begin());\n+static void sigmoid_kernel(Tensor& result, const Tensor& self) {\n+  AT_DISPATCH_FLOATING_TYPES(self.type(), \"sigmoid\", [&] {\n+    using Vec = vec256::Vec256<scalar_t>;\n+    _parallel_vector_map<scalar_t>(\n+        [](const Vec& x) { return vec256::sigmoid(x); },\n+        [](const scalar_t y_) {\n+          scalar_t y = -y_;\n+          ZEROUPPER\n+          y = std::exp(y);\n+          y = ((scalar_t)1) + y;\n+          return ((scalar_t)1.0) / y;\n         },\n-        ap);\n-  }\n+        result,\n+        self);});\n }\n \n-static void abs_kernel(Tensor& result, const Tensor& self) {\n-  AT_DISPATCH_ALL_TYPES(self.type(), \"abs\", [&] {\n-    parallel_apply<scalar_t>(\n+static void frac_kernel(Tensor& result, const Tensor& self) {\n+  AT_DISPATCH_FLOATING_TYPES(self.type(), \"frac\", [&] {\n+    using Vec = vec256::Vec256<scalar_t>;\n+    _parallel_vector_map<scalar_t>(\n+        [](const Vec& x) { return vec256::frac(x); },\n+        [](const scalar_t y) { return y - std::trunc(y); },\n         result,\n-        self,\n-        [](const Vec256<scalar_t>& x) { return x.abs(); });  });\n+        self);\n+  });\n }\n \n-static void rsqrt_kernel(Tensor& result, const Tensor& self) {\n-  AT_DISPATCH_FLOATING_TYPES(self.type(), \"rsqrt\", [&] {\n-    parallel_apply<scalar_t>(\n-        result,\n-        self,\n-        [](const Vec256<scalar_t>& x) { return Vec256<scalar_t>((scalar_t)(1)) / x.sqrt(); });  });\n+static void clamp_kernel(\n+    Tensor& result,\n+    const Tensor& self,\n+    Scalar& min_,\n+    Scalar& max_) {\n+  if (at::isFloatingType(self.type().scalarType())) {\n+    AT_DISPATCH_FLOATING_TYPES(self.type(), \"clamp\", [&] {\n+      const scalar_t min_val = min_.to<scalar_t>();\n+      const scalar_t max_val = max_.to<scalar_t>();\n+      using Vec = vec256::Vec256<scalar_t>;\n+      _parallel_vector_map<scalar_t>(\n+          [min_val, max_val](const Vec& x) {\n+            Vec max_vec = Vec(max_val);\n+            Vec min_vec = Vec(min_val);\n+            return at::vec256::max(min_vec, at::vec256::min(max_vec, x));\n+          },\n+          [min_val, max_val](const scalar_t y) {\n+            return std::max(min_val, std::min(max_val, y));\n+          },\n+          result,\n+          self);\n+    });\n+  } else {\n+    AT_DISPATCH_ALL_TYPES(self.type(), \"clamp\", [&] {\n+      const scalar_t min_val = min_.to<scalar_t>();\n+      const scalar_t max_val = max_.to<scalar_t>();\n+      CPU_tensor_parallel_apply2<scalar_t, scalar_t>(\n+          result, self, [min_val, max_val](scalar_t& x, scalar_t& y) {\n+            x = std::max(min_val, std::min(max_val, y));\n+          });\n+    });\n+  }\n+}\n+\n+static void fill_kernel(Tensor& self, Scalar& value_) {\n+  AT_DISPATCH_ALL_TYPES(self.type(), \"fill\", [&] {\n+    const scalar_t value = value_.to<scalar_t>();\n+    CPU_tensor_parallel_kernel_apply1<scalar_t>(\n+        self, [value](int64_t size, scalar_t* x, int64_t stridex) {\n+          if (stridex == 1) {\n+            using Vec = vec256::Vec256<scalar_t>;\n+            int64_t d = 0;\n+            Vec output_vec(value);\n+            for (; d < size - (size % Vec::size); d += Vec::size) {\n+              output_vec.storeu(x + d);\n+            }\n+            if (size - d > 0) {\n+              output_vec.storeu(x + d, size - d);\n+            }\n+          } else {\n+            for (int64_t i = 0; i < size; i++) {\n+              x[stridex * i] = value;\n+            }\n+          }\n+        });\n+  });\n }\n \n-#define IMPLEMENT_FLOAT_KERNEL(op)                                             \\\n-  static void op##_kernel(Tensor& result, const Tensor& self) {                \\\n-    AT_DISPATCH_FLOATING_TYPES(self.type(), #op, [&] {                         \\\n-      parallel_apply<scalar_t>(                                                \\\n-          result, self, [](const Vec256<scalar_t>& x) { return x.op(); }); \\\n-    });                                                                        \\\n-  }                                                                            \\\n+#define IMPLEMENT_COMPUTEBOUND_KERNEL(types, op, opfn)                     \\\n+  static void op##_kernel(Tensor& result, const Tensor& self) {            \\\n+    AT_DISPATCH_##types##_TYPES(self.type(), #op, [&] {                    \\\n+      static constexpr int WIDTH = 128 / sizeof(scalar_t);                 \\\n+      CPU_tensor_parallel_kernel_apply2<scalar_t, scalar_t>(               \\\n+          result,                                                          \\\n+          self,                                                            \\\n+          [](int64_t size,                                                 \\\n+             scalar_t* x,                                                  \\\n+             scalar_t* y,                                                  \\\n+             int64_t stridex,                                              \\\n+             int64_t stridey) {                                            \\\n+            using Vec = vec256::Vec256<scalar_t>;                          \\\n+            if (stridex == 1 && stridey == 1) {                            \\\n+              vec256::map(                                                 \\\n+                  [](const Vec& x) { return vec256::op(x); }, x, y, size); \\\n+            } else {                                                       \\\n+              int64_t i = 0;                                               \\\n+              if (size > WIDTH) {                                          \\\n+                for (; i < size - size % WIDTH; i += WIDTH) {              \\\n+                  scalar_t buffer[WIDTH];                                  \\\n+                  for (int64_t j = 0; j < WIDTH; j++)                      \\\n+                    buffer[j] = y[stridey * (j + i)];                      \\\n+                  vec256::map_(                                            \\\n+                      [](const Vec& x) { return vec256::op(x); },          \\\n+                      buffer,                                              \\\n+                      WIDTH);                                              \\\n+                  for (int64_t j = 0; j < WIDTH; j++)                      \\\n+                    x[stridex * (j + i)] = buffer[j];                      \\\n+                }                                                          \\\n+              }                                                            \\\n+              for (; i < size; i++) {                                      \\\n+                ZEROUPPER                                                  \\\n+                x[stridex * i] = opfn(y[stridey * i]);                     \\\n+              }                                                            \\\n+            }                                                              \\\n+          });                                                              \\\n+    });                                                                    \\\n+  }                                                                        \\\n+  REGISTER_DISPATCH(op##Impl, &op##_kernel)\n+\n+#define IMPLEMENT_KERNEL(types, op, opfn)                       \\\n+  static void op##_kernel(Tensor& result, const Tensor& self) { \\\n+    AT_DISPATCH_##types##_TYPES(self.type(), #op, [&] {         \\\n+      using Vec = vec256::Vec256<scalar_t>;                     \\\n+      _parallel_vector_map<scalar_t>(                           \\\n+          [](const Vec& x) { return vec256::op(x); },           \\\n+          [](const scalar_t y) {                                \\\n+            ZEROUPPER                                           \\\n+            return opfn(y);                                     \\\n+          },                                                    \\\n+          result,                                               \\\n+          self);                                                \\\n+    });                                                         \\\n+  }                                                             \\\n+  REGISTER_DISPATCH(op##Impl, &op##_kernel)\n+\n+#define IMPLEMENT_KERNEL_LOOP(types, op, opfn)                  \\\n+  static void op##_kernel(Tensor& result, const Tensor& self) { \\\n+    AT_DISPATCH_##types##_TYPES(self.type(), #op, [&] {         \\\n+      CPU_tensor_parallel_apply2<scalar_t, scalar_t>(           \\\n+          result, self, [](scalar_t& x, scalar_t& y) {          \\\n+            ZEROUPPER                                           \\", "path": "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "position": null, "original_position": 257, "commit_id": "7d71bee290679800bae475a261ac70f97ad1b472", "original_commit_id": "421e042c7a71184d3bbea8a414812bc38f0b4203", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why do we ZEROUPPER before every call if we're only using those calls? It seems unnecessary", "created_at": "2018-05-23T11:16:39Z", "updated_at": "2018-11-23T15:44:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/7655#discussion_r190208858", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7655", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190208858"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7655#discussion_r190208858"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7655"}}, "body_html": "<p>Why do we ZEROUPPER before every call if we're only using those calls? It seems unnecessary</p>", "body_text": "Why do we ZEROUPPER before every call if we're only using those calls? It seems unnecessary"}