{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7655", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7655/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7655/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7655/events", "html_url": "https://github.com/pytorch/pytorch/pull/7655", "id": 324170793, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg4ODUwMDA2", "number": 7655, "title": "Kernel apply functions", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-05-17T20:18:38Z", "updated_at": "2018-11-23T15:44:35Z", "closed_at": "2018-06-20T00:30:56Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7655", "html_url": "https://github.com/pytorch/pytorch/pull/7655", "diff_url": "https://github.com/pytorch/pytorch/pull/7655.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7655.patch"}, "body_html": "<p>Using a kernel instead of a pointwise op allows the developer to branch on various settings. For compute bound kernels it might make sense to aggregate data into a temporary buffer and then write it back out in a non-sequential fashion, therefore trading compute for memory. For kernels such as sin, cos etc. where SLEEF doesn't cover the full range you might want to first check whether the values are within that range and default to std::sin if they are not. It'll also enable to use vectorized instructions within inner regions of a tensor, even if it's non-contiguous overall.</p>\n<p>This abstraction also significantly reduces the complexity of the algorithm. Further, these kernels should be more general purpose and potentially reusable in other scenarios.</p>\n<p>Timings are yet to follow, but look better than master.</p>\n<p>EDIT: All the timings are on a single core.</p>", "body_text": "Using a kernel instead of a pointwise op allows the developer to branch on various settings. For compute bound kernels it might make sense to aggregate data into a temporary buffer and then write it back out in a non-sequential fashion, therefore trading compute for memory. For kernels such as sin, cos etc. where SLEEF doesn't cover the full range you might want to first check whether the values are within that range and default to std::sin if they are not. It'll also enable to use vectorized instructions within inner regions of a tensor, even if it's non-contiguous overall.\nThis abstraction also significantly reduces the complexity of the algorithm. Further, these kernels should be more general purpose and potentially reusable in other scenarios.\nTimings are yet to follow, but look better than master.\nEDIT: All the timings are on a single core.", "body": "Using a kernel instead of a pointwise op allows the developer to branch on various settings. For compute bound kernels it might make sense to aggregate data into a temporary buffer and then write it back out in a non-sequential fashion, therefore trading compute for memory. For kernels such as sin, cos etc. where SLEEF doesn't cover the full range you might want to first check whether the values are within that range and default to std::sin if they are not. It'll also enable to use vectorized instructions within inner regions of a tensor, even if it's non-contiguous overall.\r\n\r\nThis abstraction also significantly reduces the complexity of the algorithm. Further, these kernels should be more general purpose and potentially reusable in other scenarios.\r\n\r\nTimings are yet to follow, but look better than master.\r\n\r\nEDIT: All the timings are on a single core."}