{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189442568", "pull_request_review_id": 121627132, "id": 189442568, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQ0MjU2OA==", "diff_hunk": "@@ -32,116 +31,191 @@ Tensor& fill_(Tensor& self, const Tensor& value) {\n   return self._fill_(value);\n }\n \n+Tensor& zero_(Tensor& self) {\n+  self._th_zero_();\n+  return self;\n+}\n+\n+Tensor sign(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return native::sign_out(result, self);\n+}\n+\n+Tensor& sign_(Tensor& self) {\n+  return native::sign_out(self, self);\n+}\n+\n+Tensor& sign_out(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  result.copy_(_sign(self));\n+  return result;\n+}\n+\n+Tensor clamp(const Tensor& self, Scalar value1, Scalar value2) {\n+  Tensor result = self.type().tensor();\n+  return native::clamp_out(result, self, value1, value2);\n+}\n+\n+Tensor& clamp_(Tensor& self, Scalar value1, Scalar value2) {\n+  return self._clamp_(value1, value2);\n+}\n+\n+Tensor& clamp_out(Tensor& result, const Tensor& self, Scalar value1, Scalar value2) {\n+  result.resize_(self.sizes());\n+  result.copy_(self);\n+  return result._clamp_(value1, value2);\n+}\n+\n+Tensor clamp_max(const Tensor& self, Scalar value) {\n+  Tensor result = self.type().tensor();\n+  return native::clamp_max_out(result, self, value);\n+}\n+\n+Tensor& clamp_max_(Tensor& self, Scalar value) {\n+  return self._clamp_max_(value);\n+}\n+\n+Tensor& clamp_max_out(Tensor& result, const Tensor& self, Scalar value1) {\n+  result.resize_(self.sizes());\n+  result.copy_(self);\n+  return result._clamp_max_(value1);\n+}\n+\n+Tensor clamp_min(const Tensor& self, Scalar value) {\n+  Tensor result = self.type().tensor();\n+  return native::clamp_min_out(result, self, value);\n+}\n+\n+Tensor& clamp_min_(Tensor& self, Scalar value) {\n+  return self._clamp_min_(value);\n+}\n+\n+Tensor& clamp_min_out(Tensor& result, const Tensor& self, Scalar value1) {\n+  result.resize_(self.sizes());\n+  result.copy_(self);\n+  return result._clamp_min_(value1);\n+}\n+\n+Tensor frac(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return frac_out(result, self);\n+}\n+\n+Tensor& frac_(Tensor& self) {\n+  return frac_out(self, self);\n+}\n+\n+Tensor& _frac_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_frac_out(result, self);\n+}\n+\n+Tensor erfinv(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return erfinv_out(result, self);\n+}\n+\n+Tensor& _erfinv__cpu(Tensor& self) {\n+  return _erfinv_out_cpu(self, self);\n+}\n+\n+Tensor& _erfinv_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_erfinv_out(result, self);\n+}\n+\n+Tensor sigmoid(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return sigmoid_out(result, self);\n+}\n+\n+Tensor& sigmoid_(Tensor& self) {\n+  return sigmoid_out(self, self);\n+}\n+\n+Tensor& _sigmoid_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_sigmoid_out(result, self);\n+}\n+\n+Tensor clone(const Tensor& self) {\n+  return self.type()._th_clone(self);\n+}\n+\n+Tensor _contiguous_cpu(const Tensor& self) {\n+  return self.type()._th_contiguous(self);\n+}\n+\n+Tensor neg(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return neg_out(result, self);\n+}\n+\n+Tensor& neg_(Tensor& self) {\n+  return neg_out(self, self);\n+}\n+\n+Tensor& _neg_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_neg_out(result, self);\n+}\n+\n+Tensor reciprocal(const Tensor& self) {\n+  Tensor result = self.type().tensor();\n+  return reciprocal_out(result, self);\n+}\n+\n+Tensor& reciprocal_(Tensor& self) {\n+  return reciprocal_out(self, self);\n+}\n+\n+Tensor& _reciprocal_out_cpu(Tensor& result, const Tensor& self) {\n+  result.resize_(self.sizes());\n+  return at::_th_reciprocal_out(result, self);\n+}", "path": "aten/src/ATen/native/UnaryOps.cpp", "position": null, "original_position": 155, "commit_id": "7d71bee290679800bae475a261ac70f97ad1b472", "original_commit_id": "17685fff7430239ff7014dc3d81d94095ff8bea2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Those looks quite repetitive again. Why can't we use the macros below to implement them? If they are not vectorized, why are the moving to ATen, and why can't we have an unvectorized macro for them?", "created_at": "2018-05-19T19:15:50Z", "updated_at": "2018-11-23T15:44:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/7655#discussion_r189442568", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7655", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189442568"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7655#discussion_r189442568"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7655"}}, "body_html": "<p>Those looks quite repetitive again. Why can't we use the macros below to implement them? If they are not vectorized, why are the moving to ATen, and why can't we have an unvectorized macro for them?</p>", "body_text": "Those looks quite repetitive again. Why can't we use the macros below to implement them? If they are not vectorized, why are the moving to ATen, and why can't we have an unvectorized macro for them?"}