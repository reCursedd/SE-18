{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206561653", "pull_request_review_id": 141995940, "id": 206561653, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjU2MTY1Mw==", "diff_hunk": "@@ -0,0 +1,782 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/native/GridSampler.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include \"ATen/cuda/detail/TensorInfo.cuh\"\n+#include \"ATen/cuda/detail/IndexUtils.cuh\"\n+#include \"ATen/cuda/detail/KernelUtils.h\"\n+\n+namespace at { namespace native {\n+\n+using namespace at::cuda::detail;\n+\n+using at::native::detail::GridSamplerInterpolation;\n+using at::native::detail::GridSamplerPadding;\n+\n+namespace {\n+\tstatic __forceinline__ __device__\n+\tint clip_coordinates(int in, int clip_limit) {\n+\t  return ::min(clip_limit - 1, ::max(in, static_cast<int>(0)));\n+\t}\n+\n+\tstatic __forceinline__ __device__\n+\tbool within_bounds_2d(int h, int w, int H, int W) {\n+\t  return h >= 0 && h < H && w >= 0 && w < W;\n+\t}\n+\n+\tstatic __forceinline__ __device__\n+\tbool within_bounds_3d(int d, int h, int w, int D, int H, int W) {\n+\t  return d >= 0 && d < D && h >= 0 && h < H && w >= 0 && w < W;\n+\t}\n+\n+  template<typename scalar_t>\n+\tstatic __forceinline__ __device__\n+  void safe_add_2d(scalar_t *data, int h, int w,\n+                   int sH, int sW, int H, int W,\n+                   scalar_t delta) {\n+    if (h >= 0 && h < H && w >= 0 && w < W) {\n+      atomicAdd(data + h * sH + w * sW, delta);\n+    }\n+  }\n+\n+  template<typename scalar_t>\n+\tstatic __forceinline__ __device__\n+  void safe_add_3d(scalar_t *data, int d, int h, int w,\n+                   int sD, int sH, int sW, int D, int H, int W,\n+                   scalar_t delta) {\n+    if (d >= 0 && d < D && h >= 0 && h < H && w >= 0 && w < W) {\n+      atomicAdd(data + d * sD + h * sH + w * sW, delta);\n+    }\n+  }\n+\n+\ttemplate <typename scalar_t>\n+\t__launch_bounds__(1024)\n+\t__global__ void grid_sampler_2d_kernel(\n+\t    const int nthreads,\n+\t    TensorInfo<scalar_t, int> input,\n+\t    TensorInfo<scalar_t, int> grid,\n+\t    TensorInfo<scalar_t, int> output,\n+\t    const GridSamplerPadding padding_mode) {\n+\n+\t  int C = input.sizes[1];\n+\t  int inp_H = input.sizes[2];\n+\t  int inp_W = input.sizes[3];\n+\t  int out_H = grid.sizes[1];\n+\t  int out_W = grid.sizes[2];\n+\t  int inp_sN = input.strides[0];\n+\t  int inp_sC = input.strides[1];\n+\t  int inp_sH = input.strides[2];\n+\t  int inp_sW = input.strides[3];\n+\t  int grid_sN = grid.strides[0];\n+\t  int grid_sH = grid.strides[1];\n+\t  int grid_sW = grid.strides[2];\n+\t  int grid_sCoor = grid.strides[3];\n+\t  int out_sN = output.strides[0];\n+\t  int out_sC = output.strides[1];\n+\t  int out_sH = output.strides[2];\n+\t  int out_sW = output.strides[3];\n+\n+\t  CUDA_KERNEL_LOOP(index, nthreads) {\n+\t  \tconst int w = index % out_W;\n+\t  \tconst int h = (index / out_W) % out_H;\n+\t  \tconst int n = index / (out_H * out_W);\n+\t  \tconst int grid_offset = n * grid_sN + h * grid_sH + w * grid_sW;\n+\n+\t    // get the corresponding input x, y co-ordinates from grid\n+\t    scalar_t ix = grid.data[grid_offset];\n+\t    scalar_t iy = grid.data[grid_offset + grid_sCoor];\n+\n+\t    // normalize ix, iy from [-1, 1] to [0, IH-1] & [0, IW-1]\n+\t    float ixf = ((ix + 1.f) / 2) * (inp_W - 1);\n+\t    float iyf = ((iy + 1.f) / 2) * (inp_H - 1);\n+\n+\t    ix = static_cast<scalar_t>(ixf);\n+\t    iy = static_cast<scalar_t>(iyf);\n+\n+\t    // get NE, NW, SE, SW pixel values from (x, y)\n+\t    int ix_nw = static_cast<int>(::floor(ixf));\n+\t    int iy_nw = static_cast<int>(::floor(iyf));\n+\t    int ix_ne = ix_nw + 1;\n+\t    int iy_ne = iy_nw;\n+\t    int ix_sw = ix_nw;\n+\t    int iy_sw = iy_nw + 1;\n+\t    int ix_se = ix_nw + 1;\n+\t    int iy_se = iy_nw + 1;\n+\n+\t    // get surfaces to each neighbor:\n+\t    scalar_t nw = (ix_se - ix)    * (iy_se - iy);\n+\t    scalar_t ne = (ix    - ix_sw) * (iy_sw - iy);\n+\t    scalar_t sw = (ix_ne - ix)    * (iy    - iy_ne);\n+\t    scalar_t se = (ix    - ix_nw) * (iy    - iy_nw);\n+\n+\t    // calculate bilinear weighted pixel value and set output pixel\n+\t\t\tif (padding_mode == GridSamplerPadding::Border) {\n+\t\t\t  // clip coordinates to image borders\n+\t\t\t  ix_nw = clip_coordinates(ix_nw, inp_W);\n+\t\t\t  iy_nw = clip_coordinates(iy_nw, inp_H);\n+\t\t\t  ix_ne = clip_coordinates(ix_ne, inp_W);\n+\t\t\t  iy_ne = clip_coordinates(iy_ne, inp_H);\n+\t\t\t  ix_sw = clip_coordinates(ix_sw, inp_W);\n+\t\t\t  iy_sw = clip_coordinates(iy_sw, inp_H);\n+\t\t\t  ix_se = clip_coordinates(ix_se, inp_W);\n+\t\t\t  iy_se = clip_coordinates(iy_se, inp_H);\n+\t\t\t}\n+\n+\t    auto inp_ptr_NC = input.data + n * inp_sN;\n+\t    auto out_ptr_NCHW = output.data + n * out_sN + h * out_sH + w * out_sW;\n+\t    for (int c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCHW += out_sC) {\n+\t      *out_ptr_NCHW = static_cast<scalar_t>(0);\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {\n+\t        *out_ptr_NCHW += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;\n+\t      }\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {\n+\t        *out_ptr_NCHW += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;\n+\t      }\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {\n+\t        *out_ptr_NCHW += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;\n+\t      }\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {\n+\t        *out_ptr_NCHW += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;\n+\t      }\n+\t    }\n+\t  }\n+\t}\n+\n+\ttemplate <typename scalar_t>\n+\t__launch_bounds__(1024)\n+\t__global__ void grid_sampler_3d_kernel(\n+\t    const int nthreads,\n+\t    TensorInfo<scalar_t, int> input,\n+\t    TensorInfo<scalar_t, int> grid,\n+\t    TensorInfo<scalar_t, int> output,\n+\t    const GridSamplerPadding padding_mode) {\n+\n+    int C = input.sizes[1];\n+    int inp_D = input.sizes[2];\n+    int inp_H = input.sizes[3];\n+    int inp_W = input.sizes[4];\n+    int out_D = grid.sizes[1];\n+    int out_H = grid.sizes[2];\n+    int out_W = grid.sizes[3];\n+    int inp_sN = input.strides[0];\n+    int inp_sC = input.strides[1];\n+    int inp_sD = input.strides[2];\n+    int inp_sH = input.strides[3];\n+    int inp_sW = input.strides[4];\n+    int grid_sN = grid.strides[0];\n+    int grid_sD = grid.strides[1];\n+    int grid_sH = grid.strides[2];\n+    int grid_sW = grid.strides[3];\n+    int grid_sCoor = grid.strides[4];\n+    int out_sN = output.strides[0];\n+    int out_sC = output.strides[1];\n+    int out_sD = output.strides[2];\n+    int out_sH = output.strides[3];\n+    int out_sW = output.strides[4];\n+\n+\t  CUDA_KERNEL_LOOP(index, nthreads) {\n+\t  \tconst int w = index % out_W;\n+\t  \tconst int h = (index / out_W) % out_H;\n+\t  \tconst int d = (index / (out_H * out_W)) % out_D;\n+\t  \tconst int n = index / (out_D * out_H * out_W);\n+\t  \tconst int grid_offset = n * grid_sN + d * grid_sD + h * grid_sH + w * grid_sW;\n+\n+\t    // get the corresponding input x, y, z co-ordinates from grid\n+\t    scalar_t ix = grid.data[grid_offset];\n+\t    scalar_t iy = grid.data[grid_offset + grid_sCoor];\n+\t    scalar_t iz = grid.data[grid_offset + 2 * grid_sCoor];\n+\n+      // normalize ix, iy, iz from [-1, 1] to [0, inp_W-1] & [0, inp_H-1] & [0, inp_D-1]\n+\t    float ixf = ((ix + 1.f) / 2) * (inp_W - 1);\n+\t    float iyf = ((iy + 1.f) / 2) * (inp_H - 1);\n+\t    float izf = ((iz + 1.f) / 2) * (inp_D - 1);\n+\n+\t    ix = static_cast<scalar_t>(ixf);\n+\t    iy = static_cast<scalar_t>(iyf);\n+\t    iz = static_cast<scalar_t>(izf);\n+\n+      // get corner pixel values from (x, y, z)\n+      // for 4d, we used north-east-south-west\n+      // for 5d, we add top-bottom\n+      int ix_tnw = static_cast<int>(::floor(ix));\n+      int iy_tnw = static_cast<int>(::floor(iy));\n+      int iz_tnw = static_cast<int>(::floor(iz));\n+\n+      int ix_tne = ix_tnw + 1;\n+      int iy_tne = iy_tnw;\n+      int iz_tne = iz_tnw;\n+\n+      int ix_tsw = ix_tnw;\n+      int iy_tsw = iy_tnw + 1;\n+      int iz_tsw = iz_tnw;\n+\n+      int ix_tse = ix_tnw + 1;\n+      int iy_tse = iy_tnw + 1;\n+      int iz_tse = iz_tnw;\n+\n+      int ix_bnw = ix_tnw;\n+      int iy_bnw = iy_tnw;\n+      int iz_bnw = iz_tnw + 1;\n+\n+      int ix_bne = ix_tnw + 1;\n+      int iy_bne = iy_tnw;\n+      int iz_bne = iz_tnw + 1;\n+\n+      int ix_bsw = ix_tnw;\n+      int iy_bsw = iy_tnw + 1;\n+      int iz_bsw = iz_tnw + 1;\n+\n+      int ix_bse = ix_tnw + 1;\n+      int iy_bse = iy_tnw + 1;\n+      int iz_bse = iz_tnw + 1;\n+\n+      // get surfaces to each neighbor:\n+      scalar_t tnw = (ix_bse - ix)    * (iy_bse - iy)    * (iz_bse - iz);\n+      scalar_t tne = (ix    - ix_bsw) * (iy_bsw - iy)    * (iz_bsw - iz);\n+      scalar_t tsw = (ix_bne - ix)    * (iy    - iy_bne) * (iz_bne - iz);\n+      scalar_t tse = (ix    - ix_bnw) * (iy    - iy_bnw) * (iz_bnw - iz);\n+      scalar_t bnw = (ix_tse - ix)    * (iy_tse - iy)    * (iz - iz_tse);\n+      scalar_t bne = (ix    - ix_tsw) * (iy_tsw - iy)    * (iz - iz_tsw);\n+      scalar_t bsw = (ix_tne - ix)    * (iy    - iy_tne) * (iz - iz_tne);\n+      scalar_t bse = (ix    - ix_tnw) * (iy    - iy_tnw) * (iz - iz_tnw);\n+\n+      if (padding_mode == GridSamplerPadding::Border) {\n+        // clip coordinates to image borders\n+        ix_tnw = clip_coordinates(ix_tnw, inp_W);\n+        iy_tnw = clip_coordinates(iy_tnw, inp_H);\n+        iz_tnw = clip_coordinates(iz_tnw, inp_D);\n+        ix_tne = clip_coordinates(ix_tne, inp_W);\n+        iy_tne = clip_coordinates(iy_tne, inp_H);\n+        iz_tne = clip_coordinates(iz_tne, inp_D);\n+        ix_tsw = clip_coordinates(ix_tsw, inp_W);\n+        iy_tsw = clip_coordinates(iy_tsw, inp_H);\n+        iz_tsw = clip_coordinates(iz_tsw, inp_D);\n+        ix_tse = clip_coordinates(ix_tse, inp_W);\n+        iy_tse = clip_coordinates(iy_tse, inp_H);\n+        iz_tse = clip_coordinates(iz_tse, inp_D);\n+        ix_bnw = clip_coordinates(ix_bnw, inp_W);\n+        iy_bnw = clip_coordinates(iy_bnw, inp_H);\n+        iz_bnw = clip_coordinates(iz_bnw, inp_D);\n+        ix_bne = clip_coordinates(ix_bne, inp_W);\n+        iy_bne = clip_coordinates(iy_bne, inp_H);\n+        iz_bne = clip_coordinates(iz_bne, inp_D);\n+        ix_bsw = clip_coordinates(ix_bsw, inp_W);\n+        iy_bsw = clip_coordinates(iy_bsw, inp_H);\n+        iz_bsw = clip_coordinates(iz_bsw, inp_D);\n+        ix_bse = clip_coordinates(ix_bse, inp_W);\n+        iy_bse = clip_coordinates(iy_bse, inp_H);\n+        iz_bse = clip_coordinates(iz_bse, inp_D);\n+      }\n+\n+\t    auto inp_ptr_NC = input.data + n * inp_sN;\n+\t    auto out_ptr_NCDHW = output.data + n * out_sN + d * out_sD + h * out_sH + w * out_sW;\n+\t    for (int c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCDHW += out_sC) {\n+        //   (c, iz_tnw, iy_tnw, ix_tnw) * tnw + (c, iz_tne, iy_tne, ix_tne) * tne\n+        // + (c, iz_tsw, iy_tsw, ix_tsw) * tsw + (c, iz_tse, iy_tse, ix_tse) * tse\n+        // + (c, iz_bnw, iy_bnw, ix_bnw) * bnw + (c, iz_bne, iy_bne, ix_bne) * bne\n+        // + (c, iz_bsw, iy_bsw, ix_bsw) * bsw + (c, iz_bse, iy_bse, ix_bse) * bse\n+        *out_ptr_NCDHW = static_cast<scalar_t>(0);\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_tnw, iy_tnw, ix_tnw, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_tnw * inp_sD + iy_tnw * inp_sH + ix_tnw * inp_sW] * tnw;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_tne, iy_tne, ix_tne, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_tne * inp_sD + iy_tne * inp_sH + ix_tne * inp_sW] * tne;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_tsw, iy_tsw, ix_tsw, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_tsw * inp_sD + iy_tsw * inp_sH + ix_tsw * inp_sW] * tsw;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_tse, iy_tse, ix_tse, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_tse * inp_sD + iy_tse * inp_sH + ix_tse * inp_sW] * tse;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_bnw, iy_bnw, ix_bnw, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_bnw * inp_sD + iy_bnw * inp_sH + ix_bnw * inp_sW] * bnw;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_bne, iy_bne, ix_bne, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_bne * inp_sD + iy_bne * inp_sH + ix_bne * inp_sW] * bne;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_bsw, iy_bsw, ix_bsw, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_bsw * inp_sD + iy_bsw * inp_sH + ix_bsw * inp_sW] * bsw;\n+        }\n+        if (padding_mode != GridSamplerPadding::Zeros || within_bounds_3d(iz_bse, iy_bse, ix_bse, inp_D, inp_H, inp_W)) {\n+          *out_ptr_NCDHW += inp_ptr_NC[iz_bse * inp_sD + iy_bse * inp_sH + ix_bse * inp_sW] * bse;\n+        }\n+\t    }\n+\t  }\n+\t}\n+\n+\ttemplate <typename scalar_t>\n+\t__launch_bounds__(1024)\n+\t__global__ void grid_sampler_2d_backward_kernel(\n+\t    const int nthreads,\n+\t    TensorInfo<scalar_t, int> grad_output,\n+\t    TensorInfo<scalar_t, int> input,\n+\t    TensorInfo<scalar_t, int> grid,\n+\t    TensorInfo<scalar_t, int> grad_input,  // initialized to zeros\n+\t    TensorInfo<scalar_t, int> grad_grid,   // initialized to empty\n+\t    const GridSamplerPadding padding_mode) {\n+\n+    int C = input.sizes[1];\n+    int inp_H = input.sizes[2];\n+    int inp_W = input.sizes[3];\n+\t  int out_H = grid.sizes[1];\n+\t  int out_W = grid.sizes[2];\n+    int inp_sN = input.strides[0];\n+    int inp_sC = input.strides[1];\n+    int inp_sH = input.strides[2];\n+    int inp_sW = input.strides[3];\n+    int grid_sN = grid.strides[0];\n+    int grid_sH = grid.strides[1];\n+    int grid_sW = grid.strides[2];\n+    int grid_sCoor = grid.strides[3];\n+    int gOut_sN = grad_output.strides[0];\n+    int gOut_sC = grad_output.strides[1];\n+    int gOut_sH = grad_output.strides[2];\n+    int gOut_sW = grad_output.strides[3];\n+    int gInp_sN = grad_input.strides[0];\n+    int gInp_sC = grad_input.strides[1];\n+    int gInp_sH = grad_input.strides[2];\n+    int gInp_sW = grad_input.strides[3];\n+    int gGrid_sW = grad_grid.strides[2];\n+\n+\t  CUDA_KERNEL_LOOP(index, nthreads) {\n+\t  \tconst int w = index % out_W;\n+\t  \tconst int h = (index / out_W) % out_H;\n+\t  \tconst int n = index / (out_H * out_W);\n+\t  \tconst int grid_offset = n * grid_sN + h * grid_sH + w * grid_sW;\n+\n+\t    // get the corresponding input x, y co-ordinates from grid\n+\t    scalar_t ix = grid.data[grid_offset];\n+\t    scalar_t iy = grid.data[grid_offset + grid_sCoor];\n+\n+\t    // normalize ix, iy from [-1, 1] to [0, IH-1] & [0, IW-1]\n+\t    float ixf = ((ix + 1.f) / 2) * (inp_W - 1);\n+\t    float iyf = ((iy + 1.f) / 2) * (inp_H - 1);\n+\n+\t    ix = static_cast<scalar_t>(ixf);\n+\t    iy = static_cast<scalar_t>(iyf);\n+\n+\t    // get NE, NW, SE, SW pixel values from (x, y)\n+\t    int ix_nw = static_cast<int>(::floor(ixf));\n+\t    int iy_nw = static_cast<int>(::floor(iyf));\n+\t    int ix_ne = ix_nw + 1;\n+\t    int iy_ne = iy_nw;\n+\t    int ix_sw = ix_nw;\n+\t    int iy_sw = iy_nw + 1;\n+\t    int ix_se = ix_nw + 1;\n+\t    int iy_se = iy_nw + 1;\n+\n+\t    // get surfaces to each neighbor:\n+\t    scalar_t nw = (ix_se - ix)    * (iy_se - iy);\n+\t    scalar_t ne = (ix    - ix_sw) * (iy_sw - iy);\n+\t    scalar_t sw = (ix_ne - ix)    * (iy    - iy_ne);\n+\t    scalar_t se = (ix    - ix_nw) * (iy    - iy_nw);\n+\n+\t    int ix_nw_cl, iy_nw_cl, ix_ne_cl, iy_ne_cl, ix_sw_cl, iy_sw_cl, ix_se_cl, iy_se_cl;\n+\n+\t    // calculate bilinear weighted pixel value and set output pixel\n+\t\t\tif (padding_mode == GridSamplerPadding::Border) {\n+\t\t\t  // clip coordinates to image borders\n+\t\t\t  ix_nw_cl = clip_coordinates(ix_nw, inp_W);\n+\t\t\t  iy_nw_cl = clip_coordinates(iy_nw, inp_H);\n+\t\t\t  ix_ne_cl = clip_coordinates(ix_ne, inp_W);\n+\t\t\t  iy_ne_cl = clip_coordinates(iy_ne, inp_H);\n+\t\t\t  ix_sw_cl = clip_coordinates(ix_sw, inp_W);\n+\t\t\t  iy_sw_cl = clip_coordinates(iy_sw, inp_H);\n+\t\t\t  ix_se_cl = clip_coordinates(ix_se, inp_W);\n+\t\t\t  iy_se_cl = clip_coordinates(iy_se, inp_H);\n+\t\t\t} else {\n+\t      ix_nw_cl = ix_nw;\n+\t      iy_nw_cl = iy_nw;\n+\t      ix_ne_cl = ix_ne;\n+\t      iy_ne_cl = iy_ne;\n+\t      ix_sw_cl = ix_sw;\n+\t      iy_sw_cl = iy_sw;\n+\t      ix_se_cl = ix_se;\n+\t      iy_se_cl = iy_se;\n+\t\t\t}\n+\n+      scalar_t gix = static_cast<scalar_t>(0), giy = static_cast<scalar_t>(0);\n+      scalar_t *gOut_ptr_NCHW = grad_output.data + n * gOut_sN + h * gOut_sH + w * gOut_sW;\n+      scalar_t *gInp_ptr_NC = grad_input.data + n * gInp_sN;\n+      scalar_t *inp_ptr_NC = input.data + n * inp_sN;\n+\t    for (int c = 0; c < C; ++c, inp_ptr_NC += inp_sC, gInp_ptr_NC += gInp_sC, gOut_ptr_NCHW += gOut_sC) {\n+        scalar_t gOut = *gOut_ptr_NCHW;\n+\n+\t      // calculate and set grad_input\n+\t      safe_add_2d(gInp_ptr_NC, iy_nw_cl, ix_nw_cl, gInp_sH, gInp_sW, inp_H, inp_W, nw * gOut);\n+\t      safe_add_2d(gInp_ptr_NC, iy_ne_cl, ix_ne_cl, gInp_sH, gInp_sW, inp_H, inp_W, ne * gOut);\n+\t      safe_add_2d(gInp_ptr_NC, iy_sw_cl, ix_sw_cl, gInp_sH, gInp_sW, inp_H, inp_W, sw * gOut);\n+\t      safe_add_2d(gInp_ptr_NC, iy_se_cl, ix_se_cl, gInp_sH, gInp_sW, inp_H, inp_W, se * gOut);\n+\n+\t      // calculate grad_grid\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_nw_cl, ix_nw_cl, inp_H, inp_W)) {\n+\t      \tscalar_t nw_val = inp_ptr_NC[iy_nw_cl * inp_sH + ix_nw_cl * inp_sW];\n+\t      \tgix -= nw_val * (iy_se - iy) * gOut;\n+      \t\tgiy -= nw_val * (ix_se - ix) * gOut;\n+\t      }\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_ne_cl, ix_ne_cl, inp_H, inp_W)) {\n+\t      \tscalar_t ne_val = inp_ptr_NC[iy_ne_cl * inp_sH + ix_ne_cl * inp_sW];\n+\t      \tgix += ne_val * (iy_sw - iy) * gOut;\n+      \t\tgiy -= ne_val * (ix - ix_sw) * gOut;\n+\t      }\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_sw_cl, ix_sw_cl, inp_H, inp_W)) {\n+\t      \tscalar_t sw_val = inp_ptr_NC[iy_sw_cl * inp_sH + ix_sw_cl * inp_sW];\n+\t      \tgix -= sw_val * (iy - iy_ne) * gOut;\n+      \t\tgiy += sw_val * (ix_ne - ix) * gOut;\n+\t      }\n+\t      if (padding_mode != GridSamplerPadding::Zeros || within_bounds_2d(iy_se_cl, ix_se_cl, inp_H, inp_W)) {\n+\t      \tscalar_t se_val = inp_ptr_NC[iy_se_cl * inp_sH + ix_se_cl * inp_sW];\n+\t        gix += se_val * (iy - iy_nw) * gOut;\n+\t        giy += se_val * (ix - ix_nw) * gOut;\n+\t      }\n+\t    }\n+\n+\t    // un-normalize grad_grid values back to [-1, 1] constraints\n+\t    gix = gix * (inp_W - 1.f) / 2;\n+\t    giy = giy * (inp_H - 1.f) / 2;\n+\n+      // assuming grad_grid is contiguous", "path": "aten/src/ATen/native/cuda/GridSampler.cu", "position": null, "original_position": 438, "commit_id": "7acb794792a691f5ebd29a6b634f917536353723", "original_commit_id": "50e624fe8ccd9545cc2bb7d0ce88a6121ac4178a", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Yeah, (2) doesn't require contiguity. In the THUCNN code, `grad_grid` is initialized as all zeros tensor, and each element is accumulated exactly once, so I changed to directly writing.\r\n\r\nThe contiguity assumption is used in computing `gGrid_ptr_NHW` and just assigning `gGrid_ptr_NHW[0]` and `gGrid_ptr_NHW[1]` (assuming last dimension has stride 1). \r\n\r\nI'll add some comments in code.", "created_at": "2018-07-31T14:55:09Z", "updated_at": "2018-11-23T15:48:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/9961#discussion_r206561653", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9961", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206561653"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9961#discussion_r206561653"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9961"}}, "body_html": "<p>Yeah, (2) doesn't require contiguity. In the THUCNN code, <code>grad_grid</code> is initialized as all zeros tensor, and each element is accumulated exactly once, so I changed to directly writing.</p>\n<p>The contiguity assumption is used in computing <code>gGrid_ptr_NHW</code> and just assigning <code>gGrid_ptr_NHW[0]</code> and <code>gGrid_ptr_NHW[1]</code> (assuming last dimension has stride 1).</p>\n<p>I'll add some comments in code.</p>", "body_text": "Yeah, (2) doesn't require contiguity. In the THUCNN code, grad_grid is initialized as all zeros tensor, and each element is accumulated exactly once, so I changed to directly writing.\nThe contiguity assumption is used in computing gGrid_ptr_NHW and just assigning gGrid_ptr_NHW[0] and gGrid_ptr_NHW[1] (assuming last dimension has stride 1).\nI'll add some comments in code.", "in_reply_to_id": 206557992}