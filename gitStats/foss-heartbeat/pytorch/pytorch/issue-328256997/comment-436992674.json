{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436992674", "html_url": "https://github.com/pytorch/pytorch/issues/7999#issuecomment-436992674", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7999", "id": 436992674, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjk5MjY3NA==", "user": {"login": "qiliux7", "id": 44420074, "node_id": "MDQ6VXNlcjQ0NDIwMDc0", "avatar_url": "https://avatars1.githubusercontent.com/u/44420074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qiliux7", "html_url": "https://github.com/qiliux7", "followers_url": "https://api.github.com/users/qiliux7/followers", "following_url": "https://api.github.com/users/qiliux7/following{/other_user}", "gists_url": "https://api.github.com/users/qiliux7/gists{/gist_id}", "starred_url": "https://api.github.com/users/qiliux7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qiliux7/subscriptions", "organizations_url": "https://api.github.com/users/qiliux7/orgs", "repos_url": "https://api.github.com/users/qiliux7/repos", "events_url": "https://api.github.com/users/qiliux7/events{/privacy}", "received_events_url": "https://api.github.com/users/qiliux7/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-08T13:25:40Z", "updated_at": "2018-11-08T13:37:51Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a><br>\nAnyone considers the problem about the effect of the pre-trained models if we replace the output behavior of ReLU. I am working on a project which needs to extract the features of the pre-trained model like Vgg16. However, is it possible that you will <strong>get NaN from the ReLU layer</strong> if you use <strong>the pre-trained model of the previous pytorch version</strong>? Thus, you can not perform torch.svg on the extracted features.</p>\n<p>I think it is definitely possible if the pre-trained models have not been updated using the latest pytorch version.</p>", "body_text": "@SsnL\nAnyone considers the problem about the effect of the pre-trained models if we replace the output behavior of ReLU. I am working on a project which needs to extract the features of the pre-trained model like Vgg16. However, is it possible that you will get NaN from the ReLU layer if you use the pre-trained model of the previous pytorch version? Thus, you can not perform torch.svg on the extracted features.\nI think it is definitely possible if the pre-trained models have not been updated using the latest pytorch version.", "body": "@SsnL  \r\nAnyone considers the problem about the effect of the pre-trained models if we replace the output behavior of ReLU. I am working on a project which needs to extract the features of the pre-trained model like Vgg16. However, is it possible that you will **get NaN from the ReLU layer** if you use **the pre-trained model of the previous pytorch version**? Thus, you can not perform torch.svg on the extracted features.\r\n\r\nI think it is definitely possible if the pre-trained models have not been updated using the latest pytorch version."}