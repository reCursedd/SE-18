{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8149", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8149/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8149/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8149/events", "html_url": "https://github.com/pytorch/pytorch/issues/8149", "id": 329289772, "node_id": "MDU6SXNzdWUzMjkyODk3NzI=", "number": 8149, "title": "Is optimizer.step() thread safe?", "user": {"login": "nilsjohanbjorck", "id": 19216884, "node_id": "MDQ6VXNlcjE5MjE2ODg0", "avatar_url": "https://avatars1.githubusercontent.com/u/19216884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nilsjohanbjorck", "html_url": "https://github.com/nilsjohanbjorck", "followers_url": "https://api.github.com/users/nilsjohanbjorck/followers", "following_url": "https://api.github.com/users/nilsjohanbjorck/following{/other_user}", "gists_url": "https://api.github.com/users/nilsjohanbjorck/gists{/gist_id}", "starred_url": "https://api.github.com/users/nilsjohanbjorck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nilsjohanbjorck/subscriptions", "organizations_url": "https://api.github.com/users/nilsjohanbjorck/orgs", "repos_url": "https://api.github.com/users/nilsjohanbjorck/repos", "events_url": "https://api.github.com/users/nilsjohanbjorck/events{/privacy}", "received_events_url": "https://api.github.com/users/nilsjohanbjorck/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-05T03:31:25Z", "updated_at": "2018-06-05T14:55:22Z", "closed_at": "2018-06-05T14:55:22Z", "author_association": "NONE", "body_html": "<p>In the example of asynchronous training (examples/mnist_hogwild/train.py) a model is made shared via model.share_memory(). After that, the different threads simply call optimizer.step() asynchronously. In the documentation (<a href=\"https://pytorch.org/docs/master/notes/multiprocessing.html\" rel=\"nofollow\">https://pytorch.org/docs/master/notes/multiprocessing.html</a>) it is recommended to use queues for passing tensors, but that is evidently not done in the example.</p>\n<p>Is the optimizer.step() function guaranteed to be thread safe?</p>", "body_text": "In the example of asynchronous training (examples/mnist_hogwild/train.py) a model is made shared via model.share_memory(). After that, the different threads simply call optimizer.step() asynchronously. In the documentation (https://pytorch.org/docs/master/notes/multiprocessing.html) it is recommended to use queues for passing tensors, but that is evidently not done in the example.\nIs the optimizer.step() function guaranteed to be thread safe?", "body": "In the example of asynchronous training (examples/mnist_hogwild/train.py) a model is made shared via model.share_memory(). After that, the different threads simply call optimizer.step() asynchronously. In the documentation (https://pytorch.org/docs/master/notes/multiprocessing.html) it is recommended to use queues for passing tensors, but that is evidently not done in the example.\r\n\r\nIs the optimizer.step() function guaranteed to be thread safe?"}