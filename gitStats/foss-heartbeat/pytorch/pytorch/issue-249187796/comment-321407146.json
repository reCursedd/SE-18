{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/321407146", "html_url": "https://github.com/pytorch/pytorch/pull/2364#issuecomment-321407146", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2364", "id": 321407146, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTQwNzE0Ng==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T23:16:42Z", "updated_at": "2017-08-09T23:16:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Addresses at least some of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"241375417\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2012\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2012/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2012\">#2012</a>.</p>\n<p>Here's a microbenchmark (modeled after <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>'s) that measures 4 common cases (outplace vs inplace, 2 args vs 3 args):</p>\n<pre><code>import torch\nfrom timeit import timeit\n\nx = torch.randn(5, 5)\ny = torch.randn(5, 5)\nz = torch.randn(5, 5)\n\nprint(\"outplace2\", timeit('x + y', number=1000000, globals=locals()))\nprint(\"outplace3\", timeit('x.addcmul(y,z)', number=1000000, globals=locals()))\nprint(\"inplace1\", timeit('x.add_(y)', number=1000000, globals=locals()))\nprint(\"inplace2\", timeit('x.addcmul_(y,z)', number=1000000, globals=locals()))\n</code></pre>\n<p>without broadcasting code:</p>\n<pre><code>outplace2 1.170712536200881\noutplace3 0.6207181625068188\ninplace1 0.28007462061941624\ninplace2 0.2738169729709625\n</code></pre>\n<p>with existing broadcasting code:</p>\n<pre><code>outplace2 2.0662140138447285\noutplace3 1.7219338584691286\ninplace1 0.703955814242363\ninplace2 0.9586600493639708\n</code></pre>\n<p>with new broadcasting code:</p>\n<pre><code>outplace2 1.1887976974248886\noutplace3 0.6337768454104662\ninplace1 0.27564410865306854\ninplace2 0.29433456622064114\n</code></pre>\n<p>Note that this is potentially slower in the case where broadcasting is required, since we do an extra shape check before executing the old code.  Obviously, this could optimized with some extra work, but it's already probably a good tradeoff given broadcasting is probably relatively rare.  To compare the case where broadcasting is enabled, change the benchmark line that sets y to:</p>\n<pre><code>y = torch.randn(5)\n</code></pre>\n<p>Here are the results then:<br>\nbefore change:</p>\n<pre><code>outplace2 1.8439689576625824\noutplace3 1.8589385487139225\ninplace1 0.7429333087056875\ninplace2 1.070244874805212\n</code></pre>\n<p>after change:</p>\n<pre><code>outplace2 1.841021191328764\noutplace3 1.8419923204928637\ninplace1 0.7541772909462452\ninplace2 1.0395784191787243\n</code></pre>\n<p>so it doesn't seem to make much of a difference and it's probably not worth further optimizing before getting this in.</p>", "body_text": "Addresses at least some of #2012.\nHere's a microbenchmark (modeled after @apaszke's) that measures 4 common cases (outplace vs inplace, 2 args vs 3 args):\nimport torch\nfrom timeit import timeit\n\nx = torch.randn(5, 5)\ny = torch.randn(5, 5)\nz = torch.randn(5, 5)\n\nprint(\"outplace2\", timeit('x + y', number=1000000, globals=locals()))\nprint(\"outplace3\", timeit('x.addcmul(y,z)', number=1000000, globals=locals()))\nprint(\"inplace1\", timeit('x.add_(y)', number=1000000, globals=locals()))\nprint(\"inplace2\", timeit('x.addcmul_(y,z)', number=1000000, globals=locals()))\n\nwithout broadcasting code:\noutplace2 1.170712536200881\noutplace3 0.6207181625068188\ninplace1 0.28007462061941624\ninplace2 0.2738169729709625\n\nwith existing broadcasting code:\noutplace2 2.0662140138447285\noutplace3 1.7219338584691286\ninplace1 0.703955814242363\ninplace2 0.9586600493639708\n\nwith new broadcasting code:\noutplace2 1.1887976974248886\noutplace3 0.6337768454104662\ninplace1 0.27564410865306854\ninplace2 0.29433456622064114\n\nNote that this is potentially slower in the case where broadcasting is required, since we do an extra shape check before executing the old code.  Obviously, this could optimized with some extra work, but it's already probably a good tradeoff given broadcasting is probably relatively rare.  To compare the case where broadcasting is enabled, change the benchmark line that sets y to:\ny = torch.randn(5)\n\nHere are the results then:\nbefore change:\noutplace2 1.8439689576625824\noutplace3 1.8589385487139225\ninplace1 0.7429333087056875\ninplace2 1.070244874805212\n\nafter change:\noutplace2 1.841021191328764\noutplace3 1.8419923204928637\ninplace1 0.7541772909462452\ninplace2 1.0395784191787243\n\nso it doesn't seem to make much of a difference and it's probably not worth further optimizing before getting this in.", "body": "Addresses at least some of https://github.com/pytorch/pytorch/issues/2012.\r\n\r\nHere's a microbenchmark (modeled after @apaszke's) that measures 4 common cases (outplace vs inplace, 2 args vs 3 args):\r\n```\r\nimport torch\r\nfrom timeit import timeit\r\n\r\nx = torch.randn(5, 5)\r\ny = torch.randn(5, 5)\r\nz = torch.randn(5, 5)\r\n\r\nprint(\"outplace2\", timeit('x + y', number=1000000, globals=locals()))\r\nprint(\"outplace3\", timeit('x.addcmul(y,z)', number=1000000, globals=locals()))\r\nprint(\"inplace1\", timeit('x.add_(y)', number=1000000, globals=locals()))\r\nprint(\"inplace2\", timeit('x.addcmul_(y,z)', number=1000000, globals=locals()))\r\n```\r\n\r\nwithout broadcasting code:\r\n```\r\noutplace2 1.170712536200881\r\noutplace3 0.6207181625068188\r\ninplace1 0.28007462061941624\r\ninplace2 0.2738169729709625\r\n```\r\n\r\nwith existing broadcasting code:\r\n```\r\noutplace2 2.0662140138447285\r\noutplace3 1.7219338584691286\r\ninplace1 0.703955814242363\r\ninplace2 0.9586600493639708\r\n```\r\n\r\nwith new broadcasting code:\r\n```\r\noutplace2 1.1887976974248886\r\noutplace3 0.6337768454104662\r\ninplace1 0.27564410865306854\r\ninplace2 0.29433456622064114\r\n```\r\n\r\nNote that this is potentially slower in the case where broadcasting is required, since we do an extra shape check before executing the old code.  Obviously, this could optimized with some extra work, but it's already probably a good tradeoff given broadcasting is probably relatively rare.  To compare the case where broadcasting is enabled, change the benchmark line that sets y to:\r\n```\r\ny = torch.randn(5)\r\n```\r\n\r\nHere are the results then:\r\nbefore change:\r\n```\r\noutplace2 1.8439689576625824\r\noutplace3 1.8589385487139225\r\ninplace1 0.7429333087056875\r\ninplace2 1.070244874805212\r\n```\r\n\r\nafter change:\r\n```\r\noutplace2 1.841021191328764\r\noutplace3 1.8419923204928637\r\ninplace1 0.7541772909462452\r\ninplace2 1.0395784191787243\r\n```\r\n\r\nso it doesn't seem to make much of a difference and it's probably not worth further optimizing before getting this in."}