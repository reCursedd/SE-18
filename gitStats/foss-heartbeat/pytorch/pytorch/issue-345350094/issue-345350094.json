{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9950", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9950/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9950/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9950/events", "html_url": "https://github.com/pytorch/pytorch/issues/9950", "id": 345350094, "node_id": "MDU6SXNzdWUzNDUzNTAwOTQ=", "number": 9950, "title": "Stop passing inplace/out arguments as (non-const) Tensor& to functions", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-27T20:03:20Z", "updated_at": "2018-07-27T20:03:20Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Currently, when you define an inplace function or a function that writes to an output function, you get a non-const reference. E.g.,</p>\n<pre><code>Tensor&amp; add_out(Tensor&amp; result, const Tensor&amp; self, const Tensor&amp; other);\n</code></pre>\n<p>The use of a non-const reference here is highly misleading, and can lead newbies down the wrong path when writing implementations of these functions. We should instead pass these arguments as ordinary <code>const Tensor&amp;</code> references.</p>\n<p><strong>Detailed discussion.</strong></p>\n<p>Consider the function above.  Assuming that <code>add</code> is implemented for tensors, is the following a valid implementation of the function?</p>\n<pre><code>Tensor&amp; add_out(Tensor&amp; result, const Tensor&amp; self, const Tensor&amp; other) {\n  result = self.add(other);\n  return result;\n}\n</code></pre>\n<p>C++ will not give any error when you do this, but the function will not behave correctly when used from Python:</p>\n<pre><code>x = torch.randn(N)\ny = torch.randn(N)\nout = torch.zeros(N)\nout2 = out # refers to the same tensor\nadd_out(x, y, out=out2)\nprint(out) # out is all zeros?!\n</code></pre>\n<p>The intention of the <code>add_out</code> function was to write the result of <code>x + y</code> into the preallocated memory of <code>zeros</code>. <code>out2</code> and <code>out</code> alias the same memory, so modifications to out2 should be reflected in out.</p>\n<p>The reason for this is that when you wrote <code>result = self.add(other);</code>, you did not actually change the allocated memory of out, because your implementation was semantically equivalent to this Python code:</p>\n<pre><code>x = torch.randn(N)\ny = torch.randn(N)\nout = torch.zeros(N)\nout2 = out\nout2 = x.add(y)  # the code you wrote\nprint(out) # well of course this is zeros\n</code></pre>\n<p>Yes, you modified the C++ reference, but C++ references are meaningless in Python (there is no such thing as C++ style references in Python). You need to modify the actual <em>underlying memory</em> of the out tensor object when you implement <code>add_out</code>, and assignment over a reference doesn't do that; all it does is change the <code>Tensor</code> <em>pointer</em> that was passed in. Other aliases to the same memory will not see the difference.</p>\n<p><strong>What should you do instead?</strong></p>\n<p>If the final operation you do is call another function, see if you can call it's inplace/out variant instead. For example, you could implement <code>sub_out</code> as:</p>\n<pre><code>Tensor&amp; sub_out(Tensor&amp; result, const Tensor&amp; self, const Tensor&amp; other) {\n  return add_out(result, self, -other);\n}\n</code></pre>\n<p>If you are actually writing a kernel (e.g., you're writing code that writes into memory locations in a loop), simply make sure that result is the right size, and just write your results directly into it (rather than allocate a fresh buffer which you would have returned). In this case, the function is typically factored into an <code>_out</code> variant that doesn't allocate, and a regular, functional variant that simply calls the <code>_out</code> implementation after allocating the result.  Example in aten/src/ATen/native/ReduceOps.cpp:</p>\n<pre><code>Tensor _prod(const Tensor &amp;self, int64_t dim_, bool keepdim) {\n  int64_t dim = maybe_wrap_dim(dim_, self.dim());\n  Tensor result = self.type().tensor();\n  return at::_prod_out(result, self, dim, keepdim);\n}\n\nTensor&amp; prod_out(Tensor&amp; result, const Tensor&amp; self, int64_t dim, bool keepdim) {\n  // code that writes the result into result\n}\n</code></pre>\n<p>If you're writing some code that calls into an external library, and it insists on giving you a fresh tensor, you can always smooth over the problem by doing a copy. So, to fix our add_out example from the beginning:</p>\n<pre><code>Tensor&amp; add_out(Tensor&amp; result, const Tensor&amp; self, const Tensor&amp; other) {\n  result.resize_(self.sizes());\n  result.copy_(self.add(other));\n}\n</code></pre>\n<p>Another idiom is, if an inplace version of the function is available, copy the input into the eventual result buffer, and then do the inplace operation on that buffer. Example from UnaryOps.cpp</p>\n<pre><code>Tensor&amp; _clamp_out_cpu(Tensor&amp; result, const Tensor&amp; self, Scalar min, Scalar max) {\n  result.resize_(self.sizes());\n  result.copy_(self);\n  return _th_clamp_(result, min, max);\n} \n</code></pre>\n<p>These strategies are less efficient, so they should be used as a last resort.</p>", "body_text": "Currently, when you define an inplace function or a function that writes to an output function, you get a non-const reference. E.g.,\nTensor& add_out(Tensor& result, const Tensor& self, const Tensor& other);\n\nThe use of a non-const reference here is highly misleading, and can lead newbies down the wrong path when writing implementations of these functions. We should instead pass these arguments as ordinary const Tensor& references.\nDetailed discussion.\nConsider the function above.  Assuming that add is implemented for tensors, is the following a valid implementation of the function?\nTensor& add_out(Tensor& result, const Tensor& self, const Tensor& other) {\n  result = self.add(other);\n  return result;\n}\n\nC++ will not give any error when you do this, but the function will not behave correctly when used from Python:\nx = torch.randn(N)\ny = torch.randn(N)\nout = torch.zeros(N)\nout2 = out # refers to the same tensor\nadd_out(x, y, out=out2)\nprint(out) # out is all zeros?!\n\nThe intention of the add_out function was to write the result of x + y into the preallocated memory of zeros. out2 and out alias the same memory, so modifications to out2 should be reflected in out.\nThe reason for this is that when you wrote result = self.add(other);, you did not actually change the allocated memory of out, because your implementation was semantically equivalent to this Python code:\nx = torch.randn(N)\ny = torch.randn(N)\nout = torch.zeros(N)\nout2 = out\nout2 = x.add(y)  # the code you wrote\nprint(out) # well of course this is zeros\n\nYes, you modified the C++ reference, but C++ references are meaningless in Python (there is no such thing as C++ style references in Python). You need to modify the actual underlying memory of the out tensor object when you implement add_out, and assignment over a reference doesn't do that; all it does is change the Tensor pointer that was passed in. Other aliases to the same memory will not see the difference.\nWhat should you do instead?\nIf the final operation you do is call another function, see if you can call it's inplace/out variant instead. For example, you could implement sub_out as:\nTensor& sub_out(Tensor& result, const Tensor& self, const Tensor& other) {\n  return add_out(result, self, -other);\n}\n\nIf you are actually writing a kernel (e.g., you're writing code that writes into memory locations in a loop), simply make sure that result is the right size, and just write your results directly into it (rather than allocate a fresh buffer which you would have returned). In this case, the function is typically factored into an _out variant that doesn't allocate, and a regular, functional variant that simply calls the _out implementation after allocating the result.  Example in aten/src/ATen/native/ReduceOps.cpp:\nTensor _prod(const Tensor &self, int64_t dim_, bool keepdim) {\n  int64_t dim = maybe_wrap_dim(dim_, self.dim());\n  Tensor result = self.type().tensor();\n  return at::_prod_out(result, self, dim, keepdim);\n}\n\nTensor& prod_out(Tensor& result, const Tensor& self, int64_t dim, bool keepdim) {\n  // code that writes the result into result\n}\n\nIf you're writing some code that calls into an external library, and it insists on giving you a fresh tensor, you can always smooth over the problem by doing a copy. So, to fix our add_out example from the beginning:\nTensor& add_out(Tensor& result, const Tensor& self, const Tensor& other) {\n  result.resize_(self.sizes());\n  result.copy_(self.add(other));\n}\n\nAnother idiom is, if an inplace version of the function is available, copy the input into the eventual result buffer, and then do the inplace operation on that buffer. Example from UnaryOps.cpp\nTensor& _clamp_out_cpu(Tensor& result, const Tensor& self, Scalar min, Scalar max) {\n  result.resize_(self.sizes());\n  result.copy_(self);\n  return _th_clamp_(result, min, max);\n} \n\nThese strategies are less efficient, so they should be used as a last resort.", "body": "Currently, when you define an inplace function or a function that writes to an output function, you get a non-const reference. E.g.,\r\n\r\n```\r\nTensor& add_out(Tensor& result, const Tensor& self, const Tensor& other);\r\n```\r\n\r\nThe use of a non-const reference here is highly misleading, and can lead newbies down the wrong path when writing implementations of these functions. We should instead pass these arguments as ordinary `const Tensor&` references.\r\n\r\n**Detailed discussion.**\r\n\r\nConsider the function above.  Assuming that `add` is implemented for tensors, is the following a valid implementation of the function?\r\n\r\n```\r\nTensor& add_out(Tensor& result, const Tensor& self, const Tensor& other) {\r\n  result = self.add(other);\r\n  return result;\r\n}\r\n```\r\n\r\nC++ will not give any error when you do this, but the function will not behave correctly when used from Python:\r\n\r\n```\r\nx = torch.randn(N)\r\ny = torch.randn(N)\r\nout = torch.zeros(N)\r\nout2 = out # refers to the same tensor\r\nadd_out(x, y, out=out2)\r\nprint(out) # out is all zeros?!\r\n```\r\n\r\nThe intention of the `add_out` function was to write the result of `x + y` into the preallocated memory of `zeros`. `out2` and `out` alias the same memory, so modifications to out2 should be reflected in out.\r\n\r\nThe reason for this is that when you wrote `result = self.add(other);`, you did not actually change the allocated memory of out, because your implementation was semantically equivalent to this Python code:\r\n\r\n```\r\nx = torch.randn(N)\r\ny = torch.randn(N)\r\nout = torch.zeros(N)\r\nout2 = out\r\nout2 = x.add(y)  # the code you wrote\r\nprint(out) # well of course this is zeros\r\n```\r\n\r\nYes, you modified the C++ reference, but C++ references are meaningless in Python (there is no such thing as C++ style references in Python). You need to modify the actual *underlying memory* of the out tensor object when you implement `add_out`, and assignment over a reference doesn't do that; all it does is change the `Tensor` *pointer* that was passed in. Other aliases to the same memory will not see the difference.\r\n\r\n**What should you do instead?**\r\n\r\nIf the final operation you do is call another function, see if you can call it's inplace/out variant instead. For example, you could implement `sub_out` as:\r\n\r\n```\r\nTensor& sub_out(Tensor& result, const Tensor& self, const Tensor& other) {\r\n  return add_out(result, self, -other);\r\n}\r\n```\r\n\r\nIf you are actually writing a kernel (e.g., you're writing code that writes into memory locations in a loop), simply make sure that result is the right size, and just write your results directly into it (rather than allocate a fresh buffer which you would have returned). In this case, the function is typically factored into an `_out` variant that doesn't allocate, and a regular, functional variant that simply calls the `_out` implementation after allocating the result.  Example in aten/src/ATen/native/ReduceOps.cpp:\r\n\r\n```\r\nTensor _prod(const Tensor &self, int64_t dim_, bool keepdim) {\r\n  int64_t dim = maybe_wrap_dim(dim_, self.dim());\r\n  Tensor result = self.type().tensor();\r\n  return at::_prod_out(result, self, dim, keepdim);\r\n}\r\n\r\nTensor& prod_out(Tensor& result, const Tensor& self, int64_t dim, bool keepdim) {\r\n  // code that writes the result into result\r\n}\r\n```\r\n\r\nIf you're writing some code that calls into an external library, and it insists on giving you a fresh tensor, you can always smooth over the problem by doing a copy. So, to fix our add_out example from the beginning:\r\n\r\n```\r\nTensor& add_out(Tensor& result, const Tensor& self, const Tensor& other) {\r\n  result.resize_(self.sizes());\r\n  result.copy_(self.add(other));\r\n}\r\n```\r\n\r\nAnother idiom is, if an inplace version of the function is available, copy the input into the eventual result buffer, and then do the inplace operation on that buffer. Example from UnaryOps.cpp\r\n\r\n```\r\nTensor& _clamp_out_cpu(Tensor& result, const Tensor& self, Scalar min, Scalar max) {\r\n  result.resize_(self.sizes());\r\n  result.copy_(self);\r\n  return _th_clamp_(result, min, max);\r\n} \r\n```\r\n\r\nThese strategies are less efficient, so they should be used as a last resort."}