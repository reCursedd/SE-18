{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10004", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10004/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10004/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10004/events", "html_url": "https://github.com/pytorch/pytorch/issues/10004", "id": 345728636, "node_id": "MDU6SXNzdWUzNDU3Mjg2MzY=", "number": 10004, "title": "Detaching gradients instead of using no_grad in spectral_norm", "user": {"login": "ferrine", "id": 11705326, "node_id": "MDQ6VXNlcjExNzA1MzI2", "avatar_url": "https://avatars0.githubusercontent.com/u/11705326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ferrine", "html_url": "https://github.com/ferrine", "followers_url": "https://api.github.com/users/ferrine/followers", "following_url": "https://api.github.com/users/ferrine/following{/other_user}", "gists_url": "https://api.github.com/users/ferrine/gists{/gist_id}", "starred_url": "https://api.github.com/users/ferrine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ferrine/subscriptions", "organizations_url": "https://api.github.com/users/ferrine/orgs", "repos_url": "https://api.github.com/users/ferrine/repos", "events_url": "https://api.github.com/users/ferrine/events{/privacy}", "received_events_url": "https://api.github.com/users/ferrine/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-07-30T12:03:09Z", "updated_at": "2018-07-30T20:51:51Z", "closed_at": "2018-07-30T17:32:55Z", "author_association": "NONE", "body_html": "<p>Looking at <a href=\"https://github.com/pytorch/pytorch/blob/3609977d7f4629f75dd1f8d904ddd5b52388124f/torch/nn/utils/spectral_norm.py#L53\">this</a> code I've noticed that there is an easy way to make <code>u</code>, <code>v</code> differentiable wrt <code>W</code> (it is useful as normalizing constant <code>sigma</code> depends on <code>u</code>, <code>v</code> and <code>u</code>, <code>v</code> depend on <code>W</code>).<br>\nIf we <code>detach</code> starting point of power iteration <code>u</code> we avoid cumulating gradients problem while allowing gradients pass through power iteration. As it adds some computational overhead it can be optional.</p>", "body_text": "Looking at this code I've noticed that there is an easy way to make u, v differentiable wrt W (it is useful as normalizing constant sigma depends on u, v and u, v depend on W).\nIf we detach starting point of power iteration u we avoid cumulating gradients problem while allowing gradients pass through power iteration. As it adds some computational overhead it can be optional.", "body": "Looking at [this](https://github.com/pytorch/pytorch/blob/3609977d7f4629f75dd1f8d904ddd5b52388124f/torch/nn/utils/spectral_norm.py#L53) code I've noticed that there is an easy way to make `u`, `v` differentiable wrt `W` (it is useful as normalizing constant `sigma` depends on `u`, `v` and `u`, `v` depend on `W`). \r\nIf we `detach` starting point of power iteration `u` we avoid cumulating gradients problem while allowing gradients pass through power iteration. As it adds some computational overhead it can be optional.\r\n"}