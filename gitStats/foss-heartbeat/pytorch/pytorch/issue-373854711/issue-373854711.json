{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13115", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13115/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13115/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13115/events", "html_url": "https://github.com/pytorch/pytorch/issues/13115", "id": 373854711, "node_id": "MDU6SXNzdWUzNzM4NTQ3MTE=", "number": 13115, "title": "running time", "user": {"login": "YunYang1994", "id": 30433053, "node_id": "MDQ6VXNlcjMwNDMzMDUz", "avatar_url": "https://avatars3.githubusercontent.com/u/30433053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YunYang1994", "html_url": "https://github.com/YunYang1994", "followers_url": "https://api.github.com/users/YunYang1994/followers", "following_url": "https://api.github.com/users/YunYang1994/following{/other_user}", "gists_url": "https://api.github.com/users/YunYang1994/gists{/gist_id}", "starred_url": "https://api.github.com/users/YunYang1994/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YunYang1994/subscriptions", "organizations_url": "https://api.github.com/users/YunYang1994/orgs", "repos_url": "https://api.github.com/users/YunYang1994/repos", "events_url": "https://api.github.com/users/YunYang1994/events{/privacy}", "received_events_url": "https://api.github.com/users/YunYang1994/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-25T09:29:00Z", "updated_at": "2018-10-26T14:26:10Z", "closed_at": "2018-10-25T14:44:30Z", "author_association": "NONE", "body_html": "<p>I detach <a href=\"https://github.com/zdevito/ATen/tree/master/aten\">ATen</a> library from pytorch and compile them into dynamic library (\".so\" files), then target my neural network model(\".cpp\" files) into the ATen (\".so\") and compile it. <em>My question is that It seems running on ATen library need much more time than pytorch(GPU). I don't think this is reasonable !</em></p>\n<h2>running time</h2>\n<table>\n<thead>\n<tr>\n<th>iterations</th>\n<th>10</th>\n<th>100</th>\n<th>1000</th>\n<th>5000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>pytorch(GPU)</code></td>\n<td>0.1174318790 s</td>\n<td>1.107207059 s</td>\n<td>11.13777494 s</td>\n<td>55.82848405 s</td>\n</tr>\n<tr>\n<td><code>pytorch(CPU)</code></td>\n<td>0.7973520755 s</td>\n<td>7.873554949 s</td>\n<td>78.74021005 s</td>\n<td>392.61105028 s</td>\n</tr>\n<tr>\n<td><code>ATen</code></td>\n<td>0.805274 s</td>\n<td>1.8661 s</td>\n<td>11.983 s</td>\n<td>58.0243 s</td>\n</tr>\n</tbody>\n</table>\n<p>here is my code for both pytorch and ATen to test resnet18 model</p>\n<h2>using pytorch</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> src.resnet18_onnx <span class=\"pl-k\">import</span> Predictor\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> time\n\nmodel <span class=\"pl-k\">=</span> Predictor()\nimage <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">224</span>,<span class=\"pl-c1\">224</span>).to(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> model.to('cpu')</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> result = model.predict(image)</span>\n\nstart <span class=\"pl-k\">=</span> time.time()\niteration <span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(iteration):\n    result <span class=\"pl-k\">=</span> model.predict(image)\nresult.detach()\nend <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c1\">print</span> end <span class=\"pl-k\">-</span> start</pre></div>\n<h2>using ATen library</h2>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>memory<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>vector<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>ATen/ATen.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>time.h<span class=\"pl-pds\">&gt;</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> #include &lt;ATen/Context.h&gt;</span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>predict.h<span class=\"pl-pds\">&gt;</span></span>  <span class=\"pl-c\"><span class=\"pl-c\">//</span> see predict.h in the include directory</span>\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">at</span><span class=\"pl-k\">;</span>\n\n<span class=\"pl-k\">int</span> <span class=\"pl-en\">main</span>(<span class=\"pl-k\">int</span> argc, <span class=\"pl-k\">char</span>** argv) {\n\n    std::shared_ptr&lt;Predictor&gt; a = std::make_shared&lt;Predictor&gt; ();\n    std::vector&lt;<span class=\"pl-k\">float</span>&gt; <span class=\"pl-c1\">data</span>(<span class=\"pl-c1\">1</span>*<span class=\"pl-c1\">3</span>*<span class=\"pl-c1\">224</span>*<span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">1</span>);\n\n    <span class=\"pl-k\">int</span> iteration = <span class=\"pl-c1\">1000</span>;\n    <span class=\"pl-c1\">clock_t</span> t_start, t_end;\n    <span class=\"pl-k\">double</span> totaltime;\n\n    t_start = <span class=\"pl-c1\">clock</span>();\n    <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> i=<span class=\"pl-c1\">0</span>; i&lt;iteration; i++){\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> std::cout &lt;&lt; a-&gt;predict(&amp;data[0]) &lt;&lt; std::endl;</span>\n        a-&gt;<span class=\"pl-c1\">predict</span>(&amp;data[<span class=\"pl-c1\">0</span>]);\n    }\n    t_end = <span class=\"pl-c1\">clock</span>();\n    totaltime = (<span class=\"pl-k\">double</span>)(t_end - t_start)/CLOCKS_PER_SEC;\n    std::cout &lt;&lt; totaltime &lt;&lt; std::endl;\n\n}</pre></div>\n<p><a href=\"https://github.com/YunYang1994/Onnx2Aten/blob/master/include/predict.h\">predict.h</a></p>\n<p>more details please see <a href=\"https://github.com/YunYang1994/Onnx2Aten\">my code</a>, do you have any ideas about it? Looking forward to your reply!</p>", "body_text": "I detach ATen library from pytorch and compile them into dynamic library (\".so\" files), then target my neural network model(\".cpp\" files) into the ATen (\".so\") and compile it. My question is that It seems running on ATen library need much more time than pytorch(GPU). I don't think this is reasonable !\nrunning time\n\n\n\niterations\n10\n100\n1000\n5000\n\n\n\n\npytorch(GPU)\n0.1174318790 s\n1.107207059 s\n11.13777494 s\n55.82848405 s\n\n\npytorch(CPU)\n0.7973520755 s\n7.873554949 s\n78.74021005 s\n392.61105028 s\n\n\nATen\n0.805274 s\n1.8661 s\n11.983 s\n58.0243 s\n\n\n\nhere is my code for both pytorch and ATen to test resnet18 model\nusing pytorch\nfrom src.resnet18_onnx import Predictor\nimport torch\nimport time\n\nmodel = Predictor()\nimage = torch.ones(1,3,224,224).to('cpu')\n# model.to('cpu')\n# result = model.predict(image)\n\nstart = time.time()\niteration =1000\nfor i in range(iteration):\n    result = model.predict(image)\nresult.detach()\nend = time.time()\nprint end - start\nusing ATen library\n#include <iostream>\n#include <memory>\n#include <vector>\n#include <ATen/ATen.h>\n#include <time.h>\n// #include <ATen/Context.h>\n#include <predict.h>  // see predict.h in the include directory\nusing namespace at;\n\nint main(int argc, char** argv) {\n\n    std::shared_ptr<Predictor> a = std::make_shared<Predictor> ();\n    std::vector<float> data(1*3*224*224, 1);\n\n    int iteration = 1000;\n    clock_t t_start, t_end;\n    double totaltime;\n\n    t_start = clock();\n    for(int i=0; i<iteration; i++){\n        // std::cout << a->predict(&data[0]) << std::endl;\n        a->predict(&data[0]);\n    }\n    t_end = clock();\n    totaltime = (double)(t_end - t_start)/CLOCKS_PER_SEC;\n    std::cout << totaltime << std::endl;\n\n}\npredict.h\nmore details please see my code, do you have any ideas about it? Looking forward to your reply!", "body": "I detach [ATen](https://github.com/zdevito/ATen/tree/master/aten) library from pytorch and compile them into dynamic library (\".so\" files), then target my neural network model(\".cpp\" files) into the ATen (\".so\") and compile it. *My question is that It seems running on ATen library need much more time than pytorch(GPU). I don't think this is reasonable !*\r\n## running time\r\n| iterations | 10 | 100 | 1000 | 5000 |\r\n| ---------- | -----------| ---------- | -----------| ---------- |\r\n| `pytorch(GPU)`   | 0.1174318790 s  | 1.107207059 s | 11.13777494 s | 55.82848405 s |\r\n| `pytorch(CPU)`   | 0.7973520755 s  | 7.873554949 s | 78.74021005 s |  392.61105028 s |\r\n| `ATen`   | 0.805274 s  | 1.8661 s | 11.983 s | 58.0243 s |\r\n\r\n\r\nhere is my code for both pytorch and ATen to test resnet18 model\r\n## using pytorch\r\n```py\r\nfrom src.resnet18_onnx import Predictor\r\nimport torch\r\nimport time\r\n\r\nmodel = Predictor()\r\nimage = torch.ones(1,3,224,224).to('cpu')\r\n# model.to('cpu')\r\n# result = model.predict(image)\r\n\r\nstart = time.time()\r\niteration =1000\r\nfor i in range(iteration):\r\n    result = model.predict(image)\r\nresult.detach()\r\nend = time.time()\r\nprint end - start\r\n```\r\n\r\n## using ATen library\r\n```cpp\r\n#include <iostream>\r\n#include <memory>\r\n#include <vector>\r\n#include <ATen/ATen.h>\r\n#include <time.h>\r\n// #include <ATen/Context.h>\r\n#include <predict.h>  // see predict.h in the include directory\r\nusing namespace at;\r\n\r\nint main(int argc, char** argv) {\r\n\r\n    std::shared_ptr<Predictor> a = std::make_shared<Predictor> ();\r\n    std::vector<float> data(1*3*224*224, 1);\r\n\r\n    int iteration = 1000;\r\n    clock_t t_start, t_end;\r\n    double totaltime;\r\n\r\n    t_start = clock();\r\n    for(int i=0; i<iteration; i++){\r\n        // std::cout << a->predict(&data[0]) << std::endl;\r\n        a->predict(&data[0]);\r\n    }\r\n    t_end = clock();\r\n    totaltime = (double)(t_end - t_start)/CLOCKS_PER_SEC;\r\n    std::cout << totaltime << std::endl;\r\n\r\n}\r\n```\r\n[predict.h](https://github.com/YunYang1994/Onnx2Aten/blob/master/include/predict.h) \r\n\r\nmore details please see [my code](https://github.com/YunYang1994/Onnx2Aten), do you have any ideas about it? Looking forward to your reply!\r\n"}