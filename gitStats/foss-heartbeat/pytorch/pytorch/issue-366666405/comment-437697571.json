{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/437697571", "html_url": "https://github.com/pytorch/pytorch/issues/12318#issuecomment-437697571", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12318", "id": 437697571, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzY5NzU3MQ==", "user": {"login": "abuvaneswari", "id": 15253225, "node_id": "MDQ6VXNlcjE1MjUzMjI1", "avatar_url": "https://avatars2.githubusercontent.com/u/15253225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abuvaneswari", "html_url": "https://github.com/abuvaneswari", "followers_url": "https://api.github.com/users/abuvaneswari/followers", "following_url": "https://api.github.com/users/abuvaneswari/following{/other_user}", "gists_url": "https://api.github.com/users/abuvaneswari/gists{/gist_id}", "starred_url": "https://api.github.com/users/abuvaneswari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abuvaneswari/subscriptions", "organizations_url": "https://api.github.com/users/abuvaneswari/orgs", "repos_url": "https://api.github.com/users/abuvaneswari/repos", "events_url": "https://api.github.com/users/abuvaneswari/events{/privacy}", "received_events_url": "https://api.github.com/users/abuvaneswari/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-11T19:31:09Z", "updated_at": "2018-11-11T19:43:28Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8038998\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/patterson163\">@patterson163</a> , thanks a lot for sharing valuable info on what libraries to use for CTCLoss and CTC decode going forward in PyTorch 1.0</p>\n<p>I tried comparing the results from CTCLoss of warp-ctc in PyTorch 0.4.0 and that of torch.nn in PyTorch 1.0 and I see the following difference.</p>\n<p>Am I doing anything wrong in passing the parameters?<br>\nDo I have to reshape the tensors that I originally pass into warp-ctc's CTCLoss when I start to use torch.nn.CTCLoss?</p>\n<p>As a side note, when I swapped in the CTCLoss of torch.nn in <a href=\"https://github.com/SeanNaren/deepspeech.pytorch\">deepspeech2 </a> code the CTC loss exploded to nan within 4 steps into training. I did not change the shape of the tensors as I am not clear if the APIs of torch.nn.CTCLoss and warpctc_pytorch.CTCLoss are different from each other.</p>\n<h3>-------- warp-ctc, CTCLoss ------</h3>\n<p>[ds2@blipp73 ]$ python<br>\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)<br>\n[GCC 7.2.0] on linux<br>\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>import torch<br>\nfrom warpctc_pytorch import CTCLoss<br>\nctc_loss = CTCLoss()</p>\n<p>probs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous() # expected shape of seqLength x batchSize x alphabet_size<br>\nlabels = torch.IntTensor([1, 2])<br>\nlabel_sizes = torch.IntTensor([2])<br>\nprobs_sizes = torch.IntTensor([2])<br>\nprobs.requires_grad_(True)  # tells autograd to compute gradients for probs<br>\ntensor([[[ 0.1000,  0.6000,  0.1000,  0.1000,  0.1000]],        [[ 0.1000,  0.1000,  0.6000,  0.1000,  0.1000]]])<br>\ncost = ctc_loss(probs, labels, probs_sizes, label_sizes)<br>\ncost.backward()<br>\ncost.data<br>\n<strong>tensor([ 2.4629])</strong></p>\n</blockquote>\n</blockquote>\n</blockquote>\n<h3>-------- PyTorch 1.0, CTCLoss ---</h3>\n<p>[ds2pth1@blipp73 ]$ python<br>\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)<br>\n[GCC 7.2.0] on linux<br>\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>import torch<br>\nfrom torch.nn import CTCLoss</p>\n<p>ctc_loss = CTCLoss()       #try with default reduction</p>\n<p>probs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous()    #  seqLength x batchSize x alphabet_size<br>\nlabels = torch.IntTensor([1, 2])<br>\nlabel_sizes = torch.IntTensor([2])<br>\nprobs_sizes = torch.IntTensor([2])<br>\nprobs.requires_grad_(True)  # tells autograd to compute gradients for probs<br>\ntensor([[[0.1000, 0.6000, 0.1000, 0.1000, 0.1000]],        [[0.1000, 0.1000, 0.6000, 0.1000, 0.1000]]], requires_grad=True)<br>\ncost = ctc_loss(probs, labels, probs_sizes, label_sizes)<br>\ncost.backward()<br>\ncost.data<br>\n<strong>tensor(-0.6000)</strong></p>\n<p>ctc_loss = CTCLoss(reduction='none')<br>\ncost = ctc_loss(probs, labels, probs_sizes, label_sizes)<br>\ncost.backward()<br>\ncost.data<br>\n<strong>tensor([-1.2000])</strong></p>\n</blockquote>\n</blockquote>\n</blockquote>", "body_text": "@patterson163 , thanks a lot for sharing valuable info on what libraries to use for CTCLoss and CTC decode going forward in PyTorch 1.0\nI tried comparing the results from CTCLoss of warp-ctc in PyTorch 0.4.0 and that of torch.nn in PyTorch 1.0 and I see the following difference.\nAm I doing anything wrong in passing the parameters?\nDo I have to reshape the tensors that I originally pass into warp-ctc's CTCLoss when I start to use torch.nn.CTCLoss?\nAs a side note, when I swapped in the CTCLoss of torch.nn in deepspeech2  code the CTC loss exploded to nan within 4 steps into training. I did not change the shape of the tensors as I am not clear if the APIs of torch.nn.CTCLoss and warpctc_pytorch.CTCLoss are different from each other.\n-------- warp-ctc, CTCLoss ------\n[ds2@blipp73 ]$ python\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\n[GCC 7.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport torch\nfrom warpctc_pytorch import CTCLoss\nctc_loss = CTCLoss()\nprobs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous() # expected shape of seqLength x batchSize x alphabet_size\nlabels = torch.IntTensor([1, 2])\nlabel_sizes = torch.IntTensor([2])\nprobs_sizes = torch.IntTensor([2])\nprobs.requires_grad_(True)  # tells autograd to compute gradients for probs\ntensor([[[ 0.1000,  0.6000,  0.1000,  0.1000,  0.1000]],        [[ 0.1000,  0.1000,  0.6000,  0.1000,  0.1000]]])\ncost = ctc_loss(probs, labels, probs_sizes, label_sizes)\ncost.backward()\ncost.data\ntensor([ 2.4629])\n\n\n\n-------- PyTorch 1.0, CTCLoss ---\n[ds2pth1@blipp73 ]$ python\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\n[GCC 7.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport torch\nfrom torch.nn import CTCLoss\nctc_loss = CTCLoss()       #try with default reduction\nprobs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous()    #  seqLength x batchSize x alphabet_size\nlabels = torch.IntTensor([1, 2])\nlabel_sizes = torch.IntTensor([2])\nprobs_sizes = torch.IntTensor([2])\nprobs.requires_grad_(True)  # tells autograd to compute gradients for probs\ntensor([[[0.1000, 0.6000, 0.1000, 0.1000, 0.1000]],        [[0.1000, 0.1000, 0.6000, 0.1000, 0.1000]]], requires_grad=True)\ncost = ctc_loss(probs, labels, probs_sizes, label_sizes)\ncost.backward()\ncost.data\ntensor(-0.6000)\nctc_loss = CTCLoss(reduction='none')\ncost = ctc_loss(probs, labels, probs_sizes, label_sizes)\ncost.backward()\ncost.data\ntensor([-1.2000])", "body": "@patterson163 , thanks a lot for sharing valuable info on what libraries to use for CTCLoss and CTC decode going forward in PyTorch 1.0\r\n\r\nI tried comparing the results from CTCLoss of warp-ctc in PyTorch 0.4.0 and that of torch.nn in PyTorch 1.0 and I see the following difference. \r\n\r\nAm I doing anything wrong in passing the parameters? \r\nDo I have to reshape the tensors that I originally pass into warp-ctc's CTCLoss when I start to use torch.nn.CTCLoss?\r\n\r\nAs a side note, when I swapped in the CTCLoss of torch.nn in [deepspeech2 ](https://github.com/SeanNaren/deepspeech.pytorch) code the CTC loss exploded to nan within 4 steps into training. I did not change the shape of the tensors as I am not clear if the APIs of torch.nn.CTCLoss and warpctc_pytorch.CTCLoss are different from each other.\r\n\r\n### -------- warp-ctc, CTCLoss ------\r\n[ds2@blipp73 ]$ python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> \r\n>>> import torch\r\n>>> from warpctc_pytorch import CTCLoss\r\n>>> ctc_loss = CTCLoss()\r\n>>> \r\n>>> probs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous() # expected shape of seqLength x batchSize x alphabet_size\r\n>>> labels = torch.IntTensor([1, 2])\r\n>>> label_sizes = torch.IntTensor([2])\r\n>>> probs_sizes = torch.IntTensor([2])\r\n>>> probs.requires_grad_(True)  # tells autograd to compute gradients for probs\r\ntensor([[[ 0.1000,  0.6000,  0.1000,  0.1000,  0.1000]],        [[ 0.1000,  0.1000,  0.6000,  0.1000,  0.1000]]])\r\n>>> cost = ctc_loss(probs, labels, probs_sizes, label_sizes)\r\n>>> cost.backward()\r\n>>> cost.data\r\n**tensor([ 2.4629])**\r\n\r\n\r\n### -------- PyTorch 1.0, CTCLoss ---\r\n[ds2pth1@blipp73 ]$ python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> from torch.nn import CTCLoss\r\n>>> \r\n>>> ctc_loss = CTCLoss()       #try with default reduction\r\n>>> \r\n>>> probs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous()    #  seqLength x batchSize x alphabet_size\r\n>>> labels = torch.IntTensor([1, 2])\r\n>>> label_sizes = torch.IntTensor([2])\r\n>>> probs_sizes = torch.IntTensor([2])\r\n>>> probs.requires_grad_(True)  # tells autograd to compute gradients for probs\r\ntensor([[[0.1000, 0.6000, 0.1000, 0.1000, 0.1000]],        [[0.1000, 0.1000, 0.6000, 0.1000, 0.1000]]], requires_grad=True)\r\n>>> cost = ctc_loss(probs, labels, probs_sizes, label_sizes)\r\n>>> cost.backward()\r\n>>> cost.data\r\n**tensor(-0.6000)**\r\n>>> \r\n>>> \r\n>>> ctc_loss = CTCLoss(reduction='none')\r\n>>> cost = ctc_loss(probs, labels, probs_sizes, label_sizes)\r\n>>> cost.backward()\r\n>>> cost.data\r\n**tensor([-1.2000])**\r\n\r\n"}