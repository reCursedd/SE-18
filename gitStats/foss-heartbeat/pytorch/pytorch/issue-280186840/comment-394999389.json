{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394999389", "html_url": "https://github.com/pytorch/pytorch/issues/4073#issuecomment-394999389", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4073", "id": 394999389, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDk5OTM4OQ==", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-06T09:06:49Z", "updated_at": "2018-06-06T09:07:48Z", "author_association": "NONE", "body_html": "<p>As far as I understood, the key here is to operate on the most compact momery buffer possible. Here is a general summary of this idea : <a href=\"https://devblogs.nvidia.com/how-access-global-memory-efficiently-cuda-c-kernels/\" rel=\"nofollow\">https://devblogs.nvidia.com/how-access-global-memory-efficiently-cuda-c-kernels/</a></p>\n<p>So when you are doing the operation <code>torch.sum(rinput1[b, i:i+K, j:j+K] * rinput2[b, i+k:i+k+K, j+l:j+l+K]</code> in a cuda kernel, you need the strides of both patches to be the lowest possible.</p>\n<p>When C is the last dimension, strides are <code>W*C, C, 1</code> . When C is the second dimension, strides are <code>H*W, W, 1</code><br>\nNow, depending on your number of channels, you might want to use one way of storing information or the other, but more generally, we have C &lt; H, hence the transposition.</p>\n<p>The weird thing however is that something similar to the <code>CUDA_KERNEL_LOOP</code> Macro is used (see here : <a href=\"https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/\" rel=\"nofollow\">https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/</a>) but only considering the number of channels. Why not do that directly with the index ? Is there a CUDA master able to explain this ?</p>\n<p>we might have to ask Fitsum Reda and Alexey Dosovitskiy for this as I believe Dosovitskiy wrote the first correlation kernel in caffe and  Reda adapted it to pytorch, but without touching too much the inner mechanisms.</p>", "body_text": "As far as I understood, the key here is to operate on the most compact momery buffer possible. Here is a general summary of this idea : https://devblogs.nvidia.com/how-access-global-memory-efficiently-cuda-c-kernels/\nSo when you are doing the operation torch.sum(rinput1[b, i:i+K, j:j+K] * rinput2[b, i+k:i+k+K, j+l:j+l+K] in a cuda kernel, you need the strides of both patches to be the lowest possible.\nWhen C is the last dimension, strides are W*C, C, 1 . When C is the second dimension, strides are H*W, W, 1\nNow, depending on your number of channels, you might want to use one way of storing information or the other, but more generally, we have C < H, hence the transposition.\nThe weird thing however is that something similar to the CUDA_KERNEL_LOOP Macro is used (see here : https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/) but only considering the number of channels. Why not do that directly with the index ? Is there a CUDA master able to explain this ?\nwe might have to ask Fitsum Reda and Alexey Dosovitskiy for this as I believe Dosovitskiy wrote the first correlation kernel in caffe and  Reda adapted it to pytorch, but without touching too much the inner mechanisms.", "body": "As far as I understood, the key here is to operate on the most compact momery buffer possible. Here is a general summary of this idea : https://devblogs.nvidia.com/how-access-global-memory-efficiently-cuda-c-kernels/\r\n\r\nSo when you are doing the operation `torch.sum(rinput1[b, i:i+K, j:j+K] * rinput2[b, i+k:i+k+K, j+l:j+l+K]` in a cuda kernel, you need the strides of both patches to be the lowest possible.\r\n\r\nWhen C is the last dimension, strides are `W*C, C, 1` . When C is the second dimension, strides are `H*W, W, 1`\r\nNow, depending on your number of channels, you might want to use one way of storing information or the other, but more generally, we have C < H, hence the transposition.\r\n\r\nThe weird thing however is that something similar to the `CUDA_KERNEL_LOOP` Macro is used (see here : https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/) but only considering the number of channels. Why not do that directly with the index ? Is there a CUDA master able to explain this ?\r\n\r\nwe might have to ask Fitsum Reda and Alexey Dosovitskiy for this as I believe Dosovitskiy wrote the first correlation kernel in caffe and  Reda adapted it to pytorch, but without touching too much the inner mechanisms."}