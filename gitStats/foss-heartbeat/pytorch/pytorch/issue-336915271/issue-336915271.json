{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9022", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9022/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9022/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9022/events", "html_url": "https://github.com/pytorch/pytorch/issues/9022", "id": 336915271, "node_id": "MDU6SXNzdWUzMzY5MTUyNzE=", "number": 9022, "title": "Help needed in torch.diag()", "user": {"login": "mirraaj", "id": 15613912, "node_id": "MDQ6VXNlcjE1NjEzOTEy", "avatar_url": "https://avatars0.githubusercontent.com/u/15613912?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mirraaj", "html_url": "https://github.com/mirraaj", "followers_url": "https://api.github.com/users/mirraaj/followers", "following_url": "https://api.github.com/users/mirraaj/following{/other_user}", "gists_url": "https://api.github.com/users/mirraaj/gists{/gist_id}", "starred_url": "https://api.github.com/users/mirraaj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mirraaj/subscriptions", "organizations_url": "https://api.github.com/users/mirraaj/orgs", "repos_url": "https://api.github.com/users/mirraaj/repos", "events_url": "https://api.github.com/users/mirraaj/events{/privacy}", "received_events_url": "https://api.github.com/users/mirraaj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-06-29T08:47:51Z", "updated_at": "2018-07-01T14:38:49Z", "closed_at": "2018-06-29T14:47:52Z", "author_association": "NONE", "body_html": "<pre><code>class ContinousActorCritic(nn.Module):\n  def __init__(self, observation_space, action_space, hidden_size, sigma=0.3):\n    super(ContinousActorCritic, self).__init__()\n    self.sigma = torch.tensor([sigma])\n    self.state_size = observation_space.shape[0]\n    # Action Space for COntinous Space\n    self.action_size = action_space.shape[0]\n\n    self.relu = nn.ReLU(inplace=True)\n    \n    self.fc1 = nn.Linear(self.state_size, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, hidden_size)\n\n    self.fc_actor = nn.Linear(hidden_size, self.action_size)\n    self.var_layer = nn.Linear(hidden_size, self.action_size)\n    self.var = nn.Softplus() # To get the softplus results\n    self.fc_critic_value = nn.Linear(hidden_size, 1)\n\n    self.action_input_layer = nn.Linear(self.action_size, hidden_size)\n    self.fc_critic_advantage = nn.Linear(hidden_size, 1)\n\n  def forward(self, x0):\n    Q = None\n    state = x0\n    x1 = self.relu(self.fc1(x0))\n    h = self.fc2(x1)  # h is (hidden state, cell state)\n    x = h # TODO : Remove this line from code\n    policy = self.fc_actor(x)  # Prevent 1s and hence NaNs\n\n    var_layer = self.var_layer(x)\n    var = self.var(var_layer)\n    print (var, policy)\n    sleep(10)\n\n    # action = policy.data + torch.normal(torch.zeros(policy.size()), torch.ones(policy.size())*0.01)\n    m = MultivariateNormal(policy, torch.diagnol(var))\n    action = m.sample()\n    print (action)\n    sleep(100)\n    action_samples = [Variable(m.sample()) for _ in range(5)]\n    advantage_samples = torch.cat([self.Advantage(x, action_sample).unsqueeze(-1) for action_sample in action_samples], -1)\n    print (advantage_samples)\n    sleep(100)\n    A = self.Advantage(x, action)\n    Q = V + A - advantage_samples.mean(-1)\n    return policy, Q, V, action#, var.log()\n</code></pre>\n<p>I am using the following code as a neural network for my reinforcement learning codes. I need to sample actions from a Gaussian distribution. My objective is to get the variance from the neural network and diagonalize it for sampling. Please let me know how should I proceed. It seems that torch.diag doesn't support batches of input.</p>", "body_text": "class ContinousActorCritic(nn.Module):\n  def __init__(self, observation_space, action_space, hidden_size, sigma=0.3):\n    super(ContinousActorCritic, self).__init__()\n    self.sigma = torch.tensor([sigma])\n    self.state_size = observation_space.shape[0]\n    # Action Space for COntinous Space\n    self.action_size = action_space.shape[0]\n\n    self.relu = nn.ReLU(inplace=True)\n    \n    self.fc1 = nn.Linear(self.state_size, hidden_size)\n    self.fc2 = nn.Linear(hidden_size, hidden_size)\n\n    self.fc_actor = nn.Linear(hidden_size, self.action_size)\n    self.var_layer = nn.Linear(hidden_size, self.action_size)\n    self.var = nn.Softplus() # To get the softplus results\n    self.fc_critic_value = nn.Linear(hidden_size, 1)\n\n    self.action_input_layer = nn.Linear(self.action_size, hidden_size)\n    self.fc_critic_advantage = nn.Linear(hidden_size, 1)\n\n  def forward(self, x0):\n    Q = None\n    state = x0\n    x1 = self.relu(self.fc1(x0))\n    h = self.fc2(x1)  # h is (hidden state, cell state)\n    x = h # TODO : Remove this line from code\n    policy = self.fc_actor(x)  # Prevent 1s and hence NaNs\n\n    var_layer = self.var_layer(x)\n    var = self.var(var_layer)\n    print (var, policy)\n    sleep(10)\n\n    # action = policy.data + torch.normal(torch.zeros(policy.size()), torch.ones(policy.size())*0.01)\n    m = MultivariateNormal(policy, torch.diagnol(var))\n    action = m.sample()\n    print (action)\n    sleep(100)\n    action_samples = [Variable(m.sample()) for _ in range(5)]\n    advantage_samples = torch.cat([self.Advantage(x, action_sample).unsqueeze(-1) for action_sample in action_samples], -1)\n    print (advantage_samples)\n    sleep(100)\n    A = self.Advantage(x, action)\n    Q = V + A - advantage_samples.mean(-1)\n    return policy, Q, V, action#, var.log()\n\nI am using the following code as a neural network for my reinforcement learning codes. I need to sample actions from a Gaussian distribution. My objective is to get the variance from the neural network and diagonalize it for sampling. Please let me know how should I proceed. It seems that torch.diag doesn't support batches of input.", "body": "```\r\nclass ContinousActorCritic(nn.Module):\r\n  def __init__(self, observation_space, action_space, hidden_size, sigma=0.3):\r\n    super(ContinousActorCritic, self).__init__()\r\n    self.sigma = torch.tensor([sigma])\r\n    self.state_size = observation_space.shape[0]\r\n    # Action Space for COntinous Space\r\n    self.action_size = action_space.shape[0]\r\n\r\n    self.relu = nn.ReLU(inplace=True)\r\n    \r\n    self.fc1 = nn.Linear(self.state_size, hidden_size)\r\n    self.fc2 = nn.Linear(hidden_size, hidden_size)\r\n\r\n    self.fc_actor = nn.Linear(hidden_size, self.action_size)\r\n    self.var_layer = nn.Linear(hidden_size, self.action_size)\r\n    self.var = nn.Softplus() # To get the softplus results\r\n    self.fc_critic_value = nn.Linear(hidden_size, 1)\r\n\r\n    self.action_input_layer = nn.Linear(self.action_size, hidden_size)\r\n    self.fc_critic_advantage = nn.Linear(hidden_size, 1)\r\n\r\n  def forward(self, x0):\r\n    Q = None\r\n    state = x0\r\n    x1 = self.relu(self.fc1(x0))\r\n    h = self.fc2(x1)  # h is (hidden state, cell state)\r\n    x = h # TODO : Remove this line from code\r\n    policy = self.fc_actor(x)  # Prevent 1s and hence NaNs\r\n\r\n    var_layer = self.var_layer(x)\r\n    var = self.var(var_layer)\r\n    print (var, policy)\r\n    sleep(10)\r\n\r\n    # action = policy.data + torch.normal(torch.zeros(policy.size()), torch.ones(policy.size())*0.01)\r\n    m = MultivariateNormal(policy, torch.diagnol(var))\r\n    action = m.sample()\r\n    print (action)\r\n    sleep(100)\r\n    action_samples = [Variable(m.sample()) for _ in range(5)]\r\n    advantage_samples = torch.cat([self.Advantage(x, action_sample).unsqueeze(-1) for action_sample in action_samples], -1)\r\n    print (advantage_samples)\r\n    sleep(100)\r\n    A = self.Advantage(x, action)\r\n    Q = V + A - advantage_samples.mean(-1)\r\n    return policy, Q, V, action#, var.log()\r\n```\r\n\r\nI am using the following code as a neural network for my reinforcement learning codes. I need to sample actions from a Gaussian distribution. My objective is to get the variance from the neural network and diagonalize it for sampling. Please let me know how should I proceed. It seems that torch.diag doesn't support batches of input."}