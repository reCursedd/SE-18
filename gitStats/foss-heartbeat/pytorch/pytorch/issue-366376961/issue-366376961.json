{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12285", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12285/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12285/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12285/events", "html_url": "https://github.com/pytorch/pytorch/issues/12285", "id": 366376961, "node_id": "MDU6SXNzdWUzNjYzNzY5NjE=", "number": 12285, "title": "Documentation not clear on torch.expand() alternatives when performing torch.autograd.gradcheck", "user": {"login": "nwschurink", "id": 12720130, "node_id": "MDQ6VXNlcjEyNzIwMTMw", "avatar_url": "https://avatars3.githubusercontent.com/u/12720130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nwschurink", "html_url": "https://github.com/nwschurink", "followers_url": "https://api.github.com/users/nwschurink/followers", "following_url": "https://api.github.com/users/nwschurink/following{/other_user}", "gists_url": "https://api.github.com/users/nwschurink/gists{/gist_id}", "starred_url": "https://api.github.com/users/nwschurink/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nwschurink/subscriptions", "organizations_url": "https://api.github.com/users/nwschurink/orgs", "repos_url": "https://api.github.com/users/nwschurink/repos", "events_url": "https://api.github.com/users/nwschurink/events{/privacy}", "received_events_url": "https://api.github.com/users/nwschurink/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-03T14:45:56Z", "updated_at": "2018-10-04T23:44:33Z", "closed_at": "2018-10-04T23:44:33Z", "author_association": "NONE", "body_html": "<p>I am writing a custom loss funtion in which I expanded an array to several duplicates using unsqueeze() and the torch.expand() function. e.g. Tensor(1,1,100,100,100)-&gt; Tensor(1,1,100,100,100,12).</p>\n<p>My torch.autograd.gradcheck is failing (without pointing out why exactly), however looking in the documentation I see that it might be related to this tensor expansion. However, the documentation does not give any additional suggestions about how to resolve around this issue..</p>\n<p>To obtain a similar tensor expansion without gradcheck to fail, should I then use torch.cat and call it several times instead?? Memory wise this doesn't sound like the smartest move as I am then actually copying the same data several times in memory right?.</p>\n<p>Would you suggest to use the cat funtion during the gradcheck and then change it to an expansion when used for training? Or is the expansion not at all possible within a loss function?</p>\n<p>Hope to hear your toughts.</p>", "body_text": "I am writing a custom loss funtion in which I expanded an array to several duplicates using unsqueeze() and the torch.expand() function. e.g. Tensor(1,1,100,100,100)-> Tensor(1,1,100,100,100,12).\nMy torch.autograd.gradcheck is failing (without pointing out why exactly), however looking in the documentation I see that it might be related to this tensor expansion. However, the documentation does not give any additional suggestions about how to resolve around this issue..\nTo obtain a similar tensor expansion without gradcheck to fail, should I then use torch.cat and call it several times instead?? Memory wise this doesn't sound like the smartest move as I am then actually copying the same data several times in memory right?.\nWould you suggest to use the cat funtion during the gradcheck and then change it to an expansion when used for training? Or is the expansion not at all possible within a loss function?\nHope to hear your toughts.", "body": "I am writing a custom loss funtion in which I expanded an array to several duplicates using unsqueeze() and the torch.expand() function. e.g. Tensor(1,1,100,100,100)-> Tensor(1,1,100,100,100,12).\r\n\r\nMy torch.autograd.gradcheck is failing (without pointing out why exactly), however looking in the documentation I see that it might be related to this tensor expansion. However, the documentation does not give any additional suggestions about how to resolve around this issue..\r\n\r\nTo obtain a similar tensor expansion without gradcheck to fail, should I then use torch.cat and call it several times instead?? Memory wise this doesn't sound like the smartest move as I am then actually copying the same data several times in memory right?. \r\n\r\nWould you suggest to use the cat funtion during the gradcheck and then change it to an expansion when used for training? Or is the expansion not at all possible within a loss function?\r\n\r\nHope to hear your toughts.\r\n\r\n\r\n"}