{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223922588", "pull_request_review_id": 163168353, "id": 223922588, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzkyMjU4OA==", "diff_hunk": "@@ -3094,59 +3094,54 @@ def _test_broadcast(self, cast):\n                 # map and map2 are not implementd on CUDA tensors\n                 continue\n \n-            # TODO: fix masked_scatter and masked_fill broadcasting\n-            if hasattr(large_expanded, fn) and fn not in ['masked_scatter', 'masked_fill']:\n+            def specialized(myfn, t1, t2, t3):\n+                if fn == \"lerp\":\n+                    return myfn(t1, t2, 0.5)\n+                elif fn == \"masked_select\":\n+                    return myfn(t1, t2 < 0)\n+                elif fn == \"masked_scatter\":\n+                    expanded = torch.broadcast_tensors(t1, t2)[0]\n+                    return myfn(t1, t2 < 0.5, torch.ones_like(expanded))\n+                elif fn == \"masked_fill\":\n+                    return myfn(t1, t2 < 0.5, 1.0)\n+                elif fn in fns_3_args:\n+                    return myfn(t1, 1.0, t2, t3)\n+                else:\n+                    return myfn(t1, t2)\n+\n+            if hasattr(large_expanded, fn):\n                 # run through tensor versions of functions\n                 # and verify fully expanded inputs give same results\n                 expanded = {large: large_expanded, small: small_expanded, small2: small2_expanded}\n \n-                def tensorfn(myfn, t1, t2):\n-                    if fn == \"lerp\":\n-                        return myfn(t1, 0.5)\n-                    elif fn == \"masked_select\":\n-                        return myfn(t1 < 0)", "path": "test/test_torch.py", "position": null, "original_position": 30, "commit_id": "e93aba5f1f4f8c8bb389fb69eba7508b306e5676", "original_commit_id": "a193822f829eaa8831628735a9e6e0df2a51d8f9", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "body": "`masked_fill` and `masked_scatter` is not specialized here, because these two operators are explicitly excluded by the `if hasattr(large_expanded, fn) and fn not in ['masked_scatter', 'masked_fill']:` above, but now since it is no longer excluded, we need to specialize these two operators here as in the `def torchfn` below. Then this function and `def torchfn` become duplicate, so I merge them and name it `specialized`", "created_at": "2018-10-10T02:51:37Z", "updated_at": "2018-11-23T15:52:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/12413#discussion_r223922588", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12413", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223922588"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12413#discussion_r223922588"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12413"}}, "body_html": "<p><code>masked_fill</code> and <code>masked_scatter</code> is not specialized here, because these two operators are explicitly excluded by the <code>if hasattr(large_expanded, fn) and fn not in ['masked_scatter', 'masked_fill']:</code> above, but now since it is no longer excluded, we need to specialize these two operators here as in the <code>def torchfn</code> below. Then this function and <code>def torchfn</code> become duplicate, so I merge them and name it <code>specialized</code></p>", "body_text": "masked_fill and masked_scatter is not specialized here, because these two operators are explicitly excluded by the if hasattr(large_expanded, fn) and fn not in ['masked_scatter', 'masked_fill']: above, but now since it is no longer excluded, we need to specialize these two operators here as in the def torchfn below. Then this function and def torchfn become duplicate, so I merge them and name it specialized"}