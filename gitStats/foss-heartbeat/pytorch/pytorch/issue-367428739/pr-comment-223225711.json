{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223225711", "pull_request_review_id": 162302321, "id": 223225711, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzIyNTcxMQ==", "diff_hunk": "@@ -3140,10 +3140,17 @@ def torchfn(t1, t2, t3):\n                 for first, second, third in [(large, small, small2), (small, large, small2),\n                                              (small2, small, large), (small2, large, small)]:\n                     if first is None:\n-                        break  # ignore last iter when small2 is None\n-                    r1 = torchfn(expanded[first], expanded[second], expanded[third])\n-                    r2 = torchfn(first, second, third)\n-                    self.assertEqual(r1, r2)\n+                        return  # ignore last iter when small2 is None\n+                    # in-place tensor is not broadcastable; test only guaranteed\n+                    # to work by broadcasting other argument(s)\n+                    if not hasattr(large_expanded, fn + \"_\"):\n+                        r1 = torchfn(expanded[first], expanded[second], expanded[third])\n+                        r2 = torchfn(first, second, third)\n+                        self.assertEqual(r1, r2)\n+                    else:\n+                        r1 = torchfn(expanded[first], expanded[second], expanded[third])\n+                        r2 = torchfn(expanded[first], second, third)\n+                        self.assertEqual(r1, r2)", "path": "test/test_torch.py", "position": null, "original_position": 18, "commit_id": "e93aba5f1f4f8c8bb389fb69eba7508b306e5676", "original_commit_id": "835e3ad0692f01db30b9a38c8a4265b0584ceb2e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can we just do it like this (to minimize duplication):\r\n```python\r\n# in-place tensor is not broadcastable; test only guaranteed\r\n# to work by broadcasting other argument(s)\r\nfirst_maybe_expand = expanded[first] if hasattr(large_expanded, fn + \"_\") else first\r\nr1 = torchfn(expanded[first], expanded[second], expanded[third])\r\nr2 = torchfn(first_maybe_expand, second, third)\r\nself.assertEqual(r1, r2)\r\n```\r\n\r\nAlso, it's kind of weird that those ops support this partial broadcasting. I think we should make them fully broadcasting if that's possible.", "created_at": "2018-10-07T19:51:17Z", "updated_at": "2018-11-23T15:52:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/12413#discussion_r223225711", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12413", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223225711"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12413#discussion_r223225711"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12413"}}, "body_html": "<p>Can we just do it like this (to minimize duplication):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> in-place tensor is not broadcastable; test only guaranteed</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to work by broadcasting other argument(s)</span>\nfirst_maybe_expand <span class=\"pl-k\">=</span> expanded[first] <span class=\"pl-k\">if</span> <span class=\"pl-c1\">hasattr</span>(large_expanded, fn <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">else</span> first\nr1 <span class=\"pl-k\">=</span> torchfn(expanded[first], expanded[second], expanded[third])\nr2 <span class=\"pl-k\">=</span> torchfn(first_maybe_expand, second, third)\n<span class=\"pl-c1\">self</span>.assertEqual(r1, r2)</pre></div>\n<p>Also, it's kind of weird that those ops support this partial broadcasting. I think we should make them fully broadcasting if that's possible.</p>", "body_text": "Can we just do it like this (to minimize duplication):\n# in-place tensor is not broadcastable; test only guaranteed\n# to work by broadcasting other argument(s)\nfirst_maybe_expand = expanded[first] if hasattr(large_expanded, fn + \"_\") else first\nr1 = torchfn(expanded[first], expanded[second], expanded[third])\nr2 = torchfn(first_maybe_expand, second, third)\nself.assertEqual(r1, r2)\nAlso, it's kind of weird that those ops support this partial broadcasting. I think we should make them fully broadcasting if that's possible."}