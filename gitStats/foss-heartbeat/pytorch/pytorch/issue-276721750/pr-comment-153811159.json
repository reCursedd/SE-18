{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153811159", "pull_request_review_id": 79874458, "id": 153811159, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzgxMTE1OQ==", "diff_hunk": "@@ -1,42 +1,208 @@\n+#include \"Python.h\"\n #include \"interpreter.h\"\n #include \"torch/csrc/jit/ir.h\"\n #include \"torch/csrc/autograd/profiler.h\"\n #include \"torch/csrc/jit/generated/aten_dispatch.h\"\n+#include \"Python.h\"\n+#include \"pybind11/pybind11.h\"\n+#include \"torch/csrc/utils/auto_gil.h\"\n+#include \"torch/csrc/autograd/variable.h\"\n+#include \"torch/csrc/autograd/python_variable.h\"\n+#include \"torch/csrc/autograd/python_engine.h\"\n+#include \"torch/csrc/autograd/functions/special.h\"\n #ifdef WITH_CUDA\n #include \"torch/csrc/jit/fusion_compiler.h\"\n #endif\n \n+namespace py = pybind11;\n+\n namespace torch { namespace jit {\n \n+struct DummyFunction : autograd::Function {\n+  DummyFunction() {\n+    num_inputs = 0;\n+    is_executable = true;\n+  }\n+  virtual autograd::variable_list apply(const autograd::variable_list& inputs) override {\n+    throw std::logic_error(\"DummyFunction::apply() called, but it should be blocked by a callback returning false\");\n+  }\n+};\n+\n+struct Handle : at::RefCounted {\n+  std::shared_ptr<DummyFunction> forward_inputs;\n+  autograd::function_list forward_outputs;\n+};\n+\n+at::Tensor unsafeTensorBorrow(at::RefCounted * rc) {\n+  return at::Tensor(static_cast<at::TensorImpl*>(rc), true);\n+}\n+\n+struct HandleBuilder {\n+  HandleBuilder(bool requires_handle) {\n+    if(requires_handle) {\n+      handle = new Handle();\n+      handle->forward_inputs = std::make_shared<DummyFunction>();\n+    }\n+  }\n+  autograd::Variable addInput(at::RefCounted* input, const VariableFlags & flags_) {\n+    if(handle && flags_.requires_grad) {\n+      autograd::VarFlags flags = {flags_.requires_grad, flags_.is_volatile};\n+      return autograd::make_variable(\n+        unsafeTensorBorrow(input),\n+        flags,\n+        handle->forward_inputs->num_inputs++,\n+        handle->forward_inputs);\n+    } else {\n+      autograd::VarFlags flags = {false, false};\n+      return autograd::make_variable(unsafeTensorBorrow(input), flags);\n+    }\n+  }\n+  at::RefCounted* addOutput(const autograd::Variable & output) {\n+    if(handle) {\n+      handle->forward_outputs.emplace_back(output.grad_fn(),output.output_nr());\n+    }\n+    at::Tensor tensor = output.data();\n+    return tensor.detach();\n+  }\n+  void writeTo(refcounted_list & outputs) {\n+    // note: no if(handle) guard\n+    // because an unused handle is still produced as an output\n+    outputs.push_back(handle);\n+  }\n+private:\n+  Handle* handle = nullptr;\n+};\n+\n+bool hasHandleOutput(Node * n) {\n+  if(n->outputs().size() == 0)\n+    return false;\n+  auto & last = n->outputs().back();\n+  return last->isHandle() && last->uses().size() > 0; // don't bother creating a handle if it is never used\n+}\n+\n+Operation createPythonCallback(PythonOp* op) {\n+  py::object func = py::handle(op->pyobj.get()).attr(\"apply\");\n+  bool has_handle = hasHandleOutput(op);\n+  return [=](const refcounted_list & inputs, refcounted_list & outputs) {\n+    AutoGIL gil;\n+    py::tuple py_inputs(op->cconv.size());\n+    size_t i = 0;\n+    size_t next_scalar = 0;\n+    size_t next_tensor = 0;\n+    HandleBuilder builder(has_handle);\n+    for(auto arg_type : op->cconv) {\n+      if(arg_type == 's') {\n+        py_inputs[i] = py::reinterpret_borrow<py::object>(op->scalar_args[next_scalar++].get());\n+      } else if(arg_type == 't') {\n+        py_inputs[i] = THPVariable_Wrap(\n+          builder.addInput(inputs.at(next_tensor), op->var_flags.at(next_tensor)));\n+        next_tensor++;\n+      }\n+      i++;\n+    }\n+    py::object py_outputs(func(*py_inputs));\n+\n+    auto addOutput = [&](py::handle entry) {\n+      if(!THPVariable_Check(entry.ptr())) {\n+        throw std::runtime_error(\"Function.apply returned a non-Variable output\");\n+      }\n+      THPVariable *var = (THPVariable*) entry.ptr();\n+      outputs.push_back(builder.addOutput(var->cdata));\n+    };\n+    if(!PyTuple_Check(py_outputs.ptr())) {\n+      addOutput(py_outputs);\n+    } else {\n+      for(py::handle entry : py::tuple(py_outputs)) {\n+        addOutput(entry);\n+      }\n+    }\n+    builder.writeTo(outputs);\n+  };\n+}\n+\n+Operation createCppCallback(CppOp* op) {\n+  std::shared_ptr<autograd::Function> func = op->fn;\n+  bool has_handle = hasHandleOutput(op);\n+  return [=](const refcounted_list & inputs, refcounted_list & outputs) {\n+    HandleBuilder builder(has_handle);\n+    autograd::variable_list v_inputs;\n+    for(size_t i = 0; i < inputs.size(); i++) {\n+      v_inputs.push_back(builder.addInput(inputs[i], op->var_flags[i]));\n+    }\n+    autograd::variable_list v_outputs = func->apply(v_inputs);\n+    for(auto & output : v_outputs) {\n+      outputs.push_back(builder.addOutput(output));\n+    }\n+    builder.writeTo(outputs);\n+  };\n+}\n+\n+Operation createEvalCallback(CppOp * op) {\n+  bool has_handle_output = hasHandleOutput(op);\n+  return [=](const refcounted_list & inputs,\n+             refcounted_list & outputs) {\n+    Handle * handle_in = dynamic_cast<Handle*>(inputs.back());\n+    JIT_ASSERT(handle_in);\n+    HandleBuilder builder(has_handle_output);\n+    auto& engine = autograd::python::PythonEngine::getDefaultEngine();\n+    autograd::variable_list v_inputs;\n+    for(size_t i = 0; i < inputs.size() - 1; i++) {\n+      v_inputs.push_back(builder.addInput(inputs[i], op->var_flags[i]));\n+    }\n+    autograd::Engine::pre_callback_map callbacks;\n+    callbacks.emplace(handle_in->forward_inputs.get(), [&](autograd::Function * _unused, autograd::variable_list & values) -> bool {\n+      for(auto & v : values) {\n+        outputs.push_back(builder.addOutput(v));\n+      }\n+      return false; // stop output and do not run DummyFunction\n+    });\n+    // node handle_in->use_count() == 1 means that we are guarenteed that we have the only\n+    // only copy of the backward pass, but it is not clear that it is safe even in that case", "path": "torch/csrc/jit/interpreter.cpp", "position": null, "original_position": 160, "commit_id": "9ed71c792b2ea33bbc30a18bd9b0bc7cc7ea5c84", "original_commit_id": "1c5a385c904352750860eb388e46801d8de5d803", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "So you have something like:\r\n\r\nstage 1: handle0 -> Function0 -> Function1 -> Function2 -> Grad Accumulator\r\nstage 2: handle1 -> Function3 -> Function 4 -> Function0 -> Grad Accumulator\r\n\r\nFunction 0 exists in both graphs because copied_next_fns told handle1 to inherit it from handle0 to calculate a derivative path that goes through the earlier stage.\r\n\r\nEven if handle1.use_count() == 1, you can't free its graph unless handle0->use_count() == 0.\r\nI think the 'right' solution here is for 'keep_graph=False' to actually mean \"release the variables incrementally as this is run if we are guaranteed that the Function will be freed after the autograd finishes\". But that condition is hard to check so is probably not worth trying to do.", "created_at": "2017-11-29T14:59:43Z", "updated_at": "2018-11-23T15:36:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/3866#discussion_r153811159", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3866", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153811159"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3866#discussion_r153811159"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3866"}}, "body_html": "<p>So you have something like:</p>\n<p>stage 1: handle0 -&gt; Function0 -&gt; Function1 -&gt; Function2 -&gt; Grad Accumulator<br>\nstage 2: handle1 -&gt; Function3 -&gt; Function 4 -&gt; Function0 -&gt; Grad Accumulator</p>\n<p>Function 0 exists in both graphs because copied_next_fns told handle1 to inherit it from handle0 to calculate a derivative path that goes through the earlier stage.</p>\n<p>Even if handle1.use_count() == 1, you can't free its graph unless handle0-&gt;use_count() == 0.<br>\nI think the 'right' solution here is for 'keep_graph=False' to actually mean \"release the variables incrementally as this is run if we are guaranteed that the Function will be freed after the autograd finishes\". But that condition is hard to check so is probably not worth trying to do.</p>", "body_text": "So you have something like:\nstage 1: handle0 -> Function0 -> Function1 -> Function2 -> Grad Accumulator\nstage 2: handle1 -> Function3 -> Function 4 -> Function0 -> Grad Accumulator\nFunction 0 exists in both graphs because copied_next_fns told handle1 to inherit it from handle0 to calculate a derivative path that goes through the earlier stage.\nEven if handle1.use_count() == 1, you can't free its graph unless handle0->use_count() == 0.\nI think the 'right' solution here is for 'keep_graph=False' to actually mean \"release the variables incrementally as this is run if we are guaranteed that the Function will be freed after the autograd finishes\". But that condition is hard to check so is probably not worth trying to do.", "in_reply_to_id": 153159390}