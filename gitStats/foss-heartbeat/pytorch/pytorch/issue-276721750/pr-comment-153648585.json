{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153648585", "pull_request_review_id": 79682748, "id": 153648585, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzY0ODU4NQ==", "diff_hunk": "@@ -20,19 +20,75 @@ using operator_constructor = std::function<TensorOp(jit::Node*)>;\n \n namespace {\n \n-void pack_list(std::vector<Tensor> & outputs, Tensor v) { outputs.push_back(v); }\n-void pack_list(std::vector<Tensor> & outputs, Scalar v) { outputs.push_back(v.toTensor()); }\n-void pack_list(std::vector<Tensor> & outputs, const std::vector<Tensor> & t) {\n-  outputs.insert(outputs.end(), t.begin(), t.end());\n+// a temporary Tensor that does not alter the refcount of impl on\n+// acquisition or release, avoids any refcounting in dispatch functions\n+struct TensorTemporary {\n+  explicit TensorTemporary(at::Retainable * impl)\n+  : temp(static_cast<at::TensorImpl*>(impl), false /* do not retain*/) {}\n+  const at::Tensor & value() {\n+    return temp;\n+  }\n+  ~TensorTemporary() {\n+    // don't reduce the refcount on deletion\n+    temp.detach();\n+  }\n+private:\n+  at::Tensor temp;\n+};\n+\n+// same list of Tensors that does not alter the refcount on acquisition or\n+// release of the refcount temporaries, only used rarely (e.g. for cat)\n+struct TensorTemporaryList {\n+  explicit TensorTemporaryList(const list_of_retainable & ts) {\n+    tensors.reserve(ts.size());\n+    for(auto & t : ts) {\n+      tensors.push_back(at::Tensor(static_cast<at::TensorImpl*>(t), false /*do not retain*/));\n+    }\n+  }\n+  // TensorTemporaryList only exposes a TensorList,\n+  // not its underlying std::vector<at::Tensor>.\n+  // This ArrayRef has the desired semantics: if you get out an at::Tensor from it,\n+  // the refcount is bumped;\n+  // if you take a reference, it is only guaranteed to stay live as long as the ArrayRef is live,\n+  operator TensorList() const {\n+    return tensors;\n+  }\n+  ~TensorTemporaryList() {\n+    // we didnt retain the tensors when we created the list\n+    // so make sure we don't release them when we free it\n+    for(auto & t : tensors) {\n+      t.detach();\n+    }\n+  }\n+private:\n+  std::vector<at::Tensor> tensors;\n+};\n+\n+using list_of_retainable = std::vector<at::Retainable*>;\n+\n+// pack list takes the return values of aten functions and puts them into a\n+// refcounted list. Each pack_list variant takes a Tensor by value, ensuring\n+// it has a owning reference and then that reference is stolen ad added to the\n+// list_of_retainable output list.\n+// pack_list never operates on tensor temporaries.\n+void pack_list(list_of_retainable & outputs, Tensor v) { outputs.push_back(toRetainableSteal(std::move(v))); }", "path": "tools/jit/templates/aten_dispatch.cpp", "position": 59, "original_position": 59, "commit_id": "9ed71c792b2ea33bbc30a18bd9b0bc7cc7ea5c84", "original_commit_id": "0354127c52b21be07d61d7441a8babcc1c35d529", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "It's still a bit ugly that `list_of_retainable` doesn't manage the lifetime of the pointers it holds - some code might look as if it should free memory when an error happens, while in reality you have these unsafe moments before you plug these pointers back to registers, when any exceptions will basically leak memory.\r\n\r\nCan't we just have an `std::vector<at::Retainable*>` for inputs, because they are temporaries that don't bump refcounts, and `std::vector<std::unique_ptr<at::Retainable>>` (with a custom deleter) to ensure we will never leak outputs?", "created_at": "2017-11-28T22:55:18Z", "updated_at": "2018-11-23T15:36:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/3866#discussion_r153648585", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3866", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153648585"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3866#discussion_r153648585"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3866"}}, "body_html": "<p>It's still a bit ugly that <code>list_of_retainable</code> doesn't manage the lifetime of the pointers it holds - some code might look as if it should free memory when an error happens, while in reality you have these unsafe moments before you plug these pointers back to registers, when any exceptions will basically leak memory.</p>\n<p>Can't we just have an <code>std::vector&lt;at::Retainable*&gt;</code> for inputs, because they are temporaries that don't bump refcounts, and <code>std::vector&lt;std::unique_ptr&lt;at::Retainable&gt;&gt;</code> (with a custom deleter) to ensure we will never leak outputs?</p>", "body_text": "It's still a bit ugly that list_of_retainable doesn't manage the lifetime of the pointers it holds - some code might look as if it should free memory when an error happens, while in reality you have these unsafe moments before you plug these pointers back to registers, when any exceptions will basically leak memory.\nCan't we just have an std::vector<at::Retainable*> for inputs, because they are temporaries that don't bump refcounts, and std::vector<std::unique_ptr<at::Retainable>> (with a custom deleter) to ensure we will never leak outputs?"}