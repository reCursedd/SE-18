{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315225923", "html_url": "https://github.com/pytorch/pytorch/issues/2024#issuecomment-315225923", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2024", "id": 315225923, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTIyNTkyMw==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-13T23:07:03Z", "updated_at": "2017-07-13T23:07:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7891333\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jvmancuso\">@jvmancuso</a> With analogy to VAEs, training involves injecting noise via \u00b5 + \u03c3\f\u03b5, but testing involves taking just the mean \u03c3 (which would be equivalent to setting \u03b5 to 0 rather than \u03b5 ~ N(0, 1)). Unless I'm missing something here, setting \u03b5 to 1 would result in adding a constant bias to the weight and bias matrices, which seems wrong, especially considering the expectation of the noise is 0. Don't want to be adjusting \u00b5 or \u03c3 parameters, just the noise \f\u03b5. Either way, it'd be good to send your final implementation to Meire for confirmation.</p>", "body_text": "@jvmancuso With analogy to VAEs, training involves injecting noise via \u00b5 + \u03c3\u03b5, but testing involves taking just the mean \u03c3 (which would be equivalent to setting \u03b5 to 0 rather than \u03b5 ~ N(0, 1)). Unless I'm missing something here, setting \u03b5 to 1 would result in adding a constant bias to the weight and bias matrices, which seems wrong, especially considering the expectation of the noise is 0. Don't want to be adjusting \u00b5 or \u03c3 parameters, just the noise \u03b5. Either way, it'd be good to send your final implementation to Meire for confirmation.", "body": "@jvmancuso With analogy to VAEs, training involves injecting noise via \u00b5 + \u03c3\f\u03b5, but testing involves taking just the mean \u03c3 (which would be equivalent to setting \u03b5 to 0 rather than \u03b5 ~ N(0, 1)). Unless I'm missing something here, setting \u03b5 to 1 would result in adding a constant bias to the weight and bias matrices, which seems wrong, especially considering the expectation of the noise is 0. Don't want to be adjusting \u00b5 or \u03c3 parameters, just the noise \f\u03b5. Either way, it'd be good to send your final implementation to Meire for confirmation."}