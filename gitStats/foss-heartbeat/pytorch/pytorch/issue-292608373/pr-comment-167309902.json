{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167309902", "pull_request_review_id": 95518813, "id": 167309902, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzMwOTkwMg==", "diff_hunk": "@@ -54,23 +67,39 @@ class BatchNorm1d(_BatchNorm):\n     the mini-batches and gamma and beta are learnable parameter vectors\n     of size C (where C is the input size).\n \n-    During training, this layer keeps a running estimate of its computed mean\n-    and variance. The running sum is kept with a default momentum of 0.1.\n+    By default, during training this layer keeps running estimates of its\n+    computed mean and variance, which are then used for normalization during\n+    evaluation. The running estimates are kept with a default :attr:`momentum`\n+    of 0.1.\n \n-    During evaluation, this running mean/variance is used for normalization.\n+    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n+    keep running estimates, and batch statistics are instead used during\n+    evaluation time as well.\n+\n+    .. note::\n+        This :attr:`momentum` argument is different from one used in optimizer\n+        classes and the conventional notion of momentum. Mathematically, the\n+        update rule for running statistics here is\n+        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x}_\\text{new} + \\text{momemtum} \\times x_t`,\n+        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n+        new observed value.\n \n     Because the BatchNorm is done over the `C` dimension, computing statistics\n     on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n \n     Args:\n         num_features: num_features from an expected input of size\n-            `batch_size x num_features [x width]`\n+            `[batch_size x num_features (x width)]`", "path": "torch/nn/modules/batchnorm.py", "position": null, "original_position": 97, "commit_id": "b1359c8212165027bb3a1fbf500a1328ef7e0f39", "original_commit_id": "e790c04ac7c97c33a3fc0378b6dc6a8eba1cefe8", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "body": "For consistency with convolutions this should be `length` for 1D signals (L for 1D, H x W for 2D, D x H x W for 3D).", "created_at": "2018-02-09T18:34:45Z", "updated_at": "2018-11-23T15:39:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/4922#discussion_r167309902", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4922", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167309902"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4922#discussion_r167309902"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4922"}}, "body_html": "<p>For consistency with convolutions this should be <code>length</code> for 1D signals (L for 1D, H x W for 2D, D x H x W for 3D).</p>", "body_text": "For consistency with convolutions this should be length for 1D signals (L for 1D, H x W for 2D, D x H x W for 3D)."}