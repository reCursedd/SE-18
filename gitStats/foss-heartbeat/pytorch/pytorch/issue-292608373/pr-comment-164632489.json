{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164632489", "pull_request_review_id": 92433306, "id": 164632489, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDYzMjQ4OQ==", "diff_hunk": "@@ -0,0 +1,116 @@\n+from .batchnorm import _BatchNorm\n+from .. import functional as F\n+\n+\n+class LayerNorm(_BatchNorm):\n+    r\"\"\"Applies Layer Normalization over a mini-batch of inputs as described in\n+    the paper `Layer Normalization`_ .\n+\n+    .. math::\n+        y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta\n+\n+    The mean and standard-deviation are calculated separately for all dimensions", "path": "torch/nn/modules/layernorm.py", "position": null, "original_position": 12, "commit_id": "b1359c8212165027bb3a1fbf500a1328ef7e0f39", "original_commit_id": "99fe7ed626e6d18e421d4ee3af7df8a42dc70553", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Your argument on RNN is valid. The problem with RNN is that it has two aggregating dimensions (batch and time). It's my bad for forgetting LN's popularity in NLP and focusing only on use cases similar to BN and IN. I believe an API like the tf.contrib one is reasonable in this case. \r\n\r\nHowever, what is the difference between IN and LN then? From the user's perspective, what LN can do is a superset of what IN can do.", "created_at": "2018-01-30T03:26:03Z", "updated_at": "2018-11-23T15:38:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/4922#discussion_r164632489", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4922", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164632489"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4922#discussion_r164632489"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4922"}}, "body_html": "<p>Your argument on RNN is valid. The problem with RNN is that it has two aggregating dimensions (batch and time). It's my bad for forgetting LN's popularity in NLP and focusing only on use cases similar to BN and IN. I believe an API like the tf.contrib one is reasonable in this case.</p>\n<p>However, what is the difference between IN and LN then? From the user's perspective, what LN can do is a superset of what IN can do.</p>", "body_text": "Your argument on RNN is valid. The problem with RNN is that it has two aggregating dimensions (batch and time). It's my bad for forgetting LN's popularity in NLP and focusing only on use cases similar to BN and IN. I believe an API like the tf.contrib one is reasonable in this case.\nHowever, what is the difference between IN and LN then? From the user's perspective, what LN can do is a superset of what IN can do.", "in_reply_to_id": 164627382}