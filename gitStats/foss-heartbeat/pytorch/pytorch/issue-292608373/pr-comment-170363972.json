{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170363972", "pull_request_review_id": 99047779, "id": 170363972, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDM2Mzk3Mg==", "diff_hunk": "@@ -69,6 +71,109 @@ def __repr__(self):\n             + ', k=' + str(self.k) + ')'\n \n \n+class LayerNorm(Module):\n+    r\"\"\"Applies Layer Normalization over a mini-batch of inputs as described in\n+    the paper `Layer Normalization`_ .\n+\n+    .. math::\n+        y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta\n+\n+    The mean and standard-deviation are calculated separately over the last\n+    certain number dimensions with shape specified by :attr:`normalized_shape`.\n+    Gamma and beta are learnable parameters of :attr:`normalized_shape` if\n+    :attr:`elementwise_affine` is ``True``.\n+\n+    .. note::\n+        Unlike Batch Normalization and Instance Normalization, which applies\n+        scalar scale and bias for each entire channel/plane with the\n+        :attr:`affine` option, Layer Normalization applies per-element scale and\n+        bias with :attr:`elementwise_affine`.\n+\n+    By default, this layer uses statistics computed from input data in both\n+    training and evaluation modes.\n+\n+    If :attr:`track_running_stats` is set to ``True``, during training this\n+    layer keeps running estimates of its computed mean and variance, which are\n+    then used for normalization during evaluation. The running estimates are\n+    kept with a default :attr:`momentum` of 0.1.\n+\n+    .. note::\n+        This :attr:`momentum` argument is different from one used in optimizer\n+        classes and the conventional notion of momentum. Mathematically, the\n+        update rule for running statistics here is\n+        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x}_\\text{new} + \\text{momemtum} \\times x_t`,\n+        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n+        new observed value.\n+\n+    Args:\n+        normalized_shape (list or torch.Size): input shape from an expected input of size\n+            `[* x normalized_shape[0] x normalized_shape[1] x ... x normalized_shape[-1]]`\n+        eps: a value added to the denominator for numerical stability. Default: 1e-5\n+        momentum: the value used for the running_mean and running_var computation. Default: 0.1\n+        elementwise_affine: a boolean value that when set to ``True``, this module\n+            has learnable per-element affine parameters. Default: ``True``\n+        track_running_stats: a boolean value that when set to ``True``, this\n+            module tracks the running mean and variance, and when set to ``False``,\n+            this module does not track such statistics and always uses batch\n+            statistics in both training and eval modes. Default: ``False``\n+\n+    Shape:\n+        - Input: :math:`(N, *)`\n+        - Output: :math:`(N, *)` (same shape as input)\n+\n+    Examples:\n+        >>> input = autograd.Variable(torch.randn(20, 5, 10, 10))\n+        >>> # With Learnable Parameters\n+        >>> m = nn.LayerNorm(input.size()[1:])\n+        >>> # Without Learnable Parameters\n+        >>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n+        >>> output = m(input)\n+\n+    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450\n+    \"\"\"\n+    def __init__(self, normalized_shape, eps=1e-5, momentum=0.1,\n+                 elementwise_affine=True, track_running_stats=False):\n+        super(LayerNorm, self).__init__()\n+        self.normalized_shape = torch.Size(normalized_shape)\n+        self.eps = eps\n+        self.momentum = momentum\n+        self.elementwise_affine = elementwise_affine\n+        self.track_running_stats = track_running_stats\n+        if self.elementwise_affine:\n+            self.weight = Parameter(torch.Tensor(*normalized_shape))\n+            self.bias = Parameter(torch.Tensor(*normalized_shape))\n+        else:\n+            self.register_parameter('weight', None)\n+            self.register_parameter('bias', None)\n+        if self.track_running_stats:\n+            self.register_buffer('running_mean', torch.zeros(1))\n+            self.register_buffer('running_var', torch.ones(1))\n+        else:\n+            self.register_parameter('running_mean', None)\n+            self.register_parameter('running_var', None)\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        if self.track_running_stats:\n+            self.running_mean.zero_()\n+            self.running_var.fill_(1)\n+        if self.elementwise_affine:\n+            self.weight.data.uniform_()", "path": "torch/nn/modules/normalization.py", "position": 116, "original_position": 116, "commit_id": "b1359c8212165027bb3a1fbf500a1328ef7e0f39", "original_commit_id": "b1359c8212165027bb3a1fbf500a1328ef7e0f39", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "I'm following how BN and IN initialize weights here.", "created_at": "2018-02-23T20:49:19Z", "updated_at": "2018-11-23T15:39:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/4922#discussion_r170363972", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4922", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170363972"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4922#discussion_r170363972"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4922"}}, "body_html": "<p>I'm following how BN and IN initialize weights here.</p>", "body_text": "I'm following how BN and IN initialize weights here.", "in_reply_to_id": 170166948}