{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367886082", "html_url": "https://github.com/pytorch/pytorch/pull/4922#issuecomment-367886082", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4922", "id": 367886082, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Nzg4NjA4Mg==", "user": {"login": "meder411", "id": 6818607, "node_id": "MDQ6VXNlcjY4MTg2MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6818607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meder411", "html_url": "https://github.com/meder411", "followers_url": "https://api.github.com/users/meder411/followers", "following_url": "https://api.github.com/users/meder411/following{/other_user}", "gists_url": "https://api.github.com/users/meder411/gists{/gist_id}", "starred_url": "https://api.github.com/users/meder411/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meder411/subscriptions", "organizations_url": "https://api.github.com/users/meder411/orgs", "repos_url": "https://api.github.com/users/meder411/repos", "events_url": "https://api.github.com/users/meder411/events{/privacy}", "received_events_url": "https://api.github.com/users/meder411/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-23T02:00:09Z", "updated_at": "2018-02-23T02:00:39Z", "author_association": "NONE", "body_html": "<p>Is there a particular reason that the <code>normalized_shape</code> input must be a tensor? For fully connected layers or other 1D inputs, it seems that the constructor ought to be able to accept an integer. As of now, I am using</p>\n<p><code>nn.LayerNorm(torch.LongTensor([256])</code></p>\n<p>but why not</p>\n<p><code>nn.LayerNorm(256)</code></p>\n<p>as one would with BatchNorm. From the paper, there doesn't seem to be any reason you wouldn't use Layer Normalization in this way. It's a minor change, but it may be a more seemless interface.</p>", "body_text": "Is there a particular reason that the normalized_shape input must be a tensor? For fully connected layers or other 1D inputs, it seems that the constructor ought to be able to accept an integer. As of now, I am using\nnn.LayerNorm(torch.LongTensor([256])\nbut why not\nnn.LayerNorm(256)\nas one would with BatchNorm. From the paper, there doesn't seem to be any reason you wouldn't use Layer Normalization in this way. It's a minor change, but it may be a more seemless interface.", "body": "Is there a particular reason that the `normalized_shape` input must be a tensor? For fully connected layers or other 1D inputs, it seems that the constructor ought to be able to accept an integer. As of now, I am using \r\n\r\n`nn.LayerNorm(torch.LongTensor([256])`\r\n\r\nbut why not\r\n\r\n`nn.LayerNorm(256)`\r\n\r\nas one would with BatchNorm. From the paper, there doesn't seem to be any reason you wouldn't use Layer Normalization in this way. It's a minor change, but it may be a more seemless interface."}