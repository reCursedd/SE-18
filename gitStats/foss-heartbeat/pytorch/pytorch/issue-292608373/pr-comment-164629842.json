{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164629842", "pull_request_review_id": 92430305, "id": 164629842, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDYyOTg0Mg==", "diff_hunk": "@@ -0,0 +1,116 @@\n+from .batchnorm import _BatchNorm\n+from .. import functional as F\n+\n+\n+class LayerNorm(_BatchNorm):\n+    r\"\"\"Applies Layer Normalization over a mini-batch of inputs as described in\n+    the paper `Layer Normalization`_ .\n+\n+    .. math::\n+        y = \\frac{x - mean[x]}{ \\sqrt{Var[x]} + \\epsilon} * gamma + beta\n+\n+    The mean and standard-deviation are calculated separately for all dimensions", "path": "torch/nn/modules/layernorm.py", "position": null, "original_position": 12, "commit_id": "b1359c8212165027bb3a1fbf500a1328ef7e0f39", "original_commit_id": "99fe7ed626e6d18e421d4ee3af7df8a42dc70553", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "body": "The TF implementation has a parameterizable number of dimensions to normalize over; it always normalizes over the last `n` dimensions but `n` is configurable. That seems like an OK API, since the channel dimension is virtually always last.\r\nThe original paper normalizes over \"all the hidden units in a layer,\" which is a little ambiguous, but clarifies that in recurrent networks the normalization is performed \"at the current timestep,\" which would translate to normalizing only over the channel dimension if the input tensor also had a time dimension; that's the strategy adopted in e.g. [the Transformer network](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_layers.py#L517).", "created_at": "2018-01-30T03:00:58Z", "updated_at": "2018-11-23T15:38:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/4922#discussion_r164629842", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4922", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164629842"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4922#discussion_r164629842"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4922"}}, "body_html": "<p>The TF implementation has a parameterizable number of dimensions to normalize over; it always normalizes over the last <code>n</code> dimensions but <code>n</code> is configurable. That seems like an OK API, since the channel dimension is virtually always last.<br>\nThe original paper normalizes over \"all the hidden units in a layer,\" which is a little ambiguous, but clarifies that in recurrent networks the normalization is performed \"at the current timestep,\" which would translate to normalizing only over the channel dimension if the input tensor also had a time dimension; that's the strategy adopted in e.g. <a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_layers.py#L517\">the Transformer network</a>.</p>", "body_text": "The TF implementation has a parameterizable number of dimensions to normalize over; it always normalizes over the last n dimensions but n is configurable. That seems like an OK API, since the channel dimension is virtually always last.\nThe original paper normalizes over \"all the hidden units in a layer,\" which is a little ambiguous, but clarifies that in recurrent networks the normalization is performed \"at the current timestep,\" which would translate to normalizing only over the channel dimension if the input tensor also had a time dimension; that's the strategy adopted in e.g. the Transformer network.", "in_reply_to_id": 164627382}