{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4812", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4812/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4812/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4812/events", "html_url": "https://github.com/pytorch/pytorch/pull/4812", "id": 290966427, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY0NjY5NzU5", "number": 4812, "title": "Fix output_nr not incremented correctly", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-01-23T19:36:55Z", "updated_at": "2018-11-23T15:38:51Z", "closed_at": "2018-02-02T17:39:34Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4812", "html_url": "https://github.com/pytorch/pytorch/pull/4812", "diff_url": "https://github.com/pytorch/pytorch/pull/4812.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4812.patch"}, "body_html": "<p><code>output_nr</code> is not incremented properly in <code>rebase_history</code> and <code>set_history</code> when some tensors are undefined. This causes the autograd engine incorrectly putting input tensors at wrong indices in <code>InputBuffer</code>. See the following extremely simple double backward example that reproduces the bug:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable, grad\n\nconv = nn.Conv2d(1, 10, 5)\ninput = Variable(torch.randn(1,1,32,32))\nloss1 = conv(input).sum()\ngrad_bias, = grad(loss1, conv.bias, create_graph=True)\nloss2 = grad_bias.sum()\nloss2.backward()\n</code></pre>\n<p>Because <code>input</code> doesn't require gradient, the <code>ggW</code> and <code>ggb</code> terms are set to incorrect indices, and it throws this weird error message:</p>\n<pre><code>RuntimeError: Expected 1-dimensional input for 1-dimensional weight [10], but got \ninput of size [1, 1, 32, 32] instead\n</code></pre>\n<p>Why is this not detected in our tests: our double backward tests usually sets all parameters to <code>requires_grad=True</code>. Therefore the case where a backward function call returns an undefined tensor is not tested.</p>\n<p>An issue has been submitted on testing with more diverse configurations: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"290967371\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4813\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4813/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4813\">#4813</a>.</p>\n<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> for helping me finding the cause.</p>\n<p>Relevant forum post: <a href=\"https://discuss.pytorch.org/t/autograd-grad-dimension-error/12083/8\" rel=\"nofollow\">https://discuss.pytorch.org/t/autograd-grad-dimension-error/12083/8</a></p>", "body_text": "output_nr is not incremented properly in rebase_history and set_history when some tensors are undefined. This causes the autograd engine incorrectly putting input tensors at wrong indices in InputBuffer. See the following extremely simple double backward example that reproduces the bug:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable, grad\n\nconv = nn.Conv2d(1, 10, 5)\ninput = Variable(torch.randn(1,1,32,32))\nloss1 = conv(input).sum()\ngrad_bias, = grad(loss1, conv.bias, create_graph=True)\nloss2 = grad_bias.sum()\nloss2.backward()\n\nBecause input doesn't require gradient, the ggW and ggb terms are set to incorrect indices, and it throws this weird error message:\nRuntimeError: Expected 1-dimensional input for 1-dimensional weight [10], but got \ninput of size [1, 1, 32, 32] instead\n\nWhy is this not detected in our tests: our double backward tests usually sets all parameters to requires_grad=True. Therefore the case where a backward function call returns an undefined tensor is not tested.\nAn issue has been submitted on testing with more diverse configurations: #4813.\nThanks @ezyang for helping me finding the cause.\nRelevant forum post: https://discuss.pytorch.org/t/autograd-grad-dimension-error/12083/8", "body": "`output_nr` is not incremented properly in `rebase_history` and `set_history` when some tensors are undefined. This causes the autograd engine incorrectly putting input tensors at wrong indices in `InputBuffer`. See the following extremely simple double backward example that reproduces the bug:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable, grad\r\n\r\nconv = nn.Conv2d(1, 10, 5)\r\ninput = Variable(torch.randn(1,1,32,32))\r\nloss1 = conv(input).sum()\r\ngrad_bias, = grad(loss1, conv.bias, create_graph=True)\r\nloss2 = grad_bias.sum()\r\nloss2.backward()\r\n```\r\nBecause `input` doesn't require gradient, the `ggW` and `ggb` terms are set to incorrect indices, and it throws this weird error message: \r\n```\r\nRuntimeError: Expected 1-dimensional input for 1-dimensional weight [10], but got \r\ninput of size [1, 1, 32, 32] instead\r\n```\r\n\r\nWhy is this not detected in our tests: our double backward tests usually sets all parameters to `requires_grad=True`. Therefore the case where a backward function call returns an undefined tensor is not tested.\r\n\r\nAn issue has been submitted on testing with more diverse configurations: https://github.com/pytorch/pytorch/issues/4813.\r\n\r\nThanks @ezyang for helping me finding the cause.\r\n\r\nRelevant forum post: https://discuss.pytorch.org/t/autograd-grad-dimension-error/12083/8"}