{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1327", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1327/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1327/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1327/events", "html_url": "https://github.com/pytorch/pytorch/issues/1327", "id": 223531969, "node_id": "MDU6SXNzdWUyMjM1MzE5Njk=", "number": 1327, "title": "Adding the CReLU Activation", "user": {"login": "keskarnitish", "id": 5945552, "node_id": "MDQ6VXNlcjU5NDU1NTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/5945552?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keskarnitish", "html_url": "https://github.com/keskarnitish", "followers_url": "https://api.github.com/users/keskarnitish/followers", "following_url": "https://api.github.com/users/keskarnitish/following{/other_user}", "gists_url": "https://api.github.com/users/keskarnitish/gists{/gist_id}", "starred_url": "https://api.github.com/users/keskarnitish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keskarnitish/subscriptions", "organizations_url": "https://api.github.com/users/keskarnitish/orgs", "repos_url": "https://api.github.com/users/keskarnitish/repos", "events_url": "https://api.github.com/users/keskarnitish/events{/privacy}", "received_events_url": "https://api.github.com/users/keskarnitish/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-04-22T01:39:58Z", "updated_at": "2018-10-12T09:09:37Z", "closed_at": "2017-04-25T14:52:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was trying to reproduce some of the experiments of <a href=\"https://arxiv.org/abs/1702.08591\" rel=\"nofollow\">the new paper on shattered gradients</a> and needed to use the <a href=\"https://arxiv.org/abs/1603.05201\" rel=\"nofollow\">CReLU activation</a> which was unavailable in PyTorch. Unfortunately, it doesn't seem to be a trivial extension of ReLU to code. (Should?) Could we add this functionality to PyTorch?</p>\n<p>Implementations of CReLU can be found in:<br>\n<a href=\"https://github.com/torch/nn/blob/master/CReLU.lua\">Lua Torch</a><br>\n<a href=\"http://docs.chainer.org/en/latest/_modules/chainer/functions/activation/crelu.html\" rel=\"nofollow\">Chainer</a><br>\n<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/crelu\" rel=\"nofollow\">Tensorflow</a></p>\n<p>If I am not mistaken, this function would be added to <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/activation.py\"><code>nn/_functions/thnn/activation</code></a>?</p>\n<h3>Update:</h3>\n<p>For CNNs and Linear layers, I think I have a temporary solution.</p>\n<pre><code>class CReLU(nn.Module):\n    def __init__(self):\n        super(CReLU, self).__init__()\n    def forward(self, x):\n        return torch.cat((F.relu(x), F.relu(-x)), 1)\n</code></pre>\n<p>Obviously, there are some issues with this (hard-coded <code>axis</code> argument, doesn't allow inplace operations etc.)</p>", "body_text": "I was trying to reproduce some of the experiments of the new paper on shattered gradients and needed to use the CReLU activation which was unavailable in PyTorch. Unfortunately, it doesn't seem to be a trivial extension of ReLU to code. (Should?) Could we add this functionality to PyTorch?\nImplementations of CReLU can be found in:\nLua Torch\nChainer\nTensorflow\nIf I am not mistaken, this function would be added to nn/_functions/thnn/activation?\nUpdate:\nFor CNNs and Linear layers, I think I have a temporary solution.\nclass CReLU(nn.Module):\n    def __init__(self):\n        super(CReLU, self).__init__()\n    def forward(self, x):\n        return torch.cat((F.relu(x), F.relu(-x)), 1)\n\nObviously, there are some issues with this (hard-coded axis argument, doesn't allow inplace operations etc.)", "body": "I was trying to reproduce some of the experiments of [the new paper on shattered gradients](https://arxiv.org/abs/1702.08591) and needed to use the [CReLU activation](https://arxiv.org/abs/1603.05201) which was unavailable in PyTorch. Unfortunately, it doesn't seem to be a trivial extension of ReLU to code. (Should?) Could we add this functionality to PyTorch?\r\n\r\nImplementations of CReLU can be found in:\r\n[Lua Torch](https://github.com/torch/nn/blob/master/CReLU.lua)\r\n[Chainer](http://docs.chainer.org/en/latest/_modules/chainer/functions/activation/crelu.html)\r\n[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/crelu)\r\n\r\nIf I am not mistaken, this function would be added to [``nn/_functions/thnn/activation``](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/activation.py)?\r\n\r\n### Update:\r\nFor CNNs and Linear layers, I think I have a temporary solution.\r\n\r\n```\r\nclass CReLU(nn.Module):\r\n    def __init__(self):\r\n        super(CReLU, self).__init__()\r\n    def forward(self, x):\r\n        return torch.cat((F.relu(x), F.relu(-x)), 1)\r\n```\r\n\r\nObviously, there are some issues with this (hard-coded `axis` argument, doesn't allow inplace operations etc.)"}