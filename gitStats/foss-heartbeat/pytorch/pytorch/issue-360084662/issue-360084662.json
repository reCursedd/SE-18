{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11670", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11670/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11670/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11670/events", "html_url": "https://github.com/pytorch/pytorch/issues/11670", "id": 360084662, "node_id": "MDU6SXNzdWUzNjAwODQ2NjI=", "number": 11670, "title": "ONNX export issue (To CoreML)", "user": {"login": "leonardoaraujosantos", "id": 898383, "node_id": "MDQ6VXNlcjg5ODM4Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/898383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leonardoaraujosantos", "html_url": "https://github.com/leonardoaraujosantos", "followers_url": "https://api.github.com/users/leonardoaraujosantos/followers", "following_url": "https://api.github.com/users/leonardoaraujosantos/following{/other_user}", "gists_url": "https://api.github.com/users/leonardoaraujosantos/gists{/gist_id}", "starred_url": "https://api.github.com/users/leonardoaraujosantos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leonardoaraujosantos/subscriptions", "organizations_url": "https://api.github.com/users/leonardoaraujosantos/orgs", "repos_url": "https://api.github.com/users/leonardoaraujosantos/repos", "events_url": "https://api.github.com/users/leonardoaraujosantos/events{/privacy}", "received_events_url": "https://api.github.com/users/leonardoaraujosantos/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-13T22:09:21Z", "updated_at": "2018-09-13T22:36:15Z", "closed_at": "2018-09-13T22:36:15Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Use of ONNX models exported from Pytorch 0.4.1, causes issues when converting to coreML</p>\n<p>Provide a short description.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.onnx <span class=\"pl-k\">as</span> torch_onnx\n<span class=\"pl-k\">from</span> onnx_coreml <span class=\"pl-k\">import</span> convert\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv(inputs)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> The view create the reshape/unsqueeze issue when exporting to coreml</span>\n        x <span class=\"pl-k\">=</span> x.view(x.size()[<span class=\"pl-c1\">0</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>x = x.view(1, 307328)</span>\n        <span class=\"pl-c1\">print</span>(x.size())\n        <span class=\"pl-k\">return</span> x\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Use this an input trace to serialize the model</span>\ninput_shape <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)\nmodel_onnx_path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>torch_model.onnx<span class=\"pl-pds\">\"</span></span>\nmodel <span class=\"pl-k\">=</span> Model()\nmodel.train(<span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Export the model to an ONNX file</span>\ndummy_input <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-k\">*</span>input_shape)\noutput <span class=\"pl-k\">=</span> torch_onnx.export(model, \n                          dummy_input, \n                          model_onnx_path, \n                          <span class=\"pl-v\">export_params</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                          <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Export to coreml</span>\ncoreml_model <span class=\"pl-k\">=</span> convert(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./torch_model.onnx<span class=\"pl-pds\">'</span></span>)\ncoreml_model.save(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./test.mlmodel<span class=\"pl-pds\">'</span></span>)</pre></div>\n<h2>Results</h2>\n<div class=\"highlight highlight-source-shell\"><pre>torch.Size([1, 307328])\ngraph(%0 <span class=\"pl-c1\">:</span> Float(1, 3, 100, 100)\n      %1 <span class=\"pl-c1\">:</span> Float(32, 3, 3, 3)\n      %2 <span class=\"pl-c1\">:</span> Float(32)) {\n  %3 <span class=\"pl-c1\">:</span> Float(1, 32, 98, 98) = onnx::Conv[dilations<span class=\"pl-k\">=</span>[1, 1], group<span class=\"pl-k\">=</span>1, kernel_shape<span class=\"pl-k\">=</span>[3, 3], pads<span class=\"pl-k\">=</span>[0, 0, 0, 0], strides<span class=\"pl-k\">=</span>[1, 1]](%0, %1, %2), scope: Model/Conv2d[conv]\n  %4 <span class=\"pl-c1\">:</span> Dynamic <span class=\"pl-k\">=</span> onnx::Shape(%3), scope: Model\n  %5 <span class=\"pl-c1\">:</span> Dynamic <span class=\"pl-k\">=</span> onnx::Slice[axes<span class=\"pl-k\">=</span>[0], ends<span class=\"pl-k\">=</span>[1], starts<span class=\"pl-k\">=</span>[0]](%4), scope: Model\n  %6 <span class=\"pl-c1\">:</span> <span class=\"pl-en\">Long</span>() = onnx::Squeeze[axes<span class=\"pl-k\">=</span>[0]](%5), scope: Model\n  %7 <span class=\"pl-c1\">:</span> <span class=\"pl-en\">Long</span>() = onnx::Constant[value<span class=\"pl-k\">=</span>{-1}](), scope: Model\n  %8 <span class=\"pl-c1\">:</span> Dynamic <span class=\"pl-k\">=</span> onnx::Unsqueeze[axes<span class=\"pl-k\">=</span>[0]](%6), scope: Model\n  %9 <span class=\"pl-c1\">:</span> Dynamic <span class=\"pl-k\">=</span> onnx::Unsqueeze[axes<span class=\"pl-k\">=</span>[0]](%7), scope: Model\n  %10 <span class=\"pl-c1\">:</span> Dynamic <span class=\"pl-k\">=</span> onnx::Concat[axis<span class=\"pl-k\">=</span>0](%8, %9), scope: Model\n  %11 <span class=\"pl-c1\">:</span> Float(1, 307328) <span class=\"pl-k\">=</span> onnx::Reshape(%3, %10), scope: Model\n  <span class=\"pl-k\">return</span> (%11)<span class=\"pl-k\">;</span>\n}\n\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython-input-39-c67c0dcc89da<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n     33 \n     34 <span class=\"pl-c\"><span class=\"pl-c\">#</span> Export to coreml</span>\n---<span class=\"pl-k\">&gt;</span> 35 coreml_model <span class=\"pl-k\">=</span> convert(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./torch_model.onnx<span class=\"pl-pds\">'</span></span>)\n     36 coreml_model.save(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./test.mlmodel<span class=\"pl-pds\">'</span></span>)\n\n/usr/local/lib/python3.5/dist-packages/onnx_coreml/converter.py <span class=\"pl-k\">in</span> convert(model, mode, image_input_names, preprocessing_args, image_output_names, deprocessing_args, class_labels, predicted_feature_name, add_custom_layers, custom_conversion_functions)\n    396     <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>\n<span class=\"pl-s\">    397     if not add_custom_layers:</span>\n<span class=\"pl-s\">--&gt; 398         _check_unsupported_ops(graph.nodes)</span>\n<span class=\"pl-s\">    399 </span>\n<span class=\"pl-s\">    400     err = ErrorHandling(add_custom_layers,</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">/usr/local/lib/python3.5/dist-packages/onnx_coreml/converter.py in _check_unsupported_ops(nodes)</span>\n<span class=\"pl-s\">    102     if len(unsupported_op_types) &gt; 0:</span>\n<span class=\"pl-s\">    103         raise NotImplementedError(\"Unsupported ONNX ops of type: %s\" % (</span>\n<span class=\"pl-s\">--&gt; 104             <span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.join(unsupported_op_types)))</span>\n<span class=\"pl-s\">    105 </span>\n<span class=\"pl-s\">    106 </span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">NotImplementedError: Unsupported ONNX ops of type: Shape,Squeeze,Unsqueeze</span></pre></div>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04 LTS<br>\nGCC version: (Ubuntu 5.3.1-14ubuntu2.1) 5.3.1 20160413<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.5<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration:<br>\nGPU 0: TitanX</p>\n<p>Nvidia driver version: 375.66<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nUse of ONNX models exported from Pytorch 0.4.1, causes issues when converting to coreML\nProvide a short description.\nCode example\nimport torch\nimport torch.nn as nn\nimport torch.onnx as torch_onnx\nfrom onnx_coreml import convert\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=1, padding=0, bias=True)\n\n    def forward(self, inputs):\n        x = self.conv(inputs)\n        # The view create the reshape/unsqueeze issue when exporting to coreml\n        x = x.view(x.size()[0], -1)\n        #x = x.view(1, 307328)\n        print(x.size())\n        return x\n\n# Use this an input trace to serialize the model\ninput_shape = (3, 100, 100)\nmodel_onnx_path = \"torch_model.onnx\"\nmodel = Model()\nmodel.train(False)\n\n# Export the model to an ONNX file\ndummy_input = torch.randn(1, *input_shape)\noutput = torch_onnx.export(model, \n                          dummy_input, \n                          model_onnx_path, \n                          export_params=True,\n                          verbose=True)\n\n# Export to coreml\ncoreml_model = convert('./torch_model.onnx')\ncoreml_model.save('./test.mlmodel')\nResults\ntorch.Size([1, 307328])\ngraph(%0 : Float(1, 3, 100, 100)\n      %1 : Float(32, 3, 3, 3)\n      %2 : Float(32)) {\n  %3 : Float(1, 32, 98, 98) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%0, %1, %2), scope: Model/Conv2d[conv]\n  %4 : Dynamic = onnx::Shape(%3), scope: Model\n  %5 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%4), scope: Model\n  %6 : Long() = onnx::Squeeze[axes=[0]](%5), scope: Model\n  %7 : Long() = onnx::Constant[value={-1}](), scope: Model\n  %8 : Dynamic = onnx::Unsqueeze[axes=[0]](%6), scope: Model\n  %9 : Dynamic = onnx::Unsqueeze[axes=[0]](%7), scope: Model\n  %10 : Dynamic = onnx::Concat[axis=0](%8, %9), scope: Model\n  %11 : Float(1, 307328) = onnx::Reshape(%3, %10), scope: Model\n  return (%11);\n}\n\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n<ipython-input-39-c67c0dcc89da> in <module>()\n     33 \n     34 # Export to coreml\n---> 35 coreml_model = convert('./torch_model.onnx')\n     36 coreml_model.save('./test.mlmodel')\n\n/usr/local/lib/python3.5/dist-packages/onnx_coreml/converter.py in convert(model, mode, image_input_names, preprocessing_args, image_output_names, deprocessing_args, class_labels, predicted_feature_name, add_custom_layers, custom_conversion_functions)\n    396     '''\n    397     if not add_custom_layers:\n--> 398         _check_unsupported_ops(graph.nodes)\n    399 \n    400     err = ErrorHandling(add_custom_layers,\n\n/usr/local/lib/python3.5/dist-packages/onnx_coreml/converter.py in _check_unsupported_ops(nodes)\n    102     if len(unsupported_op_types) > 0:\n    103         raise NotImplementedError(\"Unsupported ONNX ops of type: %s\" % (\n--> 104             ','.join(unsupported_op_types)))\n    105 \n    106 \n\nNotImplementedError: Unsupported ONNX ops of type: Shape,Squeeze,Unsqueeze\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04 LTS\nGCC version: (Ubuntu 5.3.1-14ubuntu2.1) 5.3.1 20160413\nCMake version: version 3.5.1\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration:\nGPU 0: TitanX\nNvidia driver version: 375.66\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "## Issue description\r\nUse of ONNX models exported from Pytorch 0.4.1, causes issues when converting to coreML\r\n\r\nProvide a short description.\r\n\r\n## Code example\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.onnx as torch_onnx\r\nfrom onnx_coreml import convert\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=1, padding=0, bias=True)\r\n\r\n    def forward(self, inputs):\r\n        x = self.conv(inputs)\r\n        # The view create the reshape/unsqueeze issue when exporting to coreml\r\n        x = x.view(x.size()[0], -1)\r\n        #x = x.view(1, 307328)\r\n        print(x.size())\r\n        return x\r\n\r\n# Use this an input trace to serialize the model\r\ninput_shape = (3, 100, 100)\r\nmodel_onnx_path = \"torch_model.onnx\"\r\nmodel = Model()\r\nmodel.train(False)\r\n\r\n# Export the model to an ONNX file\r\ndummy_input = torch.randn(1, *input_shape)\r\noutput = torch_onnx.export(model, \r\n                          dummy_input, \r\n                          model_onnx_path, \r\n                          export_params=True,\r\n                          verbose=True)\r\n\r\n# Export to coreml\r\ncoreml_model = convert('./torch_model.onnx')\r\ncoreml_model.save('./test.mlmodel')\r\n```\r\n\r\n## Results\r\n```bash\r\ntorch.Size([1, 307328])\r\ngraph(%0 : Float(1, 3, 100, 100)\r\n      %1 : Float(32, 3, 3, 3)\r\n      %2 : Float(32)) {\r\n  %3 : Float(1, 32, 98, 98) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%0, %1, %2), scope: Model/Conv2d[conv]\r\n  %4 : Dynamic = onnx::Shape(%3), scope: Model\r\n  %5 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%4), scope: Model\r\n  %6 : Long() = onnx::Squeeze[axes=[0]](%5), scope: Model\r\n  %7 : Long() = onnx::Constant[value={-1}](), scope: Model\r\n  %8 : Dynamic = onnx::Unsqueeze[axes=[0]](%6), scope: Model\r\n  %9 : Dynamic = onnx::Unsqueeze[axes=[0]](%7), scope: Model\r\n  %10 : Dynamic = onnx::Concat[axis=0](%8, %9), scope: Model\r\n  %11 : Float(1, 307328) = onnx::Reshape(%3, %10), scope: Model\r\n  return (%11);\r\n}\r\n\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-39-c67c0dcc89da> in <module>()\r\n     33 \r\n     34 # Export to coreml\r\n---> 35 coreml_model = convert('./torch_model.onnx')\r\n     36 coreml_model.save('./test.mlmodel')\r\n\r\n/usr/local/lib/python3.5/dist-packages/onnx_coreml/converter.py in convert(model, mode, image_input_names, preprocessing_args, image_output_names, deprocessing_args, class_labels, predicted_feature_name, add_custom_layers, custom_conversion_functions)\r\n    396     '''\r\n    397     if not add_custom_layers:\r\n--> 398         _check_unsupported_ops(graph.nodes)\r\n    399 \r\n    400     err = ErrorHandling(add_custom_layers,\r\n\r\n/usr/local/lib/python3.5/dist-packages/onnx_coreml/converter.py in _check_unsupported_ops(nodes)\r\n    102     if len(unsupported_op_types) > 0:\r\n    103         raise NotImplementedError(\"Unsupported ONNX ops of type: %s\" % (\r\n--> 104             ','.join(unsupported_op_types)))\r\n    105 \r\n    106 \r\n\r\nNotImplementedError: Unsupported ONNX ops of type: Shape,Squeeze,Unsqueeze\r\n```\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04 LTS\r\nGCC version: (Ubuntu 5.3.1-14ubuntu2.1) 5.3.1 20160413\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TitanX\r\n\r\nNvidia driver version: 375.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect"}