{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5247", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5247/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5247/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5247/events", "html_url": "https://github.com/pytorch/pytorch/issues/5247", "id": 297259631, "node_id": "MDU6SXNzdWUyOTcyNTk2MzE=", "number": 5247, "title": "\"Bus error\" on /dev/shm OOM; hard/impossible to fix", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-14T21:56:51Z", "updated_at": "2018-02-15T10:25:08Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> helped me diagnose this; posting an issue here to document.</p>\n<p>Running out of space on /dev/shm leads to a \"Bus error\" that dumps core inside of <code>THTensor_(rawCopy)</code>, with no stack trace, error message, etc. It's kind of similar to a regular OOM except that you can't even look at <code>top</code> to see that you're running out of memory, or <code>/var/log/messages</code> to see that you were killed by OOM, etc.</p>\n<p>I'm not sure there's any way to fix this. The behavior is very strange, because the OS doesn't seem to provide any error when we run out of space. The TH <code>_map_alloc</code> function calls <code>ftruncate</code> which returns successfully even if there's not enough space. And then <code>mmap</code>ing the file also succeeds, and gives a non-null pointer. It's only when I try to write to it (inside the subsequent copy) that I get a bus error.</p>\n<p>Here's an example to repro:</p>\n<pre><code>import torch\n\ntensors = []\nfor i in range(1000):\n    print(i)\n    a = torch.Tensor(1000000000)\n    a.share_memory_()\n    tensors.append(a)\n</code></pre>", "body_text": "@colesbury helped me diagnose this; posting an issue here to document.\nRunning out of space on /dev/shm leads to a \"Bus error\" that dumps core inside of THTensor_(rawCopy), with no stack trace, error message, etc. It's kind of similar to a regular OOM except that you can't even look at top to see that you're running out of memory, or /var/log/messages to see that you were killed by OOM, etc.\nI'm not sure there's any way to fix this. The behavior is very strange, because the OS doesn't seem to provide any error when we run out of space. The TH _map_alloc function calls ftruncate which returns successfully even if there's not enough space. And then mmaping the file also succeeds, and gives a non-null pointer. It's only when I try to write to it (inside the subsequent copy) that I get a bus error.\nHere's an example to repro:\nimport torch\n\ntensors = []\nfor i in range(1000):\n    print(i)\n    a = torch.Tensor(1000000000)\n    a.share_memory_()\n    tensors.append(a)", "body": "@colesbury helped me diagnose this; posting an issue here to document.\r\n\r\nRunning out of space on /dev/shm leads to a \"Bus error\" that dumps core inside of `THTensor_(rawCopy)`, with no stack trace, error message, etc. It's kind of similar to a regular OOM except that you can't even look at `top` to see that you're running out of memory, or `/var/log/messages` to see that you were killed by OOM, etc.\r\n\r\nI'm not sure there's any way to fix this. The behavior is very strange, because the OS doesn't seem to provide any error when we run out of space. The TH `_map_alloc` function calls `ftruncate` which returns successfully even if there's not enough space. And then `mmap`ing the file also succeeds, and gives a non-null pointer. It's only when I try to write to it (inside the subsequent copy) that I get a bus error.\r\n\r\nHere's an example to repro:\r\n\r\n```\r\nimport torch\r\n\r\ntensors = []\r\nfor i in range(1000):\r\n    print(i)\r\n    a = torch.Tensor(1000000000)\r\n    a.share_memory_()\r\n    tensors.append(a)\r\n```"}