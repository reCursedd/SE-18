{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385205503", "html_url": "https://github.com/pytorch/pytorch/pull/6582#issuecomment-385205503", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6582", "id": 385205503, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTIwNTUwMw==", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-28T21:08:11Z", "updated_at": "2018-04-28T21:08:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yeah, sorry, let me rephrase this to \"<code>at::Tensor</code> becomes the defacto Variable type <strong>in the public C++ API</strong>\". As you know, PyTorch 0.4 exposes only tensors <em>with the ability to record gradients</em> (where a tensor with <code>requires_grad=False</code> is like a non-gradient recording Tensor, effectively). We want the same for C++, i.e. there should only be one <em>public</em> tensor type, which will be called <code>torch::Tensor</code>, but is really <code>autograd::Variable</code>. Now, we need the type we expose to users to be <code>at::Tensor</code> (which we call <code>torch::Tensor</code>), since the whole ATen library uses <code>at::Tensor</code>. But when users create tensors, they really create <code>autograd::Variable</code>s, which are upcast to <code>at::Tensor</code>. The way we brought back gradient-related functionality to users in the C++ extensions API is to then have free functions, e.g. <code>void set_requires_grad(at::Tensor)</code>, which inside would downcast the tensor to a Variable and call <code>.set_requires_grad()</code> on the downcast tensor. This is because we didn't want users to know of the distinction between variables and tensors -- they should only see tensors (like in Python). What this PR now does is instead put the gradient-related functions as virtual methods on the <code>at::Tensor</code> type directly, so that we can call <code>set_requires_grad</code> on <code>at::Tensor</code>, which will work when the <code>at::Tensor</code> is actually a variable, which it always should be for public users, and throw an exception for the base tensor, since it remains non-gradient-recording. Does this make sense?</p>", "body_text": "Yeah, sorry, let me rephrase this to \"at::Tensor becomes the defacto Variable type in the public C++ API\". As you know, PyTorch 0.4 exposes only tensors with the ability to record gradients (where a tensor with requires_grad=False is like a non-gradient recording Tensor, effectively). We want the same for C++, i.e. there should only be one public tensor type, which will be called torch::Tensor, but is really autograd::Variable. Now, we need the type we expose to users to be at::Tensor (which we call torch::Tensor), since the whole ATen library uses at::Tensor. But when users create tensors, they really create autograd::Variables, which are upcast to at::Tensor. The way we brought back gradient-related functionality to users in the C++ extensions API is to then have free functions, e.g. void set_requires_grad(at::Tensor), which inside would downcast the tensor to a Variable and call .set_requires_grad() on the downcast tensor. This is because we didn't want users to know of the distinction between variables and tensors -- they should only see tensors (like in Python). What this PR now does is instead put the gradient-related functions as virtual methods on the at::Tensor type directly, so that we can call set_requires_grad on at::Tensor, which will work when the at::Tensor is actually a variable, which it always should be for public users, and throw an exception for the base tensor, since it remains non-gradient-recording. Does this make sense?", "body": "Yeah, sorry, let me rephrase this to \"`at::Tensor` becomes the defacto Variable type __in the public C++ API__\". As you know, PyTorch 0.4 exposes only tensors *with the ability to record gradients* (where a tensor with `requires_grad=False` is like a non-gradient recording Tensor, effectively). We want the same for C++, i.e. there should only be one *public* tensor type, which will be called `torch::Tensor`, but is really `autograd::Variable`. Now, we need the type we expose to users to be `at::Tensor` (which we call `torch::Tensor`), since the whole ATen library uses `at::Tensor`. But when users create tensors, they really create `autograd::Variable`s, which are upcast to `at::Tensor`. The way we brought back gradient-related functionality to users in the C++ extensions API is to then have free functions, e.g. `void set_requires_grad(at::Tensor)`, which inside would downcast the tensor to a Variable and call `.set_requires_grad()` on the downcast tensor. This is because we didn't want users to know of the distinction between variables and tensors -- they should only see tensors (like in Python). What this PR now does is instead put the gradient-related functions as virtual methods on the `at::Tensor` type directly, so that we can call `set_requires_grad` on `at::Tensor`, which will work when the `at::Tensor` is actually a variable, which it always should be for public users, and throw an exception for the base tensor, since it remains non-gradient-recording. Does this make sense?\r\n\r\n"}