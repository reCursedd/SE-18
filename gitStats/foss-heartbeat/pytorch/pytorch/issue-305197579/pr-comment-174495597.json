{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174495597", "pull_request_review_id": 103862780, "id": 174495597, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDQ5NTU5Nw==", "diff_hunk": "@@ -0,0 +1,118 @@\n+#include \"ATen/native/cpu/ReduceOpsKernel.h\"\n+#include \"ATen/Dispatch.h\"\n+#include <iostream>\n+\n+namespace at {\n+namespace native {\n+\n+using namespace vec256;\n+\n+// This adds the content of arr to sum\n+template <class scalar_t, template <class> class PRED, CPUCapability C>\n+inline scalar_t allreduce_kernel_(const scalar_t *arr, size_t start, size_t end,\n+                                  scalar_t sum) {\n+  Vec256<scalar_t> part_sum;\n+  // Use all 16 registers.\n+  Vec256<scalar_t> tmp_sum[4], tmp_sum1, tmp_sum2, tmp_sum3;\n+  Vec256<scalar_t> a[8];\n+  size_t width =\n+      256 / sizeof(scalar_t); // primitives per 256 bytes (two cache lines)\n+  size_t epr = 32 / sizeof(scalar_t); // primitives per Vec256\n+  size_t k = 0;\n+  for (; k < (end - start) / width; k++) {\n+    for (size_t i = 0; i < 8; i++) {\n+      a[i].load(arr + (k * width) + i * epr + start);\n+    }\n+    for (size_t i = 0; i < 8; i += 2) {\n+      tmp_sum[i / 2] = PRED<Vec256<scalar_t>>()(a[i], a[i + 1]);\n+    }\n+    tmp_sum1 = PRED<Vec256<scalar_t>>()(tmp_sum[0], tmp_sum[1]);\n+    tmp_sum2 = PRED<Vec256<scalar_t>>()(tmp_sum[2], tmp_sum[3]);\n+    if (k == 0) {\n+      part_sum = PRED<Vec256<scalar_t>>()(tmp_sum1, tmp_sum2);\n+    } else {\n+      tmp_sum3 = PRED<Vec256<scalar_t>>()(tmp_sum1, tmp_sum2);\n+      part_sum = PRED<Vec256<scalar_t>>()(part_sum, tmp_sum3);\n+    }\n+  }\n+  if (k > 0) {\n+    scalar_t sarr[32 / sizeof(scalar_t)];\n+    part_sum.store(sarr);\n+    for (size_t i = 0; i < part_sum.size(); i++) {\n+      sum = PRED<scalar_t>()(sum, sarr[i]);\n+    }\n+  }\n+  k = k * width + start;\n+  for (; k < end; k++) {\n+    sum = PRED<scalar_t>()(sum, arr[k]);\n+  }\n+  return sum;\n+}\n+\n+// This overwrites the content of outarr\n+template <class scalar_t, template <class> class PRED, CPUCapability C>\n+inline void dimreduce_kernel_(const scalar_t *arr, scalar_t *outarr,\n+                              size_t num_rows, size_t num_cols) {\n+  size_t width =\n+      256 / (sizeof(scalar_t)); // primitives per 256 bytes (two cache lines)\n+  Vec256<scalar_t> a[8];\n+  Vec256<scalar_t> b[8];\n+  size_t epr = a[0].size(); // primitives per Vec256\n+  size_t tile = 0;\n+  for (; tile < (num_cols) / width; tile++) {\n+    size_t row_ind = tile * width;\n+    for (size_t i = 0; i < num_rows; i += 1) {\n+      for (int ib = 0; ib < 8; ib++) {\n+        if (i == 0) {\n+          b[ib].load(arr + i * num_cols + tile * width + ib * epr);\n+        } else {\n+          a[ib].load(arr + i * num_cols + tile * width + ib * epr);\n+          b[ib] = PRED<Vec256<scalar_t>>()(b[ib], a[ib]);\n+        }\n+      }\n+    }\n+    for (int ib = 0; ib < 8; ib++) {\n+      b[ib].store(outarr + row_ind + ib * epr);\n+    }\n+  }\n+  size_t k = tile * width;\n+  for (; k < num_cols; k++) {\n+    for (size_t i = 0; i < num_rows; i += 1) {\n+      if (i == 0) {\n+        outarr[k] = arr[i * num_cols + k];\n+      } else {\n+        outarr[k] = PRED<scalar_t>()(outarr[k], arr[i * num_cols + k]);\n+      }\n+    }\n+  }\n+}\n+\n+template <template <class> class PRED, CPUCapability C>\n+inline void allImpl(Tensor & result, const Tensor & self, size_t dim, bool all, std::string name, int64_t init) {", "path": "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "position": null, "original_position": 91, "commit_id": "47baed6dbba87b2e9eca0f00cba4a10a02a0e024", "original_commit_id": "7a22ea7d52dd0b1f52071c10449058651d7516d5", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Use `const char* name`. It will fix a build error and without it there will be an unnecessary std::string allocation every call (at least in older GCC versions)", "created_at": "2018-03-14T15:15:21Z", "updated_at": "2018-11-23T15:40:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/5776#discussion_r174495597", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5776", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174495597"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5776#discussion_r174495597"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5776"}}, "body_html": "<p>Use <code>const char* name</code>. It will fix a build error and without it there will be an unnecessary std::string allocation every call (at least in older GCC versions)</p>", "body_text": "Use const char* name. It will fix a build error and without it there will be an unnecessary std::string allocation every call (at least in older GCC versions)"}