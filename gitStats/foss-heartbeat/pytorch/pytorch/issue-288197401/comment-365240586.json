{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/365240586", "html_url": "https://github.com/pytorch/pytorch/pull/4640#issuecomment-365240586", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4640", "id": 365240586, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTI0MDU4Ng==", "user": {"login": "AlexanderRadionov", "id": 17885281, "node_id": "MDQ6VXNlcjE3ODg1Mjgx", "avatar_url": "https://avatars0.githubusercontent.com/u/17885281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlexanderRadionov", "html_url": "https://github.com/AlexanderRadionov", "followers_url": "https://api.github.com/users/AlexanderRadionov/followers", "following_url": "https://api.github.com/users/AlexanderRadionov/following{/other_user}", "gists_url": "https://api.github.com/users/AlexanderRadionov/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlexanderRadionov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlexanderRadionov/subscriptions", "organizations_url": "https://api.github.com/users/AlexanderRadionov/orgs", "repos_url": "https://api.github.com/users/AlexanderRadionov/repos", "events_url": "https://api.github.com/users/AlexanderRadionov/events{/privacy}", "received_events_url": "https://api.github.com/users/AlexanderRadionov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-13T11:33:00Z", "updated_at": "2018-02-13T11:33:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I made some syntetic tests, just iterate by dataloader. And i have not see any issue.<br>\nHere are code:</p>\n<pre><code>import time\nfrom datetime import datetime\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nGLOBAL_SEED = 1024\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nGLOBAL_WORKER_ID = None\ndef worker_init_fn(worker_id):\n    global GLOBAL_WORKER_ID\n    GLOBAL_WORKER_ID = worker_id\n    set_seed(GLOBAL_SEED + worker_id)\n\n\nclass RandomAugmentationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, item):\n        global GLOBAL_WORKER_ID\n        return item, GLOBAL_WORKER_ID, self.data[item] + np.random.random_sample(self.data[item].shape)\n\n    def __len__(self):\n        return self.data.shape[0]\n\nset_seed(GLOBAL_SEED)\ndata = np.random.random_sample((12800, 12800))\ndataset = RandomAugmentationDataset(data)\n\ns = time.time()\nfor i in range(0, 11):\n    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn, ind_worker_queue=True)\n    for i, (items, worker_ids, batch) in enumerate(dataloader):\n        pass\ne = time.time()\nprint(\"time\", e-s)\n</code></pre>\n<p>I run it with <code>ind_worker_queue=True</code> and <code>ind_worker_queue=False</code> several times.<br>\n<code>False</code> (default):<br>\ntime 17.274786710739136<br>\ntime 16.919730186462402<br>\n<code>True</code> (overrided):<br>\ntime 16.521615266799927<br>\ntime 16.571028470993042</p>\n<p>I think my changes should not cause slowdown because individual worker behavior is not default.</p>", "body_text": "I made some syntetic tests, just iterate by dataloader. And i have not see any issue.\nHere are code:\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nGLOBAL_SEED = 1024\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nGLOBAL_WORKER_ID = None\ndef worker_init_fn(worker_id):\n    global GLOBAL_WORKER_ID\n    GLOBAL_WORKER_ID = worker_id\n    set_seed(GLOBAL_SEED + worker_id)\n\n\nclass RandomAugmentationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, item):\n        global GLOBAL_WORKER_ID\n        return item, GLOBAL_WORKER_ID, self.data[item] + np.random.random_sample(self.data[item].shape)\n\n    def __len__(self):\n        return self.data.shape[0]\n\nset_seed(GLOBAL_SEED)\ndata = np.random.random_sample((12800, 12800))\ndataset = RandomAugmentationDataset(data)\n\ns = time.time()\nfor i in range(0, 11):\n    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn, ind_worker_queue=True)\n    for i, (items, worker_ids, batch) in enumerate(dataloader):\n        pass\ne = time.time()\nprint(\"time\", e-s)\n\nI run it with ind_worker_queue=True and ind_worker_queue=False several times.\nFalse (default):\ntime 17.274786710739136\ntime 16.919730186462402\nTrue (overrided):\ntime 16.521615266799927\ntime 16.571028470993042\nI think my changes should not cause slowdown because individual worker behavior is not default.", "body": "I made some syntetic tests, just iterate by dataloader. And i have not see any issue.\r\nHere are code:\r\n```\r\nimport time\r\nfrom datetime import datetime\r\nimport numpy as np\r\nimport random\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import Dataset\r\n\r\nGLOBAL_SEED = 1024\r\n\r\ndef set_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n\r\nGLOBAL_WORKER_ID = None\r\ndef worker_init_fn(worker_id):\r\n    global GLOBAL_WORKER_ID\r\n    GLOBAL_WORKER_ID = worker_id\r\n    set_seed(GLOBAL_SEED + worker_id)\r\n\r\n\r\nclass RandomAugmentationDataset(Dataset):\r\n    def __init__(self, data):\r\n        self.data = data\r\n\r\n    def __getitem__(self, item):\r\n        global GLOBAL_WORKER_ID\r\n        return item, GLOBAL_WORKER_ID, self.data[item] + np.random.random_sample(self.data[item].shape)\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]\r\n\r\nset_seed(GLOBAL_SEED)\r\ndata = np.random.random_sample((12800, 12800))\r\ndataset = RandomAugmentationDataset(data)\r\n\r\ns = time.time()\r\nfor i in range(0, 11):\r\n    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn, ind_worker_queue=True)\r\n    for i, (items, worker_ids, batch) in enumerate(dataloader):\r\n        pass\r\ne = time.time()\r\nprint(\"time\", e-s)\r\n```\r\nI run it with `ind_worker_queue=True` and `ind_worker_queue=False` several times.\r\n`False` (default):\r\ntime 17.274786710739136\r\ntime 16.919730186462402\r\n`True` (overrided):\r\ntime 16.521615266799927\r\ntime 16.571028470993042\r\n\r\nI think my changes should not cause slowdown because individual worker behavior is not default."}