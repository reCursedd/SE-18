{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4640", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4640/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4640/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4640/events", "html_url": "https://github.com/pytorch/pytorch/pull/4640", "id": 288197401, "node_id": "MDExOlB1bGxSZXF1ZXN0MTYyNjg4OTEy", "number": 4640, "title": "Fixed non-determinate preprocessing on DataLoader", "user": {"login": "AlexanderRadionov", "id": 17885281, "node_id": "MDQ6VXNlcjE3ODg1Mjgx", "avatar_url": "https://avatars0.githubusercontent.com/u/17885281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlexanderRadionov", "html_url": "https://github.com/AlexanderRadionov", "followers_url": "https://api.github.com/users/AlexanderRadionov/followers", "following_url": "https://api.github.com/users/AlexanderRadionov/following{/other_user}", "gists_url": "https://api.github.com/users/AlexanderRadionov/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlexanderRadionov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlexanderRadionov/subscriptions", "organizations_url": "https://api.github.com/users/AlexanderRadionov/orgs", "repos_url": "https://api.github.com/users/AlexanderRadionov/repos", "events_url": "https://api.github.com/users/AlexanderRadionov/events{/privacy}", "received_events_url": "https://api.github.com/users/AlexanderRadionov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 32, "created_at": "2018-01-12T17:49:27Z", "updated_at": "2018-11-23T15:39:31Z", "closed_at": "2018-03-23T21:44:00Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4640", "html_url": "https://github.com/pytorch/pytorch/pull/4640", "diff_url": "https://github.com/pytorch/pytorch/pull/4640.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4640.patch"}, "body_html": "<p>Added ind_worker_queue parameter to data.DataLoader. It makes preprocessing determinate.</p>\n<p>DataLoader in multiprocessing mode may cause non-deterministic issue. Even if <code>radom_seed</code> has frozen, each subprocess may get tasks in unstable order. This is caused by different I/O time while data loads. If you use augmentation while data loading, it makes results unreproduceble. Look at the <a href=\"https://discuss.pytorch.org/t/deterministic-non-deterministic-results-with-pytorch/9087\" rel=\"nofollow\">https://discuss.pytorch.org/t/deterministic-non-deterministic-results-with-pytorch/9087</a></p>\n<p>To fix this issue I have added the individual queue for each worker. In this case each worker get tasks in the stable order. In summary, subprocess produces the stable results.</p>\n<p>To reproduce issue you may change ind_worker_queue to <code>False</code> and run the script several times.<br>\nCode to reproduce issue.</p>\n<pre><code>import time\nimport numpy as np\nimport random\nfrom datetime import datetime\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nGLOBAL_SEED = 1024\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nGLOBAL_WORKER_ID = None\ndef worker_init_fn(worker_id):\n    global GLOBAL_WORKER_ID\n    GLOBAL_WORKER_ID = worker_id\n    set_seed(GLOBAL_SEED + worker_id)\n\n\nclass RandomAugmentationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, item):\n        global GLOBAL_WORKER_ID\n        rnd = datetime.now().microsecond % 5 # it should be unstable and not depend on random_seed\n        if rnd &lt; 3:\n            time.sleep(rnd * 0.1)\n        return item, GLOBAL_WORKER_ID, self.data[item] + np.random.random_sample(self.data[item].shape)\n\n    def __len__(self):\n        return self.data.shape[0]\n\nset_seed(GLOBAL_SEED)\ndata = np.random.random_sample((128, 128))\ndataset = RandomAugmentationDataset(data)\n\nexpected = [\n    (2070.646908929146, [97, 0, 79, 2, 85, 121, 117, 56, 83, 43, 70, 60, 108, 47, 101, 95], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n    (2063.337454820789, [71, 91, 20, 34, 107, 27, 52, 55, 69, 90, 127, 84, 81, 105, 46, 124], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    (2031.0978096882036, [118, 86, 44, 14, 18, 92, 98, 36, 49, 13, 111, 28, 116, 10, 104, 87], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n    (2045.9375171891238, [103, 15, 19, 80, 29, 120, 1, 106, 123, 24, 39, 31, 3, 65, 45, 50], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n    (2083.1401737400893, [11, 89, 12, 58, 54, 32, 67, 17, 110, 40, 82, 9, 30, 94, 125, 35], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    (2035.8920455019213, [41, 74, 33, 68, 64, 8, 76, 42, 126, 53, 122, 26, 114, 75, 57, 112], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n    (2078.9789912514643, [109, 119, 113, 37, 5, 22, 59, 51, 62, 25, 38, 66, 6, 102, 73, 96], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n    (2054.459126893385, [16, 72, 88, 115, 48, 61, 7, 93, 4, 77, 63, 100, 99, 21, 78, 23], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n]\n\ns = time.time()\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn, ind_worker_queue=True)\nfor i, (items, worker_ids, batch) in enumerate(dataloader):\n    assert round(expected[i][0], 5) == round(np.sum(batch.numpy()), 5)\n    assert expected[i][1] == items.numpy().tolist()\n\ne = time.time()\nprint(\"time\", e-s)\n</code></pre>", "body_text": "Added ind_worker_queue parameter to data.DataLoader. It makes preprocessing determinate.\nDataLoader in multiprocessing mode may cause non-deterministic issue. Even if radom_seed has frozen, each subprocess may get tasks in unstable order. This is caused by different I/O time while data loads. If you use augmentation while data loading, it makes results unreproduceble. Look at the https://discuss.pytorch.org/t/deterministic-non-deterministic-results-with-pytorch/9087\nTo fix this issue I have added the individual queue for each worker. In this case each worker get tasks in the stable order. In summary, subprocess produces the stable results.\nTo reproduce issue you may change ind_worker_queue to False and run the script several times.\nCode to reproduce issue.\nimport time\nimport numpy as np\nimport random\nfrom datetime import datetime\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nGLOBAL_SEED = 1024\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nGLOBAL_WORKER_ID = None\ndef worker_init_fn(worker_id):\n    global GLOBAL_WORKER_ID\n    GLOBAL_WORKER_ID = worker_id\n    set_seed(GLOBAL_SEED + worker_id)\n\n\nclass RandomAugmentationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, item):\n        global GLOBAL_WORKER_ID\n        rnd = datetime.now().microsecond % 5 # it should be unstable and not depend on random_seed\n        if rnd < 3:\n            time.sleep(rnd * 0.1)\n        return item, GLOBAL_WORKER_ID, self.data[item] + np.random.random_sample(self.data[item].shape)\n\n    def __len__(self):\n        return self.data.shape[0]\n\nset_seed(GLOBAL_SEED)\ndata = np.random.random_sample((128, 128))\ndataset = RandomAugmentationDataset(data)\n\nexpected = [\n    (2070.646908929146, [97, 0, 79, 2, 85, 121, 117, 56, 83, 43, 70, 60, 108, 47, 101, 95], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n    (2063.337454820789, [71, 91, 20, 34, 107, 27, 52, 55, 69, 90, 127, 84, 81, 105, 46, 124], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    (2031.0978096882036, [118, 86, 44, 14, 18, 92, 98, 36, 49, 13, 111, 28, 116, 10, 104, 87], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n    (2045.9375171891238, [103, 15, 19, 80, 29, 120, 1, 106, 123, 24, 39, 31, 3, 65, 45, 50], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n    (2083.1401737400893, [11, 89, 12, 58, 54, 32, 67, 17, 110, 40, 82, 9, 30, 94, 125, 35], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    (2035.8920455019213, [41, 74, 33, 68, 64, 8, 76, 42, 126, 53, 122, 26, 114, 75, 57, 112], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n    (2078.9789912514643, [109, 119, 113, 37, 5, 22, 59, 51, 62, 25, 38, 66, 6, 102, 73, 96], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n    (2054.459126893385, [16, 72, 88, 115, 48, 61, 7, 93, 4, 77, 63, 100, 99, 21, 78, 23], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n]\n\ns = time.time()\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn, ind_worker_queue=True)\nfor i, (items, worker_ids, batch) in enumerate(dataloader):\n    assert round(expected[i][0], 5) == round(np.sum(batch.numpy()), 5)\n    assert expected[i][1] == items.numpy().tolist()\n\ne = time.time()\nprint(\"time\", e-s)", "body": "Added ind_worker_queue parameter to data.DataLoader. It makes preprocessing determinate.\r\n\r\nDataLoader in multiprocessing mode may cause non-deterministic issue. Even if `radom_seed` has frozen, each subprocess may get tasks in unstable order. This is caused by different I/O time while data loads. If you use augmentation while data loading, it makes results unreproduceble. Look at the https://discuss.pytorch.org/t/deterministic-non-deterministic-results-with-pytorch/9087\r\n\r\nTo fix this issue I have added the individual queue for each worker. In this case each worker get tasks in the stable order. In summary, subprocess produces the stable results.\r\n\r\nTo reproduce issue you may change ind_worker_queue to `False` and run the script several times.\r\nCode to reproduce issue.\r\n```\r\nimport time\r\nimport numpy as np\r\nimport random\r\nfrom datetime import datetime\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import Dataset\r\n\r\nGLOBAL_SEED = 1024\r\n\r\ndef set_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n\r\nGLOBAL_WORKER_ID = None\r\ndef worker_init_fn(worker_id):\r\n    global GLOBAL_WORKER_ID\r\n    GLOBAL_WORKER_ID = worker_id\r\n    set_seed(GLOBAL_SEED + worker_id)\r\n\r\n\r\nclass RandomAugmentationDataset(Dataset):\r\n    def __init__(self, data):\r\n        self.data = data\r\n\r\n    def __getitem__(self, item):\r\n        global GLOBAL_WORKER_ID\r\n        rnd = datetime.now().microsecond % 5 # it should be unstable and not depend on random_seed\r\n        if rnd < 3:\r\n            time.sleep(rnd * 0.1)\r\n        return item, GLOBAL_WORKER_ID, self.data[item] + np.random.random_sample(self.data[item].shape)\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]\r\n\r\nset_seed(GLOBAL_SEED)\r\ndata = np.random.random_sample((128, 128))\r\ndataset = RandomAugmentationDataset(data)\r\n\r\nexpected = [\r\n    (2070.646908929146, [97, 0, 79, 2, 85, 121, 117, 56, 83, 43, 70, 60, 108, 47, 101, 95], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\r\n    (2063.337454820789, [71, 91, 20, 34, 107, 27, 52, 55, 69, 90, 127, 84, 81, 105, 46, 124], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\r\n    (2031.0978096882036, [118, 86, 44, 14, 18, 92, 98, 36, 49, 13, 111, 28, 116, 10, 104, 87], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\r\n    (2045.9375171891238, [103, 15, 19, 80, 29, 120, 1, 106, 123, 24, 39, 31, 3, 65, 45, 50], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\r\n    (2083.1401737400893, [11, 89, 12, 58, 54, 32, 67, 17, 110, 40, 82, 9, 30, 94, 125, 35], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\r\n    (2035.8920455019213, [41, 74, 33, 68, 64, 8, 76, 42, 126, 53, 122, 26, 114, 75, 57, 112], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\r\n    (2078.9789912514643, [109, 119, 113, 37, 5, 22, 59, 51, 62, 25, 38, 66, 6, 102, 73, 96], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\r\n    (2054.459126893385, [16, 72, 88, 115, 48, 61, 7, 93, 4, 77, 63, 100, 99, 21, 78, 23], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\r\n]\r\n\r\ns = time.time()\r\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn, ind_worker_queue=True)\r\nfor i, (items, worker_ids, batch) in enumerate(dataloader):\r\n    assert round(expected[i][0], 5) == round(np.sum(batch.numpy()), 5)\r\n    assert expected[i][1] == items.numpy().tolist()\r\n\r\ne = time.time()\r\nprint(\"time\", e-s)\r\n```\r\n"}