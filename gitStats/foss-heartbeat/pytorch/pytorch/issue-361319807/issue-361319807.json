{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11797", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11797/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11797/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11797/events", "html_url": "https://github.com/pytorch/pytorch/issues/11797", "id": 361319807, "node_id": "MDU6SXNzdWUzNjEzMTk4MDc=", "number": 11797, "title": "Sparse Scipy matrices are not accepted in autograd.Function parameters, but dense ones are.", "user": {"login": "matthieuheitz", "id": 2835332, "node_id": "MDQ6VXNlcjI4MzUzMzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2835332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthieuheitz", "html_url": "https://github.com/matthieuheitz", "followers_url": "https://api.github.com/users/matthieuheitz/followers", "following_url": "https://api.github.com/users/matthieuheitz/following{/other_user}", "gists_url": "https://api.github.com/users/matthieuheitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthieuheitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthieuheitz/subscriptions", "organizations_url": "https://api.github.com/users/matthieuheitz/orgs", "repos_url": "https://api.github.com/users/matthieuheitz/repos", "events_url": "https://api.github.com/users/matthieuheitz/events{/privacy}", "received_events_url": "https://api.github.com/users/matthieuheitz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-09-18T14:01:23Z", "updated_at": "2018-09-25T13:12:11Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>It seems that sparse scipy matrices create errors when transmitted as parameters of a custom autograd.Function, whereas dense scipy (numpy) matrices don't.<br>\nI don't know if this is on purpose or not...<br>\nIf it should be supported, then I think it is a bug.<br>\nIf you won't support it, then it would be nice to have a clearer and consistent (across sparse formats) message to explain that scipy sparse matrices are not supported.<br>\nFor CSR and LIL formats, the infinite recursion happens in the function <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/91b6458e2d0dba935da2cc7c2cdc6d7907bc3f48/torch/autograd/gradcheck.py#L36\">pytorch/torch/autograd/gradcheck.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 36\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/91b6458e2d0dba935da2cc7c2cdc6d7907bc3f48\">91b6458</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L36\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"36\"></td>\n          <td id=\"LC36\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">def</span> <span class=\"pl-en\">iter_tensors</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">only_requiring_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>): </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<h2>Code example</h2>\n<pre><code>import torch\nimport scipy.sparse.linalg\nfrom torch.autograd import gradcheck\n\nclass MyCustomFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, A, b):\n        ctx.intermediate = (A,b)\n        return torch.tensor(A.dot(b.detach().numpy()))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Not implemented\n        return None, grad_output\n\nN = 250\nA = 2*scipy.eye(N)                  # gradcheck works, but it returns false because the backward is not implemented.\n# A = 2*scipy.sparse.eye(N).todia()   # gradcheck gives error \"'dia_matrix' object is not subscriptable\"\n# A = 2*scipy.sparse.eye(N).tocoo()   # gradcheck gives error \"'coo_matrix' object is not subscriptable\"\n# A = 2*scipy.sparse.eye(N).tocsr()   # gradcheck gives error \"RecursionError: maximum recursion depth exceeded while calling a Python object\".\n# A = 2*scipy.sparse.eye(N).tolil()   # gradcheck gives error \"RecursionError: maximum recursion depth exceeded in comparison\".\nx = torch.ones(N, dtype=torch.float64, requires_grad=True)\ninput = (A,x)\ntest = gradcheck(MyCustomFunction.apply, input, eps=1e-6, atol=1e-4)\nprint(test)\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.5.0a0+bdbbcf0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.44</p>\n<p>OS: Ubuntu 16.04.5 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.11.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 7.5.17<br>\nGPU models and configuration: GPU 0: Quadro M2000<br>\nNvidia driver version: 384.130<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7<br>\n/usr/local/MATLAB/R2017a/bin/glnxa64/libcudnn.so.5.1.5</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.5)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.5.0a0+bdbbcf0)<br>\n[conda] magma-cuda80              2.3.0                         1    pytorch<br>\n[conda] torch                     0.5.0a0+bdbbcf0           </p>", "body_text": "Issue description\nIt seems that sparse scipy matrices create errors when transmitted as parameters of a custom autograd.Function, whereas dense scipy (numpy) matrices don't.\nI don't know if this is on purpose or not...\nIf it should be supported, then I think it is a bug.\nIf you won't support it, then it would be nice to have a clearer and consistent (across sparse formats) message to explain that scipy sparse matrices are not supported.\nFor CSR and LIL formats, the infinite recursion happens in the function \n  \n    \n      pytorch/torch/autograd/gradcheck.py\n    \n    \n         Line 36\n      in\n      91b6458\n    \n    \n    \n    \n\n        \n          \n           def iter_tensors(x, only_requiring_grad=False): \n        \n    \n  \n\n\nCode example\nimport torch\nimport scipy.sparse.linalg\nfrom torch.autograd import gradcheck\n\nclass MyCustomFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, A, b):\n        ctx.intermediate = (A,b)\n        return torch.tensor(A.dot(b.detach().numpy()))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Not implemented\n        return None, grad_output\n\nN = 250\nA = 2*scipy.eye(N)                  # gradcheck works, but it returns false because the backward is not implemented.\n# A = 2*scipy.sparse.eye(N).todia()   # gradcheck gives error \"'dia_matrix' object is not subscriptable\"\n# A = 2*scipy.sparse.eye(N).tocoo()   # gradcheck gives error \"'coo_matrix' object is not subscriptable\"\n# A = 2*scipy.sparse.eye(N).tocsr()   # gradcheck gives error \"RecursionError: maximum recursion depth exceeded while calling a Python object\".\n# A = 2*scipy.sparse.eye(N).tolil()   # gradcheck gives error \"RecursionError: maximum recursion depth exceeded in comparison\".\nx = torch.ones(N, dtype=torch.float64, requires_grad=True)\ninput = (A,x)\ntest = gradcheck(MyCustomFunction.apply, input, eps=1e-6, atol=1e-4)\nprint(test)\n\nSystem Info\nPyTorch version: 0.5.0a0+bdbbcf0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.44\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.11.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 7.5.17\nGPU models and configuration: GPU 0: Quadro M2000\nNvidia driver version: 384.130\ncuDNN version: Probably one of the following:\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7\n/usr/local/MATLAB/R2017a/bin/glnxa64/libcudnn.so.5.1.5\nVersions of relevant libraries:\n[pip3] numpy (1.14.5)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.5.0a0+bdbbcf0)\n[conda] magma-cuda80              2.3.0                         1    pytorch\n[conda] torch                     0.5.0a0+bdbbcf0", "body": "## Issue description\r\n\r\nIt seems that sparse scipy matrices create errors when transmitted as parameters of a custom autograd.Function, whereas dense scipy (numpy) matrices don't.\r\nI don't know if this is on purpose or not...\r\nIf it should be supported, then I think it is a bug.\r\nIf you won't support it, then it would be nice to have a clearer and consistent (across sparse formats) message to explain that scipy sparse matrices are not supported.\r\nFor CSR and LIL formats, the infinite recursion happens in the function https://github.com/pytorch/pytorch/blob/91b6458e2d0dba935da2cc7c2cdc6d7907bc3f48/torch/autograd/gradcheck.py#L36\r\n\r\n## Code example\r\n\r\n```\r\nimport torch\r\nimport scipy.sparse.linalg\r\nfrom torch.autograd import gradcheck\r\n\r\nclass MyCustomFunction(torch.autograd.Function):\r\n\r\n    @staticmethod\r\n    def forward(ctx, A, b):\r\n        ctx.intermediate = (A,b)\r\n        return torch.tensor(A.dot(b.detach().numpy()))\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        # Not implemented\r\n        return None, grad_output\r\n\r\nN = 250\r\nA = 2*scipy.eye(N)                  # gradcheck works, but it returns false because the backward is not implemented.\r\n# A = 2*scipy.sparse.eye(N).todia()   # gradcheck gives error \"'dia_matrix' object is not subscriptable\"\r\n# A = 2*scipy.sparse.eye(N).tocoo()   # gradcheck gives error \"'coo_matrix' object is not subscriptable\"\r\n# A = 2*scipy.sparse.eye(N).tocsr()   # gradcheck gives error \"RecursionError: maximum recursion depth exceeded while calling a Python object\".\r\n# A = 2*scipy.sparse.eye(N).tolil()   # gradcheck gives error \"RecursionError: maximum recursion depth exceeded in comparison\".\r\nx = torch.ones(N, dtype=torch.float64, requires_grad=True)\r\ninput = (A,x)\r\ntest = gradcheck(MyCustomFunction.apply, input, eps=1e-6, atol=1e-4)\r\nprint(test)\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.5.0a0+bdbbcf0                                                \r\nIs debug build: No                                                              \r\nCUDA used to build PyTorch: 8.0.44                                              \r\n                                                                                \r\nOS: Ubuntu 16.04.5 LTS                                                          \r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609                    \r\nCMake version: version 3.11.1                                                   \r\n                                                                                \r\nPython version: 3.6                                                             \r\nIs CUDA available: Yes                                                          \r\nCUDA runtime version: 7.5.17                                                    \r\nGPU models and configuration: GPU 0: Quadro M2000                               \r\nNvidia driver version: 384.130                                                  \r\ncuDNN version: Probably one of the following:                                   \r\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7                          \r\n/usr/local/MATLAB/R2017a/bin/glnxa64/libcudnn.so.5.1.5                          \r\n                                                                                \r\nVersions of relevant libraries:                                                 \r\n[pip3] numpy (1.14.5)                                                           \r\n[pip3] numpydoc (0.7.0)                                                         \r\n[pip3] torch (0.5.0a0+bdbbcf0)                                                  \r\n[conda] magma-cuda80              2.3.0                         1    pytorch    \r\n[conda] torch                     0.5.0a0+bdbbcf0           <pip>               \r\n\r\n"}