{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210188757", "pull_request_review_id": 146349664, "id": 210188757, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDE4ODc1Nw==", "diff_hunk": "@@ -0,0 +1,138 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include \"ATen/native/SobolEngineOpsUtils.h\"\n+\n+#include <vector>\n+\n+namespace at {\n+namespace native {\n+\n+/// This is the core function to draw samples from a `SobolEngine` given\n+/// its state variables (`sobolstate` and `quasi`). `dimension` can be\n+/// inferred from `sobolstate`, but choosing to pass it explicitly to avoid\n+/// an extra operation to obtain the size of the first dimension of\n+/// `sobolstate`.\n+std::tuple<Tensor, Tensor> _sobol_engine_draw(const Tensor& quasi, int64_t n, const Tensor& sobolstate,\n+                                              int64_t dimension, int64_t num_generated) {\n+  Tensor wquasi = quasi.clone();\n+  std::vector<Tensor> result;\n+  AT_CHECK(sobolstate.type().scalarType() == at::ScalarType::Long,\n+           \"sobolstate needs to be of type \", at::ScalarType::Long);\n+  AT_CHECK(wquasi.type().scalarType() == at::ScalarType::Long,\n+           \"quasi needs to be of type \", at::ScalarType::Long);\n+\n+  for (int64_t i = 0; i < n; ++i) {\n+    int64_t l = rightmost_zero(num_generated);\n+    Tensor inter_res = wquasi.__ixor__(sobolstate.select(1, l - 1));\n+    result.emplace_back(inter_res.clone());\n+    num_generated++;\n+  }\n+\n+  return std::make_tuple(at::native::stack(result, 0).toType(at::kFloat).mul_(RECIPD), wquasi);\n+}\n+\n+/// This is the core function to fast-forward a `SobolEngine` given\n+/// its state variables (`sobolstate` and `quasi`). `dimension` can be\n+/// inferred from `sobolstate`, but is passed as an argument for the same reasons\n+/// specified above.\n+Tensor _sobol_engine_ff(const Tensor& quasi, int64_t n, const Tensor& sobolstate,\n+                        int64_t dimension, int64_t num_generated) {\n+  Tensor wquasi = quasi.clone();\n+  AT_CHECK(sobolstate.type().scalarType() == at::ScalarType::Long,\n+           \"sobolstate needs to be of type \", at::ScalarType::Long);\n+  AT_CHECK(wquasi.type().scalarType() == at::ScalarType::Long,\n+           \"quasi needs to be of type \", at::ScalarType::Long);\n+\n+  for (int64_t i = 0; i < n; ++i) {\n+    int64_t l = rightmost_zero(num_generated);\n+    wquasi.__ixor__(sobolstate.select(1, l - 1));\n+    num_generated++;\n+  }\n+  return wquasi;\n+}\n+\n+/// This is an implicit function used for randomizing the state variables of the.\n+/// `SobolEngine`. Arguments are a randomized `sobolstate` state variables\n+/// and a list of random lower triangular matrices consisting of 0s and 1s. `dimension` is\n+/// passed explicitly again.\n+Tensor _sobol_engine_scramble(const Tensor& sobolstate, const Tensor& ltm, int64_t dimension) {\n+  Tensor wsobolstate = sobolstate.clone();\n+  AT_CHECK(sobolstate.type().scalarType() == at::ScalarType::Long,\n+           \"sobolstate needs to be of type \", at::ScalarType::Long);\n+  AT_CHECK(ltm.dim() == 3 && ltm.size(-1) == ltm.size(-2),\n+           \"ltm needs to be batch of square matrices\");\n+\n+  // Require a tensor accessor for `sobolstate`\n+  auto ss_a = wsobolstate.accessor<int64_t, 2>();\n+\n+  // For every tensor in the list of tensors, the diagonals are made 1\n+  // Require the the a dot product of every row with a specific vector of each of the matrices in `ltm`.\n+  // Instead, we perform an element-wise product of all the matrices and sum over the last dimension.\n+  // The required product of the m^{th} row in the d^{th} square matrix in `ltm` can be accessed\n+  // using ltm_d_a[d][m] m and d are zero-indexed\n+  Tensor diag_true = (at::native::eye(MAXBIT, wsobolstate.options()) == 1).expand_as(ltm);\n+  diag_true = at::where(diag_true, at::ones({}, ltm.options()), ltm);\n+  Tensor ltm_dots = cdot_pow2(diag_true);\n+  auto ltm_d_a = ltm_dots.accessor<int64_t, 2>();\n+\n+  // Main scrambling loop\n+  for (int64_t d = 0; d < dimension; ++d) {", "path": "aten/src/ATen/native/SobolEngineOps.cpp", "position": 89, "original_position": 80, "commit_id": "fca2445f6c70ee50686c1d54fae1d10013f80ef7", "original_commit_id": "6ef3ed9687e75679cab3aee2ddf8759c8e15c7da", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "body": "If OpenMP is present, I think we can add a guarded pragma:\r\n\r\n```\r\n#ifdef _OPENMP\r\n#pragma omp parallel for\r\n#endif\r\n\r\nI don't think there are any dependencies involving the outer loop.", "created_at": "2018-08-15T07:22:03Z", "updated_at": "2018-11-23T15:49:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/10505#discussion_r210188757", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10505", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210188757"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10505#discussion_r210188757"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10505"}}, "body_html": "<p>If OpenMP is present, I think we can add a guarded pragma:</p>\n<pre><code>#ifdef _OPENMP\n#pragma omp parallel for\n#endif\n\nI don't think there are any dependencies involving the outer loop.\n</code></pre>", "body_text": "If OpenMP is present, I think we can add a guarded pragma:\n#ifdef _OPENMP\n#pragma omp parallel for\n#endif\n\nI don't think there are any dependencies involving the outer loop."}