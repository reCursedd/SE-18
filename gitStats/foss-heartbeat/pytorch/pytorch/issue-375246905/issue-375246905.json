{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13276", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13276/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13276/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13276/events", "html_url": "https://github.com/pytorch/pytorch/issues/13276", "id": 375246905, "node_id": "MDU6SXNzdWUzNzUyNDY5MDU=", "number": 13276, "title": "Memory error for batched inverse", "user": {"login": "MarcoForte", "id": 540338, "node_id": "MDQ6VXNlcjU0MDMzOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/540338?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarcoForte", "html_url": "https://github.com/MarcoForte", "followers_url": "https://api.github.com/users/MarcoForte/followers", "following_url": "https://api.github.com/users/MarcoForte/following{/other_user}", "gists_url": "https://api.github.com/users/MarcoForte/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarcoForte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarcoForte/subscriptions", "organizations_url": "https://api.github.com/users/MarcoForte/orgs", "repos_url": "https://api.github.com/users/MarcoForte/repos", "events_url": "https://api.github.com/users/MarcoForte/events{/privacy}", "received_events_url": "https://api.github.com/users/MarcoForte/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2018-10-29T22:41:49Z", "updated_at": "2018-11-04T09:36:57Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>There seems to be a limit of N=256x256 for the number of batches one can invert on the gpu. I don't think this limit corresponds properly to available memory. Because I can run a N=256x256 -1 batched inverse on a gpu with less availiable memory. I have tested tensorflow on google collab and it allows batched inverse of size N=6144x6144.</p>\n<h2>To Reproduce</h2>\n<pre><code>import torch\nN = 256\nx = torch.randn(N*N , 2, 2).cuda()\ny = torch.inverse(x)\nprint(y.cpu().numpy())\n</code></pre>\n<p>Error:</p>\n<blockquote>\n<p>----&gt; 1 y.cpu().numpy()</p>\n<p>RuntimeError: cuda runtime error (9) : invalid configuration argument at /opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/THC/THCTensorCopy.cu:205</p>\n</blockquote>\n<h2>Expected behavior</h2>\n<p>It works if I reduce the batch dimension by one.<br>\n<code>x = torch.randn(N*N -1 , 2, 2).cuda() </code></p>\n<h2>Environment</h2>\n<blockquote>\n<p>PyTorch version: 1.0.0.dev20181029<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609<br>\nCMake version: version 3.11.0</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.1.85<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080<br>\nGPU 1: GeForce GTX 1080</p>\n<p>Nvidia driver version: 387.26<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7<br>\n/usr/local/cuda-9.1/lib64/libcudnn.so<br>\n/usr/local/cuda-9.1/lib64/libcudnn.so.7<br>\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.0.5<br>\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.1<br>\n/usr/local/cuda-9.1/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] cuda80                    1.0                           0    soumith<br>\n[conda] cuda91                    1.0                  h4c16780_0    pytorch<br>\n[conda] guided-filter-pytorch     1.1.1                     <br>\n[conda] magma-cuda91              2.3.0                         1    pytorch<br>\n[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torch                     0.3.1                     </p>\n</blockquote>", "body_text": "\ud83d\udc1b Bug\nThere seems to be a limit of N=256x256 for the number of batches one can invert on the gpu. I don't think this limit corresponds properly to available memory. Because I can run a N=256x256 -1 batched inverse on a gpu with less availiable memory. I have tested tensorflow on google collab and it allows batched inverse of size N=6144x6144.\nTo Reproduce\nimport torch\nN = 256\nx = torch.randn(N*N , 2, 2).cuda()\ny = torch.inverse(x)\nprint(y.cpu().numpy())\n\nError:\n\n----> 1 y.cpu().numpy()\nRuntimeError: cuda runtime error (9) : invalid configuration argument at /opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/THC/THCTensorCopy.cu:205\n\nExpected behavior\nIt works if I reduce the batch dimension by one.\nx = torch.randn(N*N -1 , 2, 2).cuda() \nEnvironment\n\nPyTorch version: 1.0.0.dev20181029\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609\nCMake version: version 3.11.0\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration:\nGPU 0: GeForce GTX 1080\nGPU 1: GeForce GTX 1080\nNvidia driver version: 387.26\ncuDNN version: Probably one of the following:\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7\n/usr/local/cuda-9.1/lib64/libcudnn.so\n/usr/local/cuda-9.1/lib64/libcudnn.so.7\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.0.5\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.1\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] cuda80                    1.0                           0    soumith\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\n[conda] guided-filter-pytorch     1.1.1                     \n[conda] magma-cuda91              2.3.0                         1    pytorch\n[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch\n[conda] torch                     0.4.0                     \n[conda] torch                     0.3.1", "body": "## \ud83d\udc1b Bug\r\n\r\nThere seems to be a limit of N=256x256 for the number of batches one can invert on the gpu. I don't think this limit corresponds properly to available memory. Because I can run a N=256x256 -1 batched inverse on a gpu with less availiable memory. I have tested tensorflow on google collab and it allows batched inverse of size N=6144x6144.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\nN = 256\r\nx = torch.randn(N*N , 2, 2).cuda()\r\ny = torch.inverse(x)\r\nprint(y.cpu().numpy())\r\n```\r\n\r\nError:\r\n\r\n> ----> 1 y.cpu().numpy()\r\n> \r\n> RuntimeError: cuda runtime error (9) : invalid configuration argument at /opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/THC/THCTensorCopy.cu:205\r\n\r\n## Expected behavior\r\nIt works if I reduce the batch dimension by one.\r\n`x = torch.randn(N*N -1 , 2, 2).cuda()\r\n`\r\n## Environment\r\n\r\n> PyTorch version: 1.0.0.dev20181029\r\n> Is debug build: No\r\n> CUDA used to build PyTorch: 8.0.61\r\n> \r\n> OS: Ubuntu 16.04.4 LTS\r\n> GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609\r\n> CMake version: version 3.11.0\r\n> \r\n> Python version: 3.6\r\n> Is CUDA available: Yes\r\n> CUDA runtime version: 9.1.85\r\n> GPU models and configuration: \r\n> GPU 0: GeForce GTX 1080\r\n> GPU 1: GeForce GTX 1080\r\n> \r\n> Nvidia driver version: 387.26\r\n> cuDNN version: Probably one of the following:\r\n> /usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7\r\n> /usr/local/cuda-9.1/lib64/libcudnn.so\r\n> /usr/local/cuda-9.1/lib64/libcudnn.so.7\r\n> /usr/local/cuda-9.1/lib64/libcudnn.so.7.0.5\r\n> /usr/local/cuda-9.1/lib64/libcudnn.so.7.1.1\r\n> /usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n> \r\n> Versions of relevant libraries:\r\n> [pip] Could not collect\r\n> [conda] cuda80                    1.0                           0    soumith\r\n> [conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n> [conda] guided-filter-pytorch     1.1.1                     <pip>\r\n> [conda] magma-cuda91              2.3.0                         1    pytorch\r\n> [conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch\r\n> [conda] torch                     0.4.0                     <pip>\r\n> [conda] torch                     0.3.1                     <pip>\r\n"}