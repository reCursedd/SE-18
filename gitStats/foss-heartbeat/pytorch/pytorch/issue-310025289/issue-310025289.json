{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6126", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6126/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6126/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6126/events", "html_url": "https://github.com/pytorch/pytorch/issues/6126", "id": 310025289, "node_id": "MDU6SXNzdWUzMTAwMjUyODk=", "number": 6126, "title": "Inf and nan loss ", "user": {"login": "ayush1999", "id": 24913958, "node_id": "MDQ6VXNlcjI0OTEzOTU4", "avatar_url": "https://avatars2.githubusercontent.com/u/24913958?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ayush1999", "html_url": "https://github.com/ayush1999", "followers_url": "https://api.github.com/users/ayush1999/followers", "following_url": "https://api.github.com/users/ayush1999/following{/other_user}", "gists_url": "https://api.github.com/users/ayush1999/gists{/gist_id}", "starred_url": "https://api.github.com/users/ayush1999/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ayush1999/subscriptions", "organizations_url": "https://api.github.com/users/ayush1999/orgs", "repos_url": "https://api.github.com/users/ayush1999/repos", "events_url": "https://api.github.com/users/ayush1999/events{/privacy}", "received_events_url": "https://api.github.com/users/ayush1999/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-30T09:47:47Z", "updated_at": "2018-03-30T11:15:54Z", "closed_at": "2018-03-30T11:15:54Z", "author_association": "NONE", "body_html": "<p>Here is my network:</p>\n<pre><code>import torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size , hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, input, hidden):\n        h = self.softmax(self.h2h(hidden)+  self.i2h(input))\n        o = self.softmax(self.h2o(h))\n        return o, h\n\n    def init_hidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n\n</code></pre>\n<p>Then, I created the network and the training loops as :</p>\n<pre><code>\nrnn = RNN(n_chars, 90, n_chars)\ncriterion = nn.KLDivLoss()\nlearning_rate = 0.05\noptimizer = torch.optim.Adam(rnn.parameters(), lr = learning_rate)\nhidden = rnn.init_hidden()\nepochs = 5\n\nfor epoch in range(epochs):\n    for i in range(len(X)):\n        for ele in X[i]:\n            output, hidden = rnn(Variable(ele.t()), hidden)\n        loss = criterion(output, Y[i])\n        \n        loss.backward(retain_graph=True)\n        optimizer.zero_grad()\n        optimizer.step()\n        print('Current loss is: ', loss)\n</code></pre>\n<p>The initial output was very large, then it became Inf and finally Nan. Why is this happening?</p>", "body_text": "Here is my network:\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size , hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, input, hidden):\n        h = self.softmax(self.h2h(hidden)+  self.i2h(input))\n        o = self.softmax(self.h2o(h))\n        return o, h\n\n    def init_hidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n\n\nThen, I created the network and the training loops as :\n\nrnn = RNN(n_chars, 90, n_chars)\ncriterion = nn.KLDivLoss()\nlearning_rate = 0.05\noptimizer = torch.optim.Adam(rnn.parameters(), lr = learning_rate)\nhidden = rnn.init_hidden()\nepochs = 5\n\nfor epoch in range(epochs):\n    for i in range(len(X)):\n        for ele in X[i]:\n            output, hidden = rnn(Variable(ele.t()), hidden)\n        loss = criterion(output, Y[i])\n        \n        loss.backward(retain_graph=True)\n        optimizer.zero_grad()\n        optimizer.step()\n        print('Current loss is: ', loss)\n\nThe initial output was very large, then it became Inf and finally Nan. Why is this happening?", "body": "Here is my network:\r\n```\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size, output_size):\r\n        super().__init__()\r\n        self.hidden_size = hidden_size\r\n        self.i2h = nn.Linear(input_size , hidden_size)\r\n        self.h2o = nn.Linear(hidden_size, output_size)\r\n        self.h2h = nn.Linear(hidden_size, hidden_size)\r\n        self.softmax = nn.LogSoftmax(dim = 1)\r\n\r\n    def forward(self, input, hidden):\r\n        h = self.softmax(self.h2h(hidden)+  self.i2h(input))\r\n        o = self.softmax(self.h2o(h))\r\n        return o, h\r\n\r\n    def init_hidden(self):\r\n        return Variable(torch.zeros(1, self.hidden_size))\r\n\r\n```\r\nThen, I created the network and the training loops as :\r\n```\r\n\r\nrnn = RNN(n_chars, 90, n_chars)\r\ncriterion = nn.KLDivLoss()\r\nlearning_rate = 0.05\r\noptimizer = torch.optim.Adam(rnn.parameters(), lr = learning_rate)\r\nhidden = rnn.init_hidden()\r\nepochs = 5\r\n\r\nfor epoch in range(epochs):\r\n    for i in range(len(X)):\r\n        for ele in X[i]:\r\n            output, hidden = rnn(Variable(ele.t()), hidden)\r\n        loss = criterion(output, Y[i])\r\n        \r\n        loss.backward(retain_graph=True)\r\n        optimizer.zero_grad()\r\n        optimizer.step()\r\n        print('Current loss is: ', loss)\r\n```\r\n\r\nThe initial output was very large, then it became Inf and finally Nan. Why is this happening?"}