{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5604", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5604/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5604/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5604/events", "html_url": "https://github.com/pytorch/pytorch/issues/5604", "id": 302934968, "node_id": "MDU6SXNzdWUzMDI5MzQ5Njg=", "number": 5604, "title": "An error happends when using torch.backends.cudnn.CuDNNError before calling  torch.backends.cudnn._libcudnn", "user": {"login": "kamo-naoyuki", "id": 19261024, "node_id": "MDQ6VXNlcjE5MjYxMDI0", "avatar_url": "https://avatars0.githubusercontent.com/u/19261024?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kamo-naoyuki", "html_url": "https://github.com/kamo-naoyuki", "followers_url": "https://api.github.com/users/kamo-naoyuki/followers", "following_url": "https://api.github.com/users/kamo-naoyuki/following{/other_user}", "gists_url": "https://api.github.com/users/kamo-naoyuki/gists{/gist_id}", "starred_url": "https://api.github.com/users/kamo-naoyuki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kamo-naoyuki/subscriptions", "organizations_url": "https://api.github.com/users/kamo-naoyuki/orgs", "repos_url": "https://api.github.com/users/kamo-naoyuki/repos", "events_url": "https://api.github.com/users/kamo-naoyuki/events{/privacy}", "received_events_url": "https://api.github.com/users/kamo-naoyuki/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-07T01:50:31Z", "updated_at": "2018-03-13T15:54:55Z", "closed_at": "2018-03-13T15:54:55Z", "author_association": "NONE", "body_html": "<ul>\n<li>OS: CentOS7</li>\n<li>PyTorch version:  0.3.1.post2</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version:  3.6</li>\n<li>CUDA/cuDNN version: 9.1/7</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source): 4.8.5</li>\n</ul>\n<p>I'd like to use pytorch with multiprocessing, however, I met an error by the following code.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.multiprocessing could also reproduce</span>\n<span class=\"pl-k\">import</span> multiprocessing\n<span class=\"pl-k\">import</span> traceback\n\n<span class=\"pl-k\">import</span> torch\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">_ExceptionWithTraceback</span>:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Hack to embed stringification of remote traceback in local traceback<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">exc</span>, <span class=\"pl-smi\">tb</span>):\n        tb <span class=\"pl-k\">=</span> traceback.format_exception(<span class=\"pl-c1\">type</span>(exc), exc, tb)\n        tb <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join(tb)\n        <span class=\"pl-c1\">self</span>.exc <span class=\"pl-k\">=</span> exc\n        <span class=\"pl-c1\">self</span>.tb <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span>\"\"\"<span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">%s</span>\"\"\"<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> tb\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__reduce__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> _rebuild_exc, (<span class=\"pl-c1\">self</span>.exc, <span class=\"pl-c1\">self</span>.tb)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_rebuild_exc</span>(<span class=\"pl-smi\">exc</span>, <span class=\"pl-smi\">tb</span>):\n    exc.__cause__ <span class=\"pl-k\">=</span> _RemoteTraceback(tb)\n    <span class=\"pl-k\">return</span> exc\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">_RemoteTraceback</span>(<span class=\"pl-c1\">Exception</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">tb</span>):\n        <span class=\"pl-c1\">self</span>.tb <span class=\"pl-k\">=</span> tb\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__str__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.tb\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">queue</span>):\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> version() in order to invoke _libcudnn()</span>\n        torch.backends.cudnn.version()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Emulate the case that some cudnn error happens</span>\n        <span class=\"pl-k\">raise</span> torch.backends.cudnn.CuDNNError(<span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">except</span> torch.backends.cudnn.CuDNNError <span class=\"pl-k\">as</span> e:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> _ExceptionWithTraceback, which is a technique for rebuild full traceback from other process, has no relations to this issue, but is inserted for  your understanding of my situation</span>\n        queue.put(_ExceptionWithTraceback(e, e.<span class=\"pl-c1\">__traceback__</span>))\n\nq <span class=\"pl-k\">=</span> multiprocessing.Queue()\np <span class=\"pl-k\">=</span> multiprocessing.Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>f, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>[q])\np.start()\nq.get()</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"a.py\", line 43, in &lt;module&gt;\n    q.get()\n  File \".../lib/python3.6/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \".../lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 116, in __init__\n    msg = '{}: {}'.format(status, get_error_string(status))\n  File .../lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 278, in get_error_string\n    return lib.cudnnGetErrorString(status)\nAttributeError: 'NoneType' object has no attribute 'cudnnGetErrorString'\n</code></pre>\n<p>This error is caused by instantiating CuDNNError before initialization of cudnn when unpickling from the other process.</p>\n<p>To say, this error can be reproduced by this small code,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\ntorch.backends.cudnn.CuDNNError(<span class=\"pl-c1\">1</span>)</pre></div>\n<p>This may be edge case,  however, because torch.backends.cudnn.CuDNNError, check_error, and etc. seems public functions, they should be enable to be invoked without exception even if torch.backends.cudnn._libcudnn never be called.</p>", "body_text": "OS: CentOS7\nPyTorch version:  0.3.1.post2\nHow you installed PyTorch (conda, pip, source): conda\nPython version:  3.6\nCUDA/cuDNN version: 9.1/7\nGPU models and configuration:\nGCC version (if compiling from source): 4.8.5\n\nI'd like to use pytorch with multiprocessing, however, I met an error by the following code.\n# torch.multiprocessing could also reproduce\nimport multiprocessing\nimport traceback\n\nimport torch\n\n\nclass _ExceptionWithTraceback:\n    \"\"\"Hack to embed stringification of remote traceback in local traceback\"\"\"\n    def __init__(self, exc, tb):\n        tb = traceback.format_exception(type(exc), exc, tb)\n        tb = ''.join(tb)\n        self.exc = exc\n        self.tb = '\\n\"\"\"\\n%s\"\"\"' % tb\n\n    def __reduce__(self):\n        return _rebuild_exc, (self.exc, self.tb)\n\n\ndef _rebuild_exc(exc, tb):\n    exc.__cause__ = _RemoteTraceback(tb)\n    return exc\n\n\nclass _RemoteTraceback(Exception):\n    def __init__(self, tb):\n        self.tb = tb\n\n    def __str__(self):\n        return self.tb\n\n\ndef f(queue):\n    try:\n        # version() in order to invoke _libcudnn()\n        torch.backends.cudnn.version()\n        # Emulate the case that some cudnn error happens\n        raise torch.backends.cudnn.CuDNNError(1)\n    except torch.backends.cudnn.CuDNNError as e:\n        # _ExceptionWithTraceback, which is a technique for rebuild full traceback from other process, has no relations to this issue, but is inserted for  your understanding of my situation\n        queue.put(_ExceptionWithTraceback(e, e.__traceback__))\n\nq = multiprocessing.Queue()\np = multiprocessing.Process(target=f, args=[q])\np.start()\nq.get()\nTraceback (most recent call last):\n  File \"a.py\", line 43, in <module>\n    q.get()\n  File \".../lib/python3.6/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \".../lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 116, in __init__\n    msg = '{}: {}'.format(status, get_error_string(status))\n  File .../lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 278, in get_error_string\n    return lib.cudnnGetErrorString(status)\nAttributeError: 'NoneType' object has no attribute 'cudnnGetErrorString'\n\nThis error is caused by instantiating CuDNNError before initialization of cudnn when unpickling from the other process.\nTo say, this error can be reproduced by this small code,\nimport torch\ntorch.backends.cudnn.CuDNNError(1)\nThis may be edge case,  however, because torch.backends.cudnn.CuDNNError, check_error, and etc. seems public functions, they should be enable to be invoked without exception even if torch.backends.cudnn._libcudnn never be called.", "body": "- OS: CentOS7\r\n- PyTorch version:  0.3.1.post2\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version:  3.6\r\n- CUDA/cuDNN version: 9.1/7\r\n- GPU models and configuration:  \r\n- GCC version (if compiling from source): 4.8.5\r\n\r\nI'd like to use pytorch with multiprocessing, however, I met an error by the following code.\r\n\r\n```python\r\n# torch.multiprocessing could also reproduce\r\nimport multiprocessing\r\nimport traceback\r\n\r\nimport torch\r\n\r\n\r\nclass _ExceptionWithTraceback:\r\n    \"\"\"Hack to embed stringification of remote traceback in local traceback\"\"\"\r\n    def __init__(self, exc, tb):\r\n        tb = traceback.format_exception(type(exc), exc, tb)\r\n        tb = ''.join(tb)\r\n        self.exc = exc\r\n        self.tb = '\\n\"\"\"\\n%s\"\"\"' % tb\r\n\r\n    def __reduce__(self):\r\n        return _rebuild_exc, (self.exc, self.tb)\r\n\r\n\r\ndef _rebuild_exc(exc, tb):\r\n    exc.__cause__ = _RemoteTraceback(tb)\r\n    return exc\r\n\r\n\r\nclass _RemoteTraceback(Exception):\r\n    def __init__(self, tb):\r\n        self.tb = tb\r\n\r\n    def __str__(self):\r\n        return self.tb\r\n\r\n\r\ndef f(queue):\r\n    try:\r\n        # version() in order to invoke _libcudnn()\r\n        torch.backends.cudnn.version()\r\n        # Emulate the case that some cudnn error happens\r\n        raise torch.backends.cudnn.CuDNNError(1)\r\n    except torch.backends.cudnn.CuDNNError as e:\r\n        # _ExceptionWithTraceback, which is a technique for rebuild full traceback from other process, has no relations to this issue, but is inserted for  your understanding of my situation\r\n        queue.put(_ExceptionWithTraceback(e, e.__traceback__))\r\n\r\nq = multiprocessing.Queue()\r\np = multiprocessing.Process(target=f, args=[q])\r\np.start()\r\nq.get()\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 43, in <module>\r\n    q.get()\r\n  File \".../lib/python3.6/multiprocessing/queues.py\", line 113, in get\r\n    return _ForkingPickler.loads(res)\r\n  File \".../lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 116, in __init__\r\n    msg = '{}: {}'.format(status, get_error_string(status))\r\n  File .../lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 278, in get_error_string\r\n    return lib.cudnnGetErrorString(status)\r\nAttributeError: 'NoneType' object has no attribute 'cudnnGetErrorString'\r\n```\r\n\r\nThis error is caused by instantiating CuDNNError before initialization of cudnn when unpickling from the other process. \r\n\r\nTo say, this error can be reproduced by this small code,\r\n\r\n```python\r\nimport torch\r\ntorch.backends.cudnn.CuDNNError(1)\r\n```\r\n\r\nThis may be edge case,  however, because torch.backends.cudnn.CuDNNError, check_error, and etc. seems public functions, they should be enable to be invoked without exception even if torch.backends.cudnn._libcudnn never be called."}