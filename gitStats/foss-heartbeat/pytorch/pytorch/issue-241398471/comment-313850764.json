{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/313850764", "html_url": "https://github.com/pytorch/pytorch/issues/2013#issuecomment-313850764", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2013", "id": 313850764, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzg1MDc2NA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-08T11:36:03Z", "updated_at": "2017-07-08T11:36:03Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15276915\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mhgump\">@mhgump</a> <code>torch.view</code> only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.<br>\nAs an example, say we have a tensor <code>x</code> of size <code>5x4x3x2</code>. Doing <code>x.view(5, -1)</code>  just rearranges the sizes (you can see by creating a <code>torch.arange(0, 5 * 4 * 3 * 2)</code> tensor, view it and view back).</p>\n<p>For the source code, it was moved to <code>C</code> for efficiency, but you can find the old python implementation <a href=\"https://github.com/pytorch/pytorch/blob/518864a7e094f32044ef4c0de82c0f20f4ed99c2/torch/tensor.py#L178-L217\">here</a>.</p>\n<p>One more note, if your tensor is of shape <code>N x C x H x W</code>, and you want to apply softmax on <code>C</code>, you need to transpose it before viewing it. Something like</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.rand(N, C, H, W)\na_t <span class=\"pl-k\">=</span> a.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>).contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, C)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ... perform softmax</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> view back</span>\nres <span class=\"pl-k\">=</span> res_t.view(N, H, W, C).permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)</pre></div>", "body_text": "@mhgump torch.view only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.\nAs an example, say we have a tensor x of size 5x4x3x2. Doing x.view(5, -1)  just rearranges the sizes (you can see by creating a torch.arange(0, 5 * 4 * 3 * 2) tensor, view it and view back).\nFor the source code, it was moved to C for efficiency, but you can find the old python implementation here.\nOne more note, if your tensor is of shape N x C x H x W, and you want to apply softmax on C, you need to transpose it before viewing it. Something like\na = torch.rand(N, C, H, W)\na_t = a.permute(0, 2, 3, 1).contiguous().view(-1, C)\n# ... perform softmax\n# view back\nres = res_t.view(N, H, W, C).permute(0, 3, 1, 2)", "body": "@mhgump `torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.\r\nAs an example, say we have a tensor `x` of size `5x4x3x2`. Doing `x.view(5, -1)`  just rearranges the sizes (you can see by creating a `torch.arange(0, 5 * 4 * 3 * 2)` tensor, view it and view back).\r\n\r\nFor the source code, it was moved to `C` for efficiency, but you can find the old python implementation [here](https://github.com/pytorch/pytorch/blob/518864a7e094f32044ef4c0de82c0f20f4ed99c2/torch/tensor.py#L178-L217).\r\n\r\nOne more note, if your tensor is of shape `N x C x H x W`, and you want to apply softmax on `C`, you need to transpose it before viewing it. Something like\r\n\r\n```python\r\na = torch.rand(N, C, H, W)\r\na_t = a.permute(0, 2, 3, 1).contiguous().view(-1, C)\r\n# ... perform softmax\r\n# view back\r\nres = res_t.view(N, H, W, C).permute(0, 3, 1, 2)\r\n```"}