{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196943513", "pull_request_review_id": 130590129, "id": 196943513, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Njk0MzUxMw==", "diff_hunk": "@@ -0,0 +1,76 @@\n+#pragma once\n+\n+#include \"caffe2/core/dispatch/DeviceId.h\"\n+#include \"caffe2/core/dispatch/LayoutId.h\"\n+#include \"caffe2/core/typeid.h\"\n+\n+#include <vector>\n+#include <functional>\n+#include \"caffe2/utils/Array.h\"\n+\n+namespace c10 {\n+\n+namespace details {\n+struct TensorParameterDispatchKey final {\n+  DeviceTypeId deviceTypeId;\n+  LayoutId layoutId;\n+  // TODO Move this CaffeTypeId to c10 namespace\n+  caffe2::CaffeTypeId dataType;\n+};\n+inline constexpr bool operator==(const TensorParameterDispatchKey& lhs, const TensorParameterDispatchKey& rhs) {\n+  return lhs.deviceTypeId == rhs.deviceTypeId && lhs.layoutId == rhs.layoutId && lhs.dataType == rhs.dataType;\n+}\n+}  // namespace details\n+}  // namespace c10\n+\n+namespace std {\n+  template<>\n+  struct hash<c10::details::TensorParameterDispatchKey> {\n+    // TODO constexpr hashing\n+    size_t operator()(const c10::details::TensorParameterDispatchKey& obj) const {\n+      return std::hash<c10::DeviceTypeId>()(obj.deviceTypeId) ^ std::hash<c10::LayoutId>()(obj.layoutId) ^ std::hash<caffe2::CaffeTypeId>()(obj.dataType);\n+    }\n+  };\n+}  // namespace std\n+\n+namespace c10 {\n+/**\n+ * The dispatch key encodes the runtime type identity of a function call arguments,\n+ * specifying what aspects of this identity can be dynamically dispatched on.\n+ *\n+ * Intuitively, given a function signature like f(Tensor, int), a valid dispatch\n+ * key for the arguments might be [CPUFloatTensor] (notice that 'f' is NOT included\n+ * in the dispatch key, and the runtime type of 'int' is NOT considered for dispatch\n+ * (since it is trivial).\n+ *\n+ * Dispatch keys permit equality tests and are hashable.\n+ *\n+ * @tparam num_dispatch_args The number of dispatchable arguments", "path": "caffe2/core/dispatch/DispatchKey.h", "position": 49, "original_position": 48, "commit_id": "dc0577c9f521b81b236c172357c8522af30c68b0", "original_commit_id": "ef14f37bd6952e3d71fb910c2aba78fe88d0b331", "user": {"login": "smessmer", "id": 2373925, "node_id": "MDQ6VXNlcjIzNzM5MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2373925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smessmer", "html_url": "https://github.com/smessmer", "followers_url": "https://api.github.com/users/smessmer/followers", "following_url": "https://api.github.com/users/smessmer/following{/other_user}", "gists_url": "https://api.github.com/users/smessmer/gists{/gist_id}", "starred_url": "https://api.github.com/users/smessmer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smessmer/subscriptions", "organizations_url": "https://api.github.com/users/smessmer/orgs", "repos_url": "https://api.github.com/users/smessmer/repos", "events_url": "https://api.github.com/users/smessmer/events{/privacy}", "received_events_url": "https://api.github.com/users/smessmer/received_events", "type": "User", "site_admin": false}, "body": "This is a multi-dispatch-by-default design. We can rehash this discussion if we want to, but I don't think we converged on saying we do single-dispatch-by-default.", "created_at": "2018-06-20T21:09:41Z", "updated_at": "2018-11-23T15:45:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/8662#discussion_r196943513", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8662", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196943513"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8662#discussion_r196943513"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8662"}}, "body_html": "<p>This is a multi-dispatch-by-default design. We can rehash this discussion if we want to, but I don't think we converged on saying we do single-dispatch-by-default.</p>", "body_text": "This is a multi-dispatch-by-default design. We can rehash this discussion if we want to, but I don't think we converged on saying we do single-dispatch-by-default.", "in_reply_to_id": 196620689}