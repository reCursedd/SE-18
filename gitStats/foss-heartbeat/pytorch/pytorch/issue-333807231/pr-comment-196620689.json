{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196620689", "pull_request_review_id": 130208338, "id": 196620689, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjYyMDY4OQ==", "diff_hunk": "@@ -0,0 +1,76 @@\n+#pragma once\n+\n+#include \"caffe2/core/dispatch/DeviceId.h\"\n+#include \"caffe2/core/dispatch/LayoutId.h\"\n+#include \"caffe2/core/typeid.h\"\n+\n+#include <vector>\n+#include <functional>\n+#include \"caffe2/utils/Array.h\"\n+\n+namespace c10 {\n+\n+namespace details {\n+struct TensorParameterDispatchKey final {\n+  DeviceTypeId deviceTypeId;\n+  LayoutId layoutId;\n+  // TODO Move this CaffeTypeId to c10 namespace\n+  caffe2::CaffeTypeId dataType;\n+};\n+inline constexpr bool operator==(const TensorParameterDispatchKey& lhs, const TensorParameterDispatchKey& rhs) {\n+  return lhs.deviceTypeId == rhs.deviceTypeId && lhs.layoutId == rhs.layoutId && lhs.dataType == rhs.dataType;\n+}\n+}  // namespace details\n+}  // namespace c10\n+\n+namespace std {\n+  template<>\n+  struct hash<c10::details::TensorParameterDispatchKey> {\n+    // TODO constexpr hashing\n+    size_t operator()(const c10::details::TensorParameterDispatchKey& obj) const {\n+      return std::hash<c10::DeviceTypeId>()(obj.deviceTypeId) ^ std::hash<c10::LayoutId>()(obj.layoutId) ^ std::hash<caffe2::CaffeTypeId>()(obj.dataType);\n+    }\n+  };\n+}  // namespace std\n+\n+namespace c10 {\n+/**\n+ * The dispatch key encodes the runtime type identity of a function call arguments,\n+ * specifying what aspects of this identity can be dynamically dispatched on.\n+ *\n+ * Intuitively, given a function signature like f(Tensor, int), a valid dispatch\n+ * key for the arguments might be [CPUFloatTensor] (notice that 'f' is NOT included\n+ * in the dispatch key, and the runtime type of 'int' is NOT considered for dispatch\n+ * (since it is trivial).\n+ *\n+ * Dispatch keys permit equality tests and are hashable.\n+ *\n+ * @tparam num_dispatch_args The number of dispatchable arguments", "path": "caffe2/core/dispatch/DispatchKey.h", "position": 49, "original_position": 48, "commit_id": "dc0577c9f521b81b236c172357c8522af30c68b0", "original_commit_id": "ef14f37bd6952e3d71fb910c2aba78fe88d0b331", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "IIRC, @zdevito proposed today that we not turn on multiple dispatch unconditionally for all operators; instead, we only apply it for the dozen or so ops that actually need multiple dispatch. It would be nice for this axis of the design space to be pointed out here, because this class, and `getTensorTypeIds_`, strongly imply that this design is NOT what is being planned for.", "created_at": "2018-06-20T00:46:09Z", "updated_at": "2018-11-23T15:45:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/8662#discussion_r196620689", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8662", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196620689"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8662#discussion_r196620689"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8662"}}, "body_html": "<p>IIRC, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> proposed today that we not turn on multiple dispatch unconditionally for all operators; instead, we only apply it for the dozen or so ops that actually need multiple dispatch. It would be nice for this axis of the design space to be pointed out here, because this class, and <code>getTensorTypeIds_</code>, strongly imply that this design is NOT what is being planned for.</p>", "body_text": "IIRC, @zdevito proposed today that we not turn on multiple dispatch unconditionally for all operators; instead, we only apply it for the dozen or so ops that actually need multiple dispatch. It would be nice for this axis of the design space to be pointed out here, because this class, and getTensorTypeIds_, strongly imply that this design is NOT what is being planned for."}