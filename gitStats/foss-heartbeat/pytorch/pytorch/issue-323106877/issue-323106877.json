{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7568", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7568/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7568/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7568/events", "html_url": "https://github.com/pytorch/pytorch/issues/7568", "id": 323106877, "node_id": "MDU6SXNzdWUzMjMxMDY4Nzc=", "number": 7568, "title": "[Bug report] error when using weight_norm and DataParallel at the same time.", "user": {"login": "liqing-ustc", "id": 10334851, "node_id": "MDQ6VXNlcjEwMzM0ODUx", "avatar_url": "https://avatars3.githubusercontent.com/u/10334851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liqing-ustc", "html_url": "https://github.com/liqing-ustc", "followers_url": "https://api.github.com/users/liqing-ustc/followers", "following_url": "https://api.github.com/users/liqing-ustc/following{/other_user}", "gists_url": "https://api.github.com/users/liqing-ustc/gists{/gist_id}", "starred_url": "https://api.github.com/users/liqing-ustc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liqing-ustc/subscriptions", "organizations_url": "https://api.github.com/users/liqing-ustc/orgs", "repos_url": "https://api.github.com/users/liqing-ustc/repos", "events_url": "https://api.github.com/users/liqing-ustc/events{/privacy}", "received_events_url": "https://api.github.com/users/liqing-ustc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-05-15T08:00:21Z", "updated_at": "2018-05-30T22:08:23Z", "closed_at": "2018-05-30T22:08:23Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When I try to use weight_norm (dim=None) and DataParallel to use multiple gpus at the same time, there is an error:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10334851/40043728-f9492b84-5857-11e8-960f-a8e88e8b5913.png\"><img src=\"https://user-images.githubusercontent.com/10334851/40043728-f9492b84-5857-11e8-960f-a8e88e8b5913.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>After digging into the code,  I found the reason is that the \"weight_g\" in weight_norm (dim=None) is a 0-dim tensor. This is due to the line 10 in torch/nn/utils/weight_norm.py: <code>return p.norm()</code>.<br>\n<code>norm()</code> returns a 0-dim tensor (scalar) in pytorch0.4.0, while in pytorch0.3.0, it returns a 1-dim tensor.<br>\nThe 0-dim \"weight_g\" somehow generates the above error when replicating across multiple gpus as in the line 12 of torch/nn/parallel/replicate.py: \"param_copies = Broadcast.apply(devices, *params)\"</p>\n<p>for now, my solution is to reshape the \"weight_g\" into a 1-dim tensor by changing <code>return p.norm()</code> in the line 10 of torch/nn/utils/weight_norm.py into <code>return p.norm().view(-1)</code>. It solves the error.</p>\n<h2>Code example</h2>\n<pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\nimport torch\nfrom torch import nn\nfrom torch.nn.utils import weight_norm\n\ndevice = torch.device('cuda')\nmodel = weight_norm(nn.Linear(20, 30), dim=None)\nmodel = nn.DataParallel(model).to(device)\n\nx = torch.rand(40, 20).to(device)\ny = model(x)\nloss = y.mean()\nloss.backward()\n</code></pre>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.4.0</li>\n<li>Python version: 2.7</li>\n<li>CUDA/cuDNN version: 8.0</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nWhen I try to use weight_norm (dim=None) and DataParallel to use multiple gpus at the same time, there is an error:\n\nAfter digging into the code,  I found the reason is that the \"weight_g\" in weight_norm (dim=None) is a 0-dim tensor. This is due to the line 10 in torch/nn/utils/weight_norm.py: return p.norm().\nnorm() returns a 0-dim tensor (scalar) in pytorch0.4.0, while in pytorch0.3.0, it returns a 1-dim tensor.\nThe 0-dim \"weight_g\" somehow generates the above error when replicating across multiple gpus as in the line 12 of torch/nn/parallel/replicate.py: \"param_copies = Broadcast.apply(devices, *params)\"\nfor now, my solution is to reshape the \"weight_g\" into a 1-dim tensor by changing return p.norm() in the line 10 of torch/nn/utils/weight_norm.py into return p.norm().view(-1). It solves the error.\nCode example\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\nimport torch\nfrom torch import nn\nfrom torch.nn.utils import weight_norm\n\ndevice = torch.device('cuda')\nmodel = weight_norm(nn.Linear(20, 30), dim=None)\nmodel = nn.DataParallel(model).to(device)\n\nx = torch.rand(40, 20).to(device)\ny = model(x)\nloss = y.mean()\nloss.backward()\n\nSystem Info\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): pip\nBuild command you used (if compiling from source):\nOS: Ubuntu 16.04\nPyTorch version: 0.4.0\nPython version: 2.7\nCUDA/cuDNN version: 8.0\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nWhen I try to use weight_norm (dim=None) and DataParallel to use multiple gpus at the same time, there is an error:\r\n![image](https://user-images.githubusercontent.com/10334851/40043728-f9492b84-5857-11e8-960f-a8e88e8b5913.png)\r\n\r\n\r\n\r\nAfter digging into the code,  I found the reason is that the \"weight_g\" in weight_norm (dim=None) is a 0-dim tensor. This is due to the line 10 in torch/nn/utils/weight_norm.py: `return p.norm()`.\r\n`norm()` returns a 0-dim tensor (scalar) in pytorch0.4.0, while in pytorch0.3.0, it returns a 1-dim tensor.\r\nThe 0-dim \"weight_g\" somehow generates the above error when replicating across multiple gpus as in the line 12 of torch/nn/parallel/replicate.py: \"param_copies = Broadcast.apply(devices, *params)\"\r\n\r\nfor now, my solution is to reshape the \"weight_g\" into a 1-dim tensor by changing `return p.norm()` in the line 10 of torch/nn/utils/weight_norm.py into `return p.norm().view(-1)`. It solves the error.\r\n\r\n## Code example\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn.utils import weight_norm\r\n\r\ndevice = torch.device('cuda')\r\nmodel = weight_norm(nn.Linear(20, 30), dim=None)\r\nmodel = nn.DataParallel(model).to(device)\r\n\r\nx = torch.rand(40, 20).to(device)\r\ny = model(x)\r\nloss = y.mean()\r\nloss.backward()\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: PyTorch \r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Build command you used (if compiling from source):\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.4.0\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: 8.0\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}