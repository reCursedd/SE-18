{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232256254", "pull_request_review_id": 173414104, "id": 232256254, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjI1NjI1NA==", "diff_hunk": "@@ -148,28 +167,23 @@ struct THCCachingAllocator\n   }\n \n   /** allocates a block which is safe to use from the provided stream */\n-  cudaError_t malloc(void** devPtr, size_t size, cudaStream_t stream)\n+  void malloc(void** devPtr, size_t size, cudaStream_t stream)\n   {\n     std::lock_guard<std::mutex> lock(mutex);\n \n     int device;\n-    cudaError_t err = cudaGetDevice(&device);\n-    if (err != cudaSuccess) {\n-      return err;\n-    }\n+    AT_CUDA_CHECK(cudaGetDevice(&device));\n \n-    err = process_events();\n-    if (err != cudaSuccess) {\n-      return err;\n-    }\n+    // process outstanding cudaEvents\n+    process_events();\n \n     size = round_size(size);\n     bool small = size <= kSmallAlloc;\n \n     DeviceStats &stats = get_stats_for_device(device);\n \n     Block search_key(device, stream, size);\n-    auto& free_blocks = small ? large_blocks : small_blocks;\n+    auto& free_blocks = small ? small_blocks : large_blocks;", "path": "aten/src/THC/THCCachingAllocator.cpp", "position": 56, "original_position": 56, "commit_id": "eb622af61ef8c28aa31b95d9a66fa07fbb4601bf", "original_commit_id": "fd95e1ce1007b5826dac9df7e35968a56f2b5ce5", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Hmmm how is it possible that this is equivalent? I can\u2019t see where\u2019s the other swap that would cancel this one out.", "created_at": "2018-11-09T13:42:28Z", "updated_at": "2018-11-23T15:54:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/13751#discussion_r232256254", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13751", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232256254"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13751#discussion_r232256254"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13751"}}, "body_html": "<p>Hmmm how is it possible that this is equivalent? I can\u2019t see where\u2019s the other swap that would cancel this one out.</p>", "body_text": "Hmmm how is it possible that this is equivalent? I can\u2019t see where\u2019s the other swap that would cancel this one out.", "in_reply_to_id": 232087787}