{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/78694042", "pull_request_review_id": null, "id": 78694042, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc4Njk0MDQy", "diff_hunk": "@@ -0,0 +1,135 @@\n+import torch\n+from torch.autograd.function import Function\n+from torch._thnn import type2backend\n+\n+\n+class CrossMapLRN2dFunction(Function):\n+\n+    def __init__(self, size, alpha=1e-4, beta=0.75, k=1):\n+        super(CrossMapLRN2dFunction, self).__init__()\n+        self.size = size\n+        self.alpha = alpha\n+        self.beta = beta\n+        self.k = k\n+        self.backend = None\n+        self.scale = None\n+\n+    def forward(self, input):\n+        assert input.dim() == 4\n+\n+        self.scale = self.scale or input.new()\n+        output = input.new()\n+\n+        backend = type2backend.get(type(input))\n+        if backend is not None:\n+            try:\n+                backend.CrossMapLRN2dFunction\n+                self.backend = backend\n+            except NotImplementedError:\n+                pass\n+\n+        if self.backend is not None:\n+            self.backend.SpatialCrossMapLRN_updateOutput(\n+                self._backend.library_state,\n+                input,\n+                output,\n+                self.scale,\n+                self.size,\n+                self.alpha,\n+                self.beta,\n+                self.k\n+            )\n+        else:\n+            batch_size   = input.size(0)\n+            channels    = input.size(1)\n+            input_height = input.size(2)\n+            input_width  = input.size(3)\n+\n+            output.resizeAs_(input)\n+            self.scale.resizeAs_(input)\n+\n+            # use output storage as temporary buffer\n+            input_square = output\n+            torch.pow(input_square, input, 2)\n+\n+            pre_pad = int((self.size - 1)/2 + 1)\n+            pre_pad_crop = channels if pre_pad > channels else pre_pad\n+\n+            scale_first = self.scale.select(1, 0)\n+            scale_first.zero_()\n+            # compute first feature map normalization\n+            for c in range(pre_pad_crop):\n+                scale_first.add_(input_square.select(1, c))\n+\n+            # reuse computations for next feature maps normalization\n+            # by adding the next feature map and removing the previous\n+            for c in range(1, channels):\n+                scale_previous = self.scale.select(1, c - 1)\n+                scale_current  = self.scale.select(1, c)\n+                scale_current.copy_(scale_previous)\n+                if c < channels - pre_pad + 1:\n+                    square_next = input_square.select(1, c + pre_pad - 1)\n+                    scale_current.add_(1, square_next)\n+\n+                if c > pre_pad:\n+                    square_previous = input_square.select(1, c - pre_pad)\n+                    scale_current.add_(-1, square_previous)\n+\n+            self.scale.mul_(self.alpha / self.size).add_(self.k)\n+\n+            torch.pow(output, self.scale, -self.beta)\n+            output.mul_(input)\n+\n+        # TODO: can scale be freed or is it needed in bw?", "path": "torch/nn/functions/normalization.py", "position": null, "original_position": 83, "commit_id": "c05d33986a99a271b8c95c8100760313b9df9cfd", "original_commit_id": "0333a85e3dc04d5c97cdee9c55a5aec370e0c26c", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "`scale` is needed for the backward pass for efficiency.\n", "created_at": "2016-09-14T06:49:17Z", "updated_at": "2018-11-23T15:31:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/24#discussion_r78694042", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/24", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/78694042"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/24#discussion_r78694042"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/24"}}, "body_html": "<p><code>scale</code> is needed for the backward pass for efficiency.</p>", "body_text": "scale is needed for the backward pass for efficiency."}