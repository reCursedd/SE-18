{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3941", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3941/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3941/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3941/events", "html_url": "https://github.com/pytorch/pytorch/issues/3941", "id": 277902320, "node_id": "MDU6SXNzdWUyNzc5MDIzMjA=", "number": 3941, "title": "WeightNorm CUDNN_STATUS_INTERNAL_ERROR", "user": {"login": "jtatusko", "id": 4348839, "node_id": "MDQ6VXNlcjQzNDg4Mzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4348839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jtatusko", "html_url": "https://github.com/jtatusko", "followers_url": "https://api.github.com/users/jtatusko/followers", "following_url": "https://api.github.com/users/jtatusko/following{/other_user}", "gists_url": "https://api.github.com/users/jtatusko/gists{/gist_id}", "starred_url": "https://api.github.com/users/jtatusko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jtatusko/subscriptions", "organizations_url": "https://api.github.com/users/jtatusko/orgs", "repos_url": "https://api.github.com/users/jtatusko/repos", "events_url": "https://api.github.com/users/jtatusko/events{/privacy}", "received_events_url": "https://api.github.com/users/jtatusko/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-29T20:51:36Z", "updated_at": "2018-04-05T11:55:55Z", "closed_at": "2017-11-29T22:02:14Z", "author_association": "NONE", "body_html": "<p>Fails with CUDNN, works without. Tested on 0.2 and master</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.nn.modules.conv <span class=\"pl-k\">import</span> Conv2d\n<span class=\"pl-k\">from</span> torch.nn.utils <span class=\"pl-k\">import</span> weight_norm\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv <span class=\"pl-k\">=</span> weight_norm(Conv2d(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.conv(x)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    use_cuda <span class=\"pl-k\">=</span> torch.cuda.is_available()\n    FloatTensor <span class=\"pl-k\">=</span> torch.cuda.FloatTensor <span class=\"pl-k\">if</span> use_cuda <span class=\"pl-k\">else</span> torch.FloatTensor\n\n    net <span class=\"pl-k\">=</span> Net()\n    <span class=\"pl-k\">if</span> use_cuda:\n        net.cuda()\n\n    a <span class=\"pl-k\">=</span> Variable(FloatTensor(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>))\n    <span class=\"pl-c1\">print</span>(net(a))</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"bleh.py\", line 26, in &lt;module&gt;\n    print(net(a))\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"bleh.py\", line 14, in forward\n    return self.conv(x)\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 282, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\", line 90, in conv2d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_INTERNAL_ERROR\n</code></pre>", "body_text": "Fails with CUDNN, works without. Tested on 0.2 and master\nimport torch\nfrom torch import nn\nfrom torch.nn.modules.conv import Conv2d\nfrom torch.nn.utils import weight_norm\nfrom torch.autograd import Variable\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = weight_norm(Conv2d(2, 2, 3, padding=1))\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nif __name__ == '__main__':\n    use_cuda = torch.cuda.is_available()\n    FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n\n    net = Net()\n    if use_cuda:\n        net.cuda()\n\n    a = Variable(FloatTensor(1, 2, 5, 5))\n    print(net(a))\nTraceback (most recent call last):\n  File \"bleh.py\", line 26, in <module>\n    print(net(a))\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"bleh.py\", line 14, in forward\n    return self.conv(x)\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 282, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\", line 90, in conv2d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_INTERNAL_ERROR", "body": "Fails with CUDNN, works without. Tested on 0.2 and master\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn.modules.conv import Conv2d\r\nfrom torch.nn.utils import weight_norm\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = weight_norm(Conv2d(2, 2, 3, padding=1))\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\n\r\nif __name__ == '__main__':\r\n    use_cuda = torch.cuda.is_available()\r\n    FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\r\n\r\n    net = Net()\r\n    if use_cuda:\r\n        net.cuda()\r\n\r\n    a = Variable(FloatTensor(1, 2, 5, 5))\r\n    print(net(a))\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bleh.py\", line 26, in <module>\r\n    print(net(a))\r\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"bleh.py\", line 14, in forward\r\n    return self.conv(x)\r\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 282, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"/home/heron/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\", line 90, in conv2d\r\n    return f(input, weight, bias)\r\nRuntimeError: CUDNN_STATUS_INTERNAL_ERROR\r\n```"}