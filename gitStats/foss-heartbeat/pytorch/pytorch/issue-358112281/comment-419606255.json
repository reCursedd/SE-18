{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419606255", "html_url": "https://github.com/pytorch/pytorch/pull/11379#issuecomment-419606255", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11379", "id": 419606255, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTYwNjI1NQ==", "user": {"login": "peterjc123", "id": 9998726, "node_id": "MDQ6VXNlcjk5OTg3MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9998726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterjc123", "html_url": "https://github.com/peterjc123", "followers_url": "https://api.github.com/users/peterjc123/followers", "following_url": "https://api.github.com/users/peterjc123/following{/other_user}", "gists_url": "https://api.github.com/users/peterjc123/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterjc123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterjc123/subscriptions", "organizations_url": "https://api.github.com/users/peterjc123/orgs", "repos_url": "https://api.github.com/users/peterjc123/repos", "events_url": "https://api.github.com/users/peterjc123/events{/privacy}", "received_events_url": "https://api.github.com/users/peterjc123/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-08T02:28:08Z", "updated_at": "2018-09-08T02:28:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The reason outlaid by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25254\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tj\">@tj</a> XU from the conversion:</p>\n<blockquote>\n<p>One observation is that in Pytorch's ci script it's forcing TORCH_CUDA_ARCH_LIST=5.2, but as soon as we use an arch&gt;=5.3, the native f16 support functions in Eigen/src/Core/arch/CUDA/Half.h would cause the failure David was seeing.<br>\none thing we can do is to patch Half.h to see if CUDA version is &gt;=9, then use this signature:  return __hadd(::__half(a), ::__half(b));<br>\ninstead of   return __hadd(a, b);</p>\n</blockquote>\n<p>So we need to patch that submodule to make it work, just like what we do <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/THCHalfAutoNumerics.cuh#L199\">here</a>.</p>", "body_text": "The reason outlaid by @tj XU from the conversion:\n\nOne observation is that in Pytorch's ci script it's forcing TORCH_CUDA_ARCH_LIST=5.2, but as soon as we use an arch>=5.3, the native f16 support functions in Eigen/src/Core/arch/CUDA/Half.h would cause the failure David was seeing.\none thing we can do is to patch Half.h to see if CUDA version is >=9, then use this signature:  return __hadd(::__half(a), ::__half(b));\ninstead of   return __hadd(a, b);\n\nSo we need to patch that submodule to make it work, just like what we do here.", "body": "The reason outlaid by @TJ XU from the conversion:\r\n> One observation is that in Pytorch's ci script it's forcing TORCH_CUDA_ARCH_LIST=5.2, but as soon as we use an arch>=5.3, the native f16 support functions in Eigen/src/Core/arch/CUDA/Half.h would cause the failure David was seeing.\r\none thing we can do is to patch Half.h to see if CUDA version is >=9, then use this signature:  return __hadd(::__half(a), ::__half(b));\r\n  instead of   return __hadd(a, b);\r\n\r\nSo we need to patch that submodule to make it work, just like what we do [here](https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/THCHalfAutoNumerics.cuh#L199)."}