{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2512", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2512/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2512/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2512/events", "html_url": "https://github.com/pytorch/pytorch/issues/2512", "id": 251970039, "node_id": "MDU6SXNzdWUyNTE5NzAwMzk=", "number": 2512, "title": "detach_() variant that affects all past uses too", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-22T14:08:41Z", "updated_at": "2017-08-23T19:02:57Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>Right now <code>detach_()</code> only modifies the current Variable, without modifying the graph in any way, which doesn't stop backprop for its previous uses. This behavior is sometimes wanted when e.g. doing BPTT and reusing some of the steps (say you do BPTT over 20 steps with stride of 2 steps).</p>\n<p>Current behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> a <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\nc <span class=\"pl-k\">=</span> b <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\nb.detach_()\nc.sum().backward()\n<span class=\"pl-k\">assert</span> a.grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span></pre></div>\n<p>Proposed fix:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> a <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\nc <span class=\"pl-k\">=</span> b <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\nb.detach_(<span class=\"pl-v\">with_uses</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nc.sum().backward()\n<span class=\"pl-k\">assert</span> a.grad <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span></pre></div>\n<p>Implementation is not straightforward at this point and some parts still need to be worked out.</p>", "body_text": "Right now detach_() only modifies the current Variable, without modifying the graph in any way, which doesn't stop backprop for its previous uses. This behavior is sometimes wanted when e.g. doing BPTT and reusing some of the steps (say you do BPTT over 20 steps with stride of 2 steps).\nCurrent behavior:\na = Variable(torch.randn(2, 2), requires_grad=True)\nb = a * 2\nc = b * 2\nb.detach_()\nc.sum().backward()\nassert a.grad is not None\nProposed fix:\na = Variable(torch.randn(2, 2), requires_grad=True)\nb = a * 2\nc = b * 2\nb.detach_(with_uses=True)\nc.sum().backward()\nassert a.grad is None\nImplementation is not straightforward at this point and some parts still need to be worked out.", "body": "Right now `detach_()` only modifies the current Variable, without modifying the graph in any way, which doesn't stop backprop for its previous uses. This behavior is sometimes wanted when e.g. doing BPTT and reusing some of the steps (say you do BPTT over 20 steps with stride of 2 steps).\r\n\r\nCurrent behavior:\r\n```python\r\na = Variable(torch.randn(2, 2), requires_grad=True)\r\nb = a * 2\r\nc = b * 2\r\nb.detach_()\r\nc.sum().backward()\r\nassert a.grad is not None\r\n```\r\n\r\nProposed fix:\r\n```python\r\na = Variable(torch.randn(2, 2), requires_grad=True)\r\nb = a * 2\r\nc = b * 2\r\nb.detach_(with_uses=True)\r\nc.sum().backward()\r\nassert a.grad is None\r\n```\r\n\r\nImplementation is not straightforward at this point and some parts still need to be worked out."}