{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3625", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3625/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3625/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3625/events", "html_url": "https://github.com/pytorch/pytorch/issues/3625", "id": 272993984, "node_id": "MDU6SXNzdWUyNzI5OTM5ODQ=", "number": 3625, "title": "Proposal: combine requires_grad and retain_grad()", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-11-10T16:48:51Z", "updated_at": "2018-09-18T17:09:38Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>Currently, requires_grad means two things:</p>\n<ol>\n<li>That we should compute gradients for this variable and functions of this variable</li>\n<li>On a \"leaf\" variable, it means we should store the gradient to the \"grad\" attribute</li>\n</ol>\n<p>The <code>retain_grad()</code> functions is used to signify that we should store the gradient on non-\"leaf\" variables to the \"grad\" attribute.</p>\n<p>We should change <code>requires_grad</code> so that it signifies that we should store the \"grad\" attribute on all variables (leaf and non-leaf). We should add a new read-only attribute, <code>compute_grad</code>, which serves the first purpose (whether or not we should compute gradients).</p>\n<p><code>retain_grad()</code> can be deprecated and simply set requires_grad=True for backwards compatibility.</p>", "body_text": "Currently, requires_grad means two things:\n\nThat we should compute gradients for this variable and functions of this variable\nOn a \"leaf\" variable, it means we should store the gradient to the \"grad\" attribute\n\nThe retain_grad() functions is used to signify that we should store the gradient on non-\"leaf\" variables to the \"grad\" attribute.\nWe should change requires_grad so that it signifies that we should store the \"grad\" attribute on all variables (leaf and non-leaf). We should add a new read-only attribute, compute_grad, which serves the first purpose (whether or not we should compute gradients).\nretain_grad() can be deprecated and simply set requires_grad=True for backwards compatibility.", "body": "Currently, requires_grad means two things: \r\n1) That we should compute gradients for this variable and functions of this variable\r\n2) On a \"leaf\" variable, it means we should store the gradient to the \"grad\" attribute\r\n\r\nThe `retain_grad()` functions is used to signify that we should store the gradient on non-\"leaf\" variables to the \"grad\" attribute.\r\n\r\nWe should change `requires_grad` so that it signifies that we should store the \"grad\" attribute on all variables (leaf and non-leaf). We should add a new read-only attribute, `compute_grad`, which serves the first purpose (whether or not we should compute gradients).\r\n\r\n `retain_grad()` can be deprecated and simply set requires_grad=True for backwards compatibility."}