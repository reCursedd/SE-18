{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/421410107", "html_url": "https://github.com/pytorch/pytorch/issues/3625#issuecomment-421410107", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3625", "id": 421410107, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTQxMDEwNw==", "user": {"login": "saluto", "id": 22053721, "node_id": "MDQ6VXNlcjIyMDUzNzIx", "avatar_url": "https://avatars3.githubusercontent.com/u/22053721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saluto", "html_url": "https://github.com/saluto", "followers_url": "https://api.github.com/users/saluto/followers", "following_url": "https://api.github.com/users/saluto/following{/other_user}", "gists_url": "https://api.github.com/users/saluto/gists{/gist_id}", "starred_url": "https://api.github.com/users/saluto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saluto/subscriptions", "organizations_url": "https://api.github.com/users/saluto/orgs", "repos_url": "https://api.github.com/users/saluto/repos", "events_url": "https://api.github.com/users/saluto/events{/privacy}", "received_events_url": "https://api.github.com/users/saluto/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-14T16:19:34Z", "updated_at": "2018-09-16T09:30:27Z", "author_association": "NONE", "body_html": "<p>I see problems with the current behaviour, too:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> --- example 1:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If `input.requires_grad = False`, `output.requires_grad_(True)` causes retaining `output.grad`. - Is this expected or not?</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If `input.requires_grad = True`, `output.requires_grad_(True)` does NOT cause retaining `output.grad`. - Is this expected or not?</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> At least the two different effects of `output.requires_grad_(True)` based on `input.requires_grad` is confusing and misleading.</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.tensor(<span class=\"pl-c1\">5</span>., <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\noutput <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">input</span>\noutput.requires_grad_(<span class=\"pl-c1\">True</span>)\noutput.backward()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>.grad, output.grad)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> --- example 2:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If `input.requires_grad = True`, `output.retain_grad()` causes retaining `output.grad`. - As expected.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If `input.requires_grad = False`, `output.retain_grad()` does NOT cause `output.requires_grad = True` (would require additional `output.requires_grad_(True)`) and therefore CAN NOT retain `output.grad`, leading to an exception. - Is this expected or not?</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> At least the two different effects of `output.retain_grad()` based on `input.requires_grad` is confusing and misleading.</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.tensor(<span class=\"pl-c1\">5</span>., <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\noutput <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">input</span>\noutput.retain_grad()\noutput.backward()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>.grad, output.grad)</pre></div>\n<p>I expected, that <code>output.requires_grad_(True)</code> and <code>output.retain_grad()</code> have an effect on <code>output.grad</code> that is independent of <code>input.requires_grad</code>. That this is not the case seems really bad to me.</p>\n<p>I suggest the following:</p>\n<ol>\n<li>Remove any ability to change <code>requires_grad</code> directly by user (only indirect, see (2.)).<br>\n(It should be just a read-only flag, to allow passing the need of <code>grad_fn</code> to child tensors, independent of whether the gradient actually should be retained in <code>.grad</code>. For retaining, see (2.).)<br>\nIn tensor factories like <code>torch.tensor()</code>, rename the argument <code>requires_grad</code> to <code>retains_grad</code>, causing both <code>requires_grad</code> and <code>retains_grad</code> to be set to <code>True</code> (same effect as calling <code>retain_grad_(True)</code>, see (2.)).</li>\n<li>Because we removed <code>requires_grad_(True/False)</code> (see (1.)), let us define <code>retain_grad(retain_grad=True)</code> (and <code>retain_grad_(retain_grad=True)</code> as inplace variant) to allow setting <code>retains_grad = True/False</code>.<br>\nOn leaves, this should also set <code>requires_grad = retains_grad</code>.<br>\nOn non-leaves, it should set <code>requires_grad = retains_grad if not any(input.requires_grad == True for input in self.inputs) else True</code>. (pseudo code)</li>\n</ol>\n<p>I really understand the need for backward compatibility. So let us first make (1.) just a deprecation. I think (2.) is fine. For me that even is the expected behaviour.</p>\n<p>For me this seems as quite a fundamental source of confusion and bugs. When to fix this, if not now? Maybe even with version 1.0, considering the introduction of this <a href=\"https://pytorch.org/2018/05/02/road-to-1.0.html\" rel=\"nofollow\">post</a> about targeting a stable API? Better early than late.</p>\n<p>Thanks for reading. And for helping to develop this incredible piece of my life.</p>", "body_text": "I see problems with the current behaviour, too:\nimport torch\n\n# --- example 1:\n# If `input.requires_grad = False`, `output.requires_grad_(True)` causes retaining `output.grad`. - Is this expected or not?\n# If `input.requires_grad = True`, `output.requires_grad_(True)` does NOT cause retaining `output.grad`. - Is this expected or not?\n# At least the two different effects of `output.requires_grad_(True)` based on `input.requires_grad` is confusing and misleading.\ninput = torch.tensor(5., requires_grad=True)\noutput = 2 * input\noutput.requires_grad_(True)\noutput.backward()\nprint(input.grad, output.grad)\n\n# --- example 2:\n# If `input.requires_grad = True`, `output.retain_grad()` causes retaining `output.grad`. - As expected.\n# If `input.requires_grad = False`, `output.retain_grad()` does NOT cause `output.requires_grad = True` (would require additional `output.requires_grad_(True)`) and therefore CAN NOT retain `output.grad`, leading to an exception. - Is this expected or not?\n# At least the two different effects of `output.retain_grad()` based on `input.requires_grad` is confusing and misleading.\ninput = torch.tensor(5., requires_grad=False)\noutput = 2 * input\noutput.retain_grad()\noutput.backward()\nprint(input.grad, output.grad)\nI expected, that output.requires_grad_(True) and output.retain_grad() have an effect on output.grad that is independent of input.requires_grad. That this is not the case seems really bad to me.\nI suggest the following:\n\nRemove any ability to change requires_grad directly by user (only indirect, see (2.)).\n(It should be just a read-only flag, to allow passing the need of grad_fn to child tensors, independent of whether the gradient actually should be retained in .grad. For retaining, see (2.).)\nIn tensor factories like torch.tensor(), rename the argument requires_grad to retains_grad, causing both requires_grad and retains_grad to be set to True (same effect as calling retain_grad_(True), see (2.)).\nBecause we removed requires_grad_(True/False) (see (1.)), let us define retain_grad(retain_grad=True) (and retain_grad_(retain_grad=True) as inplace variant) to allow setting retains_grad = True/False.\nOn leaves, this should also set requires_grad = retains_grad.\nOn non-leaves, it should set requires_grad = retains_grad if not any(input.requires_grad == True for input in self.inputs) else True. (pseudo code)\n\nI really understand the need for backward compatibility. So let us first make (1.) just a deprecation. I think (2.) is fine. For me that even is the expected behaviour.\nFor me this seems as quite a fundamental source of confusion and bugs. When to fix this, if not now? Maybe even with version 1.0, considering the introduction of this post about targeting a stable API? Better early than late.\nThanks for reading. And for helping to develop this incredible piece of my life.", "body": "I see problems with the current behaviour, too:\r\n\r\n```python\r\nimport torch\r\n\r\n# --- example 1:\r\n# If `input.requires_grad = False`, `output.requires_grad_(True)` causes retaining `output.grad`. - Is this expected or not?\r\n# If `input.requires_grad = True`, `output.requires_grad_(True)` does NOT cause retaining `output.grad`. - Is this expected or not?\r\n# At least the two different effects of `output.requires_grad_(True)` based on `input.requires_grad` is confusing and misleading.\r\ninput = torch.tensor(5., requires_grad=True)\r\noutput = 2 * input\r\noutput.requires_grad_(True)\r\noutput.backward()\r\nprint(input.grad, output.grad)\r\n\r\n# --- example 2:\r\n# If `input.requires_grad = True`, `output.retain_grad()` causes retaining `output.grad`. - As expected.\r\n# If `input.requires_grad = False`, `output.retain_grad()` does NOT cause `output.requires_grad = True` (would require additional `output.requires_grad_(True)`) and therefore CAN NOT retain `output.grad`, leading to an exception. - Is this expected or not?\r\n# At least the two different effects of `output.retain_grad()` based on `input.requires_grad` is confusing and misleading.\r\ninput = torch.tensor(5., requires_grad=False)\r\noutput = 2 * input\r\noutput.retain_grad()\r\noutput.backward()\r\nprint(input.grad, output.grad)\r\n```\r\n\r\nI expected, that `output.requires_grad_(True)` and `output.retain_grad()` have an effect on `output.grad` that is independent of `input.requires_grad`. That this is not the case seems really bad to me.\r\n\r\nI suggest the following:\r\n\r\n1. Remove any ability to change `requires_grad` directly by user (only indirect, see (2.)).\r\n(It should be just a read-only flag, to allow passing the need of `grad_fn` to child tensors, independent of whether the gradient actually should be retained in `.grad`. For retaining, see (2.).)\r\nIn tensor factories like `torch.tensor()`, rename the argument `requires_grad` to `retains_grad`, causing both `requires_grad` and `retains_grad` to be set to `True` (same effect as calling `retain_grad_(True)`, see (2.)).\r\n2. Because we removed `requires_grad_(True/False)` (see (1.)), let us define `retain_grad(retain_grad=True)` (and `retain_grad_(retain_grad=True)` as inplace variant) to allow setting `retains_grad = True/False`.\r\nOn leaves, this should also set `requires_grad = retains_grad`.\r\nOn non-leaves, it should set `requires_grad = retains_grad if not any(input.requires_grad == True for input in self.inputs) else True`. (pseudo code)\r\n\r\nI really understand the need for backward compatibility. So let us first make (1.) just a deprecation. I think (2.) is fine. For me that even is the expected behaviour.\r\n\r\nFor me this seems as quite a fundamental source of confusion and bugs. When to fix this, if not now? Maybe even with version 1.0, considering the introduction of this [post](https://pytorch.org/2018/05/02/road-to-1.0.html) about targeting a stable API? Better early than late.\r\n\r\nThanks for reading. And for helping to develop this incredible piece of my life."}