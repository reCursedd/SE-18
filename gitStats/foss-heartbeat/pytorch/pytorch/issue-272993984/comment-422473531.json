{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422473531", "html_url": "https://github.com/pytorch/pytorch/issues/3625#issuecomment-422473531", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3625", "id": 422473531, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjQ3MzUzMQ==", "user": {"login": "saluto", "id": 22053721, "node_id": "MDQ6VXNlcjIyMDUzNzIx", "avatar_url": "https://avatars3.githubusercontent.com/u/22053721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saluto", "html_url": "https://github.com/saluto", "followers_url": "https://api.github.com/users/saluto/followers", "following_url": "https://api.github.com/users/saluto/following{/other_user}", "gists_url": "https://api.github.com/users/saluto/gists{/gist_id}", "starred_url": "https://api.github.com/users/saluto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saluto/subscriptions", "organizations_url": "https://api.github.com/users/saluto/orgs", "repos_url": "https://api.github.com/users/saluto/repos", "events_url": "https://api.github.com/users/saluto/events{/privacy}", "received_events_url": "https://api.github.com/users/saluto/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T17:09:38Z", "updated_at": "2018-09-18T17:09:38Z", "author_association": "NONE", "body_html": "<p>The \"problem\"/bad API I see is that the effect of <code>requires_grad_()</code> and <code>retain_grad()</code> depends on whether an input <code>requires_grad</code> or not. So you cannot rely on either of these calls for non-leaves to just \"get\" the grad for it on <code>backward()</code>. You always have to check whether an input <code>requires_grad</code> or you have to call both <code>requires_grad_()</code> and <code>retain_grad()</code> on the non-leave just to be sure. There is no generic way to signal with one call that \"I want the grad for this tensor on every <code>backward()</code>\".</p>", "body_text": "The \"problem\"/bad API I see is that the effect of requires_grad_() and retain_grad() depends on whether an input requires_grad or not. So you cannot rely on either of these calls for non-leaves to just \"get\" the grad for it on backward(). You always have to check whether an input requires_grad or you have to call both requires_grad_() and retain_grad() on the non-leave just to be sure. There is no generic way to signal with one call that \"I want the grad for this tensor on every backward()\".", "body": "The \"problem\"/bad API I see is that the effect of `requires_grad_()` and `retain_grad()` depends on whether an input `requires_grad` or not. So you cannot rely on either of these calls for non-leaves to just \"get\" the grad for it on `backward()`. You always have to check whether an input `requires_grad` or you have to call both `requires_grad_()` and `retain_grad()` on the non-leave just to be sure. There is no generic way to signal with one call that \"I want the grad for this tensor on every `backward()`\"."}