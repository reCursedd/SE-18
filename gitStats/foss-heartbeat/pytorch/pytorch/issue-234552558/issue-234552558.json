{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1754", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1754/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1754/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1754/events", "html_url": "https://github.com/pytorch/pytorch/issues/1754", "id": 234552558, "node_id": "MDU6SXNzdWUyMzQ1NTI1NTg=", "number": 1754, "title": "pycuda bug with autograd", "user": {"login": "mariogeiger", "id": 333780, "node_id": "MDQ6VXNlcjMzMzc4MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/333780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariogeiger", "html_url": "https://github.com/mariogeiger", "followers_url": "https://api.github.com/users/mariogeiger/followers", "following_url": "https://api.github.com/users/mariogeiger/following{/other_user}", "gists_url": "https://api.github.com/users/mariogeiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariogeiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariogeiger/subscriptions", "organizations_url": "https://api.github.com/users/mariogeiger/orgs", "repos_url": "https://api.github.com/users/mariogeiger/repos", "events_url": "https://api.github.com/users/mariogeiger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariogeiger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-08T14:46:26Z", "updated_at": "2017-06-08T22:05:32Z", "closed_at": "2017-06-08T22:05:32Z", "author_association": "NONE", "body_html": "<p>I created a minimal code to reproduce my error.<br>\nI create two operations (complex number square and cube) using pycuda.<br>\nThe two functions works properly when called directly</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">2</span>).cuda()\ncomplex_cube(x)</pre></div>\n<p>But if I implement my own <code>torch.autograd.Function</code> using my two functions, then it crashes when <code>backward</code> is called.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> inspired by https://gist.github.com/szagoruyko/89f83b6f5f4833d3c8adf81ee49f22a8</span>\n\n<span class=\"pl-k\">from</span> functools <span class=\"pl-k\">import</span> lru_cache\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">import</span> pycuda.autoinit\n<span class=\"pl-k\">import</span> pycuda.driver <span class=\"pl-k\">as</span> cuda\n<span class=\"pl-k\">from</span> pycuda.compiler <span class=\"pl-k\">import</span> SourceModule\n\n<span class=\"pl-c1\">CUDA_NUM_THREADS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1024</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">GET_BLOCKS</span>(<span class=\"pl-smi\">N</span>):\n    <span class=\"pl-k\">return</span> (N <span class=\"pl-k\">+</span> <span class=\"pl-c1\">CUDA_NUM_THREADS</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">//</span> <span class=\"pl-c1\">CUDA_NUM_THREADS</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Holder</span>(<span class=\"pl-e\">cuda</span>.<span class=\"pl-e\">PointerHolderBase</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">t</span>):\n        <span class=\"pl-c1\">super</span>(Holder, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-k\">assert</span> t.is_cuda\n        <span class=\"pl-c1\">self</span>.t <span class=\"pl-k\">=</span> t\n        <span class=\"pl-c1\">self</span>.gpudata <span class=\"pl-k\">=</span> t.data_ptr()\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_pointer</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.t.data_ptr()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">complex_square</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">assert</span> x.is_cuda\n    <span class=\"pl-k\">assert</span> x.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span>\n    batch_size <span class=\"pl-k\">=</span> x.size()[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n    x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> [batch, complex] (nbatch, 2)</span>\n    nbatch <span class=\"pl-k\">=</span> x.size(<span class=\"pl-c1\">0</span>)\n\n    cuda_kernel <span class=\"pl-k\">=</span> _setup_complex_square_cuda_kernel(nbatch)\n\n    output <span class=\"pl-k\">=</span> torch.cuda.FloatTensor(nbatch, <span class=\"pl-c1\">2</span>)\n    cuda_kernel(Holder(x), Holder(output),\n                <span class=\"pl-v\">block</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">CUDA_NUM_THREADS</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n                <span class=\"pl-v\">grid</span><span class=\"pl-k\">=</span>(GET_BLOCKS(nbatch), <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [batch, complex] (nbatch, 2)</span>\n\n    output <span class=\"pl-k\">=</span> output.view(<span class=\"pl-k\">*</span>batch_size, <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-k\">return</span> output\n\n<span class=\"pl-en\">@lru_cache</span>(<span class=\"pl-v\">maxsize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_setup_complex_square_cuda_kernel</span>(<span class=\"pl-smi\">nbatch</span>):\n    kernel <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">extern \"C\"</span>\n<span class=\"pl-s\">__global__ void main_(const float* in, float* out) {</span>\n<span class=\"pl-s\">    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index &lt; <span class=\"pl-c1\">{nbatch}</span>; index += blockDim.x * gridDim.x) {</span>\n<span class=\"pl-s\">        float in_re = in[index * 2 + 0];</span>\n<span class=\"pl-s\">        float in_im = in[index * 2 + 1];</span>\n<span class=\"pl-s\">        out[index * 2 + 0] = in_re * in_re - in_im * in_im;</span>\n<span class=\"pl-s\">        out[index * 2 + 1] = 2.0 * in_re * in_im;</span>\n<span class=\"pl-s\">    }</span>\n<span class=\"pl-s\">}</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>.replace(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{nbatch}</span><span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">str</span>(nbatch))\n    \n    <span class=\"pl-k\">return</span> SourceModule(kernel).get_function(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>main_<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">complex_cube</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">assert</span> x.is_cuda\n    <span class=\"pl-k\">assert</span> x.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span>\n    batch_size <span class=\"pl-k\">=</span> x.size()[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n    x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> [batch, complex] (nbatch, 2)</span>\n    nbatch <span class=\"pl-k\">=</span> x.size(<span class=\"pl-c1\">0</span>)\n\n    cuda_kernel <span class=\"pl-k\">=</span> _setup_complex_cube_cuda_kernel(nbatch)\n\n    output <span class=\"pl-k\">=</span> torch.cuda.FloatTensor(nbatch, <span class=\"pl-c1\">2</span>)\n    cuda_kernel(Holder(x), Holder(output),\n                <span class=\"pl-v\">block</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">CUDA_NUM_THREADS</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n                <span class=\"pl-v\">grid</span><span class=\"pl-k\">=</span>(GET_BLOCKS(nbatch), <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [batch, complex] (nbatch, 2)</span>\n\n    output <span class=\"pl-k\">=</span> output.view(<span class=\"pl-k\">*</span>batch_size, <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-k\">return</span> output\n\n<span class=\"pl-en\">@lru_cache</span>(<span class=\"pl-v\">maxsize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_setup_complex_cube_cuda_kernel</span>(<span class=\"pl-smi\">nbatch</span>):\n    kernel <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">extern \"C\"</span>\n<span class=\"pl-s\">__global__ void main_(const float* in, float* out) {</span>\n<span class=\"pl-s\">    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index &lt; <span class=\"pl-c1\">{nbatch}</span>; index += blockDim.x * gridDim.x) {</span>\n<span class=\"pl-s\">        float in_re = in[index * 2 + 0];</span>\n<span class=\"pl-s\">        float in_im = in[index * 2 + 1];</span>\n<span class=\"pl-s\">        out[index * 2 + 0] = in_re * (in_re * in_re - 3.0 * in_im * in_im);</span>\n<span class=\"pl-s\">        out[index * 2 + 1] = in_im * (3.0 * in_re * in_re - in_im * in_im);</span>\n<span class=\"pl-s\">    }</span>\n<span class=\"pl-s\">}</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>.replace(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{nbatch}</span><span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">str</span>(nbatch))\n\n    <span class=\"pl-k\">return</span> SourceModule(kernel).get_function(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>main_<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">FooBar</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(FooBar, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> complex_square(x)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-k\">return</span> complex_cube(grad_output)</pre></div>\n<p>The following code crashes</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nx <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">2</span>).cuda()\nx <span class=\"pl-k\">=</span> Variable(x, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nop <span class=\"pl-k\">=</span> FooBar()\ny <span class=\"pl-k\">=</span> op(x)\n\nz <span class=\"pl-k\">=</span> y.sum()\nz.backward()</pre></div>\n<p>Errors:</p>\n<pre><code>---------------------------------------------------------------------------\nLogicError                                Traceback (most recent call last)\n&lt;ipython-input-2-3272bbf3c18d&gt; in &lt;module&gt;()\n      8 \n      9 z = y.sum()\n---&gt; 10 z.backward()\n\n/home/mario/.local/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_variables)\n    144                     'or with gradient w.r.t. the variable')\n    145             gradient = self.data.new().resize_as_(self.data).fill_(1)\n--&gt; 146         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n    147 \n    148     def register_hook(self, hook):\n\n&lt;ipython-input-1-8ba3f2e35a97&gt; in backward(self, grad_output)\n    100 \n    101     def backward(self, grad_output):\n--&gt; 102         return complex_cube(grad_output)\n\n&lt;ipython-input-1-8ba3f2e35a97&gt; in complex_cube(x)\n     65     nbatch = x.size(0)\n     66 \n---&gt; 67     cuda_kernel = _setup_complex_cube_cuda_kernel(nbatch)\n     68 \n     69     output = torch.cuda.FloatTensor(nbatch, 2)\n\n&lt;ipython-input-1-8ba3f2e35a97&gt; in _setup_complex_cube_cuda_kernel(nbatch)\n     90 '''.replace(\"{nbatch}\", str(nbatch))\n     91 \n---&gt; 92     return SourceModule(kernel).get_function(\"main_\")\n     93 \n     94 class FooBar(torch.autograd.Function):\n\n/home/mario/.local/lib/python3.5/site-packages/pycuda/compiler.py in __init__(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\n    292 \n    293         from pycuda.driver import module_from_buffer\n--&gt; 294         self.module = module_from_buffer(cubin)\n    295 \n    296         self._bind_module()\n\nLogicError: explicit_context_dependent failed: invalid device context - no currently active context?\n</code></pre>\n<p>But the following one execute without complain:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nx <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">2</span>).cuda()\nx <span class=\"pl-k\">=</span> Variable(x, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nop <span class=\"pl-k\">=</span> FooBar()\ny <span class=\"pl-k\">=</span> op(x)\n\ncomplex_cube(y.data) <span class=\"pl-c\"><span class=\"pl-c\">#</span> this line is added</span>\n\nz <span class=\"pl-k\">=</span> y.sum()\nz.backward()</pre></div>", "body_text": "I created a minimal code to reproduce my error.\nI create two operations (complex number square and cube) using pycuda.\nThe two functions works properly when called directly\nx = torch.rand(10, 2).cuda()\ncomplex_cube(x)\nBut if I implement my own torch.autograd.Function using my two functions, then it crashes when backward is called.\n# inspired by https://gist.github.com/szagoruyko/89f83b6f5f4833d3c8adf81ee49f22a8\n\nfrom functools import lru_cache\nimport torch\n\nimport pycuda.autoinit\nimport pycuda.driver as cuda\nfrom pycuda.compiler import SourceModule\n\nCUDA_NUM_THREADS = 1024\n\ndef GET_BLOCKS(N):\n    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\n\nclass Holder(cuda.PointerHolderBase):\n    def __init__(self, t):\n        super(Holder, self).__init__()\n        assert t.is_cuda\n        self.t = t\n        self.gpudata = t.data_ptr()\n    def get_pointer(self):\n        return self.t.data_ptr()\n\ndef complex_square(x):\n    assert x.is_cuda\n    assert x.size(-1) == 2\n    batch_size = x.size()[:-1]\n\n    x = x.view(-1, 2) # [batch, complex] (nbatch, 2)\n    nbatch = x.size(0)\n\n    cuda_kernel = _setup_complex_square_cuda_kernel(nbatch)\n\n    output = torch.cuda.FloatTensor(nbatch, 2)\n    cuda_kernel(Holder(x), Holder(output),\n                block=(CUDA_NUM_THREADS, 1, 1),\n                grid=(GET_BLOCKS(nbatch), 1, 1))\n    # [batch, complex] (nbatch, 2)\n\n    output = output.view(*batch_size, 2)\n    return output\n\n@lru_cache(maxsize=32)\ndef _setup_complex_square_cuda_kernel(nbatch):\n    kernel = '''\nextern \"C\"\n__global__ void main_(const float* in, float* out) {\n    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < {nbatch}; index += blockDim.x * gridDim.x) {\n        float in_re = in[index * 2 + 0];\n        float in_im = in[index * 2 + 1];\n        out[index * 2 + 0] = in_re * in_re - in_im * in_im;\n        out[index * 2 + 1] = 2.0 * in_re * in_im;\n    }\n}\n'''.replace(\"{nbatch}\", str(nbatch))\n    \n    return SourceModule(kernel).get_function(\"main_\")\n\ndef complex_cube(x):\n    assert x.is_cuda\n    assert x.size(-1) == 2\n    batch_size = x.size()[:-1]\n\n    x = x.view(-1, 2) # [batch, complex] (nbatch, 2)\n    nbatch = x.size(0)\n\n    cuda_kernel = _setup_complex_cube_cuda_kernel(nbatch)\n\n    output = torch.cuda.FloatTensor(nbatch, 2)\n    cuda_kernel(Holder(x), Holder(output),\n                block=(CUDA_NUM_THREADS, 1, 1),\n                grid=(GET_BLOCKS(nbatch), 1, 1))\n    # [batch, complex] (nbatch, 2)\n\n    output = output.view(*batch_size, 2)\n    return output\n\n@lru_cache(maxsize=32)\ndef _setup_complex_cube_cuda_kernel(nbatch):\n    kernel = '''\nextern \"C\"\n__global__ void main_(const float* in, float* out) {\n    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < {nbatch}; index += blockDim.x * gridDim.x) {\n        float in_re = in[index * 2 + 0];\n        float in_im = in[index * 2 + 1];\n        out[index * 2 + 0] = in_re * (in_re * in_re - 3.0 * in_im * in_im);\n        out[index * 2 + 1] = in_im * (3.0 * in_re * in_re - in_im * in_im);\n    }\n}\n'''.replace(\"{nbatch}\", str(nbatch))\n\n    return SourceModule(kernel).get_function(\"main_\")\n\nclass FooBar(torch.autograd.Function):\n    def __init__(self):\n        super(FooBar, self).__init__()\n\n    def forward(self, x):\n        return complex_square(x)\n\n    def backward(self, grad_output):\n        return complex_cube(grad_output)\nThe following code crashes\nfrom torch.autograd import Variable\n\nx = torch.rand(100, 100, 2).cuda()\nx = Variable(x, requires_grad=True)\n\nop = FooBar()\ny = op(x)\n\nz = y.sum()\nz.backward()\nErrors:\n---------------------------------------------------------------------------\nLogicError                                Traceback (most recent call last)\n<ipython-input-2-3272bbf3c18d> in <module>()\n      8 \n      9 z = y.sum()\n---> 10 z.backward()\n\n/home/mario/.local/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_variables)\n    144                     'or with gradient w.r.t. the variable')\n    145             gradient = self.data.new().resize_as_(self.data).fill_(1)\n--> 146         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n    147 \n    148     def register_hook(self, hook):\n\n<ipython-input-1-8ba3f2e35a97> in backward(self, grad_output)\n    100 \n    101     def backward(self, grad_output):\n--> 102         return complex_cube(grad_output)\n\n<ipython-input-1-8ba3f2e35a97> in complex_cube(x)\n     65     nbatch = x.size(0)\n     66 \n---> 67     cuda_kernel = _setup_complex_cube_cuda_kernel(nbatch)\n     68 \n     69     output = torch.cuda.FloatTensor(nbatch, 2)\n\n<ipython-input-1-8ba3f2e35a97> in _setup_complex_cube_cuda_kernel(nbatch)\n     90 '''.replace(\"{nbatch}\", str(nbatch))\n     91 \n---> 92     return SourceModule(kernel).get_function(\"main_\")\n     93 \n     94 class FooBar(torch.autograd.Function):\n\n/home/mario/.local/lib/python3.5/site-packages/pycuda/compiler.py in __init__(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\n    292 \n    293         from pycuda.driver import module_from_buffer\n--> 294         self.module = module_from_buffer(cubin)\n    295 \n    296         self._bind_module()\n\nLogicError: explicit_context_dependent failed: invalid device context - no currently active context?\n\nBut the following one execute without complain:\nfrom torch.autograd import Variable\n\nx = torch.rand(100, 100, 2).cuda()\nx = Variable(x, requires_grad=True)\n\nop = FooBar()\ny = op(x)\n\ncomplex_cube(y.data) # this line is added\n\nz = y.sum()\nz.backward()", "body": "I created a minimal code to reproduce my error.\r\nI create two operations (complex number square and cube) using pycuda.\r\nThe two functions works properly when called directly\r\n```python\r\nx = torch.rand(10, 2).cuda()\r\ncomplex_cube(x)\r\n```\r\n\r\nBut if I implement my own `torch.autograd.Function` using my two functions, then it crashes when `backward` is called. \r\n\r\n```python\r\n# inspired by https://gist.github.com/szagoruyko/89f83b6f5f4833d3c8adf81ee49f22a8\r\n\r\nfrom functools import lru_cache\r\nimport torch\r\n\r\nimport pycuda.autoinit\r\nimport pycuda.driver as cuda\r\nfrom pycuda.compiler import SourceModule\r\n\r\nCUDA_NUM_THREADS = 1024\r\n\r\ndef GET_BLOCKS(N):\r\n    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\r\n\r\nclass Holder(cuda.PointerHolderBase):\r\n    def __init__(self, t):\r\n        super(Holder, self).__init__()\r\n        assert t.is_cuda\r\n        self.t = t\r\n        self.gpudata = t.data_ptr()\r\n    def get_pointer(self):\r\n        return self.t.data_ptr()\r\n\r\ndef complex_square(x):\r\n    assert x.is_cuda\r\n    assert x.size(-1) == 2\r\n    batch_size = x.size()[:-1]\r\n\r\n    x = x.view(-1, 2) # [batch, complex] (nbatch, 2)\r\n    nbatch = x.size(0)\r\n\r\n    cuda_kernel = _setup_complex_square_cuda_kernel(nbatch)\r\n\r\n    output = torch.cuda.FloatTensor(nbatch, 2)\r\n    cuda_kernel(Holder(x), Holder(output),\r\n                block=(CUDA_NUM_THREADS, 1, 1),\r\n                grid=(GET_BLOCKS(nbatch), 1, 1))\r\n    # [batch, complex] (nbatch, 2)\r\n\r\n    output = output.view(*batch_size, 2)\r\n    return output\r\n\r\n@lru_cache(maxsize=32)\r\ndef _setup_complex_square_cuda_kernel(nbatch):\r\n    kernel = '''\r\nextern \"C\"\r\n__global__ void main_(const float* in, float* out) {\r\n    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < {nbatch}; index += blockDim.x * gridDim.x) {\r\n        float in_re = in[index * 2 + 0];\r\n        float in_im = in[index * 2 + 1];\r\n        out[index * 2 + 0] = in_re * in_re - in_im * in_im;\r\n        out[index * 2 + 1] = 2.0 * in_re * in_im;\r\n    }\r\n}\r\n'''.replace(\"{nbatch}\", str(nbatch))\r\n    \r\n    return SourceModule(kernel).get_function(\"main_\")\r\n\r\ndef complex_cube(x):\r\n    assert x.is_cuda\r\n    assert x.size(-1) == 2\r\n    batch_size = x.size()[:-1]\r\n\r\n    x = x.view(-1, 2) # [batch, complex] (nbatch, 2)\r\n    nbatch = x.size(0)\r\n\r\n    cuda_kernel = _setup_complex_cube_cuda_kernel(nbatch)\r\n\r\n    output = torch.cuda.FloatTensor(nbatch, 2)\r\n    cuda_kernel(Holder(x), Holder(output),\r\n                block=(CUDA_NUM_THREADS, 1, 1),\r\n                grid=(GET_BLOCKS(nbatch), 1, 1))\r\n    # [batch, complex] (nbatch, 2)\r\n\r\n    output = output.view(*batch_size, 2)\r\n    return output\r\n\r\n@lru_cache(maxsize=32)\r\ndef _setup_complex_cube_cuda_kernel(nbatch):\r\n    kernel = '''\r\nextern \"C\"\r\n__global__ void main_(const float* in, float* out) {\r\n    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < {nbatch}; index += blockDim.x * gridDim.x) {\r\n        float in_re = in[index * 2 + 0];\r\n        float in_im = in[index * 2 + 1];\r\n        out[index * 2 + 0] = in_re * (in_re * in_re - 3.0 * in_im * in_im);\r\n        out[index * 2 + 1] = in_im * (3.0 * in_re * in_re - in_im * in_im);\r\n    }\r\n}\r\n'''.replace(\"{nbatch}\", str(nbatch))\r\n\r\n    return SourceModule(kernel).get_function(\"main_\")\r\n\r\nclass FooBar(torch.autograd.Function):\r\n    def __init__(self):\r\n        super(FooBar, self).__init__()\r\n\r\n    def forward(self, x):\r\n        return complex_square(x)\r\n\r\n    def backward(self, grad_output):\r\n        return complex_cube(grad_output)\r\n```\r\n\r\nThe following code crashes\r\n```python\r\nfrom torch.autograd import Variable\r\n\r\nx = torch.rand(100, 100, 2).cuda()\r\nx = Variable(x, requires_grad=True)\r\n\r\nop = FooBar()\r\ny = op(x)\r\n\r\nz = y.sum()\r\nz.backward()\r\n```\r\n\r\nErrors:\r\n```\r\n---------------------------------------------------------------------------\r\nLogicError                                Traceback (most recent call last)\r\n<ipython-input-2-3272bbf3c18d> in <module>()\r\n      8 \r\n      9 z = y.sum()\r\n---> 10 z.backward()\r\n\r\n/home/mario/.local/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_variables)\r\n    144                     'or with gradient w.r.t. the variable')\r\n    145             gradient = self.data.new().resize_as_(self.data).fill_(1)\r\n--> 146         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n    147 \r\n    148     def register_hook(self, hook):\r\n\r\n<ipython-input-1-8ba3f2e35a97> in backward(self, grad_output)\r\n    100 \r\n    101     def backward(self, grad_output):\r\n--> 102         return complex_cube(grad_output)\r\n\r\n<ipython-input-1-8ba3f2e35a97> in complex_cube(x)\r\n     65     nbatch = x.size(0)\r\n     66 \r\n---> 67     cuda_kernel = _setup_complex_cube_cuda_kernel(nbatch)\r\n     68 \r\n     69     output = torch.cuda.FloatTensor(nbatch, 2)\r\n\r\n<ipython-input-1-8ba3f2e35a97> in _setup_complex_cube_cuda_kernel(nbatch)\r\n     90 '''.replace(\"{nbatch}\", str(nbatch))\r\n     91 \r\n---> 92     return SourceModule(kernel).get_function(\"main_\")\r\n     93 \r\n     94 class FooBar(torch.autograd.Function):\r\n\r\n/home/mario/.local/lib/python3.5/site-packages/pycuda/compiler.py in __init__(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\r\n    292 \r\n    293         from pycuda.driver import module_from_buffer\r\n--> 294         self.module = module_from_buffer(cubin)\r\n    295 \r\n    296         self._bind_module()\r\n\r\nLogicError: explicit_context_dependent failed: invalid device context - no currently active context?\r\n```\r\n\r\nBut the following one execute without complain:\r\n```python\r\nfrom torch.autograd import Variable\r\n\r\nx = torch.rand(100, 100, 2).cuda()\r\nx = Variable(x, requires_grad=True)\r\n\r\nop = FooBar()\r\ny = op(x)\r\n\r\ncomplex_cube(y.data) # this line is added\r\n\r\nz = y.sum()\r\nz.backward()\r\n```"}