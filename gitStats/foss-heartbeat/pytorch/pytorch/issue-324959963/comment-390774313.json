{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390774313", "html_url": "https://github.com/pytorch/pytorch/issues/7731#issuecomment-390774313", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7731", "id": 390774313, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDc3NDMxMw==", "user": {"login": "calebh", "id": 542102, "node_id": "MDQ6VXNlcjU0MjEwMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/542102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/calebh", "html_url": "https://github.com/calebh", "followers_url": "https://api.github.com/users/calebh/followers", "following_url": "https://api.github.com/users/calebh/following{/other_user}", "gists_url": "https://api.github.com/users/calebh/gists{/gist_id}", "starred_url": "https://api.github.com/users/calebh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/calebh/subscriptions", "organizations_url": "https://api.github.com/users/calebh/orgs", "repos_url": "https://api.github.com/users/calebh/repos", "events_url": "https://api.github.com/users/calebh/events{/privacy}", "received_events_url": "https://api.github.com/users/calebh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-21T20:35:10Z", "updated_at": "2018-05-21T20:35:10Z", "author_association": "NONE", "body_html": "<p>If fixing this issue is impossible, then there should at least be a big warning in the documentation. I've spent a lot of time in the documentation and I haven't seen any warning about these operators. Another approach would be to disable these operators completely unless a global flag has been set which tells PyTorch \"it's okay, I know what I'm doing.\" This would allow power users to still get the performance they want without sacrificing safety for new people.</p>\n<p>This issue calls into question a lot of the work that I've done in the past couple months. I occasionally use the arithmetic assignment operators, and I was assuming that += was just syntax sugar for the + operation. I wouldn't want someone else to go through this same trouble.</p>", "body_text": "If fixing this issue is impossible, then there should at least be a big warning in the documentation. I've spent a lot of time in the documentation and I haven't seen any warning about these operators. Another approach would be to disable these operators completely unless a global flag has been set which tells PyTorch \"it's okay, I know what I'm doing.\" This would allow power users to still get the performance they want without sacrificing safety for new people.\nThis issue calls into question a lot of the work that I've done in the past couple months. I occasionally use the arithmetic assignment operators, and I was assuming that += was just syntax sugar for the + operation. I wouldn't want someone else to go through this same trouble.", "body": "If fixing this issue is impossible, then there should at least be a big warning in the documentation. I've spent a lot of time in the documentation and I haven't seen any warning about these operators. Another approach would be to disable these operators completely unless a global flag has been set which tells PyTorch \"it's okay, I know what I'm doing.\" This would allow power users to still get the performance they want without sacrificing safety for new people.\r\n\r\nThis issue calls into question a lot of the work that I've done in the past couple months. I occasionally use the arithmetic assignment operators, and I was assuming that += was just syntax sugar for the + operation. I wouldn't want someone else to go through this same trouble."}