{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390693977", "html_url": "https://github.com/pytorch/pytorch/issues/7731#issuecomment-390693977", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7731", "id": 390693977, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDY5Mzk3Nw==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-21T15:44:38Z", "updated_at": "2018-05-21T15:44:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the report, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=542102\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/calebh\">@calebh</a>. When tensors share the same storage, in-place operations are incorrect. You should generally avoid in-place operations because they can have other bad side effects, like making backward computation impossible (autograd will throw an error if that is the case though).</p>\n<p>Ideally we'd throw an error or warn if the tensors involved in an in-place operation have overlapping storage, but determining that fully is equivalent to finding a solution to a linear diophantine equation in <code>n</code> variables, where <code>n</code> is proportional to the number of dimensions in the input tensors.</p>", "body_text": "Thanks for the report, @calebh. When tensors share the same storage, in-place operations are incorrect. You should generally avoid in-place operations because they can have other bad side effects, like making backward computation impossible (autograd will throw an error if that is the case though).\nIdeally we'd throw an error or warn if the tensors involved in an in-place operation have overlapping storage, but determining that fully is equivalent to finding a solution to a linear diophantine equation in n variables, where n is proportional to the number of dimensions in the input tensors.", "body": "Thanks for the report, @calebh. When tensors share the same storage, in-place operations are incorrect. You should generally avoid in-place operations because they can have other bad side effects, like making backward computation impossible (autograd will throw an error if that is the case though).\r\n\r\nIdeally we'd throw an error or warn if the tensors involved in an in-place operation have overlapping storage, but determining that fully is equivalent to finding a solution to a linear diophantine equation in `n` variables, where `n` is proportional to the number of dimensions in the input tensors."}