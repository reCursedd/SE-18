{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4948", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4948/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4948/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4948/events", "html_url": "https://github.com/pytorch/pytorch/issues/4948", "id": 292947929, "node_id": "MDU6SXNzdWUyOTI5NDc5Mjk=", "number": 4948, "title": "Better engineering on perf testing scripts", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-01-30T21:42:17Z", "updated_at": "2018-02-02T21:57:12Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Our perf testing scripts (as seen in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"292924343\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4945\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4945/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4945\">#4945</a>) have a lot of hardcoded bits that let us get them up and running, but now there is some better engineering work to be done.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Record exact specifications for performance numbers, and specify the applicability of our numbers to them. As our performance tests are being done on Amazon EC 2 infrastructure, it is sufficient to identify what EC2 instance type (<a href=\"https://aws.amazon.com/ec2/instance-types/\" rel=\"nofollow\">https://aws.amazon.com/ec2/instance-types/</a>) is being run and record this alongside the test. This improves reproducibility as interested parties can spin up an equivalent EC2 instance and rerun the numbers if they want to.\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> A nice subtask is to document the process for getting an EC2 instance to reproduce results. Maybe Docker can help here too.</li>\n</ul>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Move <code>compare_with_baseline.py</code> out of line from the shell script</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Refactor <code>compare_with_baseline.py</code> to separate the recorded test data, and the z-value computation (deduplicating the z-value code between the cpu and gpu perf tests)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Make tests not rely on <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a> gists and GitHubs, so that they can be easily updated by other parties without having to synchronize on <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Performance numbers are supposed to be correlated with a particular commit, but in general the commit that something gets merged to master will be different from the commit you can actually conveniently put into the benchmark commit when you are upgrading baselines. Not sure what to do here; maybe we can link to Pull Requests.</li>\n</ul>", "body_text": "Our perf testing scripts (as seen in #4945) have a lot of hardcoded bits that let us get them up and running, but now there is some better engineering work to be done.\n\n Record exact specifications for performance numbers, and specify the applicability of our numbers to them. As our performance tests are being done on Amazon EC 2 infrastructure, it is sufficient to identify what EC2 instance type (https://aws.amazon.com/ec2/instance-types/) is being run and record this alongside the test. This improves reproducibility as interested parties can spin up an equivalent EC2 instance and rerun the numbers if they want to.\n\n A nice subtask is to document the process for getting an EC2 instance to reproduce results. Maybe Docker can help here too.\n\n\n Move compare_with_baseline.py out of line from the shell script\n Refactor compare_with_baseline.py to separate the recorded test data, and the z-value computation (deduplicating the z-value code between the cpu and gpu perf tests)\n Make tests not rely on @yf225 gists and GitHubs, so that they can be easily updated by other parties without having to synchronize on @yf225\n Performance numbers are supposed to be correlated with a particular commit, but in general the commit that something gets merged to master will be different from the commit you can actually conveniently put into the benchmark commit when you are upgrading baselines. Not sure what to do here; maybe we can link to Pull Requests.", "body": "Our perf testing scripts (as seen in https://github.com/pytorch/pytorch/pull/4945) have a lot of hardcoded bits that let us get them up and running, but now there is some better engineering work to be done.\r\n\r\n- [ ] Record exact specifications for performance numbers, and specify the applicability of our numbers to them. As our performance tests are being done on Amazon EC 2 infrastructure, it is sufficient to identify what EC2 instance type (https://aws.amazon.com/ec2/instance-types/) is being run and record this alongside the test. This improves reproducibility as interested parties can spin up an equivalent EC2 instance and rerun the numbers if they want to.\r\n  - [ ] A nice subtask is to document the process for getting an EC2 instance to reproduce results. Maybe Docker can help here too.\r\n- [ ] Move `compare_with_baseline.py` out of line from the shell script\r\n- [ ] Refactor `compare_with_baseline.py` to separate the recorded test data, and the z-value computation (deduplicating the z-value code between the cpu and gpu perf tests)\r\n- [ ] Make tests not rely on @yf225 gists and GitHubs, so that they can be easily updated by other parties without having to synchronize on @yf225 \r\n- [ ] Performance numbers are supposed to be correlated with a particular commit, but in general the commit that something gets merged to master will be different from the commit you can actually conveniently put into the benchmark commit when you are upgrading baselines. Not sure what to do here; maybe we can link to Pull Requests.\r\n"}