{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12863", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12863/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12863/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12863/events", "html_url": "https://github.com/pytorch/pytorch/issues/12863", "id": 371811198, "node_id": "MDU6SXNzdWUzNzE4MTExOTg=", "number": 12863, "title": "Memory leak when using forward hook and backward hook simultaneously", "user": {"login": "wwiiiii", "id": 7707859, "node_id": "MDQ6VXNlcjc3MDc4NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7707859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wwiiiii", "html_url": "https://github.com/wwiiiii", "followers_url": "https://api.github.com/users/wwiiiii/followers", "following_url": "https://api.github.com/users/wwiiiii/following{/other_user}", "gists_url": "https://api.github.com/users/wwiiiii/gists{/gist_id}", "starred_url": "https://api.github.com/users/wwiiiii/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wwiiiii/subscriptions", "organizations_url": "https://api.github.com/users/wwiiiii/orgs", "repos_url": "https://api.github.com/users/wwiiiii/repos", "events_url": "https://api.github.com/users/wwiiiii/events{/privacy}", "received_events_url": "https://api.github.com/users/wwiiiii/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-19T05:22:57Z", "updated_at": "2018-10-22T15:27:05Z", "closed_at": "2018-10-22T15:27:05Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>When I use simple forward hook and backward hook on resnet101 model from torchvision, memory leak occurs.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<ol>\n<li>Run the following code snippet.</li>\n</ol>\n\n<pre><code>import gc\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\n\n\ndef foo():\n    model = models.resnet101()\n    model.eval()\n    a, b = None, None\n    target_layer = model.layer4[-1].conv3\n    \n    def forward_hook(module, input, output):\n        nonlocal a\n        a = output.clone()\n\n    def backward_hook(module, grad_input, grad_output):\n        nonlocal b\n        b = grad_output[0].clone()\n    \n    target_layer.register_forward_hook(forward_hook)\n    target_layer.register_backward_hook(backward_hook)\n    image = torch.randn(1, 3, 224, 224)\n    model(image)\n    \n\ndef main():\n    for _ in range(10):\n        foo()\n        gc.collect()\n        cnt = 0\n        for obj in gc.get_objects():\n            try:\n                if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n                    cnt += 1\n            except:\n                pass\n        print('cnt', cnt)\n    \n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<h2>Expected behavior</h2>\n<p><code>cnt</code> should be zero all the time, as there's no any reference for variables defined inside the function <code>foo</code>.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 0.4.1</li>\n<li>OS (e.g., Linux): Microsoft Windows 10 Enterprise</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): conda</li>\n<li>Build command you used (if compiling from source):</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: No CUDA</li>\n<li>GPU models and configuration: No CUDA</li>\n<li>Any other relevant information:</li>\n</ul>\n<h2>Additional context</h2>\n<p>if I do any one of following options, the cnt becomes zero.</p>\n<ol>\n<li><code>del a, b</code> at the end of the function bar.</li>\n<li>comment out <code>register_forward_hook</code> (backward hook still working)</li>\n<li>comment out <code>register_backward_hook</code> (forward hook still working)</li>\n<li>comment out <code>nonlocal a</code> or <code>nonlocal b</code></li>\n<li>detach <code>output</code> or <code>grad_output</code> before clone it in hook function.</li>\n</ol>\n\n<p>Related forum topic: <a href=\"https://discuss.pytorch.org/t/memory-leak-when-using-forward-hook-and-backward-hook-simultaneously/27555\" rel=\"nofollow\">https://discuss.pytorch.org/t/memory-leak-when-using-forward-hook-and-backward-hook-simultaneously/27555</a></p>", "body_text": "\ud83d\udc1b Bug\n\nWhen I use simple forward hook and backward hook on resnet101 model from torchvision, memory leak occurs.\nTo Reproduce\nSteps to reproduce the behavior:\n\nRun the following code snippet.\n\n\nimport gc\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\n\n\ndef foo():\n    model = models.resnet101()\n    model.eval()\n    a, b = None, None\n    target_layer = model.layer4[-1].conv3\n    \n    def forward_hook(module, input, output):\n        nonlocal a\n        a = output.clone()\n\n    def backward_hook(module, grad_input, grad_output):\n        nonlocal b\n        b = grad_output[0].clone()\n    \n    target_layer.register_forward_hook(forward_hook)\n    target_layer.register_backward_hook(backward_hook)\n    image = torch.randn(1, 3, 224, 224)\n    model(image)\n    \n\ndef main():\n    for _ in range(10):\n        foo()\n        gc.collect()\n        cnt = 0\n        for obj in gc.get_objects():\n            try:\n                if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n                    cnt += 1\n            except:\n                pass\n        print('cnt', cnt)\n    \n\nif __name__ == '__main__':\n    main()\n\nExpected behavior\ncnt should be zero all the time, as there's no any reference for variables defined inside the function foo.\nEnvironment\n\nPyTorch Version (e.g., 1.0): 0.4.1\nOS (e.g., Linux): Microsoft Windows 10 Enterprise\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source):\nPython version: 3.6\nCUDA/cuDNN version: No CUDA\nGPU models and configuration: No CUDA\nAny other relevant information:\n\nAdditional context\nif I do any one of following options, the cnt becomes zero.\n\ndel a, b at the end of the function bar.\ncomment out register_forward_hook (backward hook still working)\ncomment out register_backward_hook (forward hook still working)\ncomment out nonlocal a or nonlocal b\ndetach output or grad_output before clone it in hook function.\n\n\nRelated forum topic: https://discuss.pytorch.org/t/memory-leak-when-using-forward-hook-and-backward-hook-simultaneously/27555", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen I use simple forward hook and backward hook on resnet101 model from torchvision, memory leak occurs.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following code snippet.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\nimport gc\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torchvision import models\r\n\r\n\r\ndef foo():\r\n    model = models.resnet101()\r\n    model.eval()\r\n    a, b = None, None\r\n    target_layer = model.layer4[-1].conv3\r\n    \r\n    def forward_hook(module, input, output):\r\n        nonlocal a\r\n        a = output.clone()\r\n\r\n    def backward_hook(module, grad_input, grad_output):\r\n        nonlocal b\r\n        b = grad_output[0].clone()\r\n    \r\n    target_layer.register_forward_hook(forward_hook)\r\n    target_layer.register_backward_hook(backward_hook)\r\n    image = torch.randn(1, 3, 224, 224)\r\n    model(image)\r\n    \r\n\r\ndef main():\r\n    for _ in range(10):\r\n        foo()\r\n        gc.collect()\r\n        cnt = 0\r\n        for obj in gc.get_objects():\r\n            try:\r\n                if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\r\n                    cnt += 1\r\n            except:\r\n                pass\r\n        print('cnt', cnt)\r\n    \r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```cnt``` should be zero all the time, as there's no any reference for variables defined inside the function ```foo```.\r\n\r\n## Environment\r\n - PyTorch Version (e.g., 1.0): 0.4.1\r\n - OS (e.g., Linux): Microsoft Windows 10 Enterprise\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: No CUDA\r\n - GPU models and configuration: No CUDA\r\n - Any other relevant information:\r\n\r\n## Additional context\r\nif I do any one of following options, the cnt becomes zero.\r\n1. ```del a, b``` at the end of the function bar.\r\n2. comment out ```register_forward_hook``` (backward hook still working)\r\n3. comment out ```register_backward_hook``` (forward hook still working)\r\n4. comment out ```nonlocal a``` or ```nonlocal b```\r\n5. detach ```output``` or ```grad_output``` before clone it in hook function.\r\n<!-- Add any other context about the problem here. -->\r\nRelated forum topic: https://discuss.pytorch.org/t/memory-leak-when-using-forward-hook-and-backward-hook-simultaneously/27555"}