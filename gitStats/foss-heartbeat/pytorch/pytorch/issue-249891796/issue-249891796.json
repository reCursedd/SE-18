{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2403", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2403/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2403/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2403/events", "html_url": "https://github.com/pytorch/pytorch/issues/2403", "id": 249891796, "node_id": "MDU6SXNzdWUyNDk4OTE3OTY=", "number": 2403, "title": "BCEWithLogitsLoss bug", "user": {"login": "justanothercoder", "id": 4433928, "node_id": "MDQ6VXNlcjQ0MzM5Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/4433928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justanothercoder", "html_url": "https://github.com/justanothercoder", "followers_url": "https://api.github.com/users/justanothercoder/followers", "following_url": "https://api.github.com/users/justanothercoder/following{/other_user}", "gists_url": "https://api.github.com/users/justanothercoder/gists{/gist_id}", "starred_url": "https://api.github.com/users/justanothercoder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justanothercoder/subscriptions", "organizations_url": "https://api.github.com/users/justanothercoder/orgs", "repos_url": "https://api.github.com/users/justanothercoder/repos", "events_url": "https://api.github.com/users/justanothercoder/events{/privacy}", "received_events_url": "https://api.github.com/users/justanothercoder/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-13T18:53:41Z", "updated_at": "2017-08-14T11:34:54Z", "closed_at": "2017-08-14T10:25:22Z", "author_association": "NONE", "body_html": "<p>Hello!<br>\nAfter I've upgraded PyTorch to 0.2.0, I've got a quite strange problem with <code>torch.nn.BCEWithLogitsLoss</code>.</p>\n<p>Loss is defined this way:<br>\n<code>self.loss_fn = nn.BCEWithLogitsLoss(size_average=True)</code></p>\n<p>When I call it I receive the following traceback:</p>\n<pre><code>  File \"/srv/hd1/data/vyanush/torch/stoch_logreg.py\", line 52, in forward\n    f = self.loss_fn(logits, y)\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/modules/loss.py\", line 333, in forward\n    return F.binary_cross_entropy_with_logits(input, target, size_average=self.size_average)\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/functional.py\", line 795, in binary_cross_entropy_with_logits\n    if not target.is_same_size(input):\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/autograd/variable.py\", line 309, in is_same_size\n    return self.data.is_same_size(other_var.data)\nTypeError: is_same_size received an invalid combination of arguments - got (torch.cuda.FloatTensor), but expected (torch.cuda.ByteTensor other)\n\n</code></pre>\n<p>That is a bug because <code>logits</code> and <code>y</code> shouldn't have the same type, but rather only the same size.</p>", "body_text": "Hello!\nAfter I've upgraded PyTorch to 0.2.0, I've got a quite strange problem with torch.nn.BCEWithLogitsLoss.\nLoss is defined this way:\nself.loss_fn = nn.BCEWithLogitsLoss(size_average=True)\nWhen I call it I receive the following traceback:\n  File \"/srv/hd1/data/vyanush/torch/stoch_logreg.py\", line 52, in forward\n    f = self.loss_fn(logits, y)\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/modules/loss.py\", line 333, in forward\n    return F.binary_cross_entropy_with_logits(input, target, size_average=self.size_average)\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/functional.py\", line 795, in binary_cross_entropy_with_logits\n    if not target.is_same_size(input):\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/autograd/variable.py\", line 309, in is_same_size\n    return self.data.is_same_size(other_var.data)\nTypeError: is_same_size received an invalid combination of arguments - got (torch.cuda.FloatTensor), but expected (torch.cuda.ByteTensor other)\n\n\nThat is a bug because logits and y shouldn't have the same type, but rather only the same size.", "body": "Hello! \r\nAfter I've upgraded PyTorch to 0.2.0, I've got a quite strange problem with `torch.nn.BCEWithLogitsLoss`. \r\n\r\nLoss is defined this way:\r\n`self.loss_fn = nn.BCEWithLogitsLoss(size_average=True)`\r\n\r\nWhen I call it I receive the following traceback:\r\n```\r\n  File \"/srv/hd1/data/vyanush/torch/stoch_logreg.py\", line 52, in forward\r\n    f = self.loss_fn(logits, y)\r\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/modules/loss.py\", line 333, in forward\r\n    return F.binary_cross_entropy_with_logits(input, target, size_average=self.size_average)\r\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/nn/functional.py\", line 795, in binary_cross_entropy_with_logits\r\n    if not target.is_same_size(input):\r\n  File \"/srv/hd1/data/vyanush/new-anaconda3/envs/torchenv/lib/python3.5/site-packages/torch/autograd/variable.py\", line 309, in is_same_size\r\n    return self.data.is_same_size(other_var.data)\r\nTypeError: is_same_size received an invalid combination of arguments - got (torch.cuda.FloatTensor), but expected (torch.cuda.ByteTensor other)\r\n\r\n```\r\nThat is a bug because `logits` and `y` shouldn't have the same type, but rather only the same size."}