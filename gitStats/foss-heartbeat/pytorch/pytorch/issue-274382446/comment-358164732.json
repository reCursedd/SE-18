{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358164732", "html_url": "https://github.com/pytorch/pytorch/issues/3732#issuecomment-358164732", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3732", "id": 358164732, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODE2NDczMg==", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-17T01:29:32Z", "updated_at": "2018-01-17T01:50:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm getting this issue as well. Its been driving me crazy all day because I'm on a shared computer with 4 GPUs. I need to control which GPU's I'm on, so I'm using torch.device to force my programs to run on GPU2. However, whenever I run my DataLoader I end up allocating 153MB on GPU0. It gets worse when I use multiprocessing because the overhead is multiplied by the number of processes.</p>\n<p>After a long day of looking at this I think I can conclude that its an opencv bug/issue.</p>\n<p>Consider the following MWE:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-k\">import</span> cv2\n    <span class=\"pl-k\">import</span> subprocess\n    <span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>img = (np.random.rand(32, 32, 3) * 255).astype(np.uint8)</span>\n    img <span class=\"pl-k\">=</span> cv2.cvtColor(img, cv2.<span class=\"pl-c1\">COLOR_BGR2RGB</span>)\n    subprocess.call(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nvidia-smi<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>Calling this script produces the following result:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  TITAN Xp            Off  | 00000000:05:00.0  On |                  N/A |\n| 27%   45C    P8    19W / 250W |     48MiB / 12188MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\n| 25%   44C    P8    18W / 250W |    434MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\n| 35%   58C    P2    63W / 250W |   2396MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\n| 23%   33C    P8     9W / 250W |     11MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1675      G   /usr/lib/xorg/Xorg                            36MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>There are no GPUs being used.</p>\n<p>However, if I un-comment the call to opencv</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">import</span> cv2\n    <span class=\"pl-k\">import</span> subprocess\n    <span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n    img <span class=\"pl-k\">=</span> (np.random.rand(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">255</span>).astype(np.uint8)\n    img <span class=\"pl-k\">=</span> cv2.cvtColor(img, cv2.<span class=\"pl-c1\">COLOR_BGR2RGB</span>)\n    subprocess.call(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nvidia-smi<span class=\"pl-pds\">'</span></span>)</pre></div>\n<pre><code>I get this:\nTue Jan 16 20:26:54 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  TITAN Xp            Off  | 00000000:05:00.0  On |                  N/A |\n| 27%   45C    P2    61W / 250W |    201MiB / 12188MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\n| 25%   44C    P8    18W / 250W |    434MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\n| 35%   58C    P2    63W / 250W |   2396MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\n| 23%   33C    P8     9W / 250W |     11MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1675      G   /usr/lib/xorg/Xorg                            36MiB |\n|    0     18235      C   python                                       153MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>It seems that opencv  (I'm on version 3.1) is initializing a block of memory on the GPU when it is called. I'm not sure if this is a desirable behavior. I certainly find it annoying. Its likely fixable by recompiling opencv without GPU support.</p>\n<p>EDIT: I just pip installed <code>python-opencv</code> 3.4 and the issue seems to have gone away.</p>\n<p>Perhaps this issue can be closed?</p>\n<p>Apparently its addressed in this issue: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"186072194\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/opencv/opencv/issues/7581\" data-hovercard-type=\"issue\" data-hovercard-url=\"/opencv/opencv/issues/7581/hovercard\" href=\"https://github.com/opencv/opencv/issues/7581\">opencv/opencv#7581</a><br>\nThe solution is to disable OpenCL support</p>", "body_text": "I'm getting this issue as well. Its been driving me crazy all day because I'm on a shared computer with 4 GPUs. I need to control which GPU's I'm on, so I'm using torch.device to force my programs to run on GPU2. However, whenever I run my DataLoader I end up allocating 153MB on GPU0. It gets worse when I use multiprocessing because the overhead is multiplied by the number of processes.\nAfter a long day of looking at this I think I can conclude that its an opencv bug/issue.\nConsider the following MWE:\nif __name__ == '__main__':\n    import cv2\n    import subprocess\n    import numpy as np\n    #img = (np.random.rand(32, 32, 3) * 255).astype(np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    subprocess.call('nvidia-smi')\nCalling this script produces the following result:\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  TITAN Xp            Off  | 00000000:05:00.0  On |                  N/A |\n| 27%   45C    P8    19W / 250W |     48MiB / 12188MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\n| 25%   44C    P8    18W / 250W |    434MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\n| 35%   58C    P2    63W / 250W |   2396MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\n| 23%   33C    P8     9W / 250W |     11MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1675      G   /usr/lib/xorg/Xorg                            36MiB |\n+-----------------------------------------------------------------------------+\n\nThere are no GPUs being used.\nHowever, if I un-comment the call to opencv\n    import cv2\n    import subprocess\n    import numpy as np\n    img = (np.random.rand(32, 32, 3) * 255).astype(np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    subprocess.call('nvidia-smi')\nI get this:\nTue Jan 16 20:26:54 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  TITAN Xp            Off  | 00000000:05:00.0  On |                  N/A |\n| 27%   45C    P2    61W / 250W |    201MiB / 12188MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\n| 25%   44C    P8    18W / 250W |    434MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\n| 35%   58C    P2    63W / 250W |   2396MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\n| 23%   33C    P8     9W / 250W |     11MiB / 12189MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1675      G   /usr/lib/xorg/Xorg                            36MiB |\n|    0     18235      C   python                                       153MiB |\n+-----------------------------------------------------------------------------+\n\nIt seems that opencv  (I'm on version 3.1) is initializing a block of memory on the GPU when it is called. I'm not sure if this is a desirable behavior. I certainly find it annoying. Its likely fixable by recompiling opencv without GPU support.\nEDIT: I just pip installed python-opencv 3.4 and the issue seems to have gone away.\nPerhaps this issue can be closed?\nApparently its addressed in this issue: opencv/opencv#7581\nThe solution is to disable OpenCL support", "body": "I'm getting this issue as well. Its been driving me crazy all day because I'm on a shared computer with 4 GPUs. I need to control which GPU's I'm on, so I'm using torch.device to force my programs to run on GPU2. However, whenever I run my DataLoader I end up allocating 153MB on GPU0. It gets worse when I use multiprocessing because the overhead is multiplied by the number of processes. \r\n\r\nAfter a long day of looking at this I think I can conclude that its an opencv bug/issue. \r\n\r\nConsider the following MWE:\r\n\r\n```python\r\nif __name__ == '__main__':\r\n    import cv2\r\n    import subprocess\r\n    import numpy as np\r\n    #img = (np.random.rand(32, 32, 3) * 255).astype(np.uint8)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    subprocess.call('nvidia-smi')\r\n```\r\n\r\nCalling this script produces the following result:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN Xp            Off  | 00000000:05:00.0  On |                  N/A |\r\n| 27%   45C    P8    19W / 250W |     48MiB / 12188MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 25%   44C    P8    18W / 250W |    434MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 35%   58C    P2    63W / 250W |   2396MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| 23%   33C    P8     9W / 250W |     11MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1675      G   /usr/lib/xorg/Xorg                            36MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nThere are no GPUs being used. \r\n\r\nHowever, if I un-comment the call to opencv\r\n\r\n```python\r\n\r\n    import cv2\r\n    import subprocess\r\n    import numpy as np\r\n    img = (np.random.rand(32, 32, 3) * 255).astype(np.uint8)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    subprocess.call('nvidia-smi')\r\n```\r\n\r\n```\r\nI get this:\r\nTue Jan 16 20:26:54 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN Xp            Off  | 00000000:05:00.0  On |                  N/A |\r\n| 27%   45C    P2    61W / 250W |    201MiB / 12188MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 25%   44C    P8    18W / 250W |    434MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 35%   58C    P2    63W / 250W |   2396MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| 23%   33C    P8     9W / 250W |     11MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1675      G   /usr/lib/xorg/Xorg                            36MiB |\r\n|    0     18235      C   python                                       153MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nIt seems that opencv  (I'm on version 3.1) is initializing a block of memory on the GPU when it is called. I'm not sure if this is a desirable behavior. I certainly find it annoying. Its likely fixable by recompiling opencv without GPU support. \r\n\r\nEDIT: I just pip installed `python-opencv` 3.4 and the issue seems to have gone away. \r\n\r\nPerhaps this issue can be closed?\r\n\r\nApparently its addressed in this issue: https://github.com/opencv/opencv/issues/7581\r\nThe solution is to disable OpenCL support"}