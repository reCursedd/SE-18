{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227644296", "pull_request_review_id": 167732481, "id": 227644296, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzY0NDI5Ng==", "diff_hunk": "@@ -59,195 +63,271 @@ namespace torch { namespace jit {\n //         been written. We place the variable-length index at the end and do\n //         not put any indicies into the header to fulfill this constraint.\n \n+// The serialized model, which contains all the metadata information,", "path": "caffe2/serialize/inline_container.h", "position": 16, "original_position": 15, "commit_id": "8beeb484bf4d149767c4ea9d1d376a4c3d0b7c3e", "original_commit_id": "fe3d6f2fdbf89d9f8393150b3fff4673b94c8c57", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "I guess it's more because of the continuous writing. If header is in front I might not yet know beforehand what are the offsets of all tensors are because the serialized header depends on them. That's why putting header in the end allows to hard code offsets. That's the format of ZIP file too for example. (and allows for appending more tensors easily though we probably care less of this one)", "created_at": "2018-10-24T05:17:09Z", "updated_at": "2018-11-23T15:53:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/12993#discussion_r227644296", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12993", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227644296"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12993#discussion_r227644296"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12993"}}, "body_html": "<p>I guess it's more because of the continuous writing. If header is in front I might not yet know beforehand what are the offsets of all tensors are because the serialized header depends on them. That's why putting header in the end allows to hard code offsets. That's the format of ZIP file too for example. (and allows for appending more tensors easily though we probably care less of this one)</p>", "body_text": "I guess it's more because of the continuous writing. If header is in front I might not yet know beforehand what are the offsets of all tensors are because the serialized header depends on them. That's why putting header in the end allows to hard code offsets. That's the format of ZIP file too for example. (and allows for appending more tensors easily though we probably care less of this one)"}