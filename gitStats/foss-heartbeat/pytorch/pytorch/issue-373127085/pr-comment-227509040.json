{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227509040", "pull_request_review_id": 167560635, "id": 227509040, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzUwOTA0MA==", "diff_hunk": "@@ -119,135 +134,202 @@ class PyTorchStreamReader {\n     auto ptr = malloc(size);\n     at::DataPtr retval(ptr, ptr, free, at::kCPU);\n \n-    in.read((char*)ptr, size);\n-    cursor += size;\n+    in_.read((char*)ptr, size);\n+    cursor_ += size;\n     seekToNextAlignmentBoundary();\n     return std::tuple<at::DataPtr, size_t>(std::move(retval), size);\n   }\n+\n+  // return dataptr, key, size\n+  std::tuple<at::DataPtr, int64_t, int64_t> getNextRecord() {\n+    int64_t key = cursor_;\n+    if (!hasNextRecord()) {\n+      throw std::runtime_error(\"No more record, but hasNextRecord is called.\");\n+    }\n+    auto tag = read64BitIntegerLittleEndian();\n+    if (tag != RecordTags::STORAGE) {\n+      throw std::runtime_error(\n+          \"Attempted to read a record of non-storage type\");\n+    }\n+    auto size = read64BitIntegerLittleEndian();\n+    seekToNextAlignmentBoundary();\n+    auto ptr = malloc(size);\n+    at::DataPtr retval(ptr, ptr, free, at::kCPU);\n+\n+    in_.read((char*)ptr, size);\n+    cursor_ += size;\n+    seekToNextAlignmentBoundary();\n+    return std::tuple<at::DataPtr, int64_t, int64_t>(\n+        std::move(retval), key, size);\n+  }\n+\n+  bool hasNextRecord() {\n+    if (cursor_ + kFieldAlignment * 2 < file_size_) {\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  bool close() {\n+    if (closed_) {\n+      return false;\n+    }\n+    closed_ = true;\n+    return true;\n+  }\n+\n+  bool closed() {\n+    return closed_;\n+  }\n+\n   ~PyTorchStreamReader() {\n   }\n+\n  private:\n-  std::istream& in;\n-  size_t cursor = 0;\n-  size_t file_size;\n-  size_t last_record_offset;\n+  std::istream& in_;\n+  size_t cursor_ = 0;\n+  size_t file_size_;\n+  size_t last_record_offset_;\n+  bool closed_ = false;\n \n   // Utility functions\n   uint64_t read64BitIntegerLittleEndian() {\n-   uint64_t retval;\n-   // TODO endian swap on platforms that need it?\n-   in.read(reinterpret_cast<char *>(&retval), 8);\n-   std::streamsize read_bytes = in.gcount();\n-   if (read_bytes != 8) {\n-     std::ostringstream errmsg;\n-     errmsg << \"Expected to read 8 bytes but got \" << read_bytes;\n-     throw std::runtime_error(errmsg.str());\n-   }\n-   cursor += read_bytes;\n-   return retval;\n+    uint64_t retval;\n+    // TODO endian swap on platforms that need it?\n+    in_.read(reinterpret_cast<char*>(&retval), 8);\n+    std::streamsize read_bytes = in_.gcount();\n+    if (read_bytes != 8) {\n+      std::ostringstream errmsg;\n+      errmsg << \"Expected to read 8 bytes but got \" << read_bytes;\n+      throw std::runtime_error(errmsg.str());\n+    }\n+    cursor_ += read_bytes;\n+    return retval;\n   }\n \n   void seekToNextAlignmentBoundary() {\n-   size_t next_offset = (cursor + kFieldAlignment) - (cursor % kFieldAlignment);\n-   size_t pad_amount = next_offset - cursor;\n-   cursor += pad_amount;\n-   in.seekg(cursor);\n+    size_t next_offset =\n+        (cursor_ + kFieldAlignment) - (cursor_ % kFieldAlignment);\n+    size_t pad_amount = next_offset - cursor_;\n+    cursor_ += pad_amount;\n+    in_.seekg(cursor_);\n   }\n \n   // File format deserialization functions\n-  void readAndValidateFileHeader() {\n-   // Validate magic number\n-   uint64_t magic = read64BitIntegerLittleEndian();\n-   if (magic != kFileMagicNumber) {\n-     throw std::runtime_error(\"Magic number mismatch in PyTorch file. File may\"\n-                              \" be corrupted or is not actually a PyTorch file.\");\n-   }\n-   uint64_t file_format_version = read64BitIntegerLittleEndian();\n-   if (file_format_version > kMaxSupportedFileFormatVersion) {\n-     std::ostringstream errmsg;\n-     errmsg << \"Attempted to read a PyTorch file with version \" << file_format_version\n-            << \" but the maximum supported version for reading is \" << kMaxSupportedFileFormatVersion\n-            << \". Your PyTorch installation may be too old.\";\n-     throw std::runtime_error(errmsg.str());\n-   }\n-   seekToNextAlignmentBoundary();\n-  }\n-  void readAndValidateFileFooter() {\n+  bool readAndValidateFileHeader() {\n+    // Validate magic number\n+    cursor_ = 0;\n+    in_.seekg(cursor_);\n+    uint64_t magic = read64BitIntegerLittleEndian();\n+    if (magic != kFileMagicNumber) {\n+      throw std::runtime_error(\n+          \"Magic number mismatch in PyTorch file. File may\"\n+          \" be corrupted or is not actually a PyTorch file.\");\n+    }\n+    uint64_t file_format_version = read64BitIntegerLittleEndian();\n+    if (file_format_version > kMaxSupportedFileFormatVersion) {\n+      std::ostringstream errmsg;\n+      errmsg << \"Attempted to read a PyTorch file with version \"\n+             << file_format_version\n+             << \" but the maximum supported version for reading is \"\n+             << kMaxSupportedFileFormatVersion\n+             << \". Your PyTorch installation may be too old.\";\n+      throw std::runtime_error(errmsg.str());\n+    }\n+    seekToNextAlignmentBoundary();\n+    return true;\n+  }\n+\n+  bool readAndValidateFileFooter() {\n     // Seek to location of file footer. We've already validated that the file\n     // length is a multiple of the alignment size\n-    cursor = file_size - kFieldAlignment;\n-    in.seekg(cursor);\n+    cursor_ = file_size_ - kFieldAlignment;\n+    in_.seekg(cursor_);\n     auto tag = read64BitIntegerLittleEndian();\n     if (tag != RecordTags::FOOTER) {\n       throw std::runtime_error(\"File footer has wrong record type. Is this\"\n                                \" file corrupted?\");\n     }\n-    last_record_offset = read64BitIntegerLittleEndian();\n-    if (last_record_offset > file_size) {\n+    last_record_offset_ = read64BitIntegerLittleEndian();\n+    if (last_record_offset_ > file_size_) {\n       throw std::runtime_error(\"Offset of last record is higher than the size\"\n                                \" of the file! Is this file corrupted?\");\n     }\n+    return true;\n   }\n };\n \n-class PyTorchStreamWriter {\n+class PyTorchStreamWriter final {\n  public:\n-  PyTorchStreamWriter(std::ostream& out_) : out(out_) {\n+  PyTorchStreamWriter(std::ostream& out) : out_(out) {\n     writeFileHeader();\n     // In the case that we do not write any records into this file, the last\n     // record index written into the footer will point to the footer itself.\n-    last_record_idx = cursor;\n+    last_record_idx_ = cursor_;\n   }\n+\n   uint64_t writeRecord(const char* data, size_t size) {", "path": "caffe2/serialize/inline_container.h", "position": null, "original_position": 291, "commit_id": "8beeb484bf4d149767c4ea9d1d376a4c3d0b7c3e", "original_commit_id": "ba909b162a712710d63e679d9ce71dcc7dc4ecb6", "user": {"login": "BIT-silence", "id": 3357667, "node_id": "MDQ6VXNlcjMzNTc2Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3357667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BIT-silence", "html_url": "https://github.com/BIT-silence", "followers_url": "https://api.github.com/users/BIT-silence/followers", "following_url": "https://api.github.com/users/BIT-silence/following{/other_user}", "gists_url": "https://api.github.com/users/BIT-silence/gists{/gist_id}", "starred_url": "https://api.github.com/users/BIT-silence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BIT-silence/subscriptions", "organizations_url": "https://api.github.com/users/BIT-silence/orgs", "repos_url": "https://api.github.com/users/BIT-silence/repos", "events_url": "https://api.github.com/users/BIT-silence/events{/privacy}", "received_events_url": "https://api.github.com/users/BIT-silence/received_events", "type": "User", "site_admin": false}, "body": "I'd suggest to make it something like\r\n\r\nstd::uint64_t WriteRecord(const void* data, const std::size_t size) { ... }", "created_at": "2018-10-23T18:19:20Z", "updated_at": "2018-11-23T15:53:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/12993#discussion_r227509040", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12993", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227509040"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12993#discussion_r227509040"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12993"}}, "body_html": "<p>I'd suggest to make it something like</p>\n<p>std::uint64_t WriteRecord(const void* data, const std::size_t size) { ... }</p>", "body_text": "I'd suggest to make it something like\nstd::uint64_t WriteRecord(const void* data, const std::size_t size) { ... }"}