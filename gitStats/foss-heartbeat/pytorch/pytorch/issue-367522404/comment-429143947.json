{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/429143947", "html_url": "https://github.com/pytorch/pytorch/pull/12430#issuecomment-429143947", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12430", "id": 429143947, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTE0Mzk0Nw==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-11T22:39:19Z", "updated_at": "2018-10-11T22:39:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Thanks for the clarification! I think your proposal sounds reasonable to me now. One important thing is to define <code>zero locations</code> at SparseTensor as <code>masked locations</code>, i.e., not involved in computations during either forward or backward. This allows SparseTensor ops to share the same names as in dense tensor with autograd support. A new type sounds fine to me. Is it meant to replace the current dispatch mechanism for SparseTensor?</p>", "body_text": "@apaszke Thanks for the clarification! I think your proposal sounds reasonable to me now. One important thing is to define zero locations at SparseTensor as masked locations, i.e., not involved in computations during either forward or backward. This allows SparseTensor ops to share the same names as in dense tensor with autograd support. A new type sounds fine to me. Is it meant to replace the current dispatch mechanism for SparseTensor?", "body": "@apaszke Thanks for the clarification! I think your proposal sounds reasonable to me now. One important thing is to define `zero locations` at SparseTensor as `masked locations`, i.e., not involved in computations during either forward or backward. This allows SparseTensor ops to share the same names as in dense tensor with autograd support. A new type sounds fine to me. Is it meant to replace the current dispatch mechanism for SparseTensor?"}