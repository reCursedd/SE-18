{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12430", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12430/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12430/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12430/events", "html_url": "https://github.com/pytorch/pytorch/pull/12430", "id": 367522404, "node_id": "MDExOlB1bGxSZXF1ZXN0MjIwOTI3OTk2", "number": 12430, "title": "[sparse] torch.sparse.sum()", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 31, "created_at": "2018-10-07T05:16:57Z", "updated_at": "2018-11-23T15:55:15Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/12430", "html_url": "https://github.com/pytorch/pytorch/pull/12430", "diff_url": "https://github.com/pytorch/pytorch/pull/12430.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/12430.patch"}, "body_html": "<ul class=\"contains-task-list\">\n<li>to <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #12241.\">fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"365676144\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12241\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12241/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12241\">#12241</a></li>\n<li>add <code>_sparse_sum()</code> to ATen, and expose as <code>torch.sparse.sum()</code>, not support <code>SparseTensor.sum()</code> currently</li>\n<li>this PR depends on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357010970\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11253\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11253/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11253\">#11253</a>, and will need to be updated upon it lands</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> implement forward</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> implement backward</li>\n<li>performance <a href=\"https://gist.github.com/weiyangfb/f4c55c88b6092ef8f7e348f6b9ad8946#file-sparse_sum_benchmark-py\">benchmark script</a>:\n<ul>\n<li>sum all dims is fastest for sparse tensor</li>\n<li>when input is sparse enough nnz = 0.1%, sum of sparse tensor is faster than dense in CPU, but not necessary in CUDA</li>\n<li>CUDA backward is comparable (&lt;2x) between <code>sum several dims</code> vs <code>sum all dims</code> in sparse</li>\n<li>CPU backward uses binary search is still slow in sparse, takes <code>5x</code> time in <code>sum [0, 2, 3] dims</code> vs <code>sum all dims</code>\n<ul>\n<li>optimize CUDA backward for now\n<ul>\n<li>using thrust for sort and binary search, but runtime not improved</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>both of CPU and CUDA forward are slow in sparse (<code>sum several dims</code> vs <code>sum all dims</code>), at most <code>20x</code> slower in CPU, and <code>10x</code> in CUDA\n<ul>\n<li>improve CPU and CUDA forward kernels</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>(nnz, sizes, sum_dims, keepdim, sum all or dims, bk=backward)</th>\n<th>CPU (sparse vs dense)</th>\n<th>CUDA(sparse vs dense)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)</td>\n<td>8.77 \u00b5s vs 72.9 \u00b5s</td>\n<td>42.5 \u00b5s vs 108 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD)</td>\n<td>112 \u00b5s vs 4.47 ms</td>\n<td>484 \u00b5s vs 407 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)</td>\n<td>141 \u00b5s vs 148 \u00b5s</td>\n<td>647 \u00b5s vs 231 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)</td>\n<td>235 \u00b5s vs 1.23 ms</td>\n<td>781 \u00b5s vs 213 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD)</td>\n<td>48.5 \u00b5s vs 360 \u00b5s</td>\n<td>160 \u00b5s vs 2.03 ms</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)</td>\n<td>258 \u00b5s vs 1.22 ms</td>\n<td>798 \u00b5s vs 224 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)</td>\n<td>204 \u00b5s vs 882 \u00b5s</td>\n<td>443 \u00b5s vs 133 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)</td>\n<td>709 \u00b5s vs 1.15 ms</td>\n<td>893 \u00b5s vs 202 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)</td>\n<td>39.8 \u00b5s vs 81 \u00b5s</td>\n<td>42.4 \u00b5s vs 113 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD)</td>\n<td>747 \u00b5s vs 4.7 ms</td>\n<td>2.4 ms vs 414 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)</td>\n<td>1.04 ms vs 126 \u00b5s</td>\n<td>5.03 ms vs 231 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)</td>\n<td>1.12 ms vs 1.24 ms</td>\n<td>5.99 ms vs 213 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD)</td>\n<td>133 \u00b5s vs 366 \u00b5s</td>\n<td>463 \u00b5s vs 2.03 ms</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)</td>\n<td>1.56 ms vs 1.22 ms</td>\n<td>6.11 ms vs 229 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)</td>\n<td>1.53 ms vs 799 \u00b5s</td>\n<td>824 \u00b5s vs 134 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)</td>\n<td>5.15 ms vs 1.09 ms</td>\n<td>7.02 ms vs 205 \u00b5s</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>after improving CPU and CUDA forward kernels\n<ul>\n<li>in <code>(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)</code> forward, CPU takes <del><code>171 \u00b5s</code></del>, in which <code>130 \u00b5s</code> is spent on <code>coalesce()</code>, for CUDA, total time is <del><code>331 \u00b5s</code></del>, in which <code>141 \u00b5s</code> is spent on <code>coalesce()</code>, we need to reduce time at other places outside <code>coalesce()</code>.</li>\n<li>after a few simple tweaks, now in the forward, it is at most <code>10x</code> slower in CPU, and <code>7x</code> in CUDA. And time takes in <code>sum dense dims only [2, 3]</code> is <code>~2x</code> of <code>sum all dims</code>. Speed of <code>sum all sparse dims [0, 1]</code> is on bar with <code>sum all dims</code></li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>(nnz,   sizes, sum_dims, keepdim, sum all or dims, bk=backward)</th>\n<th>CPU (sparse vs dense)</th>\n<th>CUDA(sparse vs dense)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)</td>\n<td>7 \u00b5s vs 69.5 \u00b5s</td>\n<td>31.5 \u00b5s vs 61.6 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD)</td>\n<td>11.3 \u00b5s vs 4.72 ms</td>\n<td>35.2 \u00b5s vs 285 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)</td>\n<td>197 \u00b5s vs 124 \u00b5s</td>\n<td>857 \u00b5s vs 134 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)</td>\n<td>124 \u00b5s vs 833 \u00b5s</td>\n<td>796 \u00b5s vs 106 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD)</td>\n<td>20.5 \u00b5s vs 213 \u00b5s</td>\n<td>39.4 \u00b5s vs 1.24 ms</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)</td>\n<td>131 \u00b5s vs 830 \u00b5s</td>\n<td>881 \u00b5s vs 132 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)</td>\n<td>95.8 \u00b5s vs 409 \u00b5s</td>\n<td>246 \u00b5s vs 87.2 \u00b5s</td>\n</tr>\n<tr>\n<td>(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)</td>\n<td>624 \u00b5s vs 820 \u00b5s</td>\n<td>953 \u00b5s vs 124 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)</td>\n<td>45.3 \u00b5s vs 72.9 \u00b5s</td>\n<td>33.9 \u00b5s vs 57.2 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD)</td>\n<td>81.4 \u00b5s vs 4.49 ms</td>\n<td>39.7 \u00b5s vs 280 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)</td>\n<td>984 \u00b5s vs 111 \u00b5s</td>\n<td>6.41 ms vs 121 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)</td>\n<td>1.45 ms vs 828 \u00b5s</td>\n<td>6.77 ms vs 113 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD)</td>\n<td>74.9 \u00b5s vs 209 \u00b5s</td>\n<td>37.7 \u00b5s vs 1.23 ms</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)</td>\n<td>1.48 ms vs 845 \u00b5s</td>\n<td>6.96 ms vs 132 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)</td>\n<td>1.14 ms vs 411 \u00b5s</td>\n<td>252 \u00b5s vs 87.8 \u00b5s</td>\n</tr>\n<tr>\n<td>(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)</td>\n<td>4.53 ms vs 851 \u00b5s</td>\n<td>7.12 ms vs 128 \u00b5s</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>time takes in CUDA backward of sparse is super long with large variance (in case of nnz=10000, it normally takes 6-7ms). To improve backward of sparse ops, we will need to debug at places other than CUDA kernels. here is a benchmark of <code>torch.copy_()</code>:</li>\n</ul>\n<pre><code>&gt;&gt;&gt; d = [1000, 1000, 2, 2]\n&gt;&gt;&gt; nnz = 10000\n&gt;&gt;&gt; I = torch.cat([torch.randint(0, d[0], size=(nnz,)), \n               torch.randint(0, d[1], size=(nnz,))], 0).reshape(2, nnz)\n&gt;&gt;&gt; V = torch.randn(nnz, d[2], d[3])\n&gt;&gt;&gt; size = torch.Size(d)\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce().cuda()\n&gt;&gt;&gt; S2 = torch.sparse_coo_tensor(I, V, size).coalesce().cuda().requires_grad_()\n&gt;&gt;&gt; data = S2.clone()\n&gt;&gt;&gt; S.copy_(S2)\n&gt;&gt;&gt; y = S * 2\n&gt;&gt;&gt; torch.cuda.synchronize()\n&gt;&gt;&gt; %timeit y.backward(data, retain_graph=True); torch.cuda.synchronize()\n7.07 ms \u00b1 3.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>", "body_text": "to fix #12241\nadd _sparse_sum() to ATen, and expose as torch.sparse.sum(), not support SparseTensor.sum() currently\nthis PR depends on #11253, and will need to be updated upon it lands\n implement forward\n implement backward\nperformance benchmark script:\n\nsum all dims is fastest for sparse tensor\nwhen input is sparse enough nnz = 0.1%, sum of sparse tensor is faster than dense in CPU, but not necessary in CUDA\nCUDA backward is comparable (<2x) between sum several dims vs sum all dims in sparse\nCPU backward uses binary search is still slow in sparse, takes 5x time in sum [0, 2, 3] dims vs sum all dims\n\noptimize CUDA backward for now\n\nusing thrust for sort and binary search, but runtime not improved\n\n\n\n\nboth of CPU and CUDA forward are slow in sparse (sum several dims vs sum all dims), at most 20x slower in CPU, and 10x in CUDA\n\nimprove CPU and CUDA forward kernels\n\n\n\n\n\n\n\n\n(nnz, sizes, sum_dims, keepdim, sum all or dims, bk=backward)\nCPU (sparse vs dense)\nCUDA(sparse vs dense)\n\n\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)\n8.77 \u00b5s vs 72.9 \u00b5s\n42.5 \u00b5s vs 108 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD)\n112 \u00b5s vs 4.47 ms\n484 \u00b5s vs 407 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)\n141 \u00b5s vs 148 \u00b5s\n647 \u00b5s vs 231 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)\n235 \u00b5s vs 1.23 ms\n781 \u00b5s vs 213 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD)\n48.5 \u00b5s vs 360 \u00b5s\n160 \u00b5s vs 2.03 ms\n\n\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)\n258 \u00b5s vs 1.22 ms\n798 \u00b5s vs 224 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)\n204 \u00b5s vs 882 \u00b5s\n443 \u00b5s vs 133 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)\n709 \u00b5s vs 1.15 ms\n893 \u00b5s vs 202 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)\n39.8 \u00b5s vs 81 \u00b5s\n42.4 \u00b5s vs 113 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD)\n747 \u00b5s vs 4.7 ms\n2.4 ms vs 414 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)\n1.04 ms vs 126 \u00b5s\n5.03 ms vs 231 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)\n1.12 ms vs 1.24 ms\n5.99 ms vs 213 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD)\n133 \u00b5s vs 366 \u00b5s\n463 \u00b5s vs 2.03 ms\n\n\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)\n1.56 ms vs 1.22 ms\n6.11 ms vs 229 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)\n1.53 ms vs 799 \u00b5s\n824 \u00b5s vs 134 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)\n5.15 ms vs 1.09 ms\n7.02 ms vs 205 \u00b5s\n\n\n\n\nafter improving CPU and CUDA forward kernels\n\nin (1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD) forward, CPU takes 171 \u00b5s, in which 130 \u00b5s is spent on coalesce(), for CUDA, total time is 331 \u00b5s, in which 141 \u00b5s is spent on coalesce(), we need to reduce time at other places outside coalesce().\nafter a few simple tweaks, now in the forward, it is at most 10x slower in CPU, and 7x in CUDA. And time takes in sum dense dims only [2, 3] is ~2x of sum all dims. Speed of sum all sparse dims [0, 1] is on bar with sum all dims\n\n\n\n\n\n\n(nnz,   sizes, sum_dims, keepdim, sum all or dims, bk=backward)\nCPU (sparse vs dense)\nCUDA(sparse vs dense)\n\n\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)\n7 \u00b5s vs 69.5 \u00b5s\n31.5 \u00b5s vs 61.6 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD)\n11.3 \u00b5s vs 4.72 ms\n35.2 \u00b5s vs 285 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)\n197 \u00b5s vs 124 \u00b5s\n857 \u00b5s vs 134 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)\n124 \u00b5s vs 833 \u00b5s\n796 \u00b5s vs 106 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD)\n20.5 \u00b5s vs 213 \u00b5s\n39.4 \u00b5s vs 1.24 ms\n\n\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)\n131 \u00b5s vs 830 \u00b5s\n881 \u00b5s vs 132 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)\n95.8 \u00b5s vs 409 \u00b5s\n246 \u00b5s vs 87.2 \u00b5s\n\n\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)\n624 \u00b5s vs 820 \u00b5s\n953 \u00b5s vs 124 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll)\n45.3 \u00b5s vs 72.9 \u00b5s\n33.9 \u00b5s vs 57.2 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD)\n81.4 \u00b5s vs 4.49 ms\n39.7 \u00b5s vs 280 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk)\n984 \u00b5s vs 111 \u00b5s\n6.41 ms vs 121 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk)\n1.45 ms vs 828 \u00b5s\n6.77 ms vs 113 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD)\n74.9 \u00b5s vs 209 \u00b5s\n37.7 \u00b5s vs 1.23 ms\n\n\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk)\n1.48 ms vs 845 \u00b5s\n6.96 ms vs 132 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)\n1.14 ms vs 411 \u00b5s\n252 \u00b5s vs 87.8 \u00b5s\n\n\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk)\n4.53 ms vs 851 \u00b5s\n7.12 ms vs 128 \u00b5s\n\n\n\n\ntime takes in CUDA backward of sparse is super long with large variance (in case of nnz=10000, it normally takes 6-7ms). To improve backward of sparse ops, we will need to debug at places other than CUDA kernels. here is a benchmark of torch.copy_():\n\n>>> d = [1000, 1000, 2, 2]\n>>> nnz = 10000\n>>> I = torch.cat([torch.randint(0, d[0], size=(nnz,)), \n               torch.randint(0, d[1], size=(nnz,))], 0).reshape(2, nnz)\n>>> V = torch.randn(nnz, d[2], d[3])\n>>> size = torch.Size(d)\n>>> S = torch.sparse_coo_tensor(I, V, size).coalesce().cuda()\n>>> S2 = torch.sparse_coo_tensor(I, V, size).coalesce().cuda().requires_grad_()\n>>> data = S2.clone()\n>>> S.copy_(S2)\n>>> y = S * 2\n>>> torch.cuda.synchronize()\n>>> %timeit y.backward(data, retain_graph=True); torch.cuda.synchronize()\n7.07 ms \u00b1 3.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)", "body": "- to fix #12241\r\n- add `_sparse_sum()` to ATen, and expose as `torch.sparse.sum()`, not support `SparseTensor.sum()` currently\r\n- this PR depends on #11253, and will need to be updated upon it lands\r\n- [x] implement forward\r\n- [x] implement backward\r\n- performance [benchmark script](https://gist.github.com/weiyangfb/f4c55c88b6092ef8f7e348f6b9ad8946#file-sparse_sum_benchmark-py):\r\n  - sum all dims is fastest for sparse tensor\r\n  - when input is sparse enough nnz = 0.1%, sum of sparse tensor is faster than dense in CPU, but not necessary in CUDA\r\n  - CUDA backward is comparable (<2x) between `sum several dims` vs `sum all dims` in sparse\r\n  - CPU backward uses binary search is still slow in sparse, takes `5x` time in `sum [0, 2, 3] dims` vs `sum all dims`\r\n    - optimize CUDA backward for now\r\n      - using thrust for sort and binary search, but runtime not improved\r\n  - both of CPU and CUDA forward are slow in sparse (`sum several dims` vs `sum all dims`), at most `20x` slower in CPU, and `10x` in CUDA\r\n    - improve CPU and CUDA forward kernels\r\n\r\n(nnz, sizes, sum_dims, keepdim, sum all or dims, bk=backward) | CPU (sparse vs dense) | CUDA(sparse vs dense)\r\n-- | -- | --\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll) | 8.77 \u00b5s vs 72.9 \u00b5s | 42.5 \u00b5s vs 108 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD) | 112 \u00b5s vs 4.47 ms | 484 \u00b5s vs 407 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk) | 141 \u00b5s vs 148 \u00b5s | 647 \u00b5s vs 231 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk) | 235 \u00b5s vs 1.23 ms | 781 \u00b5s vs 213 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD) | 48.5 \u00b5s vs 360 \u00b5s | 160 \u00b5s vs 2.03 ms\r\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk) | 258 \u00b5s vs 1.22 ms | 798 \u00b5s vs 224 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD) | 204 \u00b5s vs 882 \u00b5s | 443 \u00b5s vs 133 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk) | 709 \u00b5s vs 1.15 ms | 893 \u00b5s vs 202 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll) | 39.8 \u00b5s vs 81 \u00b5s | 42.4 \u00b5s vs 113 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD) | 747 \u00b5s vs 4.7 ms | 2.4 ms vs 414 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk) | 1.04 ms vs 126 \u00b5s | 5.03 ms vs 231 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk) | 1.12 ms vs 1.24 ms | 5.99 ms vs 213 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD) | 133 \u00b5s vs 366 \u00b5s | 463 \u00b5s vs 2.03 ms\r\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk) | 1.56 ms vs 1.22 ms | 6.11 ms vs 229 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD) | 1.53 ms vs 799 \u00b5s | 824 \u00b5s vs 134 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk) | 5.15 ms vs 1.09 ms | 7.02 ms vs 205 \u00b5s\r\n\r\n- after improving CPU and CUDA forward kernels\r\n  - in `(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD)` forward, CPU takes ~~`171 \u00b5s`~~, in which `130 \u00b5s` is spent on `coalesce()`, for CUDA, total time is ~~`331 \u00b5s`~~, in which `141 \u00b5s` is spent on `coalesce()`, we need to reduce time at other places outside `coalesce()`.\r\n  - after a few simple tweaks, now in the forward, it is at most `10x` slower in CPU, and `7x` in CUDA. And time takes in `sum dense dims only [2, 3]` is `~2x` of `sum all dims`. Speed of `sum all sparse dims [0, 1]` is on bar with `sum all dims`\r\n\r\n\r\n(nnz,   sizes, sum_dims, keepdim, sum all or dims, bk=backward) | CPU (sparse vs dense) | CUDA(sparse vs dense)\r\n-- | -- | --\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll) | 7 \u00b5s vs 69.5 \u00b5s | 31.5 \u00b5s vs 61.6 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD) | 11.3 \u00b5s vs 4.72 ms | 35.2 \u00b5s vs 285 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk) | 197 \u00b5s vs 124 \u00b5s | 857 \u00b5s vs 134 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk) | 124 \u00b5s vs 833 \u00b5s | 796 \u00b5s vs 106 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD) | 20.5 \u00b5s vs 213 \u00b5s | 39.4 \u00b5s vs 1.24 ms\r\n(1000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk) | 131 \u00b5s vs 830 \u00b5s | 881 \u00b5s vs 132 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD) | 95.8 \u00b5s vs 409 \u00b5s | 246 \u00b5s vs 87.2 \u00b5s\r\n(1000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk) | 624 \u00b5s vs 820 \u00b5s | 953 \u00b5s vs 124 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll) | 45.3 \u00b5s vs 72.9 \u00b5s | 33.9 \u00b5s vs 57.2 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD) | 81.4 \u00b5s vs 4.49 ms | 39.7 \u00b5s vs 280 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumAll, bk) | 984 \u00b5s vs 111 \u00b5s | 6.41 ms vs 121 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 1], False, sumD, bk) | 1.45 ms vs 828 \u00b5s | 6.77 ms vs 113 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD) | 74.9 \u00b5s vs 209 \u00b5s | 37.7 \u00b5s vs 1.23 ms\r\n(10000,   [1000, 1000, 2, 2], [2, 3], False, sumD, bk) | 1.48 ms vs 845 \u00b5s | 6.96 ms vs 132 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD) | 1.14 ms vs 411 \u00b5s | 252 \u00b5s vs 87.8 \u00b5s\r\n(10000,   [1000, 1000, 2, 2], [0, 2, 3], False, sumD, bk) | 4.53 ms vs 851 \u00b5s | 7.12 ms vs 128 \u00b5s\r\n\r\n- time takes in CUDA backward of sparse is super long with large variance (in case of nnz=10000, it normally takes 6-7ms). To improve backward of sparse ops, we will need to debug at places other than CUDA kernels. here is a benchmark of `torch.copy_()`:\r\n```\r\n>>> d = [1000, 1000, 2, 2]\r\n>>> nnz = 10000\r\n>>> I = torch.cat([torch.randint(0, d[0], size=(nnz,)), \r\n               torch.randint(0, d[1], size=(nnz,))], 0).reshape(2, nnz)\r\n>>> V = torch.randn(nnz, d[2], d[3])\r\n>>> size = torch.Size(d)\r\n>>> S = torch.sparse_coo_tensor(I, V, size).coalesce().cuda()\r\n>>> S2 = torch.sparse_coo_tensor(I, V, size).coalesce().cuda().requires_grad_()\r\n>>> data = S2.clone()\r\n>>> S.copy_(S2)\r\n>>> y = S * 2\r\n>>> torch.cuda.synchronize()\r\n>>> %timeit y.backward(data, retain_graph=True); torch.cuda.synchronize()\r\n7.07 ms \u00b1 3.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n\r\n"}