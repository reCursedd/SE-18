{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/428733267", "html_url": "https://github.com/pytorch/pytorch/pull/12430#issuecomment-428733267", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12430", "id": 428733267, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODczMzI2Nw==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T21:11:00Z", "updated_at": "2018-10-10T21:11:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> ok, I think here is more about the badness of <code>sparse_X</code> ops, to address it, how about to add a <code>sparse=bool</code> args for each op supports SparseTensor? Similar to embedding, <code>sparse=True</code> indicates sparse gradients during the backward. About <code>masked tensor</code> representation, it is more of an option of possible way to implement backward since it might not be universally optimal for all ops.</p>", "body_text": "@apaszke ok, I think here is more about the badness of sparse_X ops, to address it, how about to add a sparse=bool args for each op supports SparseTensor? Similar to embedding, sparse=True indicates sparse gradients during the backward. About masked tensor representation, it is more of an option of possible way to implement backward since it might not be universally optimal for all ops.", "body": "@apaszke ok, I think here is more about the badness of `sparse_X` ops, to address it, how about to add a `sparse=bool` args for each op supports SparseTensor? Similar to embedding, `sparse=True` indicates sparse gradients during the backward. About `masked tensor` representation, it is more of an option of possible way to implement backward since it might not be universally optimal for all ops."}