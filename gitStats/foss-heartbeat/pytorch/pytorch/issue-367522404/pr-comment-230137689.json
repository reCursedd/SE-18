{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230137689", "pull_request_review_id": 170806992, "id": 230137689, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMDEzNzY4OQ==", "diff_hunk": "@@ -799,4 +800,241 @@ Tensor sspaddmm(const Tensor& self, const Tensor& mat1, const Tensor& mat2,\n   return result;\n }\n \n+// --------------------------------------------------------------------\n+// sparse.sum()\n+// --------------------------------------------------------------------\n+Tensor _sparse_sum(const SparseTensor& input) {\n+  return input.values().sum();\n+}\n+\n+Tensor _sparse_sum(const SparseTensor& input, ScalarType dtype) {\n+  return at::_sparse_sum(input.to(dtype));\n+}\n+\n+Tensor _sparse_sum(const SparseTensor& input, IntList dims_to_sum) {\n+  return at::_sparse_sum(input, dims_to_sum, false);\n+}\n+\n+Tensor _sparse_sum(const SparseTensor& input, IntList dims_to_sum, ScalarType dtype) {\n+  return at::_sparse_sum(input.to(dtype), dims_to_sum, false);\n+}\n+\n+Tensor _sparse_sum(const SparseTensor& input, IntList dims_to_sum, bool keepdim) {\n+\n+  AT_CHECK(input.is_coalesced(), \"To support autograd, input SparseTensor has to be coalesced.\");", "path": "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "position": null, "original_position": 33, "commit_id": "f3db2514189f34fbdf9ea48fbebeacd8fa711b7c", "original_commit_id": "8399bdf6b12eb79a790ffcd3f35c8a47252662dc", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "I think it's totally reasonable to not implement the non-coalesced case. However, I don't think the justification given here is quite right. It would totally be possible, in principle, to support sparse sum on non-coalesced tensors: the non-dims-to-sum case is able to do precisely this! (And in principle, coalescing the tensor and then calling this function would get the correct autograd formula, right?) So this is more of a, \"We didn't implement it situation\" than \"Autograd is not meaningful when the input is not coalesced.\"", "created_at": "2018-11-01T17:57:49Z", "updated_at": "2018-11-23T15:54:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/12430#discussion_r230137689", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12430", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230137689"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12430#discussion_r230137689"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12430"}}, "body_html": "<p>I think it's totally reasonable to not implement the non-coalesced case. However, I don't think the justification given here is quite right. It would totally be possible, in principle, to support sparse sum on non-coalesced tensors: the non-dims-to-sum case is able to do precisely this! (And in principle, coalescing the tensor and then calling this function would get the correct autograd formula, right?) So this is more of a, \"We didn't implement it situation\" than \"Autograd is not meaningful when the input is not coalesced.\"</p>", "body_text": "I think it's totally reasonable to not implement the non-coalesced case. However, I don't think the justification given here is quite right. It would totally be possible, in principle, to support sparse sum on non-coalesced tensors: the non-dims-to-sum case is able to do precisely this! (And in principle, coalescing the tensor and then calling this function would get the correct autograd formula, right?) So this is more of a, \"We didn't implement it situation\" than \"Autograd is not meaningful when the input is not coalesced.\""}