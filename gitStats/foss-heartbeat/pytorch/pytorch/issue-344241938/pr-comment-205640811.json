{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205640811", "pull_request_review_id": 140954161, "id": 205640811, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTY0MDgxMQ==", "diff_hunk": "@@ -566,6 +882,18 @@ void CompiledFusionFunction::launch_with_tensors(at::ArrayRef<at::Tensor> inputs\n       }\n     }\n   }\n+\n+  #ifdef USE_CUDA\n+    // If the kernel call contains a random op, we need to pass in random seeds as\n+    // well.\n+    if(has_random && this->backend() == at::kCUDA) {\n+      auto gen_ = THCRandom_getGenerator(at::globalContext().getTHCState());\n+      uint64_t offset = gen_->state.philox_seed_offset.fetch_add(20);", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": null, "original_position": 400, "commit_id": "d802d7a10e5ef692c0fa662605c62fc79ffc8957", "original_commit_id": "dae9db045de4dc496fb5be6cc99be05a10d4df0b", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "body": "I believe that 10 or 20 estimate was specific to the case of generating Poisson-distributed random numbers.  \r\n\r\nIf you're just generating uniformly distributed floats or ints one at a time, you're manually invoking the generator, so you know exactly how many times you're calling the generator per thread, as @zdevito says.  \r\n\r\nThe original tensorflow Philox anticipates that the initial `offset` is a multiple of 4, where `offset` is the number of 32-bit values generated so far (aka, in prior kernel launches) for this particular thread's subsequence.  We should stick to that convention to be safe.  You're already dividing `offset` by 4 [when you construct PhiloxRandom](https://github.com/pytorch/pytorch/pull/9795/files#diff-c76c51fc543bd5257d3ae9bb400bdbd8R357) within PhiloxWrapper, which is fine.  After the kernel launch, to maintain `offset` as a multiple of 4, what we want is\r\n```\r\noffset += 4*max times Philox state was incremented within each thread\r\n```\r\nSome threads may increment the state one more time than others due to tail effects, and each Philox state increment within a thread produces 4 32-bit values.  I think we can account for all this with the following:\r\n```\r\noffset += 4*ceil(numel/(4*num_threads_per_block*number_of_blocks_launched))\r\n```\r\nwhich agrees with the implementation in @ngimel's PR.  I wanted to flesh out the logic (partially as an exercise in organizing my own thoughts).", "created_at": "2018-07-27T00:39:16Z", "updated_at": "2018-11-23T15:48:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/9795#discussion_r205640811", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9795", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205640811"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9795#discussion_r205640811"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9795"}}, "body_html": "<p>I believe that 10 or 20 estimate was specific to the case of generating Poisson-distributed random numbers.</p>\n<p>If you're just generating uniformly distributed floats or ints one at a time, you're manually invoking the generator, so you know exactly how many times you're calling the generator per thread, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> says.</p>\n<p>The original tensorflow Philox anticipates that the initial <code>offset</code> is a multiple of 4, where <code>offset</code> is the number of 32-bit values generated so far (aka, in prior kernel launches) for this particular thread's subsequence.  We should stick to that convention to be safe.  You're already dividing <code>offset</code> by 4 <a href=\"https://github.com/pytorch/pytorch/pull/9795/files#diff-c76c51fc543bd5257d3ae9bb400bdbd8R357\">when you construct PhiloxRandom</a> within PhiloxWrapper, which is fine.  After the kernel launch, to maintain <code>offset</code> as a multiple of 4, what we want is</p>\n<pre><code>offset += 4*max times Philox state was incremented within each thread\n</code></pre>\n<p>Some threads may increment the state one more time than others due to tail effects, and each Philox state increment within a thread produces 4 32-bit values.  I think we can account for all this with the following:</p>\n<pre><code>offset += 4*ceil(numel/(4*num_threads_per_block*number_of_blocks_launched))\n</code></pre>\n<p>which agrees with the implementation in <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a>'s PR.  I wanted to flesh out the logic (partially as an exercise in organizing my own thoughts).</p>", "body_text": "I believe that 10 or 20 estimate was specific to the case of generating Poisson-distributed random numbers.\nIf you're just generating uniformly distributed floats or ints one at a time, you're manually invoking the generator, so you know exactly how many times you're calling the generator per thread, as @zdevito says.\nThe original tensorflow Philox anticipates that the initial offset is a multiple of 4, where offset is the number of 32-bit values generated so far (aka, in prior kernel launches) for this particular thread's subsequence.  We should stick to that convention to be safe.  You're already dividing offset by 4 when you construct PhiloxRandom within PhiloxWrapper, which is fine.  After the kernel launch, to maintain offset as a multiple of 4, what we want is\noffset += 4*max times Philox state was incremented within each thread\n\nSome threads may increment the state one more time than others due to tail effects, and each Philox state increment within a thread produces 4 32-bit values.  I think we can account for all this with the following:\noffset += 4*ceil(numel/(4*num_threads_per_block*number_of_blocks_launched))\n\nwhich agrees with the implementation in @ngimel's PR.  I wanted to flesh out the logic (partially as an exercise in organizing my own thoughts).", "in_reply_to_id": 205586219}