{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9602", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9602/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9602/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9602/events", "html_url": "https://github.com/pytorch/pytorch/issues/9602", "id": 342886200, "node_id": "MDU6SXNzdWUzNDI4ODYyMDA=", "number": 9602, "title": "Bugs in batch normalization layer of nn.batchNorm1d()?", "user": {"login": "pandezhao", "id": 34227540, "node_id": "MDQ6VXNlcjM0MjI3NTQw", "avatar_url": "https://avatars1.githubusercontent.com/u/34227540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pandezhao", "html_url": "https://github.com/pandezhao", "followers_url": "https://api.github.com/users/pandezhao/followers", "following_url": "https://api.github.com/users/pandezhao/following{/other_user}", "gists_url": "https://api.github.com/users/pandezhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/pandezhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pandezhao/subscriptions", "organizations_url": "https://api.github.com/users/pandezhao/orgs", "repos_url": "https://api.github.com/users/pandezhao/repos", "events_url": "https://api.github.com/users/pandezhao/events{/privacy}", "received_events_url": "https://api.github.com/users/pandezhao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-19T20:55:48Z", "updated_at": "2018-07-21T21:42:23Z", "closed_at": "2018-07-21T21:42:23Z", "author_association": "NONE", "body_html": "<p>There is something wrong with the 1 dimension batch normalization layer, the output of normalization layer is not related to the input, but only related to the BN's parameter.<br>\nFor example:<br>\n<code>a = Variable(torch.Tensor([[1000,5000],[50,20]])) b=torch.nn.BatchNorm1d(2) c = b(a) </code><br>\nthen we only got<br>\n<code>c tensor([[ 0.8070,  0.8190], [-0.8070, -0.8190]])</code><br>\nThis value is not related with input, but only related to Batch Normalization layer's parameters.<br>\n<code>b.weight Parameter containing: tensor([ 0.8070,  0.8190])</code></p>\n<p>But when we tried three channel input, the output value will not related to the weight.<br>\n`a=torch.randn(3,2)<br>\nb=torch.randn(3,2)<br>\nbn=BatchNorm1d(2)<br>\na1=bn(a)<br>\nb1=bn(b)<br>\na1<br>\ntensor([[-0.9728,  0.0101],<br>\n[ 0.8335,  0.2478],<br>\n[ 0.1393, -0.2579]])</p>\n<p>b1<br>\ntensor([[ 0.7941,  0.0316],<br>\n[-0.9947, -0.2673],<br>\n[ 0.2006,  0.2357]])<br>\nbn.weight<br>\nParameter containing:<br>\ntensor([ 0.7440,  0.2066])<br>\n`<br>\nThis bug only happens when input batch size equals 2.<br>\nAnd my torch vision is 0.4.0.</p>", "body_text": "There is something wrong with the 1 dimension batch normalization layer, the output of normalization layer is not related to the input, but only related to the BN's parameter.\nFor example:\na = Variable(torch.Tensor([[1000,5000],[50,20]])) b=torch.nn.BatchNorm1d(2) c = b(a) \nthen we only got\nc tensor([[ 0.8070,  0.8190], [-0.8070, -0.8190]])\nThis value is not related with input, but only related to Batch Normalization layer's parameters.\nb.weight Parameter containing: tensor([ 0.8070,  0.8190])\nBut when we tried three channel input, the output value will not related to the weight.\n`a=torch.randn(3,2)\nb=torch.randn(3,2)\nbn=BatchNorm1d(2)\na1=bn(a)\nb1=bn(b)\na1\ntensor([[-0.9728,  0.0101],\n[ 0.8335,  0.2478],\n[ 0.1393, -0.2579]])\nb1\ntensor([[ 0.7941,  0.0316],\n[-0.9947, -0.2673],\n[ 0.2006,  0.2357]])\nbn.weight\nParameter containing:\ntensor([ 0.7440,  0.2066])\n`\nThis bug only happens when input batch size equals 2.\nAnd my torch vision is 0.4.0.", "body": "There is something wrong with the 1 dimension batch normalization layer, the output of normalization layer is not related to the input, but only related to the BN's parameter.\r\nFor example:\r\n`a = Variable(torch.Tensor([[1000,5000],[50,20]]))\r\nb=torch.nn.BatchNorm1d(2)\r\nc = b(a)\r\n`\r\nthen we only got  \r\n`c tensor([[ 0.8070,  0.8190], [-0.8070, -0.8190]])`\r\nThis value is not related with input, but only related to Batch Normalization layer's parameters.\r\n`b.weight\r\nParameter containing:\r\ntensor([ 0.8070,  0.8190])`\r\n\r\nBut when we tried three channel input, the output value will not related to the weight.\r\n`a=torch.randn(3,2)\r\nb=torch.randn(3,2)\r\nbn=BatchNorm1d(2)\r\na1=bn(a)\r\nb1=bn(b)\r\na1\r\ntensor([[-0.9728,  0.0101],\r\n        [ 0.8335,  0.2478],\r\n        [ 0.1393, -0.2579]])\r\n\r\nb1\r\ntensor([[ 0.7941,  0.0316],\r\n        [-0.9947, -0.2673],\r\n        [ 0.2006,  0.2357]])\r\nbn.weight\r\nParameter containing:\r\ntensor([ 0.7440,  0.2066])\r\n`\r\nThis bug only happens when input batch size equals 2. \r\nAnd my torch vision is 0.4.0.\r\n"}