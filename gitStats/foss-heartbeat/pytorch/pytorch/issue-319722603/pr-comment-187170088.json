{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187170088", "pull_request_review_id": 118895531, "id": 187170088, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzE3MDA4OA==", "diff_hunk": "@@ -23,10 +23,108 @@ def __init__(self, ptr):\n         self.cdata = ptr\n \n \n-# mapping from handles to StorageRef objects\n+# mapping from handles to StorageRef objects. Doesn't include CUDA handles.\n shared_cache = weakref.WeakValueDictionary()\n \n \n+# CUDA storage caching strategy\n+# -----------------------------\n+#\n+# All CUDA storages are cached into 'cuda_cache'.\n+#\n+# Call a \"CUDA storage that processes wish to share\" a \"real storage\".\n+# \"real storages\" are the storages that other processes want to and\n+# eventually will receive.\n+#\n+# Call a \"CUDA storage that points to the base of an allocation block\" a\n+# \"base storage\". These are obtained by opening CUDA handles to the\n+# base of an allocation block, where multiple \"real storages\" may live.\n+# Those \"real storages\" can be obtained by adding an offset to \"base storage\"\n+#\n+# \"base storages\" are cached into the \"base cache\" and \"real storages\" are\n+# cached into the \"real cache\"\n+#\n+# When a process shares a cuda storage:\n+# - If the cuda storage is from opening a received handle, find the handle\n+#   in the metadata cache and send the returned metadata.\n+# - If the cuda storage is from allocating storage, get the handle, build\n+#   the metadata, and send it.\n+#\n+# When a process receives a cuda storage:\n+# - Check the caches:\n+#   - Check the \"real cache\" for the real storage.\n+#   - If it's not there, look in the \"base cache\".\n+#   - If the storage exists in the \"base cache\", recreate the real storage by\n+#     add it to the \"base storage\"\n+# - If it's not in the caches, recreate the real and base storages and cache\n+#   both.\n+# - In addition, if this process received a handle that it has opened\n+#   (either now or in the past), we cache the storage's metadata in the\n+#   metadata cache. This is so that the process can access the handle\n+#   to send to other processes.\n+#   A process that didn't open a handle (the process that allocated the storage)\n+#   doesn't cache the metadata.\n+#\n+# \"base storages\" are uniquely identified by their CUDA handle.\n+# \"real storages\" are uniquely identified by (handle, offset).\n+class CUDASharedCache(object):\n+    def __init__(self):\n+        # (handle) -> \"base storage\" ref\n+        self.base_cache = weakref.WeakValueDictionary()\n+\n+        # (handle, offset) -> \"real storage\" ref\n+        self.real_cache = weakref.WeakValueDictionary()\n+\n+        # dataptr -> \"real storage\" ref\n+        self.data_cache = weakref.WeakValueDictionary()", "path": "torch/multiprocessing/reductions.py", "position": 58, "original_position": 58, "commit_id": "bb9d7b36ff4bd90e197fb645de2cd1393ca6654b", "original_commit_id": "1d96f36b8ceeae37bba31ecf769ace4061213e63", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can we call it `metadata_cache`? That's what you call it everywhere", "created_at": "2018-05-09T20:42:42Z", "updated_at": "2018-11-23T15:43:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/7204#discussion_r187170088", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7204", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187170088"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7204#discussion_r187170088"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7204"}}, "body_html": "<p>Can we call it <code>metadata_cache</code>? That's what you call it everywhere</p>", "body_text": "Can we call it metadata_cache? That's what you call it everywhere"}