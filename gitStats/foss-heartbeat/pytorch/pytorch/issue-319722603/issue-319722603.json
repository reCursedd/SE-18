{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7204", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7204/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7204/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7204/events", "html_url": "https://github.com/pytorch/pytorch/pull/7204", "id": 319722603, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg1NTY5MjIw", "number": 7204, "title": "Support passing the same CUDA tensor between multiple processes", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-02T21:42:47Z", "updated_at": "2018-11-23T15:44:01Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7204", "html_url": "https://github.com/pytorch/pytorch/pull/7204", "diff_url": "https://github.com/pytorch/pytorch/pull/7204.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7204.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #7096.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"318928255\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7096\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7096/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7096\">#7096</a>.</p>\n<p>This PR is an update to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"319295871\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7146\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7146/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7146\">#7146</a></p>\n<h1>Problem</h1>\n<p>Right now, if one attempts to share a CUDA tensor and get it from a<br>\nqueue from the same process it was saved in, a crash can occur.</p>\n<p>Here is some example code to demonstrate this:</p>\n<pre><code>import torch\nimport torch.multiprocessing as mp\nimport numpy.random as npr\n\nmp.set_start_method('spawn')\n\nq = mp.Queue()\nx = torch.randn(1000).cuda()\ntensor = torch.ones(10).cuda()\n\nq.put(tensor)\nout = q.get()\nprint(out)\n</code></pre>\n<p>One of two behaviors can occur:</p>\n<ul>\n<li>Crashing with an error that some \"CUDA arguments are incorrect\"</li>\n<li>The <code>out</code> tensor is valid but contains incorrect data (zeros instead<br>\nof ones).</li>\n</ul>\n<p>On master the first behavior occurs but I've noticed the second behavior<br>\nhappen on previous commits.</p>\n<p>This occurs because caching of the shared storages happens differently<br>\ndepending on the process. Call the process where <code>tensor</code> is created<br>\nthe \"originating process\". When a process grabs a tensor from the queue<br>\n<code>q</code>, there are two cases:</p>\n<ol>\n<li>The process is the \"originating process\". The cached storage is<br>\nsometimes the desired storage.</li>\n<li>The process is not the \"originating process\". The cached storage (if<br>\nit has been cached) is a storage that points to the base of the<br>\nallocation. One needs to add the offset to this storage to retrieve the<br>\ndesired storage.</li>\n</ol>\n<p>Case (2) is OK, but the code doesn't handle case (1) right now (the<br>\nrebuilding_storage_cuda code attempts to add the offset to whatever is<br>\ncached, leading to an incorrect storage pointer).</p>\n<h1>Solution: New CUDA storage caching strategy</h1>\n<p>After discussion with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>\n<p>All CUDA storages are cached into the new 'cuda_cache' that has two<br>\n(well, actually three) components: a 'real cache' and a 'base cache'.<br>\nAll CPU storages are cached into the original 'shared_cache'.</p>\n<p>Call a \"CUDA storage that processes wish to share\" a \"real storage\".<br>\n\"real storages\" are the storages that other processes want to and<br>\neventually will receive.</p>\n<p>Call a \"CUDA storage that points to the base of an allocation block\" a<br>\n\"base storage\". These are obtained by opening CUDA handles to the<br>\nbase of an allocation block, where multiple \"real storages\" may live.<br>\nThose \"real storages\" can be obtained by adding an offset to \"base storage\"</p>\n<p>\"base storages\" are cached into the \"base cache\" and \"real storages\" are<br>\ncached into the \"real cache\"</p>\n<p>When a process shares a cuda storage:</p>\n<ul>\n<li>If the cuda storage is from opening a received handle, find the handle<br>\nin the metadata cache and send the returned metadata.</li>\n<li>If the cuda storage is from allocating storage, get the handle, build<br>\nthe metadata, and send it.</li>\n</ul>\n<p>When a process receives a cuda storage:</p>\n<ul>\n<li>Check the caches:\n<ul>\n<li>Check the \"real cache\" for the real storage.</li>\n<li>If it's not there, look in the \"base cache\".</li>\n<li>If the storage exists in the \"base cache\", recreate the real storage by<br>\nadd it to the \"base storage\"</li>\n</ul>\n</li>\n<li>If it's not in the caches, recreate the real and base storages and cache<br>\nboth.</li>\n<li>In addition, if this process received a handle that it has opened<br>\n(either now or in the past), we cache the storage's metadata in the<br>\nmetadata cache. This is so that the process can access the handle<br>\nto send to other processes.<br>\nA process that didn't open a handle (the process that allocated the storage)<br>\ndoesn't cache the metadata.</li>\n</ul>\n<p>\"base storages\" are uniquely identified by their CUDA handle.<br>\n\"real storages\" are uniquely identified by (handle, offset).</p>", "body_text": "Fixes #7096.\nThis PR is an update to #7146\nProblem\nRight now, if one attempts to share a CUDA tensor and get it from a\nqueue from the same process it was saved in, a crash can occur.\nHere is some example code to demonstrate this:\nimport torch\nimport torch.multiprocessing as mp\nimport numpy.random as npr\n\nmp.set_start_method('spawn')\n\nq = mp.Queue()\nx = torch.randn(1000).cuda()\ntensor = torch.ones(10).cuda()\n\nq.put(tensor)\nout = q.get()\nprint(out)\n\nOne of two behaviors can occur:\n\nCrashing with an error that some \"CUDA arguments are incorrect\"\nThe out tensor is valid but contains incorrect data (zeros instead\nof ones).\n\nOn master the first behavior occurs but I've noticed the second behavior\nhappen on previous commits.\nThis occurs because caching of the shared storages happens differently\ndepending on the process. Call the process where tensor is created\nthe \"originating process\". When a process grabs a tensor from the queue\nq, there are two cases:\n\nThe process is the \"originating process\". The cached storage is\nsometimes the desired storage.\nThe process is not the \"originating process\". The cached storage (if\nit has been cached) is a storage that points to the base of the\nallocation. One needs to add the offset to this storage to retrieve the\ndesired storage.\n\nCase (2) is OK, but the code doesn't handle case (1) right now (the\nrebuilding_storage_cuda code attempts to add the offset to whatever is\ncached, leading to an incorrect storage pointer).\nSolution: New CUDA storage caching strategy\nAfter discussion with @colesbury\nAll CUDA storages are cached into the new 'cuda_cache' that has two\n(well, actually three) components: a 'real cache' and a 'base cache'.\nAll CPU storages are cached into the original 'shared_cache'.\nCall a \"CUDA storage that processes wish to share\" a \"real storage\".\n\"real storages\" are the storages that other processes want to and\neventually will receive.\nCall a \"CUDA storage that points to the base of an allocation block\" a\n\"base storage\". These are obtained by opening CUDA handles to the\nbase of an allocation block, where multiple \"real storages\" may live.\nThose \"real storages\" can be obtained by adding an offset to \"base storage\"\n\"base storages\" are cached into the \"base cache\" and \"real storages\" are\ncached into the \"real cache\"\nWhen a process shares a cuda storage:\n\nIf the cuda storage is from opening a received handle, find the handle\nin the metadata cache and send the returned metadata.\nIf the cuda storage is from allocating storage, get the handle, build\nthe metadata, and send it.\n\nWhen a process receives a cuda storage:\n\nCheck the caches:\n\nCheck the \"real cache\" for the real storage.\nIf it's not there, look in the \"base cache\".\nIf the storage exists in the \"base cache\", recreate the real storage by\nadd it to the \"base storage\"\n\n\nIf it's not in the caches, recreate the real and base storages and cache\nboth.\nIn addition, if this process received a handle that it has opened\n(either now or in the past), we cache the storage's metadata in the\nmetadata cache. This is so that the process can access the handle\nto send to other processes.\nA process that didn't open a handle (the process that allocated the storage)\ndoesn't cache the metadata.\n\n\"base storages\" are uniquely identified by their CUDA handle.\n\"real storages\" are uniquely identified by (handle, offset).", "body": "Fixes #7096.\r\n\r\nThis PR is an update to #7146\r\n\r\n# Problem\r\nRight now, if one attempts to share a CUDA tensor and get it from a\r\nqueue from the same process it was saved in, a crash can occur.\r\n\r\nHere is some example code to demonstrate this:\r\n\r\n```\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport numpy.random as npr\r\n\r\nmp.set_start_method('spawn')\r\n\r\nq = mp.Queue()\r\nx = torch.randn(1000).cuda()\r\ntensor = torch.ones(10).cuda()\r\n\r\nq.put(tensor)\r\nout = q.get()\r\nprint(out)\r\n```\r\n\r\nOne of two behaviors can occur:\r\n- Crashing with an error that some \"CUDA arguments are incorrect\"\r\n- The `out` tensor is valid but contains incorrect data (zeros instead\r\nof ones).\r\n\r\nOn master the first behavior occurs but I've noticed the second behavior\r\nhappen on previous commits.\r\n\r\nThis occurs because caching of the shared storages happens differently\r\ndepending on the process. Call the process where `tensor` is created\r\nthe \"originating process\". When a process grabs a tensor from the queue\r\n`q`, there are two cases:\r\n1) The process is the \"originating process\". The cached storage is\r\nsometimes the desired storage.\r\n2) The process is not the \"originating process\". The cached storage (if\r\nit has been cached) is a storage that points to the base of the\r\nallocation. One needs to add the offset to this storage to retrieve the\r\ndesired storage.\r\n\r\nCase (2) is OK, but the code doesn't handle case (1) right now (the\r\nrebuilding_storage_cuda code attempts to add the offset to whatever is\r\ncached, leading to an incorrect storage pointer).\r\n\r\n# Solution: New CUDA storage caching strategy\r\n\r\nAfter discussion with @colesbury \r\n\r\nAll CUDA storages are cached into the new 'cuda_cache' that has two \r\n(well, actually three) components: a 'real cache' and a 'base cache'. \r\nAll CPU storages are cached into the original 'shared_cache'.\r\n\r\nCall a \"CUDA storage that processes wish to share\" a \"real storage\".\r\n\"real storages\" are the storages that other processes want to and\r\neventually will receive.\r\n\r\nCall a \"CUDA storage that points to the base of an allocation block\" a\r\n\"base storage\". These are obtained by opening CUDA handles to the\r\nbase of an allocation block, where multiple \"real storages\" may live.\r\nThose \"real storages\" can be obtained by adding an offset to \"base storage\"\r\n\r\n\"base storages\" are cached into the \"base cache\" and \"real storages\" are\r\ncached into the \"real cache\"\r\n\r\nWhen a process shares a cuda storage:\r\n- If the cuda storage is from opening a received handle, find the handle\r\n  in the metadata cache and send the returned metadata.\r\n- If the cuda storage is from allocating storage, get the handle, build\r\n  the metadata, and send it.\r\n\r\nWhen a process receives a cuda storage:\r\n- Check the caches:\r\n  - Check the \"real cache\" for the real storage.\r\n  - If it's not there, look in the \"base cache\".\r\n  - If the storage exists in the \"base cache\", recreate the real storage by\r\n    add it to the \"base storage\"\r\n- If it's not in the caches, recreate the real and base storages and cache\r\n  both.\r\n- In addition, if this process received a handle that it has opened\r\n  (either now or in the past), we cache the storage's metadata in the\r\n  metadata cache. This is so that the process can access the handle\r\n  to send to other processes.\r\n  A process that didn't open a handle (the process that allocated the storage)\r\n  doesn't cache the metadata.\r\n\r\n\"base storages\" are uniquely identified by their CUDA handle.\r\n\"real storages\" are uniquely identified by (handle, offset)."}