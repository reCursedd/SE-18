{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13773", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13773/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13773/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13773/events", "html_url": "https://github.com/pytorch/pytorch/issues/13773", "id": 379196834, "node_id": "MDU6SXNzdWUzNzkxOTY4MzQ=", "number": 13773, "title": "Specifying `pos_weight` in F.binary_cross_entropy_with_logits may lead to NaN", "user": {"login": "jatentaki", "id": 22394841, "node_id": "MDQ6VXNlcjIyMzk0ODQx", "avatar_url": "https://avatars1.githubusercontent.com/u/22394841?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jatentaki", "html_url": "https://github.com/jatentaki", "followers_url": "https://api.github.com/users/jatentaki/followers", "following_url": "https://api.github.com/users/jatentaki/following{/other_user}", "gists_url": "https://api.github.com/users/jatentaki/gists{/gist_id}", "starred_url": "https://api.github.com/users/jatentaki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jatentaki/subscriptions", "organizations_url": "https://api.github.com/users/jatentaki/orgs", "repos_url": "https://api.github.com/users/jatentaki/repos", "events_url": "https://api.github.com/users/jatentaki/events{/privacy}", "received_events_url": "https://api.github.com/users/jatentaki/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-09T15:03:46Z", "updated_at": "2018-11-13T06:06:02Z", "closed_at": "2018-11-13T06:06:02Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>The weight of positive examples should default to <code>1.</code>. However, there are inputs for which the result is finite without specifying <code>pos_weight</code> and <code>nan</code> with <code>pos_weight=torch.tensor([1.])</code>.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch, math \n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\nlogits <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">0</span>., <span class=\"pl-k\">-</span><span class=\"pl-c1\">120</span>.])\ntarget <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">1</span>.])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> passes</span>\n<span class=\"pl-k\">assert</span> math.isfinite(\n    F.binary_cross_entropy_with_logits(logits, target).item()\n)\n\npos_weight <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> fails</span>\n<span class=\"pl-k\">assert</span> math.isfinite(\n    F.binary_cross_entropy_with_logits(logits, target, <span class=\"pl-v\">pos_weight</span><span class=\"pl-k\">=</span>pos_weight).item()\n)</pre></div>\n<p>Note that for moderate inputs (<code>-10</code> instead of <code>-120</code>) this works as expected</p>\n<h2>Expected behavior</h2>\n<p>Both cases should lead to the same result, because the latter is just explicitly passing the default value.</p>\n\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0.dev20181108<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Ubuntu 18.04.1 LTS<br>\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0<br>\nCMake version: version 3.10.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] pytorch-nightly-cpu       1.0.0.dev20181108     py3.6_cpu_0    pytorch<br>\n[conda] torch-dimcheck            0.0.1                     <br>\n[conda] torchvision               0.2.1                     </p>\n<h2>Additional context</h2>\n<p>I find the same issue when running on GPU</p>", "body_text": "\ud83d\udc1b Bug\n\nThe weight of positive examples should default to 1.. However, there are inputs for which the result is finite without specifying pos_weight and nan with pos_weight=torch.tensor([1.]).\nTo Reproduce\nSteps to reproduce the behavior:\nimport torch, math \nimport torch.nn.functional as F\n\nlogits = torch.tensor([0., -120.])\ntarget = torch.tensor([0., 1.])\n\n# passes\nassert math.isfinite(\n    F.binary_cross_entropy_with_logits(logits, target).item()\n)\n\npos_weight = torch.tensor([1., 1.])\n\n# fails\nassert math.isfinite(\n    F.binary_cross_entropy_with_logits(logits, target, pos_weight=pos_weight).item()\n)\nNote that for moderate inputs (-10 instead of -120) this works as expected\nExpected behavior\nBoth cases should lead to the same result, because the latter is just explicitly passing the default value.\n\nEnvironment\nPyTorch version: 1.0.0.dev20181108\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.10.2\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] pytorch-nightly-cpu       1.0.0.dev20181108     py3.6_cpu_0    pytorch\n[conda] torch-dimcheck            0.0.1                     \n[conda] torchvision               0.2.1                     \nAdditional context\nI find the same issue when running on GPU", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nThe weight of positive examples should default to `1.`. However, there are inputs for which the result is finite without specifying `pos_weight` and `nan` with `pos_weight=torch.tensor([1.])`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```python\r\nimport torch, math \r\nimport torch.nn.functional as F\r\n\r\nlogits = torch.tensor([0., -120.])\r\ntarget = torch.tensor([0., 1.])\r\n\r\n# passes\r\nassert math.isfinite(\r\n    F.binary_cross_entropy_with_logits(logits, target).item()\r\n)\r\n\r\npos_weight = torch.tensor([1., 1.])\r\n\r\n# fails\r\nassert math.isfinite(\r\n    F.binary_cross_entropy_with_logits(logits, target, pos_weight=pos_weight).item()\r\n)\r\n```\r\n\r\nNote that for moderate inputs (`-10` instead of `-120`) this works as expected\r\n## Expected behavior\r\n\r\nBoth cases should lead to the same result, because the latter is just explicitly passing the default value.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nPyTorch version: 1.0.0.dev20181108\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] pytorch-nightly-cpu       1.0.0.dev20181108     py3.6_cpu_0    pytorch\r\n[conda] torch-dimcheck            0.0.1                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n\r\n## Additional context\r\nI find the same issue when running on GPU"}