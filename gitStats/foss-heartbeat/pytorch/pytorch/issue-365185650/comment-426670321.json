{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/426670321", "html_url": "https://github.com/pytorch/pytorch/issues/12201#issuecomment-426670321", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12201", "id": 426670321, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjY3MDMyMQ==", "user": {"login": "jinserk", "id": 823222, "node_id": "MDQ6VXNlcjgyMzIyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/823222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinserk", "html_url": "https://github.com/jinserk", "followers_url": "https://api.github.com/users/jinserk/followers", "following_url": "https://api.github.com/users/jinserk/following{/other_user}", "gists_url": "https://api.github.com/users/jinserk/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinserk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinserk/subscriptions", "organizations_url": "https://api.github.com/users/jinserk/orgs", "repos_url": "https://api.github.com/users/jinserk/repos", "events_url": "https://api.github.com/users/jinserk/events{/privacy}", "received_events_url": "https://api.github.com/users/jinserk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-03T15:01:03Z", "updated_at": "2018-10-03T15:23:50Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a>,</p>\n<p>I have a little different question for the newly added <code>CTCLoss</code>. I think the 'elementwise_mean' option is the same to be averaging along the batch and the sequence length, but <code>nn.CTCLoss</code> produce almost 10x higher loss than <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6707363\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SeanNaren\">@SeanNaren</a> 's warp-ctc pytorch binding. In my understanding, the interface is almost the same, so I just replace the <code>warpctc_pytorch.CTCLoss</code> to <code>nn.CTCLoss</code>, but the result was so. Is it normal? The loss reduction in training is obviously not so smooth as warp-ctc did. I had tried Adam instead of SGD what I did use as the optimizer, which makes the convergence better, but still the slope is very low compared to the previous one.</p>\n<p>I have one more questions. If I want to use CUDNN, do I need to make the all elements of <code>input_lengths</code> to be T as described in Note? If so how I can figure out the different lengths of the input tensor sequences? Have the all input arguments to be cuda tensors?</p>", "body_text": "Hi @t-vi,\nI have a little different question for the newly added CTCLoss. I think the 'elementwise_mean' option is the same to be averaging along the batch and the sequence length, but nn.CTCLoss produce almost 10x higher loss than @SeanNaren 's warp-ctc pytorch binding. In my understanding, the interface is almost the same, so I just replace the warpctc_pytorch.CTCLoss to nn.CTCLoss, but the result was so. Is it normal? The loss reduction in training is obviously not so smooth as warp-ctc did. I had tried Adam instead of SGD what I did use as the optimizer, which makes the convergence better, but still the slope is very low compared to the previous one.\nI have one more questions. If I want to use CUDNN, do I need to make the all elements of input_lengths to be T as described in Note? If so how I can figure out the different lengths of the input tensor sequences? Have the all input arguments to be cuda tensors?", "body": "Hi @t-vi,\r\n\r\nI have a little different question for the newly added `CTCLoss`. I think the 'elementwise_mean' option is the same to be averaging along the batch and the sequence length, but `nn.CTCLoss` produce almost 10x higher loss than @SeanNaren 's warp-ctc pytorch binding. In my understanding, the interface is almost the same, so I just replace the `warpctc_pytorch.CTCLoss` to `nn.CTCLoss`, but the result was so. Is it normal? The loss reduction in training is obviously not so smooth as warp-ctc did. I had tried Adam instead of SGD what I did use as the optimizer, which makes the convergence better, but still the slope is very low compared to the previous one.\r\n\r\nI have one more questions. If I want to use CUDNN, do I need to make the all elements of `input_lengths` to be T as described in Note? If so how I can figure out the different lengths of the input tensor sequences? Have the all input arguments to be cuda tensors?\r\n"}