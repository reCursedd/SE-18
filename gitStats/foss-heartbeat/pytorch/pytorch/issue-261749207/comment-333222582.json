{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/333222582", "html_url": "https://github.com/pytorch/pytorch/issues/2904#issuecomment-333222582", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2904", "id": 333222582, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzIyMjU4Mg==", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-29T19:51:15Z", "updated_at": "2017-09-29T19:51:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a><br>\nIt might make the code cleaner.</p>\n<p>e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.FloatTensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\n<span class=\"pl-k\">if</span> torch.cuda.is_available():\n    a <span class=\"pl-k\">=</span> Variable(a).cuda()\n<span class=\"pl-k\">else</span>:\n    a <span class=\"pl-k\">=</span> Variable(a)\nb <span class=\"pl-k\">=</span> f(a)\n<span class=\"pl-k\">if</span> torch.cuda.is_available():\n    <span class=\"pl-c1\">print</span>(b.grad.cpu().numpy())\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">print</span>(b.grad.numpy())</pre></div>\n<p>can be replaced by</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.FloatTensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\nb <span class=\"pl-k\">=</span> f(utils.tensor_wrapper(a, <span class=\"pl-v\">to_Variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">to_GPU</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n<span class=\"pl-c1\">print</span>(utils.tensor_unwrapper(b.grad, <span class=\"pl-v\">to_Numpy</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">to_CPU</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))</pre></div>\n<p>Another convenience might be automatic switch between CPU/GPU mode without changing every line which contains .cpu() or cuda().</p>\n<p>Or maybe not :) ?</p>", "body_text": "@soumith\nIt might make the code cleaner.\ne.g.\na = torch.FloatTensor([1, 2, 3])\nif torch.cuda.is_available():\n    a = Variable(a).cuda()\nelse:\n    a = Variable(a)\nb = f(a)\nif torch.cuda.is_available():\n    print(b.grad.cpu().numpy())\nelse:\n    print(b.grad.numpy())\ncan be replaced by\na = torch.FloatTensor([1, 2, 3])\nb = f(utils.tensor_wrapper(a, to_Variable=True, to_GPU=True))\nprint(utils.tensor_unwrapper(b.grad, to_Numpy=True, to_CPU=True))\nAnother convenience might be automatic switch between CPU/GPU mode without changing every line which contains .cpu() or cuda().\nOr maybe not :) ?", "body": "@soumith \r\nIt might make the code cleaner. \r\n\r\ne.g. \r\n\r\n```python\r\na = torch.FloatTensor([1, 2, 3])\r\nif torch.cuda.is_available():\r\n    a = Variable(a).cuda()\r\nelse:\r\n    a = Variable(a)\r\nb = f(a)\r\nif torch.cuda.is_available():\r\n    print(b.grad.cpu().numpy())\r\nelse:\r\n    print(b.grad.numpy())\r\n```\r\n\r\ncan be replaced by \r\n```python\r\na = torch.FloatTensor([1, 2, 3])\r\nb = f(utils.tensor_wrapper(a, to_Variable=True, to_GPU=True))\r\nprint(utils.tensor_unwrapper(b.grad, to_Numpy=True, to_CPU=True))\r\n```\r\n\r\nAnother convenience might be automatic switch between CPU/GPU mode without changing every line which contains .cpu() or cuda(). \r\n\r\nOr maybe not :) ?"}