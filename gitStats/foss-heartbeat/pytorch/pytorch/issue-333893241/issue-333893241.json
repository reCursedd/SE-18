{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8679", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8679/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8679/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8679/events", "html_url": "https://github.com/pytorch/pytorch/pull/8679", "id": 333893241, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk1OTk5NjM2", "number": 8679, "title": "[JIT] Adds fp16 support to the jit", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-06-20T01:29:20Z", "updated_at": "2018-11-23T15:45:56Z", "closed_at": "2018-06-21T22:14:52Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8679", "html_url": "https://github.com/pytorch/pytorch/pull/8679", "diff_url": "https://github.com/pytorch/pytorch/pull/8679.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8679.patch"}, "body_html": "<p>This PR adds support for fp16 tensors to the jit, and it adds a test to ensure they're working properly.</p>\n<p>Half tensors are treated just like in reductions. Half inputs are immediately upconverted to float, mathematical operations are performed only on floats, and floats are downconverted to half when writing to half outputs.</p>\n<p>To add support for half tensors I extended the (renamed) \"SupportedTypes\" to include halfs, created a snippet with the appropriate half conversions defined and included it as a string literal (this snippet courtesy of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a>), then check for half types and write the appropriate access line.</p>\n<p>For testing I added a new test and (very slightly) refactored three existing tests to reuse the functions they defined. Rerunning each test in test_jit.py for every datatype seems excessive, especially since many of the functions deal with graph structure. Further, half types are different from other data types since we cannot simply run a half tensor through an unfused version of the same operations and expect the same final result as the fused version. Instead, a float tensor that's representable in half must be passed through the unfused network while an equivalent tensor in half is passed through the fused model, and the results of the unfused network cast to half for comparison. This process mimics the conversions done by the fused kernel and is tricky to merge with existing code paths without greatly complicating them.</p>", "body_text": "This PR adds support for fp16 tensors to the jit, and it adds a test to ensure they're working properly.\nHalf tensors are treated just like in reductions. Half inputs are immediately upconverted to float, mathematical operations are performed only on floats, and floats are downconverted to half when writing to half outputs.\nTo add support for half tensors I extended the (renamed) \"SupportedTypes\" to include halfs, created a snippet with the appropriate half conversions defined and included it as a string literal (this snippet courtesy of @ngimel), then check for half types and write the appropriate access line.\nFor testing I added a new test and (very slightly) refactored three existing tests to reuse the functions they defined. Rerunning each test in test_jit.py for every datatype seems excessive, especially since many of the functions deal with graph structure. Further, half types are different from other data types since we cannot simply run a half tensor through an unfused version of the same operations and expect the same final result as the fused version. Instead, a float tensor that's representable in half must be passed through the unfused network while an equivalent tensor in half is passed through the fused model, and the results of the unfused network cast to half for comparison. This process mimics the conversions done by the fused kernel and is tricky to merge with existing code paths without greatly complicating them.", "body": "This PR adds support for fp16 tensors to the jit, and it adds a test to ensure they're working properly.\r\n\r\nHalf tensors are treated just like in reductions. Half inputs are immediately upconverted to float, mathematical operations are performed only on floats, and floats are downconverted to half when writing to half outputs.\r\n\r\nTo add support for half tensors I extended the (renamed) \"SupportedTypes\" to include halfs, created a snippet with the appropriate half conversions defined and included it as a string literal (this snippet courtesy of @ngimel), then check for half types and write the appropriate access line.\r\n\r\nFor testing I added a new test and (very slightly) refactored three existing tests to reuse the functions they defined. Rerunning each test in test_jit.py for every datatype seems excessive, especially since many of the functions deal with graph structure. Further, half types are different from other data types since we cannot simply run a half tensor through an unfused version of the same operations and expect the same final result as the fused version. Instead, a float tensor that's representable in half must be passed through the unfused network while an equivalent tensor in half is passed through the fused model, and the results of the unfused network cast to half for comparison. This process mimics the conversions done by the fused kernel and is tricky to merge with existing code paths without greatly complicating them. \r\n\r\n\r\n"}