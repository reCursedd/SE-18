{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8080", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8080/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8080/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8080/events", "html_url": "https://github.com/pytorch/pytorch/issues/8080", "id": 328811696, "node_id": "MDU6SXNzdWUzMjg4MTE2OTY=", "number": 8080, "title": "Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True", "user": {"login": "nikcheerla", "id": 7505423, "node_id": "MDQ6VXNlcjc1MDU0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7505423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikcheerla", "html_url": "https://github.com/nikcheerla", "followers_url": "https://api.github.com/users/nikcheerla/followers", "following_url": "https://api.github.com/users/nikcheerla/following{/other_user}", "gists_url": "https://api.github.com/users/nikcheerla/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikcheerla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikcheerla/subscriptions", "organizations_url": "https://api.github.com/users/nikcheerla/orgs", "repos_url": "https://api.github.com/users/nikcheerla/repos", "events_url": "https://api.github.com/users/nikcheerla/events{/privacy}", "received_events_url": "https://api.github.com/users/nikcheerla/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-06-03T08:53:51Z", "updated_at": "2018-06-15T21:38:51Z", "closed_at": "2018-06-15T16:32:17Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I am trying to work with gradients using the <code>F.grid_sample</code> function from STNs. The issue I have is that sometimes the output of <code>torch.autograd.grad()</code> is a tensor with <code>requires_grad() = True</code>, and sometimes it has <code>requires_grad() = False</code> instead (based on what seem to be trivial modifications to the code). I assumed that when using <code>create_graph=True</code>, the output would always be a differentiable tensor? Was this assumption incorrect? Could this be a bug in <code>F.grid_sample()</code> or in <code>autograd</code>, or am I somehow implementing this wrong?</p>\n<h2>Code example</h2>\n<p>I've provided a minimal example of the issue here.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\na <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>).requires_grad_().float()\nb <span class=\"pl-k\">=</span> torch.tensor(<span class=\"pl-c1\">0.01</span>).requires_grad_().float()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>):\n\tgrid <span class=\"pl-k\">=</span> F.affine_grid(torch.eye(<span class=\"pl-c1\">3</span>).unsqueeze(<span class=\"pl-c1\">0</span>)[:, <span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">2</span>], \\\n\t\t\t\t<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>torch.Size((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>)))\n\t<span class=\"pl-k\">return</span> F.grid_sample(a<span class=\"pl-k\">+</span>b, grid).mean()\n\nf_val <span class=\"pl-k\">=</span> f(a, b)\n\n(dfda,) <span class=\"pl-k\">=</span> torch.autograd.grad(f_val, [a], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">print</span> (<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">df/da=</span><span class=\"pl-c1\">{</span>dfda.shape<span class=\"pl-c1\">}</span><span class=\"pl-s\">, requires_grad=</span><span class=\"pl-c1\">{</span>dfda.requires_grad<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">g</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>):\n\tgrid <span class=\"pl-k\">=</span> F.affine_grid(torch.eye(<span class=\"pl-c1\">3</span>).unsqueeze(<span class=\"pl-c1\">0</span>)[:, <span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">2</span>], \\\n\t\t\t\t<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>torch.Size((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>)))\n\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> Basically the same function, just with multiplication instead of addition</span>\n\t<span class=\"pl-k\">return</span> F.grid_sample(a<span class=\"pl-k\">*</span>b, grid).mean() \n\ng_val <span class=\"pl-k\">=</span> g(a, b)\n\n(dgda,) <span class=\"pl-k\">=</span> torch.autograd.grad(g_val, [a], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">print</span> (<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">dg/da=</span><span class=\"pl-c1\">{</span>dgda.shape<span class=\"pl-c1\">}</span><span class=\"pl-s\">, requires_grad=</span><span class=\"pl-c1\">{</span>dgda.requires_grad<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>)</pre></div>\n<p>Output:</p>\n<pre><code>df/da=torch.Size([1, 3, 32, 32]), requires_grad=False\ndg/da=torch.Size([1, 3, 32, 32]), requires_grad=True\n</code></pre>\n<p>The functions <code>f()</code> and <code>g()</code> are almost identical, except one has multiplication instead of addition, yet the outputs differ in the requires_grad property.</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Mac OSX 10.13.2<br>\nGCC version: Could not collect<br>\nCMake version: version 3.5.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.13.3)<br>\n[pip] numpydoc (0.6.0)<br>\n[pip] torch (0.4.0)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.0           py36_cuda0.0_cudnn0.0_1    pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nI am trying to work with gradients using the F.grid_sample function from STNs. The issue I have is that sometimes the output of torch.autograd.grad() is a tensor with requires_grad() = True, and sometimes it has requires_grad() = False instead (based on what seem to be trivial modifications to the code). I assumed that when using create_graph=True, the output would always be a differentiable tensor? Was this assumption incorrect? Could this be a bug in F.grid_sample() or in autograd, or am I somehow implementing this wrong?\nCode example\nI've provided a minimal example of the issue here.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\na = torch.randn(1, 3, 32, 32).requires_grad_().float()\nb = torch.tensor(0.01).requires_grad_().float()\n\ndef f(a, b):\n\tgrid = F.affine_grid(torch.eye(3).unsqueeze(0)[:, 0:2], \\\n\t\t\t\tsize=torch.Size((1, 2, 64, 64)))\n\treturn F.grid_sample(a+b, grid).mean()\n\nf_val = f(a, b)\n\n(dfda,) = torch.autograd.grad(f_val, [a], create_graph=True, retain_graph=True)\nprint (f\"df/da={dfda.shape}, requires_grad={dfda.requires_grad}\")\n\ndef g(a, b):\n\tgrid = F.affine_grid(torch.eye(3).unsqueeze(0)[:, 0:2], \\\n\t\t\t\tsize=torch.Size((1, 2, 64, 64)))\n\n\t# Basically the same function, just with multiplication instead of addition\n\treturn F.grid_sample(a*b, grid).mean() \n\ng_val = g(a, b)\n\n(dgda,) = torch.autograd.grad(g_val, [a], create_graph=True, retain_graph=True)\nprint (f\"dg/da={dgda.shape}, requires_grad={dgda.requires_grad}\")\nOutput:\ndf/da=torch.Size([1, 3, 32, 32]), requires_grad=False\ndg/da=torch.Size([1, 3, 32, 32]), requires_grad=True\n\nThe functions f() and g() are almost identical, except one has multiplication instead of addition, yet the outputs differ in the requires_grad property.\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Mac OSX 10.13.2\nGCC version: Could not collect\nCMake version: version 3.5.2\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip] numpy (1.13.3)\n[pip] numpydoc (0.6.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda0.0_cudnn0.0_1    pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\nI am trying to work with gradients using the `F.grid_sample` function from STNs. The issue I have is that sometimes the output of `torch.autograd.grad()` is a tensor with `requires_grad() = True`, and sometimes it has `requires_grad() = False` instead (based on what seem to be trivial modifications to the code). I assumed that when using `create_graph=True`, the output would always be a differentiable tensor? Was this assumption incorrect? Could this be a bug in `F.grid_sample()` or in `autograd`, or am I somehow implementing this wrong?\r\n\r\n## Code example\r\n\r\nI've provided a minimal example of the issue here.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\na = torch.randn(1, 3, 32, 32).requires_grad_().float()\r\nb = torch.tensor(0.01).requires_grad_().float()\r\n\r\ndef f(a, b):\r\n\tgrid = F.affine_grid(torch.eye(3).unsqueeze(0)[:, 0:2], \\\r\n\t\t\t\tsize=torch.Size((1, 2, 64, 64)))\r\n\treturn F.grid_sample(a+b, grid).mean()\r\n\r\nf_val = f(a, b)\r\n\r\n(dfda,) = torch.autograd.grad(f_val, [a], create_graph=True, retain_graph=True)\r\nprint (f\"df/da={dfda.shape}, requires_grad={dfda.requires_grad}\")\r\n\r\ndef g(a, b):\r\n\tgrid = F.affine_grid(torch.eye(3).unsqueeze(0)[:, 0:2], \\\r\n\t\t\t\tsize=torch.Size((1, 2, 64, 64)))\r\n\r\n\t# Basically the same function, just with multiplication instead of addition\r\n\treturn F.grid_sample(a*b, grid).mean() \r\n\r\ng_val = g(a, b)\r\n\r\n(dgda,) = torch.autograd.grad(g_val, [a], create_graph=True, retain_graph=True)\r\nprint (f\"dg/da={dgda.shape}, requires_grad={dgda.requires_grad}\")\r\n```\r\nOutput:\r\n```\r\ndf/da=torch.Size([1, 3, 32, 32]), requires_grad=False\r\ndg/da=torch.Size([1, 3, 32, 32]), requires_grad=True\r\n```\r\nThe functions `f()` and `g()` are almost identical, except one has multiplication instead of addition, yet the outputs differ in the requires_grad property.\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.2\r\nGCC version: Could not collect\r\nCMake version: version 3.5.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.3)\r\n[pip] numpydoc (0.6.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda0.0_cudnn0.0_1    pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch"}