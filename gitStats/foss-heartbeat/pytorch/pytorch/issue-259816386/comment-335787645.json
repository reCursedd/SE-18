{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/335787645", "html_url": "https://github.com/pytorch/pytorch/issues/2830#issuecomment-335787645", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2830", "id": 335787645, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTc4NzY0NQ==", "user": {"login": "JianyuZhan", "id": 649042, "node_id": "MDQ6VXNlcjY0OTA0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/649042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JianyuZhan", "html_url": "https://github.com/JianyuZhan", "followers_url": "https://api.github.com/users/JianyuZhan/followers", "following_url": "https://api.github.com/users/JianyuZhan/following{/other_user}", "gists_url": "https://api.github.com/users/JianyuZhan/gists{/gist_id}", "starred_url": "https://api.github.com/users/JianyuZhan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JianyuZhan/subscriptions", "organizations_url": "https://api.github.com/users/JianyuZhan/orgs", "repos_url": "https://api.github.com/users/JianyuZhan/repos", "events_url": "https://api.github.com/users/JianyuZhan/events{/privacy}", "received_events_url": "https://api.github.com/users/JianyuZhan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T12:04:38Z", "updated_at": "2017-10-11T12:08:56Z", "author_association": "NONE", "body_html": "<p>Sorry,  I missed the reply email.</p>\n<p>I am afraid that I am unable to provide a reproducer now.  It is a work I am doing for the OpenNMT-py project:<a href=\"https://github.com/OpenNMT/OpenNMT-py\">https://github.com/OpenNMT/OpenNMT-py</a>, trying to use <code>lr_scheduler</code> for doing lr update. And I encoutered this problem when testing the <code>resume a suspended training</code> case.  So I factored out the code skeleton about this problem above.</p>\n<p>I've tried several methods, including tricks like what <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13904086\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hefeicyp\">@hefeicyp</a> suggests, but it still happens.</p>\n<p>Per my analysis, it is because the previous training was done on gpu, so when saving the <code>optimizer.state_dict</code>, the stored states(tensors) are of <code>cuda</code> version. During resuming, when we load the saved  optimizer, <code>load_state_dict()</code> loads this <code>cuda</code> version to cpu(the model(<code>nn.Module</code>) can be moved to gpu easily, but <code>torch.optimizer</code> seems lacking this ability?) , so this problem emerges.</p>", "body_text": "Sorry,  I missed the reply email.\nI am afraid that I am unable to provide a reproducer now.  It is a work I am doing for the OpenNMT-py project:https://github.com/OpenNMT/OpenNMT-py, trying to use lr_scheduler for doing lr update. And I encoutered this problem when testing the resume a suspended training case.  So I factored out the code skeleton about this problem above.\nI've tried several methods, including tricks like what @hefeicyp suggests, but it still happens.\nPer my analysis, it is because the previous training was done on gpu, so when saving the optimizer.state_dict, the stored states(tensors) are of cuda version. During resuming, when we load the saved  optimizer, load_state_dict() loads this cuda version to cpu(the model(nn.Module) can be moved to gpu easily, but torch.optimizer seems lacking this ability?) , so this problem emerges.", "body": "Sorry,  I missed the reply email.\r\n\r\nI am afraid that I am unable to provide a reproducer now.  It is a work I am doing for the OpenNMT-py project:https://github.com/OpenNMT/OpenNMT-py, trying to use `lr_scheduler` for doing lr update. And I encoutered this problem when testing the `resume a suspended training` case.  So I factored out the code skeleton about this problem above.\r\n\r\nI've tried several methods, including tricks like what @hefeicyp suggests, but it still happens.\r\n\r\nPer my analysis, it is because the previous training was done on gpu, so when saving the `optimizer.state_dict`, the stored states(tensors) are of `cuda` version. During resuming, when we load the saved  optimizer, `load_state_dict()` loads this `cuda` version to cpu(the model(`nn.Module`) can be moved to gpu easily, but `torch.optimizer` seems lacking this ability?) , so this problem emerges.   "}