{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3174", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3174/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3174/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3174/events", "html_url": "https://github.com/pytorch/pytorch/issues/3174", "id": 266768227, "node_id": "MDU6SXNzdWUyNjY3NjgyMjc=", "number": 3174, "title": "Data parallel failure when using add_module", "user": {"login": "zxytim", "id": 888911, "node_id": "MDQ6VXNlcjg4ODkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/888911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zxytim", "html_url": "https://github.com/zxytim", "followers_url": "https://api.github.com/users/zxytim/followers", "following_url": "https://api.github.com/users/zxytim/following{/other_user}", "gists_url": "https://api.github.com/users/zxytim/gists{/gist_id}", "starred_url": "https://api.github.com/users/zxytim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zxytim/subscriptions", "organizations_url": "https://api.github.com/users/zxytim/orgs", "repos_url": "https://api.github.com/users/zxytim/repos", "events_url": "https://api.github.com/users/zxytim/events{/privacy}", "received_events_url": "https://api.github.com/users/zxytim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-19T08:57:26Z", "updated_at": "2017-10-19T09:45:39Z", "closed_at": "2017-10-19T09:45:39Z", "author_association": "NONE", "body_html": "<p>code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python3</span>\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.subs <span class=\"pl-k\">=</span> [\n            nn.Sequential(\n                nn.Linear(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>), nn.ReLU(),\n                nn.Linear(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>), nn.ReLU(),\n            )\n        ]\n\n        <span class=\"pl-k\">for</span> i, s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">self</span>.subs):\n            <span class=\"pl-c1\">self</span>.add_module(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sub_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(i), s)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> [s(x) <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.subs]\n\n\nm <span class=\"pl-k\">=</span> Model()\nm <span class=\"pl-k\">=</span> nn.DataParallel(m, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\nm.cuda()\n\ninpvar <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">2</span>)).cuda()\nout <span class=\"pl-k\">=</span> m(inpvar)</pre></div>\n<p>output:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ ./test_data_parallel.py                                                                                                                                                  1\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./test_data_parallel.py<span class=\"pl-pds\">\"</span></span>, line 31, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    out = m(inpvar)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 224, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py<span class=\"pl-pds\">\"</span></span>, line 60, <span class=\"pl-k\">in</span> forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py<span class=\"pl-pds\">\"</span></span>, line 70, <span class=\"pl-k\">in</span> parallel_apply\n    <span class=\"pl-k\">return</span> parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py<span class=\"pl-pds\">\"</span></span>, line 75, <span class=\"pl-k\">in</span> parallel_apply\n    raise output\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py<span class=\"pl-pds\">\"</span></span>, line 50, <span class=\"pl-k\">in</span> _worker\n    output = module(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 224, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./test_data_parallel.py<span class=\"pl-pds\">\"</span></span>, line 23, <span class=\"pl-k\">in</span> forward\n    <span class=\"pl-k\">return</span> [s(x) <span class=\"pl-k\">for</span> <span class=\"pl-smi\">s</span> <span class=\"pl-k\">in</span> self.subs]\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./test_data_parallel.py<span class=\"pl-pds\">\"</span></span>, line 23, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>listcomp<span class=\"pl-k\">&gt;</span>\n    <span class=\"pl-k\">return</span> [s(x) <span class=\"pl-k\">for</span> <span class=\"pl-smi\">s</span> <span class=\"pl-k\">in</span> self.subs]\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 224, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/container.py<span class=\"pl-pds\">\"</span></span>, line 67, <span class=\"pl-k\">in</span> forward\n    input = module(input)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 224, <span class=\"pl-k\">in</span> __call__\n    result = self.forward(<span class=\"pl-k\">*</span>input, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py<span class=\"pl-pds\">\"</span></span>, line 53, <span class=\"pl-k\">in</span> forward\n    <span class=\"pl-k\">return</span> F.linear(input, self.weight, self.bias)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/nn/functional.py<span class=\"pl-pds\">\"</span></span>, line 554, <span class=\"pl-k\">in</span> linear\n    <span class=\"pl-k\">return</span> torch.addmm(bias, input, <span class=\"pl-en\">weight.t</span>())\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/variable.py<span class=\"pl-pds\">\"</span></span>, line 924, <span class=\"pl-k\">in</span> addmm\n    <span class=\"pl-k\">return</span> cls._blas(Addmm, args, False)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/variable.py<span class=\"pl-pds\">\"</span></span>, line 920, <span class=\"pl-k\">in</span> _blas\n    <span class=\"pl-k\">return</span> cls.apply(<span class=\"pl-k\">*</span>(tensors + (alpha, beta, inplace)))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/_functions/blas.py<span class=\"pl-pds\">\"</span></span>, line 26, <span class=\"pl-k\">in</span> forward\n    matrix1, matrix2, out=output)\nRuntimeError: arguments are located on different GPUs at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:232\n</pre></div>", "body_text": "code:\n#!/usr/bin/env python3\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.subs = [\n            nn.Sequential(\n                nn.Linear(2, 3), nn.ReLU(),\n                nn.Linear(3, 1), nn.ReLU(),\n            )\n        ]\n\n        for i, s in enumerate(self.subs):\n            self.add_module('sub_{}'.format(i), s)\n\n    def forward(self, x):\n        return [s(x) for s in self.subs]\n\n\nm = Model()\nm = nn.DataParallel(m, device_ids=[0, 1])\nm.cuda()\n\ninpvar = Variable(torch.zeros(8, 2)).cuda()\nout = m(inpvar)\noutput:\n$ ./test_data_parallel.py                                                                                                                                                  1\nTraceback (most recent call last):\n  File \"./test_data_parallel.py\", line 31, in <module>\n    out = m(inpvar)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 60, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 70, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 75, in parallel_apply\n    raise output\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 50, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"./test_data_parallel.py\", line 23, in forward\n    return [s(x) for s in self.subs]\n  File \"./test_data_parallel.py\", line 23, in <listcomp>\n    return [s(x) for s in self.subs]\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/container.py\", line 67, in forward\n    input = module(input)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py\", line 53, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/functional.py\", line 554, in linear\n    return torch.addmm(bias, input, weight.t())\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/variable.py\", line 924, in addmm\n    return cls._blas(Addmm, args, False)\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/variable.py\", line 920, in _blas\n    return cls.apply(*(tensors + (alpha, beta, inplace)))\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/_functions/blas.py\", line 26, in forward\n    matrix1, matrix2, out=output)\nRuntimeError: arguments are located on different GPUs at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:232", "body": "code:\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.subs = [\r\n            nn.Sequential(\r\n                nn.Linear(2, 3), nn.ReLU(),\r\n                nn.Linear(3, 1), nn.ReLU(),\r\n            )\r\n        ]\r\n\r\n        for i, s in enumerate(self.subs):\r\n            self.add_module('sub_{}'.format(i), s)\r\n\r\n    def forward(self, x):\r\n        return [s(x) for s in self.subs]\r\n\r\n\r\nm = Model()\r\nm = nn.DataParallel(m, device_ids=[0, 1])\r\nm.cuda()\r\n\r\ninpvar = Variable(torch.zeros(8, 2)).cuda()\r\nout = m(inpvar)\r\n```\r\n\r\noutput:\r\n```shell\r\n$ ./test_data_parallel.py                                                                                                                                                  1\r\nTraceback (most recent call last):\r\n  File \"./test_data_parallel.py\", line 31, in <module>\r\n    out = m(inpvar)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 60, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 70, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 75, in parallel_apply\r\n    raise output\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 50, in _worker\r\n    output = module(*input, **kwargs)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"./test_data_parallel.py\", line 23, in forward\r\n    return [s(x) for s in self.subs]\r\n  File \"./test_data_parallel.py\", line 23, in <listcomp>\r\n    return [s(x) for s in self.subs]\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/container.py\", line 67, in forward\r\n    input = module(input)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py\", line 53, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/nn/functional.py\", line 554, in linear\r\n    return torch.addmm(bias, input, weight.t())\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/variable.py\", line 924, in addmm\r\n    return cls._blas(Addmm, args, False)\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/variable.py\", line 920, in _blas\r\n    return cls.apply(*(tensors + (alpha, beta, inplace)))\r\n  File \"/home/zxy/.local/lib/python3.5/site-packages/torch/autograd/_functions/blas.py\", line 26, in forward\r\n    matrix1, matrix2, out=output)\r\nRuntimeError: arguments are located on different GPUs at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:232\r\n\r\n```"}