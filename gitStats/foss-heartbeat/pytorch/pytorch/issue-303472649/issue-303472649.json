{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5633", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5633/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5633/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5633/events", "html_url": "https://github.com/pytorch/pytorch/issues/5633", "id": 303472649, "node_id": "MDU6SXNzdWUzMDM0NzI2NDk=", "number": 5633, "title": "Client and Server fail on mixed endian machines", "user": {"login": "seelam", "id": 5160365, "node_id": "MDQ6VXNlcjUxNjAzNjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/5160365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seelam", "html_url": "https://github.com/seelam", "followers_url": "https://api.github.com/users/seelam/followers", "following_url": "https://api.github.com/users/seelam/following{/other_user}", "gists_url": "https://api.github.com/users/seelam/gists{/gist_id}", "starred_url": "https://api.github.com/users/seelam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seelam/subscriptions", "organizations_url": "https://api.github.com/users/seelam/orgs", "repos_url": "https://api.github.com/users/seelam/repos", "events_url": "https://api.github.com/users/seelam/events{/privacy}", "received_events_url": "https://api.github.com/users/seelam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-08T13:05:25Z", "updated_at": "2018-03-09T04:14:01Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>We are trying to use use Pytorch master on a big endian machine and the workers on a little endian machines. When both master and workers are on the same platform, the below program works fine but it fails when they are on different platforms. We think there is a bug in the <code>dataChannel--&gt;init()</code> method.</p>\n<p>Below is a sample master and worker program to reproduce the problem.</p>\n<p>master.py</p>\n<pre><code>#!/usr/bin/env python3\nimport os\nimport torch\nimport torch.distributed as dist\ndist.init_process_group(backend='tcp', rank=0, world_size=2)\nprint('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\ntensor=torch.IntTensor([0x1234])\nprint('Send ', tensor)\ndist.send(tensor=tensor, dst=1)\n</code></pre>\n<p>worker.py</p>\n<pre><code>#!/usr/bin/env python3\nimport os\nimport torch\nimport torch.distributed as dist\ndist.init_process_group(backend='tcp', rank=1, world_size=2)\nprint('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\ntensor=torch.IntTensor([0])\ndist.recv(tensor=tensor, src=0)\nprint('Receive ', tensor)\n</code></pre>\n<p>Here is the error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"./run-master.py\", line 5, in &lt;module&gt;\n    dist.init_process_group(backend='tcp', rank=0, world_size=2)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 95, in init_process_group\n    group_name, rank)\nRuntimeError: std::bad_alloc at /home/ubuntu/pytorch/torch/lib/THD/process_group/General.cpp:17\n</code></pre>\n<p>Trace of the General.cpp points to this code:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THD/process_group/General.cpp#L17\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THD/process_group/General.cpp#L17</a></p>\n<p>Is this expected to work? Appreciate any pointers.</p>", "body_text": "We are trying to use use Pytorch master on a big endian machine and the workers on a little endian machines. When both master and workers are on the same platform, the below program works fine but it fails when they are on different platforms. We think there is a bug in the dataChannel-->init() method.\nBelow is a sample master and worker program to reproduce the problem.\nmaster.py\n#!/usr/bin/env python3\nimport os\nimport torch\nimport torch.distributed as dist\ndist.init_process_group(backend='tcp', rank=0, world_size=2)\nprint('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\ntensor=torch.IntTensor([0x1234])\nprint('Send ', tensor)\ndist.send(tensor=tensor, dst=1)\n\nworker.py\n#!/usr/bin/env python3\nimport os\nimport torch\nimport torch.distributed as dist\ndist.init_process_group(backend='tcp', rank=1, world_size=2)\nprint('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\ntensor=torch.IntTensor([0])\ndist.recv(tensor=tensor, src=0)\nprint('Receive ', tensor)\n\nHere is the error:\nTraceback (most recent call last):\n  File \"./run-master.py\", line 5, in <module>\n    dist.init_process_group(backend='tcp', rank=0, world_size=2)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 95, in init_process_group\n    group_name, rank)\nRuntimeError: std::bad_alloc at /home/ubuntu/pytorch/torch/lib/THD/process_group/General.cpp:17\n\nTrace of the General.cpp points to this code:\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THD/process_group/General.cpp#L17\nIs this expected to work? Appreciate any pointers.", "body": "We are trying to use use Pytorch master on a big endian machine and the workers on a little endian machines. When both master and workers are on the same platform, the below program works fine but it fails when they are on different platforms. We think there is a bug in the `dataChannel-->init()` method. \r\n\r\nBelow is a sample master and worker program to reproduce the problem.\r\n\r\nmaster.py\r\n\r\n```\r\n#!/usr/bin/env python3\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\ndist.init_process_group(backend='tcp', rank=0, world_size=2)\r\nprint('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\r\ntensor=torch.IntTensor([0x1234])\r\nprint('Send ', tensor)\r\ndist.send(tensor=tensor, dst=1)\r\n```\r\n\r\nworker.py\r\n\r\n```\r\n#!/usr/bin/env python3\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\ndist.init_process_group(backend='tcp', rank=1, world_size=2)\r\nprint('Hello from process {} (out of {})!'.format(dist.get_rank(), dist.get_world_size()))\r\ntensor=torch.IntTensor([0])\r\ndist.recv(tensor=tensor, src=0)\r\nprint('Receive ', tensor)\r\n```\r\n\r\nHere is the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./run-master.py\", line 5, in <module>\r\n    dist.init_process_group(backend='tcp', rank=0, world_size=2)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 95, in init_process_group\r\n    group_name, rank)\r\nRuntimeError: std::bad_alloc at /home/ubuntu/pytorch/torch/lib/THD/process_group/General.cpp:17\r\n```\r\n\r\nTrace of the General.cpp points to this code:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THD/process_group/General.cpp#L17\r\n\r\nIs this expected to work? Appreciate any pointers.\r\n"}