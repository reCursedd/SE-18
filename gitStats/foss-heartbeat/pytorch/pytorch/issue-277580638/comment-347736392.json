{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/347736392", "html_url": "https://github.com/pytorch/pytorch/issues/3931#issuecomment-347736392", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3931", "id": 347736392, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzczNjM5Mg==", "user": {"login": "MaigoAkisame", "id": 6993299, "node_id": "MDQ6VXNlcjY5OTMyOTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6993299?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaigoAkisame", "html_url": "https://github.com/MaigoAkisame", "followers_url": "https://api.github.com/users/MaigoAkisame/followers", "following_url": "https://api.github.com/users/MaigoAkisame/following{/other_user}", "gists_url": "https://api.github.com/users/MaigoAkisame/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaigoAkisame/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaigoAkisame/subscriptions", "organizations_url": "https://api.github.com/users/MaigoAkisame/orgs", "repos_url": "https://api.github.com/users/MaigoAkisame/repos", "events_url": "https://api.github.com/users/MaigoAkisame/events{/privacy}", "received_events_url": "https://api.github.com/users/MaigoAkisame/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T02:51:09Z", "updated_at": "2017-11-29T03:00:31Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I have made a minimum example that causes the out-of-memory error.</p>\n<p>The code generates eight batches of random data. Their shapes are identical to the actual data I used in my experiment. Note that the batches have different shapes while having similar sizes, because the sequences in different batches have different lengths. This is the key to generating the out-of-memory error -- maybe the varying shapes are creating a difficulty for memory allocation?</p>\n<p>The code implements a multi-layer LSTM in two ways: Net1 uses a single LSTM module for all the layers; Net2 uses one LSTM module for each layer.<br>\nIf we use Net1, an out-of-memory error will occur on the last batch. If we use Net2, there's no problem.</p>\n<p>I ran the code on a Tesla K20 GPU, which has around 4.7G memory.<br>\nI'm not sure if the error can be reproduced on other GPUs, but let me know if you have difficulty.<br>\nI may be able to tweak the batch shapes to reproduce the error.</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Net1(nn.Module):\n    def __init__(self, nInput, nHidden, nLayers, nOutput):\n        super(Net1, self).__init__()\n        self.lstm = nn.LSTM(nInput, nHidden, nLayers, bidirectional = True)\n        self.fc = nn.Linear(nHidden * 2, nOutput)\n\n    def forward(self, x):\n        x = self.lstm(x)[0]\n        return F.log_softmax(self.fc(x))\n\nclass Net2(nn.Module):\n    def __init__(self, nInput, nHidden, nLayers, nOutput):\n        super(Net2, self).__init__()\n        self.nLayers = nLayers\n        self.lstm0 = nn.LSTM(nInput, nHidden, 1, bidirectional = True)\n        for i in range(1, nLayers):\n            self.__setattr__('lstm%d' % i, nn.LSTM(nHidden * 2, nHidden, 1, bidirectional = True))\n        self.fc = nn.Linear(nHidden * 2, nOutput)\n\n    def forward(self, x):\n        for i in range(self.nLayers):\n            x = self.__getattr__('lstm%d' % i)(x)[0]\n        return F.log_softmax(self.fc(x))\n\nmodel = Net1(nInput = 40, nHidden = 320, nLayers = 5, nOutput = 40).cuda()\n\nshapes = [(1469, 13, 40), (1429, 13, 40), (1414, 14, 40),\n          (1395, 14, 40), (1374, 14, 40), (1358, 14, 40),\n          (1346, 14, 40), (1329, 15, 40)]\nx = [Variable(torch.rand(shape)) for shape in shapes]\n\nfor i in range(len(x)):\n    print i\n    pred = model(x[i].cuda())\n</code></pre>", "body_text": "@apaszke I have made a minimum example that causes the out-of-memory error.\nThe code generates eight batches of random data. Their shapes are identical to the actual data I used in my experiment. Note that the batches have different shapes while having similar sizes, because the sequences in different batches have different lengths. This is the key to generating the out-of-memory error -- maybe the varying shapes are creating a difficulty for memory allocation?\nThe code implements a multi-layer LSTM in two ways: Net1 uses a single LSTM module for all the layers; Net2 uses one LSTM module for each layer.\nIf we use Net1, an out-of-memory error will occur on the last batch. If we use Net2, there's no problem.\nI ran the code on a Tesla K20 GPU, which has around 4.7G memory.\nI'm not sure if the error can be reproduced on other GPUs, but let me know if you have difficulty.\nI may be able to tweak the batch shapes to reproduce the error.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Net1(nn.Module):\n    def __init__(self, nInput, nHidden, nLayers, nOutput):\n        super(Net1, self).__init__()\n        self.lstm = nn.LSTM(nInput, nHidden, nLayers, bidirectional = True)\n        self.fc = nn.Linear(nHidden * 2, nOutput)\n\n    def forward(self, x):\n        x = self.lstm(x)[0]\n        return F.log_softmax(self.fc(x))\n\nclass Net2(nn.Module):\n    def __init__(self, nInput, nHidden, nLayers, nOutput):\n        super(Net2, self).__init__()\n        self.nLayers = nLayers\n        self.lstm0 = nn.LSTM(nInput, nHidden, 1, bidirectional = True)\n        for i in range(1, nLayers):\n            self.__setattr__('lstm%d' % i, nn.LSTM(nHidden * 2, nHidden, 1, bidirectional = True))\n        self.fc = nn.Linear(nHidden * 2, nOutput)\n\n    def forward(self, x):\n        for i in range(self.nLayers):\n            x = self.__getattr__('lstm%d' % i)(x)[0]\n        return F.log_softmax(self.fc(x))\n\nmodel = Net1(nInput = 40, nHidden = 320, nLayers = 5, nOutput = 40).cuda()\n\nshapes = [(1469, 13, 40), (1429, 13, 40), (1414, 14, 40),\n          (1395, 14, 40), (1374, 14, 40), (1358, 14, 40),\n          (1346, 14, 40), (1329, 15, 40)]\nx = [Variable(torch.rand(shape)) for shape in shapes]\n\nfor i in range(len(x)):\n    print i\n    pred = model(x[i].cuda())", "body": "@apaszke I have made a minimum example that causes the out-of-memory error.\r\n\r\nThe code generates eight batches of random data. Their shapes are identical to the actual data I used in my experiment. Note that the batches have different shapes while having similar sizes, because the sequences in different batches have different lengths. This is the key to generating the out-of-memory error -- maybe the varying shapes are creating a difficulty for memory allocation?\r\n\r\nThe code implements a multi-layer LSTM in two ways: Net1 uses a single LSTM module for all the layers; Net2 uses one LSTM module for each layer.\r\nIf we use Net1, an out-of-memory error will occur on the last batch. If we use Net2, there's no problem.\r\n\r\nI ran the code on a Tesla K20 GPU, which has around 4.7G memory.\r\nI'm not sure if the error can be reproduced on other GPUs, but let me know if you have difficulty.\r\nI may be able to tweak the batch shapes to reproduce the error.\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass Net1(nn.Module):\r\n    def __init__(self, nInput, nHidden, nLayers, nOutput):\r\n        super(Net1, self).__init__()\r\n        self.lstm = nn.LSTM(nInput, nHidden, nLayers, bidirectional = True)\r\n        self.fc = nn.Linear(nHidden * 2, nOutput)\r\n\r\n    def forward(self, x):\r\n        x = self.lstm(x)[0]\r\n        return F.log_softmax(self.fc(x))\r\n\r\nclass Net2(nn.Module):\r\n    def __init__(self, nInput, nHidden, nLayers, nOutput):\r\n        super(Net2, self).__init__()\r\n        self.nLayers = nLayers\r\n        self.lstm0 = nn.LSTM(nInput, nHidden, 1, bidirectional = True)\r\n        for i in range(1, nLayers):\r\n            self.__setattr__('lstm%d' % i, nn.LSTM(nHidden * 2, nHidden, 1, bidirectional = True))\r\n        self.fc = nn.Linear(nHidden * 2, nOutput)\r\n\r\n    def forward(self, x):\r\n        for i in range(self.nLayers):\r\n            x = self.__getattr__('lstm%d' % i)(x)[0]\r\n        return F.log_softmax(self.fc(x))\r\n\r\nmodel = Net1(nInput = 40, nHidden = 320, nLayers = 5, nOutput = 40).cuda()\r\n\r\nshapes = [(1469, 13, 40), (1429, 13, 40), (1414, 14, 40),\r\n          (1395, 14, 40), (1374, 14, 40), (1358, 14, 40),\r\n          (1346, 14, 40), (1329, 15, 40)]\r\nx = [Variable(torch.rand(shape)) for shape in shapes]\r\n\r\nfor i in range(len(x)):\r\n    print i\r\n    pred = model(x[i].cuda())\r\n```"}