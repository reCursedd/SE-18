{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3931", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3931/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3931/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3931/events", "html_url": "https://github.com/pytorch/pytorch/issues/3931", "id": 277580638, "node_id": "MDU6SXNzdWUyNzc1ODA2Mzg=", "number": 3931, "title": "Suppress hidden state output of RNNs?", "user": {"login": "MaigoAkisame", "id": 6993299, "node_id": "MDQ6VXNlcjY5OTMyOTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6993299?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaigoAkisame", "html_url": "https://github.com/MaigoAkisame", "followers_url": "https://api.github.com/users/MaigoAkisame/followers", "following_url": "https://api.github.com/users/MaigoAkisame/following{/other_user}", "gists_url": "https://api.github.com/users/MaigoAkisame/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaigoAkisame/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaigoAkisame/subscriptions", "organizations_url": "https://api.github.com/users/MaigoAkisame/orgs", "repos_url": "https://api.github.com/users/MaigoAkisame/repos", "events_url": "https://api.github.com/users/MaigoAkisame/events{/privacy}", "received_events_url": "https://api.github.com/users/MaigoAkisame/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-11-28T23:52:16Z", "updated_at": "2017-11-29T21:06:22Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Currently the forward method of RNN modules (LSTM, GRU, etc) return the output as well as the hidden states of all layers. When an RNN module contains many layers, the second output can take up a significant amount of memory, even though in many cases it is not needed. E.g.</p>\n<pre><code>lstm = nn.LSTM(40, 320, 5)\ny, _ = lstm(x)    # The '_' variable takes lots of memory even though it's discarded right away\n</code></pre>\n<p>On the other hand, if I break down a multilayer RNN module into many modules representing single layers, I can discard the second output of the forward method layer by layer. With a 5-layer LSTM, I found doing so saved so much memory that I could double the batch size.</p>\n<pre><code>lstm1 = nn.LSTM(40, 320, 1)\nlstm2 = nn.LSTM(320, 320, 1)\nlstm3 = nn.LSTM(320, 320, 1)\nlstm4 = nn.LSTM(320, 320, 1)\nlstm5 = nn.LSTM(320, 320, 1)\nx, _ = lstm1(x)    # The unwanted second output is discarded layer by layer\nx, _ = lstm2(x)    # and doesn't not consume too much memory\nx, _ = lstm3(x)\nx, _ = lstm4(x)\nx, _ = lstm5(x)\n</code></pre>\n<p>Is it possible to add a switch to the forward method to specify whether the second output should be computed? E.g.</p>\n<pre><code>lstm = nn.LSTM(40, 320, 5)\ny = lstm(x, requires_hidden = False)    # Do not store the hidden states of each layer\n</code></pre>", "body_text": "Currently the forward method of RNN modules (LSTM, GRU, etc) return the output as well as the hidden states of all layers. When an RNN module contains many layers, the second output can take up a significant amount of memory, even though in many cases it is not needed. E.g.\nlstm = nn.LSTM(40, 320, 5)\ny, _ = lstm(x)    # The '_' variable takes lots of memory even though it's discarded right away\n\nOn the other hand, if I break down a multilayer RNN module into many modules representing single layers, I can discard the second output of the forward method layer by layer. With a 5-layer LSTM, I found doing so saved so much memory that I could double the batch size.\nlstm1 = nn.LSTM(40, 320, 1)\nlstm2 = nn.LSTM(320, 320, 1)\nlstm3 = nn.LSTM(320, 320, 1)\nlstm4 = nn.LSTM(320, 320, 1)\nlstm5 = nn.LSTM(320, 320, 1)\nx, _ = lstm1(x)    # The unwanted second output is discarded layer by layer\nx, _ = lstm2(x)    # and doesn't not consume too much memory\nx, _ = lstm3(x)\nx, _ = lstm4(x)\nx, _ = lstm5(x)\n\nIs it possible to add a switch to the forward method to specify whether the second output should be computed? E.g.\nlstm = nn.LSTM(40, 320, 5)\ny = lstm(x, requires_hidden = False)    # Do not store the hidden states of each layer", "body": "Currently the forward method of RNN modules (LSTM, GRU, etc) return the output as well as the hidden states of all layers. When an RNN module contains many layers, the second output can take up a significant amount of memory, even though in many cases it is not needed. E.g.\r\n```\r\nlstm = nn.LSTM(40, 320, 5)\r\ny, _ = lstm(x)    # The '_' variable takes lots of memory even though it's discarded right away\r\n```\r\nOn the other hand, if I break down a multilayer RNN module into many modules representing single layers, I can discard the second output of the forward method layer by layer. With a 5-layer LSTM, I found doing so saved so much memory that I could double the batch size.\r\n```\r\nlstm1 = nn.LSTM(40, 320, 1)\r\nlstm2 = nn.LSTM(320, 320, 1)\r\nlstm3 = nn.LSTM(320, 320, 1)\r\nlstm4 = nn.LSTM(320, 320, 1)\r\nlstm5 = nn.LSTM(320, 320, 1)\r\nx, _ = lstm1(x)    # The unwanted second output is discarded layer by layer\r\nx, _ = lstm2(x)    # and doesn't not consume too much memory\r\nx, _ = lstm3(x)\r\nx, _ = lstm4(x)\r\nx, _ = lstm5(x)\r\n```\r\nIs it possible to add a switch to the forward method to specify whether the second output should be computed? E.g.\r\n```\r\nlstm = nn.LSTM(40, 320, 5)\r\ny = lstm(x, requires_hidden = False)    # Do not store the hidden states of each layer\r\n```\r\n"}