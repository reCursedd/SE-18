{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424365788", "html_url": "https://github.com/pytorch/pytorch/issues/11980#issuecomment-424365788", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11980", "id": 424365788, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDM2NTc4OA==", "user": {"login": "jeffreyksmithjr", "id": 2244907, "node_id": "MDQ6VXNlcjIyNDQ5MDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/2244907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffreyksmithjr", "html_url": "https://github.com/jeffreyksmithjr", "followers_url": "https://api.github.com/users/jeffreyksmithjr/followers", "following_url": "https://api.github.com/users/jeffreyksmithjr/following{/other_user}", "gists_url": "https://api.github.com/users/jeffreyksmithjr/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffreyksmithjr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffreyksmithjr/subscriptions", "organizations_url": "https://api.github.com/users/jeffreyksmithjr/orgs", "repos_url": "https://api.github.com/users/jeffreyksmithjr/repos", "events_url": "https://api.github.com/users/jeffreyksmithjr/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffreyksmithjr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-25T14:28:09Z", "updated_at": "2018-09-25T14:28:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>NB on the analogous functionality from Spark: <a href=\"https://spark.apache.org/docs/2.3.1/api/scala/index.html#org.apache.spark.rdd.RDD@randomSplit(weights:Array%5BDouble%5D,seed:Long):Array%5Borg.apache.spark.rdd.RDD%5BT%5D%5D\" rel=\"nofollow\">https://spark.apache.org/docs/2.3.1/api/scala/index.html#org.apache.spark.rdd.RDD@randomSplit(weights:Array[Double],seed:Long):Array[org.apache.spark.rdd.RDD[T]]</a></p>\n<p>Just an example of an analogous API and implementation. In particular, I would highlight that the Spark implementation makes the choice of normalizing weights which don't sum to zero. An alternative for this normalization of weights functionality could be to simply throw an error, depending on the workflow desired.</p>", "body_text": "NB on the analogous functionality from Spark: https://spark.apache.org/docs/2.3.1/api/scala/index.html#org.apache.spark.rdd.RDD@randomSplit(weights:Array[Double],seed:Long):Array[org.apache.spark.rdd.RDD[T]]\nJust an example of an analogous API and implementation. In particular, I would highlight that the Spark implementation makes the choice of normalizing weights which don't sum to zero. An alternative for this normalization of weights functionality could be to simply throw an error, depending on the workflow desired.", "body": "NB on the analogous functionality from Spark: https://spark.apache.org/docs/2.3.1/api/scala/index.html#org.apache.spark.rdd.RDD@randomSplit(weights:Array[Double],seed:Long):Array[org.apache.spark.rdd.RDD[T]]\r\n\r\nJust an example of an analogous API and implementation. In particular, I would highlight that the Spark implementation makes the choice of normalizing weights which don't sum to zero. An alternative for this normalization of weights functionality could be to simply throw an error, depending on the workflow desired."}