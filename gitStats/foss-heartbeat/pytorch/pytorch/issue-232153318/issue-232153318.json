{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1680", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1680/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1680/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1680/events", "html_url": "https://github.com/pytorch/pytorch/issues/1680", "id": 232153318, "node_id": "MDU6SXNzdWUyMzIxNTMzMTg=", "number": 1680, "title": "Conv1d & ConvTranspose1d expect 2nd dimension values.", "user": {"login": "educob", "id": 12698227, "node_id": "MDQ6VXNlcjEyNjk4MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/12698227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/educob", "html_url": "https://github.com/educob", "followers_url": "https://api.github.com/users/educob/followers", "following_url": "https://api.github.com/users/educob/following{/other_user}", "gists_url": "https://api.github.com/users/educob/gists{/gist_id}", "starred_url": "https://api.github.com/users/educob/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/educob/subscriptions", "organizations_url": "https://api.github.com/users/educob/orgs", "repos_url": "https://api.github.com/users/educob/repos", "events_url": "https://api.github.com/users/educob/events{/privacy}", "received_events_url": "https://api.github.com/users/educob/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-30T06:21:48Z", "updated_at": "2017-06-01T14:31:57Z", "closed_at": "2017-06-01T13:49:05Z", "author_association": "NONE", "body_html": "<p>I am programming an adversarial convolutional autoencoder and getting all sort of errors for the 2nd dimension of parameters when in theory there should be any 2nd dimension in Conv1d &amp; ConvTranspose1d</p>\n<p>I copied and adapted the code from: <a href=\"url\">https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/deep_convolutional_gan/model.py</a><br>\nI just changed conv2d and ConvTranspose2d to 1d.</p>\n<p>My code is:</p>\n<pre><code> def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n    layers = []\n    layers.append(nn.Conv1d(c_in, c_out, k_size, (stride, 1), pad, (0, 0)))\n    if bn:\n        layers.append(nn.BatchNorm1d(c_out))\n    return nn.Sequential(*layers)\n\ndef deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n    layers = []\n    layers.append(nn.ConvTranspose1d(c_in, c_out, (k_size, 1), (stride, 1), pad, (0, 0)))\n    if bn:\n        layers.append(nn.BatchNorm1d(c_out))\n    return nn.Sequential(*layers)\n\n##################################\n# Define Networks\n##################################\n# ===== Encoder ===========\nclass Q_net(nn.Module):\n     def __init__(self, conv_dim=64):\n         super(Q_net, self).__init__()\n         self.conv1 = conv(2, conv_dim, 4, bn=False)\n         self.conv2 = conv(conv_dim, conv_dim*2, 4)\n         self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n         self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n         self.fc_2 = conv(conv_dim*8, z2_dim, 4, 1, 0, False) #int(image_size/16), 1, 0, False)\n         self.fc_cat = conv(conv_dim*8, n_classes, 4, 1, 0, False) #int(image_size/16), 1, 0, False)\n\n     def forward(self, x):                         # If image_size is 64, output shape is as below.\n         print(\"x.shape\", x.size())\n         out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 32, 32)\n         out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 16, 16)\n         out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 8, 8)\n         out = F.leaky_relu(self.conv4(out), 0.05)  # (?, 512, 4, 4)\n         z_2 = self.fc_2(out).squeeze()                #(?, z2_dim, 1, 1)\n         zcat = F.softmax(self.fc_cat(out).squeeze())  #(?, n_classes, 1, 1)\n         return zcat, z_2\n\n\n# ========= Decoder ============\nclass P_net(nn.Module):\n    \"\"\"Generator containing 7 deconvolutional layers.\"\"\"\n    def __init__(self, z_dim=z2_dim+n_classes, conv_dim=64):\n        super(P_net, self).__init__()\n        self.fc = deconv(z_dim, conv_dim*8, 4, 1, 0, bn=False)\n        self.deconv1 = deconv(conv_dim*8, conv_dim*4, 4)\n        self.deconv2 = deconv(conv_dim*4, conv_dim*2, 4)\n        self.deconv3 = deconv(conv_dim*2, conv_dim, 4)\n        self.deconv4 = deconv(conv_dim, 2, 4, bn=False)\n\n    def forward(self, z):\n        print(\"z.size:\", z.size())\n        z = z.view(z.size(0), z.size(1), 1, 1)      # If image_size is 64, output shape is as below.\n        print(\"z.size:\", z.size())\n        out = self.fc(z)                            # (?, 512, 4, 4)\n        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 256, 8, 8)\n        out = F.leaky_relu(self.deconv2(out), 0.05)  # (?, 128, 16, 16)\n        out = F.leaky_relu(self.deconv3(out), 0.05)  # (?, 64, 32, 32)\n        out = self.deconv4(out) #F.tanh(self.deconv4(out)) # (?, 2, 64, 64)\n        return out\n</code></pre>\n<p>Notice that in conv and deconv functions I had to add a 2nd dimenstion for kernel, stride and dilation otherwise I get these errors:</p>\n<p>RuntimeError: output adjustment must be smaller than stride, but got adjH: 0 adjW: 32642 dH: 1 dW: 1 at /py/conda-bld/pytorch_1490903321756/work/torch/lib/THNN/generic/S</p>\n<p>RuntimeError: dilation should be greater than zero, but got dilationH: 0, dilationW: 0 at /py/conda-bld/pytorch_1490903321756/work/torch/lib/THNN/generic/SpatialDilatedC</p>\n<p>When I add the 2nd dimension to these 3 parameters I get these error:<br>\nFile \"/home/c3po/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 62, in conv1d<br>\nreturn f(input, weight, bias)<br>\nRuntimeError: expected 3D tensor</p>\n<p>What drives me mad is that in the doc there is not much info on input &amp; parameters details so it is a guess work.</p>\n<p>Thanks for pytorch which is an amazing tool.</p>\n<p>ps: code is partially badly shown. No idea why is that.</p>", "body_text": "I am programming an adversarial convolutional autoencoder and getting all sort of errors for the 2nd dimension of parameters when in theory there should be any 2nd dimension in Conv1d & ConvTranspose1d\nI copied and adapted the code from: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/deep_convolutional_gan/model.py\nI just changed conv2d and ConvTranspose2d to 1d.\nMy code is:\n def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n    layers = []\n    layers.append(nn.Conv1d(c_in, c_out, k_size, (stride, 1), pad, (0, 0)))\n    if bn:\n        layers.append(nn.BatchNorm1d(c_out))\n    return nn.Sequential(*layers)\n\ndef deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n    layers = []\n    layers.append(nn.ConvTranspose1d(c_in, c_out, (k_size, 1), (stride, 1), pad, (0, 0)))\n    if bn:\n        layers.append(nn.BatchNorm1d(c_out))\n    return nn.Sequential(*layers)\n\n##################################\n# Define Networks\n##################################\n# ===== Encoder ===========\nclass Q_net(nn.Module):\n     def __init__(self, conv_dim=64):\n         super(Q_net, self).__init__()\n         self.conv1 = conv(2, conv_dim, 4, bn=False)\n         self.conv2 = conv(conv_dim, conv_dim*2, 4)\n         self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n         self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n         self.fc_2 = conv(conv_dim*8, z2_dim, 4, 1, 0, False) #int(image_size/16), 1, 0, False)\n         self.fc_cat = conv(conv_dim*8, n_classes, 4, 1, 0, False) #int(image_size/16), 1, 0, False)\n\n     def forward(self, x):                         # If image_size is 64, output shape is as below.\n         print(\"x.shape\", x.size())\n         out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 32, 32)\n         out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 16, 16)\n         out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 8, 8)\n         out = F.leaky_relu(self.conv4(out), 0.05)  # (?, 512, 4, 4)\n         z_2 = self.fc_2(out).squeeze()                #(?, z2_dim, 1, 1)\n         zcat = F.softmax(self.fc_cat(out).squeeze())  #(?, n_classes, 1, 1)\n         return zcat, z_2\n\n\n# ========= Decoder ============\nclass P_net(nn.Module):\n    \"\"\"Generator containing 7 deconvolutional layers.\"\"\"\n    def __init__(self, z_dim=z2_dim+n_classes, conv_dim=64):\n        super(P_net, self).__init__()\n        self.fc = deconv(z_dim, conv_dim*8, 4, 1, 0, bn=False)\n        self.deconv1 = deconv(conv_dim*8, conv_dim*4, 4)\n        self.deconv2 = deconv(conv_dim*4, conv_dim*2, 4)\n        self.deconv3 = deconv(conv_dim*2, conv_dim, 4)\n        self.deconv4 = deconv(conv_dim, 2, 4, bn=False)\n\n    def forward(self, z):\n        print(\"z.size:\", z.size())\n        z = z.view(z.size(0), z.size(1), 1, 1)      # If image_size is 64, output shape is as below.\n        print(\"z.size:\", z.size())\n        out = self.fc(z)                            # (?, 512, 4, 4)\n        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 256, 8, 8)\n        out = F.leaky_relu(self.deconv2(out), 0.05)  # (?, 128, 16, 16)\n        out = F.leaky_relu(self.deconv3(out), 0.05)  # (?, 64, 32, 32)\n        out = self.deconv4(out) #F.tanh(self.deconv4(out)) # (?, 2, 64, 64)\n        return out\n\nNotice that in conv and deconv functions I had to add a 2nd dimenstion for kernel, stride and dilation otherwise I get these errors:\nRuntimeError: output adjustment must be smaller than stride, but got adjH: 0 adjW: 32642 dH: 1 dW: 1 at /py/conda-bld/pytorch_1490903321756/work/torch/lib/THNN/generic/S\nRuntimeError: dilation should be greater than zero, but got dilationH: 0, dilationW: 0 at /py/conda-bld/pytorch_1490903321756/work/torch/lib/THNN/generic/SpatialDilatedC\nWhen I add the 2nd dimension to these 3 parameters I get these error:\nFile \"/home/c3po/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 62, in conv1d\nreturn f(input, weight, bias)\nRuntimeError: expected 3D tensor\nWhat drives me mad is that in the doc there is not much info on input & parameters details so it is a guess work.\nThanks for pytorch which is an amazing tool.\nps: code is partially badly shown. No idea why is that.", "body": "I am programming an adversarial convolutional autoencoder and getting all sort of errors for the 2nd dimension of parameters when in theory there should be any 2nd dimension in Conv1d & ConvTranspose1d\r\n\r\nI copied and adapted the code from: [https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/deep_convolutional_gan/model.py](url)\r\nI just changed conv2d and ConvTranspose2d to 1d.\r\n\r\nMy code is: \r\n\r\n```\r\n def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\r\n    \"\"\"Custom convolutional layer for simplicity.\"\"\"\r\n    layers = []\r\n    layers.append(nn.Conv1d(c_in, c_out, k_size, (stride, 1), pad, (0, 0)))\r\n    if bn:\r\n        layers.append(nn.BatchNorm1d(c_out))\r\n    return nn.Sequential(*layers)\r\n\r\ndef deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\r\n    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\r\n    layers = []\r\n    layers.append(nn.ConvTranspose1d(c_in, c_out, (k_size, 1), (stride, 1), pad, (0, 0)))\r\n    if bn:\r\n        layers.append(nn.BatchNorm1d(c_out))\r\n    return nn.Sequential(*layers)\r\n\r\n##################################\r\n# Define Networks\r\n##################################\r\n# ===== Encoder ===========\r\nclass Q_net(nn.Module):\r\n     def __init__(self, conv_dim=64):\r\n         super(Q_net, self).__init__()\r\n         self.conv1 = conv(2, conv_dim, 4, bn=False)\r\n         self.conv2 = conv(conv_dim, conv_dim*2, 4)\r\n         self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\r\n         self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\r\n         self.fc_2 = conv(conv_dim*8, z2_dim, 4, 1, 0, False) #int(image_size/16), 1, 0, False)\r\n         self.fc_cat = conv(conv_dim*8, n_classes, 4, 1, 0, False) #int(image_size/16), 1, 0, False)\r\n\r\n     def forward(self, x):                         # If image_size is 64, output shape is as below.\r\n         print(\"x.shape\", x.size())\r\n         out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 32, 32)\r\n         out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 16, 16)\r\n         out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 8, 8)\r\n         out = F.leaky_relu(self.conv4(out), 0.05)  # (?, 512, 4, 4)\r\n         z_2 = self.fc_2(out).squeeze()                #(?, z2_dim, 1, 1)\r\n         zcat = F.softmax(self.fc_cat(out).squeeze())  #(?, n_classes, 1, 1)\r\n         return zcat, z_2\r\n\r\n\r\n# ========= Decoder ============\r\nclass P_net(nn.Module):\r\n    \"\"\"Generator containing 7 deconvolutional layers.\"\"\"\r\n    def __init__(self, z_dim=z2_dim+n_classes, conv_dim=64):\r\n        super(P_net, self).__init__()\r\n        self.fc = deconv(z_dim, conv_dim*8, 4, 1, 0, bn=False)\r\n        self.deconv1 = deconv(conv_dim*8, conv_dim*4, 4)\r\n        self.deconv2 = deconv(conv_dim*4, conv_dim*2, 4)\r\n        self.deconv3 = deconv(conv_dim*2, conv_dim, 4)\r\n        self.deconv4 = deconv(conv_dim, 2, 4, bn=False)\r\n\r\n    def forward(self, z):\r\n        print(\"z.size:\", z.size())\r\n        z = z.view(z.size(0), z.size(1), 1, 1)      # If image_size is 64, output shape is as below.\r\n        print(\"z.size:\", z.size())\r\n        out = self.fc(z)                            # (?, 512, 4, 4)\r\n        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 256, 8, 8)\r\n        out = F.leaky_relu(self.deconv2(out), 0.05)  # (?, 128, 16, 16)\r\n        out = F.leaky_relu(self.deconv3(out), 0.05)  # (?, 64, 32, 32)\r\n        out = self.deconv4(out) #F.tanh(self.deconv4(out)) # (?, 2, 64, 64)\r\n        return out\r\n```\r\n\r\nNotice that in conv and deconv functions I had to add a 2nd dimenstion for kernel, stride and dilation otherwise I get these errors:\r\n\r\nRuntimeError: output adjustment must be smaller than stride, but got adjH: 0 adjW: 32642 dH: 1 dW: 1 at /py/conda-bld/pytorch_1490903321756/work/torch/lib/THNN/generic/S\r\n\r\nRuntimeError: dilation should be greater than zero, but got dilationH: 0, dilationW: 0 at /py/conda-bld/pytorch_1490903321756/work/torch/lib/THNN/generic/SpatialDilatedC\r\n\r\nWhen I add the 2nd dimension to these 3 parameters I get these error:\r\nFile \"/home/c3po/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 62, in conv1d\r\n    return f(input, weight, bias)\r\nRuntimeError: expected 3D tensor\r\n\r\nWhat drives me mad is that in the doc there is not much info on input & parameters details so it is a guess work.\r\n\r\nThanks for pytorch which is an amazing tool.\r\n\r\nps: code is partially badly shown. No idea why is that.\r\n\r\n"}