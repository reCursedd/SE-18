{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377612198", "html_url": "https://github.com/pytorch/pytorch/pull/4921#issuecomment-377612198", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4921", "id": 377612198, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzYxMjE5OA==", "user": {"login": "cyang49", "id": 7364402, "node_id": "MDQ6VXNlcjczNjQ0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7364402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cyang49", "html_url": "https://github.com/cyang49", "followers_url": "https://api.github.com/users/cyang49/followers", "following_url": "https://api.github.com/users/cyang49/following{/other_user}", "gists_url": "https://api.github.com/users/cyang49/gists{/gist_id}", "starred_url": "https://api.github.com/users/cyang49/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cyang49/subscriptions", "organizations_url": "https://api.github.com/users/cyang49/orgs", "repos_url": "https://api.github.com/users/cyang49/repos", "events_url": "https://api.github.com/users/cyang49/events{/privacy}", "received_events_url": "https://api.github.com/users/cyang49/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T20:13:37Z", "updated_at": "2018-03-30T20:34:46Z", "author_association": "NONE", "body_html": "<p>Adding <code>NCCL_DEBUG=INFO</code>  gives perhaps some hint to what happened. Maybe it's memory leak? I observed nvidia-smi and see that the device 0 memory ran out but device 1 memory usage stays constant. I'm not sure if the tensor is being correctly allocated on the corresponding devices?</p>\n<pre><code>c460c041:101211:106353 [0] INFO CUDA Dev 0, IB Ports : mlx5_2/1(SOC) mlx5_0/1(SOC)\nc460c041:101211:106353 [0] init.cu:218 WARN Cuda failure 'out of memory'\nc460c041:101211:106353 [0] transport/p2p.cu:404 WARN rank 1 failed to get CUDA IPC handle to device 0 : 11 invalid argument\nc460c041:101211:106353 [0] INFO init.cu:191 -&gt; 3\nc460c041:101211:106353 [0] INFO init.cu:266 -&gt; 3\nc460c041:101211:106353 [0] INFO init.cu:460 -&gt; 3\nc460c041:101211:106353 [0] INFO init.cu:517 -&gt; 3\nc460c041:101211:106353 [0] INFO misc/group.cu:70 -&gt; 3 [Async thread]\nTraceback (most recent call last):\n  File \"toy.py\", line 33, in &lt;module&gt;\n    init_processes(args.rank, size, run, 'nccl')\n  File \"toy.py\", line 24, in init_processes\n    fn(rank, size)\n  File \"toy.py\", line 10, in run\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, internal error\n</code></pre>", "body_text": "Adding NCCL_DEBUG=INFO  gives perhaps some hint to what happened. Maybe it's memory leak? I observed nvidia-smi and see that the device 0 memory ran out but device 1 memory usage stays constant. I'm not sure if the tensor is being correctly allocated on the corresponding devices?\nc460c041:101211:106353 [0] INFO CUDA Dev 0, IB Ports : mlx5_2/1(SOC) mlx5_0/1(SOC)\nc460c041:101211:106353 [0] init.cu:218 WARN Cuda failure 'out of memory'\nc460c041:101211:106353 [0] transport/p2p.cu:404 WARN rank 1 failed to get CUDA IPC handle to device 0 : 11 invalid argument\nc460c041:101211:106353 [0] INFO init.cu:191 -> 3\nc460c041:101211:106353 [0] INFO init.cu:266 -> 3\nc460c041:101211:106353 [0] INFO init.cu:460 -> 3\nc460c041:101211:106353 [0] INFO init.cu:517 -> 3\nc460c041:101211:106353 [0] INFO misc/group.cu:70 -> 3 [Async thread]\nTraceback (most recent call last):\n  File \"toy.py\", line 33, in <module>\n    init_processes(args.rank, size, run, 'nccl')\n  File \"toy.py\", line 24, in init_processes\n    fn(rank, size)\n  File \"toy.py\", line 10, in run\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, internal error", "body": "Adding `NCCL_DEBUG=INFO`  gives perhaps some hint to what happened. Maybe it's memory leak? I observed nvidia-smi and see that the device 0 memory ran out but device 1 memory usage stays constant. I'm not sure if the tensor is being correctly allocated on the corresponding devices?\r\n```\r\nc460c041:101211:106353 [0] INFO CUDA Dev 0, IB Ports : mlx5_2/1(SOC) mlx5_0/1(SOC)\r\nc460c041:101211:106353 [0] init.cu:218 WARN Cuda failure 'out of memory'\r\nc460c041:101211:106353 [0] transport/p2p.cu:404 WARN rank 1 failed to get CUDA IPC handle to device 0 : 11 invalid argument\r\nc460c041:101211:106353 [0] INFO init.cu:191 -> 3\r\nc460c041:101211:106353 [0] INFO init.cu:266 -> 3\r\nc460c041:101211:106353 [0] INFO init.cu:460 -> 3\r\nc460c041:101211:106353 [0] INFO init.cu:517 -> 3\r\nc460c041:101211:106353 [0] INFO misc/group.cu:70 -> 3 [Async thread]\r\nTraceback (most recent call last):\r\n  File \"toy.py\", line 33, in <module>\r\n    init_processes(args.rank, size, run, 'nccl')\r\n  File \"toy.py\", line 24, in init_processes\r\n    fn(rank, size)\r\n  File \"toy.py\", line 10, in run\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\r\n    return torch._C._dist_all_reduce(tensor, op, group)\r\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, internal error\r\n```"}