{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377098629", "html_url": "https://github.com/pytorch/pytorch/pull/4921#issuecomment-377098629", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4921", "id": 377098629, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzA5ODYyOQ==", "user": {"login": "Yi-Li", "id": 30933421, "node_id": "MDQ6VXNlcjMwOTMzNDIx", "avatar_url": "https://avatars0.githubusercontent.com/u/30933421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yi-Li", "html_url": "https://github.com/Yi-Li", "followers_url": "https://api.github.com/users/Yi-Li/followers", "following_url": "https://api.github.com/users/Yi-Li/following{/other_user}", "gists_url": "https://api.github.com/users/Yi-Li/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yi-Li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yi-Li/subscriptions", "organizations_url": "https://api.github.com/users/Yi-Li/orgs", "repos_url": "https://api.github.com/users/Yi-Li/repos", "events_url": "https://api.github.com/users/Yi-Li/events{/privacy}", "received_events_url": "https://api.github.com/users/Yi-Li/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-29T02:16:53Z", "updated_at": "2018-03-29T02:16:53Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Hi Teng Li,\n\n\nMy system administrator seems to get the glibc2.19 ready. I want to test the following small code (try.py) which you put on forum website. As you suggest, I will run \"python try.py 1\" on one terminal, and run \"python try.py 2\" on another terminal, and should get the output of \"2\".\n\n\nI want to check whether this code can be run on two gpu nodes. In init_processes, I set ?\n\ninit_method=\"tcp://10.6.48.150:13530\"?. My understanding is that 10.6.48.150 is the ip address of the master gpu node. Is it correct that we don't need to set the ip address of any slave gpu node? Thank you for your help!</div>\n<div class=\"email-quoted-reply\">cat try.py</div>\n<div class=\"email-fragment\">import torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    print('begin rank', rank)\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda()\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    init_method=\"tcp://10.6.48.150:13530\"\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\n    print('after init')\n    fn(rank, size)\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-signature-reply\">________________________________\nFrom: Teng Li &lt;notifications@github.com&gt;\nSent: Tuesday, February 27, 2018 5:03 PM\nTo: pytorch/pytorch\nCc: Yi Li; Mention\nSubject: Re: [pytorch/pytorch] Release NCCL distributed backend from experimental (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"292602418\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4921\" href=\"https://github.com/pytorch/pytorch/pull/4921\">#4921</a>)\n\n\nyeah, I have seen similar issues with a mismatching glibc version.\n\n-\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub&lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"292602418\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4921\" href=\"https://github.com/pytorch/pytorch/pull/4921#issuecomment-369042391\">#4921 (comment)</a>&gt;, or mute the thread&lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AdgBremmrKMGnITDdFBEQcLqRpWyMeGEks5tZHt0gaJpZM4Rxif7\">https://github.com/notifications/unsubscribe-auth/AdgBremmrKMGnITDdFBEQcLqRpWyMeGEks5tZHt0gaJpZM4Rxif7</a>&gt;.\n---\n\nThe information in this email, including attachments, may be confidential and is intended solely for the addressee(s). If you believe you received this email by mistake, please notify the sender by return email as soon as possible.</div>\n</div>", "body_text": "Hi Teng Li,\n\n\nMy system administrator seems to get the glibc2.19 ready. I want to test the following small code (try.py) which you put on forum website. As you suggest, I will run \"python try.py 1\" on one terminal, and run \"python try.py 2\" on another terminal, and should get the output of \"2\".\n\n\nI want to check whether this code can be run on two gpu nodes. In init_processes, I set ?\n\ninit_method=\"tcp://10.6.48.150:13530\"?. My understanding is that 10.6.48.150 is the ip address of the master gpu node. Is it correct that we don't need to set the ip address of any slave gpu node? Thank you for your help!\ncat try.py\nimport torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    print('begin rank', rank)\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda()\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    init_method=\"tcp://10.6.48.150:13530\"\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\n    print('after init')\n    fn(rank, size)\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')\n\u2026\n________________________________\nFrom: Teng Li <notifications@github.com>\nSent: Tuesday, February 27, 2018 5:03 PM\nTo: pytorch/pytorch\nCc: Yi Li; Mention\nSubject: Re: [pytorch/pytorch] Release NCCL distributed backend from experimental (#4921)\n\n\nyeah, I have seen similar issues with a mismatching glibc version.\n\n-\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<#4921 (comment)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AdgBremmrKMGnITDdFBEQcLqRpWyMeGEks5tZHt0gaJpZM4Rxif7>.\n---\n\nThe information in this email, including attachments, may be confidential and is intended solely for the addressee(s). If you believe you received this email by mistake, please notify the sender by return email as soon as possible.", "body": "Hi Teng Li,\n\n\nMy system administrator seems to get the glibc2.19 ready. I want to test the following small code (try.py) which you put on forum website. As you suggest, I will run \"python try.py 1\" on one terminal, and run \"python try.py 2\" on another terminal, and should get the output of \"2\".\n\n\nI want to check whether this code can be run on two gpu nodes. In init_processes, I set ?\n\ninit_method=\"tcp://10.6.48.150:13530\"?. My understanding is that 10.6.48.150 is the ip address of the master gpu node. Is it correct that we don't need to set the ip address of any slave gpu node? Thank you for your help!\n\n\n>cat try.py\n\nimport torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    print('begin rank', rank)\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda()\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    init_method=\"tcp://10.6.48.150:13530\"\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\n    print('after init')\n    fn(rank, size)\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')\n\n\n________________________________\nFrom: Teng Li <notifications@github.com>\nSent: Tuesday, February 27, 2018 5:03 PM\nTo: pytorch/pytorch\nCc: Yi Li; Mention\nSubject: Re: [pytorch/pytorch] Release NCCL distributed backend from experimental (#4921)\n\n\nyeah, I have seen similar issues with a mismatching glibc version.\n\n-\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/pytorch/pytorch/pull/4921#issuecomment-369042391>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AdgBremmrKMGnITDdFBEQcLqRpWyMeGEks5tZHt0gaJpZM4Rxif7>.\n---\n\nThe information in this email, including attachments, may be confidential and is intended solely for the addressee(s). If you believe you received this email by mistake, please notify the sender by return email as soon as possible.\n"}