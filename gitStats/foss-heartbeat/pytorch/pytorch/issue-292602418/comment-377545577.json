{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377545577", "html_url": "https://github.com/pytorch/pytorch/pull/4921#issuecomment-377545577", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4921", "id": 377545577, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzU0NTU3Nw==", "user": {"login": "cyang49", "id": 7364402, "node_id": "MDQ6VXNlcjczNjQ0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7364402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cyang49", "html_url": "https://github.com/cyang49", "followers_url": "https://api.github.com/users/cyang49/followers", "following_url": "https://api.github.com/users/cyang49/following{/other_user}", "gists_url": "https://api.github.com/users/cyang49/gists{/gist_id}", "starred_url": "https://api.github.com/users/cyang49/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cyang49/subscriptions", "organizations_url": "https://api.github.com/users/cyang49/orgs", "repos_url": "https://api.github.com/users/cyang49/repos", "events_url": "https://api.github.com/users/cyang49/events{/privacy}", "received_events_url": "https://api.github.com/users/cyang49/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T15:09:31Z", "updated_at": "2018-03-30T15:09:31Z", "author_association": "NONE", "body_html": "<p>I'm having the same issue as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30933421\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Yi-Li\">@Yi-Li</a> with the imagenet example. I tried the toy.py she posted and found I couldn't reproduce it. I realized that the difference is that I set <code>CUDA_VISIBLE_DEVICES</code> when I run imagenet. After adding this when I run toy.py, I can reproduce the same error.</p>\n<p>The modified toy.py I used:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> argparse\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Simple point-to-point communication. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>begin rank<span class=\"pl-pds\">'</span></span>, rank)\n    group <span class=\"pl-k\">=</span> dist.new_group([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n    tensor <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>).cuda()\n    dist.all_reduce(tensor, <span class=\"pl-v\">op</span><span class=\"pl-k\">=</span>dist.reduce_op.<span class=\"pl-c1\">SUM</span>, <span class=\"pl-v\">group</span><span class=\"pl-k\">=</span>group)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Rank <span class=\"pl-pds\">'</span></span>, rank, <span class=\"pl-s\"><span class=\"pl-pds\">'</span> has data <span class=\"pl-pds\">'</span></span>, tensor[<span class=\"pl-c1\">0</span>])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">init_processes</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">fn</span>, <span class=\"pl-smi\">backend</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Initialize the distributed environment. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before init<span class=\"pl-pds\">'</span></span>)\n    init_method<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>file://./sync<span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)</span>\n    dist.init_process_group(backend,<span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>size,<span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span>init_method)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>after init<span class=\"pl-pds\">'</span></span>)\n    fn(rank, size)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--rank<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rank<span class=\"pl-pds\">'</span></span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n    init_processes(args.rank, size, run, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nccl<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>The commands I used on different terminal of the same machine to trigger the error:</p>\n<pre><code># terminal 0\nCUDA_VISIBLE_DEVICES=0 python toy.py --rank 0\n</code></pre>\n<pre><code># terminal 1\nCUDA_VISIBLE_DEVICES=1 python toy.py --rank 1\n</code></pre>\n<p>I'm not exactly sure this is the same problem with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30933421\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Yi-Li\">@Yi-Li</a> but could anyone help with using NCCL on different CUDA devices?</p>", "body_text": "I'm having the same issue as @Yi-Li with the imagenet example. I tried the toy.py she posted and found I couldn't reproduce it. I realized that the difference is that I set CUDA_VISIBLE_DEVICES when I run imagenet. After adding this when I run toy.py, I can reproduce the same error.\nThe modified toy.py I used:\nimport torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    print('begin rank', rank)\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda()\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    init_method=\"file://./sync\"\n    #dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\n    dist.init_process_group(backend,world_size=size,init_method=init_method)\n    print('after init')\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,\n                        help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')\nThe commands I used on different terminal of the same machine to trigger the error:\n# terminal 0\nCUDA_VISIBLE_DEVICES=0 python toy.py --rank 0\n\n# terminal 1\nCUDA_VISIBLE_DEVICES=1 python toy.py --rank 1\n\nI'm not exactly sure this is the same problem with @Yi-Li but could anyone help with using NCCL on different CUDA devices?", "body": "I'm having the same issue as @Yi-Li with the imagenet example. I tried the toy.py she posted and found I couldn't reproduce it. I realized that the difference is that I set `CUDA_VISIBLE_DEVICES` when I run imagenet. After adding this when I run toy.py, I can reproduce the same error. \r\n\r\nThe modified toy.py I used:\r\n``` python\r\nimport torch\r\nimport torch.distributed as dist\r\nimport argparse\r\n\r\ndef run(rank, size):\r\n    \"\"\" Simple point-to-point communication. \"\"\"\r\n    print('begin rank', rank)\r\n    group = dist.new_group([0, 1])\r\n    tensor = torch.ones(1).cuda()\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n    print('Rank ', rank, ' has data ', tensor[0])\r\n\r\ndef init_processes(rank, size, fn, backend):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    print('before init')\r\n    init_method=\"file://./sync\"\r\n    #dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\r\n    dist.init_process_group(backend,world_size=size,init_method=init_method)\r\n    print('after init')\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--rank', default=-1, type=int,\r\n                        help='rank')\r\n    args = parser.parse_args()\r\n    init_processes(args.rank, size, run, 'nccl')\r\n```\r\n\r\nThe commands I used on different terminal of the same machine to trigger the error:\r\n```\r\n# terminal 0\r\nCUDA_VISIBLE_DEVICES=0 python toy.py --rank 0\r\n```\r\n```\r\n# terminal 1\r\nCUDA_VISIBLE_DEVICES=1 python toy.py --rank 1\r\n```\r\n\r\nI'm not exactly sure this is the same problem with @Yi-Li but could anyone help with using NCCL on different CUDA devices?"}