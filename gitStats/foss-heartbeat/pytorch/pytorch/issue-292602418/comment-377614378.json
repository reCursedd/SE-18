{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377614378", "html_url": "https://github.com/pytorch/pytorch/pull/4921#issuecomment-377614378", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4921", "id": 377614378, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzYxNDM3OA==", "user": {"login": "cyang49", "id": 7364402, "node_id": "MDQ6VXNlcjczNjQ0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7364402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cyang49", "html_url": "https://github.com/cyang49", "followers_url": "https://api.github.com/users/cyang49/followers", "following_url": "https://api.github.com/users/cyang49/following{/other_user}", "gists_url": "https://api.github.com/users/cyang49/gists{/gist_id}", "starred_url": "https://api.github.com/users/cyang49/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cyang49/subscriptions", "organizations_url": "https://api.github.com/users/cyang49/orgs", "repos_url": "https://api.github.com/users/cyang49/repos", "events_url": "https://api.github.com/users/cyang49/events{/privacy}", "received_events_url": "https://api.github.com/users/cyang49/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T20:23:29Z", "updated_at": "2018-03-30T20:41:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30933421\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Yi-Li\">@Yi-Li</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">init_processes</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">fn</span>, <span class=\"pl-smi\">backend</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Initialize the distributed environment. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before init<span class=\"pl-pds\">'</span></span>)\n    torch.cuda.set_device(rank)\n    init_method<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tcp://127.0.0.1:16543<span class=\"pl-pds\">\"</span></span>\n    dist.init_process_group(backend,<span class=\"pl-v\">rank</span><span class=\"pl-k\">=</span>rank,<span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>size,<span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span>init_method)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>after init<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000</span>):\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span>th operation<span class=\"pl-pds\">'</span></span>.format(i))\n      fn(rank, size)</pre></div>\n<p>Edit:<br>\nI think the <code>torch.cuda.device()</code> didn't really take effect. I used <code>torch.cuda.set_device()</code> instead and nvidia-smi shows both devices are being used now. The same out of memory error still happens.</p>", "body_text": "@Yi-Li\ndef init_processes(rank, size, fn, backend):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    torch.cuda.set_device(rank)\n    init_method=\"tcp://127.0.0.1:16543\"\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\n    print('after init')\n    for i in range(100000):\n      print('{}th operation'.format(i))\n      fn(rank, size)\nEdit:\nI think the torch.cuda.device() didn't really take effect. I used torch.cuda.set_device() instead and nvidia-smi shows both devices are being used now. The same out of memory error still happens.", "body": "@Yi-Li\r\n``` python \r\ndef init_processes(rank, size, fn, backend):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    print('before init')\r\n    torch.cuda.set_device(rank)\r\n    init_method=\"tcp://127.0.0.1:16543\"\r\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\r\n    print('after init')\r\n    for i in range(100000):\r\n      print('{}th operation'.format(i))\r\n      fn(rank, size)\r\n```\r\n\r\nEdit:\r\nI think the `torch.cuda.device()` didn't really take effect. I used `torch.cuda.set_device()` instead and nvidia-smi shows both devices are being used now. The same out of memory error still happens.\r\n\r\n"}