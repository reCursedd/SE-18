{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377607198", "html_url": "https://github.com/pytorch/pytorch/pull/4921#issuecomment-377607198", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4921", "id": 377607198, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzYwNzE5OA==", "user": {"login": "cyang49", "id": 7364402, "node_id": "MDQ6VXNlcjczNjQ0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7364402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cyang49", "html_url": "https://github.com/cyang49", "followers_url": "https://api.github.com/users/cyang49/followers", "following_url": "https://api.github.com/users/cyang49/following{/other_user}", "gists_url": "https://api.github.com/users/cyang49/gists{/gist_id}", "starred_url": "https://api.github.com/users/cyang49/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cyang49/subscriptions", "organizations_url": "https://api.github.com/users/cyang49/orgs", "repos_url": "https://api.github.com/users/cyang49/repos", "events_url": "https://api.github.com/users/cyang49/events{/privacy}", "received_events_url": "https://api.github.com/users/cyang49/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T19:49:27Z", "updated_at": "2018-03-30T20:02:24Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> Using <code>torch.cuda.device()</code> worked. However I put the call to <code>fn(rank, size)</code> in a loop and it has some error in rank 1 (the other one hangs) after a while:</p>\n<pre><code>1258th operation\nbegin rank 1\nRank  1  has data  \n 2\n[torch.cuda.FloatTensor of size () (GPU 0)]\n\n1259th operation\nbegin rank 1\nTraceback (most recent call last):\n  File \"toy.py\", line 33, in &lt;module&gt;\n    init_processes(args.rank, size, run, 'nccl')\n  File \"toy.py\", line 24, in init_processes\n    fn(rank, size)\n  File \"toy.py\", line 10, in run\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, internal error\n</code></pre>\n<p>The above error is reproducible on my system and happens at the same 1259th operation on rank 1.</p>\n<p>And I also got this other kind of error:</p>\n<pre><code>928th operation\nbegin rank 1\nmlx5: c460login01: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n00000001 00000000 00000000 00000000\n00000000 9d00c311 00012ecb 0003b3e2\nTraceback (most recent call last):\n  File \"toy.py\", line 33, in &lt;module&gt;\n    init_processes(args.rank, size, run, 'nccl')\n  File \"toy.py\", line 24, in init_processes\n    fn(rank, size)\n  File \"toy.py\", line 10, in run\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, unhandled system error\n</code></pre>\n<p>The code is here</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> argparse\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Simple point-to-point communication. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>begin rank<span class=\"pl-pds\">'</span></span>, rank)\n    group <span class=\"pl-k\">=</span> dist.new_group([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n    tensor <span class=\"pl-k\">=</span> torch.FloatTensor(torch.ones(<span class=\"pl-c1\">1</span>)).cuda()\n    dist.all_reduce(tensor, <span class=\"pl-v\">op</span><span class=\"pl-k\">=</span>dist.reduce_op.<span class=\"pl-c1\">SUM</span>, <span class=\"pl-v\">group</span><span class=\"pl-k\">=</span>group)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Rank <span class=\"pl-pds\">'</span></span>, rank, <span class=\"pl-s\"><span class=\"pl-pds\">'</span> has data <span class=\"pl-pds\">'</span></span>, tensor[<span class=\"pl-c1\">0</span>])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">init_processes</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">fn</span>, <span class=\"pl-smi\">backend</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Initialize the distributed environment. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before init<span class=\"pl-pds\">'</span></span>)\n    torch.cuda.device(rank)\n    init_method<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tcp://127.0.0.1:16543<span class=\"pl-pds\">\"</span></span>\n    dist.init_process_group(backend,<span class=\"pl-v\">rank</span><span class=\"pl-k\">=</span>rank,<span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>size,<span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span>init_method)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>after init<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000</span>):\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span>th operation<span class=\"pl-pds\">'</span></span>.format(i))\n      fn(rank, size)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--rank<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rank<span class=\"pl-pds\">'</span></span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n    init_processes(args.rank, size, run, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nccl<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "@apaszke @teng-li Using torch.cuda.device() worked. However I put the call to fn(rank, size) in a loop and it has some error in rank 1 (the other one hangs) after a while:\n1258th operation\nbegin rank 1\nRank  1  has data  \n 2\n[torch.cuda.FloatTensor of size () (GPU 0)]\n\n1259th operation\nbegin rank 1\nTraceback (most recent call last):\n  File \"toy.py\", line 33, in <module>\n    init_processes(args.rank, size, run, 'nccl')\n  File \"toy.py\", line 24, in init_processes\n    fn(rank, size)\n  File \"toy.py\", line 10, in run\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, internal error\n\nThe above error is reproducible on my system and happens at the same 1259th operation on rank 1.\nAnd I also got this other kind of error:\n928th operation\nbegin rank 1\nmlx5: c460login01: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n00000001 00000000 00000000 00000000\n00000000 9d00c311 00012ecb 0003b3e2\nTraceback (most recent call last):\n  File \"toy.py\", line 33, in <module>\n    init_processes(args.rank, size, run, 'nccl')\n  File \"toy.py\", line 24, in init_processes\n    fn(rank, size)\n  File \"toy.py\", line 10, in run\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, unhandled system error\n\nThe code is here\nimport torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    print('begin rank', rank)\n    group = dist.new_group([0, 1])\n    tensor = torch.FloatTensor(torch.ones(1)).cuda()\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    torch.cuda.device(rank)\n    init_method=\"tcp://127.0.0.1:16543\"\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\n    print('after init')\n    for i in range(100000):\n      print('{}th operation'.format(i))\n      fn(rank, size)\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,\n                        help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')", "body": "@apaszke @teng-li Using `torch.cuda.device()` worked. However I put the call to `fn(rank, size)` in a loop and it has some error in rank 1 (the other one hangs) after a while:\r\n```\r\n1258th operation\r\nbegin rank 1\r\nRank  1  has data  \r\n 2\r\n[torch.cuda.FloatTensor of size () (GPU 0)]\r\n\r\n1259th operation\r\nbegin rank 1\r\nTraceback (most recent call last):\r\n  File \"toy.py\", line 33, in <module>\r\n    init_processes(args.rank, size, run, 'nccl')\r\n  File \"toy.py\", line 24, in init_processes\r\n    fn(rank, size)\r\n  File \"toy.py\", line 10, in run\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\r\n    return torch._C._dist_all_reduce(tensor, op, group)\r\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, internal error\r\n```\r\nThe above error is reproducible on my system and happens at the same 1259th operation on rank 1.\r\n\r\nAnd I also got this other kind of error:\r\n```\r\n928th operation\r\nbegin rank 1\r\nmlx5: c460login01: got completion with error:\r\n00000000 00000000 00000000 00000000\r\n00000000 00000000 00000000 00000000\r\n00000001 00000000 00000000 00000000\r\n00000000 9d00c311 00012ecb 0003b3e2\r\nTraceback (most recent call last):\r\n  File \"toy.py\", line 33, in <module>\r\n    init_processes(args.rank, size, run, 'nccl')\r\n  File \"toy.py\", line 24, in init_processes\r\n    fn(rank, size)\r\n  File \"toy.py\", line 10, in run\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n  File \"/home/ccyang/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py\", line 326, in all_reduce\r\n    return torch._C._dist_all_reduce(tensor, op, group)\r\nRuntimeError: NCCL error in: /home/ccyang/gpfs/pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:324, unhandled system error\r\n```\r\n\r\nThe code is here\r\n```python\r\nimport torch\r\nimport torch.distributed as dist\r\nimport argparse\r\n\r\ndef run(rank, size):\r\n    \"\"\" Simple point-to-point communication. \"\"\"\r\n    print('begin rank', rank)\r\n    group = dist.new_group([0, 1])\r\n    tensor = torch.FloatTensor(torch.ones(1)).cuda()\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n    print('Rank ', rank, ' has data ', tensor[0])\r\n\r\ndef init_processes(rank, size, fn, backend):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    print('before init')\r\n    torch.cuda.device(rank)\r\n    init_method=\"tcp://127.0.0.1:16543\"\r\n    dist.init_process_group(backend,rank=rank,world_size=size,init_method=init_method)\r\n    print('after init')\r\n    for i in range(100000):\r\n      print('{}th operation'.format(i))\r\n      fn(rank, size)\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--rank', default=-1, type=int,\r\n                        help='rank')\r\n    args = parser.parse_args()\r\n    init_processes(args.rank, size, run, 'nccl')\r\n```"}