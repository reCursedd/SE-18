{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326221799", "html_url": "https://github.com/pytorch/pytorch/issues/2573#issuecomment-326221799", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2573", "id": 326221799, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjIyMTc5OQ==", "user": {"login": "dalegebit", "id": 9109634, "node_id": "MDQ6VXNlcjkxMDk2MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9109634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dalegebit", "html_url": "https://github.com/dalegebit", "followers_url": "https://api.github.com/users/dalegebit/followers", "following_url": "https://api.github.com/users/dalegebit/following{/other_user}", "gists_url": "https://api.github.com/users/dalegebit/gists{/gist_id}", "starred_url": "https://api.github.com/users/dalegebit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dalegebit/subscriptions", "organizations_url": "https://api.github.com/users/dalegebit/orgs", "repos_url": "https://api.github.com/users/dalegebit/repos", "events_url": "https://api.github.com/users/dalegebit/events{/privacy}", "received_events_url": "https://api.github.com/users/dalegebit/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T08:05:19Z", "updated_at": "2017-09-01T02:31:04Z", "author_association": "NONE", "body_html": "<p>Thank you for answering such stupid question! Sorry about the hasty sample code, but I did actually encounter similar problems with my model. When I trained it with multi gpus, I got zero gradient for all parameters. I tried debugging on torch.cuda.comm module, and finally confirmed this is the root of my problem. Here is the segment of code where weird thing happens.</p>\n<pre><code>File torch/cuda/comm.py   \n...\n[Line 90]\nif nccl.is_available(inputs) and inputs[0].get_device() == destination:\n    outputs = [result] + [t.new(t.size()) for t in inputs[1:]]\n    nccl.reduce(inputs, outputs, root=destination)\n    return result\n...\n</code></pre>\n<p>After executing <code>nccl.reduce(inputs, outputs, root=destination)</code>, I found all inputs are accumulated into the wrong destination (in this case, destination should be 1).</p>\n<pre><code>(Pdb) p outputs\n[\n 0\n 0\n 0\n\u22ee \n 0\n 0\n 0\n[torch.cuda.FloatTensor of size 5167872 (GPU 1)]\n, \n1.00000e-12 *\n  0.0000\n  0.0000\n  0.0000\n    \u22ee   \n  0.0000\n  0.0000\n  0.0000\n[torch.cuda.FloatTensor of size 5167872 (GPU 0)]\n]\n</code></pre>\n<p>Therefore, all the gradient returns become zero. I reckon this may relate to the different interpretations of device id for nccl and torch. Can anyone explain this?</p>", "body_text": "Thank you for answering such stupid question! Sorry about the hasty sample code, but I did actually encounter similar problems with my model. When I trained it with multi gpus, I got zero gradient for all parameters. I tried debugging on torch.cuda.comm module, and finally confirmed this is the root of my problem. Here is the segment of code where weird thing happens.\nFile torch/cuda/comm.py   \n...\n[Line 90]\nif nccl.is_available(inputs) and inputs[0].get_device() == destination:\n    outputs = [result] + [t.new(t.size()) for t in inputs[1:]]\n    nccl.reduce(inputs, outputs, root=destination)\n    return result\n...\n\nAfter executing nccl.reduce(inputs, outputs, root=destination), I found all inputs are accumulated into the wrong destination (in this case, destination should be 1).\n(Pdb) p outputs\n[\n 0\n 0\n 0\n\u22ee \n 0\n 0\n 0\n[torch.cuda.FloatTensor of size 5167872 (GPU 1)]\n, \n1.00000e-12 *\n  0.0000\n  0.0000\n  0.0000\n    \u22ee   \n  0.0000\n  0.0000\n  0.0000\n[torch.cuda.FloatTensor of size 5167872 (GPU 0)]\n]\n\nTherefore, all the gradient returns become zero. I reckon this may relate to the different interpretations of device id for nccl and torch. Can anyone explain this?", "body": "Thank you for answering such stupid question! Sorry about the hasty sample code, but I did actually encounter similar problems with my model. When I trained it with multi gpus, I got zero gradient for all parameters. I tried debugging on torch.cuda.comm module, and finally confirmed this is the root of my problem. Here is the segment of code where weird thing happens.\r\n\r\n``` \r\nFile torch/cuda/comm.py   \r\n...\r\n[Line 90]\r\nif nccl.is_available(inputs) and inputs[0].get_device() == destination:\r\n    outputs = [result] + [t.new(t.size()) for t in inputs[1:]]\r\n    nccl.reduce(inputs, outputs, root=destination)\r\n    return result\r\n...\r\n``` \r\nAfter executing `nccl.reduce(inputs, outputs, root=destination)`, I found all inputs are accumulated into the wrong destination (in this case, destination should be 1).\r\n```\r\n(Pdb) p outputs\r\n[\r\n 0\r\n 0\r\n 0\r\n\u22ee \r\n 0\r\n 0\r\n 0\r\n[torch.cuda.FloatTensor of size 5167872 (GPU 1)]\r\n, \r\n1.00000e-12 *\r\n  0.0000\r\n  0.0000\r\n  0.0000\r\n    \u22ee   \r\n  0.0000\r\n  0.0000\r\n  0.0000\r\n[torch.cuda.FloatTensor of size 5167872 (GPU 0)]\r\n]\r\n```\r\nTherefore, all the gradient returns become zero. I reckon this may relate to the different interpretations of device id for nccl and torch. Can anyone explain this?"}