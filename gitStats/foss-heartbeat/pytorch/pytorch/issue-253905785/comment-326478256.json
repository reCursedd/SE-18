{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326478256", "html_url": "https://github.com/pytorch/pytorch/issues/2573#issuecomment-326478256", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2573", "id": 326478256, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjQ3ODI1Ng==", "user": {"login": "dalegebit", "id": 9109634, "node_id": "MDQ6VXNlcjkxMDk2MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9109634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dalegebit", "html_url": "https://github.com/dalegebit", "followers_url": "https://api.github.com/users/dalegebit/followers", "following_url": "https://api.github.com/users/dalegebit/following{/other_user}", "gists_url": "https://api.github.com/users/dalegebit/gists{/gist_id}", "starred_url": "https://api.github.com/users/dalegebit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dalegebit/subscriptions", "organizations_url": "https://api.github.com/users/dalegebit/orgs", "repos_url": "https://api.github.com/users/dalegebit/repos", "events_url": "https://api.github.com/users/dalegebit/events{/privacy}", "received_events_url": "https://api.github.com/users/dalegebit/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-01T03:29:06Z", "updated_at": "2017-09-01T03:29:06Z", "author_association": "NONE", "body_html": "<p>Sure.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\ntorch.cuda.set_device(1)\na = Variable(torch.rand(5,6).cuda(), requires_grad=True)\nl = torch.nn.Linear(6,7).cuda()\np_l = torch.nn.DataParallel(l, [1,0])\np_l(a).sum().backward()\nprint l.weight.grad\n</code></pre>\n<p>I found that directly setting this line <a href=\"url\">https://github.com/pytorch/pytorch/blob/master/torch/cuda/comm.py#L92</a> to <code>nccl.reduce(inputs, outputs, root=0)</code> temporarily fixes my problem.</p>", "body_text": "Sure.\nimport torch\nfrom torch.autograd import Variable\n\ntorch.cuda.set_device(1)\na = Variable(torch.rand(5,6).cuda(), requires_grad=True)\nl = torch.nn.Linear(6,7).cuda()\np_l = torch.nn.DataParallel(l, [1,0])\np_l(a).sum().backward()\nprint l.weight.grad\n\nI found that directly setting this line https://github.com/pytorch/pytorch/blob/master/torch/cuda/comm.py#L92 to nccl.reduce(inputs, outputs, root=0) temporarily fixes my problem.", "body": "Sure.\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ntorch.cuda.set_device(1)\r\na = Variable(torch.rand(5,6).cuda(), requires_grad=True)\r\nl = torch.nn.Linear(6,7).cuda()\r\np_l = torch.nn.DataParallel(l, [1,0])\r\np_l(a).sum().backward()\r\nprint l.weight.grad\r\n```\r\nI found that directly setting this line [https://github.com/pytorch/pytorch/blob/master/torch/cuda/comm.py#L92](url) to `nccl.reduce(inputs, outputs, root=0)` temporarily fixes my problem."}