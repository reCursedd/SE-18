{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7235", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7235/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7235/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7235/events", "html_url": "https://github.com/pytorch/pytorch/issues/7235", "id": 319819999, "node_id": "MDU6SXNzdWUzMTk4MTk5OTk=", "number": 7235, "title": "ONNX FATAL: Don't know how to translate op ATen", "user": {"login": "MahdiNazemi", "id": 11276349, "node_id": "MDQ6VXNlcjExMjc2MzQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/11276349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MahdiNazemi", "html_url": "https://github.com/MahdiNazemi", "followers_url": "https://api.github.com/users/MahdiNazemi/followers", "following_url": "https://api.github.com/users/MahdiNazemi/following{/other_user}", "gists_url": "https://api.github.com/users/MahdiNazemi/gists{/gist_id}", "starred_url": "https://api.github.com/users/MahdiNazemi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MahdiNazemi/subscriptions", "organizations_url": "https://api.github.com/users/MahdiNazemi/orgs", "repos_url": "https://api.github.com/users/MahdiNazemi/repos", "events_url": "https://api.github.com/users/MahdiNazemi/events{/privacy}", "received_events_url": "https://api.github.com/users/MahdiNazemi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-03T07:56:28Z", "updated_at": "2018-05-03T19:07:27Z", "closed_at": "2018-05-03T19:07:26Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>I'm trying to convert a PyTorch model to Caffe2 through ONNX. I have installed PyTorch, Caffe2, and ONNX from source using the following command:</p>\n<p><code>python setup.py build develop --user</code></p>\n<p>I have enabled <code>USE_ATEN</code> in Caffe2's <code>CMakeLists.txt</code>.</p>\n<p>I need to use one of <code>ATen</code>'s functions that is not supported in ONNX. I do this based on <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/contrib/aten/docs/pytorch_to_caffe2.md\">this documentation</a>.</p>\n<h2>Code example</h2>\n<p>The code I run is the sample that is available <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/contrib/aten/docs/sample.py\">here</a>.</p>\n<pre><code>import numpy as np\n\nfrom torch import nn\nfrom torch.autograd import Variable, Function\nimport torch.onnx\n\nimport onnx\nimport caffe2.python.onnx.backend\n\nclass MyFunction(Function):\n    @staticmethod\n    def forward(ctx, x, y):\n        return x*x + y\n    @staticmethod\n    def symbolic(graph, x, y):\n        x2 = graph.at(\"mul\", x, x)\n        r = graph.at(\"add\", x2, y)\n        # x, y, x2, and r are 'Node' objects\n        # print(r) or print(graph) will print out a textual representation for debugging.\n        # this representation will be converted to ONNX protobufs on export.\n        return r\n\nclass MyModule(nn.Module):\n    def forward(self, x, y):\n        # you can combine your ATen ops with standard onnx ones\n        x = nn.ReLU()(x)\n        return MyFunction.apply(x, y)\n\ntorch.onnx.export(MyModule(),\n                  (Variable(torch.ones(3,4)), Variable(torch.ones(3,4))),\n                  \"output.onnx\",\n                  verbose=True)\n\n# prints the graph for debugging:\n# graph(%1 : Float(3, 4)\n#       %2 : Float(3, 4)) {\n#   %3 : Float(3, 4) = Relu(%1), uses = [%4.i0, %4.i1];\n#   %4 : UNKNOWN_TYPE = ATen[operator=mul](%3, %3), uses = [%5.i0];\n#   %5 : Float(3, 4) = ATen[operator=add](%4, %2), uses = [%0.i0];\n#   return (%5);\n# }\n\ngraph = onnx.load(\"output.onnx\")\n\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(3, 4).astype(np.float32)\n\nprepared_backend = caffe2.python.onnx.backend.prepare(graph)\nW = {graph.graph.input[0].name: a, graph.graph.input[1].name: b}\nc2_out = prepared_backend.run(W)[0]\n\nx = np.maximum(a, 0)\nr = x*x + b\nnp.testing.assert_array_almost_equal(r, c2_out)\n</code></pre>\n<p>After running <code>prepared_backend = caffe2.python.onnx.backend.prepare(graph)</code>, I get the following error:</p>\n<p>ONNX FATAL: Don't know how to translate op ATen</p>\n<h2>System Information</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.5.0a0+d154d32<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Debian GNU/Linux 8.10 (jessie)<br>\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2<br>\nCMake version: version 3.11.1</p>\n<p>Python version: 2.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0:<br>\nGPU 1:</p>\n<p>Nvidia driver version: 390.12<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-7.5/lib64/libcudnn.so<br>\n/usr/local/cuda-7.5/lib64/libcudnn.so.5<br>\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.1.3<br>\n/usr/local/cuda-7.5/lib64/libcudnn_static.a<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so.7<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5<br>\n/usr/local/cuda-9.0/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.1)<br>\n[pip] torch (0.5.0a0+d154d32, pytorch)<br>\n[pip] torchvision (0.2.0)<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nI'm trying to convert a PyTorch model to Caffe2 through ONNX. I have installed PyTorch, Caffe2, and ONNX from source using the following command:\npython setup.py build develop --user\nI have enabled USE_ATEN in Caffe2's CMakeLists.txt.\nI need to use one of ATen's functions that is not supported in ONNX. I do this based on this documentation.\nCode example\nThe code I run is the sample that is available here.\nimport numpy as np\n\nfrom torch import nn\nfrom torch.autograd import Variable, Function\nimport torch.onnx\n\nimport onnx\nimport caffe2.python.onnx.backend\n\nclass MyFunction(Function):\n    @staticmethod\n    def forward(ctx, x, y):\n        return x*x + y\n    @staticmethod\n    def symbolic(graph, x, y):\n        x2 = graph.at(\"mul\", x, x)\n        r = graph.at(\"add\", x2, y)\n        # x, y, x2, and r are 'Node' objects\n        # print(r) or print(graph) will print out a textual representation for debugging.\n        # this representation will be converted to ONNX protobufs on export.\n        return r\n\nclass MyModule(nn.Module):\n    def forward(self, x, y):\n        # you can combine your ATen ops with standard onnx ones\n        x = nn.ReLU()(x)\n        return MyFunction.apply(x, y)\n\ntorch.onnx.export(MyModule(),\n                  (Variable(torch.ones(3,4)), Variable(torch.ones(3,4))),\n                  \"output.onnx\",\n                  verbose=True)\n\n# prints the graph for debugging:\n# graph(%1 : Float(3, 4)\n#       %2 : Float(3, 4)) {\n#   %3 : Float(3, 4) = Relu(%1), uses = [%4.i0, %4.i1];\n#   %4 : UNKNOWN_TYPE = ATen[operator=mul](%3, %3), uses = [%5.i0];\n#   %5 : Float(3, 4) = ATen[operator=add](%4, %2), uses = [%0.i0];\n#   return (%5);\n# }\n\ngraph = onnx.load(\"output.onnx\")\n\na = np.random.randn(3, 4).astype(np.float32)\nb = np.random.randn(3, 4).astype(np.float32)\n\nprepared_backend = caffe2.python.onnx.backend.prepare(graph)\nW = {graph.graph.input[0].name: a, graph.graph.input[1].name: b}\nc2_out = prepared_backend.run(W)[0]\n\nx = np.maximum(a, 0)\nr = x*x + b\nnp.testing.assert_array_almost_equal(r, c2_out)\n\nAfter running prepared_backend = caffe2.python.onnx.backend.prepare(graph), I get the following error:\nONNX FATAL: Don't know how to translate op ATen\nSystem Information\nCollecting environment information...\nPyTorch version: 0.5.0a0+d154d32\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Debian GNU/Linux 8.10 (jessie)\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2\nCMake version: version 3.11.1\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0:\nGPU 1:\nNvidia driver version: 390.12\ncuDNN version: Probably one of the following:\n/usr/local/cuda-7.5/lib64/libcudnn.so\n/usr/local/cuda-7.5/lib64/libcudnn.so.5\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.1.3\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\n/usr/local/cuda-9.0/lib64/libcudnn.so\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.14.1)\n[pip] torch (0.5.0a0+d154d32, pytorch)\n[pip] torchvision (0.2.0)\n[conda] Could not collect", "body": "## Issue description\r\n\r\nI'm trying to convert a PyTorch model to Caffe2 through ONNX. I have installed PyTorch, Caffe2, and ONNX from source using the following command:\r\n\r\n`python setup.py build develop --user`\r\n\r\nI have enabled `USE_ATEN` in Caffe2's `CMakeLists.txt`.\r\n\r\nI need to use one of `ATen`'s functions that is not supported in ONNX. I do this based on [this documentation](https://github.com/pytorch/pytorch/blob/master/caffe2/contrib/aten/docs/pytorch_to_caffe2.md).\r\n\r\n## Code example\r\n\r\nThe code I run is the sample that is available [here](https://github.com/pytorch/pytorch/blob/master/caffe2/contrib/aten/docs/sample.py).\r\n\r\n```\r\nimport numpy as np\r\n\r\nfrom torch import nn\r\nfrom torch.autograd import Variable, Function\r\nimport torch.onnx\r\n\r\nimport onnx\r\nimport caffe2.python.onnx.backend\r\n\r\nclass MyFunction(Function):\r\n    @staticmethod\r\n    def forward(ctx, x, y):\r\n        return x*x + y\r\n    @staticmethod\r\n    def symbolic(graph, x, y):\r\n        x2 = graph.at(\"mul\", x, x)\r\n        r = graph.at(\"add\", x2, y)\r\n        # x, y, x2, and r are 'Node' objects\r\n        # print(r) or print(graph) will print out a textual representation for debugging.\r\n        # this representation will be converted to ONNX protobufs on export.\r\n        return r\r\n\r\nclass MyModule(nn.Module):\r\n    def forward(self, x, y):\r\n        # you can combine your ATen ops with standard onnx ones\r\n        x = nn.ReLU()(x)\r\n        return MyFunction.apply(x, y)\r\n\r\ntorch.onnx.export(MyModule(),\r\n                  (Variable(torch.ones(3,4)), Variable(torch.ones(3,4))),\r\n                  \"output.onnx\",\r\n                  verbose=True)\r\n\r\n# prints the graph for debugging:\r\n# graph(%1 : Float(3, 4)\r\n#       %2 : Float(3, 4)) {\r\n#   %3 : Float(3, 4) = Relu(%1), uses = [%4.i0, %4.i1];\r\n#   %4 : UNKNOWN_TYPE = ATen[operator=mul](%3, %3), uses = [%5.i0];\r\n#   %5 : Float(3, 4) = ATen[operator=add](%4, %2), uses = [%0.i0];\r\n#   return (%5);\r\n# }\r\n\r\ngraph = onnx.load(\"output.onnx\")\r\n\r\na = np.random.randn(3, 4).astype(np.float32)\r\nb = np.random.randn(3, 4).astype(np.float32)\r\n\r\nprepared_backend = caffe2.python.onnx.backend.prepare(graph)\r\nW = {graph.graph.input[0].name: a, graph.graph.input[1].name: b}\r\nc2_out = prepared_backend.run(W)[0]\r\n\r\nx = np.maximum(a, 0)\r\nr = x*x + b\r\nnp.testing.assert_array_almost_equal(r, c2_out)\r\n```\r\n\r\nAfter running `prepared_backend = caffe2.python.onnx.backend.prepare(graph)`, I get the following error:\r\n\r\nONNX FATAL: Don't know how to translate op ATen\r\n\r\n## System Information\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+d154d32\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 8.10 (jessie)\r\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2\r\nCMake version: version 3.11.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: \r\nGPU 1: \r\n\r\nNvidia driver version: 390.12\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-7.5/lib64/libcudnn.so\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.1.3\r\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.0/lib64/libcudnn.so\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.1)\r\n[pip] torch (0.5.0a0+d154d32, pytorch)\r\n[pip] torchvision (0.2.0)\r\n[conda] Could not collect"}