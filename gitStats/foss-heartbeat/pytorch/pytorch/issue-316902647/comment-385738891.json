{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385738891", "html_url": "https://github.com/pytorch/pytorch/pull/6867#issuecomment-385738891", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6867", "id": 385738891, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTczODg5MQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-01T17:51:22Z", "updated_at": "2018-05-01T17:51:22Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">So it\u2019s strides, considering the current code only works on contiguous\ntensora. Can we just use strides as I suggested above.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, May 2, 2018 at 01:18 Wei Yang ***@***.***&gt; wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In aten/src/ATen/native/cuda/TensorTransformations.cu\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"316902647\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6867\" href=\"https://github.com/pytorch/pytorch/pull/6867#discussion_r185277094\">#6867 (comment)</a>&gt;:\n\n &gt; +  }\n +\n +  Tensor flip_dims_t = at::zeros(CPU(kLong), {flip_dims_size});\n +  int64_t* flip_dims_t_d = flip_dims_t.data&lt;int64_t&gt;();\n +  for (int64_t i = 0; i &lt; flip_dims_size; i++) {\n +    flip_dims_t_d[i] = dims[i];\n +  }\n +\n +  auto shape = self.sizes();\n +  Tensor shape_t = at::zeros(CPU(kLong), {total_dims});\n +  int64_t* shape_t_d = shape_t.data&lt;int64_t&gt;();\n +  for (int64_t i = 0; i &lt; total_dims; i++) {\n +    shape_t_d[i] = shape[i];\n +  }\n +\n +  Tensor each_dim_len = at::zeros(CPU(kLong), {total_dims});\n\n oops... sorry I misinterpreted the question. \"sizes of a tensor\" is\n actually the same as \"shape\" here. \"each_dim_len\" is different, it is\n defined as the total number of elements in a subarray of the current\n dimension. In the example t = [[1,2], [3,4], [5,6]], I can visualize its\n 1st dim as rows, and 2nd dim as columns. If current dimension is 0 (row),\n then each_dim_len[0] = 2; if current dimension if 1 (col), then\n each_dim_len[1] = 1. So here each_dim_len = (2, 1). \"each_dim_len\" is\n mainly for the conversion between indices and linear_index\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"316902647\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6867\" href=\"https://github.com/pytorch/pytorch/pull/6867#discussion_r185277094\">#6867 (comment)</a>&gt;, or mute\n the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AFaWZV5dt57Ekp9pDX2XSOZTCGcXubBwks5tuJjcgaJpZM4TgSEk\">https://github.com/notifications/unsubscribe-auth/AFaWZV5dt57Ekp9pDX2XSOZTCGcXubBwks5tuJjcgaJpZM4TgSEk</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "So it\u2019s strides, considering the current code only works on contiguous\ntensora. Can we just use strides as I suggested above.\n\u2026\nOn Wed, May 2, 2018 at 01:18 Wei Yang ***@***.***> wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In aten/src/ATen/native/cuda/TensorTransformations.cu\n <#6867 (comment)>:\n\n > +  }\n +\n +  Tensor flip_dims_t = at::zeros(CPU(kLong), {flip_dims_size});\n +  int64_t* flip_dims_t_d = flip_dims_t.data<int64_t>();\n +  for (int64_t i = 0; i < flip_dims_size; i++) {\n +    flip_dims_t_d[i] = dims[i];\n +  }\n +\n +  auto shape = self.sizes();\n +  Tensor shape_t = at::zeros(CPU(kLong), {total_dims});\n +  int64_t* shape_t_d = shape_t.data<int64_t>();\n +  for (int64_t i = 0; i < total_dims; i++) {\n +    shape_t_d[i] = shape[i];\n +  }\n +\n +  Tensor each_dim_len = at::zeros(CPU(kLong), {total_dims});\n\n oops... sorry I misinterpreted the question. \"sizes of a tensor\" is\n actually the same as \"shape\" here. \"each_dim_len\" is different, it is\n defined as the total number of elements in a subarray of the current\n dimension. In the example t = [[1,2], [3,4], [5,6]], I can visualize its\n 1st dim as rows, and 2nd dim as columns. If current dimension is 0 (row),\n then each_dim_len[0] = 2; if current dimension if 1 (col), then\n each_dim_len[1] = 1. So here each_dim_len = (2, 1). \"each_dim_len\" is\n mainly for the conversion between indices and linear_index\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#6867 (comment)>, or mute\n the thread\n <https://github.com/notifications/unsubscribe-auth/AFaWZV5dt57Ekp9pDX2XSOZTCGcXubBwks5tuJjcgaJpZM4TgSEk>\n .", "body": "So it\u2019s strides, considering the current code only works on contiguous\ntensora. Can we just use strides as I suggested above.\n\nOn Wed, May 2, 2018 at 01:18 Wei Yang <notifications@github.com> wrote:\n\n> *@weiyangfb* commented on this pull request.\n> ------------------------------\n>\n> In aten/src/ATen/native/cuda/TensorTransformations.cu\n> <https://github.com/pytorch/pytorch/pull/6867#discussion_r185277094>:\n>\n> > +  }\n> +\n> +  Tensor flip_dims_t = at::zeros(CPU(kLong), {flip_dims_size});\n> +  int64_t* flip_dims_t_d = flip_dims_t.data<int64_t>();\n> +  for (int64_t i = 0; i < flip_dims_size; i++) {\n> +    flip_dims_t_d[i] = dims[i];\n> +  }\n> +\n> +  auto shape = self.sizes();\n> +  Tensor shape_t = at::zeros(CPU(kLong), {total_dims});\n> +  int64_t* shape_t_d = shape_t.data<int64_t>();\n> +  for (int64_t i = 0; i < total_dims; i++) {\n> +    shape_t_d[i] = shape[i];\n> +  }\n> +\n> +  Tensor each_dim_len = at::zeros(CPU(kLong), {total_dims});\n>\n> oops... sorry I misinterpreted the question. \"sizes of a tensor\" is\n> actually the same as \"shape\" here. \"each_dim_len\" is different, it is\n> defined as the total number of elements in a subarray of the current\n> dimension. In the example t = [[1,2], [3,4], [5,6]], I can visualize its\n> 1st dim as rows, and 2nd dim as columns. If current dimension is 0 (row),\n> then each_dim_len[0] = 2; if current dimension if 1 (col), then\n> each_dim_len[1] = 1. So here each_dim_len = (2, 1). \"each_dim_len\" is\n> mainly for the conversion between indices and linear_index\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/pull/6867#discussion_r185277094>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZV5dt57Ekp9pDX2XSOZTCGcXubBwks5tuJjcgaJpZM4TgSEk>\n> .\n>\n"}