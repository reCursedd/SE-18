{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/184423243", "pull_request_review_id": 115617464, "id": 184423243, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NDQyMzI0Mw==", "diff_hunk": "@@ -0,0 +1,160 @@\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/ATen.h\"\n+#include <algorithm>\n+#include <sstream>\n+\n+#include \"ATen/cuda/AccumulateType.cuh\"\n+#include \"ATen/cuda/CUDATensorMethods.cuh\"\n+#include \"ATen/cuda/CUDATypeConversion.cuh\"\n+\n+\n+namespace at {\n+namespace native {\n+\n+// Map the index of an element in tensor from 1D to nD\n+__device__ __forceinline__\n+void oneD_to_nD(int64_t oneD_index, int64_t* shape_size, int64_t shape_len, int64_t* nD_index) {\n+  int64_t res = oneD_index;\n+  for (int i = 0; i < shape_len; i++) {\n+    int64_t nD_i = oneD_index * shape_len + i;\n+    nD_index[nD_i] = res / shape_size[i];\n+    res = res % shape_size[i];\n+  }\n+}\n+\n+/*\n+Map the index of an element in tensor from nD to 1D. A tensor is originally in nD shape,\n+and 1D is the unfolded version of it (a vector).\n+\n+Example: given a 3D tensor\n+[\n+  [ [1,2], [3,4] ],\n+  [ [5,6], [7,8] ]\n+]\n+\n+Here element 3 has nD index = (0,1,0), and this corresponds to oneD index = 2\n+*/\n+__device__ __forceinline__\n+int64_t nD_to_oneD(int64_t* nD_index, int64_t shape_len, int64_t* shape_size, int64_t src_oneD_index) {\n+  int64_t dest_oneD_index = 0;\n+  for (int i = 0; i < shape_len; i++) {\n+    int64_t nD_i = src_oneD_index * shape_len + i;\n+    dest_oneD_index += nD_index[nD_i] * shape_size[i];\n+  }\n+  return dest_oneD_index;\n+}\n+\n+template <typename T>\n+__global__\n+void flip_cuda_kernel(T* in_t, T* out_t, int64_t N, int64_t* dims, int64_t* nD_index,\n+  int64_t dims_len, int64_t* shape_size, int64_t* shape, int64_t shape_len) {\n+\n+  int64_t oneD_index = blockIdx.x * blockDim.x + threadIdx.x;\n+  if (oneD_index >= N) {\n+    return;\n+  }\n+\n+  oneD_to_nD(oneD_index, shape_size, shape_len, nD_index);\n+  // flip nD index along each desired dimension\n+  for (int i = 0 ; i < dims_len; i++) {\n+    int64_t d = dims[i];\n+    int64_t nD_d = oneD_index * shape_len + d;\n+    nD_index[nD_d] = shape[d]-1-nD_index[nD_d];\n+  }\n+  int64_t dest_oneD_index = nD_to_oneD(nD_index, shape_len, shape_size, oneD_index);\n+  out_t[oneD_index] = in_t[dest_oneD_index];\n+}\n+\n+Tensor flip_cuda(const Tensor& self, IntList dims) {", "path": "aten/src/ATen/native/cuda/TensorTransformations.cu", "position": null, "original_position": 68, "commit_id": "f385f42db7dccc55ef84731a42624d797228e052", "original_commit_id": "c7a9b89336fa6c14622e4baf9aeab21095f31001", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "I think we're making a hard assumption that `self` is contiguous. If this is the case, we should either be\r\n1) operating on a contiguous tensor: use `self.contiguous()`\r\n2) Making this algorithm work for non-contiguous tensors. I don't think this is easy, so we should probably not aim for this.", "created_at": "2018-04-26T15:05:02Z", "updated_at": "2018-11-23T15:43:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/6867#discussion_r184423243", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6867", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/184423243"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6867#discussion_r184423243"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6867"}}, "body_html": "<p>I think we're making a hard assumption that <code>self</code> is contiguous. If this is the case, we should either be</p>\n<ol>\n<li>operating on a contiguous tensor: use <code>self.contiguous()</code></li>\n<li>Making this algorithm work for non-contiguous tensors. I don't think this is easy, so we should probably not aim for this.</li>\n</ol>", "body_text": "I think we're making a hard assumption that self is contiguous. If this is the case, we should either be\n\noperating on a contiguous tensor: use self.contiguous()\nMaking this algorithm work for non-contiguous tensors. I don't think this is easy, so we should probably not aim for this."}