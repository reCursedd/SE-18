{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183772061", "pull_request_review_id": 114833555, "id": 183772061, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Mzc3MjA2MQ==", "diff_hunk": "@@ -760,6 +760,16 @@\n - func: transpose_(Tensor self, int64_t dim0, int64_t dim1) -> Tensor\n   variants: method\n \n+- func: flip(Tensor self, IntList dims) -> Tensor\n+  dispatch:\n+    CPU: flip_cpu\n+    CUDA: flip_cuda\n+\n+- func: flip_backward(Tensor grad, IntList dims) -> Tensor", "path": "aten/src/ATen/native/native_functions.yaml", "position": null, "original_position": 25, "commit_id": "f385f42db7dccc55ef84731a42624d797228e052", "original_commit_id": "86dff1b7cb414bd74cc80a334beebff5e2fd0e67", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "`fn` should be put into `native_functions.yaml` when `fn` \r\n1. needs custom backward definition (either autograd doesn't know how to backward or it needs more efficient custom backward)\r\n2. needs generated python bindings.\r\n\r\nFor a native function, there are usually three ways to define backward:\r\n1. a simple formula in `derivatives.yaml`\r\n```yaml\r\n- name: atan(Tensor self)\r\n  self: grad / (self * self + 1)\r\n```\r\n2. calling a complex function in `derivatives.yaml`\r\n```yaml\r\n- name: det(Tensor self)\r\n  self: det_backward(grad, self, result)\r\n```\r\nwith the function (`det_backward` here) defined usually in `Functions.cpp` (or as another native function if it needs custom backward)\r\n3. without any entry. autograd will attempt to use the function calls in `fn` to do backward, e.g. `stft`. Note that this will be problematic if you do any low-level manipulations, because those are not tracked by autograd.", "created_at": "2018-04-24T15:15:11Z", "updated_at": "2018-11-23T15:43:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/6867#discussion_r183772061", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6867", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183772061"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6867#discussion_r183772061"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6867"}}, "body_html": "<p><code>fn</code> should be put into <code>native_functions.yaml</code> when <code>fn</code></p>\n<ol>\n<li>needs custom backward definition (either autograd doesn't know how to backward or it needs more efficient custom backward)</li>\n<li>needs generated python bindings.</li>\n</ol>\n<p>For a native function, there are usually three ways to define backward:</p>\n<ol>\n<li>a simple formula in <code>derivatives.yaml</code></li>\n</ol>\n<div class=\"highlight highlight-source-yaml\"><pre>- <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">atan(Tensor self)</span>\n  <span class=\"pl-ent\">self</span>: <span class=\"pl-s\">grad / (self * self + 1)</span></pre></div>\n<ol start=\"2\">\n<li>calling a complex function in <code>derivatives.yaml</code></li>\n</ol>\n<div class=\"highlight highlight-source-yaml\"><pre>- <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">det(Tensor self)</span>\n  <span class=\"pl-ent\">self</span>: <span class=\"pl-s\">det_backward(grad, self, result)</span></pre></div>\n<p>with the function (<code>det_backward</code> here) defined usually in <code>Functions.cpp</code> (or as another native function if it needs custom backward)<br>\n3. without any entry. autograd will attempt to use the function calls in <code>fn</code> to do backward, e.g. <code>stft</code>. Note that this will be problematic if you do any low-level manipulations, because those are not tracked by autograd.</p>", "body_text": "fn should be put into native_functions.yaml when fn\n\nneeds custom backward definition (either autograd doesn't know how to backward or it needs more efficient custom backward)\nneeds generated python bindings.\n\nFor a native function, there are usually three ways to define backward:\n\na simple formula in derivatives.yaml\n\n- name: atan(Tensor self)\n  self: grad / (self * self + 1)\n\ncalling a complex function in derivatives.yaml\n\n- name: det(Tensor self)\n  self: det_backward(grad, self, result)\nwith the function (det_backward here) defined usually in Functions.cpp (or as another native function if it needs custom backward)\n3. without any entry. autograd will attempt to use the function calls in fn to do backward, e.g. stft. Note that this will be problematic if you do any low-level manipulations, because those are not tracked by autograd.", "in_reply_to_id": 183502870}