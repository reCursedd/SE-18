{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358096525", "html_url": "https://github.com/pytorch/pytorch/issues/4661#issuecomment-358096525", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4661", "id": 358096525, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODA5NjUyNQ==", "user": {"login": "benvcutilli", "id": 23709161, "node_id": "MDQ6VXNlcjIzNzA5MTYx", "avatar_url": "https://avatars1.githubusercontent.com/u/23709161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benvcutilli", "html_url": "https://github.com/benvcutilli", "followers_url": "https://api.github.com/users/benvcutilli/followers", "following_url": "https://api.github.com/users/benvcutilli/following{/other_user}", "gists_url": "https://api.github.com/users/benvcutilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/benvcutilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benvcutilli/subscriptions", "organizations_url": "https://api.github.com/users/benvcutilli/orgs", "repos_url": "https://api.github.com/users/benvcutilli/repos", "events_url": "https://api.github.com/users/benvcutilli/events{/privacy}", "received_events_url": "https://api.github.com/users/benvcutilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-16T20:35:30Z", "updated_at": "2018-01-16T20:35:30Z", "author_association": "NONE", "body_html": "<p>I put the creation of the variable in the loop (to make it consistent with my first snippet), and still leaks (even faster):</p>\n<pre><code>for rep in range(1000000):\n  x = torch.autograd.Variable(torch.ones(1).cuda(), requires_grad=True)\n  (x*x).backward(create_graph=True)\n</code></pre>\n<p>Also, I realize that there may be some tough design decisions when it comes to this kind of stuff, but changing the paradigm from Variable-oriented calling to <code>torch.autograd</code>-module-oriented calling would be very confusing and not intuitive. Just something to consider.</p>", "body_text": "I put the creation of the variable in the loop (to make it consistent with my first snippet), and still leaks (even faster):\nfor rep in range(1000000):\n  x = torch.autograd.Variable(torch.ones(1).cuda(), requires_grad=True)\n  (x*x).backward(create_graph=True)\n\nAlso, I realize that there may be some tough design decisions when it comes to this kind of stuff, but changing the paradigm from Variable-oriented calling to torch.autograd-module-oriented calling would be very confusing and not intuitive. Just something to consider.", "body": "I put the creation of the variable in the loop (to make it consistent with my first snippet), and still leaks (even faster):\r\n\r\n```\r\nfor rep in range(1000000):\r\n  x = torch.autograd.Variable(torch.ones(1).cuda(), requires_grad=True)\r\n  (x*x).backward(create_graph=True)\r\n```\r\n\r\nAlso, I realize that there may be some tough design decisions when it comes to this kind of stuff, but changing the paradigm from Variable-oriented calling to `torch.autograd`-module-oriented calling would be very confusing and not intuitive. Just something to consider."}