{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358427344", "html_url": "https://github.com/pytorch/pytorch/issues/4661#issuecomment-358427344", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4661", "id": 358427344, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODQyNzM0NA==", "user": {"login": "benvcutilli", "id": 23709161, "node_id": "MDQ6VXNlcjIzNzA5MTYx", "avatar_url": "https://avatars1.githubusercontent.com/u/23709161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benvcutilli", "html_url": "https://github.com/benvcutilli", "followers_url": "https://api.github.com/users/benvcutilli/followers", "following_url": "https://api.github.com/users/benvcutilli/following{/other_user}", "gists_url": "https://api.github.com/users/benvcutilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/benvcutilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benvcutilli/subscriptions", "organizations_url": "https://api.github.com/users/benvcutilli/orgs", "repos_url": "https://api.github.com/users/benvcutilli/repos", "events_url": "https://api.github.com/users/benvcutilli/events{/privacy}", "received_events_url": "https://api.github.com/users/benvcutilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-17T20:05:10Z", "updated_at": "2018-01-17T20:23:57Z", "author_association": "NONE", "body_html": "<p>Yeah, I understand that, and agree that that could be confusing (I just zero out the gradients, but I suppose that someone who doesn't have a good understanding of how PyTorch works could be confused by that; I'd have to think about that a little to make that more intuitive). I like the <code>.backward()</code> semantics because it is easy to read and easy to program. For example, as long as it isn't too verbose, you could rifle off something like this, which takes one line:</p>\n<pre><code>(batch.grad).pow(2).sum().backward()\n</code></pre>\n<p>where there is a clear flow from left-to-right. This is nice for the programmer and the reader. Of course, you can also just do, as you and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> suggested (not sure if my parameters are ordered correctly -- which may be another downside of the following method, unless <code>kwargs</code> are accepted),</p>\n<pre><code>torch.autograd.grad( variableToOptimize, (batch.grad).pow(2).sum() )\n</code></pre>\n<p>and the programmer understands what is going on (but may not understand it immediately when the programmer comes back months later to work with it again), but the person who hadn't wrote it will take a little longer to understand because you've switched from OOP to regular function calling within the same line. In a math-intensive environment, I feel like the confusion of this is amplified as it is already difficult enough to keep track of every variable and function that is flying around in the code.</p>\n<p>If you could do something like <code>.backward(input_variables=(something, somethingElse))</code>, that would be in the spirit of <code>torch.autograd.grad()</code> but keep the OOP properties of <code>.backward()</code>.</p>", "body_text": "Yeah, I understand that, and agree that that could be confusing (I just zero out the gradients, but I suppose that someone who doesn't have a good understanding of how PyTorch works could be confused by that; I'd have to think about that a little to make that more intuitive). I like the .backward() semantics because it is easy to read and easy to program. For example, as long as it isn't too verbose, you could rifle off something like this, which takes one line:\n(batch.grad).pow(2).sum().backward()\n\nwhere there is a clear flow from left-to-right. This is nice for the programmer and the reader. Of course, you can also just do, as you and @colesbury suggested (not sure if my parameters are ordered correctly -- which may be another downside of the following method, unless kwargs are accepted),\ntorch.autograd.grad( variableToOptimize, (batch.grad).pow(2).sum() )\n\nand the programmer understands what is going on (but may not understand it immediately when the programmer comes back months later to work with it again), but the person who hadn't wrote it will take a little longer to understand because you've switched from OOP to regular function calling within the same line. In a math-intensive environment, I feel like the confusion of this is amplified as it is already difficult enough to keep track of every variable and function that is flying around in the code.\nIf you could do something like .backward(input_variables=(something, somethingElse)), that would be in the spirit of torch.autograd.grad() but keep the OOP properties of .backward().", "body": "Yeah, I understand that, and agree that that could be confusing (I just zero out the gradients, but I suppose that someone who doesn't have a good understanding of how PyTorch works could be confused by that; I'd have to think about that a little to make that more intuitive). I like the `.backward()` semantics because it is easy to read and easy to program. For example, as long as it isn't too verbose, you could rifle off something like this, which takes one line:\r\n\r\n```\r\n(batch.grad).pow(2).sum().backward()\r\n```\r\n\r\nwhere there is a clear flow from left-to-right. This is nice for the programmer and the reader. Of course, you can also just do, as you and @colesbury suggested (not sure if my parameters are ordered correctly -- which may be another downside of the following method, unless `kwargs` are accepted),\r\n\r\n```\r\ntorch.autograd.grad( variableToOptimize, (batch.grad).pow(2).sum() )\r\n```\r\n\r\nand the programmer understands what is going on (but may not understand it immediately when the programmer comes back months later to work with it again), but the person who hadn't wrote it will take a little longer to understand because you've switched from OOP to regular function calling within the same line. In a math-intensive environment, I feel like the confusion of this is amplified as it is already difficult enough to keep track of every variable and function that is flying around in the code.\r\n\r\nIf you could do something like `.backward(input_variables=(something, somethingElse))`, that would be in the spirit of `torch.autograd.grad()` but keep the OOP properties of `.backward()`."}