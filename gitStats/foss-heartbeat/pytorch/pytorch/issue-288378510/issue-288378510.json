{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4661", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4661/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4661/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4661/events", "html_url": "https://github.com/pytorch/pytorch/issues/4661", "id": 288378510, "node_id": "MDU6SXNzdWUyODgzNzg1MTA=", "number": 4661, "title": "Possible create_graph=True in backward() CUDA memory leak", "user": {"login": "benvcutilli", "id": 23709161, "node_id": "MDQ6VXNlcjIzNzA5MTYx", "avatar_url": "https://avatars1.githubusercontent.com/u/23709161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benvcutilli", "html_url": "https://github.com/benvcutilli", "followers_url": "https://api.github.com/users/benvcutilli/followers", "following_url": "https://api.github.com/users/benvcutilli/following{/other_user}", "gists_url": "https://api.github.com/users/benvcutilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/benvcutilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benvcutilli/subscriptions", "organizations_url": "https://api.github.com/users/benvcutilli/orgs", "repos_url": "https://api.github.com/users/benvcutilli/repos", "events_url": "https://api.github.com/users/benvcutilli/events{/privacy}", "received_events_url": "https://api.github.com/users/benvcutilli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2018-01-14T03:09:08Z", "updated_at": "2018-01-17T20:23:57Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Ok, sorry to bother you again, and really sorry if I am just doing it wrong again (I looked at the docs for things that would address this, but didn't see anything after looking at it quickly).</p>\n<p>It appears that the next segment of code leaks CUDA memory. I don't believe I had this problem on 0.2.0 (or some version close to that) on the code that I was running (that I prefer to not publicly disclose, which is why I came up with this example instead):</p>\n<pre><code>net = torch.nn.Linear(30000,1).cuda()\ndata = torch.ones(100, 30000).cuda()\nfor rep in range(1000):\n  for batchIndices in range(0,100,100):\n    batch = torch.autograd.Variable(data[batchIndices:batchIndices+10], requires_grad=True)\n    net.forward(batch).norm(2).backward(create_graph=True)\n    batch.grad.norm(2).backward()\n</code></pre>\n<p><code>nvidia-smi</code> output:</p>\n<p>After first run of this:</p>\n<pre><code>|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\n| 33%   55C    P8    14W / 250W |   2417MiB / 12189MiB |      0%      Default |\n</code></pre>\n<p>After second run of this (but without the lines defining <code>net</code> and <code>data</code>):</p>\n<pre><code>|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\n| 32%   51C    P2    58W / 250W |   4417MiB / 12189MiB |      0%      Default |\n</code></pre>\n<p>This seems like a similar issue to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"275917710\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3824\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3824/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3824\">#3824</a>. The code in that issue's first post that reproduces it is similar to mine, so I suspect I am doing it correctly (just to defend myself!).</p>\n<p><code>torch.__version__</code> is <code>'0.3.0.post4'</code></p>\n<p>I downgraded pytorch to 0.2.0 to make sure it didn't happen, and it appears not to; <code>torch.__version__</code> says <code>'0.2.0_0'</code>.</p>\n<p>Sorry if you are working on this already; I hope that some more feedback wouldn't be too annoying, at least.</p>", "body_text": "Ok, sorry to bother you again, and really sorry if I am just doing it wrong again (I looked at the docs for things that would address this, but didn't see anything after looking at it quickly).\nIt appears that the next segment of code leaks CUDA memory. I don't believe I had this problem on 0.2.0 (or some version close to that) on the code that I was running (that I prefer to not publicly disclose, which is why I came up with this example instead):\nnet = torch.nn.Linear(30000,1).cuda()\ndata = torch.ones(100, 30000).cuda()\nfor rep in range(1000):\n  for batchIndices in range(0,100,100):\n    batch = torch.autograd.Variable(data[batchIndices:batchIndices+10], requires_grad=True)\n    net.forward(batch).norm(2).backward(create_graph=True)\n    batch.grad.norm(2).backward()\n\nnvidia-smi output:\nAfter first run of this:\n|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\n| 33%   55C    P8    14W / 250W |   2417MiB / 12189MiB |      0%      Default |\n\nAfter second run of this (but without the lines defining net and data):\n|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\n| 32%   51C    P2    58W / 250W |   4417MiB / 12189MiB |      0%      Default |\n\nThis seems like a similar issue to #3824. The code in that issue's first post that reproduces it is similar to mine, so I suspect I am doing it correctly (just to defend myself!).\ntorch.__version__ is '0.3.0.post4'\nI downgraded pytorch to 0.2.0 to make sure it didn't happen, and it appears not to; torch.__version__ says '0.2.0_0'.\nSorry if you are working on this already; I hope that some more feedback wouldn't be too annoying, at least.", "body": "Ok, sorry to bother you again, and really sorry if I am just doing it wrong again (I looked at the docs for things that would address this, but didn't see anything after looking at it quickly).\r\n\r\nIt appears that the next segment of code leaks CUDA memory. I don't believe I had this problem on 0.2.0 (or some version close to that) on the code that I was running (that I prefer to not publicly disclose, which is why I came up with this example instead):\r\n\r\n```\r\nnet = torch.nn.Linear(30000,1).cuda()\r\ndata = torch.ones(100, 30000).cuda()\r\nfor rep in range(1000):\r\n  for batchIndices in range(0,100,100):\r\n    batch = torch.autograd.Variable(data[batchIndices:batchIndices+10], requires_grad=True)\r\n    net.forward(batch).norm(2).backward(create_graph=True)\r\n    batch.grad.norm(2).backward()\r\n```\r\n\r\n`nvidia-smi` output:\r\n\r\nAfter first run of this:\r\n```\r\n|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 33%   55C    P8    14W / 250W |   2417MiB / 12189MiB |      0%      Default |\r\n```\r\n\r\nAfter second run of this (but without the lines defining `net` and `data`):\r\n```\r\n|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 32%   51C    P2    58W / 250W |   4417MiB / 12189MiB |      0%      Default |\r\n```\r\n\r\nThis seems like a similar issue to https://github.com/pytorch/pytorch/issues/3824. The code in that issue's first post that reproduces it is similar to mine, so I suspect I am doing it correctly (just to defend myself!).\r\n\r\n`torch.__version__` is `'0.3.0.post4'`\r\n\r\nI downgraded pytorch to 0.2.0 to make sure it didn't happen, and it appears not to; `torch.__version__` says `'0.2.0_0'`.\r\n\r\nSorry if you are working on this already; I hope that some more feedback wouldn't be too annoying, at least."}