{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/353723055", "html_url": "https://github.com/pytorch/pytorch/issues/4333#issuecomment-353723055", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4333", "id": 353723055, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzcyMzA1NQ==", "user": {"login": "magic282", "id": 2285145, "node_id": "MDQ6VXNlcjIyODUxNDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2285145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/magic282", "html_url": "https://github.com/magic282", "followers_url": "https://api.github.com/users/magic282/followers", "following_url": "https://api.github.com/users/magic282/following{/other_user}", "gists_url": "https://api.github.com/users/magic282/gists{/gist_id}", "starred_url": "https://api.github.com/users/magic282/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/magic282/subscriptions", "organizations_url": "https://api.github.com/users/magic282/orgs", "repos_url": "https://api.github.com/users/magic282/repos", "events_url": "https://api.github.com/users/magic282/events{/privacy}", "received_events_url": "https://api.github.com/users/magic282/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-23T12:13:25Z", "updated_at": "2017-12-23T12:13:25Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Yes I did model.eval(), some_eval_func(), model.train() during training. And during testing I do model.eval() after building the model.<br>\nThe code looks like:</p>\n<div class=\"highlight highlight-source-python\"><pre>model.eval()\nlogger.warning(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Set model to <span class=\"pl-c1\">{0}</span> mode<span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> model.training <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval<span class=\"pl-pds\">'</span></span>))\nvalid_bleu <span class=\"pl-k\">=</span> evalModel(model, translator, validData)\nmodel.train()\nlogger.warning(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Set model to <span class=\"pl-c1\">{0}</span> mode<span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> model.training <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval<span class=\"pl-pds\">'</span></span>))\nlogger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Validation Score: <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (valid_bleu <span class=\"pl-k\">*</span> <span class=\"pl-c1\">100</span>))\n<span class=\"pl-k\">if</span> valid_bleu <span class=\"pl-k\">&gt;=</span> optim.best_metric:\n    saveModel(valid_bleu)</pre></div>\n<p>During testing the code looks like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> model <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n    checkpoint <span class=\"pl-k\">=</span> torch.load(opt.model)\n\n    model_opt <span class=\"pl-k\">=</span> checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>opt<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-c1\">self</span>.src_dict <span class=\"pl-k\">=</span> checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dicts<span class=\"pl-pds\">'</span></span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>src<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-c1\">self</span>.tgt_dict <span class=\"pl-k\">=</span> checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dicts<span class=\"pl-pds\">'</span></span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tgt<span class=\"pl-pds\">'</span></span>]\n\n    <span class=\"pl-c1\">self</span>.enc_rnn_size <span class=\"pl-k\">=</span> model_opt.enc_rnn_size\n    <span class=\"pl-c1\">self</span>.dec_rnn_size <span class=\"pl-k\">=</span> model_opt.dec_rnn_size\n    model <span class=\"pl-k\">=</span> some_model()\n\n    model.load_state_dict(checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model<span class=\"pl-pds\">'</span></span>])\n    generator.load_state_dict(checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>generator<span class=\"pl-pds\">'</span></span>])\n\n    <span class=\"pl-k\">if</span> opt.cuda:\n        model.cuda()\n        generator.cuda()\n    <span class=\"pl-k\">else</span>:\n        model.cpu()\n        generator.cpu()\n\n    model.generator <span class=\"pl-k\">=</span> generator\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">self</span>.src_dict <span class=\"pl-k\">=</span> dataset[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dicts<span class=\"pl-pds\">'</span></span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>src<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-c1\">self</span>.tgt_dict <span class=\"pl-k\">=</span> dataset[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dicts<span class=\"pl-pds\">'</span></span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tgt<span class=\"pl-pds\">'</span></span>]\n\n    <span class=\"pl-c1\">self</span>.enc_rnn_size <span class=\"pl-k\">=</span> opt.enc_rnn_size\n    <span class=\"pl-c1\">self</span>.dec_rnn_size <span class=\"pl-k\">=</span> opt.dec_rnn_size\n    <span class=\"pl-c1\">self</span>.opt.cuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(opt.gpus) <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">False</span>\n    <span class=\"pl-c1\">self</span>.opt.n_best <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-c1\">self</span>.opt.replace_unk <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\n<span class=\"pl-c1\">self</span>.model <span class=\"pl-k\">=</span> model\n<span class=\"pl-c1\">self</span>.model.eval()</pre></div>\n<p>About the error, I just noticed that python random.randint returns ints in [a,b] but not [a,b). After fixing it, the losses now are the same, on both Windows and Linux.</p>\n<p>So do you have any other hints about the performance difference? Thanks.</p>", "body_text": "@apaszke Yes I did model.eval(), some_eval_func(), model.train() during training. And during testing I do model.eval() after building the model.\nThe code looks like:\nmodel.eval()\nlogger.warning(\"Set model to {0} mode\".format('train' if model.training else 'eval'))\nvalid_bleu = evalModel(model, translator, validData)\nmodel.train()\nlogger.warning(\"Set model to {0} mode\".format('train' if model.training else 'eval'))\nlogger.info('Validation Score: %g' % (valid_bleu * 100))\nif valid_bleu >= optim.best_metric:\n    saveModel(valid_bleu)\nDuring testing the code looks like:\nif model is None:\n    checkpoint = torch.load(opt.model)\n\n    model_opt = checkpoint['opt']\n    self.src_dict = checkpoint['dicts']['src']\n    self.tgt_dict = checkpoint['dicts']['tgt']\n\n    self.enc_rnn_size = model_opt.enc_rnn_size\n    self.dec_rnn_size = model_opt.dec_rnn_size\n    model = some_model()\n\n    model.load_state_dict(checkpoint['model'])\n    generator.load_state_dict(checkpoint['generator'])\n\n    if opt.cuda:\n        model.cuda()\n        generator.cuda()\n    else:\n        model.cpu()\n        generator.cpu()\n\n    model.generator = generator\nelse:\n    self.src_dict = dataset['dicts']['src']\n    self.tgt_dict = dataset['dicts']['tgt']\n\n    self.enc_rnn_size = opt.enc_rnn_size\n    self.dec_rnn_size = opt.dec_rnn_size\n    self.opt.cuda = True if len(opt.gpus) >= 1 else False\n    self.opt.n_best = 1\n    self.opt.replace_unk = False\n\nself.model = model\nself.model.eval()\nAbout the error, I just noticed that python random.randint returns ints in [a,b] but not [a,b). After fixing it, the losses now are the same, on both Windows and Linux.\nSo do you have any other hints about the performance difference? Thanks.", "body": "@apaszke Yes I did model.eval(), some_eval_func(), model.train() during training. And during testing I do model.eval() after building the model.\r\nThe code looks like:\r\n```python\r\nmodel.eval()\r\nlogger.warning(\"Set model to {0} mode\".format('train' if model.training else 'eval'))\r\nvalid_bleu = evalModel(model, translator, validData)\r\nmodel.train()\r\nlogger.warning(\"Set model to {0} mode\".format('train' if model.training else 'eval'))\r\nlogger.info('Validation Score: %g' % (valid_bleu * 100))\r\nif valid_bleu >= optim.best_metric:\r\n    saveModel(valid_bleu)\r\n```\r\n\r\nDuring testing the code looks like:\r\n```python\r\nif model is None:\r\n    checkpoint = torch.load(opt.model)\r\n\r\n    model_opt = checkpoint['opt']\r\n    self.src_dict = checkpoint['dicts']['src']\r\n    self.tgt_dict = checkpoint['dicts']['tgt']\r\n\r\n    self.enc_rnn_size = model_opt.enc_rnn_size\r\n    self.dec_rnn_size = model_opt.dec_rnn_size\r\n    model = some_model()\r\n\r\n    model.load_state_dict(checkpoint['model'])\r\n    generator.load_state_dict(checkpoint['generator'])\r\n\r\n    if opt.cuda:\r\n        model.cuda()\r\n        generator.cuda()\r\n    else:\r\n        model.cpu()\r\n        generator.cpu()\r\n\r\n    model.generator = generator\r\nelse:\r\n    self.src_dict = dataset['dicts']['src']\r\n    self.tgt_dict = dataset['dicts']['tgt']\r\n\r\n    self.enc_rnn_size = opt.enc_rnn_size\r\n    self.dec_rnn_size = opt.dec_rnn_size\r\n    self.opt.cuda = True if len(opt.gpus) >= 1 else False\r\n    self.opt.n_best = 1\r\n    self.opt.replace_unk = False\r\n\r\nself.model = model\r\nself.model.eval()\r\n```\r\n\r\nAbout the error, I just noticed that python random.randint returns ints in [a,b] but not [a,b). After fixing it, the losses now are the same, on both Windows and Linux.\r\n\r\nSo do you have any other hints about the performance difference? Thanks."}