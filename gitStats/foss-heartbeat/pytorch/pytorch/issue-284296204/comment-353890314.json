{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/353890314", "html_url": "https://github.com/pytorch/pytorch/issues/4333#issuecomment-353890314", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4333", "id": 353890314, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Mzg5MDMxNA==", "user": {"login": "ahmedanis03", "id": 16265018, "node_id": "MDQ6VXNlcjE2MjY1MDE4", "avatar_url": "https://avatars3.githubusercontent.com/u/16265018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahmedanis03", "html_url": "https://github.com/ahmedanis03", "followers_url": "https://api.github.com/users/ahmedanis03/followers", "following_url": "https://api.github.com/users/ahmedanis03/following{/other_user}", "gists_url": "https://api.github.com/users/ahmedanis03/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahmedanis03/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahmedanis03/subscriptions", "organizations_url": "https://api.github.com/users/ahmedanis03/orgs", "repos_url": "https://api.github.com/users/ahmedanis03/repos", "events_url": "https://api.github.com/users/ahmedanis03/events{/privacy}", "received_events_url": "https://api.github.com/users/ahmedanis03/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-25T20:54:01Z", "updated_at": "2017-12-26T08:50:20Z", "author_association": "NONE", "body_html": "<p>here is a modified mnist example to reproduce the inconsistency:<br>\nto reproduce run:</p>\n<pre><code>python run.py --exp_name=testv1 --epochs=2 --no-cuda\npython run.py --exp_name=testv2 --epochs=1 --no-cuda\npython run.py --exp_name=testv2 --epochs=1 --no-cuda --resume-type=last\n</code></pre>\n<p>why --no-cuda: because running on GPU give a different result each time for the mnist example and i don't know why!.</p>\n<details>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets, transforms\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> shutil\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Training settings</span>\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser(<span class=\"pl-v\">description</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>PyTorch MNIST Example<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--batch-size<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input batch size for training (default: 64)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--test-batch-size<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input batch size for testing (default: 1000)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--epochs<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>number of epochs to train (default: 10)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--lr<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>LR<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning rate (default: 0.01)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--momentum<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>M<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SGD momentum (default: 0.5)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--no-cuda<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>disables CUDA training<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--seed<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>S<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>random seed (default: 1)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--log-interval<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>how many batches to wait before logging training status<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--start-epoch<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>manual epoch number (useful on restarts)<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--resume-type<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>PATH<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>load from best/last checkpoint<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--exp_name<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>dummy<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>experiment name to used across everything<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>exp_name<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nargs <span class=\"pl-k\">=</span> parser.parse_args()\nargs.cuda <span class=\"pl-k\">=</span> <span class=\"pl-k\">not</span> args.no_cuda <span class=\"pl-k\">and</span> torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\n<span class=\"pl-k\">if</span> args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n\nkwargs <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>num_workers<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>pin_memory<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">True</span>} <span class=\"pl-k\">if</span> args.cuda <span class=\"pl-k\">else</span> {}\ntrain_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                   <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((<span class=\"pl-c1\">0.1307</span>,), (<span class=\"pl-c1\">0.3081</span>,))\n                   ])),\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>args.batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-k\">**</span>kwargs)\ntest_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((<span class=\"pl-c1\">0.1307</span>,), (<span class=\"pl-c1\">0.3081</span>,))\n                   ])),\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>args.test_batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-k\">**</span>kwargs)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv2_drop <span class=\"pl-k\">=</span> nn.Dropout2d()\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">320</span>, <span class=\"pl-c1\">50</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> F.relu(F.max_pool2d(<span class=\"pl-c1\">self</span>.conv1(x), <span class=\"pl-c1\">2</span>))\n        x <span class=\"pl-k\">=</span> F.relu(F.max_pool2d(<span class=\"pl-c1\">self</span>.conv2_drop(<span class=\"pl-c1\">self</span>.conv2(x)), <span class=\"pl-c1\">2</span>))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">320</span>)\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n        x <span class=\"pl-k\">=</span> F.dropout(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.training)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc2(x)\n        <span class=\"pl-k\">return</span> F.log_softmax(x)\n\nmodel <span class=\"pl-k\">=</span> Net()\n<span class=\"pl-k\">if</span> args.cuda:\n    model.cuda()\n\noptimizer <span class=\"pl-k\">=</span> optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>args.lr, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>args.momentum)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">epoch</span>):\n    model.train()\n    <span class=\"pl-k\">for</span> batch_idx, (data, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n        <span class=\"pl-k\">if</span> args.cuda:\n            data, target <span class=\"pl-k\">=</span> data.cuda(), target.cuda()\n        data, target <span class=\"pl-k\">=</span> Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output <span class=\"pl-k\">=</span> model(data)\n        loss <span class=\"pl-k\">=</span> F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> if batch_idx % args.log_interval == 0:</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>         epoch, batch_idx * len(data), len(train_loader.dataset),</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>         100. * batch_idx / len(train_loader), loss.data[0]))</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>():\n    model.eval()\n    test_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    correct <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">for</span> data, target <span class=\"pl-k\">in</span> test_loader:\n        <span class=\"pl-k\">if</span> args.cuda:\n            data, target <span class=\"pl-k\">=</span> data.cuda(), target.cuda()\n        data, target <span class=\"pl-k\">=</span> Variable(data, <span class=\"pl-v\">volatile</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(target)\n        output <span class=\"pl-k\">=</span> model(data)\n        test_loss <span class=\"pl-k\">+=</span> F.nll_loss(output, target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>).data[<span class=\"pl-c1\">0</span>] <span class=\"pl-c\"><span class=\"pl-c\">#</span> sum up batch loss</span>\n        pred <span class=\"pl-k\">=</span> output.data.max(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keepdim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">1</span>] <span class=\"pl-c\"><span class=\"pl-c\">#</span> get the index of the max log-probability</span>\n        correct <span class=\"pl-k\">+=</span> pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">len</span>(test_loader.dataset)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span>Test set: Average loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span>, Accuracy: <span class=\"pl-c1\">{}</span>/<span class=\"pl-c1\">{}</span> (<span class=\"pl-c1\">{<span class=\"pl-k\">:.0f</span>}</span>%)<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>.format(\n        test_loss, correct, <span class=\"pl-c1\">len</span>(test_loader.dataset),\n        <span class=\"pl-c1\">100</span>. <span class=\"pl-k\">*</span> correct <span class=\"pl-k\">/</span> <span class=\"pl-c1\">len</span>(test_loader.dataset)))\n\n\n<span class=\"pl-k\">if</span> args.resume_type:\n    checkpoint_file <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> args.exp_name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>\n    checkpoint_file <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>checkpoint.pth.tar<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> args.resume_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>last<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_best.pth.tar<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">if</span> os.path.isfile(checkpoint_file):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>=&gt; loading checkpoint '<span class=\"pl-c1\">{}</span>'<span class=\"pl-pds\">\"</span></span>.format(checkpoint_file))\n        checkpoint <span class=\"pl-k\">=</span> torch.load(checkpoint_file)\n        args.start_epoch <span class=\"pl-k\">=</span> checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch<span class=\"pl-pds\">'</span></span>]\n        best_loss <span class=\"pl-k\">=</span> checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>best_loss<span class=\"pl-pds\">'</span></span>]\n        model.load_state_dict(checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>state_dict<span class=\"pl-pds\">'</span></span>])\n        optimizer.load_state_dict(checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>optimizer<span class=\"pl-pds\">'</span></span>])\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>=&gt; loaded checkpoint '<span class=\"pl-c1\">{}</span>' (epoch <span class=\"pl-c1\">{}</span>)<span class=\"pl-pds\">\"</span></span>\n              .format(checkpoint_file, checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch<span class=\"pl-pds\">'</span></span>]))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>=&gt; no checkpoint found at '<span class=\"pl-c1\">{}</span>'<span class=\"pl-pds\">\"</span></span>.format(checkpoint_file))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">save_checkpoint</span>(<span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">is_best</span>):\n    exp_weights_root_dir <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> args.exp_name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>\n    os.makedirs(exp_weights_root_dir, <span class=\"pl-v\">exist_ok</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    filename <span class=\"pl-k\">=</span> exp_weights_root_dir <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>checkpoint.pth.tar<span class=\"pl-pds\">'</span></span>\n    torch.save(state, filename)\n    <span class=\"pl-k\">if</span> is_best:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>best beaten<span class=\"pl-pds\">'</span></span>)\n        shutil.copyfile(filename, exp_weights_root_dir <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_best.pth.tar<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(args.start_epoch, args.epochs <span class=\"pl-k\">+</span> args.start_epoch):\n    train(epoch)\n    test()\n    save_checkpoint(\n                    {\n                        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch<span class=\"pl-pds\">'</span></span>: epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>,\n                        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>state_dict<span class=\"pl-pds\">'</span></span>: model.state_dict(),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>best_loss<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">0</span>,\n                        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>optimizer<span class=\"pl-pds\">'</span></span>: optimizer.state_dict(),\n                    }, <span class=\"pl-c1\">True</span>)</pre></div>\n</details>", "body_text": "here is a modified mnist example to reproduce the inconsistency:\nto reproduce run:\npython run.py --exp_name=testv1 --epochs=2 --no-cuda\npython run.py --exp_name=testv2 --epochs=1 --no-cuda\npython run.py --exp_name=testv2 --epochs=1 --no-cuda --resume-type=last\n\nwhy --no-cuda: because running on GPU give a different result each time for the mnist example and i don't know why!.\n\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport os\nimport shutil\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n                    help='manual epoch number (useful on restarts)')\nparser.add_argument('--resume-type', default='', type=str, metavar='PATH',\n                    help='load from best/last checkpoint')\nparser.add_argument('--exp_name', type=str, default='dummy',\n                    help='experiment name to used across everything', metavar='exp_name', required=True)\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = Net()\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        # if batch_idx % args.log_interval == 0:\n        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n        #         100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nif args.resume_type:\n    checkpoint_file = 'weights/' + args.exp_name + '/'\n    checkpoint_file += 'checkpoint.pth.tar' if args.resume_type == 'last' else 'model_best.pth.tar'\n    if os.path.isfile(checkpoint_file):\n        print(\"=> loading checkpoint '{}'\".format(checkpoint_file))\n        checkpoint = torch.load(checkpoint_file)\n        args.start_epoch = checkpoint['epoch']\n        best_loss = checkpoint['best_loss']\n        model.load_state_dict(checkpoint['state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\n              .format(checkpoint_file, checkpoint['epoch']))\n    else:\n        print(\"=> no checkpoint found at '{}'\".format(checkpoint_file))\n\n\ndef save_checkpoint(state, is_best):\n    exp_weights_root_dir = 'weights/' + args.exp_name + '/'\n    os.makedirs(exp_weights_root_dir, exist_ok=True)\n    filename = exp_weights_root_dir + 'checkpoint.pth.tar'\n    torch.save(state, filename)\n    if is_best:\n        print('best beaten')\n        shutil.copyfile(filename, exp_weights_root_dir + 'model_best.pth.tar')\n\n\nfor epoch in range(args.start_epoch, args.epochs + args.start_epoch):\n    train(epoch)\n    test()\n    save_checkpoint(\n                    {\n                        'epoch': epoch + 1,\n                        'state_dict': model.state_dict(),\n                        'best_loss': 0,\n                        'optimizer': optimizer.state_dict(),\n                    }, True)", "body": "here is a modified mnist example to reproduce the inconsistency:\r\nto reproduce run:\r\n```\r\npython run.py --exp_name=testv1 --epochs=2 --no-cuda\r\npython run.py --exp_name=testv2 --epochs=1 --no-cuda\r\npython run.py --exp_name=testv2 --epochs=1 --no-cuda --resume-type=last\r\n```\r\nwhy --no-cuda: because running on GPU give a different result each time for the mnist example and i don't know why!.\r\n\r\n<details>\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable\r\nimport os\r\nimport shutil\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\r\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\r\n                    help='input batch size for training (default: 64)')\r\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\r\n                    help='input batch size for testing (default: 1000)')\r\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\r\n                    help='number of epochs to train (default: 10)')\r\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\r\n                    help='learning rate (default: 0.01)')\r\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\r\n                    help='SGD momentum (default: 0.5)')\r\nparser.add_argument('--no-cuda', action='store_true', default=False,\r\n                    help='disables CUDA training')\r\nparser.add_argument('--seed', type=int, default=1, metavar='S',\r\n                    help='random seed (default: 1)')\r\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\r\n                    help='how many batches to wait before logging training status')\r\nparser.add_argument('--start-epoch', default=0, type=int, metavar='N',\r\n                    help='manual epoch number (useful on restarts)')\r\nparser.add_argument('--resume-type', default='', type=str, metavar='PATH',\r\n                    help='load from best/last checkpoint')\r\nparser.add_argument('--exp_name', type=str, default='dummy',\r\n                    help='experiment name to used across everything', metavar='exp_name', required=True)\r\n\r\nargs = parser.parse_args()\r\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\r\n\r\ntorch.manual_seed(args.seed)\r\nif args.cuda:\r\n    torch.cuda.manual_seed(args.seed)\r\n\r\n\r\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=args.batch_size, shuffle=True, **kwargs)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=args.test_batch_size, shuffle=True, **kwargs)\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\nmodel = Net()\r\nif args.cuda:\r\n    model.cuda()\r\n\r\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\r\n\r\ndef train(epoch):\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        if args.cuda:\r\n            data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data), Variable(target)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        # if batch_idx % args.log_interval == 0:\r\n        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n        #         epoch, batch_idx * len(data), len(train_loader.dataset),\r\n        #         100. * batch_idx / len(train_loader), loss.data[0]))\r\n\r\ndef test():\r\n    model.eval()\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        if args.cuda:\r\n            data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data, volatile=True), Variable(target)\r\n        output = model(data)\r\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\r\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n\r\n\r\nif args.resume_type:\r\n    checkpoint_file = 'weights/' + args.exp_name + '/'\r\n    checkpoint_file += 'checkpoint.pth.tar' if args.resume_type == 'last' else 'model_best.pth.tar'\r\n    if os.path.isfile(checkpoint_file):\r\n        print(\"=> loading checkpoint '{}'\".format(checkpoint_file))\r\n        checkpoint = torch.load(checkpoint_file)\r\n        args.start_epoch = checkpoint['epoch']\r\n        best_loss = checkpoint['best_loss']\r\n        model.load_state_dict(checkpoint['state_dict'])\r\n        optimizer.load_state_dict(checkpoint['optimizer'])\r\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\r\n              .format(checkpoint_file, checkpoint['epoch']))\r\n    else:\r\n        print(\"=> no checkpoint found at '{}'\".format(checkpoint_file))\r\n\r\n\r\ndef save_checkpoint(state, is_best):\r\n    exp_weights_root_dir = 'weights/' + args.exp_name + '/'\r\n    os.makedirs(exp_weights_root_dir, exist_ok=True)\r\n    filename = exp_weights_root_dir + 'checkpoint.pth.tar'\r\n    torch.save(state, filename)\r\n    if is_best:\r\n        print('best beaten')\r\n        shutil.copyfile(filename, exp_weights_root_dir + 'model_best.pth.tar')\r\n\r\n\r\nfor epoch in range(args.start_epoch, args.epochs + args.start_epoch):\r\n    train(epoch)\r\n    test()\r\n    save_checkpoint(\r\n                    {\r\n                        'epoch': epoch + 1,\r\n                        'state_dict': model.state_dict(),\r\n                        'best_loss': 0,\r\n                        'optimizer': optimizer.state_dict(),\r\n                    }, True)\r\n```\r\n\r\n</details>"}