{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348666145", "html_url": "https://github.com/pytorch/pytorch/issues/3966#issuecomment-348666145", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3966", "id": 348666145, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODY2NjE0NQ==", "user": {"login": "Giszy", "id": 24215899, "node_id": "MDQ6VXNlcjI0MjE1ODk5", "avatar_url": "https://avatars1.githubusercontent.com/u/24215899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Giszy", "html_url": "https://github.com/Giszy", "followers_url": "https://api.github.com/users/Giszy/followers", "following_url": "https://api.github.com/users/Giszy/following{/other_user}", "gists_url": "https://api.github.com/users/Giszy/gists{/gist_id}", "starred_url": "https://api.github.com/users/Giszy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Giszy/subscriptions", "organizations_url": "https://api.github.com/users/Giszy/orgs", "repos_url": "https://api.github.com/users/Giszy/repos", "events_url": "https://api.github.com/users/Giszy/events{/privacy}", "received_events_url": "https://api.github.com/users/Giszy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-02T04:00:52Z", "updated_at": "2017-12-02T04:12:02Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a><br>\nModify the code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.multiprocessing <span class=\"pl-k\">as</span> mp\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>():\n    x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1</span>)\n    y <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1</span>)\n    model <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n    mse <span class=\"pl-k\">=</span> nn.MSELoss()\n    model.zero_grad()\n    pred <span class=\"pl-k\">=</span> model(Variable(x))\n    loss <span class=\"pl-k\">=</span> mse(pred, Variable(y))\n    loss.backward() <span class=\"pl-c\"><span class=\"pl-c\">#</span> hangs here</span>\n    <span class=\"pl-k\">return</span> model\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">worker</span>(<span class=\"pl-smi\">rank</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rank <span class=\"pl-c1\">%d</span> start<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> rank)\n    model <span class=\"pl-k\">=</span> train()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rank <span class=\"pl-c1\">%d</span> done<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> rank)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run_distributed</span>(<span class=\"pl-smi\">N</span>):\n    ps, models <span class=\"pl-k\">=</span> [], []\n\n    <span class=\"pl-k\">for</span> rank <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n            p <span class=\"pl-k\">=</span> mp.Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>worker, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(rank,))\n            p.start()\n            ps.append(p)\n\n    <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> ps:\n        p.join()\n\n    <span class=\"pl-k\">return</span> models\n\ntrain()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> run_distributed hangs unless this line is commented</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Done main train<span class=\"pl-pds\">\"</span></span>)\nrun_distributed(<span class=\"pl-c1\">10</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Done distributed train<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>The result</p>\n<pre><code>Done main train\nDone main train\nDone distributed train\nrank 0 start\nrank 0 done\nDone main train\nDone distributed train\nrank 4 start\nrank 4 done\nDone main train\nDone distributed train\nrank 3 start\nrank 3 done\nDone main train\nDone distributed train\nrank 7 start\nrank 7 done\nDone main train\nDone distributed train\nrank 6 start\nrank 6 done\nDone main train\nDone distributed train\nrank 2 start\nrank 2 done\nDone main train\nDone distributed train\nrank 8 start\nrank 8 done\nDone main train\nDone distributed train\nrank 5 start\nrank 5 done\nDone main train\nDone distributed train\nDone main train\nDone distributed train\nrank 9 start\nrank 1 start\nrank 9 done\nrank 1 done\nDone distributed train\n</code></pre>\n<p>Is it correct?</p>", "body_text": "@adamlerer, @soumith, @colesbury\nModify the code\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\ndef train():\n    x = torch.randn(1000, 1)\n    y = torch.randn(1000, 1)\n    model = nn.Linear(1, 1)\n    mse = nn.MSELoss()\n    model.zero_grad()\n    pred = model(Variable(x))\n    loss = mse(pred, Variable(y))\n    loss.backward() # hangs here\n    return model\n\n\ndef worker(rank):\n    print(\"rank %d start\" % rank)\n    model = train()\n    print(\"rank %d done\" % rank)\n\n\ndef run_distributed(N):\n    ps, models = [], []\n\n    for rank in range(10):\n        if __name__ == '__main__':\n            p = mp.Process(target=worker, args=(rank,))\n            p.start()\n            ps.append(p)\n\n    for p in ps:\n        p.join()\n\n    return models\n\ntrain()  # run_distributed hangs unless this line is commented\nprint(\"Done main train\")\nrun_distributed(10)\nprint(\"Done distributed train\")\nThe result\nDone main train\nDone main train\nDone distributed train\nrank 0 start\nrank 0 done\nDone main train\nDone distributed train\nrank 4 start\nrank 4 done\nDone main train\nDone distributed train\nrank 3 start\nrank 3 done\nDone main train\nDone distributed train\nrank 7 start\nrank 7 done\nDone main train\nDone distributed train\nrank 6 start\nrank 6 done\nDone main train\nDone distributed train\nrank 2 start\nrank 2 done\nDone main train\nDone distributed train\nrank 8 start\nrank 8 done\nDone main train\nDone distributed train\nrank 5 start\nrank 5 done\nDone main train\nDone distributed train\nDone main train\nDone distributed train\nrank 9 start\nrank 1 start\nrank 9 done\nrank 1 done\nDone distributed train\n\nIs it correct?", "body": "@adamlerer, @soumith, @colesbury\r\nModify the code\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport torch.multiprocessing as mp\r\n\r\ndef train():\r\n    x = torch.randn(1000, 1)\r\n    y = torch.randn(1000, 1)\r\n    model = nn.Linear(1, 1)\r\n    mse = nn.MSELoss()\r\n    model.zero_grad()\r\n    pred = model(Variable(x))\r\n    loss = mse(pred, Variable(y))\r\n    loss.backward() # hangs here\r\n    return model\r\n\r\n\r\ndef worker(rank):\r\n    print(\"rank %d start\" % rank)\r\n    model = train()\r\n    print(\"rank %d done\" % rank)\r\n\r\n\r\ndef run_distributed(N):\r\n    ps, models = [], []\r\n\r\n    for rank in range(10):\r\n        if __name__ == '__main__':\r\n            p = mp.Process(target=worker, args=(rank,))\r\n            p.start()\r\n            ps.append(p)\r\n\r\n    for p in ps:\r\n        p.join()\r\n\r\n    return models\r\n\r\ntrain()  # run_distributed hangs unless this line is commented\r\nprint(\"Done main train\")\r\nrun_distributed(10)\r\nprint(\"Done distributed train\")\r\n```\r\nThe result\r\n```\r\nDone main train\r\nDone main train\r\nDone distributed train\r\nrank 0 start\r\nrank 0 done\r\nDone main train\r\nDone distributed train\r\nrank 4 start\r\nrank 4 done\r\nDone main train\r\nDone distributed train\r\nrank 3 start\r\nrank 3 done\r\nDone main train\r\nDone distributed train\r\nrank 7 start\r\nrank 7 done\r\nDone main train\r\nDone distributed train\r\nrank 6 start\r\nrank 6 done\r\nDone main train\r\nDone distributed train\r\nrank 2 start\r\nrank 2 done\r\nDone main train\r\nDone distributed train\r\nrank 8 start\r\nrank 8 done\r\nDone main train\r\nDone distributed train\r\nrank 5 start\r\nrank 5 done\r\nDone main train\r\nDone distributed train\r\nDone main train\r\nDone distributed train\r\nrank 9 start\r\nrank 1 start\r\nrank 9 done\r\nrank 1 done\r\nDone distributed train\r\n```\r\nIs it correct?"}