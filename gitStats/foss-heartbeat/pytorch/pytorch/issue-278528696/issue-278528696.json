{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3966", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3966/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3966/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3966/events", "html_url": "https://github.com/pytorch/pytorch/issues/3966", "id": 278528696, "node_id": "MDU6SXNzdWUyNzg1Mjg2OTY=", "number": 3966, "title": "`backward` hangs in multiprocess after single-process", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-12-01T16:52:38Z", "updated_at": "2018-01-02T20:05:02Z", "closed_at": "2017-12-18T06:51:27Z", "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li>Call <code>backward</code> on something in the local process</li>\n<li>Launch N subprocess that call <code>backward</code></li>\n<li>hang</li>\n</ol>\n<p>Here's a repro:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\n\ndef train():\n    x = torch.randn(1000, 1)\n    y = torch.randn(1000, 1)\n    model = nn.Linear(1, 1)\n    mse = nn.MSELoss()\n    model.zero_grad()\n    pred = model(Variable(x))\n    loss = mse(pred, Variable(y))\n    loss.backward() # hangs here\n    return model\n\n\ndef worker(rank):\n    print(\"rank %d start\" % rank)\n    model = train()\n    print(\"rank %d done\" % rank)\n\n\ndef run_distributed(N):\n    ps, models = [], []\n\n    for rank in range(10):\n        p = mp.Process(target=worker,\n                       args=(rank,))\n        p.start()\n        ps.append(p)\n\n\n    for p in ps:\n        p.join()\n\n    return models\n\ntrain()  # run_distributed hangs unless this line is commented\nprint(\"Done main train\")\nrun_distributed(5)\nprint(\"Done distributed train\")\n</code></pre>", "body_text": "Call backward on something in the local process\nLaunch N subprocess that call backward\nhang\n\nHere's a repro:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\n\ndef train():\n    x = torch.randn(1000, 1)\n    y = torch.randn(1000, 1)\n    model = nn.Linear(1, 1)\n    mse = nn.MSELoss()\n    model.zero_grad()\n    pred = model(Variable(x))\n    loss = mse(pred, Variable(y))\n    loss.backward() # hangs here\n    return model\n\n\ndef worker(rank):\n    print(\"rank %d start\" % rank)\n    model = train()\n    print(\"rank %d done\" % rank)\n\n\ndef run_distributed(N):\n    ps, models = [], []\n\n    for rank in range(10):\n        p = mp.Process(target=worker,\n                       args=(rank,))\n        p.start()\n        ps.append(p)\n\n\n    for p in ps:\n        p.join()\n\n    return models\n\ntrain()  # run_distributed hangs unless this line is commented\nprint(\"Done main train\")\nrun_distributed(5)\nprint(\"Done distributed train\")", "body": "1. Call `backward` on something in the local process\r\n2. Launch N subprocess that call `backward`\r\n3. hang\r\n\r\nHere's a repro:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport torch.multiprocessing as mp\r\n\r\n\r\ndef train():\r\n    x = torch.randn(1000, 1)\r\n    y = torch.randn(1000, 1)\r\n    model = nn.Linear(1, 1)\r\n    mse = nn.MSELoss()\r\n    model.zero_grad()\r\n    pred = model(Variable(x))\r\n    loss = mse(pred, Variable(y))\r\n    loss.backward() # hangs here\r\n    return model\r\n\r\n\r\ndef worker(rank):\r\n    print(\"rank %d start\" % rank)\r\n    model = train()\r\n    print(\"rank %d done\" % rank)\r\n\r\n\r\ndef run_distributed(N):\r\n    ps, models = [], []\r\n\r\n    for rank in range(10):\r\n        p = mp.Process(target=worker,\r\n                       args=(rank,))\r\n        p.start()\r\n        ps.append(p)\r\n\r\n\r\n    for p in ps:\r\n        p.join()\r\n\r\n    return models\r\n\r\ntrain()  # run_distributed hangs unless this line is commented\r\nprint(\"Done main train\")\r\nrun_distributed(5)\r\nprint(\"Done distributed train\")\r\n```"}