{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12540", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12540/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12540/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12540/events", "html_url": "https://github.com/pytorch/pytorch/issues/12540", "id": 368788745, "node_id": "MDU6SXNzdWUzNjg3ODg3NDU=", "number": 12540, "title": "[RFC] Removing Nervana GPU", "user": {"login": "mratsim", "id": 22738317, "node_id": "MDQ6VXNlcjIyNzM4MzE3", "avatar_url": "https://avatars3.githubusercontent.com/u/22738317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mratsim", "html_url": "https://github.com/mratsim", "followers_url": "https://api.github.com/users/mratsim/followers", "following_url": "https://api.github.com/users/mratsim/following{/other_user}", "gists_url": "https://api.github.com/users/mratsim/gists{/gist_id}", "starred_url": "https://api.github.com/users/mratsim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mratsim/subscriptions", "organizations_url": "https://api.github.com/users/mratsim/orgs", "repos_url": "https://api.github.com/users/mratsim/repos", "events_url": "https://api.github.com/users/mratsim/events{/privacy}", "received_events_url": "https://api.github.com/users/mratsim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-10T18:14:30Z", "updated_at": "2018-10-15T18:06:06Z", "closed_at": "2018-10-15T18:06:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>NervanaGPU is currently included as a <a href=\"https://github.com/pytorch/pytorch/tree/c2a57d082d36f30f32d5c4cc7147ab57ce2c3097/third_party\">third-party module</a>. However the repository is not under active development anymore and has not been modified since 3 years ago.</p>\n<p>Checking the commit that added it, it was <a href=\"https://github.com/pytorch/pytorch/commit/9201cdd029aa820ebc92c71bd4f6d4105163ca80\">this one</a> for the optimized GEMM kernel that was developed at Nervana.</p>\n<p>There are 2 kernels of interest in Nervana's work:</p>\n<h3>Nervana GEMM</h3>\n<p>While Nervana GEMM is extremely tuned for Maxwell Architecture, but with Volta and Turing, GPUs are now offering Tensor Cores that are much faster. I expect that Nvidia tuned their GEMM implementation since then.</p>\n<p>Alternatives:</p>\n<ul>\n<li>CuBLAS</li>\n<li><a href=\"https://github.com/NervanaSystems/maxas\">Nervana Maxas</a> which isolated the GEMM kernel and is a much smaller library than the full NervanaGPU, so it's probably possible to clone and maintain it.</li>\n<li>Honorary mention: <a href=\"https://github.com/NVIDIA/cutlass\">Nvidia Cutlass</a>, which achieves 90% speed of CuBLAS and supports Tensor cores without assembly.</li>\n<li>Nvidia's TensorRT for inference.</li>\n</ul>\n<h3>Nervana convolution kernel</h3>\n<p>(If used, but i don't think it is)</p>\n<p>The main appeal is the winograd kernel <a href=\"https://github.com/soumith/convnet-benchmarks/issues/93\" data-hovercard-type=\"issue\" data-hovercard-url=\"/soumith/convnet-benchmarks/issues/93/hovercard\">discussed extensively here</a> but it's <a href=\"https://github.com/NervanaSystems/nervanagpu/issues/25#issuecomment-203168418\" data-hovercard-type=\"issue\" data-hovercard-url=\"/NervanaSystems/nervanagpu/issues/25/hovercard\">not even in NervanaGPU repo but in Neon repo.</a>.<br>\nFurthermore it has been integrated into CuDNN with torch NCHW layout instead of Neon's CHWN layout and convolution was probably one of Nvidia focus in the past 2 years.</p>\n<h3>Conclusion</h3>\n<p>Should the benchmarks show that alternatives to Nervana achieves the same speed on current GPU for a wide range of input size (as tensor cores are currently very restrictive), I think NervanaGPU code should be dropped to save on code, maintenance, dependencies and build options.</p>", "body_text": "NervanaGPU is currently included as a third-party module. However the repository is not under active development anymore and has not been modified since 3 years ago.\nChecking the commit that added it, it was this one for the optimized GEMM kernel that was developed at Nervana.\nThere are 2 kernels of interest in Nervana's work:\nNervana GEMM\nWhile Nervana GEMM is extremely tuned for Maxwell Architecture, but with Volta and Turing, GPUs are now offering Tensor Cores that are much faster. I expect that Nvidia tuned their GEMM implementation since then.\nAlternatives:\n\nCuBLAS\nNervana Maxas which isolated the GEMM kernel and is a much smaller library than the full NervanaGPU, so it's probably possible to clone and maintain it.\nHonorary mention: Nvidia Cutlass, which achieves 90% speed of CuBLAS and supports Tensor cores without assembly.\nNvidia's TensorRT for inference.\n\nNervana convolution kernel\n(If used, but i don't think it is)\nThe main appeal is the winograd kernel discussed extensively here but it's not even in NervanaGPU repo but in Neon repo..\nFurthermore it has been integrated into CuDNN with torch NCHW layout instead of Neon's CHWN layout and convolution was probably one of Nvidia focus in the past 2 years.\nConclusion\nShould the benchmarks show that alternatives to Nervana achieves the same speed on current GPU for a wide range of input size (as tensor cores are currently very restrictive), I think NervanaGPU code should be dropped to save on code, maintenance, dependencies and build options.", "body": "NervanaGPU is currently included as a [third-party module](https://github.com/pytorch/pytorch/tree/c2a57d082d36f30f32d5c4cc7147ab57ce2c3097/third_party). However the repository is not under active development anymore and has not been modified since 3 years ago.\r\n\r\nChecking the commit that added it, it was [this one](https://github.com/pytorch/pytorch/commit/9201cdd029aa820ebc92c71bd4f6d4105163ca80) for the optimized GEMM kernel that was developed at Nervana.\r\n\r\nThere are 2 kernels of interest in Nervana's work:\r\n\r\n### Nervana GEMM\r\n\r\nWhile Nervana GEMM is extremely tuned for Maxwell Architecture, but with Volta and Turing, GPUs are now offering Tensor Cores that are much faster. I expect that Nvidia tuned their GEMM implementation since then.\r\n\r\nAlternatives:\r\n- CuBLAS\r\n- [Nervana Maxas](https://github.com/NervanaSystems/maxas) which isolated the GEMM kernel and is a much smaller library than the full NervanaGPU, so it's probably possible to clone and maintain it.\r\n- Honorary mention: [Nvidia Cutlass](https://github.com/NVIDIA/cutlass), which achieves 90% speed of CuBLAS and supports Tensor cores without assembly.\r\n- Nvidia's TensorRT for inference.\r\n\r\n### Nervana convolution kernel\r\n\r\n(If used, but i don't think it is)\r\n\r\nThe main appeal is the winograd kernel [discussed extensively here](https://github.com/soumith/convnet-benchmarks/issues/93) but it's [not even in NervanaGPU repo but in Neon repo.](https://github.com/NervanaSystems/nervanagpu/issues/25#issuecomment-203168418).\r\nFurthermore it has been integrated into CuDNN with torch NCHW layout instead of Neon's CHWN layout and convolution was probably one of Nvidia focus in the past 2 years.\r\n\r\n### Conclusion\r\n\r\nShould the benchmarks show that alternatives to Nervana achieves the same speed on current GPU for a wide range of input size (as tensor cores are currently very restrictive), I think NervanaGPU code should be dropped to save on code, maintenance, dependencies and build options.\r\n\r\n\r\n"}