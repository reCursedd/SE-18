{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3526", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3526/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3526/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3526/events", "html_url": "https://github.com/pytorch/pytorch/pull/3526", "id": 271740447, "node_id": "MDExOlB1bGxSZXF1ZXN0MTUxMDQ2NTU3", "number": 3526, "title": "Reuse intermediate results over multiple backwards grad_inputs", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-07T08:01:09Z", "updated_at": "2018-11-23T15:36:09Z", "closed_at": "2017-11-08T06:57:44Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3526", "html_url": "https://github.com/pytorch/pytorch/pull/3526", "diff_url": "https://github.com/pytorch/pytorch/pull/3526.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3526.patch"}, "body_html": "<p>The first two commits are just a little bit of refactoring.</p>\n<p>The next two support defining gradient for multiple inputs simultaneously in <code>derivatives.yaml</code>, and then an example of how to use it via <code>atan2</code>. The basic model is, instead of saying <code>output1: returns a tensor</code> and <code>output2: returns a tensor</code>, you just say <code>output1, output2: returns a tuple of tensors</code></p>\n<p>I am not entirely sure I have done the <code>output_mask</code> handling idiomatically. This definitely seems like an opportunity for some compiler-y techniques. For example, one way to implement this assuming you have a working compiler is to define the computation once assuming every output is needed, and then for every output_mask permutation, DCE the unneeded outputs. (This is, of course, assuming that there isn't a totally different algorithm that is applicable when you can remove required grads; in the case of atan2, this is definitely not the case.) In any case I don't plan to do this on this PR.</p>\n<p>Looking at derivatives.yaml, here are some more opportunities for reusing intermediate computations with addbmm and dist (which already have gradients.) I'll do these once I confirm the pattern looks good.</p>", "body_text": "The first two commits are just a little bit of refactoring.\nThe next two support defining gradient for multiple inputs simultaneously in derivatives.yaml, and then an example of how to use it via atan2. The basic model is, instead of saying output1: returns a tensor and output2: returns a tensor, you just say output1, output2: returns a tuple of tensors\nI am not entirely sure I have done the output_mask handling idiomatically. This definitely seems like an opportunity for some compiler-y techniques. For example, one way to implement this assuming you have a working compiler is to define the computation once assuming every output is needed, and then for every output_mask permutation, DCE the unneeded outputs. (This is, of course, assuming that there isn't a totally different algorithm that is applicable when you can remove required grads; in the case of atan2, this is definitely not the case.) In any case I don't plan to do this on this PR.\nLooking at derivatives.yaml, here are some more opportunities for reusing intermediate computations with addbmm and dist (which already have gradients.) I'll do these once I confirm the pattern looks good.", "body": "The first two commits are just a little bit of refactoring.\r\n\r\nThe next two support defining gradient for multiple inputs simultaneously in `derivatives.yaml`, and then an example of how to use it via `atan2`. The basic model is, instead of saying `output1: returns a tensor` and `output2: returns a tensor`, you just say `output1, output2: returns a tuple of tensors`\r\n\r\nI am not entirely sure I have done the `output_mask` handling idiomatically. This definitely seems like an opportunity for some compiler-y techniques. For example, one way to implement this assuming you have a working compiler is to define the computation once assuming every output is needed, and then for every output_mask permutation, DCE the unneeded outputs. (This is, of course, assuming that there isn't a totally different algorithm that is applicable when you can remove required grads; in the case of atan2, this is definitely not the case.) In any case I don't plan to do this on this PR.\r\n\r\nLooking at derivatives.yaml, here are some more opportunities for reusing intermediate computations with addbmm and dist (which already have gradients.) I'll do these once I confirm the pattern looks good."}