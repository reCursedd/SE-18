{"url": "https://api.github.com/repos/pytorch/pytorch/issues/598", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/598/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/598/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/598/events", "html_url": "https://github.com/pytorch/pytorch/issues/598", "id": 203417987, "node_id": "MDU6SXNzdWUyMDM0MTc5ODc=", "number": 598, "title": "Problem with backward hook function", "user": {"login": "ludc", "id": 5995229, "node_id": "MDQ6VXNlcjU5OTUyMjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5995229?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ludc", "html_url": "https://github.com/ludc", "followers_url": "https://api.github.com/users/ludc/followers", "following_url": "https://api.github.com/users/ludc/following{/other_user}", "gists_url": "https://api.github.com/users/ludc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ludc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ludc/subscriptions", "organizations_url": "https://api.github.com/users/ludc/orgs", "repos_url": "https://api.github.com/users/ludc/repos", "events_url": "https://api.github.com/users/ludc/events{/privacy}", "received_events_url": "https://api.github.com/users/ludc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-01-26T16:30:31Z", "updated_at": "2018-07-09T17:56:00Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>there is something strange in the <code>backward</code> step (or maybe something I don't understand). If I define a Module that takes 3 inputs, the <code>grad_input</code> has to be of size 3, right ? But this is not the case here (from the backward_hook point of view):</p>\n<pre lang=\"import\" data-meta=\"torch\"><code>from torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef bh(m,go,gi):\n    print(\"Grad Input\")\n    print(go)\n    print(\"Grad Output\")\n    print(gi)\n\nclass M(nn.Module):\n    def __init__(self):\n        super(M,self).__init__()\n        self.register_backward_hook(bh)\n\n\n    def forward(self,x,y,z):\n        return (x+y+z)\n\nx=Variable(torch.randn(1,5),requires_grad=True)\ny=Variable(torch.randn(1,5),requires_grad=True)\nz=Variable(torch.randn(1,5),requires_grad=True)\n\ncriterion=nn.MSELoss()\nmod=M()\nout=mod(x,y,z)\nloss=criterion(out,Variable(torch.randn(1,5)))\nloss.backward()```\n\nIn that case, when I print grad_input throught the hook function, it is just composed of two elements... Could you tell me where am I wrong ? But `x.grad, y.grad and z.grad` seem correctly computed\n</code></pre>", "body_text": "Hi,\nthere is something strange in the backward step (or maybe something I don't understand). If I define a Module that takes 3 inputs, the grad_input has to be of size 3, right ? But this is not the case here (from the backward_hook point of view):\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef bh(m,go,gi):\n    print(\"Grad Input\")\n    print(go)\n    print(\"Grad Output\")\n    print(gi)\n\nclass M(nn.Module):\n    def __init__(self):\n        super(M,self).__init__()\n        self.register_backward_hook(bh)\n\n\n    def forward(self,x,y,z):\n        return (x+y+z)\n\nx=Variable(torch.randn(1,5),requires_grad=True)\ny=Variable(torch.randn(1,5),requires_grad=True)\nz=Variable(torch.randn(1,5),requires_grad=True)\n\ncriterion=nn.MSELoss()\nmod=M()\nout=mod(x,y,z)\nloss=criterion(out,Variable(torch.randn(1,5)))\nloss.backward()```\n\nIn that case, when I print grad_input throught the hook function, it is just composed of two elements... Could you tell me where am I wrong ? But `x.grad, y.grad and z.grad` seem correctly computed", "body": "Hi,\r\n\r\nthere is something strange in the `backward` step (or maybe something I don't understand). If I define a Module that takes 3 inputs, the `grad_input` has to be of size 3, right ? But this is not the case here (from the backward_hook point of view):\r\n\r\n```import torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ndef bh(m,go,gi):\r\n    print(\"Grad Input\")\r\n    print(go)\r\n    print(\"Grad Output\")\r\n    print(gi)\r\n\r\nclass M(nn.Module):\r\n    def __init__(self):\r\n        super(M,self).__init__()\r\n        self.register_backward_hook(bh)\r\n\r\n\r\n    def forward(self,x,y,z):\r\n        return (x+y+z)\r\n\r\nx=Variable(torch.randn(1,5),requires_grad=True)\r\ny=Variable(torch.randn(1,5),requires_grad=True)\r\nz=Variable(torch.randn(1,5),requires_grad=True)\r\n\r\ncriterion=nn.MSELoss()\r\nmod=M()\r\nout=mod(x,y,z)\r\nloss=criterion(out,Variable(torch.randn(1,5)))\r\nloss.backward()```\r\n\r\nIn that case, when I print grad_input throught the hook function, it is just composed of two elements... Could you tell me where am I wrong ? But `x.grad, y.grad and z.grad` seem correctly computed"}