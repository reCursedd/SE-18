{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/372150869", "html_url": "https://github.com/pytorch/pytorch/issues/1776#issuecomment-372150869", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1776", "id": 372150869, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjE1MDg2OQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-11T21:28:26Z", "updated_at": "2018-03-13T13:12:49Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=263228\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/LMescheder\">@LMescheder</a> sorry for a late reply. This is because the snippet uses a number of older APIs that are not compatible with <code>master</code>. There have been a number of changes. I'm attaching a script that showcases how this can be done (and works for me locally):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch                                                    \n                                                                \n<span class=\"pl-k\">class</span> <span class=\"pl-en\">exampleFct</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):                      \n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>                                               \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):                                       \n        <span class=\"pl-c1\">self</span>.save_for_backward(x)                               \n        <span class=\"pl-k\">return</span> x <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>                                           \n                                                                \n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>                                               \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dy</span>):                                     \n        x, <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.saved_variables                               \n        <span class=\"pl-k\">with</span> torch.enable_grad():                               \n            y <span class=\"pl-k\">=</span> x <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>                                          \n            <span class=\"pl-k\">return</span> torch.autograd.grad(y, x, dy)\n                                                                \n                                                                \nx <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>]], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)                  \nm <span class=\"pl-k\">=</span> exampleFct.apply(x).sum().backward()                        \n<span class=\"pl-c1\">print</span>(x.grad.data)                                              </pre></div>", "body_text": "@LMescheder sorry for a late reply. This is because the snippet uses a number of older APIs that are not compatible with master. There have been a number of changes. I'm attaching a script that showcases how this can be done (and works for me locally):\nimport torch                                                    \n                                                                \nclass exampleFct(torch.autograd.Function):                      \n    @staticmethod                                               \n    def forward(self, x):                                       \n        self.save_for_backward(x)                               \n        return x ** 2                                           \n                                                                \n    @staticmethod                                               \n    def backward(self, dy):                                     \n        x, = self.saved_variables                               \n        with torch.enable_grad():                               \n            y = x ** 2                                          \n            return torch.autograd.grad(y, x, dy)\n                                                                \n                                                                \nx = torch.tensor([[3, 4]], requires_grad=True)                  \nm = exampleFct.apply(x).sum().backward()                        \nprint(x.grad.data)", "body": "@LMescheder sorry for a late reply. This is because the snippet uses a number of older APIs that are not compatible with `master`. There have been a number of changes. I'm attaching a script that showcases how this can be done (and works for me locally):\r\n```python\r\nimport torch                                                    \r\n                                                                \r\nclass exampleFct(torch.autograd.Function):                      \r\n    @staticmethod                                               \r\n    def forward(self, x):                                       \r\n        self.save_for_backward(x)                               \r\n        return x ** 2                                           \r\n                                                                \r\n    @staticmethod                                               \r\n    def backward(self, dy):                                     \r\n        x, = self.saved_variables                               \r\n        with torch.enable_grad():                               \r\n            y = x ** 2                                          \r\n            return torch.autograd.grad(y, x, dy)\r\n                                                                \r\n                                                                \r\nx = torch.tensor([[3, 4]], requires_grad=True)                  \r\nm = exampleFct.apply(x).sum().backward()                        \r\nprint(x.grad.data)                                              \r\n```"}