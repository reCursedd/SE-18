{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3122", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3122/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3122/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3122/events", "html_url": "https://github.com/pytorch/pytorch/issues/3122", "id": 265538308, "node_id": "MDU6SXNzdWUyNjU1MzgzMDg=", "number": 3122, "title": "Bug - BatchNormalization - normalization var and running var are not compatible", "user": {"login": "robertoalotufo", "id": 8480730, "node_id": "MDQ6VXNlcjg0ODA3MzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/8480730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robertoalotufo", "html_url": "https://github.com/robertoalotufo", "followers_url": "https://api.github.com/users/robertoalotufo/followers", "following_url": "https://api.github.com/users/robertoalotufo/following{/other_user}", "gists_url": "https://api.github.com/users/robertoalotufo/gists{/gist_id}", "starred_url": "https://api.github.com/users/robertoalotufo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robertoalotufo/subscriptions", "organizations_url": "https://api.github.com/users/robertoalotufo/orgs", "repos_url": "https://api.github.com/users/robertoalotufo/repos", "events_url": "https://api.github.com/users/robertoalotufo/events{/privacy}", "received_events_url": "https://api.github.com/users/robertoalotufo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-15T01:46:22Z", "updated_at": "2017-10-15T09:20:52Z", "closed_at": "2017-10-15T09:20:52Z", "author_association": "NONE", "body_html": "<p>After noticing the different behavior of a network with BatchNorm during training and eval(), I realized that the running_var is updated using <strong>unbiased var</strong> while the normalization var uses <strong>biased var</strong>:</p>\n<p>File: <code>pytorch/torch/lib/THNN/generic/BatchNormalization.c</code></p>\n<p><code>Line 40:         invstd = (real) (1 / sqrt(sum/n + eps));</code></p>\n<p><code>Line 48:         accreal unbiased_var = sum / (n - 1);</code></p>\n<p><code>invstd</code> (line 40) is used to normalize the mini-batch<br>\n<code>unbiased_var</code> (line 48) is used to update the <code>running_var</code> with <code>momentum</code> factor.</p>\n<p>This can be corrected either by replacing <code>n</code> by <code>(n-1)</code> in line40 or replacing <code>(n-1)</code> by <code>n</code> in line48.</p>\n<p>I have seen some complaints of other users regarding problems with batchnorm, difference from training and eval(). This may be the problem.</p>", "body_text": "After noticing the different behavior of a network with BatchNorm during training and eval(), I realized that the running_var is updated using unbiased var while the normalization var uses biased var:\nFile: pytorch/torch/lib/THNN/generic/BatchNormalization.c\nLine 40:         invstd = (real) (1 / sqrt(sum/n + eps));\nLine 48:         accreal unbiased_var = sum / (n - 1);\ninvstd (line 40) is used to normalize the mini-batch\nunbiased_var (line 48) is used to update the running_var with momentum factor.\nThis can be corrected either by replacing n by (n-1) in line40 or replacing (n-1) by n in line48.\nI have seen some complaints of other users regarding problems with batchnorm, difference from training and eval(). This may be the problem.", "body": "After noticing the different behavior of a network with BatchNorm during training and eval(), I realized that the running_var is updated using **unbiased var** while the normalization var uses **biased var**:\r\n\r\nFile: `pytorch/torch/lib/THNN/generic/BatchNormalization.c`\r\n\r\n`Line 40:         invstd = (real) (1 / sqrt(sum/n + eps));`\r\n\r\n`Line 48:         accreal unbiased_var = sum / (n - 1);`\r\n\r\n`invstd` (line 40) is used to normalize the mini-batch\r\n`unbiased_var` (line 48) is used to update the `running_var` with `momentum` factor.\r\n\r\nThis can be corrected either by replacing `n` by `(n-1)` in line40 or replacing `(n-1)` by `n` in line48.\r\n\r\nI have seen some complaints of other users regarding problems with batchnorm, difference from training and eval(). This may be the problem."}