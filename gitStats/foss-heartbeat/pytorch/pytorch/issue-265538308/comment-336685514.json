{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/336685514", "html_url": "https://github.com/pytorch/pytorch/issues/3122#issuecomment-336685514", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3122", "id": 336685514, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjY4NTUxNA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-15T04:24:02Z", "updated_at": "2017-10-15T04:24:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Implementation follows batch normalization paper, <a href=\"https://arxiv.org/abs/1502.03167\" rel=\"nofollow\">https://arxiv.org/abs/1502.03167</a> that calls for unbiased variance when computing moving averages (page 4), and biased variance during training (page 3). It is expected that batch norm results are different during training and evaluation, because during training the batch is normalized with its own mean and (biased)  variance, and during evaluation accumulated running variance and mean are used, thus making evaluation batch-independent.</p>", "body_text": "Implementation follows batch normalization paper, https://arxiv.org/abs/1502.03167 that calls for unbiased variance when computing moving averages (page 4), and biased variance during training (page 3). It is expected that batch norm results are different during training and evaluation, because during training the batch is normalized with its own mean and (biased)  variance, and during evaluation accumulated running variance and mean are used, thus making evaluation batch-independent.", "body": "Implementation follows batch normalization paper, https://arxiv.org/abs/1502.03167 that calls for unbiased variance when computing moving averages (page 4), and biased variance during training (page 3). It is expected that batch norm results are different during training and evaluation, because during training the batch is normalized with its own mean and (biased)  variance, and during evaluation accumulated running variance and mean are used, thus making evaluation batch-independent."}