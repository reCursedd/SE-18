{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/361835491", "html_url": "https://github.com/pytorch/pytorch/issues/4957#issuecomment-361835491", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4957", "id": 361835491, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTgzNTQ5MQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T06:18:47Z", "updated_at": "2018-01-31T06:20:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Well, you are using <code>itertools.chain</code> wrongly. It takes a variable number of iterables, and your code should read <code>torch.optim.Adam(itertools.chain(net.parameters(), [W]))</code>.</p>\n<p>Your first line fails because each tensor happens to be an iterable of slices so the optimizer class receives slices of parameters, i.e. non-leafs.</p>\n<p>Your second line works because it is essentially constructing a list with all parameters plus <code>W</code>. Since <code>net.parameters()</code> is finite, it is effectively the same with <code>itertools.chain(net.parameters(), [W])</code>, albeit slower.</p>\n<p>Edit: there is nothing we can do about the error message because all the optimizer class sees are slices and pytorch has no way of knowing that they are sliced because of a <code>itertools.chain</code>.</p>", "body_text": "Well, you are using itertools.chain wrongly. It takes a variable number of iterables, and your code should read torch.optim.Adam(itertools.chain(net.parameters(), [W])).\nYour first line fails because each tensor happens to be an iterable of slices so the optimizer class receives slices of parameters, i.e. non-leafs.\nYour second line works because it is essentially constructing a list with all parameters plus W. Since net.parameters() is finite, it is effectively the same with itertools.chain(net.parameters(), [W]), albeit slower.\nEdit: there is nothing we can do about the error message because all the optimizer class sees are slices and pytorch has no way of knowing that they are sliced because of a itertools.chain.", "body": "Well, you are using `itertools.chain` wrongly. It takes a variable number of iterables, and your code should read `torch.optim.Adam(itertools.chain(net.parameters(), [W]))`. \r\n\r\nYour first line fails because each tensor happens to be an iterable of slices so the optimizer class receives slices of parameters, i.e. non-leafs.\r\n\r\nYour second line works because it is essentially constructing a list with all parameters plus `W`. Since `net.parameters()` is finite, it is effectively the same with `itertools.chain(net.parameters(), [W])`, albeit slower.\r\n\r\nEdit: there is nothing we can do about the error message because all the optimizer class sees are slices and pytorch has no way of knowing that they are sliced because of a `itertools.chain`."}