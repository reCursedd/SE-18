{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4957", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4957/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4957/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4957/events", "html_url": "https://github.com/pytorch/pytorch/issues/4957", "id": 293037301, "node_id": "MDU6SXNzdWUyOTMwMzczMDE=", "number": 4957, "title": "confusing error message `ValueError: can't optimize a non-leaf Variable`", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-31T05:38:56Z", "updated_at": "2018-02-01T01:51:06Z", "closed_at": "2018-02-01T01:51:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When submitting a bug report, please include the following information (where relevant):</p>\n<ul>\n<li>OS: macOS</li>\n<li>PyTorch version: 0.3.0.post4</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: 3.6.2</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> itertools\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nnet <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\nW <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This does NOT work!</span>\ntorch.optim.Adam(itertools.chain(<span class=\"pl-k\">*</span>net.parameters(), W))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ... but this works just fine.</span>\ntorch.optim.Adam([<span class=\"pl-k\">*</span>net.parameters(), W])</pre></div>\n<p>If one uses <code>torch.optim.Adam(itertools.chain(*net.parameters(), *[W]))</code>, there's a nasty error:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/Development/icml2018/leaf_variable_issue.py in &lt;module&gt;()\n      8 W = Variable(torch.zeros(1, 1), requires_grad=True)\n      9 \n---&gt; 10 torch.optim.Adam(itertools.chain(*net.parameters(), *[W]))\n     11 # torch.optim.Adam([*net.parameters(), *[W]])\n\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/adam.py in __init__(self, params, lr, betas, eps, weight_decay)\n     27         defaults = dict(lr=lr, betas=betas, eps=eps,\n     28                         weight_decay=weight_decay)\n---&gt; 29         super(Adam, self).__init__(params, defaults)\n     30 \n     31     def step(self, closure=None):\n\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/optimizer.py in __init__(self, params, defaults)\n     37 \n     38         for param_group in param_groups:\n---&gt; 39             self.add_param_group(param_group)\n     40 \n     41     def __getstate__(self):\n\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/optimizer.py in add_param_group(self, param_group)\n    153                 raise ValueError(\"optimizing a parameter that doesn't require gradients\")\n    154             if not param.is_leaf:\n--&gt; 155                 raise ValueError(\"can't optimize a non-leaf Variable\")\n    156 \n    157         for name, default in self.defaults.items():\n\nValueError: can't optimize a non-leaf Variable\n</code></pre>\n<p>Even if these optimization classes aren't meant to support itertools.chain somehow, this error message is just not correct.</p>", "body_text": "When submitting a bug report, please include the following information (where relevant):\n\nOS: macOS\nPyTorch version: 0.3.0.post4\nHow you installed PyTorch (conda, pip, source): pip\nPython version: 3.6.2\n\nimport itertools\nimport torch\nfrom torch.autograd import Variable\n\nnet = torch.nn.Linear(1, 1)\nW = Variable(torch.zeros(1, 1), requires_grad=True)\n\n# This does NOT work!\ntorch.optim.Adam(itertools.chain(*net.parameters(), W))\n\n# ... but this works just fine.\ntorch.optim.Adam([*net.parameters(), W])\nIf one uses torch.optim.Adam(itertools.chain(*net.parameters(), *[W])), there's a nasty error:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/Development/icml2018/leaf_variable_issue.py in <module>()\n      8 W = Variable(torch.zeros(1, 1), requires_grad=True)\n      9 \n---> 10 torch.optim.Adam(itertools.chain(*net.parameters(), *[W]))\n     11 # torch.optim.Adam([*net.parameters(), *[W]])\n\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/adam.py in __init__(self, params, lr, betas, eps, weight_decay)\n     27         defaults = dict(lr=lr, betas=betas, eps=eps,\n     28                         weight_decay=weight_decay)\n---> 29         super(Adam, self).__init__(params, defaults)\n     30 \n     31     def step(self, closure=None):\n\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/optimizer.py in __init__(self, params, defaults)\n     37 \n     38         for param_group in param_groups:\n---> 39             self.add_param_group(param_group)\n     40 \n     41     def __getstate__(self):\n\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/optimizer.py in add_param_group(self, param_group)\n    153                 raise ValueError(\"optimizing a parameter that doesn't require gradients\")\n    154             if not param.is_leaf:\n--> 155                 raise ValueError(\"can't optimize a non-leaf Variable\")\n    156 \n    157         for name, default in self.defaults.items():\n\nValueError: can't optimize a non-leaf Variable\n\nEven if these optimization classes aren't meant to support itertools.chain somehow, this error message is just not correct.", "body": "When submitting a bug report, please include the following information (where relevant):\r\n- OS: macOS\r\n- PyTorch version: 0.3.0.post4\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: 3.6.2\r\n\r\n```python\r\nimport itertools\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nnet = torch.nn.Linear(1, 1)\r\nW = Variable(torch.zeros(1, 1), requires_grad=True)\r\n\r\n# This does NOT work!\r\ntorch.optim.Adam(itertools.chain(*net.parameters(), W))\r\n\r\n# ... but this works just fine.\r\ntorch.optim.Adam([*net.parameters(), W])\r\n```\r\n\r\nIf one uses `torch.optim.Adam(itertools.chain(*net.parameters(), *[W]))`, there's a nasty error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/Development/icml2018/leaf_variable_issue.py in <module>()\r\n      8 W = Variable(torch.zeros(1, 1), requires_grad=True)\r\n      9 \r\n---> 10 torch.optim.Adam(itertools.chain(*net.parameters(), *[W]))\r\n     11 # torch.optim.Adam([*net.parameters(), *[W]])\r\n\r\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/adam.py in __init__(self, params, lr, betas, eps, weight_decay)\r\n     27         defaults = dict(lr=lr, betas=betas, eps=eps,\r\n     28                         weight_decay=weight_decay)\r\n---> 29         super(Adam, self).__init__(params, defaults)\r\n     30 \r\n     31     def step(self, closure=None):\r\n\r\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/optimizer.py in __init__(self, params, defaults)\r\n     37 \r\n     38         for param_group in param_groups:\r\n---> 39             self.add_param_group(param_group)\r\n     40 \r\n     41     def __getstate__(self):\r\n\r\n~/Development/icml2018/venv/lib/python3.6/site-packages/torch/optim/optimizer.py in add_param_group(self, param_group)\r\n    153                 raise ValueError(\"optimizing a parameter that doesn't require gradients\")\r\n    154             if not param.is_leaf:\r\n--> 155                 raise ValueError(\"can't optimize a non-leaf Variable\")\r\n    156 \r\n    157         for name, default in self.defaults.items():\r\n\r\nValueError: can't optimize a non-leaf Variable\r\n```\r\nEven if these optimization classes aren't meant to support itertools.chain somehow, this error message is just not correct."}