{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/117043927", "pull_request_review_id": 38714667, "id": 117043927, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNzA0MzkyNw==", "diff_hunk": "@@ -120,204 +143,432 @@ static auto view3d(const Tensor& tensor) -> std::unique_ptr<Tensor> {\n }\n \n auto ConvForward::apply(const variable_list& inputs) -> variable_list {\n-  if (inputs.size() != 3) throw std::runtime_error(\"expected three inputs\");\n-  if (is_padding_neg()) throw std::runtime_error(\"negative padding is not supported\");\n-  if (is_output_padding_neg()) throw std::runtime_error(\"negative output_padding is not supported\");\n-\n-  AutoGPU guard(inputs[0]->data->getDevice());\n-  auto input = inputs[0]->data->contiguous();\n-  std::unique_ptr<Tensor> weight(inputs[1]->data->clone_shallow());\n-  std::unique_ptr<Tensor> bias(inputs[2] ? inputs[2]->data->clone_shallow() : nullptr);\n-\n-  int k = input->nDim();\n-  if (k == 3) {\n-    view1d_as_2d();\n-    input = view4d(*input);\n-    weight = view4d(*weight);\n-  }\n+  if (forward_mode) {\n+      std::cout << \"ConvForward in forward mode\" << std::endl;\n+      if (inputs.size() != 3) throw std::runtime_error(\"expected three inputs\");\n+      if (is_padding_neg()) throw std::runtime_error(\"negative padding is not supported\");\n+      if (is_output_padding_neg()) throw std::runtime_error(\"negative output_padding is not supported\");\n+\n+      AutoGPU guard(inputs[0]->data->getDevice());\n+      auto input = inputs[0]->data->contiguous();\n+      std::unique_ptr<Tensor> weight(inputs[1]->data->clone_shallow());\n+      std::unique_ptr<Tensor> bias(inputs[2] ? inputs[2]->data->clone_shallow() : nullptr);\n+\n+      int k = input->nDim();\n+      if (k == 3) {\n+        view1d_as_2d();\n+        input = view4d(*input);\n+        weight = view4d(*weight);\n+      }\n \n-  auto weight_size = weight->sizes();\n-  std::vector<long> kernel_size(weight_size.begin() + 2, weight_size.end());\n+      auto weight_size = weight->sizes();\n+      std::vector<long> kernel_size(weight_size.begin() + 2, weight_size.end());\n+\n+      std::unique_ptr<Tensor> output;\n+      tensor_list columns(groups);\n+      tensor_list ones(groups);\n+      std::shared_ptr<Convolution> convolution;\n+\n+      if (use_cudnn(*input)) {\n+    #ifdef WITH_CUDNN\n+        output = input->newTensor();\n+        output->resize(output_size(*input, *weight));\n+//        if (transposed) {\n+//          convolution.reset(cudnn_convolution_transpose_full_forward(\n+//              state, torch::cudnn::getCudnnHandle(), torch::cudnn::getCudnnDataType(*input),\n+//              (THVoidTensor*)input->cdata(), (THVoidTensor*)weight->cdata(),\n+//              bias ? (THVoidTensor*)bias->cdata() : nullptr, (THVoidTensor*)output->cdata(),\n+//              padding, stride, dilation, groups, benchmark));\n+//        } else {\n+          convolution.reset(cudnn_convolution_full_forward(\n+              state, torch::cudnn::getCudnnHandle(), torch::cudnn::getCudnnDataType(*input),\n+              (THVoidTensor*)input->cdata(), (THVoidTensor*)weight->cdata(),\n+              bias ? (THVoidTensor*)bias->cdata() : nullptr, (THVoidTensor*)output->cdata(),\n+              padding, stride, dilation, groups, benchmark));\n+//        }\n+    #endif\n+      } else {\n+        for (int g = 0; g < groups; ++g) {\n+          columns[g] = input->newTensor();\n+          ones[g] = input->newTensor();\n+        }\n+        if (groups == 1) {\n+          output = compute_output(\n+              input.get(), weight.get(), bias.get(),\n+              columns[0].get(), ones[0].get(), kernel_size, *this, false);\n+        } else {\n+          tensor_list outputs(groups);\n+          for (int g = 0; g < groups; ++g) {\n+            auto input_g = subtensor(input.get(), 1, groups, g);\n+            auto weight_g = subtensor(weight.get(), 0, groups, g);\n+            auto bias_g = subtensor(bias.get(), 0, groups, g);\n+            outputs[g] = compute_output(\n+                input_g.get(), weight_g.get(), bias_g.get(),\n+                columns[g].get(), ones[g].get(), kernel_size, *this, false);\n+          }\n+          output = cat(outputs, 1);\n+        }\n+      }\n \n-  std::unique_ptr<Tensor> output;\n-  tensor_list columns(groups);\n-  tensor_list ones(groups);\n-  std::unique_ptr<Convolution> convolution;\n+      if (k == 3) {\n+        output = view3d(*output);\n+      }\n \n-  if (use_cudnn(*input)) {\n-#ifdef WITH_CUDNN\n-    output = input->newTensor();\n-    output->resize(output_size(*input, *weight));\n-    if (transposed) {\n-      convolution.reset(cudnn_convolution_transpose_full_forward(\n-          state, torch::cudnn::getCudnnHandle(), torch::cudnn::getCudnnDataType(*input),\n-          (THVoidTensor*)input->cdata(), (THVoidTensor*)weight->cdata(),\n-          bias ? (THVoidTensor*)bias->cdata() : nullptr, (THVoidTensor*)output->cdata(),\n-          padding, stride, dilation, groups, benchmark));\n-    } else {\n-      convolution.reset(cudnn_convolution_full_forward(\n-          state, torch::cudnn::getCudnnHandle(), torch::cudnn::getCudnnDataType(*input),\n-          (THVoidTensor*)input->cdata(), (THVoidTensor*)weight->cdata(),\n-          bias ? (THVoidTensor*)bias->cdata() : nullptr, (THVoidTensor*)output->cdata(),\n-          padding, stride, dilation, groups, benchmark));\n-    }\n-#endif\n+      auto outputs = as_tensor_list(std::move(output));\n+      std::cout << \"Created Backward convolution\" << convolution.get() << std::endl;\n+      return wrap_outputs(inputs, std::move(outputs), [&](FunctionFlags f) {\n+        return std::make_shared<ConvBackward>(\n+            f, *this,\n+            inputs[0]->save(this), inputs[1]->save(this), Variable::save_opt(inputs[2].get(), this),\n+            std::move(columns), std::move(ones), convolution);\n+      });\n   } else {\n-    for (int g = 0; g < groups; ++g) {\n-      columns[g] = input->newTensor();\n-      ones[g] = input->newTensor();\n-    }\n-    if (groups == 1) {\n-      output = compute_output(\n-          input.get(), weight.get(), bias.get(),\n-          columns[0].get(), ones[0].get(), kernel_size, *this);\n-    } else {\n-      tensor_list outputs(groups);\n-      for (int g = 0; g < groups; ++g) {\n-        auto input_g = subtensor(input.get(), 1, groups, g);\n-        auto weight_g = subtensor(weight.get(), 0, groups, g);\n-        auto bias_g = subtensor(bias.get(), 0, groups, g);\n-        outputs[g] = compute_output(\n-            input_g.get(), weight_g.get(), bias_g.get(),\n-            columns[g].get(), ones[g].get(), kernel_size, *this);\n+      std::cout << \"ConvForward in grad mode\" << std::endl;", "path": "torch/csrc/autograd/functions/convolution.cpp", "position": 196, "original_position": 196, "commit_id": "55a4e103628b2c384f8b60a00ed8405a2815d8a9", "original_commit_id": "d3a75727aca9af4f2696698f3e2975e44fd81bb1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Wait, why are there two modes in the forward function?", "created_at": "2017-05-17T16:02:27Z", "updated_at": "2018-11-23T15:33:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/1569#discussion_r117043927", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1569", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/117043927"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1569#discussion_r117043927"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1569"}}, "body_html": "<p>Wait, why are there two modes in the forward function?</p>", "body_text": "Wait, why are there two modes in the forward function?"}