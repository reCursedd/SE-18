{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1569", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1569/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1569/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1569/events", "html_url": "https://github.com/pytorch/pytorch/pull/1569", "id": 228989982, "node_id": "MDExOlB1bGxSZXF1ZXN0MTIwNzk3NDc2", "number": 1569, "title": "[Need Discussion] Implement twice backward of ConvNd", "user": {"login": "caogang", "id": 2537027, "node_id": "MDQ6VXNlcjI1MzcwMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2537027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caogang", "html_url": "https://github.com/caogang", "followers_url": "https://api.github.com/users/caogang/followers", "following_url": "https://api.github.com/users/caogang/following{/other_user}", "gists_url": "https://api.github.com/users/caogang/gists{/gist_id}", "starred_url": "https://api.github.com/users/caogang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caogang/subscriptions", "organizations_url": "https://api.github.com/users/caogang/orgs", "repos_url": "https://api.github.com/users/caogang/repos", "events_url": "https://api.github.com/users/caogang/events{/privacy}", "received_events_url": "https://api.github.com/users/caogang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-05-16T10:49:23Z", "updated_at": "2018-11-23T15:33:30Z", "closed_at": "2017-05-25T01:18:43Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1569", "html_url": "https://github.com/pytorch/pytorch/pull/1569", "diff_url": "https://github.com/pytorch/pytorch/pull/1569.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1569.patch"}, "body_html": "<p>Hi, all.</p>\n<p>This PR is following the PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"228595462\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1555\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/1555/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/1555\">#1555</a> with cleaner rebase.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I have followed your suggestion to modify the <code>ConvForward</code> directly. And change the implementation methods of <code>ConvTransposeNd</code> to use <code>ConvBackward</code> directly instead of <code>ConvForward(transposed)</code>. Here is the commit. And I have used <code>gradcheck</code> to check the two gradient. It passed the following test.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> gradcheck\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> (autograd.Variable(torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">22</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),)\ntest <span class=\"pl-k\">=</span> gradcheck(nn.Conv1d(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">12</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">input</span>, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-3</span>, <span class=\"pl-v\">atol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n<span class=\"pl-c1\">print</span>(test)\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> (autograd.Variable(torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">22</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),)\ntest <span class=\"pl-k\">=</span> gradcheck(nn.ConvTranspose1d(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">12</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">input</span>, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-3</span>, <span class=\"pl-v\">atol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n<span class=\"pl-c1\">print</span>(test)</pre></div>\n<p>However when I use it to perform grad of grad of <code>ConvForward</code>, it raise error. So for now, do not merge this PR.</p>\n<p>Assume the forward formula to calculate, is ConvForward(forward_mode) -&gt; ConvForward(forward_mode) -&gt; ConvBackward(grad_mode) -&gt;ConvBackward(grad_mode). And the backward process is ConvForward(grad_mode) -&gt; ConvForward(grad_mode) -&gt; ConvBackward(grad_mode) -&gt; ConvBackward (grad_mode). It raised a <code>Segment Fault</code> Error at the <strong>third backward process unit,ConvBackward(grad_mode)</strong></p>\n<p>I have found the problem is because of the <code>std::move(convolution)</code> in <code>ConvBackward::apply</code>. During the forward process of <code>ConvBackward(grad_mode)</code>, it will use <code>std::move(convolution)</code> to pass arguments, and <code>convolution</code> of itself will be zero. When the second time processing this unit in backward process, some parameters like <code>convolution</code> will cause this <code>Segment error</code>.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> So how can I pass this <code>convolution</code> parameters with itself retained not be cleared when using <code>std::move</code> in the following place?</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-v\">file</span> : torch<span class=\"pl-k\">/</span>csrc<span class=\"pl-k\">/</span>autograd<span class=\"pl-k\">/</span>functions<span class=\"pl-k\">/</span>convolution.cpp\n<span class=\"pl-c1\">568</span><span class=\"pl-k\">+</span>            std::move(columns), std::move(ones), std::move(convolution))<span class=\"pl-bu\">;</span></pre></div>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> And the problem remained in PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"228595462\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1555\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/1555/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/1555\">#1555</a></li>\n</ul>\n<blockquote>\n<p>I have a problem. Assume the forward formula to calculate, is ConvForward -&gt; ConvForward -&gt; ConvBackward -&gt;ConvBackward. And the backward process is ConvForward -&gt; ConvForward -&gt; ConvBackward -&gt; ConvBackward. Should I accumulate grad of weight and bias in every backward process unit? Or just do that in the ConvBackward unit.</p>\n</blockquote>", "body_text": "Hi, all.\nThis PR is following the PR #1555 with cleaner rebase.\n@apaszke I have followed your suggestion to modify the ConvForward directly. And change the implementation methods of ConvTransposeNd to use ConvBackward directly instead of ConvForward(transposed). Here is the commit. And I have used gradcheck to check the two gradient. It passed the following test.\nfrom torch.autograd import gradcheck\ninput = (autograd.Variable(torch.randn(1, 20, 22), requires_grad=True),)\ntest = gradcheck(nn.Conv1d(20, 12, 4), input, eps=1e-3, atol=1e-4)\nprint(test)\ninput = (autograd.Variable(torch.randn(1, 20, 22), requires_grad=True),)\ntest = gradcheck(nn.ConvTranspose1d(20, 12, 4), input, eps=1e-3, atol=1e-4)\nprint(test)\nHowever when I use it to perform grad of grad of ConvForward, it raise error. So for now, do not merge this PR.\nAssume the forward formula to calculate, is ConvForward(forward_mode) -> ConvForward(forward_mode) -> ConvBackward(grad_mode) ->ConvBackward(grad_mode). And the backward process is ConvForward(grad_mode) -> ConvForward(grad_mode) -> ConvBackward(grad_mode) -> ConvBackward (grad_mode). It raised a Segment Fault Error at the third backward process unit,ConvBackward(grad_mode)\nI have found the problem is because of the std::move(convolution) in ConvBackward::apply. During the forward process of ConvBackward(grad_mode), it will use std::move(convolution) to pass arguments, and convolution of itself will be zero. When the second time processing this unit in backward process, some parameters like convolution will cause this Segment error.\n\n So how can I pass this convolution parameters with itself retained not be cleared when using std::move in the following place?\n\nfile : torch/csrc/autograd/functions/convolution.cpp\n568+            std::move(columns), std::move(ones), std::move(convolution));\n\n And the problem remained in PR #1555\n\n\nI have a problem. Assume the forward formula to calculate, is ConvForward -> ConvForward -> ConvBackward ->ConvBackward. And the backward process is ConvForward -> ConvForward -> ConvBackward -> ConvBackward. Should I accumulate grad of weight and bias in every backward process unit? Or just do that in the ConvBackward unit.", "body": "Hi, all.\r\n\r\nThis PR is following the PR #1555 with cleaner rebase.\r\n\r\n@apaszke I have followed your suggestion to modify the `ConvForward` directly. And change the implementation methods of `ConvTransposeNd` to use `ConvBackward` directly instead of `ConvForward(transposed)`. Here is the commit. And I have used `gradcheck` to check the two gradient. It passed the following test.\r\n\r\n```python\r\nfrom torch.autograd import gradcheck\r\ninput = (autograd.Variable(torch.randn(1, 20, 22), requires_grad=True),)\r\ntest = gradcheck(nn.Conv1d(20, 12, 4), input, eps=1e-3, atol=1e-4)\r\nprint(test)\r\ninput = (autograd.Variable(torch.randn(1, 20, 22), requires_grad=True),)\r\ntest = gradcheck(nn.ConvTranspose1d(20, 12, 4), input, eps=1e-3, atol=1e-4)\r\nprint(test)\r\n```\r\n\r\nHowever when I use it to perform grad of grad of `ConvForward`, it raise error. So for now, do not merge this PR.\r\n\r\nAssume the forward formula to calculate, is ConvForward(forward_mode) -> ConvForward(forward_mode) -> ConvBackward(grad_mode) ->ConvBackward(grad_mode). And the backward process is ConvForward(grad_mode) -> ConvForward(grad_mode) -> ConvBackward(grad_mode) -> ConvBackward (grad_mode). It raised a `Segment Fault` Error at the **third backward process unit,ConvBackward(grad_mode)**\r\n\r\nI have found the problem is because of the `std::move(convolution)` in `ConvBackward::apply`. During the forward process of `ConvBackward(grad_mode)`, it will use `std::move(convolution)` to pass arguments, and `convolution` of itself will be zero. When the second time processing this unit in backward process, some parameters like `convolution` will cause this `Segment error`.\r\n\r\n- [x] So how can I pass this `convolution` parameters with itself retained not be cleared when using `std::move` in the following place? \r\n```python\r\nfile : torch/csrc/autograd/functions/convolution.cpp\r\n568+            std::move(columns), std::move(ones), std::move(convolution));\r\n```\r\n- [ ] And the problem remained in PR #1555 \r\n\r\n> I have a problem. Assume the forward formula to calculate, is ConvForward -> ConvForward -> ConvBackward ->ConvBackward. And the backward process is ConvForward -> ConvForward -> ConvBackward -> ConvBackward. Should I accumulate grad of weight and bias in every backward process unit? Or just do that in the ConvBackward unit."}