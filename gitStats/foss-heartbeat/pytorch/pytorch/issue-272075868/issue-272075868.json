{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3560", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3560/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3560/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3560/events", "html_url": "https://github.com/pytorch/pytorch/issues/3560", "id": 272075868, "node_id": "MDU6SXNzdWUyNzIwNzU4Njg=", "number": 3560, "title": "Improve \"the number of sizes provided must be greater or equal to the number of dimensions in the tensor\" error", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2017-11-08T05:05:03Z", "updated_at": "2017-12-01T22:38:37Z", "closed_at": "2017-12-01T22:38:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The new ATen backend makes a lot of implicit calls to expand, which can fail if the sizes are not correct. Unfortunately, when something like this fails, you get a fairly unhelpful backtrace:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test/test_nn.py\", line 3633, in &lt;lambda&gt;\n    setattr(TestNN, test_name, lambda self, test=test: test(self))\n  File \"/Users/ezyang/Dev/pytorch/test/common_nn.py\", line 771, in __call__\n    self._do_test(test_case, module, input)\n  File \"test/test_nn.py\", line 106, in _do_test\n    lambda x, *args, **kw: test_case._forward(module, x), (input,) + params)\n  File \"test/test_nn.py\", line 74, in _assertGradAndGradgradChecks\n    test_case.assertTrue(gradgradcheck(apply_fn, inputs, grad_y,))\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 247, in gradgradcheck\n    return gradcheck(new_func, inputs + grad_outputs, eps, atol, rtol)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 176, in gradcheck\n    analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 110, in get_analytical_jacobian\n    output.backward(grad_output, create_graph=True)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/variable.py\", line 168, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/__init__.py\", line 99, in backward\n    variables, grad_variables, retain_graph)\nRuntimeError: the number of sizes provided must be greater or equal to the number of dimensions in the tensor\n</code></pre>\n<p>Let's make these traces better.</p>\n<p>Relatedly, we almost certainly have a bug in the new codepath related to this, see: <a href=\"https://discuss.pytorch.org/t/error-at-backward-the-number-of-sizes-provided-must-be-greater-or-equal-to-the-number-of-dimensions-in-the-tensor/9203/3\" rel=\"nofollow\">https://discuss.pytorch.org/t/error-at-backward-the-number-of-sizes-provided-must-be-greater-or-equal-to-the-number-of-dimensions-in-the-tensor/9203/3</a></p>\n<p><strong>Update.</strong> In this particular case, the error arose because I did <code>x += y</code> where x was a 1-dim tensor and y was a 3-dim tensor.</p>\n<p><strong>Update 2.</strong> And the root cause was that I had returned a tuple of gradients in the wrong order.</p>\n<p><strong>Update 3.</strong> And the reason why I returned in the wrong order was that the ATen binding of the backward function had a different argument order than the original, Python autograd function implementation.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a></p>", "body_text": "The new ATen backend makes a lot of implicit calls to expand, which can fail if the sizes are not correct. Unfortunately, when something like this fails, you get a fairly unhelpful backtrace:\nTraceback (most recent call last):\n  File \"test/test_nn.py\", line 3633, in <lambda>\n    setattr(TestNN, test_name, lambda self, test=test: test(self))\n  File \"/Users/ezyang/Dev/pytorch/test/common_nn.py\", line 771, in __call__\n    self._do_test(test_case, module, input)\n  File \"test/test_nn.py\", line 106, in _do_test\n    lambda x, *args, **kw: test_case._forward(module, x), (input,) + params)\n  File \"test/test_nn.py\", line 74, in _assertGradAndGradgradChecks\n    test_case.assertTrue(gradgradcheck(apply_fn, inputs, grad_y,))\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 247, in gradgradcheck\n    return gradcheck(new_func, inputs + grad_outputs, eps, atol, rtol)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 176, in gradcheck\n    analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 110, in get_analytical_jacobian\n    output.backward(grad_output, create_graph=True)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/variable.py\", line 168, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/__init__.py\", line 99, in backward\n    variables, grad_variables, retain_graph)\nRuntimeError: the number of sizes provided must be greater or equal to the number of dimensions in the tensor\n\nLet's make these traces better.\nRelatedly, we almost certainly have a bug in the new codepath related to this, see: https://discuss.pytorch.org/t/error-at-backward-the-number-of-sizes-provided-must-be-greater-or-equal-to-the-number-of-dimensions-in-the-tensor/9203/3\nUpdate. In this particular case, the error arose because I did x += y where x was a 1-dim tensor and y was a 3-dim tensor.\nUpdate 2. And the root cause was that I had returned a tuple of gradients in the wrong order.\nUpdate 3. And the reason why I returned in the wrong order was that the ATen binding of the backward function had a different argument order than the original, Python autograd function implementation.\nCC @gchanan", "body": "The new ATen backend makes a lot of implicit calls to expand, which can fail if the sizes are not correct. Unfortunately, when something like this fails, you get a fairly unhelpful backtrace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test/test_nn.py\", line 3633, in <lambda>\r\n    setattr(TestNN, test_name, lambda self, test=test: test(self))\r\n  File \"/Users/ezyang/Dev/pytorch/test/common_nn.py\", line 771, in __call__\r\n    self._do_test(test_case, module, input)\r\n  File \"test/test_nn.py\", line 106, in _do_test\r\n    lambda x, *args, **kw: test_case._forward(module, x), (input,) + params)\r\n  File \"test/test_nn.py\", line 74, in _assertGradAndGradgradChecks\r\n    test_case.assertTrue(gradgradcheck(apply_fn, inputs, grad_y,))\r\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 247, in gradgradcheck\r\n    return gradcheck(new_func, inputs + grad_outputs, eps, atol, rtol)\r\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 176, in gradcheck\r\n    analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\r\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/gradcheck.py\", line 110, in get_analytical_jacobian\r\n    output.backward(grad_output, create_graph=True)\r\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/variable.py\", line 168, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/Users/ezyang/Dev/pytorch/torch/autograd/__init__.py\", line 99, in backward\r\n    variables, grad_variables, retain_graph)\r\nRuntimeError: the number of sizes provided must be greater or equal to the number of dimensions in the tensor\r\n```\r\n\r\nLet's make these traces better.\r\n\r\nRelatedly, we almost certainly have a bug in the new codepath related to this, see: https://discuss.pytorch.org/t/error-at-backward-the-number-of-sizes-provided-must-be-greater-or-equal-to-the-number-of-dimensions-in-the-tensor/9203/3\r\n\r\n**Update.** In this particular case, the error arose because I did `x += y` where x was a 1-dim tensor and y was a 3-dim tensor.\r\n\r\n**Update 2.** And the root cause was that I had returned a tuple of gradients in the wrong order.\r\n\r\n**Update 3.** And the reason why I returned in the wrong order was that the ATen binding of the backward function had a different argument order than the original, Python autograd function implementation.\r\n\r\nCC @gchanan "}