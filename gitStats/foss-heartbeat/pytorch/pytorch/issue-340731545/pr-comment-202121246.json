{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202121246", "pull_request_review_id": 136757684, "id": 202121246, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjEyMTI0Ng==", "diff_hunk": "@@ -1,63 +1,290 @@\n #include \"torch/csrc/jit/passes/to_batch.h\"\n #include \"torch/csrc/jit/script/compiler.h\"\n+#include \"torch/csrc/jit/tensor_conversions.h\"\n \n namespace torch { namespace jit {\n \n std::unordered_map<std::string, std::shared_ptr<Graph>> ToBatch::batch_operator_table;\n \n-void ToBatch::toBatch(Block* block, Block* res_block) {\n-  // change inputs of a graph - expand tensor to {data, mask, dims}\n+// eg: \"a.1\" -> {\"a\", \"1\"}; \"a\" -> {\"a\"}\n+std::vector<std::string> ToBatch::get_name(std::string name) {\n+  auto last_dot_pos = name.find_last_of('.');\n+  if (last_dot_pos != std::string::npos && last_dot_pos + 1 != name.size()) {\n+    if (name.find_first_not_of(\"0123456789\", last_dot_pos + 1) == std::string::npos) {\n+      auto suffix = name.substr(last_dot_pos + 1);\n+      std::string name_base = name.substr(0, last_dot_pos);\n+      return {name_base, suffix};\n+    }\n+  }\n+  return {name};\n+}\n+\n+// replace aten operator node with BatchTensor operator graph\n+void ToBatch::visitAten(Node* n, Block* block, Block* res_block, std::unordered_map<std::string, Value*>& var_map){\n+  if(n->outputs().size() > 1){\n+    throw std::runtime_error(\"Cannot process multiple assignment\");\n+  }\n+  auto batch_graph = batch_operator_table.at(n->kind().toUnqualString());\n+  std::vector<Value*> new_inputs;\n+  for(Value *input : n->inputs()){\n+    if(batch_map.find(input) != batch_map.end()){\n+      auto new_input = batch_map.at(input);\n+      new_inputs.insert(new_inputs.end(), new_input.begin(), new_input.end());\n+    }\n+    else{\n+      throw std::runtime_error(\"NYI: non-tensor input for aten operator is not supported yet\");\n+    }\n+  }\n+  auto outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_graph, new_inputs);\n+\n+  // do update on assignment\n+  auto name_base = get_name(n->output()->uniqueName())[0];\n+  if(var_map.find(name_base) != var_map.end()){\n+    std::vector<Value*> inputs(batch_map.at(var_map.at(name_base)));\n+    inputs.insert(inputs.end(), outputs.begin(), outputs.end());\n+    outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"update\"), inputs);\n+  }\n+  // Assume all outputs from inlined operator implementation are in the triple form.\n+  for(size_t i = 0; i < n->outputs().size(); i++){\n+    auto output = n->outputs()[i];\n+    batch_map[output] = std::vector<Value*>(outputs.begin() + i * 3, outputs.begin() + i * 3 + 3);\n+    if(output->hasUniqueName()){\n+      var_map[get_name(output->uniqueName())[0]] = output;\n+    }\n+  }\n+}\n+\n+// clone prim::Constant to new graph", "path": "torch/csrc/jit/passes/to_batch.cpp", "position": 82, "original_position": 59, "commit_id": "154e4eb8cd13cacd121fe3577831ed0590a1a5d5", "original_commit_id": "ce22a0d435a64aa32daf3602ff150fb07285b146", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "body": "So in the case that we see `prim::Constant` in the old graph we just copy it over? Don't you have to apply a batching transformation to it as well?", "created_at": "2018-07-12T17:43:42Z", "updated_at": "2018-11-23T15:47:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/9392#discussion_r202121246", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202121246"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9392#discussion_r202121246"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9392"}}, "body_html": "<p>So in the case that we see <code>prim::Constant</code> in the old graph we just copy it over? Don't you have to apply a batching transformation to it as well?</p>", "body_text": "So in the case that we see prim::Constant in the old graph we just copy it over? Don't you have to apply a batching transformation to it as well?"}