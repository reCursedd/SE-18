{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202131303", "pull_request_review_id": 136769810, "id": 202131303, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjEzMTMwMw==", "diff_hunk": "@@ -1,63 +1,290 @@\n #include \"torch/csrc/jit/passes/to_batch.h\"\n #include \"torch/csrc/jit/script/compiler.h\"\n+#include \"torch/csrc/jit/tensor_conversions.h\"\n \n namespace torch { namespace jit {\n \n std::unordered_map<std::string, std::shared_ptr<Graph>> ToBatch::batch_operator_table;\n \n-void ToBatch::toBatch(Block* block, Block* res_block) {\n-  // change inputs of a graph - expand tensor to {data, mask, dims}\n+// eg: \"a.1\" -> {\"a\", \"1\"}; \"a\" -> {\"a\"}\n+std::vector<std::string> ToBatch::get_name(std::string name) {\n+  auto last_dot_pos = name.find_last_of('.');\n+  if (last_dot_pos != std::string::npos && last_dot_pos + 1 != name.size()) {\n+    if (name.find_first_not_of(\"0123456789\", last_dot_pos + 1) == std::string::npos) {\n+      auto suffix = name.substr(last_dot_pos + 1);\n+      std::string name_base = name.substr(0, last_dot_pos);\n+      return {name_base, suffix};\n+    }\n+  }\n+  return {name};\n+}\n+\n+// replace aten operator node with BatchTensor operator graph\n+void ToBatch::visitAten(Node* n, Block* block, Block* res_block, std::unordered_map<std::string, Value*>& var_map){\n+  if(n->outputs().size() > 1){\n+    throw std::runtime_error(\"Cannot process multiple assignment\");\n+  }\n+  auto batch_graph = batch_operator_table.at(n->kind().toUnqualString());\n+  std::vector<Value*> new_inputs;\n+  for(Value *input : n->inputs()){\n+    if(batch_map.find(input) != batch_map.end()){\n+      auto new_input = batch_map.at(input);\n+      new_inputs.insert(new_inputs.end(), new_input.begin(), new_input.end());\n+    }\n+    else{\n+      throw std::runtime_error(\"NYI: non-tensor input for aten operator is not supported yet\");\n+    }\n+  }\n+  auto outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_graph, new_inputs);\n+\n+  // do update on assignment\n+  auto name_base = get_name(n->output()->uniqueName())[0];\n+  if(var_map.find(name_base) != var_map.end()){\n+    std::vector<Value*> inputs(batch_map.at(var_map.at(name_base)));\n+    inputs.insert(inputs.end(), outputs.begin(), outputs.end());\n+    outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"update\"), inputs);\n+  }\n+  // Assume all outputs from inlined operator implementation are in the triple form.\n+  for(size_t i = 0; i < n->outputs().size(); i++){\n+    auto output = n->outputs()[i];\n+    batch_map[output] = std::vector<Value*>(outputs.begin() + i * 3, outputs.begin() + i * 3 + 3);\n+    if(output->hasUniqueName()){\n+      var_map[get_name(output->uniqueName())[0]] = output;\n+    }\n+  }\n+}\n+\n+// clone prim::Constant to new graph", "path": "torch/csrc/jit/passes/to_batch.cpp", "position": 82, "original_position": 59, "commit_id": "154e4eb8cd13cacd121fe3577831ed0590a1a5d5", "original_commit_id": "ce22a0d435a64aa32daf3602ff150fb07285b146", "user": {"login": "ChunliF", "id": 36351432, "node_id": "MDQ6VXNlcjM2MzUxNDMy", "avatar_url": "https://avatars0.githubusercontent.com/u/36351432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChunliF", "html_url": "https://github.com/ChunliF", "followers_url": "https://api.github.com/users/ChunliF/followers", "following_url": "https://api.github.com/users/ChunliF/following{/other_user}", "gists_url": "https://api.github.com/users/ChunliF/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChunliF/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChunliF/subscriptions", "organizations_url": "https://api.github.com/users/ChunliF/orgs", "repos_url": "https://api.github.com/users/ChunliF/repos", "events_url": "https://api.github.com/users/ChunliF/events{/privacy}", "received_events_url": "https://api.github.com/users/ChunliF/received_events", "type": "User", "site_admin": false}, "body": "I apply a batching transformation to the output of `prim::NumToTensor`. So if there is a `prim::NumToTensor` following `prim::Constant`, it will be transformed.", "created_at": "2018-07-12T18:15:34Z", "updated_at": "2018-11-23T15:47:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/9392#discussion_r202131303", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202131303"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9392#discussion_r202131303"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9392"}}, "body_html": "<p>I apply a batching transformation to the output of <code>prim::NumToTensor</code>. So if there is a <code>prim::NumToTensor</code> following <code>prim::Constant</code>, it will be transformed.</p>", "body_text": "I apply a batching transformation to the output of prim::NumToTensor. So if there is a prim::NumToTensor following prim::Constant, it will be transformed.", "in_reply_to_id": 202121246}