{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202417181", "pull_request_review_id": 137119395, "id": 202417181, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjQxNzE4MQ==", "diff_hunk": "@@ -1,63 +1,441 @@\n #include \"torch/csrc/jit/passes/to_batch.h\"\n #include \"torch/csrc/jit/script/compiler.h\"\n+#include \"torch/csrc/jit/tensor_conversions.h\"\n \n namespace torch { namespace jit {\n \n std::unordered_map<std::string, std::shared_ptr<Graph>> ToBatch::batch_operator_table;\n \n-void ToBatch::toBatch(Block* block, Block* res_block) {\n-  // change inputs of a graph - expand tensor to {data, mask, dims}\n+// replace aten operator node with BatchTensor operator graph\n+void ToBatch::visitAten(Node* n, Block* block, Block* res_block, std::unordered_map<std::string, Value*>& var_map){\n+  if(n->outputs().size() > 1){\n+    throw std::runtime_error(\"NYI: multiple assignment is not supported yet\");\n+  }\n+  auto batch_graph = batch_operator_table.at(n->kind().toUnqualString());\n+  std::vector<Value*> new_inputs;\n+  for(Value *input : n->inputs()){\n+    if(batch_map.find(input) != batch_map.end()){\n+      auto new_input = batch_map.at(input);\n+      new_inputs.insert(new_inputs.end(), new_input.begin(), new_input.end());\n+    }\n+    else{\n+      throw std::runtime_error(\"NYI: non-tensor input for aten operator is not supported yet\");\n+    }\n+  }\n+  auto outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_graph, new_inputs);\n+\n+  // do update on assignment\n+  auto name_base = n->output()->getNameBaseSuffix(n->output()->uniqueName())[0];\n+  if(var_map.find(name_base) != var_map.end()){\n+    std::vector<Value*> inputs = batch_map.at(var_map.at(name_base));\n+    inputs.insert(inputs.end(), outputs.begin(), outputs.end());\n+    outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"update\"), inputs);\n+  }\n+  // Assume all outputs from inlined operator implementation are in the triple form.\n+  for(size_t i = 0; i < n->outputs().size(); i++){\n+    auto output = n->outputs()[i];\n+    batch_map[output] = std::vector<Value*>(outputs.begin() + i * EXP_BTENSOR_SIZE, outputs.begin() + i * EXP_BTENSOR_SIZE + EXP_BTENSOR_SIZE);\n+    if(output->hasUniqueName()){\n+      var_map[output->getNameBaseSuffix(output->uniqueName())[0]] = output;\n+    }\n+  }\n+}\n+\n+// clone prim::Constant to new graph\n+// batching transformation is applied to the output of prim::NumToTensor.\n+// If there is a prim::NumToTensor following prim::Constant, it will be finally transformed to BatchTensor.\n+void ToBatch::visitConstant(Node* n, Block* block, Block* res_block){\n+  auto res_graph = res_block->owningGraph();\n+  auto* r_node = res_graph->createClone(n, rn_fn);\n+  r_node->setStage(n->stage());\n+  res_block->appendNode(r_node);\n+  rn_env[n->output()] = r_node->output();\n+}\n+\n+// change return tensor to expanded batched tensor, eg: {data, mask, dims}\n+void ToBatch::visitNumToTensor(Node* n, Block* block, Block* res_block){\n+  auto res_graph = res_block->owningGraph();\n+  auto* r_node = res_graph->createClone(n, rn_fn);\n+  r_node->setStage(n->stage());\n+  res_graph->appendNode(r_node);\n+  auto outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"batch_from_scalar_tensor\"), r_node->outputs());\n+  batch_map[n->output()] = outputs;\n+}\n+\n+// prim::If transformation:\n+// elif is not supported\n+// assume every variable assigned in an if statement is already defined before\n+//\n+// transformation example:\n+// @torch.jit.batch(batch_size=4)\n+// def batch_if(a, b):\n+//     if a > b:\n+//         a += b\n+//     else:\n+//         a -= b\n+//     return a\n+//\n+// original graph:\n+// graph(%a.1 : Dynamic\n+//       %b : Dynamic) {\n+//   %2 : Dynamic = aten::gt(%a.1, %b)\n+//   %a : Dynamic = prim::If(%2)\n+//     block0() {\n+//       %a.2 : Dynamic = aten::add[alpha={1}](%a.1, %b)\n+//       -> (%a.2)\n+//     }\n+//     block1() {\n+//       %a.3 : Dynamic = aten::sub[alpha={1}](%a.1, %b)\n+//       -> (%a.3)\n+//     }\n+//   return (%a);\n+// }\n+//\n+// transformed graph:\n+// graph(%a_data.1 : Dynamic\n+//       %a_mask.1 : Dynamic\n+//       %a_dims.1 : Dynamic\n+//       %b_data : Dynamic\n+//       %b_mask : Dynamic\n+//       %b_dims : Dynamic) {\n+//   %6 : Dynamic = aten::gt(%a_data.1, %b_data)\n+//   %7 : Dynamic = aten::mul(%a_mask.1, %b_mask)\n+//   %8 : Dynamic = aten::__or__(%a_dims.1, %b_dims)\n+//   %9 : Dynamic = aten::mul(%6, %7)\n+//   %10 : Dynamic = aten::sum(%9)\n+//   %11 : Dynamic = aten::gt[other={0}](%10)  // cond_any\n+//   %a.1 : Dynamic, %17 : Dynamic, %18 : Dynamic = prim::If(%11)\n+//     block0() {\n+//       %data.1 : Dynamic = aten::add[alpha={1}](%a_data.1, %b_data)\n+//       %mask.1 : Dynamic = aten::mul(%a_mask.1, %b_mask)\n+//       %dims.1 : Dynamic = aten::__or__(%a_dims.1, %b_dims)\n+//       %data.2 : Dynamic = aten::where(%mask.1, %data.1, %a_data.1)\n+//       -> (%data.2, %mask.1, %dims.1)\n+//     }\n+//     block1() {\n+//       -> (%a_data.1, %a_mask.1, %a_dims.1)\n+//     }\n+//   %19 : Dynamic = aten::zeros_like(%6)\n+//   %data.3 : Dynamic = aten::eq(%6, %19)\n+//   %21 : Dynamic = aten::mul(%data.3, %7)\n+//   %22 : Dynamic = aten::sum(%21)\n+//   %23 : Dynamic = aten::gt[other={0}](%22)  // else_cond_any\n+//   %a : Dynamic, %29 : Dynamic, %30 : Dynamic = prim::If(%23)\n+//     block0() {\n+//       %data.4 : Dynamic = aten::sub[alpha={1}](%a_data.1, %b_data)\n+//       %mask : Dynamic = aten::mul(%a_mask.1, %b_mask)\n+//       %dims : Dynamic = aten::__or__(%a_dims.1, %b_dims)\n+//       %data : Dynamic = aten::where(%mask, %data.4, %a_data.1)\n+//       -> (%data, %mask, %dims)\n+//     }\n+//     block1() {\n+//       -> (%a_data.1, %a_mask.1, %a_dims.1)\n+//     }\n+//   %res_data : Dynamic = aten::where(%6, %a.1, %a)  // combine results from two if nodes\n+//   %res_mask : Dynamic = aten::where(%6, %17, %29)\n+//   %res_dims : Dynamic = aten::__or__(%18, %30)\n+//   return (%res_data, %res_mask, %res_dims);\n+// }\n+void ToBatch::visitIf(Node* n, Block* block, Block* res_block, std::unordered_map<std::string, Value*>& var_map){\n+  auto res_graph = res_block->owningGraph();\n+\n+  // create prim::If node for res_block\n+  auto add_if_node = [this, &res_block, &res_graph, &n, &var_map](Block* block, std::shared_ptr<Graph> cond_graph, std::vector<Value*> cond, std::vector<Value*> unchanged_outputs){\n+    auto outputs = script::inlineCallTo(*res_block->owningGraph(), *cond_graph, cond); // if condition graph: any/else_any\n+    rn_env[n->input()] = outputs[0];\n+    auto* r_node = res_graph->createClone(n, rn_fn, /*copy_blocks=*/false);\n+    r_node->setStage(n->stage());\n+    res_graph->appendNode(r_node);\n+    auto then_block = r_node->addBlock();\n+    toBatch(block, then_block, var_map);\n+    // variables assigned in then_block will remain the previous value in else_block\n+    auto else_block = r_node->addBlock();\n+    for(Value* output : unchanged_outputs){\n+      else_block->registerOutput(output);\n+    }\n+    // change outputs of prim::If\n+    auto size = r_node->outputs().size();\n+    for(size_t i = 0; i < size; i++){\n+      auto output = r_node->outputs()[i * EXP_BTENSOR_SIZE];\n+      for(size_t j = 1; j < EXP_BTENSOR_SIZE; j++){\n+        r_node->insertOutput(i * EXP_BTENSOR_SIZE + j)->setType(output->type());\n+      }\n+    }\n+    return r_node;\n+  };\n+\n+  auto cond = batch_map.at(n->input());\n+  std::vector<Value*> unchanged_outputs; // used to register outputs in else_block\n+  for(Value* output : n->outputs()){\n+    output = var_map.at(output->getNameBaseSuffix(output->uniqueName())[0]);\n+    for(Value* else_output : batch_map.at(output)){\n+      unchanged_outputs.push_back(else_output);\n+    }\n+  }\n+  auto if_node = add_if_node(n->blocks()[0], batch_operator_table.at(\"any\"), cond, unchanged_outputs);\n+  auto else_node = add_if_node(n->blocks()[1], batch_operator_table.at(\"any_false\"), cond, unchanged_outputs);\n+\n+  // combine results from two if nodes\n+  for(size_t i = 0; i < n->outputs().size(); i++){\n+    std::vector<Value*> inputs(cond);\n+    for(size_t j = 0; j < EXP_BTENSOR_SIZE; j++){\n+      inputs.push_back(if_node->outputs()[i * EXP_BTENSOR_SIZE + j]);\n+    }\n+    for(size_t j = 0; j < EXP_BTENSOR_SIZE; j++){\n+      inputs.push_back(else_node->outputs()[i * EXP_BTENSOR_SIZE + j]);\n+    }\n+    auto outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"where\"), inputs);\n+    batch_map[n->outputs()[i]] = outputs;\n+  }\n+}\n+\n+// prim::Loop transformation:\n+//\n+// transformation example:\n+// @torch.jit.batch(batch_size=4)\n+// def batch_while(a, b):\n+//     while a > b:\n+//         a -= b\n+//     return a\n+//\n+// original graph:\n+// graph(%a.1 : Dynamic\n+//       %b : Dynamic) {\n+//   %2 : int = prim::Constant[value={2147483647}]()\n+//   %3 : Dynamic = aten::gt(%a.1, %b)\n+//   %a : Dynamic = prim::Loop(%2, %3, %a.1)\n+//     block0(%4 : Dynamic, %5 : Dynamic) {\n+//       %a.2 : Dynamic = aten::sub[alpha={1}](%5, %b)\n+//       %9 : Dynamic = aten::gt(%a.2, %b)\n+//       -> (%9, %a.2)\n+//     }\n+//   return (%a);\n+// }\n+//\n+// transformed graph:\n+// graph(%a_data.1 : Dynamic\n+//       %a_mask.1 : Dynamic\n+//       %a_dims.1 : Dynamic\n+//       %b_data : Dynamic\n+//       %b_mask : Dynamic\n+//       %b_dims : Dynamic) {\n+//   %6 : int = prim::Constant[value={2147483647}]()\n+//   %7 : Dynamic = aten::gt(%a_data.1, %b_data)\n+//   %8 : Dynamic = aten::mul(%a_mask.1, %b_mask)\n+//   %9 : Dynamic = aten::__or__(%a_dims.1, %b_dims)\n+//   %10 : Dynamic = aten::mul(%7, %8)\n+//   %11 : Dynamic = aten::sum(%10)\n+//   %12 : Dynamic = aten::gt[other={0}](%11)  // cond_any\n+//   %38 : Dynamic, %39 : Dynamic, %40 : Dynamic, %a : Dynamic, %36 : Dynamic, %37 : Dynamic = prim::Loop(%6, %12, %7, %8, %9, %a_data.1, %a_mask.1, %a_dims.1)\n+//     block0(%4_data : Dynamic, %cond_data : Dynamic, %cond_mask : Dynamic, %cond_dims : Dynamic, %5_data : Dynamic, %5_mask : Dynamic, %5_dims : Dynamic) {\n+//       %data.1 : Dynamic = aten::sub[alpha={1}](%5_data, %b_data)\n+//       %mask : Dynamic = aten::mul(%5_mask, %b_mask)\n+//       %dims : Dynamic = aten::__or__(%5_dims, %b_dims)\n+//       %data : Dynamic = aten::where(%mask, %data.1, %a_data.1)\n+//       %24 : Dynamic = aten::gt(%data, %b_data)  // new cond\n+//       %25 : Dynamic = aten::mul(%mask, %b_mask)\n+//       %26 : Dynamic = aten::__or__(%dims, %b_dims)\n+//       %res_data : Dynamic = aten::where(%cond_data, %data, %5_data) // update outputs\n+//       %res_mask : Dynamic = aten::where(%cond_data, %mask, %5_mask)\n+//       %res_dims : Dynamic = aten::__or__(%dims, %5_dims)\n+//       %33 : Dynamic = aten::mul(%24, %25)\n+//       %34 : Dynamic = aten::sum(%33)\n+//       %35 : Dynamic = aten::gt[other={0}](%34)  // new cond_any\n+//       -> (%35, %24, %25, %26, %res_data, %res_mask, %res_dims)\n+//     }\n+//   return (%a, %36, %37);\n+// }\n+void ToBatch::visitLoop(Node* n, Block* block, Block* res_block, std::unordered_map<std::string, Value*>& var_map){\n+  auto res_graph = res_block->owningGraph();\n+  // bool cond_is_tensor indicates whether cond is tensor\n+  // cond_is_tensor = false, eg: for loop, n->inputs()[1] = byte()\n+  // cond_is_tenspr = true, eg: in some while loop, cond is a batched tensor,\n+  //                            we need to add expanded cond to the inputs of loop node and block,\n+  //                            and compute cond_any as cond for while loop\n+  bool cond_is_tensor = (batch_map.find(n->inputs()[1]) != batch_map.end());\n+  // create prim::Loop node for res_block\n+  if(cond_is_tensor){\n+    auto cond = batch_map.at(n->inputs()[1]);\n+    auto cond_any = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"any\"), cond);\n+    rn_env[n->inputs()[1]] = cond_any[0];\n+  }\n+  for(size_t i = 2; i < n->inputs().size(); i++){\n+    auto input = n->inputs()[i];\n+    rn_env[input] = batch_map.at(input)[0];\n+  }\n+  auto* r_node = res_graph->createClone(n, rn_fn, /*copy_blocks=*/false);\n+\n+  // change inputs of prim::Loop\n+  if(cond_is_tensor){\n+    for(size_t i = 0; i < EXP_BTENSOR_SIZE; i++){\n+      auto cond = batch_map.at(n->inputs()[1]);\n+      r_node->insertInput(i + 2, cond[i]);\n+    }\n+  }\n+  for(size_t i = 2; i < n->inputs().size(); i++){\n+    for(size_t j = 1; j < EXP_BTENSOR_SIZE; j++){\n+      r_node->insertInput((i - 2) * EXP_BTENSOR_SIZE + EXP_BTENSOR_SIZE * cond_is_tensor + 2 + j, batch_map.at(n->inputs()[i])[j]);\n+    }\n+  }\n+  r_node->setStage(n->stage());\n+  res_graph->appendNode(r_node);\n+\n+  // create block for Loop node in res_block\n+  // if cond is tensor:    first 4 inputs of block: cond_any, cond_data, cond_mask, cond_dims\n+  // if cond is not tensor: first 1 input of block: cond\n+  auto loop_block = r_node->addBlock();\n+  toBatch(n->blocks()[0], loop_block, var_map);\n+\n+  // change inputs and outputs of block[0] in prim::Loop\n+  for(size_t i = EXP_BTENSOR_SIZE - 1; i > 0; i--){\n+    loop_block->eraseInput(i);\n+  }\n+  if(cond_is_tensor){\n+    for(size_t i = 0; i < EXP_BTENSOR_SIZE; i++){\n+      loop_block->insertInput(i + 1, \"cond_\" + EXP_BTENSOR_NAME[i]);\n+    }\n+  }\n+\n+  WithInsertPoint guard(loop_block);\n+\n+  // use where operator to update variables and add to outputs\n+  if(cond_is_tensor){\n+    for(size_t i = 0; i < n->outputs().size(); i++){\n+      std::vector<Value*> inputs;\n+      for(size_t j = 0; j < EXP_BTENSOR_SIZE; j++){\n+        inputs.push_back(loop_block->inputs()[j + 1]);\n+      }\n+      auto data = batch_map.at(n->blocks()[0]->outputs()[i + 1]);\n+      inputs.insert(inputs.end(), data.begin(), data.end());\n+      for(size_t j = 0; j < EXP_BTENSOR_SIZE; j++){\n+        inputs.push_back(loop_block->inputs()[i * EXP_BTENSOR_SIZE + j + EXP_BTENSOR_SIZE + 1]);\n+      }\n+      auto outputs = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"where\"), inputs);\n+      batch_map[n->outputs()[i]] = outputs;\n+      for(size_t j = 0; j < EXP_BTENSOR_SIZE; j++){\n+        loop_block->registerOutput(outputs[j]);\n+      }\n+    }\n+  }\n+  // add outputs\n+  else{\n+    for(size_t i = 0; i < n->outputs().size(); i++){\n+      auto outputs = batch_map.at(n->blocks()[0]->outputs()[i + 1]);\n+      for(size_t j = 0; j < EXP_BTENSOR_SIZE; j++){\n+        loop_block->registerOutput(outputs[j]);\n+      }\n+    }\n+  }\n+\n+  // update loop conditions\n+  if(cond_is_tensor){\n+    auto cond = batch_map.at(n->blocks()[0]->outputs()[0]);\n+    auto cond_any = script::inlineCallTo(*res_block->owningGraph(), *batch_operator_table.at(\"any\"), cond);\n+    loop_block->insertOutput(0, cond_any[0]);\n+    for(size_t i = 0; i < EXP_BTENSOR_SIZE; i++){\n+      loop_block->insertOutput(i + 1, cond[i]);\n+    }\n+  }\n+  else{\n+    auto cond = rn_env.at(n->blocks()[0]->outputs()[0]);\n+    loop_block->insertOutput(0, cond);\n+  }\n+\n+  // change outputs of prim::Loop\n+  auto size = r_node->outputs().size();\n+  for(size_t i = 0; i < size; i++){\n+    for(size_t j = 1; j < EXP_BTENSOR_SIZE; j++){\n+      r_node->insertOutput(i * EXP_BTENSOR_SIZE + j);\n+    }\n+    batch_map[n->outputs()[i]] = r_node->outputs().slice(i * EXP_BTENSOR_SIZE, EXP_BTENSOR_SIZE);\n+  }\n+  // add cond to outputs of loop node\n+  if(cond_is_tensor){\n+    for(size_t i = 0; i < EXP_BTENSOR_SIZE; i++){\n+      r_node->insertOutput(i);\n+    }\n+  }\n+}\n+\n+void ToBatch::toBatch(Block* block, Block* res_block, std::unordered_map<std::string, Value*>& upper_var_map) {", "path": "torch/csrc/jit/passes/to_batch.cpp", "position": null, "original_position": 362, "commit_id": "154e4eb8cd13cacd121fe3577831ed0590a1a5d5", "original_commit_id": "d840a83a0f6073fb074fc52b7c89a8887caab06f", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "The string names for Value nodes should be used for _debugging only_. They can be set to absolutely any string by an optimization pass. If you are putting them in a map as a key, this is a strong indication that there is a problem with how the pass is arranged. I suggest trying to change this to map from Value* to Value*, which is much more likely what you want.", "created_at": "2018-07-13T17:13:43Z", "updated_at": "2018-11-23T15:47:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/9392#discussion_r202417181", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202417181"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9392#discussion_r202417181"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9392"}}, "body_html": "<p>The string names for Value nodes should be used for <em>debugging only</em>. They can be set to absolutely any string by an optimization pass. If you are putting them in a map as a key, this is a strong indication that there is a problem with how the pass is arranged. I suggest trying to change this to map from Value* to Value*, which is much more likely what you want.</p>", "body_text": "The string names for Value nodes should be used for debugging only. They can be set to absolutely any string by an optimization pass. If you are putting them in a map as a key, this is a strong indication that there is a problem with how the pass is arranged. I suggest trying to change this to map from Value* to Value*, which is much more likely what you want."}