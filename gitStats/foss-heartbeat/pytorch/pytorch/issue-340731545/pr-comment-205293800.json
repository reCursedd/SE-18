{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205293800", "pull_request_review_id": 140537084, "id": 205293800, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTI5MzgwMA==", "diff_hunk": "@@ -1220,42 +1248,408 @@ def where(c, a, b):\n         res = [torch.where(xs_cond[j], xs[j], xs2[j]) for j in range(4)]\n         self.assertEqual(res, res_batch.examples())\n \n-    def test_lstm_cell(self):\n-        def LSTMCell(x, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n-            i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n-            f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n-            o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n-            # activations\n-            i_t = torch.sigmoid(i_t)\n-            f_t = torch.sigmoid(f_t)\n-            o_t = torch.sigmoid(o_t)\n-            # cell computations\n-            c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n-            c_t = torch.tanh(c_t)\n-            c_t = torch.mul(c, f_t) + torch.mul(i_t, c_t)\n-            h_t = torch.mul(o_t, torch.tanh(c_t))\n-            return h_t\n+    def test_batch_argmax(self):\n+        @torch.jit.batch(batch_size=4)\n+        def argmax(a):\n+            return torch.argmax(a, 1)\n+\n+        xs, batch = self.rand_batch(4, (True, 5), (True, 6))\n+        res_batch = argmax(batch)\n+        res = [torch.argmax(xs[j], 1) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.batch(batch_size=4)\n+        def argmax(a):\n+            return torch.argmax(a, 1, False)\n+\n+        res_batch = argmax(batch)\n+        res = [torch.argmax(xs[j], 1, False) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+    def test_batch_topk(self):\n+        @torch.jit.batch(batch_size=4)\n+        def topk(a):\n+            return torch.topk(a, 3, 1)\n+\n+        xs, batch = self.rand_batch(4, (False, 5), (True, 6))\n+\n+        # along static dim\n+        res_batch = topk(batch)\n+        res = [torch.topk(xs[j], 3, 1)[0] for j in range(4)]\n+        res_idx = [torch.topk(xs[j], 3, 1)[1] for j in range(4)]\n+        self.assertEqual(res, res_batch[0].examples())\n+        self.assertEqual(res_idx, res_batch[1].examples())\n+\n+        @torch.jit.batch(batch_size=4)\n+        def topk(a):\n+            return torch.topk(a, 1, 2)\n+\n+        # along dynamic dim\n+        res_batch = topk(batch)\n+        res = [torch.topk(xs[j], 1, 2)[0] for j in range(4)]\n+        res_idx = [torch.topk(xs[j], 1, 2)[1] for j in range(4)]\n+        self.assertEqual(res, res_batch[0].examples())\n+        self.assertEqual(res_idx, res_batch[1].examples())\n+\n+    def test_batch_softmax(self):\n+        @torch.jit.batch(batch_size=4)\n+        def softmax(a):\n+            return torch.softmax(a, 1)\n+\n+        xs, batch = self.rand_batch(4, (False, 5), (True, 6))\n+\n+        # along static dim\n+        res_batch = softmax(batch)\n+        res = [torch.softmax(xs[j], 1) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.batch(batch_size=4)\n+        def softmax(a):\n+            return torch.softmax(a, 2)\n+\n+        # along dynamic dim\n+        res_batch = softmax(batch)\n+        res = [torch.softmax(xs[j], 2) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+    def test_batch_view(self):\n+        @torch.jit.batch(batch_size=4)\n+        def view(a):\n+            return a.view([4, -1, 3])\n+\n+        xs, batch = self.rand_batch(4, (True, 5), (False, 3))\n+        res_batch = view(batch)\n+        res = [xs[j].view([1, -1, 3]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+    def test_batch_cat(self):\n+        @torch.jit.batch(batch_size=4)\n+        def cat2(a, b):\n+            return torch.cat([a, b], 2)\n+\n+        xs, batch = self.rand_batch(4, (True, 5), (False, 3))\n+        xs2, batch2 = xs, batch\n+        res_batch = cat2(batch, batch2)\n+        res = [torch.cat([xs[j], xs2[j]], 2) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+    def test_batch_sum(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_sum(a):\n+            return a.sum()\n+\n+        xs, batch = self.rand_batch(4, (True, 5), (False, 3))\n+        res_batch = batch_sum(batch)\n+        res = [xs[j].sum().unsqueeze(0) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+    def test_if_else(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_if(a, b):\n+            if a > b:\n+                a = a + b\n+            else:\n+                a = a - b\n+            return a\n+\n+        def single_if(a, b):\n+            if a > b:\n+                a = a + b\n+            else:\n+                a = a - b\n+            return a\n+\n+        a, batch_a = self.rand_batch(4, ())\n+        b, batch_b = self.rand_batch(4, ())\n+        res_batch = batch_if(batch_a, batch_b)\n+        res = [single_if(a[j], b[j]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.script\n+        def batch_if(a, b):\n+            if a > b:\n+                a = a + b\n+            else:\n+                a = a - b\n+            return a\n+\n+        graph = torch.to_batch_graph(batch_if.graph)\n+        self.assertExpected(str(graph))\n \n+    def test_if_else_with_scalar(self):\n         @torch.jit.batch(batch_size=4)\n-        def LSTMCell_batch(x, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n-            i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n-            f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n-            o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n-            # activations\n-            i_t = torch.sigmoid(i_t)\n-            f_t = torch.sigmoid(f_t)\n-            o_t = torch.sigmoid(o_t)\n-            # cell computations\n-            c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n-            c_t = torch.tanh(c_t)\n-            c_t = torch.mul(c, f_t) + torch.mul(i_t, c_t)\n-            h_t = torch.mul(o_t, torch.tanh(c_t))\n-            return h_t\n+        def batch_if(a, b):\n+            if a > 0.1:\n+                a = a + b\n+            else:\n+                a = a - b\n+            return a\n+\n+        def single_if(a, b):\n+            if a > 0.1:\n+                a = a + b\n+            else:\n+                a = a - b\n+            return a\n+\n+        a, batch_a = self.rand_batch(4, ())\n+        b, batch_b = self.rand_batch(4, ())\n+        res_batch = batch_if(batch_a, batch_b)\n+        res = [single_if(a[j], b[j]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.script\n+        def batch_if(a, b):\n+            if a > 0.1:\n+                a = a + b\n+            else:\n+                a = a - b\n+            return a\n+\n+        graph = torch.to_batch_graph(batch_if.graph)\n+        self.assertExpected(str(graph))\n+\n+    def test_if_noelse(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_if(a, b):\n+            if a > b:\n+                a = a + b\n+            return a\n+\n+        def single_if(a, b):\n+            if a > b:\n+                a = a + b\n+            return a\n+\n+        a, batch_a = self.rand_batch(4, ())\n+        b, batch_b = self.rand_batch(4, ())\n+        res_batch = batch_if(batch_a, batch_b)\n+        res = [single_if(a[j], b[j]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.script\n+        def batch_if(a, b):\n+            if a > b:\n+                a = a + b\n+            return a\n+\n+        graph = torch.to_batch_graph(batch_if.graph)\n+        self.assertExpected(str(graph))\n+\n+    def test_if_noelse_with_scalar(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_if(a, b):\n+            if a > 0.1:\n+                a = a + b\n+            return a\n+\n+        def single_if(a, b):\n+            if a > 0.1:\n+                a = a + b\n+            return a\n+\n+        a, batch_a = self.rand_batch(4, ())\n+        b, batch_b = self.rand_batch(4, ())\n+        res_batch = batch_if(batch_a, batch_b)\n+        res = [single_if(a[j], b[j]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.script\n+        def batch_if(a, b):\n+            if a > 0.1:\n+                a = a + b\n+            return a\n+\n+        graph = torch.to_batch_graph(batch_if.graph)\n+        self.assertExpected(str(graph))\n+\n+    def test_while(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_while(a, b):\n+            while a > b:\n+                a = a - b\n+            return a\n+\n+        def single_while(a, b):\n+            while a > b:\n+                a = a - b\n+            return a\n+\n+        a, batch_a = self.rand_batch(4, ())\n+        b = [torch.abs(torch.rand(1)) for i in range(4)]\n+        batch_b = BatchTensor(b, torch.tensor([]).byte())\n+        res_batch = batch_while(batch_a, batch_b)\n+        res = [single_while(a[j], b[j]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.script\n+        def batch_while(a, b):\n+            while a > b:\n+                a = a - b\n+            return a\n+\n+        graph = torch.to_batch_graph(batch_while.graph)\n+        self.assertExpected(str(graph))\n+\n+    def test_for(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_for(x, y):\n+            for _ in range(10):\n+                x = x + y\n+            return x\n+\n+        def single_for(x, y):\n+            for _ in range(10):\n+                x = x + y\n+            return x\n+\n+        a, batch_a = self.rand_batch(4, ())\n+        b, batch_b = self.rand_batch(4, ())\n+        res_batch = batch_for(batch_a, batch_b)\n+        res = [single_for(a[j], b[j]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n+\n+        @torch.jit.script\n+        def batch_for(x, y):\n+            for _ in range(10):\n+                x = x + y\n+            return x\n+\n+        graph = torch.to_batch_graph(batch_for.graph)\n+        self.assertExpected(str(graph))\n+\n+    def test_lstm(self):\n+        def LSTM(x_all, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n+            for i in range(x_all.size(1)):\n+                x = x_all.select(1, i)\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+            return h\n+\n+        @torch.jit.batch(batch_size=4)\n+        def LSTM_batch(x_all, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n+            for i in range(x_all.size(1)):\n+                x = x_all.select(1, i)\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+                # h = x\n+            return h\n \n         batch_size, input_size, hidden_size = 4, 3, 2\n+        xs, batch = self.rand_batch(batch_size, (True, 4), (False, input_size))\n+        hx, h_batch = self.rand_batch(batch_size, (False, hidden_size))\n+        cx, c_batch = self.rand_batch(batch_size, (False, hidden_size))\n+\n+        # input to hidden weights\n+        w_xi = torch.rand(input_size, hidden_size)\n+        w_xf = torch.rand(input_size, hidden_size)\n+        w_xo = torch.rand(input_size, hidden_size)\n+        w_xc = torch.rand(input_size, hidden_size)\n+        # hidden to hidden weights\n+        w_hi = torch.rand(hidden_size, hidden_size)\n+        w_hf = torch.rand(hidden_size, hidden_size)\n+        w_ho = torch.rand(hidden_size, hidden_size)\n+        w_hc = torch.rand(hidden_size, hidden_size)\n+        # bias terms\n+        b_i = torch.rand(hidden_size)\n+        b_f = torch.rand(hidden_size)\n+        b_o = torch.rand(hidden_size)\n+        b_c = torch.rand(hidden_size)\n+\n+        ys = [LSTM(xs[j], hx[j], cx[j], w_xi, w_xf, w_xo, w_xc,\n+                   w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c) for j in range(batch_size)]\n+        ybs = LSTM_batch(batch, h_batch, c_batch, w_xi, w_xf, w_xo, w_xc,\n+                         w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c)\n+        self.assertEqual(ys, ybs.examples())\n+\n+    def test_greedy_search(self):\n+        def greedy(x, h, c, embed, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc,\n+                   b_i, b_f, b_o, b_c, w_hs, b_s, iter_num):\n+            iter_count = torch.zeros_like(iter_num)\n+            while(iter_count < iter_num):", "path": "test/test_jit.py", "position": 368, "original_position": 421, "commit_id": "154e4eb8cd13cacd121fe3577831ed0590a1a5d5", "original_commit_id": "293895b64826f9c29bdf93f29ce2da7c7a2a628f", "user": {"login": "ChunliF", "id": 36351432, "node_id": "MDQ6VXNlcjM2MzUxNDMy", "avatar_url": "https://avatars0.githubusercontent.com/u/36351432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChunliF", "html_url": "https://github.com/ChunliF", "followers_url": "https://api.github.com/users/ChunliF/followers", "following_url": "https://api.github.com/users/ChunliF/following{/other_user}", "gists_url": "https://api.github.com/users/ChunliF/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChunliF/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChunliF/subscriptions", "organizations_url": "https://api.github.com/users/ChunliF/orgs", "repos_url": "https://api.github.com/users/ChunliF/repos", "events_url": "https://api.github.com/users/ChunliF/events{/privacy}", "received_events_url": "https://api.github.com/users/ChunliF/received_events", "type": "User", "site_admin": false}, "body": "`iter_num` is a tensor of size (batch_size,). Different example has different iterations. In real greedy search algorithm, iteration will end when <eos> has the highest probability. In this test case, all inputs are just random tensors. So I use `iter_num` to simulate this process.", "created_at": "2018-07-25T23:40:29Z", "updated_at": "2018-11-23T15:48:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/9392#discussion_r205293800", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205293800"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9392#discussion_r205293800"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9392"}}, "body_html": "<p><code>iter_num</code> is a tensor of size (batch_size,). Different example has different iterations. In real greedy search algorithm, iteration will end when  has the highest probability. In this test case, all inputs are just random tensors. So I use <code>iter_num</code> to simulate this process.</p>", "body_text": "iter_num is a tensor of size (batch_size,). Different example has different iterations. In real greedy search algorithm, iteration will end when  has the highest probability. In this test case, all inputs are just random tensors. So I use iter_num to simulate this process.", "in_reply_to_id": 205290195}