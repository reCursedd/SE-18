{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205295298", "pull_request_review_id": 140538758, "id": 205295298, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTI5NTI5OA==", "diff_hunk": "@@ -1299,110 +1312,51 @@ def softmax(a):\n         res = [torch.softmax(xs[j], 2) for j in range(4)]\n         self.assertEqual(res, res_batch.examples())\n \n-    def test_lstm(self):\n-        def LSTM(x_all, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n-            for i in range(x_all.size(1)):\n-                x = x_all.select(1, i)\n-                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n-                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n-                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n-                # activations\n-                i_t = torch.sigmoid(i_t)\n-                f_t = torch.sigmoid(f_t)\n-                o_t = torch.sigmoid(o_t)\n-                # cell computations\n-                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n-                c_t = torch.tanh(c_t)\n-                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n-                h_t = torch.mul(o_t, torch.tanh(c_t))\n-                h = h_t\n-                c = c_t\n-            return h\n-\n+    def test_batch_view(self):\n         @torch.jit.batch(batch_size=4)\n-        def LSTM_batch(x_all, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n-            for i in range(x_all.size(1)):\n-                x = x_all.select(1, i)\n-                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n-                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n-                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n-                # activations\n-                i_t = torch.sigmoid(i_t)\n-                f_t = torch.sigmoid(f_t)\n-                o_t = torch.sigmoid(o_t)\n-                # cell computations\n-                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n-                c_t = torch.tanh(c_t)\n-                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n-                h_t = torch.mul(o_t, torch.tanh(c_t))\n-                h = h_t\n-                c = c_t\n-            return h\n-\n-        batch_size, input_size, hidden_size = 4, 3, 2\n-        xs, batch = self.rand_batch(batch_size, (True, 4), (False, input_size))\n-        hx, h_batch = self.rand_batch(batch_size, (False, hidden_size))\n-        cx, c_batch = self.rand_batch(batch_size, (False, hidden_size))\n+        def view(a):\n+            return a.view([4, -1, 3])\n \n-        # input to hidden weights\n-        w_xi = torch.rand(input_size, hidden_size)\n-        w_xf = torch.rand(input_size, hidden_size)\n-        w_xo = torch.rand(input_size, hidden_size)\n-        w_xc = torch.rand(input_size, hidden_size)\n-        # hidden to hidden weights\n-        w_hi = torch.rand(hidden_size, hidden_size)\n-        w_hf = torch.rand(hidden_size, hidden_size)\n-        w_ho = torch.rand(hidden_size, hidden_size)\n-        w_hc = torch.rand(hidden_size, hidden_size)\n-        # bias terms\n-        b_i = torch.rand(hidden_size)\n-        b_f = torch.rand(hidden_size)\n-        b_o = torch.rand(hidden_size)\n-        b_c = torch.rand(hidden_size)\n-\n-        ys = [LSTM(xs[j], hx[j], cx[j], w_xi, w_xf, w_xo, w_xc,\n-                   w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c) for j in range(batch_size)]\n-        ybs = LSTM_batch(batch, h_batch, c_batch, w_xi, w_xf, w_xo, w_xc,\n-                         w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c)\n-        self.assertEqual(ys, ybs.examples())\n+        xs, batch = self.rand_batch(4, (True, 5), (False, 3))\n+        res_batch = view(batch)\n+        res = [xs[j].view([1, -1, 3]) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n \n-    def test_numToTensor(self):\n+    def test_batch_cat(self):\n         @torch.jit.batch(batch_size=4)\n-        def batch_numToTensor(a):\n-            a = a + 1\n-            return a\n+        def cat2(a, b):\n+            return torch.cat([a, b], 2)\n \n-        def single_numToTensor(a):\n-            a = a + 1\n-            return a\n-\n-        a, batch = self.rand_batch(4, ())\n-        res_batch = batch_numToTensor(batch)\n-        res = [single_numToTensor(a[j]) for j in range(4)]\n+        xs, batch = self.rand_batch(4, (True, 5), (False, 3))\n+        xs2, batch2 = xs, batch\n+        res_batch = cat2(batch, batch2)\n+        res = [torch.cat([xs[j], xs2[j]], 2) for j in range(4)]\n         self.assertEqual(res, res_batch.examples())\n \n-        @torch.jit.script\n-        def batch_numToTensor(a):\n-            a = a + 1\n-            return a\n+    def test_batch_sum(self):\n+        @torch.jit.batch(batch_size=4)\n+        def batch_sum(a):\n+            return a.sum()\n \n-        graph = torch.to_batch_graph(batch_numToTensor.graph)\n-        self.assertExpected(str(graph))\n+        xs, batch = self.rand_batch(4, (True, 5), (False, 3))\n+        res_batch = batch_sum(batch)\n+        res = [xs[j].sum().unsqueeze(0) for j in range(4)]\n+        self.assertEqual(res, res_batch.examples())\n \n     def test_if_else(self):\n         @torch.jit.batch(batch_size=4)\n         def batch_if(a, b):\n             if a > b:\n-                a += b\n+                a = a + b", "path": "test/test_jit.py", "position": null, "original_position": 150, "commit_id": "154e4eb8cd13cacd121fe3577831ed0590a1a5d5", "original_commit_id": "be41bd23b8eb04b78d9dfca27b1c24b5cc620163", "user": {"login": "ChunliF", "id": 36351432, "node_id": "MDQ6VXNlcjM2MzUxNDMy", "avatar_url": "https://avatars0.githubusercontent.com/u/36351432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChunliF", "html_url": "https://github.com/ChunliF", "followers_url": "https://api.github.com/users/ChunliF/followers", "following_url": "https://api.github.com/users/ChunliF/following{/other_user}", "gists_url": "https://api.github.com/users/ChunliF/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChunliF/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChunliF/subscriptions", "organizations_url": "https://api.github.com/users/ChunliF/orgs", "repos_url": "https://api.github.com/users/ChunliF/repos", "events_url": "https://api.github.com/users/ChunliF/events{/privacy}", "received_events_url": "https://api.github.com/users/ChunliF/received_events", "type": "User", "site_admin": false}, "body": "After adding `requires_grad=True` when creating input tensors, there is a runtime error \"RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\"", "created_at": "2018-07-25T23:50:04Z", "updated_at": "2018-11-23T15:48:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/9392#discussion_r205295298", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205295298"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9392#discussion_r205295298"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9392"}}, "body_html": "<p>After adding <code>requires_grad=True</code> when creating input tensors, there is a runtime error \"RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\"</p>", "body_text": "After adding requires_grad=True when creating input tensors, there is a runtime error \"RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\"", "in_reply_to_id": 205291739}