{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205292914", "pull_request_review_id": 140532318, "id": 205292914, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTI5MjkxNA==", "diff_hunk": "@@ -1557,13 +1511,291 @@ def single_for(x, y):\n \n         @torch.jit.script\n         def batch_for(x, y):\n-            for _i in range(10):\n-                x += y\n+            for _ in range(10):\n+                x = x + y\n             return x\n \n         graph = torch.to_batch_graph(batch_for.graph)\n         self.assertExpected(str(graph))\n \n+    def test_lstm(self):\n+        def LSTM(x_all, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n+            for i in range(x_all.size(1)):\n+                x = x_all.select(1, i)\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+            return h\n+\n+        @torch.jit.batch(batch_size=4)\n+        def LSTM_batch(x_all, h, c, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c):\n+            for i in range(x_all.size(1)):\n+                x = x_all.select(1, i)\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+                # h = x\n+            return h\n+\n+        batch_size, input_size, hidden_size = 4, 3, 2\n+        xs, batch = self.rand_batch(batch_size, (True, 4), (False, input_size))\n+        hx, h_batch = self.rand_batch(batch_size, (False, hidden_size))\n+        cx, c_batch = self.rand_batch(batch_size, (False, hidden_size))\n+\n+        # input to hidden weights\n+        w_xi = torch.rand(input_size, hidden_size)\n+        w_xf = torch.rand(input_size, hidden_size)\n+        w_xo = torch.rand(input_size, hidden_size)\n+        w_xc = torch.rand(input_size, hidden_size)\n+        # hidden to hidden weights\n+        w_hi = torch.rand(hidden_size, hidden_size)\n+        w_hf = torch.rand(hidden_size, hidden_size)\n+        w_ho = torch.rand(hidden_size, hidden_size)\n+        w_hc = torch.rand(hidden_size, hidden_size)\n+        # bias terms\n+        b_i = torch.rand(hidden_size)\n+        b_f = torch.rand(hidden_size)\n+        b_o = torch.rand(hidden_size)\n+        b_c = torch.rand(hidden_size)\n+\n+        ys = [LSTM(xs[j], hx[j], cx[j], w_xi, w_xf, w_xo, w_xc,\n+                   w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c) for j in range(batch_size)]\n+        ybs = LSTM_batch(batch, h_batch, c_batch, w_xi, w_xf, w_xo, w_xc,\n+                         w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c)\n+        self.assertEqual(ys, ybs.examples())\n+\n+    def test_greedy_search(self):\n+        def greedy(x, h, c, embed, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc,\n+                   b_i, b_f, b_o, b_c, w_hs, b_s, iter_num):\n+            iter_count = torch.zeros_like(iter_num)\n+            while(iter_count < iter_num):\n+                iter_count += 1\n+                # LSTM Cell\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+                # calculate feature with max probability\n+                s_t = torch.matmul(h_t, w_hs) + b_s\n+                p_t = torch.softmax(s_t, 1)\n+                # print(p_t)\n+                i_t = torch.argmax(p_t, 1)\n+                x = embed.index_select(0, i_t)\n+            return h\n+\n+        @torch.jit.batch(batch_size=4)\n+        def greedy_batch(x, h, c, embed, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc,\n+                         b_i, b_f, b_o, b_c, w_hs, b_s, iter_num):\n+            iter_count = torch.zeros_like(iter_num)\n+            while(iter_count < iter_num):\n+                iter_count += 1\n+                # LSTM Cell\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+                # calculate feature with max probability\n+                s_t = torch.matmul(h_t, w_hs) + b_s\n+                p_t = torch.softmax(s_t, 1)\n+                i_t = torch.argmax(p_t, 1)\n+                x = embed.index_select(1, i_t).squeeze(1)\n+            return h\n+\n+        batch_size, input_size, hidden_size, vocab_size = 4, 6, 8, 7\n+        xs, batch = self.rand_batch(batch_size, (False, input_size))\n+        hx, h_batch = self.rand_batch(batch_size, (False, hidden_size))\n+        cx, c_batch = self.rand_batch(batch_size, (False, hidden_size))\n+        embed, embed_batch = self.rand_batch(batch_size, (False, vocab_size), (False, input_size))\n+        iter_num = [torch.randint(2, 5, (1,)) for i in range(batch_size)]\n+        iter_num_batch = BatchTensor(iter_num, torch.tensor([]).byte())\n+\n+        # input to hidden weights\n+        w_xi = torch.rand(input_size, hidden_size)\n+        w_xf = torch.rand(input_size, hidden_size)\n+        w_xo = torch.rand(input_size, hidden_size)\n+        w_xc = torch.rand(input_size, hidden_size)\n+        # hidden to hidden weights\n+        w_hi = torch.rand(hidden_size, hidden_size)\n+        w_hf = torch.rand(hidden_size, hidden_size)\n+        w_ho = torch.rand(hidden_size, hidden_size)\n+        w_hc = torch.rand(hidden_size, hidden_size)\n+        # bias terms\n+        b_i = torch.rand(hidden_size)\n+        b_f = torch.rand(hidden_size)\n+        b_o = torch.rand(hidden_size)\n+        b_c = torch.rand(hidden_size)\n+        # hidden to vocab weights, bias\n+        w_hs = torch.rand(hidden_size, vocab_size)\n+        b_s = torch.rand(vocab_size)\n+\n+        ys = [greedy(xs[j], hx[j], cx[j], embed[j].squeeze(0), w_xi, w_xf, w_xo, w_xc,\n+                     w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c, w_hs, b_s, iter_num[j]) for j in range(batch_size)]\n+        ybs = greedy_batch(batch, h_batch, c_batch, embed_batch, w_xi, w_xf, w_xo, w_xc,\n+                           w_hi, w_hf, w_ho, w_hc, b_i, b_f, b_o, b_c, w_hs, b_s, iter_num_batch)\n+        self.assertEqual(ys, ybs.examples())\n+\n+    def test_beam_search(self):\n+        def beam(x, h, c, embed, w_xi, w_xf, w_xo, w_xc, w_hi, w_hf, w_ho, w_hc,\n+                 b_i, b_f, b_o, b_c, w_hs, b_s, iter_num, idx):\n+            k = 5\n+            vocab_size = embed.size(0)\n+            iter_count = torch.zeros_like(iter_num)\n+            max_len = idx.size(2)\n+            while(iter_count < iter_num):\n+                iter_count += 1\n+                # LSTM Cell\n+                i_t = torch.matmul(x, w_xi) + torch.matmul(h, w_hi) + b_i\n+                f_t = torch.matmul(x, w_xf) + torch.matmul(h, w_hf) + b_f\n+                o_t = torch.matmul(x, w_xo) + torch.matmul(h, w_ho) + b_o\n+                # activations\n+                i_t = torch.sigmoid(i_t)\n+                f_t = torch.sigmoid(f_t)\n+                o_t = torch.sigmoid(o_t)\n+                # cell computations\n+                c_t = torch.matmul(x, w_xc) + torch.matmul(h, w_hc) + b_c\n+                c_t = torch.tanh(c_t)\n+                c_t = torch.mul(c_t, f_t) + torch.mul(i_t, c_t)\n+                h_t = torch.mul(o_t, torch.tanh(c_t))\n+                h = h_t\n+                c = c_t\n+                # calculate features with max probability\n+                s_t = torch.matmul(h_t, w_hs) + b_s\n+                s_t = s_t.view([1, -1])\n+                p_t = torch.softmax(s_t, 1)\n+                # print(p_t)\n+                prob_t, i_t = torch.topk(p_t, k, 1)", "path": "test/test_jit.py", "position": null, "original_position": 501, "commit_id": "154e4eb8cd13cacd121fe3577831ed0590a1a5d5", "original_commit_id": "be41bd23b8eb04b78d9dfca27b1c24b5cc620163", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "body": "You're reassigning `i_t` here and it has a different type as before. I was super confused because I thought i_t was the I gate and you seemed to be doing index calculations with it!", "created_at": "2018-07-25T23:34:24Z", "updated_at": "2018-11-23T15:48:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/9392#discussion_r205292914", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205292914"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9392#discussion_r205292914"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9392"}}, "body_html": "<p>You're reassigning <code>i_t</code> here and it has a different type as before. I was super confused because I thought i_t was the I gate and you seemed to be doing index calculations with it!</p>", "body_text": "You're reassigning i_t here and it has a different type as before. I was super confused because I thought i_t was the I gate and you seemed to be doing index calculations with it!"}