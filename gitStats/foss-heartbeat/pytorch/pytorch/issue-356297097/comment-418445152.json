{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/418445152", "html_url": "https://github.com/pytorch/pytorch/issues/11186#issuecomment-418445152", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11186", "id": 418445152, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODQ0NTE1Mg==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-04T17:06:44Z", "updated_at": "2018-09-04T17:06:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm at <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/0d5e4a2c66816e42d084f23ee94575357460cdf8/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/0d5e4a2c66816e42d084f23ee94575357460cdf8\"><tt>0d5e4a2</tt></a>, and using torch.jit.trace</p>\n<pre><code>import torch\n\ndef constant_mul_add(x, y):\n    return (2 * x + 3 + y) * 4\n\n\nx = torch.Tensor(10240, 1024).cuda().uniform_()\ny = torch.Tensor(10240, 1024).cuda().uniform_()\ngO = torch.Tensor(10240, 1024).cuda().uniform_()\n#x.requires_grad = True\n#y.requires_grad = True\ntraced_fn1 = torch.jit.trace(constant_mul_add, (x,y), check_trace = False)\nprint(traced_fn1.graph)\nout = traced_fn1(x,y)\n#out.backward(gO)\nprint(traced_fn1.graph_for(x,y))\n</code></pre>\n<p>outputs</p>\n<pre><code>graph(%0 : Float(10240, 1024)\n      %1 : Float(10240, 1024)) {\n  %2 : Long() = prim::Constant[value={2}]()\n  %3 : Float(10240, 1024) = aten::mul(%0, %2)\n  %4 : Long() = prim::Constant[value={3}]()\n  %5 : int = prim::Constant[value=1]()\n  %6 : Float(10240, 1024) = aten::add(%3, %4, %5)\n  %7 : int = prim::Constant[value=1]()\n  %8 : Float(10240, 1024) = aten::add(%6, %1, %7)\n  %9 : Long() = prim::Constant[value={4}]()\n  %10 : Float(10240, 1024) = aten::mul(%8, %9)\n  return (%10);\n}\n\ngraph(%0 : Float(*, *)\n      %1 : Float(*, *)) {\n  %2 : Long() = prim::Constant[value={2}]()\n  %3 : Float(*, *) = aten::mul(%0, %2)\n  %4 : Long() = prim::Constant[value={3}]()\n  %5 : int = prim::Constant[value=1]()\n  %6 : Float(*, *) = aten::add(%3, %4, %5)\n  %8 : Float(*, *) = aten::add(%6, %1, %5)\n  %9 : Long() = prim::Constant[value={4}]()\n  %10 : Float(*, *) = aten::mul(%8, %9)\n  return (%10);\n}\n</code></pre>", "body_text": "I'm at 0d5e4a2, and using torch.jit.trace\nimport torch\n\ndef constant_mul_add(x, y):\n    return (2 * x + 3 + y) * 4\n\n\nx = torch.Tensor(10240, 1024).cuda().uniform_()\ny = torch.Tensor(10240, 1024).cuda().uniform_()\ngO = torch.Tensor(10240, 1024).cuda().uniform_()\n#x.requires_grad = True\n#y.requires_grad = True\ntraced_fn1 = torch.jit.trace(constant_mul_add, (x,y), check_trace = False)\nprint(traced_fn1.graph)\nout = traced_fn1(x,y)\n#out.backward(gO)\nprint(traced_fn1.graph_for(x,y))\n\noutputs\ngraph(%0 : Float(10240, 1024)\n      %1 : Float(10240, 1024)) {\n  %2 : Long() = prim::Constant[value={2}]()\n  %3 : Float(10240, 1024) = aten::mul(%0, %2)\n  %4 : Long() = prim::Constant[value={3}]()\n  %5 : int = prim::Constant[value=1]()\n  %6 : Float(10240, 1024) = aten::add(%3, %4, %5)\n  %7 : int = prim::Constant[value=1]()\n  %8 : Float(10240, 1024) = aten::add(%6, %1, %7)\n  %9 : Long() = prim::Constant[value={4}]()\n  %10 : Float(10240, 1024) = aten::mul(%8, %9)\n  return (%10);\n}\n\ngraph(%0 : Float(*, *)\n      %1 : Float(*, *)) {\n  %2 : Long() = prim::Constant[value={2}]()\n  %3 : Float(*, *) = aten::mul(%0, %2)\n  %4 : Long() = prim::Constant[value={3}]()\n  %5 : int = prim::Constant[value=1]()\n  %6 : Float(*, *) = aten::add(%3, %4, %5)\n  %8 : Float(*, *) = aten::add(%6, %1, %5)\n  %9 : Long() = prim::Constant[value={4}]()\n  %10 : Float(*, *) = aten::mul(%8, %9)\n  return (%10);\n}", "body": "I'm at 0d5e4a2c6, and using torch.jit.trace\r\n```\r\nimport torch\r\n\r\ndef constant_mul_add(x, y):\r\n    return (2 * x + 3 + y) * 4\r\n\r\n\r\nx = torch.Tensor(10240, 1024).cuda().uniform_()\r\ny = torch.Tensor(10240, 1024).cuda().uniform_()\r\ngO = torch.Tensor(10240, 1024).cuda().uniform_()\r\n#x.requires_grad = True\r\n#y.requires_grad = True\r\ntraced_fn1 = torch.jit.trace(constant_mul_add, (x,y), check_trace = False)\r\nprint(traced_fn1.graph)\r\nout = traced_fn1(x,y)\r\n#out.backward(gO)\r\nprint(traced_fn1.graph_for(x,y))\r\n```\r\noutputs \r\n```\r\ngraph(%0 : Float(10240, 1024)\r\n      %1 : Float(10240, 1024)) {\r\n  %2 : Long() = prim::Constant[value={2}]()\r\n  %3 : Float(10240, 1024) = aten::mul(%0, %2)\r\n  %4 : Long() = prim::Constant[value={3}]()\r\n  %5 : int = prim::Constant[value=1]()\r\n  %6 : Float(10240, 1024) = aten::add(%3, %4, %5)\r\n  %7 : int = prim::Constant[value=1]()\r\n  %8 : Float(10240, 1024) = aten::add(%6, %1, %7)\r\n  %9 : Long() = prim::Constant[value={4}]()\r\n  %10 : Float(10240, 1024) = aten::mul(%8, %9)\r\n  return (%10);\r\n}\r\n\r\ngraph(%0 : Float(*, *)\r\n      %1 : Float(*, *)) {\r\n  %2 : Long() = prim::Constant[value={2}]()\r\n  %3 : Float(*, *) = aten::mul(%0, %2)\r\n  %4 : Long() = prim::Constant[value={3}]()\r\n  %5 : int = prim::Constant[value=1]()\r\n  %6 : Float(*, *) = aten::add(%3, %4, %5)\r\n  %8 : Float(*, *) = aten::add(%6, %1, %5)\r\n  %9 : Long() = prim::Constant[value={4}]()\r\n  %10 : Float(*, *) = aten::mul(%8, %9)\r\n  return (%10);\r\n}\r\n```"}