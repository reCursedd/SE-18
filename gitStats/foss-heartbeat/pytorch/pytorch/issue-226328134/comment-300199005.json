{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/300199005", "html_url": "https://github.com/pytorch/pytorch/pull/1471#issuecomment-300199005", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1471", "id": 300199005, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDE5OTAwNQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T15:20:56Z", "updated_at": "2017-05-09T15:20:56Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I'm not sure what did it buy you - you have as many _sparse_select functions defined as many _sparse_masks you'd need</p>\n</blockquote>\n<p>Sorry, I wasn't clear about how the savings work.</p>\n<p>Recall that the type of <code>sparse_mask</code>: <code>THSTensor sparseMask(THTensor dense, THSTensor mask)</code>. Via our macro magic, this means that I get one implementation of sparseMask per tensor type, O(n) in total; e.g., I have:</p>\n<ul>\n<li><code>THSFloatTensor sparseMask(THFloatTensor dense, THSFloatTensor mask)</code></li>\n<li><code>THSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)</code></li>\n<li>etc</li>\n</ul>\n<p>Now, the problem is that I want to be able to call sparseMask with a Float dense tensor, but a, say, Long sparse mask. Intuitively, there's no problem with this sort of operation, since at the end of the day we only care about the indices of the sparse tensor. However, this means that if I do this with macros, I end up with O(n^2) copies of sparseMask:</p>\n<ul>\n<li><code>THSFloatTensor sparseMask(THFloatTensor dense, THSFloatTensor mask)</code></li>\n<li><code>THSFloatTensor sparseMask(THFloatTensor dense, THSLongTensor mask)</code></li>\n<li><code>THSFloatTensor sparseMask(THFloatTensor dense, ... mask)</code></li>\n<li><code>THSFloatTensor sparseMask(THLongTensor dense, THSFloatTensor mask)</code></li>\n<li><code>THSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)</code></li>\n<li><code>THSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)</code></li>\n<li><code>THSLongTensor sparseMask(THLongTensor dense, ... mask)</code></li>\n<li>etc</li>\n</ul>\n<p>In this patch, I get polymorphic sparseMask, but with only O(n) functions, since they accept the index tensor (of which there's only one type.) That's the benefit.</p>", "body_text": "I'm not sure what did it buy you - you have as many _sparse_select functions defined as many _sparse_masks you'd need\n\nSorry, I wasn't clear about how the savings work.\nRecall that the type of sparse_mask: THSTensor sparseMask(THTensor dense, THSTensor mask). Via our macro magic, this means that I get one implementation of sparseMask per tensor type, O(n) in total; e.g., I have:\n\nTHSFloatTensor sparseMask(THFloatTensor dense, THSFloatTensor mask)\nTHSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)\netc\n\nNow, the problem is that I want to be able to call sparseMask with a Float dense tensor, but a, say, Long sparse mask. Intuitively, there's no problem with this sort of operation, since at the end of the day we only care about the indices of the sparse tensor. However, this means that if I do this with macros, I end up with O(n^2) copies of sparseMask:\n\nTHSFloatTensor sparseMask(THFloatTensor dense, THSFloatTensor mask)\nTHSFloatTensor sparseMask(THFloatTensor dense, THSLongTensor mask)\nTHSFloatTensor sparseMask(THFloatTensor dense, ... mask)\nTHSFloatTensor sparseMask(THLongTensor dense, THSFloatTensor mask)\nTHSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)\nTHSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)\nTHSLongTensor sparseMask(THLongTensor dense, ... mask)\netc\n\nIn this patch, I get polymorphic sparseMask, but with only O(n) functions, since they accept the index tensor (of which there's only one type.) That's the benefit.", "body": "> I'm not sure what did it buy you - you have as many _sparse_select functions defined as many _sparse_masks you'd need\r\n\r\nSorry, I wasn't clear about how the savings work.\r\n\r\nRecall that the type of `sparse_mask`: `THSTensor sparseMask(THTensor dense, THSTensor mask)`. Via our macro magic, this means that I get one implementation of sparseMask per tensor type, O(n) in total; e.g., I have:\r\n\r\n* `THSFloatTensor sparseMask(THFloatTensor dense, THSFloatTensor mask)`\r\n* `THSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)`\r\n* etc\r\n\r\nNow, the problem is that I want to be able to call sparseMask with a Float dense tensor, but a, say, Long sparse mask. Intuitively, there's no problem with this sort of operation, since at the end of the day we only care about the indices of the sparse tensor. However, this means that if I do this with macros, I end up with O(n^2) copies of sparseMask:\r\n\r\n* `THSFloatTensor sparseMask(THFloatTensor dense, THSFloatTensor mask)`\r\n* `THSFloatTensor sparseMask(THFloatTensor dense, THSLongTensor mask)`\r\n* `THSFloatTensor sparseMask(THFloatTensor dense, ... mask)`\r\n* `THSFloatTensor sparseMask(THLongTensor dense, THSFloatTensor mask)`\r\n* `THSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)`\r\n* `THSLongTensor sparseMask(THLongTensor dense, THSLongTensor mask)`\r\n* `THSLongTensor sparseMask(THLongTensor dense, ... mask)`\r\n* etc\r\n\r\nIn this patch, I get polymorphic sparseMask, but with only O(n) functions, since they accept the index tensor (of which there's only one type.) That's the benefit."}