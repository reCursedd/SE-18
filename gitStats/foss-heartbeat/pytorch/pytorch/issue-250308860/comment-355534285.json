{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355534285", "html_url": "https://github.com/pytorch/pytorch/issues/2421#issuecomment-355534285", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2421", "id": 355534285, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTUzNDI4NQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-05T11:31:56Z", "updated_at": "2018-01-05T11:33:25Z", "author_association": "COLLABORATOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9547057\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/D-X-Y\">@D-X-Y</a> square root has no subgradient at 0. You could define a gradient by continuity but then it would be <code>+inf</code>...<br>\nGiven that pytorch is using autograd, <code>x.norm()</code> and <code>x.pow(2).sqrt()</code> (equivalent to your <code>torch.sqrt(x*x)</code>) are completely different:</p>\n<ul>\n<li>The first one is a single function that is convex and defined on R, it has a subgradient of 0 at 0.</li>\n<li>The second one is composed of two function, the first function is the square function which is differentiable and outputs values in <code>[0, +inf[</code>. The second function is the square root that is not convex and even though it is defined on <code>[0, +inf[</code>, it is only differentiable on <code>]0, +inf[</code> and it's gradient in 0 in undefined.<br>\nGiven that, even though <code>x.norm()</code> and <code>x.pow(2).sqrt()</code> will return the same value, their gradients may differ at points where it is not differentiable, this is because automatic differentiation looks at each step of the computation one by one and even though in some cases a subgradient exist (because we look at multiple operations as a single function), it is not always the case and the gradient remains undefined.</li>\n</ul>", "body_text": "@D-X-Y square root has no subgradient at 0. You could define a gradient by continuity but then it would be +inf...\nGiven that pytorch is using autograd, x.norm() and x.pow(2).sqrt() (equivalent to your torch.sqrt(x*x)) are completely different:\n\nThe first one is a single function that is convex and defined on R, it has a subgradient of 0 at 0.\nThe second one is composed of two function, the first function is the square function which is differentiable and outputs values in [0, +inf[. The second function is the square root that is not convex and even though it is defined on [0, +inf[, it is only differentiable on ]0, +inf[ and it's gradient in 0 in undefined.\nGiven that, even though x.norm() and x.pow(2).sqrt() will return the same value, their gradients may differ at points where it is not differentiable, this is because automatic differentiation looks at each step of the computation one by one and even though in some cases a subgradient exist (because we look at multiple operations as a single function), it is not always the case and the gradient remains undefined.", "body": "@D-X-Y square root has no subgradient at 0. You could define a gradient by continuity but then it would be `+inf`...\r\nGiven that pytorch is using autograd, `x.norm()` and `x.pow(2).sqrt()` (equivalent to your `torch.sqrt(x*x)`) are completely different:\r\n- The first one is a single function that is convex and defined on R, it has a subgradient of 0 at 0.\r\n- The second one is composed of two function, the first function is the square function which is differentiable and outputs values in `[0, +inf[`. The second function is the square root that is not convex and even though it is defined on `[0, +inf[`, it is only differentiable on `]0, +inf[` and it's gradient in 0 in undefined.\r\nGiven that, even though `x.norm()` and `x.pow(2).sqrt()` will return the same value, their gradients may differ at points where it is not differentiable, this is because automatic differentiation looks at each step of the computation one by one and even though in some cases a subgradient exist (because we look at multiple operations as a single function), it is not always the case and the gradient remains undefined.\r\n  "}