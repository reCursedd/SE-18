{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388175095", "html_url": "https://github.com/pytorch/pytorch/issues/2421#issuecomment-388175095", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2421", "id": 388175095, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODE3NTA5NQ==", "user": {"login": "asford", "id": 282792, "node_id": "MDQ6VXNlcjI4Mjc5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/282792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asford", "html_url": "https://github.com/asford", "followers_url": "https://api.github.com/users/asford/followers", "following_url": "https://api.github.com/users/asford/following{/other_user}", "gists_url": "https://api.github.com/users/asford/gists{/gist_id}", "starred_url": "https://api.github.com/users/asford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asford/subscriptions", "organizations_url": "https://api.github.com/users/asford/orgs", "repos_url": "https://api.github.com/users/asford/repos", "events_url": "https://api.github.com/users/asford/events{/privacy}", "received_events_url": "https://api.github.com/users/asford/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-10T20:26:47Z", "updated_at": "2018-05-10T20:26:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a></p>\n<p>I've also run into a number of problems related to the change introduced in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"258459563\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2775\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2775/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/2775\">#2775</a>. Is there a reason the subgradient is set to 0, rather than the 1? (The limit as norm-&gt;0?)</p>\n<p>As a minimal example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\nv <span class=\"pl-k\">=</span> torch.linspace(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1e-6</span>, <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>).requires_grad_()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bnorm</span>(<span class=\"pl-smi\">val</span>):\n    n <span class=\"pl-k\">=</span> val.detach().clone().requires_grad_(<span class=\"pl-c1\">True</span>)\n    c <span class=\"pl-k\">=</span> n.reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n    \n    nn <span class=\"pl-k\">=</span> c.norm(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    torch.autograd.backward(nn, torch.ones_like(nn))\n    <span class=\"pl-k\">return</span> n.grad\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">snorm</span>(<span class=\"pl-smi\">val</span>):\n    n <span class=\"pl-k\">=</span> val.detach().clone().requires_grad_(<span class=\"pl-c1\">True</span>)\n    c <span class=\"pl-k\">=</span> n.reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n    \n    nn <span class=\"pl-k\">=</span> (c <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>).sqrt()\n    \n    torch.autograd.backward(nn, torch.ones_like(nn))\n    <span class=\"pl-k\">return</span> n.grad\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>torch.__version__:<span class=\"pl-pds\">\"</span></span>)\ndisplay(torch.<span class=\"pl-c1\">__version__</span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vals:<span class=\"pl-pds\">\"</span></span>)\ndisplay(v)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>torch.norm(v, dim=-1) grad:<span class=\"pl-pds\">\"</span></span>)\ndisplay(bnorm(v))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>(v**2).sum(dim=-1).sqrt() grad:<span class=\"pl-pds\">\"</span></span>)\ndisplay(snorm(v))</pre></div>\n<p>Produces</p>\n<pre><code>torch.__version__:\n'0.4.0'\nvals:\ntensor(1.00000e-07 *\n       [ 0.0000,  1.1111,  2.2222,  3.3333,  4.4444,  5.5556,  6.6667,\n         7.7778,  8.8889, 10.0000])\ntorch.norm(v, dim=-1) grad:\ntensor([ 0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])\n(v**2).sum(dim=-1).sqrt() grad:\ntensor([nan.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.])\n</code></pre>", "body_text": "@ngimel @albanD\nI've also run into a number of problems related to the change introduced in #2775. Is there a reason the subgradient is set to 0, rather than the 1? (The limit as norm->0?)\nAs a minimal example:\nimport torch\n\nv = torch.linspace(0, 1e-6, steps=10).requires_grad_()\n\ndef bnorm(val):\n    n = val.detach().clone().requires_grad_(True)\n    c = n.reshape(-1, 1)\n    \n    nn = c.norm(dim=-1)\n    torch.autograd.backward(nn, torch.ones_like(nn))\n    return n.grad\n\ndef snorm(val):\n    n = val.detach().clone().requires_grad_(True)\n    c = n.reshape(-1, 1)\n    \n    nn = (c ** 2).sum(dim=-1).sqrt()\n    \n    torch.autograd.backward(nn, torch.ones_like(nn))\n    return n.grad\n\nprint(\"torch.__version__:\")\ndisplay(torch.__version__)\n\nprint(\"vals:\")\ndisplay(v)\n\nprint(\"torch.norm(v, dim=-1) grad:\")\ndisplay(bnorm(v))\n\nprint(\"(v**2).sum(dim=-1).sqrt() grad:\")\ndisplay(snorm(v))\nProduces\ntorch.__version__:\n'0.4.0'\nvals:\ntensor(1.00000e-07 *\n       [ 0.0000,  1.1111,  2.2222,  3.3333,  4.4444,  5.5556,  6.6667,\n         7.7778,  8.8889, 10.0000])\ntorch.norm(v, dim=-1) grad:\ntensor([ 0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])\n(v**2).sum(dim=-1).sqrt() grad:\ntensor([nan.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.])", "body": "@ngimel @albanD \r\n\r\nI've also run into a number of problems related to the change introduced in #2775. Is there a reason the subgradient is set to 0, rather than the 1? (The limit as norm->0?)\r\n\r\nAs a minimal example:\r\n\r\n```python\r\nimport torch\r\n\r\nv = torch.linspace(0, 1e-6, steps=10).requires_grad_()\r\n\r\ndef bnorm(val):\r\n    n = val.detach().clone().requires_grad_(True)\r\n    c = n.reshape(-1, 1)\r\n    \r\n    nn = c.norm(dim=-1)\r\n    torch.autograd.backward(nn, torch.ones_like(nn))\r\n    return n.grad\r\n\r\ndef snorm(val):\r\n    n = val.detach().clone().requires_grad_(True)\r\n    c = n.reshape(-1, 1)\r\n    \r\n    nn = (c ** 2).sum(dim=-1).sqrt()\r\n    \r\n    torch.autograd.backward(nn, torch.ones_like(nn))\r\n    return n.grad\r\n\r\nprint(\"torch.__version__:\")\r\ndisplay(torch.__version__)\r\n\r\nprint(\"vals:\")\r\ndisplay(v)\r\n\r\nprint(\"torch.norm(v, dim=-1) grad:\")\r\ndisplay(bnorm(v))\r\n\r\nprint(\"(v**2).sum(dim=-1).sqrt() grad:\")\r\ndisplay(snorm(v))\r\n```\r\n\r\nProduces\r\n```\r\ntorch.__version__:\r\n'0.4.0'\r\nvals:\r\ntensor(1.00000e-07 *\r\n       [ 0.0000,  1.1111,  2.2222,  3.3333,  4.4444,  5.5556,  6.6667,\r\n         7.7778,  8.8889, 10.0000])\r\ntorch.norm(v, dim=-1) grad:\r\ntensor([ 0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])\r\n(v**2).sum(dim=-1).sqrt() grad:\r\ntensor([nan.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.])\r\n```"}