{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2421", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2421/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2421/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2421/events", "html_url": "https://github.com/pytorch/pytorch/issues/2421", "id": 250308860, "node_id": "MDU6SXNzdWUyNTAzMDg4NjA=", "number": 2421, "title": "Gradient of zero norm is nan", "user": {"login": "zkolter", "id": 2465474, "node_id": "MDQ6VXNlcjI0NjU0NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/2465474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zkolter", "html_url": "https://github.com/zkolter", "followers_url": "https://api.github.com/users/zkolter/followers", "following_url": "https://api.github.com/users/zkolter/following{/other_user}", "gists_url": "https://api.github.com/users/zkolter/gists{/gist_id}", "starred_url": "https://api.github.com/users/zkolter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zkolter/subscriptions", "organizations_url": "https://api.github.com/users/zkolter/orgs", "repos_url": "https://api.github.com/users/zkolter/repos", "events_url": "https://api.github.com/users/zkolter/events{/privacy}", "received_events_url": "https://api.github.com/users/zkolter/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-08-15T13:06:31Z", "updated_at": "2018-05-11T10:11:36Z", "closed_at": "2017-09-18T16:26:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If a norm is zero, its gradient returns nan:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nx.norm().backward()\n<span class=\"pl-c1\">print</span> x.grad\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Variable containing:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> nan</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [torch.FloatTensor of size 1]</span></pre></div>\n<p>Obviously just happening because the gradient divides by the norm, but the (sub)gradient here should probably be zero, or at least not nan, since that will propagate to make all updates nan.  Probably low priority, as it's not going to be an issue in 99% of cases, but we're doing a few things with (exact) line searches where this caused a nan to appear, breaking everything downstream.</p>", "body_text": "If a norm is zero, its gradient returns nan:\nx = Variable(torch.zeros(1), requires_grad=True)\nx.norm().backward()\nprint x.grad\n\n# Variable containing:\n# nan\n# [torch.FloatTensor of size 1]\nObviously just happening because the gradient divides by the norm, but the (sub)gradient here should probably be zero, or at least not nan, since that will propagate to make all updates nan.  Probably low priority, as it's not going to be an issue in 99% of cases, but we're doing a few things with (exact) line searches where this caused a nan to appear, breaking everything downstream.", "body": "If a norm is zero, its gradient returns nan:\r\n\r\n```python\r\nx = Variable(torch.zeros(1), requires_grad=True)\r\nx.norm().backward()\r\nprint x.grad\r\n\r\n# Variable containing:\r\n# nan\r\n# [torch.FloatTensor of size 1]\r\n```\r\n\r\nObviously just happening because the gradient divides by the norm, but the (sub)gradient here should probably be zero, or at least not nan, since that will propagate to make all updates nan.  Probably low priority, as it's not going to be an issue in 99% of cases, but we're doing a few things with (exact) line searches where this caused a nan to appear, breaking everything downstream.\r\n"}