{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187784856", "pull_request_review_id": 119638415, "id": 187784856, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Nzc4NDg1Ng==", "diff_hunk": "@@ -1579,436 +1650,413 @@ __global__ void col2im_gpu_kernel_nhwc(const int n, const T* data_col,\n         if (h_k % dilation_h == 0 && w_k % dilation_w == 0) {\n           h_k /= dilation_h;\n           w_k /= dilation_w;\n-          int c_col = (h_k * patch_w + w_k) * channels + c;\n-          val += data_col[(h_col * width_col + w_col) * channels_col + c_col];\n+          const int c_col = (h_k * patch_w + w_k) * channels + c;\n+#if __CUDA_ARCH__ >= 350\n+          val += __ldg(\n+              col_data + (h_col * output_w + w_col) * channels_col + c_col);\n+#else\n+          val += col_data[(h_col * output_w + w_col) * channels_col + c_col];\n+#endif\n         }\n       }\n     }\n-    data_im[index] = val;\n+    img_data[index] = val;\n   }\n }\n \n-// Ported from caffe1\n-template <typename T, int num_axes>\n-__global__ void im2col_nd_gpu_kernel(\n-    const int n,\n-    const T* data_im,\n-    const int* im_shape,\n-    const int* col_shape,\n-    const int* kernel_shape,\n-    const int* pad,\n-    const int* stride,\n-    const int* dilation,\n-    T* data_col) {\n-  int d_offset[num_axes]; // NOLINT(runtime/arrays)\n-  int d_iter[num_axes]; // NOLINT(runtime/arrays)\n-\n-  __shared__ int shared_dilation[num_axes];\n-  __shared__ int shared_kernel_shape[num_axes];\n-  __shared__ int shared_pad[num_axes];\n-  __shared__ int shared_stride[num_axes];\n-  __shared__ int shared_col_shape[num_axes + 1];\n-  __shared__ int shared_im_shape[num_axes + 1];\n-\n-  if (threadIdx.x < num_axes) {\n-    shared_dilation[threadIdx.x] = dilation[threadIdx.x];\n-    shared_kernel_shape[threadIdx.x] = kernel_shape[threadIdx.x];\n-    shared_pad[threadIdx.x] = pad[threadIdx.x];\n-    shared_stride[threadIdx.x] = stride[threadIdx.x];\n-  }\n-  if (threadIdx.x < num_axes + 1) {\n-    shared_col_shape[threadIdx.x] = col_shape[threadIdx.x];\n-    shared_im_shape[threadIdx.x] = im_shape[threadIdx.x];\n-  }\n-  __syncthreads();\n-\n-  int i;\n-  int kernel_size = 1;\n-  for (i = 0; i < num_axes; ++i) {\n-    kernel_size *= shared_kernel_shape[i];\n-  }\n-  CUDA_1D_KERNEL_LOOP(index, n) {\n-    if (index >= col_shape[0]) {\n-      break;\n+template <typename T, int N, bool kCol2Im>\n+__global__ void Im2ColNdNCHWCUDAKernel(\n+    const int outer_size,\n+    const int inner_size,\n+    const int kernel_size,\n+    SimpleArray<int, N + 1> img_shape,\n+    SimpleArray<int, N + 1> col_shape,\n+    SimpleArray<int, N> kernel_shape,\n+    SimpleArray<int, N> stride,\n+    SimpleArray<int, N> dilation,\n+    SimpleArray<int, N> pad,\n+    const T* X_data,\n+    T* Y_data) {\n+  int d_offset[N];\n+  int d_iter[N];\n+  for (int i = blockIdx.x; i < outer_size; i += gridDim.x) {\n+    int offset_i = i;\n+#pragma unroll\n+    for (int d_i = N - 1; d_i >= 0; --d_i) {\n+      d_offset[d_i] = offset_i % kernel_shape.data[d_i];\n+      offset_i /= kernel_shape.data[d_i];\n     }\n-    // Initialize offset, computed in the loop below, with intermediate\n-    // computations used to compute the spatial indices.\n-    int offset = index;\n-    for (i = num_axes - 1; i >= 0; --i) {\n-      if (i < num_axes - 1) {\n-        offset /= shared_kernel_shape[i + 1];\n+    for (int j = threadIdx.x; j < inner_size; j += blockDim.x) {", "path": "caffe2/utils/math_gpu.cu", "position": 778, "original_position": 778, "commit_id": "cb1de76d6d8340d7b1ace6b01efaecdceb8ff730", "original_commit_id": "cb1de76d6d8340d7b1ace6b01efaecdceb8ff730", "user": {"login": "yinghai", "id": 1100089, "node_id": "MDQ6VXNlcjExMDAwODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1100089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinghai", "html_url": "https://github.com/yinghai", "followers_url": "https://api.github.com/users/yinghai/followers", "following_url": "https://api.github.com/users/yinghai/following{/other_user}", "gists_url": "https://api.github.com/users/yinghai/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinghai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinghai/subscriptions", "organizations_url": "https://api.github.com/users/yinghai/orgs", "repos_url": "https://api.github.com/users/yinghai/repos", "events_url": "https://api.github.com/users/yinghai/events{/privacy}", "received_events_url": "https://api.github.com/users/yinghai/received_events", "type": "User", "site_admin": false}, "body": "Ok. I can see the double parallel loop here. ", "created_at": "2018-05-12T22:44:03Z", "updated_at": "2018-11-23T15:43:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/7505#discussion_r187784856", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7505", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187784856"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7505#discussion_r187784856"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7505"}}, "body_html": "<p>Ok. I can see the double parallel loop here.</p>", "body_text": "Ok. I can see the double parallel loop here."}