{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388587902", "html_url": "https://github.com/pytorch/pytorch/pull/7505#issuecomment-388587902", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7505", "id": 388587902, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODU4NzkwMg==", "user": {"login": "BIT-silence", "id": 3357667, "node_id": "MDQ6VXNlcjMzNTc2Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3357667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BIT-silence", "html_url": "https://github.com/BIT-silence", "followers_url": "https://api.github.com/users/BIT-silence/followers", "following_url": "https://api.github.com/users/BIT-silence/following{/other_user}", "gists_url": "https://api.github.com/users/BIT-silence/gists{/gist_id}", "starred_url": "https://api.github.com/users/BIT-silence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BIT-silence/subscriptions", "organizations_url": "https://api.github.com/users/BIT-silence/orgs", "repos_url": "https://api.github.com/users/BIT-silence/repos", "events_url": "https://api.github.com/users/BIT-silence/events{/privacy}", "received_events_url": "https://api.github.com/users/BIT-silence/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-12T22:31:24Z", "updated_at": "2018-05-12T22:31:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The main purpose of this PR is make the math functions like Im2ColNd and Col2ImNd with all the parameters on host instead of on device. Then there will be no need for the ops to upload the host to extra tensors while calling such functions.<br>\nAt the same time, we found the origin implementation of Im2ColNd and Col2ImNd on GPU are not very well. The main problem is the size of col tensor is kernel_size * output_image_size. While our current implementation only run multi-thread on kernel_size dim, all the computation on output_image_size dim are in a single thread. That's the main performance issue which also be fixed in this PR&gt;</p>", "body_text": "The main purpose of this PR is make the math functions like Im2ColNd and Col2ImNd with all the parameters on host instead of on device. Then there will be no need for the ops to upload the host to extra tensors while calling such functions.\nAt the same time, we found the origin implementation of Im2ColNd and Col2ImNd on GPU are not very well. The main problem is the size of col tensor is kernel_size * output_image_size. While our current implementation only run multi-thread on kernel_size dim, all the computation on output_image_size dim are in a single thread. That's the main performance issue which also be fixed in this PR>", "body": "The main purpose of this PR is make the math functions like Im2ColNd and Col2ImNd with all the parameters on host instead of on device. Then there will be no need for the ops to upload the host to extra tensors while calling such functions. \r\nAt the same time, we found the origin implementation of Im2ColNd and Col2ImNd on GPU are not very well. The main problem is the size of col tensor is kernel_size * output_image_size. While our current implementation only run multi-thread on kernel_size dim, all the computation on output_image_size dim are in a single thread. That's the main performance issue which also be fixed in this PR>"}