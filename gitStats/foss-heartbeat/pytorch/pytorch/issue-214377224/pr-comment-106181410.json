{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/106181410", "pull_request_review_id": 27090819, "id": 106181410, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNjE4MTQxMA==", "diff_hunk": "@@ -46,7 +46,7 @@ def forward(self, input):\n     def backward(self, grad_output):\n         if self.dim is None:\n             input, = self.saved_tensors\n-            grad_input = grad_output.new(self.input_size).fill_(self.result)\n+            grad_input = grad_output.new(self.input_size).fill_(self.result * grad_output[0])", "path": "torch/autograd/_functions/reduce.py", "position": 5, "original_position": 5, "commit_id": "9f8889811e426a4de5945fb1b555b2ce936a3c9f", "original_commit_id": "9f8889811e426a4de5945fb1b555b2ce936a3c9f", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "This leads to a sync point in the GPU because of the `grad_output[0]`, right?\r\nA solution that doesn't have a sync point would be something like\r\n```python\r\ngrad_input = grad_output.expand(self.input_size).mul(self.result)\r\n```\r\n\r\nBut now I see that other modules (like Sum) also access `grad_output[0]`. Maybe we should change that?", "created_at": "2017-03-15T14:37:50Z", "updated_at": "2018-11-23T15:32:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/1002#discussion_r106181410", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1002", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/106181410"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1002#discussion_r106181410"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1002"}}, "body_html": "<p>This leads to a sync point in the GPU because of the <code>grad_output[0]</code>, right?<br>\nA solution that doesn't have a sync point would be something like</p>\n<div class=\"highlight highlight-source-python\"><pre>grad_input <span class=\"pl-k\">=</span> grad_output.expand(<span class=\"pl-c1\">self</span>.input_size).mul(<span class=\"pl-c1\">self</span>.result)</pre></div>\n<p>But now I see that other modules (like Sum) also access <code>grad_output[0]</code>. Maybe we should change that?</p>", "body_text": "This leads to a sync point in the GPU because of the grad_output[0], right?\nA solution that doesn't have a sync point would be something like\ngrad_input = grad_output.expand(self.input_size).mul(self.result)\nBut now I see that other modules (like Sum) also access grad_output[0]. Maybe we should change that?"}