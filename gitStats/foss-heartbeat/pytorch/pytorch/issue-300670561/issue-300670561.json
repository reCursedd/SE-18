{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5434", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5434/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5434/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5434/events", "html_url": "https://github.com/pytorch/pytorch/issues/5434", "id": 300670561, "node_id": "MDU6SXNzdWUzMDA2NzA1NjE=", "number": 5434, "title": "RuntimeError: $ Torch: not enough memory: you tried to allocate 72GB. Buy new RAM!", "user": {"login": "ponomarevsy", "id": 22173995, "node_id": "MDQ6VXNlcjIyMTczOTk1", "avatar_url": "https://avatars1.githubusercontent.com/u/22173995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ponomarevsy", "html_url": "https://github.com/ponomarevsy", "followers_url": "https://api.github.com/users/ponomarevsy/followers", "following_url": "https://api.github.com/users/ponomarevsy/following{/other_user}", "gists_url": "https://api.github.com/users/ponomarevsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/ponomarevsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ponomarevsy/subscriptions", "organizations_url": "https://api.github.com/users/ponomarevsy/orgs", "repos_url": "https://api.github.com/users/ponomarevsy/repos", "events_url": "https://api.github.com/users/ponomarevsy/events{/privacy}", "received_events_url": "https://api.github.com/users/ponomarevsy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-02-27T15:16:02Z", "updated_at": "2018-08-24T00:17:29Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>OS:</li>\n</ul>\n<p>Red Hat Enterprise Linux Server release 7.2 (Maipo)</p>\n<ul>\n<li>PyTorch version:</li>\n</ul>\n<p>pytorch                   0.3.1           py35_cuda8.0.61_cudnn7.0.5_2    pytorch<br>\ntorchvision               0.2.0            py35heaa392f_1    pytorch</p>\n<ul>\n<li>How you installed PyTorch (conda, pip, source):</li>\n</ul>\n<p>$ module load anaconda3/4.3.1<br>\n$ source activate pytorchenv<br>\n$ conda install pytorch torchvision -c pytorch</p>\n<ul>\n<li>Python version:</li>\n</ul>\n<p>$ python -V<br>\nPython 3.5.5</p>\n<ul>\n<li>CUDA/cuDNN version:</li>\n</ul>\n<p>cuda8.0.61/cudnn7.0.5_2</p>\n<ul>\n<li>GPU models and configuration:</li>\n</ul>\n<p>$ nvidia-smi<br>\nTue Feb 27 10:06:13 2018<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 384.69                 Driver Version: 384.69                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  Tesla K80           On   | 00000000:04:00.0 Off |                    0 |<br>\n| N/A   28C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   1  Tesla K80           On   | 00000000:05:00.0 Off |                    0 |<br>\n| N/A   22C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   2  Tesla K80           On   | 00000000:84:00.0 Off |                    0 |<br>\n| N/A   32C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   3  Tesla K80           On   | 00000000:85:00.0 Off |                    0 |<br>\n| N/A   24C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   4  Tesla K80           On   | 00000000:8A:00.0 Off |                    0 |<br>\n| N/A   24C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   5  Tesla K80           On   | 00000000:8B:00.0 Off |                    0 |<br>\n| N/A   33C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   6  Tesla K80           On   | 00000000:8E:00.0 Off |                    0 |<br>\n| N/A   26C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   7  Tesla K80           On   | 00000000:8F:00.0 Off |                    0 |<br>\n| N/A   35C    P8    31W / 149W |      1MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID  Type  Process name                               Usage      |<br>\n|=============================================================================|<br>\n|  No running processes found                                                 |<br>\n+-----------------------------------------------------------------------------+</p>\n<ul>\n<li>GCC version (if compiling from source):</li>\n</ul>\n<p>N/A</p>\n<p>In addition, including the following information will also be very helpful for us to diagnose the problem:</p>\n<ul>\n<li>A script to reproduce the bug. Please try to provide as minimal of a test case as possible.</li>\n<li>Error messages and/or stack traces of the bug</li>\n<li>Context around what you are trying to do</li>\n</ul>\n<p>Training a model with:</p>\n<pre><code>...\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n...\n\tvalid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False, num_workers=12)\n\t# ====== start trianing =======\n\n\tcudnn.benchmark = True\n\tN_CLASSES = 8\n\tBATCH_SIZE = 32\n</code></pre>\n<p>I've tried batch sizes from 128 to 8, and using GPUs from just one to all 8. GPU node has plenty of RAM (124G):</p>\n<p>$ free<br>\ntotal        used        free      shared  buff/cache   available<br>\nMem:      131930696     6299776   124697204       16968      933716   124718092<br>\nSwap:      16777212      441996    16335216</p>\n<p>Do you have a maximum RAM allocation limit hardcoded in PyTorch (file \"THGeneral.c\")? Thank you in advance!</p>\n<p>Complete error message:</p>\n<pre><code>$ python3 train.py ./out/ &gt; train.out\nTraceback (most recent call last):\n  File \"train.py\", line 120, in &lt;module&gt;\n    augment_img = torch.stack(augment_img)[perm_index]\n  File \"/path-to-anaconda/Anaconda3/4.3.1/envs/pytorchenv/lib/python3.5/site-packages/torch/functional.py\", line 64, in stack\n    return torch.cat(inputs, dim)\nRuntimeError: $ Torch: not enough memory: you tried to allocate 72GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1518241081361/work/torch/lib/TH/THGeneral.c:253\n</code></pre>", "body_text": "OS:\n\nRed Hat Enterprise Linux Server release 7.2 (Maipo)\n\nPyTorch version:\n\npytorch                   0.3.1           py35_cuda8.0.61_cudnn7.0.5_2    pytorch\ntorchvision               0.2.0            py35heaa392f_1    pytorch\n\nHow you installed PyTorch (conda, pip, source):\n\n$ module load anaconda3/4.3.1\n$ source activate pytorchenv\n$ conda install pytorch torchvision -c pytorch\n\nPython version:\n\n$ python -V\nPython 3.5.5\n\nCUDA/cuDNN version:\n\ncuda8.0.61/cudnn7.0.5_2\n\nGPU models and configuration:\n\n$ nvidia-smi\nTue Feb 27 10:06:13 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.69                 Driver Version: 384.69                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 00000000:04:00.0 Off |                    0 |\n| N/A   28C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 00000000:05:00.0 Off |                    0 |\n| N/A   22C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 00000000:84:00.0 Off |                    0 |\n| N/A   32C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 00000000:85:00.0 Off |                    0 |\n| N/A   24C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  Tesla K80           On   | 00000000:8A:00.0 Off |                    0 |\n| N/A   24C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  Tesla K80           On   | 00000000:8B:00.0 Off |                    0 |\n| N/A   33C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  Tesla K80           On   | 00000000:8E:00.0 Off |                    0 |\n| N/A   26C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  Tesla K80           On   | 00000000:8F:00.0 Off |                    0 |\n| N/A   35C    P8    31W / 149W |      1MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\nGCC version (if compiling from source):\n\nN/A\nIn addition, including the following information will also be very helpful for us to diagnose the problem:\n\nA script to reproduce the bug. Please try to provide as minimal of a test case as possible.\nError messages and/or stack traces of the bug\nContext around what you are trying to do\n\nTraining a model with:\n...\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n...\n\tvalid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False, num_workers=12)\n\t# ====== start trianing =======\n\n\tcudnn.benchmark = True\n\tN_CLASSES = 8\n\tBATCH_SIZE = 32\n\nI've tried batch sizes from 128 to 8, and using GPUs from just one to all 8. GPU node has plenty of RAM (124G):\n$ free\ntotal        used        free      shared  buff/cache   available\nMem:      131930696     6299776   124697204       16968      933716   124718092\nSwap:      16777212      441996    16335216\nDo you have a maximum RAM allocation limit hardcoded in PyTorch (file \"THGeneral.c\")? Thank you in advance!\nComplete error message:\n$ python3 train.py ./out/ > train.out\nTraceback (most recent call last):\n  File \"train.py\", line 120, in <module>\n    augment_img = torch.stack(augment_img)[perm_index]\n  File \"/path-to-anaconda/Anaconda3/4.3.1/envs/pytorchenv/lib/python3.5/site-packages/torch/functional.py\", line 64, in stack\n    return torch.cat(inputs, dim)\nRuntimeError: $ Torch: not enough memory: you tried to allocate 72GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1518241081361/work/torch/lib/TH/THGeneral.c:253", "body": "- OS: \r\n\r\nRed Hat Enterprise Linux Server release 7.2 (Maipo)\r\n\r\n- PyTorch version: \r\n\r\npytorch                   0.3.1           py35_cuda8.0.61_cudnn7.0.5_2    pytorch\r\ntorchvision               0.2.0            py35heaa392f_1    pytorch\r\n\r\n- How you installed PyTorch (conda, pip, source):\r\n\r\n$ module load anaconda3/4.3.1\r\n$ source activate pytorchenv\r\n$ conda install pytorch torchvision -c pytorch\r\n\r\n- Python version:\r\n\r\n$ python -V\r\nPython 3.5.5\r\n\r\n- CUDA/cuDNN version:\r\n\r\ncuda8.0.61/cudnn7.0.5_2\r\n\r\n- GPU models and configuration:\r\n\r\n$ nvidia-smi\r\nTue Feb 27 10:06:13 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.69                 Driver Version: 384.69                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 00000000:04:00.0 Off |                    0 |\r\n| N/A   28C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 00000000:05:00.0 Off |                    0 |\r\n| N/A   22C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           On   | 00000000:84:00.0 Off |                    0 |\r\n| N/A   32C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           On   | 00000000:85:00.0 Off |                    0 |\r\n| N/A   24C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           On   | 00000000:8A:00.0 Off |                    0 |\r\n| N/A   24C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           On   | 00000000:8B:00.0 Off |                    0 |\r\n| N/A   33C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           On   | 00000000:8E:00.0 Off |                    0 |\r\n| N/A   26C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           On   | 00000000:8F:00.0 Off |                    0 |\r\n| N/A   35C    P8    31W / 149W |      1MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n- GCC version (if compiling from source):\r\n\r\nN/A\r\n\r\nIn addition, including the following information will also be very helpful for us to diagnose the problem:\r\n- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.\r\n- Error messages and/or stack traces of the bug\r\n- Context around what you are trying to do\r\n\r\nTraining a model with:\r\n```\r\n...\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\r\n...\r\n\tvalid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False, num_workers=12)\r\n\t# ====== start trianing =======\r\n\r\n\tcudnn.benchmark = True\r\n\tN_CLASSES = 8\r\n\tBATCH_SIZE = 32\r\n```\r\nI've tried batch sizes from 128 to 8, and using GPUs from just one to all 8. GPU node has plenty of RAM (124G):\r\n\r\n$ free\r\n              total        used        free      shared  buff/cache   available\r\nMem:      131930696     6299776   124697204       16968      933716   124718092\r\nSwap:      16777212      441996    16335216\r\n\r\nDo you have a maximum RAM allocation limit hardcoded in PyTorch (file \"THGeneral.c\")? Thank you in advance!\r\n\r\nComplete error message:\r\n```\r\n$ python3 train.py ./out/ > train.out\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 120, in <module>\r\n    augment_img = torch.stack(augment_img)[perm_index]\r\n  File \"/path-to-anaconda/Anaconda3/4.3.1/envs/pytorchenv/lib/python3.5/site-packages/torch/functional.py\", line 64, in stack\r\n    return torch.cat(inputs, dim)\r\nRuntimeError: $ Torch: not enough memory: you tried to allocate 72GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1518241081361/work/torch/lib/TH/THGeneral.c:253\r\n```"}