{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/184987627", "pull_request_review_id": 116287830, "id": 184987627, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NDk4NzYyNw==", "diff_hunk": "@@ -0,0 +1,133 @@\n+#include \"ATen/Context.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/PinnedMemoryAllocator.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+\n+#include \"ATen/native/LinearAlgebraUtils.h\"\n+#include \"ATen/native/Gesv.h\"\n+\n+#include \"THC.h\" // for USE_MAGMA\n+\n+#ifdef USE_MAGMA\n+#include <magma.h>\n+#include <magma_types.h>\n+#endif\n+\n+namespace at {\n+namespace native {\n+\n+#ifdef USE_MAGMA\n+template<class real>\n+void magmaGesvBatched(\n+    magma_int_t n, magma_int_t nrhs, real** dA_array, magma_int_t ldda,\n+    magma_int_t** dipiv_array, real** dB_array, magma_int_t lddb,\n+    magma_int_t* dinfo_array, magma_int_t batch_count, magma_queue_t queue) {\n+  AT_ERROR(\"gesv only takes float or double Tensors\");\n+}\n+\n+template<>\n+void magmaGesvBatched<float>(\n+    magma_int_t n, magma_int_t nrhs, float** dA_array, magma_int_t ldda,\n+    magma_int_t** dipiv_array, float** dB_array, magma_int_t lddb,\n+    magma_int_t* dinfo_array, magma_int_t batch_count, magma_queue_t queue) {\n+  magma_sgesv_batched(\n+      n, nrhs, dA_array, ldda, dipiv_array,\n+      dB_array, lddb, dinfo_array, batch_count, queue);\n+}\n+\n+template<>\n+void magmaGesvBatched<double>(\n+    magma_int_t n, magma_int_t nrhs, double** dA_array, magma_int_t ldda,\n+    magma_int_t** dipiv_array, double** dB_array, magma_int_t lddb,\n+    magma_int_t* dinfo_array, magma_int_t batch_count, magma_queue_t queue) {\n+  magma_dgesv_batched(\n+      n, nrhs, dA_array, ldda, dipiv_array,\n+      dB_array, lddb, dinfo_array, batch_count, queue);\n+}\n+\n+static magma_queue_t createMagmaQueue(const Tensor& tensor) {\n+  auto& context = tensor.type().get_context();\n+  magma_queue_t magma_queue;\n+  magma_queue_create_from_cuda(\n+      tensor.get_device(),\n+      context.getCurrentCUDAStream(),\n+      THCState_getCurrentBlasHandle(context.thc_state),\n+      THCState_getCurrentSparseHandle(context.thc_state),\n+      &magma_queue);\n+  return magma_queue;\n+}\n+#endif\n+\n+// Creates an array of size elements of type T, backed by pinned memory\n+// wrapped in a Storage\n+template<class T>\n+static inline std::unique_ptr<Storage> pin_memory(int64_t size, Tensor dummy) {\n+  int64_t adjusted_size = size * sizeof(T);\n+  auto allocator = std::unique_ptr<Allocator>(new PinnedMemoryAllocator());\n+  auto& backend = dummy.type().toBackend(kCPU).toScalarType(kByte);\n+  return backend.storageWithAllocator(adjusted_size, std::move(allocator));\n+}\n+\n+#define ALLOCATE_ARRAY(name, type, size, dummy_tensor) \\\n+  auto storage_##name = pin_memory<type>(size, dummy_tensor); \\\n+  name = reinterpret_cast<type*>(storage_##name->data());\n+\n+template <typename real>\n+static void applyGesv(Tensor& b, Tensor& A, std::vector<int64_t> infos) {\n+#ifndef USE_MAGMA\n+AT_ERROR(\"gesv: MAGMA library not found in \"\n+    \"compilation. Please rebuild with MAGMA.\");\n+#else\n+  real* A_data = static_cast<real*>(A.data_ptr());\n+  real* b_data = static_cast<real*>(b.data_ptr());\n+  auto A_mat_stride = matrixStride(A);\n+  auto b_mat_stride = matrixStride(b);\n+\n+  magma_int_t batch_size = batchCount(A);", "path": "aten/src/ATen/native/cuda/Gesv.cu", "position": null, "original_position": 87, "commit_id": "fabd77594df5a7e4c4ceca8bb6fd62f036f3b7ff", "original_commit_id": "4805b223b21c2c015ba7492ecd700d214a1ed3cb", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "32-bit", "created_at": "2018-04-30T13:34:31Z", "updated_at": "2018-11-23T15:43:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/6100#discussion_r184987627", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6100", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/184987627"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6100#discussion_r184987627"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6100"}}, "body_html": "<p>32-bit</p>", "body_text": "32-bit", "in_reply_to_id": 184834601}