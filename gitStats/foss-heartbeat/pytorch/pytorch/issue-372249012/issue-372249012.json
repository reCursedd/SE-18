{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12901", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12901/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12901/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12901/events", "html_url": "https://github.com/pytorch/pytorch/issues/12901", "id": 372249012, "node_id": "MDU6SXNzdWUzNzIyNDkwMTI=", "number": 12901, "title": "torch.nn.L1Loss works incorrectly in certain situations", "user": {"login": "LiekkasY", "id": 20140034, "node_id": "MDQ6VXNlcjIwMTQwMDM0", "avatar_url": "https://avatars3.githubusercontent.com/u/20140034?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiekkasY", "html_url": "https://github.com/LiekkasY", "followers_url": "https://api.github.com/users/LiekkasY/followers", "following_url": "https://api.github.com/users/LiekkasY/following{/other_user}", "gists_url": "https://api.github.com/users/LiekkasY/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiekkasY/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiekkasY/subscriptions", "organizations_url": "https://api.github.com/users/LiekkasY/orgs", "repos_url": "https://api.github.com/users/LiekkasY/repos", "events_url": "https://api.github.com/users/LiekkasY/events{/privacy}", "received_events_url": "https://api.github.com/users/LiekkasY/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-10-20T20:18:34Z", "updated_at": "2018-10-22T17:31:19Z", "closed_at": "2018-10-22T17:31:19Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>torch.nn.L1Loss()'s parameter <em>reduction</em> doesn't work if b = net(a).</p>\n<pre><code>b = net(a)\nl = torch.nn.L1Loss(reduction='elementwise_mean')\nl(a, b) # wrong, it's summation \n\n</code></pre>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<pre><code>import torch\nimport torch.nn as nn\n\ndatadim = 33\n\n\nclass BugNet(nn.Module):\n    def __init__(self):\n        super(BugNet, self).__init__()\n        self.output = nn.Linear(datadim, datadim)\n\n    def forward(self, x):\n        return self.output(x)\n\n\na = torch.ones((4, 33))\nbug = BugNet()\nb = bug(a)\n\nloss_fn1 = nn.L1Loss(reduction='sum')\nloss_fn2 = nn.L1Loss(reduction='elementwise_mean')\nelementwise_mean_loss = lambda x, x_out: torch.mean(torch.abs(x-x_out.view_as(x)))\n\nprint(loss_fn1(a, b))  # 127.6617, True\nprint(loss_fn2(a, b))  # 127.6617, False\nprint(loss_fn2(b, a))  # 0.9671, True\nprint(elementwise_mean_loss(a, b))  # 0.9671, True\n</code></pre>\n\n<h2>Outputs</h2>\n<pre><code>tensor(127.6617, grad_fn=&lt;SumBackward0&gt;)\ntensor(127.6617, grad_fn=&lt;SumBackward0&gt;)\ntensor(0.9671, grad_fn=&lt;L1LossBackward&gt;)\ntensor(0.9671, grad_fn=&lt;MeanBackward1&gt;)\n</code></pre>\n\n<h2>Environment</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.2.148</p>\n<p>OS: Debian GNU/Linux 9.5 (stretch)<br>\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.5<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: Tesla K80<br>\nNvidia driver version: 396.44<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1<br>\n/usr/local/cuda-9.2/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] intel-numpy (1.15.1)<br>\n[pip] numpy (1.15.1)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] Could not collect</p>\n<h2>Additional context</h2>\n<p>I have checked the source. In  F.l1_loss(), it use<br>\n<code>reduction = _Reduction.get_enum(reduction)</code><br>\nto convert reduction to an integer. However, it calls _pointwise_loss(),<br>\nwhich assumes that reduction is still a str.<br>\n<code>return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)  # In _pointwise_loss()</code><br>\nThat's the reason of this bug.</p>\n", "body_text": "\ud83d\udc1b Bug\n\ntorch.nn.L1Loss()'s parameter reduction doesn't work if b = net(a).\nb = net(a)\nl = torch.nn.L1Loss(reduction='elementwise_mean')\nl(a, b) # wrong, it's summation \n\n\nTo Reproduce\nSteps to reproduce the behavior:\nimport torch\nimport torch.nn as nn\n\ndatadim = 33\n\n\nclass BugNet(nn.Module):\n    def __init__(self):\n        super(BugNet, self).__init__()\n        self.output = nn.Linear(datadim, datadim)\n\n    def forward(self, x):\n        return self.output(x)\n\n\na = torch.ones((4, 33))\nbug = BugNet()\nb = bug(a)\n\nloss_fn1 = nn.L1Loss(reduction='sum')\nloss_fn2 = nn.L1Loss(reduction='elementwise_mean')\nelementwise_mean_loss = lambda x, x_out: torch.mean(torch.abs(x-x_out.view_as(x)))\n\nprint(loss_fn1(a, b))  # 127.6617, True\nprint(loss_fn2(a, b))  # 127.6617, False\nprint(loss_fn2(b, a))  # 0.9671, True\nprint(elementwise_mean_loss(a, b))  # 0.9671, True\n\n\nOutputs\ntensor(127.6617, grad_fn=<SumBackward0>)\ntensor(127.6617, grad_fn=<SumBackward0>)\ntensor(0.9671, grad_fn=<L1LossBackward>)\ntensor(0.9671, grad_fn=<MeanBackward1>)\n\n\nEnvironment\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\nOS: Debian GNU/Linux 9.5 (stretch)\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\nCMake version: Could not collect\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: Tesla K80\nNvidia driver version: 396.44\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] intel-numpy (1.15.1)\n[pip] numpy (1.15.1)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] Could not collect\nAdditional context\nI have checked the source. In  F.l1_loss(), it use\nreduction = _Reduction.get_enum(reduction)\nto convert reduction to an integer. However, it calls _pointwise_loss(),\nwhich assumes that reduction is still a str.\nreturn torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)  # In _pointwise_loss()\nThat's the reason of this bug.", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\ntorch.nn.L1Loss()'s parameter *reduction* doesn't work if b = net(a).\r\n```\r\nb = net(a)\r\nl = torch.nn.L1Loss(reduction='elementwise_mean')\r\nl(a, b) # wrong, it's summation \r\n\r\n```\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ndatadim = 33\r\n\r\n\r\nclass BugNet(nn.Module):\r\n    def __init__(self):\r\n        super(BugNet, self).__init__()\r\n        self.output = nn.Linear(datadim, datadim)\r\n\r\n    def forward(self, x):\r\n        return self.output(x)\r\n\r\n\r\na = torch.ones((4, 33))\r\nbug = BugNet()\r\nb = bug(a)\r\n\r\nloss_fn1 = nn.L1Loss(reduction='sum')\r\nloss_fn2 = nn.L1Loss(reduction='elementwise_mean')\r\nelementwise_mean_loss = lambda x, x_out: torch.mean(torch.abs(x-x_out.view_as(x)))\r\n\r\nprint(loss_fn1(a, b))  # 127.6617, True\r\nprint(loss_fn2(a, b))  # 127.6617, False\r\nprint(loss_fn2(b, a))  # 0.9671, True\r\nprint(elementwise_mean_loss(a, b))  # 0.9671, True\r\n```\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Outputs\r\n\r\n```\r\ntensor(127.6617, grad_fn=<SumBackward0>)\r\ntensor(127.6617, grad_fn=<SumBackward0>)\r\ntensor(0.9671, grad_fn=<L1LossBackward>)\r\ntensor(0.9671, grad_fn=<MeanBackward1>)\r\n```\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Debian GNU/Linux 9.5 (stretch)\r\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCMake version: Could not collect\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] intel-numpy (1.15.1)\r\n[pip] numpy (1.15.1)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\nI have checked the source. In  F.l1_loss(), it use \r\n`reduction = _Reduction.get_enum(reduction)`\r\nto convert reduction to an integer. However, it calls _pointwise_loss(), \r\nwhich assumes that reduction is still a str.\r\n`return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)  # In _pointwise_loss()`\r\nThat's the reason of this bug.\r\n<!-- Add any other context about the problem here. -->\r\n"}