{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144694982", "pull_request_review_id": 69393051, "id": 144694982, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NDY5NDk4Mg==", "diff_hunk": "@@ -203,16 +203,39 @@ def persistent_id(obj):\n def load(f, map_location=None, pickle_module=pickle):\n     \"\"\"Loads an object saved with :func:`torch.save` from a file.\n \n-    torch.load can dynamically remap storages to be loaded on a different device\n-    using the map_location argument. If it's a callable, it will be called with\n-    two arguments: storage and location tag. It's expected to either return a\n-    storage that's been moved to a different location, or None (and the location\n-    will be resolved using the default method). If this argument is a dict it's\n-    expected to be a mapping from location tags used in a file, to location\n-    tags of the current system.\n-\n-    By default the location tags are 'cpu' for host tensors and 'cuda:device_id'\n-    (e.g. 'cuda:2') for cuda tensors. User extensions can register their own\n+    torch.load uses python's normal unpickling facilities but treats storages,\n+    which underlie tensors, specially.  Storages are first deserialized on the\n+    cpu.  Then, by default, torch.load tries to move the storage to the device\n+    it was saved from, raising an exception if this fails because the device\n+    does not exist.  If you serialize a cuda tensor with torch.save, when you\n+    deserialize it with torch.load, it will be created on the device it was\n+    originally on from a storage that has been moved there.", "path": "torch/serialization.py", "position": null, "original_position": 20, "commit_id": "d8292ee1f90e6e48a0fa00035aae533d28b888c7", "original_commit_id": "c51906a6c58d87a1c55c2fe590e4af1098f87f82", "user": {"login": "greaber", "id": 8763901, "node_id": "MDQ6VXNlcjg3NjM5MDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/8763901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/greaber", "html_url": "https://github.com/greaber", "followers_url": "https://api.github.com/users/greaber/followers", "following_url": "https://api.github.com/users/greaber/following{/other_user}", "gists_url": "https://api.github.com/users/greaber/gists{/gist_id}", "starred_url": "https://api.github.com/users/greaber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/greaber/subscriptions", "organizations_url": "https://api.github.com/users/greaber/orgs", "repos_url": "https://api.github.com/users/greaber/repos", "events_url": "https://api.github.com/users/greaber/events{/privacy}", "received_events_url": "https://api.github.com/users/greaber/received_events", "type": "User", "site_admin": false}, "body": "I can delete the last sentence if you prefer, but the point was that most users think about tensors while the mechanism actually operates on storages.  So the last sentence sums up what the upshot is if you stored a tensor.  Maybe people can draw that inference themselves though if brevity is key.", "created_at": "2017-10-14T14:18:27Z", "updated_at": "2018-11-23T15:35:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/3118#discussion_r144694982", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3118", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144694982"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3118#discussion_r144694982"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3118"}}, "body_html": "<p>I can delete the last sentence if you prefer, but the point was that most users think about tensors while the mechanism actually operates on storages.  So the last sentence sums up what the upshot is if you stored a tensor.  Maybe people can draw that inference themselves though if brevity is key.</p>", "body_text": "I can delete the last sentence if you prefer, but the point was that most users think about tensors while the mechanism actually operates on storages.  So the last sentence sums up what the upshot is if you stored a tensor.  Maybe people can draw that inference themselves though if brevity is key.", "in_reply_to_id": 144694876}