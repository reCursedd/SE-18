{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13898", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13898/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13898/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13898/events", "html_url": "https://github.com/pytorch/pytorch/issues/13898", "id": 380274542, "node_id": "MDU6SXNzdWUzODAyNzQ1NDI=", "number": 13898, "title": "Importing an opencv GpuMat", "user": {"login": "leconteur", "id": 1876772, "node_id": "MDQ6VXNlcjE4NzY3NzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1876772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leconteur", "html_url": "https://github.com/leconteur", "followers_url": "https://api.github.com/users/leconteur/followers", "following_url": "https://api.github.com/users/leconteur/following{/other_user}", "gists_url": "https://api.github.com/users/leconteur/gists{/gist_id}", "starred_url": "https://api.github.com/users/leconteur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leconteur/subscriptions", "organizations_url": "https://api.github.com/users/leconteur/orgs", "repos_url": "https://api.github.com/users/leconteur/repos", "events_url": "https://api.github.com/users/leconteur/events{/privacy}", "received_events_url": "https://api.github.com/users/leconteur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-11-13T15:16:45Z", "updated_at": "2018-11-23T10:15:27Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Contrary to opencv Mat objects, GpuMat are strided in memory. It seems impossible to tell the torch::from_blob method from the c++ api this information. We used to build our tensor directly with A10 which provided this options.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<pre><code>#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;opencv2/core/cuda.hpp&gt;\n//#include \"opencv2/gpu/gpu.hpp\"\n\nusing namespace cv;\n\nint main(int argc, const char* argv[]) {\n    Mat gray_image = Mat::ones(9, 9, CV_32FC1); \n    Mat image;\n    cvtColor(gray_image, image, COLOR_GRAY2RGB);\n\n    cuda::GpuMat gImage;\n    gImage.upload(image);\n    std::vector&lt;int64_t&gt; sizes = {static_cast&lt;int64_t&gt;(gImage.channels()),\n                              static_cast&lt;int64_t&gt;(gImage.rows),\n                              static_cast&lt;int64_t&gt;(gImage.cols)};\n\n    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA);\n    auto tensor_image = torch::from_blob(gImage.data, torch::IntList(sizes), options);\n    std::cout &lt;&lt; tensor_image &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre>\n<p>This produced the following output.</p>\n<pre><code>(1,.,.) = \n  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n\n(2,.,.) = \n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  1  1  0  0  0  0  0  0  0\n\n(3,.,.) = \n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n[ Variable[CUDAFloatType]{3,9,9} ]\nSegmentation fault (core dumped)\n</code></pre>\n<h2>Expected behavior</h2>\n<p>If I use only opencv Mat, I get a tensor filled with ones.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 1.0 nightly</li>\n<li>OS (e.g., Linux): Linux</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): pip</li>\n<li>Build command you used (if compiling from source):</li>\n<li>Python version: 3.6.5</li>\n<li>CUDA/cuDNN version: CUDA runtime version: 8.0.61</li>\n<li>GPU models and configuration: Tesla K40</li>\n</ul>", "body_text": "\ud83d\udc1b Bug\nContrary to opencv Mat objects, GpuMat are strided in memory. It seems impossible to tell the torch::from_blob method from the c++ api this information. We used to build our tensor directly with A10 which provided this options.\nTo Reproduce\nSteps to reproduce the behavior:\n#include <torch/script.h> // One-stop header.\n\n#include <iostream>\n#include <memory>\n#include <opencv2/opencv.hpp>\n#include <opencv2/core/cuda.hpp>\n//#include \"opencv2/gpu/gpu.hpp\"\n\nusing namespace cv;\n\nint main(int argc, const char* argv[]) {\n    Mat gray_image = Mat::ones(9, 9, CV_32FC1); \n    Mat image;\n    cvtColor(gray_image, image, COLOR_GRAY2RGB);\n\n    cuda::GpuMat gImage;\n    gImage.upload(image);\n    std::vector<int64_t> sizes = {static_cast<int64_t>(gImage.channels()),\n                              static_cast<int64_t>(gImage.rows),\n                              static_cast<int64_t>(gImage.cols)};\n\n    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA);\n    auto tensor_image = torch::from_blob(gImage.data, torch::IntList(sizes), options);\n    std::cout << tensor_image << std::endl;\n    return 0;\n}\n\nThis produced the following output.\n(1,.,.) = \n  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n\n(2,.,.) = \n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1\n  1  1  0  0  0  0  0  0  0\n\n(3,.,.) = \n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0\n[ Variable[CUDAFloatType]{3,9,9} ]\nSegmentation fault (core dumped)\n\nExpected behavior\nIf I use only opencv Mat, I get a tensor filled with ones.\nEnvironment\n\nPyTorch Version (e.g., 1.0): 1.0 nightly\nOS (e.g., Linux): Linux\nHow you installed PyTorch (conda, pip, source): pip\nBuild command you used (if compiling from source):\nPython version: 3.6.5\nCUDA/cuDNN version: CUDA runtime version: 8.0.61\nGPU models and configuration: Tesla K40", "body": "## \ud83d\udc1b Bug\r\n\r\nContrary to opencv Mat objects, GpuMat are strided in memory. It seems impossible to tell the torch::from_blob method from the c++ api this information. We used to build our tensor directly with A10 which provided this options. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\n#include <torch/script.h> // One-stop header.\r\n\r\n#include <iostream>\r\n#include <memory>\r\n#include <opencv2/opencv.hpp>\r\n#include <opencv2/core/cuda.hpp>\r\n//#include \"opencv2/gpu/gpu.hpp\"\r\n\r\nusing namespace cv;\r\n\r\nint main(int argc, const char* argv[]) {\r\n    Mat gray_image = Mat::ones(9, 9, CV_32FC1); \r\n    Mat image;\r\n    cvtColor(gray_image, image, COLOR_GRAY2RGB);\r\n\r\n    cuda::GpuMat gImage;\r\n    gImage.upload(image);\r\n    std::vector<int64_t> sizes = {static_cast<int64_t>(gImage.channels()),\r\n                              static_cast<int64_t>(gImage.rows),\r\n                              static_cast<int64_t>(gImage.cols)};\r\n\r\n    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA);\r\n    auto tensor_image = torch::from_blob(gImage.data, torch::IntList(sizes), options);\r\n    std::cout << tensor_image << std::endl;\r\n    return 0;\r\n}\r\n```\r\nThis produced the following output.\r\n```\r\n(1,.,.) = \r\n  1  1  1  1  1  1  1  1  1\r\n  1  1  1  1  1  1  1  1  1\r\n  1  1  1  1  1  1  1  1  1\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n\r\n(2,.,.) = \r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  1  1  1  1  1  1  1\r\n  1  1  1  1  1  1  1  1  1\r\n  1  1  1  1  1  1  1  1  1\r\n  1  1  0  0  0  0  0  0  0\r\n\r\n(3,.,.) = \r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n  0  0  0  0  0  0  0  0  0\r\n[ Variable[CUDAFloatType]{3,9,9} ]\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n## Expected behavior\r\n\r\nIf I use only opencv Mat, I get a tensor filled with ones.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0 nightly\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.5\r\n - CUDA/cuDNN version: CUDA runtime version: 8.0.61\r\n - GPU models and configuration: Tesla K40\r\n"}