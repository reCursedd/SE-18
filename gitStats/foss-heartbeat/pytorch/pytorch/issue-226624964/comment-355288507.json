{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355288507", "html_url": "https://github.com/pytorch/pytorch/issues/1489#issuecomment-355288507", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1489", "id": 355288507, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTI4ODUwNw==", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T13:58:23Z", "updated_at": "2018-01-04T13:59:03Z", "author_association": "NONE", "body_html": "<p>This is quite important as relying on the order of parameters can easily break when loading/saving the state dict. This is also important in the cases of re-using the optimizer history for training a small  subset of the model (while keeping the rest of the model fixed).</p>\n<p>What if the optimizer uses the names of the parameters when the <code>.named_parameters()</code> dict is passed ? i.e. <code>torch.optim.Adam(model.named_parameters(), lr=1e-5)</code></p>", "body_text": "This is quite important as relying on the order of parameters can easily break when loading/saving the state dict. This is also important in the cases of re-using the optimizer history for training a small  subset of the model (while keeping the rest of the model fixed).\nWhat if the optimizer uses the names of the parameters when the .named_parameters() dict is passed ? i.e. torch.optim.Adam(model.named_parameters(), lr=1e-5)", "body": "This is quite important as relying on the order of parameters can easily break when loading/saving the state dict. This is also important in the cases of re-using the optimizer history for training a small  subset of the model (while keeping the rest of the model fixed).\r\n\r\nWhat if the optimizer uses the names of the parameters when the `.named_parameters()` dict is passed ? i.e. ` torch.optim.Adam(model.named_parameters(), lr=1e-5) `\r\n  "}