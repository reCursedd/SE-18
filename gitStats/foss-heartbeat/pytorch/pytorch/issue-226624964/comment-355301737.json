{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355301737", "html_url": "https://github.com/pytorch/pytorch/issues/1489#issuecomment-355301737", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1489", "id": 355301737, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTMwMTczNw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T14:52:19Z", "updated_at": "2018-01-04T14:52:19Z", "author_association": "MEMBER", "body_html": "<p>The optimizers don't rely on the order itself. They just rely on the references to the parameters staying valid. If you change your model, you should recreate the optimizer. Even saving names doesn't help because:</p>\n<ol>\n<li>the optimizer still wouldn't have the reference to the module, so it can't even look them up</li>\n<li>it makes <code>optim</code> tied to <code>nn</code> which is undesirable</li>\n<li>it's still not robust to parameter changes, except if we list all parameters at every iteration, which is quite costly (or resort to more complicated solutions, like the one for caching DataParallel replicas, but I'd rather not do that)</li>\n</ol>", "body_text": "The optimizers don't rely on the order itself. They just rely on the references to the parameters staying valid. If you change your model, you should recreate the optimizer. Even saving names doesn't help because:\n\nthe optimizer still wouldn't have the reference to the module, so it can't even look them up\nit makes optim tied to nn which is undesirable\nit's still not robust to parameter changes, except if we list all parameters at every iteration, which is quite costly (or resort to more complicated solutions, like the one for caching DataParallel replicas, but I'd rather not do that)", "body": "The optimizers don't rely on the order itself. They just rely on the references to the parameters staying valid. If you change your model, you should recreate the optimizer. Even saving names doesn't help because:\r\n1. the optimizer still wouldn't have the reference to the module, so it can't even look them up\r\n2. it makes `optim` tied to `nn` which is undesirable \r\n3. it's still not robust to parameter changes, except if we list all parameters at every iteration, which is quite costly (or resort to more complicated solutions, like the one for caching DataParallel replicas, but I'd rather not do that)"}