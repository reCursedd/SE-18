{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/375424653", "html_url": "https://github.com/pytorch/pytorch/issues/1489#issuecomment-375424653", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1489", "id": 375424653, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTQyNDY1Mw==", "user": {"login": "lemairecarl", "id": 13444373, "node_id": "MDQ6VXNlcjEzNDQ0Mzcz", "avatar_url": "https://avatars3.githubusercontent.com/u/13444373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lemairecarl", "html_url": "https://github.com/lemairecarl", "followers_url": "https://api.github.com/users/lemairecarl/followers", "following_url": "https://api.github.com/users/lemairecarl/following{/other_user}", "gists_url": "https://api.github.com/users/lemairecarl/gists{/gist_id}", "starred_url": "https://api.github.com/users/lemairecarl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lemairecarl/subscriptions", "organizations_url": "https://api.github.com/users/lemairecarl/orgs", "repos_url": "https://api.github.com/users/lemairecarl/repos", "events_url": "https://api.github.com/users/lemairecarl/events{/privacy}", "received_events_url": "https://api.github.com/users/lemairecarl/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-22T19:11:17Z", "updated_at": "2018-03-22T19:17:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is a snippet that crashes but I think shouldn't:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">import</span> torch.autograd <span class=\"pl-k\">as</span> autograd\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_model</span>():\n    <span class=\"pl-k\">return</span> nn.Sequential(<span class=\"pl-k\">*</span>[nn.Linear(i, i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>)], nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">1</span>))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fake_training</span>():\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n        out <span class=\"pl-k\">=</span> model(autograd.Variable(torch.Tensor([<span class=\"pl-c1\">1.0</span>])))\n        optimizer.zero_grad()\n        out.backward()\n        optimizer.step()\n\nmodel <span class=\"pl-k\">=</span> build_model()\noptimizer <span class=\"pl-k\">=</span> optim.Adam(<span class=\"pl-c1\">set</span>(model.parameters()))\nfake_training()\ns <span class=\"pl-k\">=</span> optimizer.state_dict()\n\nmodel <span class=\"pl-k\">=</span> build_model()\noptimizer <span class=\"pl-k\">=</span> optim.Adam(<span class=\"pl-c1\">set</span>(model.parameters()))\noptimizer.load_state_dict(s)\nfake_training()</pre></div>\n<p>I find it useful to use set subtraction to manipulate parameter groups... My workaround for now is to use  a list comprehension instead of a set subtraction, so the order is preserved.</p>\n<p>At the very least, we should raise a warning if an optimizer receives a <code>set</code>. Something that would warn that you can't save/load your optimizer if you do this. Also we should change the docs to say that optimizers accept an iterable with consistent ordering, not just any iterable.</p>", "body_text": "Here is a snippet that crashes but I think shouldn't:\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\n\ndef build_model():\n    return nn.Sequential(*[nn.Linear(i, i + 1) for i in range(1, 20)], nn.Linear(20, 1))\n\ndef fake_training():\n    for _ in range(2):\n        out = model(autograd.Variable(torch.Tensor([1.0])))\n        optimizer.zero_grad()\n        out.backward()\n        optimizer.step()\n\nmodel = build_model()\noptimizer = optim.Adam(set(model.parameters()))\nfake_training()\ns = optimizer.state_dict()\n\nmodel = build_model()\noptimizer = optim.Adam(set(model.parameters()))\noptimizer.load_state_dict(s)\nfake_training()\nI find it useful to use set subtraction to manipulate parameter groups... My workaround for now is to use  a list comprehension instead of a set subtraction, so the order is preserved.\nAt the very least, we should raise a warning if an optimizer receives a set. Something that would warn that you can't save/load your optimizer if you do this. Also we should change the docs to say that optimizers accept an iterable with consistent ordering, not just any iterable.", "body": "Here is a snippet that crashes but I think shouldn't:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.autograd as autograd\r\n\r\ndef build_model():\r\n    return nn.Sequential(*[nn.Linear(i, i + 1) for i in range(1, 20)], nn.Linear(20, 1))\r\n\r\ndef fake_training():\r\n    for _ in range(2):\r\n        out = model(autograd.Variable(torch.Tensor([1.0])))\r\n        optimizer.zero_grad()\r\n        out.backward()\r\n        optimizer.step()\r\n\r\nmodel = build_model()\r\noptimizer = optim.Adam(set(model.parameters()))\r\nfake_training()\r\ns = optimizer.state_dict()\r\n\r\nmodel = build_model()\r\noptimizer = optim.Adam(set(model.parameters()))\r\noptimizer.load_state_dict(s)\r\nfake_training()\r\n```\r\nI find it useful to use set subtraction to manipulate parameter groups... My workaround for now is to use  a list comprehension instead of a set subtraction, so the order is preserved.\r\n\r\nAt the very least, we should raise a warning if an optimizer receives a `set`. Something that would warn that you can't save/load your optimizer if you do this. Also we should change the docs to say that optimizers accept an iterable with consistent ordering, not just any iterable."}