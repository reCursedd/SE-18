{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1489", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1489/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1489/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1489/events", "html_url": "https://github.com/pytorch/pytorch/issues/1489", "id": 226624964, "node_id": "MDU6SXNzdWUyMjY2MjQ5NjQ=", "number": 1489, "title": "Optimizer should track parameter names and not id", "user": {"login": "nitishgupta", "id": 6223213, "node_id": "MDQ6VXNlcjYyMjMyMTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6223213?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nitishgupta", "html_url": "https://github.com/nitishgupta", "followers_url": "https://api.github.com/users/nitishgupta/followers", "following_url": "https://api.github.com/users/nitishgupta/following{/other_user}", "gists_url": "https://api.github.com/users/nitishgupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/nitishgupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nitishgupta/subscriptions", "organizations_url": "https://api.github.com/users/nitishgupta/orgs", "repos_url": "https://api.github.com/users/nitishgupta/repos", "events_url": "https://api.github.com/users/nitishgupta/events{/privacy}", "received_events_url": "https://api.github.com/users/nitishgupta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-05-05T16:26:45Z", "updated_at": "2018-03-22T19:17:44Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>In the optimizer's <code>param_groups['params']</code> the order of the parameters (in which they were given to the optimizer's init) matters.<br>\nIn load_state_dict the snippet shows this :<br>\n<code>id_map = {old_id: p for old_id, p in</code><br>\n<code>zip(chain(*(g['params'] for g in saved_groups)), chain(*(g['params'] for g in groups)))}</code><br>\n<code>state = {id_map.get(k, k): v for k, v in state_dict['state'].items()}</code></p>\n<p>If we change the order in which the parameters are given to the optimizer when loading, the code breaks as the <code>state</code> dict now incorrectly maps parameters to their states.</p>\n<p>Consider model (when using, say, Adam optimizer)</p>\n<pre><code>class Model(nn.Module):\n    def __init__(self):\n    super(Model, self).__init__()\n    self.p1 = nn.Linear(2,3, False)\n    self.p2 = nn.Linear(3,4, False)\n</code></pre>\n<p>After saving, if the order in which the parameters are defined in the model changes i.e. if I change the class to have</p>\n<pre><code>self.p2 = nn.Linear(3,4, False)\nself.p1 = nn.Linear(2,3, False)\n</code></pre>\n<p>the loaded optimizer's state for p1 will be mapped to p2 and vice-versa. I tried this and this indeed happens which is wrong and now training cannot proceed (step() will, rightly so, give an error).<br>\nThe <code>nn.Module</code> class is robust to such behavior as it uses parameter names instead of id ordering.</p>\n<p>IMO the optimizer should also use parameter names instead of ids and relying on the ordering in which they are supplied to the optimizer when initializing.</p>\n<p>Corresponding <a href=\"https://discuss.pytorch.org/t/possible-issues-in-optimizer/2685\" rel=\"nofollow\">PyTorch-Discuss post</a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "In the optimizer's param_groups['params'] the order of the parameters (in which they were given to the optimizer's init) matters.\nIn load_state_dict the snippet shows this :\nid_map = {old_id: p for old_id, p in\nzip(chain(*(g['params'] for g in saved_groups)), chain(*(g['params'] for g in groups)))}\nstate = {id_map.get(k, k): v for k, v in state_dict['state'].items()}\nIf we change the order in which the parameters are given to the optimizer when loading, the code breaks as the state dict now incorrectly maps parameters to their states.\nConsider model (when using, say, Adam optimizer)\nclass Model(nn.Module):\n    def __init__(self):\n    super(Model, self).__init__()\n    self.p1 = nn.Linear(2,3, False)\n    self.p2 = nn.Linear(3,4, False)\n\nAfter saving, if the order in which the parameters are defined in the model changes i.e. if I change the class to have\nself.p2 = nn.Linear(3,4, False)\nself.p1 = nn.Linear(2,3, False)\n\nthe loaded optimizer's state for p1 will be mapped to p2 and vice-versa. I tried this and this indeed happens which is wrong and now training cannot proceed (step() will, rightly so, give an error).\nThe nn.Module class is robust to such behavior as it uses parameter names instead of id ordering.\nIMO the optimizer should also use parameter names instead of ids and relying on the ordering in which they are supplied to the optimizer when initializing.\nCorresponding PyTorch-Discuss post\n@soumith", "body": "In the optimizer's `param_groups['params']` the order of the parameters (in which they were given to the optimizer's init) matters. \r\nIn load_state_dict the snippet shows this : \r\n`id_map = {old_id: p for old_id, p in`\r\n`zip(chain(*(g['params'] for g in saved_groups)),\r\nchain(*(g['params'] for g in groups)))}`\r\n`state = {id_map.get(k, k): v for k, v in state_dict['state'].items()}`\r\n\r\nIf we change the order in which the parameters are given to the optimizer when loading, the code breaks as the `state` dict now incorrectly maps parameters to their states.\r\n\r\nConsider model (when using, say, Adam optimizer)\r\n\r\n```\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.p1 = nn.Linear(2,3, False)\r\n    self.p2 = nn.Linear(3,4, False)\r\n```\r\nAfter saving, if the order in which the parameters are defined in the model changes i.e. if I change the class to have \r\n```\r\nself.p2 = nn.Linear(3,4, False)\r\nself.p1 = nn.Linear(2,3, False)\r\n```\r\nthe loaded optimizer's state for p1 will be mapped to p2 and vice-versa. I tried this and this indeed happens which is wrong and now training cannot proceed (step() will, rightly so, give an error). \r\nThe `nn.Module` class is robust to such behavior as it uses parameter names instead of id ordering.\r\n\r\nIMO the optimizer should also use parameter names instead of ids and relying on the ordering in which they are supplied to the optimizer when initializing.\r\n\r\nCorresponding [PyTorch-Discuss post](https://discuss.pytorch.org/t/possible-issues-in-optimizer/2685)\r\n\r\n@soumith "}