{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8930", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8930/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8930/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8930/events", "html_url": "https://github.com/pytorch/pytorch/issues/8930", "id": 336064083, "node_id": "MDU6SXNzdWUzMzYwNjQwODM=", "number": 8930, "title": "Multiple GPU, Batch Normalization - RuntimeError: the derivative for 'running_mean' is not implemented", "user": {"login": "malisit", "id": 6311808, "node_id": "MDQ6VXNlcjYzMTE4MDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6311808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malisit", "html_url": "https://github.com/malisit", "followers_url": "https://api.github.com/users/malisit/followers", "following_url": "https://api.github.com/users/malisit/following{/other_user}", "gists_url": "https://api.github.com/users/malisit/gists{/gist_id}", "starred_url": "https://api.github.com/users/malisit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malisit/subscriptions", "organizations_url": "https://api.github.com/users/malisit/orgs", "repos_url": "https://api.github.com/users/malisit/repos", "events_url": "https://api.github.com/users/malisit/events{/privacy}", "received_events_url": "https://api.github.com/users/malisit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-06-27T03:25:04Z", "updated_at": "2018-07-19T19:51:49Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>I'm not able to use batchnorm when I run the code on multiple gpu. Below code works just fine when ran on single gpu.</p>\n<p>I tried this on 4.0 version as well, the result is same and also when I uncomment flatten_parameters() part.</p>\n<h2>Code example</h2>\n<pre><code>        self.inp1 = nn.Linear(4, 24)\n        self.lstm1 = nn.LSTM(24, 24, 2, dropout=0.05)\n        self.lstm1_bn = nn.BatchNorm1d(24)\n        self.out1 = nn.Linear(24, 16)\n</code></pre>\n<pre><code>        res = F.relu(self.inp1(x1))\n        #self.lstm1.flatten_parameters()\n        res, h__ = self.lstm1(res, h[0])\n        h[0] = torch.stack(h__)\n        res = F.tanh(res)\n        res = self.lstm1_bn(res.view(-1, 24))\n        res = res.unsqueeze(0)\n        res = F.max_pool2d(res, 1, stride=1)\n        res1 = F.relu(self.out1(res))\n</code></pre>\n<pre><code>Traceback (most recent call last):\n  File \"train_flood.py\", line 83, in &lt;module&gt;\n    outputs = model(input)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\n    raise output\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 53, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/msit/deep/parallel/model.py\", line 109, in forward\n    res = self.lstm1_bn(res.view(-1, 24))\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\", line 65, in forward\n    exponential_average_factor, self.eps)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 1229, in batch_norm\n    training, momentum, eps, torch.backends.cudnn.enabled\nRuntimeError: the derivative for 'running_mean' is not implemented\n\n</code></pre>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): source on conda</li>\n<li>Build command you used (if compiling from source): same as the instructions on README.md</li>\n<li>OS: RHEL 7</li>\n<li>PyTorch version: 0.5.0a0+290d20b</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 8.0.61</li>\n<li>GPU models and configuration: 2 X Tesla K80</li>\n<li>GCC version (if compiling from source): 4.8.5 20150623 (Red Hat 4.8.5-28)</li>\n<li>CMake version: 3.11.1</li>\n<li>Versions of any other relevant libraries:<br>\n[pip3] numpy (1.14.0)<br>\n[conda] magma-cuda80              2.3.0                         1    pytorch<br>\n[conda] torch                     0.5.0a0+290d20b           </li>\n</ul>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nI'm not able to use batchnorm when I run the code on multiple gpu. Below code works just fine when ran on single gpu.\nI tried this on 4.0 version as well, the result is same and also when I uncomment flatten_parameters() part.\nCode example\n        self.inp1 = nn.Linear(4, 24)\n        self.lstm1 = nn.LSTM(24, 24, 2, dropout=0.05)\n        self.lstm1_bn = nn.BatchNorm1d(24)\n        self.out1 = nn.Linear(24, 16)\n\n        res = F.relu(self.inp1(x1))\n        #self.lstm1.flatten_parameters()\n        res, h__ = self.lstm1(res, h[0])\n        h[0] = torch.stack(h__)\n        res = F.tanh(res)\n        res = self.lstm1_bn(res.view(-1, 24))\n        res = res.unsqueeze(0)\n        res = F.max_pool2d(res, 1, stride=1)\n        res1 = F.relu(self.out1(res))\n\nTraceback (most recent call last):\n  File \"train_flood.py\", line 83, in <module>\n    outputs = model(input)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\n    raise output\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 53, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/msit/deep/parallel/model.py\", line 109, in forward\n    res = self.lstm1_bn(res.view(-1, 24))\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\", line 65, in forward\n    exponential_average_factor, self.eps)\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 1229, in batch_norm\n    training, momentum, eps, torch.backends.cudnn.enabled\nRuntimeError: the derivative for 'running_mean' is not implemented\n\n\nSystem Info\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): source on conda\nBuild command you used (if compiling from source): same as the instructions on README.md\nOS: RHEL 7\nPyTorch version: 0.5.0a0+290d20b\nPython version: 3.6\nCUDA/cuDNN version: 8.0.61\nGPU models and configuration: 2 X Tesla K80\nGCC version (if compiling from source): 4.8.5 20150623 (Red Hat 4.8.5-28)\nCMake version: 3.11.1\nVersions of any other relevant libraries:\n[pip3] numpy (1.14.0)\n[conda] magma-cuda80              2.3.0                         1    pytorch\n[conda] torch                     0.5.0a0+290d20b", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\nI'm not able to use batchnorm when I run the code on multiple gpu. Below code works just fine when ran on single gpu.\r\n\r\nI tried this on 4.0 version as well, the result is same and also when I uncomment flatten_parameters() part.\r\n\r\n## Code example\r\n\r\n```\r\n        self.inp1 = nn.Linear(4, 24)\r\n        self.lstm1 = nn.LSTM(24, 24, 2, dropout=0.05)\r\n        self.lstm1_bn = nn.BatchNorm1d(24)\r\n        self.out1 = nn.Linear(24, 16)\r\n```\r\n\r\n\r\n```\r\n        res = F.relu(self.inp1(x1))\r\n        #self.lstm1.flatten_parameters()\r\n        res, h__ = self.lstm1(res, h[0])\r\n        h[0] = torch.stack(h__)\r\n        res = F.tanh(res)\r\n        res = self.lstm1_bn(res.view(-1, 24))\r\n        res = res.unsqueeze(0)\r\n        res = F.max_pool2d(res, 1, stride=1)\r\n        res1 = F.relu(self.out1(res))\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_flood.py\", line 83, in <module>\r\n    outputs = model(input)\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\r\n    raise output\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 53, in _worker\r\n    output = module(*input, **kwargs)\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/msit/deep/parallel/model.py\", line 109, in forward\r\n    res = self.lstm1_bn(res.view(-1, 24))\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\", line 65, in forward\r\n    exponential_average_factor, self.eps)\r\n  File \"/home/msit/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 1229, in batch_norm\r\n    training, momentum, eps, torch.backends.cudnn.enabled\r\nRuntimeError: the derivative for 'running_mean' is not implemented\r\n\r\n```\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): source on conda\r\n- Build command you used (if compiling from source): same as the instructions on README.md\r\n- OS: RHEL 7\r\n- PyTorch version: 0.5.0a0+290d20b\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 8.0.61\r\n- GPU models and configuration: 2 X Tesla K80\r\n- GCC version (if compiling from source): 4.8.5 20150623 (Red Hat 4.8.5-28)\r\n- CMake version: 3.11.1\r\n- Versions of any other relevant libraries:\r\n[pip3] numpy (1.14.0)\r\n[conda] magma-cuda80              2.3.0                         1    pytorch\r\n[conda] torch                     0.5.0a0+290d20b           <pip>\r\n"}