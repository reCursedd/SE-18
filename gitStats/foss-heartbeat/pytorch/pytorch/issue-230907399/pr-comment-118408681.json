{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/118408681", "pull_request_review_id": 40197574, "id": 118408681, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExODQwODY4MQ==", "diff_hunk": "@@ -1,71 +1,74 @@\n import torch\n \n from ..function import Function\n+from ..variable import Variable\n \n \n class Diag(Function):\n \n-    def __init__(self, diagonal_idx=0):\n-        super(Diag, self).__init__()\n-        self.diagonal_idx = diagonal_idx\n+    @staticmethod\n+    def forward(ctx, input, diagonal_idx=0):\n+        ctx.diagonal_idx = diagonal_idx\n+        return input.diag(ctx.diagonal_idx)\n \n-    def forward(self, input):\n-        return input.diag(self.diagonal_idx)\n-\n-    def backward(self, grad_output):\n-        return grad_output.diag(self.diagonal_idx)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.diag(ctx.diagonal_idx), None\n \n \n class Tril(Function):\n \n-    def __init__(self, diagonal_idx=0):\n-        super(Tril, self).__init__()\n-        self.diagonal_idx = diagonal_idx\n-\n-    def forward(self, input):\n-        return input.tril(self.diagonal_idx)\n+    @staticmethod\n+    def forward(ctx, input, diagonal_idx=0):\n+        ctx.diagonal_idx = diagonal_idx\n+        return input.tril(ctx.diagonal_idx)\n \n-    def backward(self, grad_output):\n-        return grad_output.tril(self.diagonal_idx)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.tril(ctx.diagonal_idx), None\n \n \n class Triu(Function):\n \n-    def __init__(self, diagonal_idx=0):\n-        super(Triu, self).__init__()\n-        self.diagonal_idx = diagonal_idx\n-\n-    def forward(self, input):\n-        return input.triu(self.diagonal_idx)\n+    @staticmethod\n+    def forward(ctx, input, diagnoal_idx=0):\n+        ctx.diagonal_idx = diagnoal_idx\n+        return input.triu(ctx.diagonal_idx)\n \n-    def backward(self, grad_output):\n-        return grad_output.triu(self.diagonal_idx)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.triu(ctx.diagonal_idx), None\n \n \n class Trace(Function):\n \n-    def forward(self, input):\n-        self.isize = input.size()\n-        return input.new((input.trace(),))\n-\n-    def backward(self, grad_output):\n-        isize = self.isize\n-        grad_input = grad_output.new(isize).zero_()\n-        grad_input.view(-1)[::(isize[1] + 1)] = grad_output[0]\n+    @staticmethod\n+    def forward(ctx, input):\n+        ctx.isize = input.size()\n+        return input.new((input.trace(), ))\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        isize = ctx.isize\n+        mask = grad_output.data.new(isize).zero_()\n+        mask.view(-1)[::(isize[1] + 1)] = 1\n+        mask = Variable(mask)\n+        grad_output_expand = grad_output.view(1, 1).expand(isize)\n+        grad_input = mask * grad_output_expand", "path": "torch/autograd/_functions/linalg.py", "position": null, "original_position": 89, "commit_id": "2546d592aee0f10cb73960e820cde532167e35c2", "original_commit_id": "ee4786630655578b251f7bb32a508d0a87df88f4", "user": {"login": "chenyuntc", "id": 9301117, "node_id": "MDQ6VXNlcjkzMDExMTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/9301117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenyuntc", "html_url": "https://github.com/chenyuntc", "followers_url": "https://api.github.com/users/chenyuntc/followers", "following_url": "https://api.github.com/users/chenyuntc/following{/other_user}", "gists_url": "https://api.github.com/users/chenyuntc/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenyuntc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenyuntc/subscriptions", "organizations_url": "https://api.github.com/users/chenyuntc/orgs", "repos_url": "https://api.github.com/users/chenyuntc/repos", "events_url": "https://api.github.com/users/chenyuntc/events{/privacy}", "received_events_url": "https://api.github.com/users/chenyuntc/received_events", "type": "User", "site_admin": false}, "body": "It will raise error `RuntimeError: inconsistent tensor size`\r\nIn fact, I tried to implement it using the old way at first.\r\n```python\r\nclass Trace(Function):\r\n\r\n    @staticmethod\r\n    def forward(ctx, input):\r\n        ctx.isize = input.size()\r\n        return input.new((input.trace(), ))\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        isize = ctx.isize\r\n        min_size = min(isize)\r\n        grad_input = Variable(grad_output.data.new(isize).zero_())\r\n        grad_input.view(-1)[::(isize[1] + 1)] = grad_output.expand(min_size)\r\n        return grad_input\r\n```\r\nIt raise another error\r\n```\r\nRuntimeError: in-place operations can be only used on variables that don't share storage with any other variables, but detected that there are 2 objects sharing it\r\n```\r\n", "created_at": "2017-05-25T02:49:14Z", "updated_at": "2018-11-23T15:33:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/1638#discussion_r118408681", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1638", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/118408681"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1638#discussion_r118408681"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1638"}}, "body_html": "<p>It will raise error <code>RuntimeError: inconsistent tensor size</code><br>\nIn fact, I tried to implement it using the old way at first.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Trace</span>(<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">input</span>):\n        ctx.isize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size()\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">input</span>.new((<span class=\"pl-c1\">input</span>.trace(), ))\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        isize <span class=\"pl-k\">=</span> ctx.isize\n        min_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">min</span>(isize)\n        grad_input <span class=\"pl-k\">=</span> Variable(grad_output.data.new(isize).zero_())\n        grad_input.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)[::(isize[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)] <span class=\"pl-k\">=</span> grad_output.expand(min_size)\n        <span class=\"pl-k\">return</span> grad_input</pre></div>\n<p>It raise another error</p>\n<pre><code>RuntimeError: in-place operations can be only used on variables that don't share storage with any other variables, but detected that there are 2 objects sharing it\n</code></pre>", "body_text": "It will raise error RuntimeError: inconsistent tensor size\nIn fact, I tried to implement it using the old way at first.\nclass Trace(Function):\n\n    @staticmethod\n    def forward(ctx, input):\n        ctx.isize = input.size()\n        return input.new((input.trace(), ))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        isize = ctx.isize\n        min_size = min(isize)\n        grad_input = Variable(grad_output.data.new(isize).zero_())\n        grad_input.view(-1)[::(isize[1] + 1)] = grad_output.expand(min_size)\n        return grad_input\nIt raise another error\nRuntimeError: in-place operations can be only used on variables that don't share storage with any other variables, but detected that there are 2 objects sharing it", "in_reply_to_id": 118332961}