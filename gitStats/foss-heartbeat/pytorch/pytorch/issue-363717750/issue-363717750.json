{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12060", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12060/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12060/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12060/events", "html_url": "https://github.com/pytorch/pytorch/issues/12060", "id": 363717750, "node_id": "MDU6SXNzdWUzNjM3MTc3NTA=", "number": 12060, "title": "cuda test hangs if GPUs in Exclusive Process mode", "user": {"login": "hartb", "id": 18429659, "node_id": "MDQ6VXNlcjE4NDI5NjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/18429659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hartb", "html_url": "https://github.com/hartb", "followers_url": "https://api.github.com/users/hartb/followers", "following_url": "https://api.github.com/users/hartb/following{/other_user}", "gists_url": "https://api.github.com/users/hartb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hartb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hartb/subscriptions", "organizations_url": "https://api.github.com/users/hartb/orgs", "repos_url": "https://api.github.com/users/hartb/repos", "events_url": "https://api.github.com/users/hartb/events{/privacy}", "received_events_url": "https://api.github.com/users/hartb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-25T18:47:26Z", "updated_at": "2018-10-01T17:47:49Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Problem was seen on ppc64le, but should happen elsewhere as well (at<br>\nleast on other linux platforms).</p>\n<p>Problem is present since at least 0.4.1 (probably earlier) and still<br>\nexists.</p>\n<p>Problem is seen with Anaconda's python 3.6; haven't tested with other<br>\ninterpeters.</p>\n<p>This is an issue in the test script, rather than with PyTorch itself.</p>\n<p>Test <code>test_multinomial_invalid_probs_cuda</code> will hang if the system's<br>\nGPUs are in Exclusive Process mode, rather than in Default mode. See:<br>\n<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-modes\" rel=\"nofollow\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-modes</a></p>\n<p>The problem occurs when the <code>test_multinomial_invalid_probs_cuda</code> test<br>\nspawns child processes. Those fail to access the GPU because the main<br>\nprocess already has an active context. The child processes fail at the<br>\ntop of <code>test_cuda.py</code> while trying to check for MAGMA support:</p>\n<pre><code>if TEST_CUDA:\n    torch.ones(1).cuda()  # has_magma shows up after cuda is initialized\n    TEST_MAGMA = torch.cuda.has_magma\n</code></pre>\n<p>That code raises a RuntimeException:</p>\n<pre><code>...\n  File \"/tmp/tmp.ymeGX7otav/test/test_cuda.py\", line 34, in\n    torch.ones(1).cuda() # has_magma shows up after cuda is initialized\nRuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable\n</code></pre>\n<p>That then repeats over and over forever.</p>\n<p>Here's a test program that demonstrates the problem:</p>\n<pre><code>import multiprocessing as mp\nimport os\n\n# True if you have GPUs in Exclusive mode; False to simulate\nif True:\n        import torch\n        torch.ones(1).cuda()\nelse:\n        if mp.current_process().name != 'MainProcess':\n                raise RuntimeError('simulate Exclusive Mode exception')\n\ndef _spawn_method(method, ctype):\n        ctx = mp.get_context(ctype)\n        with ctx.Pool(1) as pool:\n                return pool.map(method, [1, 2, 3])\n\ndef f(arg):\n        return 'foo'\n\nif __name__ == '__main__':\n        print('From fork: {}'.format(_spawn_method(f, 'fork')))\n        print('From spawn: {}'.format(_spawn_method(f, 'spawn')))\n</code></pre>\n<p>Note the problem does not occur when <code>fork</code> is used as the<br>\nmultiprocessing context method, only when <code>spawn</code> is used.</p>\n<p>I'm not exactly sure what causes the multiprocessing code to get stuck<br>\nhere, but I suspect it's similar to the issue described at:</p>\n<p><a href=\"http://jessenoller.com/2009/01/08/multiprocessingpool-and-keyboardinterrupt/\" rel=\"nofollow\">http://jessenoller.com/2009/01/08/multiprocessingpool-and-keyboardinterrupt/</a></p>\n<p>I'm also not sure of the best fix. Some possibilities are:</p>\n<ul>\n<li>\n<p>Switch the context method to <code>fork</code>? But is that available on Windows?</p>\n</li>\n<li>\n<p>Add a <code>try</code>/<code>except</code> in the code that's testing for MAGMA? But what do<br>\ndo there at both prevents the hang and doesn't disturb the rest of<br>\n<code>test_cuda</code> if some other unrelated failure occurs?</p>\n</li>\n<li>\n<p>Detect Exclusive Process mode and skip this test in that case? Compute<br>\nmode is included in the CUDA Device Properties, but that field isn't<br>\ncaptured / exposed by PyTorch now. Would require work in<br>\n<code>torch/csrc/cuda/Module.cpp</code>.</p>\n</li>\n<li>\n<p>Document a restriction, recommendation, or caution for Exclusive Mode.</p>\n</li>\n</ul>", "body_text": "Problem was seen on ppc64le, but should happen elsewhere as well (at\nleast on other linux platforms).\nProblem is present since at least 0.4.1 (probably earlier) and still\nexists.\nProblem is seen with Anaconda's python 3.6; haven't tested with other\ninterpeters.\nThis is an issue in the test script, rather than with PyTorch itself.\nTest test_multinomial_invalid_probs_cuda will hang if the system's\nGPUs are in Exclusive Process mode, rather than in Default mode. See:\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-modes\nThe problem occurs when the test_multinomial_invalid_probs_cuda test\nspawns child processes. Those fail to access the GPU because the main\nprocess already has an active context. The child processes fail at the\ntop of test_cuda.py while trying to check for MAGMA support:\nif TEST_CUDA:\n    torch.ones(1).cuda()  # has_magma shows up after cuda is initialized\n    TEST_MAGMA = torch.cuda.has_magma\n\nThat code raises a RuntimeException:\n...\n  File \"/tmp/tmp.ymeGX7otav/test/test_cuda.py\", line 34, in\n    torch.ones(1).cuda() # has_magma shows up after cuda is initialized\nRuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable\n\nThat then repeats over and over forever.\nHere's a test program that demonstrates the problem:\nimport multiprocessing as mp\nimport os\n\n# True if you have GPUs in Exclusive mode; False to simulate\nif True:\n        import torch\n        torch.ones(1).cuda()\nelse:\n        if mp.current_process().name != 'MainProcess':\n                raise RuntimeError('simulate Exclusive Mode exception')\n\ndef _spawn_method(method, ctype):\n        ctx = mp.get_context(ctype)\n        with ctx.Pool(1) as pool:\n                return pool.map(method, [1, 2, 3])\n\ndef f(arg):\n        return 'foo'\n\nif __name__ == '__main__':\n        print('From fork: {}'.format(_spawn_method(f, 'fork')))\n        print('From spawn: {}'.format(_spawn_method(f, 'spawn')))\n\nNote the problem does not occur when fork is used as the\nmultiprocessing context method, only when spawn is used.\nI'm not exactly sure what causes the multiprocessing code to get stuck\nhere, but I suspect it's similar to the issue described at:\nhttp://jessenoller.com/2009/01/08/multiprocessingpool-and-keyboardinterrupt/\nI'm also not sure of the best fix. Some possibilities are:\n\n\nSwitch the context method to fork? But is that available on Windows?\n\n\nAdd a try/except in the code that's testing for MAGMA? But what do\ndo there at both prevents the hang and doesn't disturb the rest of\ntest_cuda if some other unrelated failure occurs?\n\n\nDetect Exclusive Process mode and skip this test in that case? Compute\nmode is included in the CUDA Device Properties, but that field isn't\ncaptured / exposed by PyTorch now. Would require work in\ntorch/csrc/cuda/Module.cpp.\n\n\nDocument a restriction, recommendation, or caution for Exclusive Mode.", "body": "Problem was seen on ppc64le, but should happen elsewhere as well (at\r\nleast on other linux platforms).\r\n\r\nProblem is present since at least 0.4.1 (probably earlier) and still\r\nexists.\r\n\r\nProblem is seen with Anaconda's python 3.6; haven't tested with other\r\ninterpeters.\r\n\r\nThis is an issue in the test script, rather than with PyTorch itself.\r\n\r\n\r\nTest `test_multinomial_invalid_probs_cuda` will hang if the system's\r\nGPUs are in Exclusive Process mode, rather than in Default mode. See:\r\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/#compute-modes\r\n\r\n\r\nThe problem occurs when the `test_multinomial_invalid_probs_cuda` test\r\nspawns child processes. Those fail to access the GPU because the main\r\nprocess already has an active context. The child processes fail at the\r\ntop of `test_cuda.py` while trying to check for MAGMA support:\r\n\r\n```\r\nif TEST_CUDA:\r\n    torch.ones(1).cuda()  # has_magma shows up after cuda is initialized\r\n    TEST_MAGMA = torch.cuda.has_magma\r\n```\r\n\r\nThat code raises a RuntimeException:\r\n\r\n```\r\n...\r\n  File \"/tmp/tmp.ymeGX7otav/test/test_cuda.py\", line 34, in\r\n    torch.ones(1).cuda() # has_magma shows up after cuda is initialized\r\nRuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable\r\n```\r\n\r\nThat then repeats over and over forever.\r\n\r\n\r\nHere's a test program that demonstrates the problem:\r\n\r\n```\r\nimport multiprocessing as mp\r\nimport os\r\n\r\n# True if you have GPUs in Exclusive mode; False to simulate\r\nif True:\r\n        import torch\r\n        torch.ones(1).cuda()\r\nelse:\r\n        if mp.current_process().name != 'MainProcess':\r\n                raise RuntimeError('simulate Exclusive Mode exception')\r\n\r\ndef _spawn_method(method, ctype):\r\n        ctx = mp.get_context(ctype)\r\n        with ctx.Pool(1) as pool:\r\n                return pool.map(method, [1, 2, 3])\r\n\r\ndef f(arg):\r\n        return 'foo'\r\n\r\nif __name__ == '__main__':\r\n        print('From fork: {}'.format(_spawn_method(f, 'fork')))\r\n        print('From spawn: {}'.format(_spawn_method(f, 'spawn')))\r\n```\r\n\r\nNote the problem does not occur when `fork` is used as the\r\nmultiprocessing context method, only when `spawn` is used.\r\n\r\nI'm not exactly sure what causes the multiprocessing code to get stuck\r\nhere, but I suspect it's similar to the issue described at:\r\n\r\nhttp://jessenoller.com/2009/01/08/multiprocessingpool-and-keyboardinterrupt/\r\n\r\n\r\nI'm also not sure of the best fix. Some possibilities are:\r\n\r\n- Switch the context method to `fork`? But is that available on Windows?\r\n\r\n- Add a `try`/`except` in the code that's testing for MAGMA? But what do\r\n  do there at both prevents the hang and doesn't disturb the rest of\r\n  `test_cuda` if some other unrelated failure occurs?\r\n\r\n- Detect Exclusive Process mode and skip this test in that case? Compute\r\n  mode is included in the CUDA Device Properties, but that field isn't\r\n  captured / exposed by PyTorch now. Would require work in\r\n  `torch/csrc/cuda/Module.cpp`.\r\n\r\n- Document a restriction, recommendation, or caution for Exclusive Mode.\r\n"}