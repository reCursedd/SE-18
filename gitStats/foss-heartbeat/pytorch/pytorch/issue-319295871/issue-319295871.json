{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7146", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7146/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7146/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7146/events", "html_url": "https://github.com/pytorch/pytorch/pull/7146", "id": 319295871, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg1MjUyMTQy", "number": 7146, "title": "Support getting a shared CUDA tensor on same process", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-01T19:15:30Z", "updated_at": "2018-11-23T15:43:32Z", "closed_at": "2018-05-02T21:44:08Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7146", "html_url": "https://github.com/pytorch/pytorch/pull/7146", "diff_url": "https://github.com/pytorch/pytorch/pull/7146.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7146.patch"}, "body_html": "<p>Attempting to <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #7096.\">fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"318928255\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7096\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7096/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7096\">#7096</a></p>\n<p>Right now, if one attempts to share a CUDA tensor and get it from a<br>\nqueue from the same process it was saved in, a crash can occur.</p>\n<p>Here is some example code to demonstrate this:</p>\n<pre><code>import torch\nimport torch.multiprocessing as mp\nimport numpy.random as npr\n\nmp.set_start_method('spawn')\n\nq = mp.Queue()\nx = torch.randn(1000).cuda()\ntensor = torch.ones(10).cuda()\n\nq.put(tensor)\nout = q.get()\nprint(out)\n</code></pre>\n<p>One of two behaviors can occur:</p>\n<ul>\n<li>Crashing with an error that some \"CUDA arguments are incorrect\"</li>\n<li>The <code>out</code> tensor is valid but contains incorrect data (zeros instead<br>\nof ones).</li>\n</ul>\n<p>On master the first behavior occurs but I've noticed the second behavior<br>\nhappen on previous commits.</p>\n<p>This occurs because caching of the shared storages happens differently<br>\ndepending on the process. Call the process where <code>tensor</code> is created<br>\nthe \"originating process\". When a process grabs a tensor from the queue<br>\n<code>q</code>, there are two cases:</p>\n<ol>\n<li>The process is the \"originating process\". The cached storage is the<br>\ndesired storage.</li>\n<li>The process is not the \"originating process\". The cached storage (if<br>\nit has been cached) is a storage that points to the base of the<br>\nallocation. One needs to add the offset to this storage to retrieve the<br>\ndesired storage.</li>\n</ol>\n<p>Case (2) is OK, but the code doesn't handle case (1) right now (the<br>\nrebuilding_storage_cuda code attempts to add the offset to the desired<br>\nstorage, leading to an incorrect storage pointer).</p>\n<p>The fix in this PR is to handle case (1) separately from case (2) by:</p>\n<ul>\n<li>Including the PID of the \"originating process\" when reducing a storage</li>\n<li>Compare the current proc's PID to the PID of the \"originating process\"<br>\nto determine which case we're in and handle it accordingly.</li>\n</ul>\n<p>One alternative solution would be to not have this discrepancy in caching<br>\nstorages depending on the process, ie, \"the cached (CUDA) storage should always<br>\npoint to the base of the allocation\" would be made into an invariant.<br>\nI wasn't sure how to work with this because the \"originating process\"<br>\nwould need to save a new storage into the cache that points to the base<br>\nof the allocation. There's nothing else alive in Python-land that holds<br>\na reference to this new storage so it would be freed immediately when it<br>\ngoes out of scope.</p>\n<p>Another alternative is to just ban this behavior outright, ie, \"the<br>\noriginating process should never be allowed to rebuild a storage that<br>\ncame from a tensor it created\". Saving the pids in the <code>reduce_storage</code><br>\nstep would then be used for error checking when the storage is being<br>\nrebuilt.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> let me know what you think</p>", "body_text": "Attempting to fix #7096\nRight now, if one attempts to share a CUDA tensor and get it from a\nqueue from the same process it was saved in, a crash can occur.\nHere is some example code to demonstrate this:\nimport torch\nimport torch.multiprocessing as mp\nimport numpy.random as npr\n\nmp.set_start_method('spawn')\n\nq = mp.Queue()\nx = torch.randn(1000).cuda()\ntensor = torch.ones(10).cuda()\n\nq.put(tensor)\nout = q.get()\nprint(out)\n\nOne of two behaviors can occur:\n\nCrashing with an error that some \"CUDA arguments are incorrect\"\nThe out tensor is valid but contains incorrect data (zeros instead\nof ones).\n\nOn master the first behavior occurs but I've noticed the second behavior\nhappen on previous commits.\nThis occurs because caching of the shared storages happens differently\ndepending on the process. Call the process where tensor is created\nthe \"originating process\". When a process grabs a tensor from the queue\nq, there are two cases:\n\nThe process is the \"originating process\". The cached storage is the\ndesired storage.\nThe process is not the \"originating process\". The cached storage (if\nit has been cached) is a storage that points to the base of the\nallocation. One needs to add the offset to this storage to retrieve the\ndesired storage.\n\nCase (2) is OK, but the code doesn't handle case (1) right now (the\nrebuilding_storage_cuda code attempts to add the offset to the desired\nstorage, leading to an incorrect storage pointer).\nThe fix in this PR is to handle case (1) separately from case (2) by:\n\nIncluding the PID of the \"originating process\" when reducing a storage\nCompare the current proc's PID to the PID of the \"originating process\"\nto determine which case we're in and handle it accordingly.\n\nOne alternative solution would be to not have this discrepancy in caching\nstorages depending on the process, ie, \"the cached (CUDA) storage should always\npoint to the base of the allocation\" would be made into an invariant.\nI wasn't sure how to work with this because the \"originating process\"\nwould need to save a new storage into the cache that points to the base\nof the allocation. There's nothing else alive in Python-land that holds\na reference to this new storage so it would be freed immediately when it\ngoes out of scope.\nAnother alternative is to just ban this behavior outright, ie, \"the\noriginating process should never be allowed to rebuild a storage that\ncame from a tensor it created\". Saving the pids in the reduce_storage\nstep would then be used for error checking when the storage is being\nrebuilt.\ncc @colesbury let me know what you think", "body": "Attempting to fix #7096 \r\n\r\nRight now, if one attempts to share a CUDA tensor and get it from a\r\nqueue from the same process it was saved in, a crash can occur.\r\n\r\nHere is some example code to demonstrate this:\r\n\r\n```\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport numpy.random as npr\r\n\r\nmp.set_start_method('spawn')\r\n\r\nq = mp.Queue()\r\nx = torch.randn(1000).cuda()\r\ntensor = torch.ones(10).cuda()\r\n\r\nq.put(tensor)\r\nout = q.get()\r\nprint(out)\r\n```\r\n\r\nOne of two behaviors can occur:\r\n- Crashing with an error that some \"CUDA arguments are incorrect\"\r\n- The `out` tensor is valid but contains incorrect data (zeros instead\r\nof ones).\r\n\r\nOn master the first behavior occurs but I've noticed the second behavior\r\nhappen on previous commits.\r\n\r\nThis occurs because caching of the shared storages happens differently\r\ndepending on the process. Call the process where `tensor` is created\r\nthe \"originating process\". When a process grabs a tensor from the queue\r\n`q`, there are two cases:\r\n1) The process is the \"originating process\". The cached storage is the\r\ndesired storage.\r\n2) The process is not the \"originating process\". The cached storage (if\r\nit has been cached) is a storage that points to the base of the\r\nallocation. One needs to add the offset to this storage to retrieve the\r\ndesired storage.\r\n\r\nCase (2) is OK, but the code doesn't handle case (1) right now (the\r\nrebuilding_storage_cuda code attempts to add the offset to the desired\r\nstorage, leading to an incorrect storage pointer).\r\n\r\nThe fix in this PR is to handle case (1) separately from case (2) by:\r\n- Including the PID of the \"originating process\" when reducing a storage\r\n- Compare the current proc's PID to the PID of the \"originating process\"\r\n  to determine which case we're in and handle it accordingly.\r\n\r\nOne alternative solution would be to not have this discrepancy in caching\r\nstorages depending on the process, ie, \"the cached (CUDA) storage should always\r\npoint to the base of the allocation\" would be made into an invariant.\r\nI wasn't sure how to work with this because the \"originating process\"\r\nwould need to save a new storage into the cache that points to the base\r\nof the allocation. There's nothing else alive in Python-land that holds\r\na reference to this new storage so it would be freed immediately when it\r\ngoes out of scope.\r\n\r\nAnother alternative is to just ban this behavior outright, ie, \"the\r\noriginating process should never be allowed to rebuild a storage that\r\ncame from a tensor it created\". Saving the pids in the `reduce_storage`\r\nstep would then be used for error checking when the storage is being\r\nrebuilt.\r\n\r\ncc @colesbury let me know what you think\r\n"}