{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386126741", "html_url": "https://github.com/pytorch/pytorch/pull/7146#issuecomment-386126741", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7146", "id": 386126741, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjEyNjc0MQ==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-02T21:26:36Z", "updated_at": "2018-05-02T21:26:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Actually, the approach of \"saving the storage as-is if we're in the process that created the storage\" doesn't completely work. Here's why:<br>\nLet's say this is our allocation block:</p>\n<pre><code>|               [storage1]    [storage2]    |\n^\nbase\n</code></pre>\n<p>Now, let's say we share <code>storage1</code> and <code>storage2</code>, in that order:</p>\n<ul>\n<li>When <code>storage1</code> is put into a <code>mp.Queue</code>, we cache a reference to it using the <code>handle</code> to the entire allocation block.</li>\n<li>When <code>storage2</code> is put into the <code>mp.Queue</code>, we cache a reference to it using the <code>handle</code> to the entire allocation block, replacing the reference to <code>storage1</code>.</li>\n</ul>\n<p>Now, let's say another process sends <code>storage1</code> back to this process (that created <code>storage1</code>). It'll grab the storage for <code>storage2</code> and use that instead, which is a problem.</p>\n<p>This approach can work if we tweak it so that each process saves all cuda storages that it creates into a separate cache. I'll try doing something like that in another PR.</p>", "body_text": "Actually, the approach of \"saving the storage as-is if we're in the process that created the storage\" doesn't completely work. Here's why:\nLet's say this is our allocation block:\n|               [storage1]    [storage2]    |\n^\nbase\n\nNow, let's say we share storage1 and storage2, in that order:\n\nWhen storage1 is put into a mp.Queue, we cache a reference to it using the handle to the entire allocation block.\nWhen storage2 is put into the mp.Queue, we cache a reference to it using the handle to the entire allocation block, replacing the reference to storage1.\n\nNow, let's say another process sends storage1 back to this process (that created storage1). It'll grab the storage for storage2 and use that instead, which is a problem.\nThis approach can work if we tweak it so that each process saves all cuda storages that it creates into a separate cache. I'll try doing something like that in another PR.", "body": "Actually, the approach of \"saving the storage as-is if we're in the process that created the storage\" doesn't completely work. Here's why:\r\nLet's say this is our allocation block:\r\n```\r\n|               [storage1]    [storage2]    |\r\n^\r\nbase\r\n```\r\nNow, let's say we share `storage1` and `storage2`, in that order:\r\n- When `storage1` is put into a `mp.Queue`, we cache a reference to it using the `handle` to the entire allocation block. \r\n- When `storage2` is put into the `mp.Queue`, we cache a reference to it using the `handle` to the entire allocation block, replacing the reference to `storage1`.\r\n\r\nNow, let's say another process sends `storage1` back to this process (that created `storage1`). It'll grab the storage for `storage2` and use that instead, which is a problem.\r\n\r\nThis approach can work if we tweak it so that each process saves all cuda storages that it creates into a separate cache. I'll try doing something like that in another PR."}