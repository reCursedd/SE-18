{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8637", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8637/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8637/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8637/events", "html_url": "https://github.com/pytorch/pytorch/issues/8637", "id": 333510821, "node_id": "MDU6SXNzdWUzMzM1MTA4MjE=", "number": 8637, "title": "Issue for DataParallel", "user": {"login": "BestSonny", "id": 6027713, "node_id": "MDQ6VXNlcjYwMjc3MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6027713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BestSonny", "html_url": "https://github.com/BestSonny", "followers_url": "https://api.github.com/users/BestSonny/followers", "following_url": "https://api.github.com/users/BestSonny/following{/other_user}", "gists_url": "https://api.github.com/users/BestSonny/gists{/gist_id}", "starred_url": "https://api.github.com/users/BestSonny/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BestSonny/subscriptions", "organizations_url": "https://api.github.com/users/BestSonny/orgs", "repos_url": "https://api.github.com/users/BestSonny/repos", "events_url": "https://api.github.com/users/BestSonny/events{/privacy}", "received_events_url": "https://api.github.com/users/BestSonny/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-19T03:56:03Z", "updated_at": "2018-09-10T08:04:43Z", "closed_at": "2018-06-25T18:06:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When I tried to use multi-gpu trainning.</p>\n<ol>\n<li>If I wrap the model forward function as follows:</li>\n</ol>\n<pre><code>def __init__(...):\n        self.operation_function = self._gaussian\n\ndef forward(self, x, z):\n        output = self.operation_function(x, z)\n\ndef _gaussian(self, x, z):\n        Real forward codes\n        ...\n</code></pre>\n<p>Then</p>\n<pre><code>gpu_num = torch.cuda.device_count()\nprint('GPU NUM: {:2d}'.format(gpu_num))\nif gpu_num &gt; 1:\n    model = torch.nn.DataParallel(model, list(range(gpu_num))).cuda()\n</code></pre>\n<p>I get one error during the multi-gpu trainning</p>\n<pre><code>File \"/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\n    raise output\nRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)\n</code></pre>\n<ol start=\"2\">\n<li>If I directly put all codes of _gaussian function under forward function. The multi-gpu trainning works</li>\n</ol>\n<pre><code>def forward(self, x, z):\n        Real forward codes\n        ...\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> Do you have any comments about this?</p>", "body_text": "When I tried to use multi-gpu trainning.\n\nIf I wrap the model forward function as follows:\n\ndef __init__(...):\n        self.operation_function = self._gaussian\n\ndef forward(self, x, z):\n        output = self.operation_function(x, z)\n\ndef _gaussian(self, x, z):\n        Real forward codes\n        ...\n\nThen\ngpu_num = torch.cuda.device_count()\nprint('GPU NUM: {:2d}'.format(gpu_num))\nif gpu_num > 1:\n    model = torch.nn.DataParallel(model, list(range(gpu_num))).cuda()\n\nI get one error during the multi-gpu trainning\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\n    raise output\nRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)\n\n\nIf I directly put all codes of _gaussian function under forward function. The multi-gpu trainning works\n\ndef forward(self, x, z):\n        Real forward codes\n        ...\n\n@soumith Do you have any comments about this?", "body": "When I tried to use multi-gpu trainning.\r\n\r\n1) If I wrap the model forward function as follows:\r\n\r\n```\r\ndef __init__(...):\r\n        self.operation_function = self._gaussian\r\n\r\ndef forward(self, x, z):\r\n        output = self.operation_function(x, z)\r\n\r\ndef _gaussian(self, x, z):\r\n        Real forward codes\r\n        ...\r\n```\r\nThen \r\n```\r\ngpu_num = torch.cuda.device_count()\r\nprint('GPU NUM: {:2d}'.format(gpu_num))\r\nif gpu_num > 1:\r\n    model = torch.nn.DataParallel(model, list(range(gpu_num))).cuda()\r\n```\r\nI get one error during the multi-gpu trainning \r\n```\r\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py\", line 468, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\r\n    raise output\r\nRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)\r\n```\r\n\r\n2) If I directly put all codes of _gaussian function under forward function. The multi-gpu trainning works\r\n```\r\ndef forward(self, x, z):\r\n        Real forward codes\r\n        ...\r\n```\r\n\r\n@soumith Do you have any comments about this?"}