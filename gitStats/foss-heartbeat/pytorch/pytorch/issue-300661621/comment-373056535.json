{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373056535", "html_url": "https://github.com/pytorch/pytorch/pull/5433#issuecomment-373056535", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5433", "id": 373056535, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzA1NjUzNQ==", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T15:11:16Z", "updated_at": "2018-03-14T15:11:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Update 2: I managed to get a faster version by calling the blas primitives directly (like we already do for the backward pass). The implementation is a bit more specialized but it seems like a good trade-off in this case. In fact it's even faster than the first version I posted, here's the new timing I have for the forward pass:</p>\n<pre><code>NUMEXPR_NUM_THREADS=1 MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 taskset -c 0 python benchmark.py\n\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nruns: 10000     number of indices: 2000 maximum number of bags: 200     maximum bag size: 30\n\n====================================================================================================\ndimension:      10000   x       100\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n   1.391s          1.015s\n\n====================================================================================================\ndimension:      10000   x       1000\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n   5.569s          1.291s\n\n====================================================================================================\ndimension:      100000  x       100\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n   1.788s          1.068s\n\n====================================================================================================\ndimension:      100000  x       1000\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n  11.023s          1.304s\n</code></pre>", "body_text": "Update 2: I managed to get a faster version by calling the blas primitives directly (like we already do for the backward pass). The implementation is a bit more specialized but it seems like a good trade-off in this case. In fact it's even faster than the first version I posted, here's the new timing I have for the forward pass:\nNUMEXPR_NUM_THREADS=1 MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 taskset -c 0 python benchmark.py\n\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nruns: 10000     number of indices: 2000 maximum number of bags: 200     maximum bag size: 30\n\n====================================================================================================\ndimension:      10000   x       100\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n   1.391s          1.015s\n\n====================================================================================================\ndimension:      10000   x       1000\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n   5.569s          1.291s\n\n====================================================================================================\ndimension:      100000  x       100\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n   1.788s          1.068s\n\n====================================================================================================\ndimension:      100000  x       1000\n----------------------------------------------------------------------------------------------------\ncpu sparse      cuda sparse\n  11.023s          1.304s", "body": "Update 2: I managed to get a faster version by calling the blas primitives directly (like we already do for the backward pass). The implementation is a bit more specialized but it seems like a good trade-off in this case. In fact it's even faster than the first version I posted, here's the new timing I have for the forward pass:\r\n\r\n```\r\nNUMEXPR_NUM_THREADS=1 MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 taskset -c 0 python benchmark.py\r\n\r\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\nruns: 10000     number of indices: 2000 maximum number of bags: 200     maximum bag size: 30\r\n\r\n====================================================================================================\r\ndimension:      10000   x       100\r\n----------------------------------------------------------------------------------------------------\r\ncpu sparse      cuda sparse\r\n   1.391s          1.015s\r\n\r\n====================================================================================================\r\ndimension:      10000   x       1000\r\n----------------------------------------------------------------------------------------------------\r\ncpu sparse      cuda sparse\r\n   5.569s          1.291s\r\n\r\n====================================================================================================\r\ndimension:      100000  x       100\r\n----------------------------------------------------------------------------------------------------\r\ncpu sparse      cuda sparse\r\n   1.788s          1.068s\r\n\r\n====================================================================================================\r\ndimension:      100000  x       1000\r\n----------------------------------------------------------------------------------------------------\r\ncpu sparse      cuda sparse\r\n  11.023s          1.304s\r\n```"}