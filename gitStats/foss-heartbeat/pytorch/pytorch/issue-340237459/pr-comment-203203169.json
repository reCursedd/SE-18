{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203203169", "pull_request_review_id": 138047030, "id": 203203169, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzIwMzE2OQ==", "diff_hunk": "@@ -0,0 +1,103 @@\n+import torch\n+import torch.nn.functional as F\n+from torch.distributions import constraints\n+from torch.distributions.constraints import Constraint\n+from torch.distributions.distribution import Distribution\n+from torch.distributions.utils import broadcast_all, probs_to_logits, lazy_property, logits_to_probs\n+\n+\n+class _GreaterThanEq(Constraint):\n+    \"\"\"\n+    Constrain to a real half line `[lower_bound, inf)`.\n+    \"\"\"\n+    def __init__(self, lower_bound):\n+        self.lower_bound = lower_bound\n+\n+    def check(self, value):\n+        return self.lower_bound <= value\n+\n+\n+class _HalfOpenInterval(Constraint):\n+    \"\"\"\n+    Constrain to a real interval `[lower_bound, upper_bound)`.\n+    \"\"\"\n+    def __init__(self, lower_bound, upper_bound):\n+        self.lower_bound = lower_bound\n+        self.upper_bound = upper_bound\n+\n+    def check(self, value):\n+        return (self.lower_bound <= value) & (value < self.upper_bound)\n+\n+\n+class NegativeBinomial(Distribution):\n+    r\"\"\"\n+        Creates a Negative Binomial distribution, i.e. distribution\n+        of the number of independent identical Bernoulli trials\n+        needed before `total_count` failures are achieved. The probability\n+        of success of each Bernoulli trial is `probs`.\n+\n+    Args:\n+        total_count (float or Tensor): non-negative number of negative Bernoulli\n+            trials to stop, although the distribution is still valid for real\n+            valued count\n+        probs (Tensor): Event probabilities of success in the open interval (0, 1)\n+        logits (Tensor): Event log-odds for probabilities of success\n+    \"\"\"\n+    arg_constraints = {'total_count': _GreaterThanEq(0),\n+                       'probs': _HalfOpenInterval(0., 1.)}\n+    support = constraints.nonnegative_integer\n+\n+    def __init__(self, total_count, probs=None, logits=None, validate_args=None):\n+        if (probs is None) == (logits is None):\n+            raise ValueError(\"Either `probs` or `logits` must be specified, but not both.\")\n+        if probs is not None:\n+            self.total_count, self.probs, = broadcast_all(total_count, probs)\n+            self.total_count = self.total_count.type_as(self.probs)\n+        else:\n+            self.total_count, self.logits, = broadcast_all(total_count, logits)\n+            self.total_count = self.total_count.type_as(self.logits)\n+\n+        self._param = self.probs if probs is not None else self.logits\n+        batch_shape = self._param.size()\n+        super(NegativeBinomial, self).__init__(batch_shape, validate_args=validate_args)\n+\n+    def _new(self, *args, **kwargs):\n+        return self._param.new(*args, **kwargs)\n+\n+    @property\n+    def mean(self):\n+        return self.total_count * torch.exp(self.logits)\n+\n+    @property\n+    def variance(self):\n+        return self.mean / torch.sigmoid(-self.logits)\n+\n+    @lazy_property\n+    def logits(self):\n+        return probs_to_logits(self.probs, is_binary=True)\n+\n+    @lazy_property\n+    def probs(self):\n+        return logits_to_probs(self.logits, is_binary=True)\n+\n+    @property\n+    def param_shape(self):\n+        return self._param.size()\n+\n+    def sample(self, sample_shape=torch.Size()):\n+        with torch.no_grad():\n+            rate = torch.distributions.Gamma(concentration=self.total_count,", "path": "torch/distributions/negative_binomial.py", "position": null, "original_position": 89, "commit_id": "77785d7475f34184746e37a5a1431548727c5b64", "original_commit_id": "4d9fc6759cf08176e4dfa72003ad467e727e132e", "user": {"login": "alicanb", "id": 1093846, "node_id": "MDQ6VXNlcjEwOTM4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1093846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alicanb", "html_url": "https://github.com/alicanb", "followers_url": "https://api.github.com/users/alicanb/followers", "following_url": "https://api.github.com/users/alicanb/following{/other_user}", "gists_url": "https://api.github.com/users/alicanb/gists{/gist_id}", "starred_url": "https://api.github.com/users/alicanb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alicanb/subscriptions", "organizations_url": "https://api.github.com/users/alicanb/orgs", "repos_url": "https://api.github.com/users/alicanb/repos", "events_url": "https://api.github.com/users/alicanb/events{/privacy}", "received_events_url": "https://api.github.com/users/alicanb/received_events", "type": "User", "site_admin": false}, "body": "One thing you can do is to copy `Gamma.rsample(...)` code here to bypass the distribution creation process.\r\nOther is to create a `_gamma` property and initialize it in `__init__`. This would be cheaper in situations where you sample more than you score, your current implementation is cheaper  where you score more than you sample. I'm not sure which is better. ", "created_at": "2018-07-17T22:35:45Z", "updated_at": "2018-11-23T15:47:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/9345#discussion_r203203169", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9345", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203203169"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9345#discussion_r203203169"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9345"}}, "body_html": "<p>One thing you can do is to copy <code>Gamma.rsample(...)</code> code here to bypass the distribution creation process.<br>\nOther is to create a <code>_gamma</code> property and initialize it in <code>__init__</code>. This would be cheaper in situations where you sample more than you score, your current implementation is cheaper  where you score more than you sample. I'm not sure which is better.</p>", "body_text": "One thing you can do is to copy Gamma.rsample(...) code here to bypass the distribution creation process.\nOther is to create a _gamma property and initialize it in __init__. This would be cheaper in situations where you sample more than you score, your current implementation is cheaper  where you score more than you sample. I'm not sure which is better."}