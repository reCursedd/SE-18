{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9301", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9301/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9301/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9301/events", "html_url": "https://github.com/pytorch/pytorch/issues/9301", "id": 339750260, "node_id": "MDU6SXNzdWUzMzk3NTAyNjA=", "number": 9301, "title": "bug of multilable loss in v0.4", "user": {"login": "lee-junjie", "id": 37136308, "node_id": "MDQ6VXNlcjM3MTM2MzA4", "avatar_url": "https://avatars3.githubusercontent.com/u/37136308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lee-junjie", "html_url": "https://github.com/lee-junjie", "followers_url": "https://api.github.com/users/lee-junjie/followers", "following_url": "https://api.github.com/users/lee-junjie/following{/other_user}", "gists_url": "https://api.github.com/users/lee-junjie/gists{/gist_id}", "starred_url": "https://api.github.com/users/lee-junjie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lee-junjie/subscriptions", "organizations_url": "https://api.github.com/users/lee-junjie/orgs", "repos_url": "https://api.github.com/users/lee-junjie/repos", "events_url": "https://api.github.com/users/lee-junjie/events{/privacy}", "received_events_url": "https://api.github.com/users/lee-junjie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-10T08:53:33Z", "updated_at": "2018-08-06T15:29:54Z", "closed_at": "2018-08-06T15:29:54Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> outputs <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>) <span class=\"pl-k\">*</span> np.log(<span class=\"pl-c1\">1</span><span class=\"pl-k\">/</span><span class=\"pl-c1\">99</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> preds <span class=\"pl-k\">=</span> F.sigmoid(outputs)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> labels <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> labels[:,<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> preds\ntensor(<span class=\"pl-c1\">1.00000e-03</span> <span class=\"pl-k\">*</span>\n       [[<span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>],\n        [<span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>, <span class=\"pl-c1\">10.0000</span>]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> labels\ntensor([[ <span class=\"pl-c1\">1</span>.,  <span class=\"pl-c1\">0</span>.,  <span class=\"pl-c1\">0</span>.,  <span class=\"pl-c1\">0</span>.,  <span class=\"pl-c1\">0</span>.],\n        [ <span class=\"pl-c1\">1</span>.,  <span class=\"pl-c1\">0</span>.,  <span class=\"pl-c1\">0</span>.,  <span class=\"pl-c1\">0</span>.,  <span class=\"pl-c1\">0</span>.]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">-</span>torch.sum(labels <span class=\"pl-k\">*</span> torch.log(preds) <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>labels) <span class=\"pl-k\">*</span> torch.log(<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>preds)) <span class=\"pl-k\">/</span> outputs.size(<span class=\"pl-c1\">0</span>)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> the expected result of multilabel loss</span>\ntensor(<span class=\"pl-c1\">4.6454</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">-</span>torch.mean(labels <span class=\"pl-k\">*</span> torch.log(preds) <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>labels) <span class=\"pl-k\">*</span> torch.log(<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>preds))       <span class=\"pl-c\"><span class=\"pl-c\">#</span> a wrong way, should use sum(not mean) for binary losses of different classes</span>\ntensor(<span class=\"pl-c1\">0.9291</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.multilabel_soft_margin_loss(outputs, labels) <span class=\"pl-c\"><span class=\"pl-c\">#</span> same as above</span>\ntensor(<span class=\"pl-c1\">0.9291</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.multilabel_soft_margin_loss(outputs, labels, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)     <span class=\"pl-c\"><span class=\"pl-c\">#</span> expected behavior, but wrong for loss, should average loss from samples in a batch</span>\ntensor(<span class=\"pl-c1\">9.2907</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.multilabel_soft_margin_loss(outputs, labels, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>) <span class=\"pl-k\">/</span> outputs.size(<span class=\"pl-c1\">0</span>)    <span class=\"pl-c\"><span class=\"pl-c\">#</span> correct</span>\ntensor(<span class=\"pl-c1\">4.6454</span>)</pre></div>\n<p>doc: <a href=\"https://pytorch.org/docs/stable/nn.html?highlight=multilabel_soft_margin_loss#torch.nn.MultiLabelSoftMarginLoss\" rel=\"nofollow\">https://pytorch.org/docs/stable/nn.html?highlight=multilabel_soft_margin_loss#torch.nn.MultiLabelSoftMarginLoss</a></p>\n<p>I think the default behavior of multilabel_soft_margin_loss is incorrect: current 0.4 version will <strong><code>average</code></strong> binary losses of classes in a sample. From the doc, we should use <code>sum</code>, not <code>average</code>.</p>", "body_text": ">>> import torch\n>>> import torch.nn.functional as F\n>>> import numpy as np\n>>> outputs = torch.ones(2, 5) * np.log(1/99)\n>>> preds = F.sigmoid(outputs)\n>>> labels = torch.zeros(2, 5)\n>>> labels[:,0] = 1\n>>> preds\ntensor(1.00000e-03 *\n       [[10.0000, 10.0000, 10.0000, 10.0000, 10.0000],\n        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000]])\n>>> labels\ntensor([[ 1.,  0.,  0.,  0.,  0.],\n        [ 1.,  0.,  0.,  0.,  0.]])\n>>> -torch.sum(labels * torch.log(preds) + (1-labels) * torch.log(1-preds)) / outputs.size(0)   # the expected result of multilabel loss\ntensor(4.6454)\n>>> -torch.mean(labels * torch.log(preds) + (1-labels) * torch.log(1-preds))       # a wrong way, should use sum(not mean) for binary losses of different classes\ntensor(0.9291)\n>>> F.multilabel_soft_margin_loss(outputs, labels) # same as above\ntensor(0.9291)\n>>> F.multilabel_soft_margin_loss(outputs, labels, size_average=False)     # expected behavior, but wrong for loss, should average loss from samples in a batch\ntensor(9.2907)\n>>> F.multilabel_soft_margin_loss(outputs, labels, size_average=False) / outputs.size(0)    # correct\ntensor(4.6454)\ndoc: https://pytorch.org/docs/stable/nn.html?highlight=multilabel_soft_margin_loss#torch.nn.MultiLabelSoftMarginLoss\nI think the default behavior of multilabel_soft_margin_loss is incorrect: current 0.4 version will average binary losses of classes in a sample. From the doc, we should use sum, not average.", "body": "```python\r\n>>> import torch\r\n>>> import torch.nn.functional as F\r\n>>> import numpy as np\r\n>>> outputs = torch.ones(2, 5) * np.log(1/99)\r\n>>> preds = F.sigmoid(outputs)\r\n>>> labels = torch.zeros(2, 5)\r\n>>> labels[:,0] = 1\r\n>>> preds\r\ntensor(1.00000e-03 *\r\n       [[10.0000, 10.0000, 10.0000, 10.0000, 10.0000],\r\n        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000]])\r\n>>> labels\r\ntensor([[ 1.,  0.,  0.,  0.,  0.],\r\n        [ 1.,  0.,  0.,  0.,  0.]])\r\n>>> -torch.sum(labels * torch.log(preds) + (1-labels) * torch.log(1-preds)) / outputs.size(0)   # the expected result of multilabel loss\r\ntensor(4.6454)\r\n>>> -torch.mean(labels * torch.log(preds) + (1-labels) * torch.log(1-preds))       # a wrong way, should use sum(not mean) for binary losses of different classes\r\ntensor(0.9291)\r\n>>> F.multilabel_soft_margin_loss(outputs, labels) # same as above\r\ntensor(0.9291)\r\n>>> F.multilabel_soft_margin_loss(outputs, labels, size_average=False)     # expected behavior, but wrong for loss, should average loss from samples in a batch\r\ntensor(9.2907)\r\n>>> F.multilabel_soft_margin_loss(outputs, labels, size_average=False) / outputs.size(0)    # correct\r\ntensor(4.6454)\r\n```\r\n\r\ndoc: [https://pytorch.org/docs/stable/nn.html?highlight=multilabel_soft_margin_loss#torch.nn.MultiLabelSoftMarginLoss](https://pytorch.org/docs/stable/nn.html?highlight=multilabel_soft_margin_loss#torch.nn.MultiLabelSoftMarginLoss)\r\n\r\nI think the default behavior of multilabel_soft_margin_loss is incorrect: current 0.4 version will **`average`** binary losses of classes in a sample. From the doc, we should use `sum`, not `average`.\r\n"}