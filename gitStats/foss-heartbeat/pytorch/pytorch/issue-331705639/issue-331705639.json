{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8388", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8388/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8388/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8388/events", "html_url": "https://github.com/pytorch/pytorch/issues/8388", "id": 331705639, "node_id": "MDU6SXNzdWUzMzE3MDU2Mzk=", "number": 8388, "title": "Running simultaneous DataParallels can potentially result in locked models", "user": {"login": "daemon", "id": 6188572, "node_id": "MDQ6VXNlcjYxODg1NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6188572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daemon", "html_url": "https://github.com/daemon", "followers_url": "https://api.github.com/users/daemon/followers", "following_url": "https://api.github.com/users/daemon/following{/other_user}", "gists_url": "https://api.github.com/users/daemon/gists{/gist_id}", "starred_url": "https://api.github.com/users/daemon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daemon/subscriptions", "organizations_url": "https://api.github.com/users/daemon/orgs", "repos_url": "https://api.github.com/users/daemon/repos", "events_url": "https://api.github.com/users/daemon/events{/privacy}", "received_events_url": "https://api.github.com/users/daemon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-12T18:37:49Z", "updated_at": "2018-06-12T21:43:16Z", "closed_at": "2018-06-12T20:42:24Z", "author_association": "NONE", "body_html": "<p>I observed that running simultaneous DataParallels might result in at least one of the models being unable to progress at all.</p>\n<pre><code>ubuntu@ip-XXX:~/vrex2$ ./run.sh\nLoading data file...\nLoaded!\n^CTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 121, in &lt;module&gt;\n    main()\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 116, in main\n    train(config)\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 69, in train\n    scores = model(model_in)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 114, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 124, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 57, in parallel_apply\n    thread.join()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1056, in join\n    self._wait_for_tstate_lock()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n    elif lock.acquire(block, timeout):\nKeyboardInterrupt\n^CException ignored in: &lt;module 'threading' from '/home/ubuntu/anaconda3/lib/python3.6/threading.py'&gt;\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1294, in _shutdown\n    t.join()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1056, in join\n    self._wait_for_tstate_lock()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n    elif lock.acquire(block, timeout):\nKeyboardInterrupt\n^C^C^C^C^C^C\n</code></pre>\n<p>System configuration:</p>\n<ul>\n<li>PyTorch 0.4.0 stable release, installed using <code>conda install -c pytorch pytorch torchvision</code></li>\n<li>EC2 P2.8xlarge (8 x K80s)</li>\n<li>CUDA 8</li>\n<li>Python 3.6</li>\n</ul>\n<pre><code>ubuntu@ip-96-115-215-139:~/vrex2$ nvidia-smi\nTue Jun 12 18:35:36 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 00000000:00:17.0 Off |                    0 |\n| N/A   83C    P0   127W / 149W |   1998MiB / 11439MiB |     67%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 00000000:00:18.0 Off |                    0 |\n| N/A   67C    P0   130W / 149W |   1878MiB / 11439MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 00000000:00:19.0 Off |                    0 |\n| N/A   77C    P0   124W / 149W |   1878MiB / 11439MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 00000000:00:1A.0 Off |                    0 |\n| N/A   63C    P0   128W / 149W |   1878MiB / 11439MiB |     62%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  Tesla K80           On   | 00000000:00:1B.0 Off |                    0 |\n| N/A   79C    P0   123W / 149W |   1878MiB / 11439MiB |     60%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  Tesla K80           On   | 00000000:00:1C.0 Off |                    0 |\n| N/A   68C    P0   127W / 149W |   1878MiB / 11439MiB |     59%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  Tesla K80           On   | 00000000:00:1D.0 Off |                    0 |\n| N/A   75C    P0   112W / 149W |   1878MiB / 11439MiB |     60%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   62C    P0   135W / 149W |   1878MiB / 11439MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     76810      C   python                                      1985MiB |\n|    1     76810      C   python                                      1865MiB |\n|    2     76810      C   python                                      1865MiB |\n|    3     76810      C   python                                      1865MiB |\n|    4     76810      C   python                                      1865MiB |\n|    5     76810      C   python                                      1865MiB |\n|    6     76810      C   python                                      1865MiB |\n|    7     76810      C   python                                      1865MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>On a separate machine with 8 x Titan X, the following happened after I tried running multiple parallel models:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |\n| 22%   50C    P2    84W / 250W |   2550MiB / 12207MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\n| 22%   41C    P2    67W / 250W |   2394MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:08:00.0     Off |                  N/A |\n| 22%   40C    P2    67W / 250W |   2276MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   42C    P2    67W / 250W |   2157MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  GeForce GTX TIT...  Off  | 0000:85:00.0     Off |                  N/A |\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  GeForce GTX TIT...  Off  | 0000:86:00.0     Off |                  N/A |\n| 22%   23C    P8    15W / 250W |   1975MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  GeForce GTX TIT...  Off  | 0000:89:00.0     Off |                  N/A |\n| 22%   23C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  GeForce GTX TIT...  Off  | 0000:8A:00.0     Off |                  N/A |\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     20159    C   python                                         400MiB |\n|    1     31020    C   python                                         419MiB |\n|    2     31020    C   python                                         301MiB |\n|    3     31020    C   python                                         182MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>There's RAM that's somehow unaccounted for.</p>", "body_text": "I observed that running simultaneous DataParallels might result in at least one of the models being unable to progress at all.\nubuntu@ip-XXX:~/vrex2$ ./run.sh\nLoading data file...\nLoaded!\n^CTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 121, in <module>\n    main()\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 116, in main\n    train(config)\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 69, in train\n    scores = model(model_in)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 114, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 124, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 57, in parallel_apply\n    thread.join()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1056, in join\n    self._wait_for_tstate_lock()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n    elif lock.acquire(block, timeout):\nKeyboardInterrupt\n^CException ignored in: <module 'threading' from '/home/ubuntu/anaconda3/lib/python3.6/threading.py'>\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1294, in _shutdown\n    t.join()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1056, in join\n    self._wait_for_tstate_lock()\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n    elif lock.acquire(block, timeout):\nKeyboardInterrupt\n^C^C^C^C^C^C\n\nSystem configuration:\n\nPyTorch 0.4.0 stable release, installed using conda install -c pytorch pytorch torchvision\nEC2 P2.8xlarge (8 x K80s)\nCUDA 8\nPython 3.6\n\nubuntu@ip-96-115-215-139:~/vrex2$ nvidia-smi\nTue Jun 12 18:35:36 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 00000000:00:17.0 Off |                    0 |\n| N/A   83C    P0   127W / 149W |   1998MiB / 11439MiB |     67%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 00000000:00:18.0 Off |                    0 |\n| N/A   67C    P0   130W / 149W |   1878MiB / 11439MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 00000000:00:19.0 Off |                    0 |\n| N/A   77C    P0   124W / 149W |   1878MiB / 11439MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 00000000:00:1A.0 Off |                    0 |\n| N/A   63C    P0   128W / 149W |   1878MiB / 11439MiB |     62%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  Tesla K80           On   | 00000000:00:1B.0 Off |                    0 |\n| N/A   79C    P0   123W / 149W |   1878MiB / 11439MiB |     60%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  Tesla K80           On   | 00000000:00:1C.0 Off |                    0 |\n| N/A   68C    P0   127W / 149W |   1878MiB / 11439MiB |     59%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  Tesla K80           On   | 00000000:00:1D.0 Off |                    0 |\n| N/A   75C    P0   112W / 149W |   1878MiB / 11439MiB |     60%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   62C    P0   135W / 149W |   1878MiB / 11439MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     76810      C   python                                      1985MiB |\n|    1     76810      C   python                                      1865MiB |\n|    2     76810      C   python                                      1865MiB |\n|    3     76810      C   python                                      1865MiB |\n|    4     76810      C   python                                      1865MiB |\n|    5     76810      C   python                                      1865MiB |\n|    6     76810      C   python                                      1865MiB |\n|    7     76810      C   python                                      1865MiB |\n+-----------------------------------------------------------------------------+\n\nOn a separate machine with 8 x Titan X, the following happened after I tried running multiple parallel models:\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |\n| 22%   50C    P2    84W / 250W |   2550MiB / 12207MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\n| 22%   41C    P2    67W / 250W |   2394MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:08:00.0     Off |                  N/A |\n| 22%   40C    P2    67W / 250W |   2276MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   42C    P2    67W / 250W |   2157MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  GeForce GTX TIT...  Off  | 0000:85:00.0     Off |                  N/A |\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  GeForce GTX TIT...  Off  | 0000:86:00.0     Off |                  N/A |\n| 22%   23C    P8    15W / 250W |   1975MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  GeForce GTX TIT...  Off  | 0000:89:00.0     Off |                  N/A |\n| 22%   23C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  GeForce GTX TIT...  Off  | 0000:8A:00.0     Off |                  N/A |\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     20159    C   python                                         400MiB |\n|    1     31020    C   python                                         419MiB |\n|    2     31020    C   python                                         301MiB |\n|    3     31020    C   python                                         182MiB |\n+-----------------------------------------------------------------------------+\n\nThere's RAM that's somehow unaccounted for.", "body": "I observed that running simultaneous DataParallels might result in at least one of the models being unable to progress at all.\r\n\r\n```\r\nubuntu@ip-XXX:~/vrex2$ ./run.sh\r\nLoading data file...\r\nLoaded!\r\n^CTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 121, in <module>\r\n    main()\r\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 116, in main\r\n    train(config)\r\n  File \"/home/ubuntu/vrex2/utils/train.py\", line 69, in train\r\n    scores = model(model_in)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 114, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 124, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 57, in parallel_apply\r\n    thread.join()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1056, in join\r\n    self._wait_for_tstate_lock()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n^CException ignored in: <module 'threading' from '/home/ubuntu/anaconda3/lib/python3.6/threading.py'>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1294, in _shutdown\r\n    t.join()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1056, in join\r\n    self._wait_for_tstate_lock()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n^C^C^C^C^C^C\r\n```\r\n\r\nSystem configuration:\r\n- PyTorch 0.4.0 stable release, installed using `conda install -c pytorch pytorch torchvision`\r\n- EC2 P2.8xlarge (8 x K80s)\r\n- CUDA 8\r\n- Python 3.6\r\n\r\n```\r\nubuntu@ip-96-115-215-139:~/vrex2$ nvidia-smi\r\nTue Jun 12 18:35:36 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 00000000:00:17.0 Off |                    0 |\r\n| N/A   83C    P0   127W / 149W |   1998MiB / 11439MiB |     67%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 00000000:00:18.0 Off |                    0 |\r\n| N/A   67C    P0   130W / 149W |   1878MiB / 11439MiB |     61%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           On   | 00000000:00:19.0 Off |                    0 |\r\n| N/A   77C    P0   124W / 149W |   1878MiB / 11439MiB |     61%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           On   | 00000000:00:1A.0 Off |                    0 |\r\n| N/A   63C    P0   128W / 149W |   1878MiB / 11439MiB |     62%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           On   | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   79C    P0   123W / 149W |   1878MiB / 11439MiB |     60%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           On   | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   68C    P0   127W / 149W |   1878MiB / 11439MiB |     59%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           On   | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   75C    P0   112W / 149W |   1878MiB / 11439MiB |     60%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   62C    P0   135W / 149W |   1878MiB / 11439MiB |     61%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     76810      C   python                                      1985MiB |\r\n|    1     76810      C   python                                      1865MiB |\r\n|    2     76810      C   python                                      1865MiB |\r\n|    3     76810      C   python                                      1865MiB |\r\n|    4     76810      C   python                                      1865MiB |\r\n|    5     76810      C   python                                      1865MiB |\r\n|    6     76810      C   python                                      1865MiB |\r\n|    7     76810      C   python                                      1865MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nOn a separate machine with 8 x Titan X, the following happened after I tried running multiple parallel models:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |\r\n| 22%   50C    P2    84W / 250W |   2550MiB / 12207MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\r\n| 22%   41C    P2    67W / 250W |   2394MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX TIT...  Off  | 0000:08:00.0     Off |                  N/A |\r\n| 22%   40C    P2    67W / 250W |   2276MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\r\n| 22%   42C    P2    67W / 250W |   2157MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  GeForce GTX TIT...  Off  | 0000:85:00.0     Off |                  N/A |\r\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  GeForce GTX TIT...  Off  | 0000:86:00.0     Off |                  N/A |\r\n| 22%   23C    P8    15W / 250W |   1975MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  GeForce GTX TIT...  Off  | 0000:89:00.0     Off |                  N/A |\r\n| 22%   23C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  GeForce GTX TIT...  Off  | 0000:8A:00.0     Off |                  N/A |\r\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     20159    C   python                                         400MiB |\r\n|    1     31020    C   python                                         419MiB |\r\n|    2     31020    C   python                                         301MiB |\r\n|    3     31020    C   python                                         182MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nThere's RAM that's somehow unaccounted for."}