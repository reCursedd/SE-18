{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396731240", "html_url": "https://github.com/pytorch/pytorch/issues/8388#issuecomment-396731240", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8388", "id": 396731240, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjczMTI0MA==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T20:57:11Z", "updated_at": "2018-06-12T20:57:11Z", "author_association": "MEMBER", "body_html": "<p>To elaborate on Adam's answer. At the NVIDIA driver level, deadlocks are expected with NCCL -- there are no known workarounds, and no expected solutions. Sharing GPUs for multiple workloads in the distributed setting is not ideal for this reason.</p>\n<p>One alternative for you to consider is to move to DistributedDataParallel (even for a single machine) and use the <code>gloo</code> backend: <a href=\"https://pytorch.org/docs/stable/distributed.html#launch-utility\" rel=\"nofollow\">https://pytorch.org/docs/stable/distributed.html#launch-utility</a></p>", "body_text": "To elaborate on Adam's answer. At the NVIDIA driver level, deadlocks are expected with NCCL -- there are no known workarounds, and no expected solutions. Sharing GPUs for multiple workloads in the distributed setting is not ideal for this reason.\nOne alternative for you to consider is to move to DistributedDataParallel (even for a single machine) and use the gloo backend: https://pytorch.org/docs/stable/distributed.html#launch-utility", "body": "To elaborate on Adam's answer. At the NVIDIA driver level, deadlocks are expected with NCCL -- there are no known workarounds, and no expected solutions. Sharing GPUs for multiple workloads in the distributed setting is not ideal for this reason.\r\n\r\nOne alternative for you to consider is to move to DistributedDataParallel (even for a single machine) and use the `gloo` backend: https://pytorch.org/docs/stable/distributed.html#launch-utility"}