{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8279", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8279/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8279/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8279/events", "html_url": "https://github.com/pytorch/pytorch/pull/8279", "id": 330568940, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkzNTQ0OTUz", "number": 8279, "title": "[Fix] sparse regularization in distributed training", "user": {"login": "alex1o1o7cloud", "id": 38146646, "node_id": "MDQ6VXNlcjM4MTQ2NjQ2", "avatar_url": "https://avatars0.githubusercontent.com/u/38146646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alex1o1o7cloud", "html_url": "https://github.com/alex1o1o7cloud", "followers_url": "https://api.github.com/users/alex1o1o7cloud/followers", "following_url": "https://api.github.com/users/alex1o1o7cloud/following{/other_user}", "gists_url": "https://api.github.com/users/alex1o1o7cloud/gists{/gist_id}", "starred_url": "https://api.github.com/users/alex1o1o7cloud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alex1o1o7cloud/subscriptions", "organizations_url": "https://api.github.com/users/alex1o1o7cloud/orgs", "repos_url": "https://api.github.com/users/alex1o1o7cloud/repos", "events_url": "https://api.github.com/users/alex1o1o7cloud/events{/privacy}", "received_events_url": "https://api.github.com/users/alex1o1o7cloud/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-08T08:38:38Z", "updated_at": "2018-07-10T23:08:17Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8279", "html_url": "https://github.com/pytorch/pytorch/pull/8279", "diff_url": "https://github.com/pytorch/pytorch/pull/8279.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8279.patch"}, "body_html": "<p>Facebook: Previously, sparse regularization in distributed v2 setting is out of memory when more than 1 sparse features are included. Even with only 1 sparse feature, the training is extremely slow. This is due to a bug in the implementation. The sparse parameters are in ps, while applying sparse regularization happens on trainer. When training, it takes most of the time copy the parameters from ps to trainer. The updating should be on PS.</p>", "body_text": "Facebook: Previously, sparse regularization in distributed v2 setting is out of memory when more than 1 sparse features are included. Even with only 1 sparse feature, the training is extremely slow. This is due to a bug in the implementation. The sparse parameters are in ps, while applying sparse regularization happens on trainer. When training, it takes most of the time copy the parameters from ps to trainer. The updating should be on PS.", "body": "Facebook: Previously, sparse regularization in distributed v2 setting is out of memory when more than 1 sparse features are included. Even with only 1 sparse feature, the training is extremely slow. This is due to a bug in the implementation. The sparse parameters are in ps, while applying sparse regularization happens on trainer. When training, it takes most of the time copy the parameters from ps to trainer. The updating should be on PS."}