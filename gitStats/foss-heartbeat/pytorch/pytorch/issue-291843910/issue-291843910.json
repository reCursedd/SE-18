{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4870", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4870/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4870/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4870/events", "html_url": "https://github.com/pytorch/pytorch/pull/4870", "id": 291843910, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY1MzIxNTYw", "number": 4870, "title": "Slightly improve DistributedDataParallel (single-GPU binding) multi-process distributed training performance", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-01-26T09:32:29Z", "updated_at": "2018-01-27T11:15:46Z", "closed_at": "2018-01-27T11:15:46Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4870", "html_url": "https://github.com/pytorch/pytorch/pull/4870", "diff_url": "https://github.com/pytorch/pytorch/pull/4870.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4870.patch"}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> 's simple DistributedDataParallel (DDP) version (with sync buffers ON, for fair comparison since we always sync buffer in our DDP) can hit <strong>0.159</strong> sec per iteration on 8 GPU training using 8 DDP processes for Resnet50 32 mini-batch size / GPU, on single node with 8 P100s. And our current DDP can hit <strong>0.164</strong> sec per iteration as the current state with 8 processes single node training.</p>\n<p>This PR improves the performance from <strong>0.164</strong> sec/iteration to <strong>0.162</strong> sec/iteration on single-node dist training on 8 P100s</p>", "body_text": "@csarofeen 's simple DistributedDataParallel (DDP) version (with sync buffers ON, for fair comparison since we always sync buffer in our DDP) can hit 0.159 sec per iteration on 8 GPU training using 8 DDP processes for Resnet50 32 mini-batch size / GPU, on single node with 8 P100s. And our current DDP can hit 0.164 sec per iteration as the current state with 8 processes single node training.\nThis PR improves the performance from 0.164 sec/iteration to 0.162 sec/iteration on single-node dist training on 8 P100s", "body": "@csarofeen 's simple DistributedDataParallel (DDP) version (with sync buffers ON, for fair comparison since we always sync buffer in our DDP) can hit **0.159** sec per iteration on 8 GPU training using 8 DDP processes for Resnet50 32 mini-batch size / GPU, on single node with 8 P100s. And our current DDP can hit **0.164** sec per iteration as the current state with 8 processes single node training.\r\n\r\nThis PR improves the performance from **0.164** sec/iteration to **0.162** sec/iteration on single-node dist training on 8 P100s"}