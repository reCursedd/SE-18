{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274161725", "html_url": "https://github.com/pytorch/pytorch/issues/520#issuecomment-274161725", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/520", "id": 274161725, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDE2MTcyNQ==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-20T19:42:50Z", "updated_at": "2017-01-20T19:42:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361\">https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361</a><br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51\">https://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51</a><br>\nMath type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because</p>\n<ol>\n<li>computations with half math are not supported anywhere except Pascal and TX1</li>\n<li>Even on Pascal, 6.1 cards have very low performance fp16 math</li>\n<li>fp16 math comes with a set of accuracy problems<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></li>\n</ol>", "body_text": "In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361\nhttps://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51\nMath type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because\n\ncomputations with half math are not supported anywhere except Pascal and TX1\nEven on Pascal, 6.1 cards have very low performance fp16 math\nfp16 math comes with a set of accuracy problems\n@colesbury", "body": "In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361\r\nhttps://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51\r\nMath type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because\r\n1) computations with half math are not supported anywhere except Pascal and TX1\r\n2) Even on Pascal, 6.1 cards have very low performance fp16 math\r\n3) fp16 math comes with a set of accuracy problems\r\n@colesbury "}