{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/277505792", "html_url": "https://github.com/pytorch/pytorch/issues/520#issuecomment-277505792", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/520", "id": 277505792, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NzUwNTc5Mg==", "user": {"login": "eladhoffer", "id": 6863805, "node_id": "MDQ6VXNlcjY4NjM4MDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/6863805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eladhoffer", "html_url": "https://github.com/eladhoffer", "followers_url": "https://api.github.com/users/eladhoffer/followers", "following_url": "https://api.github.com/users/eladhoffer/following{/other_user}", "gists_url": "https://api.github.com/users/eladhoffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/eladhoffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eladhoffer/subscriptions", "organizations_url": "https://api.github.com/users/eladhoffer/orgs", "repos_url": "https://api.github.com/users/eladhoffer/repos", "events_url": "https://api.github.com/users/eladhoffer/events{/privacy}", "received_events_url": "https://api.github.com/users/eladhoffer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-05T08:55:37Z", "updated_at": "2017-02-05T08:55:37Z", "author_association": "NONE", "body_html": "<p>Thanks. Seems like batchnorm fp16 support is still missing.<br>\ne.g:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n \nx <span class=\"pl-k\">=</span> Variable(torch.rand(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">16</span>,<span class=\"pl-c1\">100</span>,<span class=\"pl-c1\">100</span>).cuda().half())\nbn <span class=\"pl-k\">=</span> nn.BatchNorm2d(<span class=\"pl-c1\">16</span>).cuda().half()\n\ny <span class=\"pl-k\">=</span> bn(x)</pre></div>\n<p>will result in</p>\n<pre><code>&lt;ipython-input-1-c9c578acf3d8&gt; in &lt;module&gt;()\n      7 bn = nn.BatchNorm2d(16).cuda().half()\n      8 \n----&gt; 9 y = bn(x)\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--&gt; 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/modules/batchnorm.pyc in forward(self, input)\n     41         return F.batch_norm(\n     42             input, self.running_mean, self.running_var, self.weight, self.bias,\n---&gt; 43             self.training, self.momentum, self.eps)\n     44 \n     45     def __repr__(self):\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\n    386     state = _functions.batchnorm.BatchNorm(\n    387         running_mean, running_var, training, momentum, eps)\n--&gt; 388     return weight and state(input, weight, bias) or state(input)\n    389 \n    390 \n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/batchnorm.pyc in forward(self, input, weight, bias)\n     37                 self._save_std, self.training, self.momentum, self.eps)\n     38         else:\n---&gt; 39             backend = type2backend[type(input)]\n     40             backend.BatchNormalization_updateOutput(\n     41                 backend.library_state, input, output, weight, bias,\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/_thnn/__init__.pyc in __getitem__(self, name)\n     13 \n     14     def __getitem__(self, name):\n---&gt; 15         return self.backends[name].load()\n     16 \n     17 \n\nKeyError: &lt;class 'torch.cuda.HalfTensor'&gt;\n\n</code></pre>", "body_text": "Thanks. Seems like batchnorm fp16 support is still missing.\ne.g:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n \nx = Variable(torch.rand(1,16,100,100).cuda().half())\nbn = nn.BatchNorm2d(16).cuda().half()\n\ny = bn(x)\nwill result in\n<ipython-input-1-c9c578acf3d8> in <module>()\n      7 bn = nn.BatchNorm2d(16).cuda().half()\n      8 \n----> 9 y = bn(x)\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--> 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/modules/batchnorm.pyc in forward(self, input)\n     41         return F.batch_norm(\n     42             input, self.running_mean, self.running_var, self.weight, self.bias,\n---> 43             self.training, self.momentum, self.eps)\n     44 \n     45     def __repr__(self):\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\n    386     state = _functions.batchnorm.BatchNorm(\n    387         running_mean, running_var, training, momentum, eps)\n--> 388     return weight and state(input, weight, bias) or state(input)\n    389 \n    390 \n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/batchnorm.pyc in forward(self, input, weight, bias)\n     37                 self._save_std, self.training, self.momentum, self.eps)\n     38         else:\n---> 39             backend = type2backend[type(input)]\n     40             backend.BatchNormalization_updateOutput(\n     41                 backend.library_state, input, output, weight, bias,\n\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/_thnn/__init__.pyc in __getitem__(self, name)\n     13 \n     14     def __getitem__(self, name):\n---> 15         return self.backends[name].load()\n     16 \n     17 \n\nKeyError: <class 'torch.cuda.HalfTensor'>", "body": "Thanks. Seems like batchnorm fp16 support is still missing.\r\ne.g:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n \r\nx = Variable(torch.rand(1,16,100,100).cuda().half())\r\nbn = nn.BatchNorm2d(16).cuda().half()\r\n\r\ny = bn(x)\r\n```\r\nwill result in\r\n```\r\n<ipython-input-1-c9c578acf3d8> in <module>()\r\n      7 bn = nn.BatchNorm2d(16).cuda().half()\r\n      8 \r\n----> 9 y = bn(x)\r\n\r\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    200 \r\n    201     def __call__(self, *input, **kwargs):\r\n--> 202         result = self.forward(*input, **kwargs)\r\n    203         for hook in self._forward_hooks.values():\r\n    204             hook_result = hook(self, input, result)\r\n\r\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/modules/batchnorm.pyc in forward(self, input)\r\n     41         return F.batch_norm(\r\n     42             input, self.running_mean, self.running_var, self.weight, self.bias,\r\n---> 43             self.training, self.momentum, self.eps)\r\n     44 \r\n     45     def __repr__(self):\r\n\r\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\r\n    386     state = _functions.batchnorm.BatchNorm(\r\n    387         running_mean, running_var, training, momentum, eps)\r\n--> 388     return weight and state(input, weight, bias) or state(input)\r\n    389 \r\n    390 \r\n\r\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/batchnorm.pyc in forward(self, input, weight, bias)\r\n     37                 self._save_std, self.training, self.momentum, self.eps)\r\n     38         else:\r\n---> 39             backend = type2backend[type(input)]\r\n     40             backend.BatchNormalization_updateOutput(\r\n     41                 backend.library_state, input, output, weight, bias,\r\n\r\n/home/ehoffer/anaconda2/lib/python2.7/site-packages/torch/_thnn/__init__.pyc in __getitem__(self, name)\r\n     13 \r\n     14     def __getitem__(self, name):\r\n---> 15         return self.backends[name].load()\r\n     16 \r\n     17 \r\n\r\nKeyError: <class 'torch.cuda.HalfTensor'>\r\n\r\n```"}