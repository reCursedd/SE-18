{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11751", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11751/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11751/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11751/events", "html_url": "https://github.com/pytorch/pytorch/issues/11751", "id": 360795052, "node_id": "MDU6SXNzdWUzNjA3OTUwNTI=", "number": 11751, "title": "Segfault in autograd using hook", "user": {"login": "saluto", "id": 22053721, "node_id": "MDQ6VXNlcjIyMDUzNzIx", "avatar_url": "https://avatars3.githubusercontent.com/u/22053721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saluto", "html_url": "https://github.com/saluto", "followers_url": "https://api.github.com/users/saluto/followers", "following_url": "https://api.github.com/users/saluto/following{/other_user}", "gists_url": "https://api.github.com/users/saluto/gists{/gist_id}", "starred_url": "https://api.github.com/users/saluto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saluto/subscriptions", "organizations_url": "https://api.github.com/users/saluto/orgs", "repos_url": "https://api.github.com/users/saluto/repos", "events_url": "https://api.github.com/users/saluto/events{/privacy}", "received_events_url": "https://api.github.com/users/saluto/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-17T09:53:23Z", "updated_at": "2018-09-26T02:11:44Z", "closed_at": "2018-09-26T02:11:44Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I found several errors in autograd:</p>\n<ul>\n<li>Segfault during <code>.backward()</code>/<code>.grad()</code> when using hook (e.g. through <code>.retain_grad()</code>) on non-reachable tensor, whose grad is implicitely calculated (<code>= 0</code>) because it is an output of a function in the gradient graph but is independent of the backprop root tensor. (See code and traceback below.)</li>\n<li>No hook is called if this non-reachable tensor is an output of an index operation (e.g. <code>a[0].register_hook()</code> while root only depends on <code>a[1]</code> and complete <code>a</code> has <code>requires_grad</code>.) (See code below.) That issue is not related to the others, but I encountered it in the same run, so I want to mention it here, too.</li>\n<li>If such a hook is called (e.g. <code>a0.register_hook()</code> from <code>a0, a1 = a.unbind()</code>), the <code>grad</code> argument is <code>None</code> but should be a tensor with <code>0</code>-values, because that is the actually used value for the required <code>a.grad</code> and therefore should be modifiable. (See code below.)</li>\n</ul>\n<p>Here is the traceback for the Segfault: (Notice the lines <code>&lt;= #6</code>. But I think, the actual source of the problem is, that in <a href=\"https://github.com/pytorch/pytorch/blob/6f6b03566ba3c4828f6ee87a772f9d161be0bae7/torch/csrc/autograd/engine.cpp#L447\">this</a> line, the inputs are not but should be initialized as variables with <code>0</code>-values, as fallbacks if no function overrides them. Or am I wrong?)</p>\n<p>I really would like to fix this, but to be honest, I'm not sure if I have enough expertise to do it right. So I open this issue for others. I hope, it helps.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\na <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">3</span>., <span class=\"pl-c1\">5</span>.], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (A)   With the following line, the callback for `a0` is not called.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       But it should be called (with `grad = 0`, implicitely calculated,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       because `a1` is independent of `a0`), because it is used</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       for the required `a.grad` and therefore should be modifyable</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       via hook. (Also see (C).)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> a0, a1 = a[0], a[1]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (B)   With the following line instead, the callback for `a0` is</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       called, but with two errors:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (B.a) The callback is called with `grad = None`, but it should be `= 0`</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       (implicitely calculated, because `a1` is independent of `a0`).</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       (Also see (C).)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (B.b) After the callback, it results in a `Segmentation fault`.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>       (Not, if no callback was registered.)</span>\na0, a1 <span class=\"pl-k\">=</span> a.unbind()\n<span class=\"pl-en\">@a0.register_hook</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">hook</span>(<span class=\"pl-smi\">grad</span>):\n    <span class=\"pl-c1\">print</span>(grad)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (C)   When `grad` is `None`, returning a non-None replacement throws</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>       the Runtime Error \"can't replace a None gradient with a non-None value\".</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>       Therefore the current behaviour allows no modification at all for</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>       implicitely calculated `0`-gradients.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> return torch.tensor(1.)</span>\na1.backward()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Above errors occure no matter whether using `.backward()` or `.grad()`.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.autograd.grad([a1], [a])</span>\n<span class=\"pl-c1\">print</span>(a.grad)</pre></div>\n<h2>Traceback</h2>\n<pre><code>#0  0x00007fffeda1bfa7 in std::__atomic_base&lt;unsigned long&gt;::operator++ (this=0x8) at /usr/include/c++/5/bits/atomic_base.h:296\n#1  0x00007fffeda2978b in c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt;::retain_ (this=0x7ffff39356f0)\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:163\n#2  0x00007fffeda28cc0 in c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt;::intrusive_ptr (this=0x7ffff39356f0, rhs=...)\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:211\n#3  0x00007fffedae1bd1 in c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt;::operator=&lt;at::TensorImpl, at::UndefinedTensorImpl&gt;(c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt; const&amp;) &amp; (this=0x7fffe00012a0, rhs=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:252\n#4  0x00007fffedae0d1b in c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt;::operator=(c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt; const&amp;) &amp; (this=0x7fffe00012a0, rhs=...)\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:244\n#5  0x00007fffedad8bbf in at::Tensor::operator=(at::Tensor const&amp;) &amp; (this=0x7fffe00012a0, x=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/Tensor.h:105\n#6  0x00007fffedc9c469 in torch::autograd::Variable::operator= (this=0x7fffe00012a0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/variable.h:83\n#7  0x00007fffedcbf973 in torch::autograd::PyFunctionPreHook::operator() (this=0x1450cb0, values=std::vector of length 2, capacity 2 = {...}) at torch/csrc/autograd/python_hook.cpp:54\n#8  0x00007fffe8c6ca99 in torch::autograd::call_pre_hooks (fn=..., inputs=std::vector of length 2, capacity 2 = {...})\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:280\n#9  0x00007fffe8c6cec6 in torch::autograd::call_function (task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:350\n#10 0x00007fffe8c6d395 in torch::autograd::Engine::evaluate_function (this=0x7fffee58b900 &lt;engine&gt;, task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:394\n#11 0x00007fffe8c6c666 in torch::autograd::Engine::thread_main (this=0x7fffee58b900 &lt;engine&gt;, graph_task=0x0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:232\n#12 0x00007fffe8c6c4f7 in torch::autograd::Engine::thread_init (this=0x7fffee58b900 &lt;engine&gt;, device=-1) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:206\n#13 0x00007fffedca0630 in torch::autograd::python::PythonEngine::thread_init (this=0x7fffee58b900 &lt;engine&gt;, device=-1) at torch/csrc/autograd/python_engine.cpp:39\n#14 0x00007fffe8c8cc02 in std::_Mem_fn_base&lt;void (torch::autograd::Engine::*)(int), true&gt;::operator()&lt;int, void&gt;(torch::autograd::Engine*, int&amp;&amp;) const (this=0x145fa58, \n    __object=0x7fffee58b900 &lt;engine&gt;) at /usr/include/c++/5/functional:600\n#15 0x00007fffe8c8cb7f in std::_Bind_simple&lt;std::_Mem_fn&lt;void (torch::autograd::Engine::*)(int)&gt; (torch::autograd::Engine*, int)&gt;::_M_invoke&lt;0ul, 1ul&gt;(std::_Index_tuple&lt;0ul, 1ul&gt;) (this=0x145fa48)\n    at /usr/include/c++/5/functional:1531\n#16 0x00007fffe8c8ca02 in std::_Bind_simple&lt;std::_Mem_fn&lt;void (torch::autograd::Engine::*)(int)&gt; (torch::autograd::Engine*, int)&gt;::operator()() (this=0x145fa48)\n    at /usr/include/c++/5/functional:1520\n#17 0x00007fffe8c8c952 in std::thread::_Impl&lt;std::_Bind_simple&lt;std::_Mem_fn&lt;void (torch::autograd::Engine::*)(int)&gt; (torch::autograd::Engine*, int)&gt; &gt;::_M_run() (this=0x145fa30)\n    at /usr/include/c++/5/thread:115\n#18 0x00007fffe79a9c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#19 0x00007ffff7bc16ba in start_thread (arg=0x7ffff3936700) at pthread_create.c:333\n#20 0x00007ffff6da441d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n\n</code></pre>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): I tested two versions (same errors for both): current master from source, v0.4.1 via pip</li>\n<li>Build command you used (if compiling from source): <code>NO_CUDA=1 DEBUG=1 python setup.py build develop</code></li>\n<li>OS: Linux Mint 18.3 Sylvia</li>\n<li>PyTorch version:  I tested two versions (same errors for both): current master, v0.4.1</li>\n<li>Python version: 3.6.6</li>\n<li>CUDA/cuDNN version: None</li>\n<li>GPU models and configuration: No CUDA</li>\n<li>GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</li>\n<li>CMake version: version 3.12.0</li>\n<li>Versions of any other relevant libraries:<br>\n[pip] 18.0</li>\n</ul>", "body_text": "Issue description\nI found several errors in autograd:\n\nSegfault during .backward()/.grad() when using hook (e.g. through .retain_grad()) on non-reachable tensor, whose grad is implicitely calculated (= 0) because it is an output of a function in the gradient graph but is independent of the backprop root tensor. (See code and traceback below.)\nNo hook is called if this non-reachable tensor is an output of an index operation (e.g. a[0].register_hook() while root only depends on a[1] and complete a has requires_grad.) (See code below.) That issue is not related to the others, but I encountered it in the same run, so I want to mention it here, too.\nIf such a hook is called (e.g. a0.register_hook() from a0, a1 = a.unbind()), the grad argument is None but should be a tensor with 0-values, because that is the actually used value for the required a.grad and therefore should be modifiable. (See code below.)\n\nHere is the traceback for the Segfault: (Notice the lines <= #6. But I think, the actual source of the problem is, that in this line, the inputs are not but should be initialized as variables with 0-values, as fallbacks if no function overrides them. Or am I wrong?)\nI really would like to fix this, but to be honest, I'm not sure if I have enough expertise to do it right. So I open this issue for others. I hope, it helps.\nCode example\nimport torch\n\na = torch.tensor([3., 5.], requires_grad=True)\n# (A)   With the following line, the callback for `a0` is not called.\n#       But it should be called (with `grad = 0`, implicitely calculated,\n#       because `a1` is independent of `a0`), because it is used\n#       for the required `a.grad` and therefore should be modifyable\n#       via hook. (Also see (C).)\n# a0, a1 = a[0], a[1]\n# (B)   With the following line instead, the callback for `a0` is\n#       called, but with two errors:\n# (B.a) The callback is called with `grad = None`, but it should be `= 0`\n#       (implicitely calculated, because `a1` is independent of `a0`).\n#       (Also see (C).)\n# (B.b) After the callback, it results in a `Segmentation fault`.\n#       (Not, if no callback was registered.)\na0, a1 = a.unbind()\n@a0.register_hook\ndef hook(grad):\n    print(grad)\n    # (C)   When `grad` is `None`, returning a non-None replacement throws\n    #       the Runtime Error \"can't replace a None gradient with a non-None value\".\n    #       Therefore the current behaviour allows no modification at all for\n    #       implicitely calculated `0`-gradients.\n    # return torch.tensor(1.)\na1.backward()\n# Above errors occure no matter whether using `.backward()` or `.grad()`.\n# torch.autograd.grad([a1], [a])\nprint(a.grad)\nTraceback\n#0  0x00007fffeda1bfa7 in std::__atomic_base<unsigned long>::operator++ (this=0x8) at /usr/include/c++/5/bits/atomic_base.h:296\n#1  0x00007fffeda2978b in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::retain_ (this=0x7ffff39356f0)\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:163\n#2  0x00007fffeda28cc0 in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::intrusive_ptr (this=0x7ffff39356f0, rhs=...)\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:211\n#3  0x00007fffedae1bd1 in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::operator=<at::TensorImpl, at::UndefinedTensorImpl>(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl> const&) & (this=0x7fffe00012a0, rhs=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:252\n#4  0x00007fffedae0d1b in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::operator=(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl> const&) & (this=0x7fffe00012a0, rhs=...)\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:244\n#5  0x00007fffedad8bbf in at::Tensor::operator=(at::Tensor const&) & (this=0x7fffe00012a0, x=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/Tensor.h:105\n#6  0x00007fffedc9c469 in torch::autograd::Variable::operator= (this=0x7fffe00012a0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/variable.h:83\n#7  0x00007fffedcbf973 in torch::autograd::PyFunctionPreHook::operator() (this=0x1450cb0, values=std::vector of length 2, capacity 2 = {...}) at torch/csrc/autograd/python_hook.cpp:54\n#8  0x00007fffe8c6ca99 in torch::autograd::call_pre_hooks (fn=..., inputs=std::vector of length 2, capacity 2 = {...})\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:280\n#9  0x00007fffe8c6cec6 in torch::autograd::call_function (task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:350\n#10 0x00007fffe8c6d395 in torch::autograd::Engine::evaluate_function (this=0x7fffee58b900 <engine>, task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:394\n#11 0x00007fffe8c6c666 in torch::autograd::Engine::thread_main (this=0x7fffee58b900 <engine>, graph_task=0x0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:232\n#12 0x00007fffe8c6c4f7 in torch::autograd::Engine::thread_init (this=0x7fffee58b900 <engine>, device=-1) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:206\n#13 0x00007fffedca0630 in torch::autograd::python::PythonEngine::thread_init (this=0x7fffee58b900 <engine>, device=-1) at torch/csrc/autograd/python_engine.cpp:39\n#14 0x00007fffe8c8cc02 in std::_Mem_fn_base<void (torch::autograd::Engine::*)(int), true>::operator()<int, void>(torch::autograd::Engine*, int&&) const (this=0x145fa58, \n    __object=0x7fffee58b900 <engine>) at /usr/include/c++/5/functional:600\n#15 0x00007fffe8c8cb7f in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (this=0x145fa48)\n    at /usr/include/c++/5/functional:1531\n#16 0x00007fffe8c8ca02 in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::operator()() (this=0x145fa48)\n    at /usr/include/c++/5/functional:1520\n#17 0x00007fffe8c8c952 in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)> >::_M_run() (this=0x145fa30)\n    at /usr/include/c++/5/thread:115\n#18 0x00007fffe79a9c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#19 0x00007ffff7bc16ba in start_thread (arg=0x7ffff3936700) at pthread_create.c:333\n#20 0x00007ffff6da441d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n\n\nSystem Info\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): I tested two versions (same errors for both): current master from source, v0.4.1 via pip\nBuild command you used (if compiling from source): NO_CUDA=1 DEBUG=1 python setup.py build develop\nOS: Linux Mint 18.3 Sylvia\nPyTorch version:  I tested two versions (same errors for both): current master, v0.4.1\nPython version: 3.6.6\nCUDA/cuDNN version: None\nGPU models and configuration: No CUDA\nGCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.12.0\nVersions of any other relevant libraries:\n[pip] 18.0", "body": "## Issue description\r\n\r\nI found several errors in autograd:\r\n* Segfault during `.backward()`/`.grad()` when using hook (e.g. through `.retain_grad()`) on non-reachable tensor, whose grad is implicitely calculated (`= 0`) because it is an output of a function in the gradient graph but is independent of the backprop root tensor. (See code and traceback below.)\r\n* No hook is called if this non-reachable tensor is an output of an index operation (e.g. `a[0].register_hook()` while root only depends on `a[1]` and complete `a` has `requires_grad`.) (See code below.) That issue is not related to the others, but I encountered it in the same run, so I want to mention it here, too.\r\n* If such a hook is called (e.g. `a0.register_hook()` from `a0, a1 = a.unbind()`), the `grad` argument is `None` but should be a tensor with `0`-values, because that is the actually used value for the required `a.grad` and therefore should be modifiable. (See code below.)\r\n\r\nHere is the traceback for the Segfault: (Notice the lines `<= #6`. But I think, the actual source of the problem is, that in [this](https://github.com/pytorch/pytorch/blob/6f6b03566ba3c4828f6ee87a772f9d161be0bae7/torch/csrc/autograd/engine.cpp#L447) line, the inputs are not but should be initialized as variables with `0`-values, as fallbacks if no function overrides them. Or am I wrong?)\r\n\r\nI really would like to fix this, but to be honest, I'm not sure if I have enough expertise to do it right. So I open this issue for others. I hope, it helps.\r\n\r\n## Code example\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.tensor([3., 5.], requires_grad=True)\r\n# (A)   With the following line, the callback for `a0` is not called.\r\n#       But it should be called (with `grad = 0`, implicitely calculated,\r\n#       because `a1` is independent of `a0`), because it is used\r\n#       for the required `a.grad` and therefore should be modifyable\r\n#       via hook. (Also see (C).)\r\n# a0, a1 = a[0], a[1]\r\n# (B)   With the following line instead, the callback for `a0` is\r\n#       called, but with two errors:\r\n# (B.a) The callback is called with `grad = None`, but it should be `= 0`\r\n#       (implicitely calculated, because `a1` is independent of `a0`).\r\n#       (Also see (C).)\r\n# (B.b) After the callback, it results in a `Segmentation fault`.\r\n#       (Not, if no callback was registered.)\r\na0, a1 = a.unbind()\r\n@a0.register_hook\r\ndef hook(grad):\r\n    print(grad)\r\n    # (C)   When `grad` is `None`, returning a non-None replacement throws\r\n    #       the Runtime Error \"can't replace a None gradient with a non-None value\".\r\n    #       Therefore the current behaviour allows no modification at all for\r\n    #       implicitely calculated `0`-gradients.\r\n    # return torch.tensor(1.)\r\na1.backward()\r\n# Above errors occure no matter whether using `.backward()` or `.grad()`.\r\n# torch.autograd.grad([a1], [a])\r\nprint(a.grad)\r\n```\r\n## Traceback\r\n\r\n\r\n```\r\n#0  0x00007fffeda1bfa7 in std::__atomic_base<unsigned long>::operator++ (this=0x8) at /usr/include/c++/5/bits/atomic_base.h:296\r\n#1  0x00007fffeda2978b in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::retain_ (this=0x7ffff39356f0)\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:163\r\n#2  0x00007fffeda28cc0 in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::intrusive_ptr (this=0x7ffff39356f0, rhs=...)\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:211\r\n#3  0x00007fffedae1bd1 in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::operator=<at::TensorImpl, at::UndefinedTensorImpl>(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl> const&) & (this=0x7fffe00012a0, rhs=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:252\r\n#4  0x00007fffedae0d1b in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::operator=(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl> const&) & (this=0x7fffe00012a0, rhs=...)\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:244\r\n#5  0x00007fffedad8bbf in at::Tensor::operator=(at::Tensor const&) & (this=0x7fffe00012a0, x=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/Tensor.h:105\r\n#6  0x00007fffedc9c469 in torch::autograd::Variable::operator= (this=0x7fffe00012a0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/variable.h:83\r\n#7  0x00007fffedcbf973 in torch::autograd::PyFunctionPreHook::operator() (this=0x1450cb0, values=std::vector of length 2, capacity 2 = {...}) at torch/csrc/autograd/python_hook.cpp:54\r\n#8  0x00007fffe8c6ca99 in torch::autograd::call_pre_hooks (fn=..., inputs=std::vector of length 2, capacity 2 = {...})\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:280\r\n#9  0x00007fffe8c6cec6 in torch::autograd::call_function (task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:350\r\n#10 0x00007fffe8c6d395 in torch::autograd::Engine::evaluate_function (this=0x7fffee58b900 <engine>, task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:394\r\n#11 0x00007fffe8c6c666 in torch::autograd::Engine::thread_main (this=0x7fffee58b900 <engine>, graph_task=0x0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:232\r\n#12 0x00007fffe8c6c4f7 in torch::autograd::Engine::thread_init (this=0x7fffee58b900 <engine>, device=-1) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:206\r\n#13 0x00007fffedca0630 in torch::autograd::python::PythonEngine::thread_init (this=0x7fffee58b900 <engine>, device=-1) at torch/csrc/autograd/python_engine.cpp:39\r\n#14 0x00007fffe8c8cc02 in std::_Mem_fn_base<void (torch::autograd::Engine::*)(int), true>::operator()<int, void>(torch::autograd::Engine*, int&&) const (this=0x145fa58, \r\n    __object=0x7fffee58b900 <engine>) at /usr/include/c++/5/functional:600\r\n#15 0x00007fffe8c8cb7f in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (this=0x145fa48)\r\n    at /usr/include/c++/5/functional:1531\r\n#16 0x00007fffe8c8ca02 in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::operator()() (this=0x145fa48)\r\n    at /usr/include/c++/5/functional:1520\r\n#17 0x00007fffe8c8c952 in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)> >::_M_run() (this=0x145fa30)\r\n    at /usr/include/c++/5/thread:115\r\n#18 0x00007fffe79a9c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#19 0x00007ffff7bc16ba in start_thread (arg=0x7ffff3936700) at pthread_create.c:333\r\n#20 0x00007ffff6da441d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n\r\n```\r\n\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): I tested two versions (same errors for both): current master from source, v0.4.1 via pip\r\n- Build command you used (if compiling from source): `NO_CUDA=1 DEBUG=1 python setup.py build develop`\r\n- OS: Linux Mint 18.3 Sylvia\r\n- PyTorch version:  I tested two versions (same errors for both): current master, v0.4.1\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: None\r\n- GPU models and configuration: No CUDA\r\n- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- CMake version: version 3.12.0\r\n- Versions of any other relevant libraries:\r\n[pip] 18.0"}