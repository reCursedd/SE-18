{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5832", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5832/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5832/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5832/events", "html_url": "https://github.com/pytorch/pytorch/issues/5832", "id": 305826938, "node_id": "MDU6SXNzdWUzMDU4MjY5Mzg=", "number": 5832, "title": "[feature request] Add a check that nn.Module is not returning a tensor.", "user": {"login": "Rizhiy", "id": 5617397, "node_id": "MDQ6VXNlcjU2MTczOTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5617397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rizhiy", "html_url": "https://github.com/Rizhiy", "followers_url": "https://api.github.com/users/Rizhiy/followers", "following_url": "https://api.github.com/users/Rizhiy/following{/other_user}", "gists_url": "https://api.github.com/users/Rizhiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rizhiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rizhiy/subscriptions", "organizations_url": "https://api.github.com/users/Rizhiy/orgs", "repos_url": "https://api.github.com/users/Rizhiy/repos", "events_url": "https://api.github.com/users/Rizhiy/events{/privacy}", "received_events_url": "https://api.github.com/users/Rizhiy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-16T07:26:49Z", "updated_at": "2018-03-16T09:17:39Z", "closed_at": "2018-03-16T09:17:39Z", "author_association": "NONE", "body_html": "<p>When running a forward pass through the network on multiple GPUs, if you forget to wrap return Tensor in a Variable you get the following error:</p>\n<div class=\"highlight highlight-source-python\"><pre>  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">357</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>\n    result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">74</span>, <span class=\"pl-k\">in</span> forward\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.gather(outputs, <span class=\"pl-c1\">self</span>.output_device)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">86</span>, <span class=\"pl-k\">in</span> gather\n    <span class=\"pl-k\">return</span> gather(outputs, output_device, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">65</span>, <span class=\"pl-k\">in</span> gather\n    <span class=\"pl-k\">return</span> gather_map(outputs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">60</span>, <span class=\"pl-k\">in</span> gather_map\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">type</span>(out)(<span class=\"pl-c1\">map</span>(gather_map, <span class=\"pl-c1\">zip</span>(<span class=\"pl-k\">*</span>outputs)))\n<span class=\"pl-c1\">TypeError</span>: torch.cuda.FloatTensor constructor received an invalid combination of arguments <span class=\"pl-k\">-</span> got (<span class=\"pl-c1\">map</span>), but expected one of:\n <span class=\"pl-k\">*</span> no arguments\n <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">int</span> <span class=\"pl-c1\">...</span>)\n      didn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t match because some of the arguments have invalid types: (!map!)<span class=\"pl-ii\"></span></span>\n <span class=\"pl-k\">*</span> (torch.cuda.FloatTensor viewed_tensor)\n      didn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t match because some of the arguments have invalid types: (!map!)<span class=\"pl-ii\"></span></span>\n <span class=\"pl-k\">*</span> (torch.Size size)\n      didn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t match because some of the arguments have invalid types: (!map!)<span class=\"pl-ii\"></span></span>\n <span class=\"pl-k\">*</span> (torch.cuda.FloatStorage data)\n      didn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t match because some of the arguments have invalid types: (!map!)<span class=\"pl-ii\"></span></span>\n <span class=\"pl-k\">*</span> (Sequence data)\n      didn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t match because some of the arguments have invalid types: (!map!)<span class=\"pl-ii\"></span></span></pre></div>\n<p>This is not very helpful and frankly a bit misleading (as it says it expects <code>torch.cuda.FloatTensor</code>).</p>\n<p>This problem only occurs when running on multiple GPUs, which makes it a bit harder to diagnose.</p>\n<p>I request to either add an assert that checks that <code>outputs</code> is not a <code>Tensor</code> or make it work with Tensors.</p>", "body_text": "When running a forward pass through the network on multiple GPUs, if you forget to wrap return Tensor in a Variable you get the following error:\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 74, in forward\n    return self.gather(outputs, self.output_device)\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 86, in gather\n    return gather(outputs, output_device, dim=self.dim)\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 65, in gather\n    return gather_map(outputs)\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 60, in gather_map\n    return type(out)(map(gather_map, zip(*outputs)))\nTypeError: torch.cuda.FloatTensor constructor received an invalid combination of arguments - got (map), but expected one of:\n * no arguments\n * (int ...)\n      didn't match because some of the arguments have invalid types: (!map!)\n * (torch.cuda.FloatTensor viewed_tensor)\n      didn't match because some of the arguments have invalid types: (!map!)\n * (torch.Size size)\n      didn't match because some of the arguments have invalid types: (!map!)\n * (torch.cuda.FloatStorage data)\n      didn't match because some of the arguments have invalid types: (!map!)\n * (Sequence data)\n      didn't match because some of the arguments have invalid types: (!map!)\nThis is not very helpful and frankly a bit misleading (as it says it expects torch.cuda.FloatTensor).\nThis problem only occurs when running on multiple GPUs, which makes it a bit harder to diagnose.\nI request to either add an assert that checks that outputs is not a Tensor or make it work with Tensors.", "body": "When running a forward pass through the network on multiple GPUs, if you forget to wrap return Tensor in a Variable you get the following error:\r\n```python\r\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 74, in forward\r\n    return self.gather(outputs, self.output_device)\r\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 86, in gather\r\n    return gather(outputs, output_device, dim=self.dim)\r\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 65, in gather\r\n    return gather_map(outputs)\r\n  File \"/home/rizhiy/anaconda3/envs/pytorch-detection/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 60, in gather_map\r\n    return type(out)(map(gather_map, zip(*outputs)))\r\nTypeError: torch.cuda.FloatTensor constructor received an invalid combination of arguments - got (map), but expected one of:\r\n * no arguments\r\n * (int ...)\r\n      didn't match because some of the arguments have invalid types: (!map!)\r\n * (torch.cuda.FloatTensor viewed_tensor)\r\n      didn't match because some of the arguments have invalid types: (!map!)\r\n * (torch.Size size)\r\n      didn't match because some of the arguments have invalid types: (!map!)\r\n * (torch.cuda.FloatStorage data)\r\n      didn't match because some of the arguments have invalid types: (!map!)\r\n * (Sequence data)\r\n      didn't match because some of the arguments have invalid types: (!map!)\r\n```\r\n\r\nThis is not very helpful and frankly a bit misleading (as it says it expects `torch.cuda.FloatTensor`).\r\n\r\nThis problem only occurs when running on multiple GPUs, which makes it a bit harder to diagnose.\r\n\r\nI request to either add an assert that checks that `outputs` is not a `Tensor` or make it work with Tensors."}