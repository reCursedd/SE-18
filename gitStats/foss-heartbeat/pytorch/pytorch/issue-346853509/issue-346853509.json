{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10156", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10156/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10156/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10156/events", "html_url": "https://github.com/pytorch/pytorch/pull/10156", "id": 346853509, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA1NjE0MjM2", "number": 10156, "title": "Optimize max_pooling for inference for MKL-DNN/IDEEP device", "user": {"login": "jgong5", "id": 8359223, "node_id": "MDQ6VXNlcjgzNTkyMjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/8359223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgong5", "html_url": "https://github.com/jgong5", "followers_url": "https://api.github.com/users/jgong5/followers", "following_url": "https://api.github.com/users/jgong5/following{/other_user}", "gists_url": "https://api.github.com/users/jgong5/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgong5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgong5/subscriptions", "organizations_url": "https://api.github.com/users/jgong5/orgs", "repos_url": "https://api.github.com/users/jgong5/repos", "events_url": "https://api.github.com/users/jgong5/events{/privacy}", "received_events_url": "https://api.github.com/users/jgong5/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-02T04:17:52Z", "updated_at": "2018-11-23T15:48:36Z", "closed_at": "2018-08-11T06:15:19Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10156", "html_url": "https://github.com/pytorch/pytorch/pull/10156", "diff_url": "https://github.com/pytorch/pytorch/pull/10156.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10156.patch"}, "body_html": "<p>Optimize the max_pooling operation for inference path by setting the \"inference\" flag to the underlying MKL-DNN, saving the computation and store of max indices which is only needed for training. To make the API compatible, training mode is still the default and inference mode is set in the optimizeForIdeep path.<br>\nTest shows the speed-up of a single max_pooling operation is up to 7X on BDW.</p>", "body_text": "Optimize the max_pooling operation for inference path by setting the \"inference\" flag to the underlying MKL-DNN, saving the computation and store of max indices which is only needed for training. To make the API compatible, training mode is still the default and inference mode is set in the optimizeForIdeep path.\nTest shows the speed-up of a single max_pooling operation is up to 7X on BDW.", "body": "Optimize the max_pooling operation for inference path by setting the \"inference\" flag to the underlying MKL-DNN, saving the computation and store of max indices which is only needed for training. To make the API compatible, training mode is still the default and inference mode is set in the optimizeForIdeep path.\r\nTest shows the speed-up of a single max_pooling operation is up to 7X on BDW."}