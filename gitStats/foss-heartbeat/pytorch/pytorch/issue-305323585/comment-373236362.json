{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373236362", "html_url": "https://github.com/pytorch/pytorch/issues/5790#issuecomment-373236362", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5790", "id": 373236362, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzIzNjM2Mg==", "user": {"login": "daemon", "id": 6188572, "node_id": "MDQ6VXNlcjYxODg1NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6188572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daemon", "html_url": "https://github.com/daemon", "followers_url": "https://api.github.com/users/daemon/followers", "following_url": "https://api.github.com/users/daemon/following{/other_user}", "gists_url": "https://api.github.com/users/daemon/gists{/gist_id}", "starred_url": "https://api.github.com/users/daemon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daemon/subscriptions", "organizations_url": "https://api.github.com/users/daemon/orgs", "repos_url": "https://api.github.com/users/daemon/repos", "events_url": "https://api.github.com/users/daemon/events{/privacy}", "received_events_url": "https://api.github.com/users/daemon/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-15T02:07:50Z", "updated_at": "2018-03-15T02:07:50Z", "author_association": "NONE", "body_html": "<p>Yep. The idea is to pass some weights <em>w</em> through a user-specified function <em>g(w)</em> for each forward pass, before the layer operates on the input. <em>g(w)</em> is then used for the weights instead of <em>w</em> for that layer. <em>g</em> would of course be the identity function in the normal case. Here are a few practical examples:</p>\n<ol>\n<li>Pruning<br>\nWe would like to zero out weights according to some criterion. Yes--we can zero the weights with the current implementation, but it's more elegant to have something like the following:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>linear <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">1024</span>)\nconv <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>)\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">magnitude_prune</span>(<span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">max_magnitude</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">...</span>, <span class=\"pl-smi\">pct</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">...</span>):\n    <span class=\"pl-c1\">...</span>\n    <span class=\"pl-k\">return</span> prune_output\nlinear.hook_weight(magnitude_prune, <span class=\"pl-v\">max_magnitude</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">pct</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\nconv.hook_weight(magnitude_prune, <span class=\"pl-v\">max_magnitude</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">pct</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)</pre></div>\n<p>This could represent pruning 0.1% of the weights (at every forward pass) that are less than 0.1. Beyond this simple example, there are other methods that necessarily involve hooking weights. Consider <a href=\"https://openreview.net/forum?id=H1Y8hhg0b\" rel=\"nofollow\">this paper</a>:</p>\n<pre><code>function g(w):\n    z &lt;- z ~ p(z | phi)\n    w_new &lt;- z * w\n    return w_new\n\nlayer.hook_weight(g)\n</code></pre>\n<p>where <code>z ~ p(z | phi)</code> is the (continuous) hard concrete distribution, <code>w</code> are the original weights, and <code>phi</code> is a learnable parameter. During training, <code>dL / dphi</code> is computed using backprop and used to update <code>phi</code>. Since <code>phi</code> is a parameter, this suggests that <code>g</code> can also take on the form of an <code>nn.Module</code>.</p>\n<ol start=\"2\">\n<li>Quantization<br>\nWe would like to quantize some weights <em>w</em>. Consider the case of <a href=\"https://arxiv.org/abs/1609.07061\" rel=\"nofollow\">1-bit quantization</a>:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">BinarizeFunction</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Outputs +/- 1 depending on magnitude<span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">return</span> x.clamp(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>).ceil() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-k\">return</span> grad_output <span class=\"pl-c\"><span class=\"pl-c\">#</span> straight-through estimator</span>\n\nbinarize_function <span class=\"pl-k\">=</span> BinarizeFunction.apply\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">binarize</span>(<span class=\"pl-smi\">w</span>):\n    w.clamp_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> binarize_function(w)\n\nlayer.hook_weight(binarize)</pre></div>\n<p>This isn't completely correct; it's just an illustrative example. For more concrete examples, please see<a href=\"https://github.com/castorini/candle/blob/master/candle/proxy.py\">[1]</a><a href=\"https://github.com/castorini/candle/blob/master/candle/prune.py\">[2]</a><a href=\"https://github.com/castorini/candle/blob/master/candle/regularize.py\">[3]</a>.</p>", "body_text": "Yep. The idea is to pass some weights w through a user-specified function g(w) for each forward pass, before the layer operates on the input. g(w) is then used for the weights instead of w for that layer. g would of course be the identity function in the normal case. Here are a few practical examples:\n\nPruning\nWe would like to zero out weights according to some criterion. Yes--we can zero the weights with the current implementation, but it's more elegant to have something like the following:\n\nlinear = nn.Linear(1024, 1024)\nconv = nn.Conv2d(3, 64, 3)\n...\ndef magnitude_prune(weight, max_magnitude=..., pct=...):\n    ...\n    return prune_output\nlinear.hook_weight(magnitude_prune, max_magnitude=0.01, pct=0.1)\nconv.hook_weight(magnitude_prune, max_magnitude=0.1, pct=0.1)\nThis could represent pruning 0.1% of the weights (at every forward pass) that are less than 0.1. Beyond this simple example, there are other methods that necessarily involve hooking weights. Consider this paper:\nfunction g(w):\n    z <- z ~ p(z | phi)\n    w_new <- z * w\n    return w_new\n\nlayer.hook_weight(g)\n\nwhere z ~ p(z | phi) is the (continuous) hard concrete distribution, w are the original weights, and phi is a learnable parameter. During training, dL / dphi is computed using backprop and used to update phi. Since phi is a parameter, this suggests that g can also take on the form of an nn.Module.\n\nQuantization\nWe would like to quantize some weights w. Consider the case of 1-bit quantization:\n\nclass BinarizeFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        \"\"\"Outputs +/- 1 depending on magnitude\"\"\"\n        return x.clamp(0, 1).ceil() * 2 - 1\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output # straight-through estimator\n\nbinarize_function = BinarizeFunction.apply\n\ndef binarize(w):\n    w.clamp_(0, 1)\n    return binarize_function(w)\n\nlayer.hook_weight(binarize)\nThis isn't completely correct; it's just an illustrative example. For more concrete examples, please see[1][2][3].", "body": "Yep. The idea is to pass some weights _w_ through a user-specified function _g(w)_ for each forward pass, before the layer operates on the input. _g(w)_ is then used for the weights instead of _w_ for that layer. _g_ would of course be the identity function in the normal case. Here are a few practical examples:\r\n\r\n1. Pruning\r\nWe would like to zero out weights according to some criterion. Yes--we can zero the weights with the current implementation, but it's more elegant to have something like the following:\r\n\r\n```python\r\nlinear = nn.Linear(1024, 1024)\r\nconv = nn.Conv2d(3, 64, 3)\r\n...\r\ndef magnitude_prune(weight, max_magnitude=..., pct=...):\r\n    ...\r\n    return prune_output\r\nlinear.hook_weight(magnitude_prune, max_magnitude=0.01, pct=0.1)\r\nconv.hook_weight(magnitude_prune, max_magnitude=0.1, pct=0.1)\r\n```\r\n\r\nThis could represent pruning 0.1% of the weights (at every forward pass) that are less than 0.1. Beyond this simple example, there are other methods that necessarily involve hooking weights. Consider [this paper](https://openreview.net/forum?id=H1Y8hhg0b):\r\n\r\n```\r\nfunction g(w):\r\n    z <- z ~ p(z | phi)\r\n    w_new <- z * w\r\n    return w_new\r\n\r\nlayer.hook_weight(g)\r\n```\r\n\r\nwhere `z ~ p(z | phi)` is the (continuous) hard concrete distribution, `w` are the original weights, and `phi` is a learnable parameter. During training, `dL / dphi` is computed using backprop and used to update `phi`. Since `phi` is a parameter, this suggests that `g` can also take on the form of an `nn.Module`.\r\n\r\n2. Quantization\r\nWe would like to quantize some weights _w_. Consider the case of [1-bit quantization](https://arxiv.org/abs/1609.07061):\r\n\r\n```python\r\nclass BinarizeFunction(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        \"\"\"Outputs +/- 1 depending on magnitude\"\"\"\r\n        return x.clamp(0, 1).ceil() * 2 - 1\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        return grad_output # straight-through estimator\r\n\r\nbinarize_function = BinarizeFunction.apply\r\n\r\ndef binarize(w):\r\n    w.clamp_(0, 1)\r\n    return binarize_function(w)\r\n\r\nlayer.hook_weight(binarize)\r\n```\r\n\r\nThis isn't completely correct; it's just an illustrative example. For more concrete examples, please see[[1]](https://github.com/castorini/candle/blob/master/candle/proxy.py)[[2]](https://github.com/castorini/candle/blob/master/candle/prune.py)[[3]](https://github.com/castorini/candle/blob/master/candle/regularize.py)."}