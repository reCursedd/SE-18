{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5790", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5790/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5790/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5790/events", "html_url": "https://github.com/pytorch/pytorch/issues/5790", "id": 305323585, "node_id": "MDU6SXNzdWUzMDUzMjM1ODU=", "number": 5790, "title": "Add hookable weights", "user": {"login": "daemon", "id": 6188572, "node_id": "MDQ6VXNlcjYxODg1NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6188572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daemon", "html_url": "https://github.com/daemon", "followers_url": "https://api.github.com/users/daemon/followers", "following_url": "https://api.github.com/users/daemon/following{/other_user}", "gists_url": "https://api.github.com/users/daemon/gists{/gist_id}", "starred_url": "https://api.github.com/users/daemon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daemon/subscriptions", "organizations_url": "https://api.github.com/users/daemon/orgs", "repos_url": "https://api.github.com/users/daemon/repos", "events_url": "https://api.github.com/users/daemon/events{/privacy}", "received_events_url": "https://api.github.com/users/daemon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-03-14T20:50:43Z", "updated_at": "2018-03-15T16:56:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>It would be nice to have hookable weights, where per-layer parameters can be operated upon and determined dynamically during each forward pass. The following is a short list of techniques that could benefit from this functionality:</p>\n<ul>\n<li>Quantization<a href=\"https://arxiv.org/abs/1609.07061\" rel=\"nofollow\">[1]</a></li>\n<li>Pruning<a href=\"https://arxiv.org/abs/1510.00149\" rel=\"nofollow\">[2]</a><a href=\"https://arxiv.org/abs/1707.06168\" rel=\"nofollow\">[3]</a><a href=\"https://arxiv.org/abs/1802.00124\" rel=\"nofollow\">[4]</a></li>\n<li>Stochastic gradient estimation<a href=\"https://arxiv.org/abs/1711.00123\" rel=\"nofollow\">[5]</a><a href=\"https://arxiv.org/abs/1703.07370\" rel=\"nofollow\">[6]</a></li>\n<li>DropConnect<a href=\"https://cs.nyu.edu/~wanli/dropc/\" rel=\"nofollow\">[7]</a></li>\n<li>Regularization<a href=\"https://openreview.net/forum?id=H1Y8hhg0b\" rel=\"nofollow\">[8]</a></li>\n<li>Manifold Tangent Classifier<a href=\"https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf\" rel=\"nofollow\">[9]</a></li>\n</ul>\n<p>I've begun work on <a href=\"https://github.com/castorini/candle\">such a framework</a>, but the code is research-oriented and not production ready. I think the community would benefit from having official support for hookable weights.</p>\n<p>I think it would also be interesting to have a more extensible backprop framework, in the sense of allowing for computation of different user-defined quantities across the computation graph. For example, the Hessian diagonals can be computed efficiently<a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf\" rel=\"nofollow\">[10]</a> using a backprop-like approach.</p>\n<p>Thanks for reading.</p>", "body_text": "It would be nice to have hookable weights, where per-layer parameters can be operated upon and determined dynamically during each forward pass. The following is a short list of techniques that could benefit from this functionality:\n\nQuantization[1]\nPruning[2][3][4]\nStochastic gradient estimation[5][6]\nDropConnect[7]\nRegularization[8]\nManifold Tangent Classifier[9]\n\nI've begun work on such a framework, but the code is research-oriented and not production ready. I think the community would benefit from having official support for hookable weights.\nI think it would also be interesting to have a more extensible backprop framework, in the sense of allowing for computation of different user-defined quantities across the computation graph. For example, the Hessian diagonals can be computed efficiently[10] using a backprop-like approach.\nThanks for reading.", "body": "It would be nice to have hookable weights, where per-layer parameters can be operated upon and determined dynamically during each forward pass. The following is a short list of techniques that could benefit from this functionality:\r\n\r\n- Quantization[[1]](https://arxiv.org/abs/1609.07061)\r\n- Pruning[[2]](https://arxiv.org/abs/1510.00149)[[3]](https://arxiv.org/abs/1707.06168)[[4]](https://arxiv.org/abs/1802.00124)\r\n- Stochastic gradient estimation[[5]](https://arxiv.org/abs/1711.00123)[[6]](https://arxiv.org/abs/1703.07370)\r\n- DropConnect[[7]](https://cs.nyu.edu/~wanli/dropc/)\r\n- Regularization[[8]](https://openreview.net/forum?id=H1Y8hhg0b)\r\n- Manifold Tangent Classifier[[9]](https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf)\r\n\r\nI've begun work on [such a framework](https://github.com/castorini/candle), but the code is research-oriented and not production ready. I think the community would benefit from having official support for hookable weights.\r\n\r\nI think it would also be interesting to have a more extensible backprop framework, in the sense of allowing for computation of different user-defined quantities across the computation graph. For example, the Hessian diagonals can be computed efficiently[[10]](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) using a backprop-like approach.\r\n\r\nThanks for reading.\r\n"}