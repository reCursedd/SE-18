{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13669", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13669/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13669/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13669/events", "html_url": "https://github.com/pytorch/pytorch/issues/13669", "id": 378314502, "node_id": "MDU6SXNzdWUzNzgzMTQ1MDI=", "number": 13669, "title": "JIT gives wrong gradients", "user": {"login": "fehiepsi", "id": 4736342, "node_id": "MDQ6VXNlcjQ3MzYzNDI=", "avatar_url": "https://avatars1.githubusercontent.com/u/4736342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fehiepsi", "html_url": "https://github.com/fehiepsi", "followers_url": "https://api.github.com/users/fehiepsi/followers", "following_url": "https://api.github.com/users/fehiepsi/following{/other_user}", "gists_url": "https://api.github.com/users/fehiepsi/gists{/gist_id}", "starred_url": "https://api.github.com/users/fehiepsi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fehiepsi/subscriptions", "organizations_url": "https://api.github.com/users/fehiepsi/orgs", "repos_url": "https://api.github.com/users/fehiepsi/repos", "events_url": "https://api.github.com/users/fehiepsi/events{/privacy}", "received_events_url": "https://api.github.com/users/fehiepsi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-07T14:36:16Z", "updated_at": "2018-11-07T21:48:22Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>In some situation, JIT gives wrong gradients in backward pass. The following script shows the problem. Note that not only <code>.exp()</code> but also with operators such as <code>.log()</code>, we get the same issue.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<pre><code>import torch\nimport torch.autograd as autograd\n\ndef fn(z):\n    return (z.exp() - 0).sum() + (z.exp() - torch.zeros(1000)).sum()\n\nz = torch.tensor(1., requires_grad=True)\nfn_jit = torch.jit.trace(fn, (z,))\nprint(fn(z))\nprint(fn_jit(z))\nprint(autograd.grad(fn(z), (z,)))\nprint(autograd.grad(fn_jit(z), (z,)))\n</code></pre>\n<p>Output:</p>\n<pre><code>tensor(2721.0005, grad_fn=&lt;AddBackward0&gt;)\ntensor(2721.0005, grad_fn=&lt;AddBackward0&gt;)\n(tensor(2721.),)\n(tensor(5436.5645),)  # expected 2721\n</code></pre>\n<h2>Expected behavior</h2>\n<p>Gradients of <code>fn</code> and <code>fn_jit</code> should be equal.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version: nightly build (20181022) using <code>conda</code></li>\n<li>OS: Linux</li>\n<li>Python version: 3.6</li>\n</ul>", "body_text": "\ud83d\udc1b Bug\nIn some situation, JIT gives wrong gradients in backward pass. The following script shows the problem. Note that not only .exp() but also with operators such as .log(), we get the same issue.\nTo Reproduce\nSteps to reproduce the behavior:\nimport torch\nimport torch.autograd as autograd\n\ndef fn(z):\n    return (z.exp() - 0).sum() + (z.exp() - torch.zeros(1000)).sum()\n\nz = torch.tensor(1., requires_grad=True)\nfn_jit = torch.jit.trace(fn, (z,))\nprint(fn(z))\nprint(fn_jit(z))\nprint(autograd.grad(fn(z), (z,)))\nprint(autograd.grad(fn_jit(z), (z,)))\n\nOutput:\ntensor(2721.0005, grad_fn=<AddBackward0>)\ntensor(2721.0005, grad_fn=<AddBackward0>)\n(tensor(2721.),)\n(tensor(5436.5645),)  # expected 2721\n\nExpected behavior\nGradients of fn and fn_jit should be equal.\nEnvironment\n\nPyTorch Version: nightly build (20181022) using conda\nOS: Linux\nPython version: 3.6", "body": "## \ud83d\udc1b Bug\r\n\r\nIn some situation, JIT gives wrong gradients in backward pass. The following script shows the problem. Note that not only `.exp()` but also with operators such as `.log()`, we get the same issue.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nimport torch.autograd as autograd\r\n\r\ndef fn(z):\r\n    return (z.exp() - 0).sum() + (z.exp() - torch.zeros(1000)).sum()\r\n\r\nz = torch.tensor(1., requires_grad=True)\r\nfn_jit = torch.jit.trace(fn, (z,))\r\nprint(fn(z))\r\nprint(fn_jit(z))\r\nprint(autograd.grad(fn(z), (z,)))\r\nprint(autograd.grad(fn_jit(z), (z,)))\r\n```\r\nOutput:\r\n```\r\ntensor(2721.0005, grad_fn=<AddBackward0>)\r\ntensor(2721.0005, grad_fn=<AddBackward0>)\r\n(tensor(2721.),)\r\n(tensor(5436.5645),)  # expected 2721\r\n```\r\n## Expected behavior\r\n\r\nGradients of `fn` and `fn_jit` should be equal.\r\n\r\n## Environment\r\n\r\n- PyTorch Version: nightly build (20181022) using `conda`\r\n- OS: Linux\r\n- Python version: 3.6"}