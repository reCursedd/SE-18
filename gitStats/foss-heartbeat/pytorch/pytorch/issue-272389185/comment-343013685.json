{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343013685", "html_url": "https://github.com/pytorch/pytorch/issues/3580#issuecomment-343013685", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3580", "id": 343013685, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzAxMzY4NQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-09T01:04:02Z", "updated_at": "2017-11-09T01:04:02Z", "author_association": "MEMBER", "body_html": "<p>I'm not sure we want to add such convenience function (but others might have different opinions).<br>\nI think that there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line</p>\n<div class=\"highlight highlight-source-python\"><pre>torch.cat([model(x) <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> <span class=\"pl-c1\">input</span>.split(batch_size, <span class=\"pl-c1\">0</span>)], <span class=\"pl-c1\">0</span>)</pre></div>\n<p>What do you think?</p>", "body_text": "I'm not sure we want to add such convenience function (but others might have different opinions).\nI think that there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line\ntorch.cat([model(x) for x in input.split(batch_size, 0)], 0)\nWhat do you think?", "body": "I'm not sure we want to add such convenience function (but others might have different opinions).\r\nI think that there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line\r\n```python\r\ntorch.cat([model(x) for x in input.split(batch_size, 0)], 0)\r\n```\r\nWhat do you think?"}