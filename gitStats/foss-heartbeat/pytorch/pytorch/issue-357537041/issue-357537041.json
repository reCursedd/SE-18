{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11328", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11328/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11328/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11328/events", "html_url": "https://github.com/pytorch/pytorch/pull/11328", "id": 357537041, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEzNTI2MDg3", "number": 11328, "title": "ADD child module at ARBITRAY POSITION in current module", "user": {"login": "tomguluson92", "id": 19585240, "node_id": "MDQ6VXNlcjE5NTg1MjQw", "avatar_url": "https://avatars1.githubusercontent.com/u/19585240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomguluson92", "html_url": "https://github.com/tomguluson92", "followers_url": "https://api.github.com/users/tomguluson92/followers", "following_url": "https://api.github.com/users/tomguluson92/following{/other_user}", "gists_url": "https://api.github.com/users/tomguluson92/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomguluson92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomguluson92/subscriptions", "organizations_url": "https://api.github.com/users/tomguluson92/orgs", "repos_url": "https://api.github.com/users/tomguluson92/repos", "events_url": "https://api.github.com/users/tomguluson92/events{/privacy}", "received_events_url": "https://api.github.com/users/tomguluson92/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-09-06T07:54:11Z", "updated_at": "2018-09-07T15:37:27Z", "closed_at": "2018-09-07T15:37:27Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11328", "html_url": "https://github.com/pytorch/pytorch/pull/11328", "diff_url": "https://github.com/pytorch/pytorch/pull/11328.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11328.patch"}, "body_html": "<p>I revised member function add_module(...) of Module. I reinforce it by add child module to arbitary position of the current module.<br>\nSamples like this:</p>\n<p>I revised member function <code>add_module(...)</code> of Module. I reinforce it by add child module to arbitary position of the current module.<br>\nSamples like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Perceptron</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_features</span>, <span class=\"pl-smi\">out_features</span>, <span class=\"pl-smi\">hidden_features</span>):\n        <span class=\"pl-c1\">super</span>(Perceptron, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.layer1 <span class=\"pl-k\">=</span> nn.Linear(in_features, hidden_features)\n        <span class=\"pl-c1\">self</span>.layer2 <span class=\"pl-k\">=</span> nn.Linear(hidden_features, <span class=\"pl-c1\">4</span>)\n        <span class=\"pl-c1\">self</span>.layer3 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">4</span>, out_features)\n\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer2(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer3(x)\n\n        <span class=\"pl-k\">return</span> x</pre></div>\n<p>When we have a lot of module like this kind. if we need to add a child module to all of this kind of module.<br>\nWe can easily specify the child module position via setting the parameter <code>index</code> to a number(default is -1, the same<br>\nas now PyTorcher's already define.)</p>\n<p>It can easily use the following code to do add a specified child module between <strong>model.layer2</strong> and <strong>model.layer3</strong>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Perceptron_ADD</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Perceptron_ADD, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        model <span class=\"pl-k\">=</span> Perceptron(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">4</span>)\n\n        model.add_module(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>arbitarylayer<span class=\"pl-pds\">\"</span></span>, nn.Linear(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\n        <span class=\"pl-c1\">self</span>.model <span class=\"pl-k\">=</span> model\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.model(x)\n</pre></div>", "body_text": "I revised member function add_module(...) of Module. I reinforce it by add child module to arbitary position of the current module.\nSamples like this:\nI revised member function add_module(...) of Module. I reinforce it by add child module to arbitary position of the current module.\nSamples like this:\nclass Perceptron(nn.Module):\n\n    def __init__(self, in_features, out_features, hidden_features):\n        super(Perceptron, self).__init__()\n\n        self.layer1 = nn.Linear(in_features, hidden_features)\n        self.layer2 = nn.Linear(hidden_features, 4)\n        self.layer3 = nn.Linear(4, out_features)\n\n\n\n    def forward(self, x):\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        return x\nWhen we have a lot of module like this kind. if we need to add a child module to all of this kind of module.\nWe can easily specify the child module position via setting the parameter index to a number(default is -1, the same\nas now PyTorcher's already define.)\nIt can easily use the following code to do add a specified child module between model.layer2 and model.layer3:\nclass Perceptron_ADD(nn.Module):\n    def __init__(self):\n        super(Perceptron_ADD, self).__init__()\n        model = Perceptron(10, 5, 4)\n\n        model.add_module(\"arbitarylayer\", nn.Linear(4, 4), index=2)\n\n        self.model = model\n\n    def forward(self, x):\n        return self.model(x)", "body": "I revised member function add_module(...) of Module. I reinforce it by add child module to arbitary position of the current module.\r\nSamples like this:\r\n\r\nI revised member function `add_module(...)` of Module. I reinforce it by add child module to arbitary position of the current module.\r\nSamples like this:\r\n``` python\r\nclass Perceptron(nn.Module):\r\n\r\n    def __init__(self, in_features, out_features, hidden_features):\r\n        super(Perceptron, self).__init__()\r\n\r\n        self.layer1 = nn.Linear(in_features, hidden_features)\r\n        self.layer2 = nn.Linear(hidden_features, 4)\r\n        self.layer3 = nn.Linear(4, out_features)\r\n\r\n\r\n\r\n    def forward(self, x):\r\n\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n\r\n        return x\r\n```\r\nWhen we have a lot of module like this kind. if we need to add a child module to all of this kind of module.\r\nWe can easily specify the child module position via setting the parameter `index` to a number(default is -1, the same\r\nas now PyTorcher's already define.)\r\n\r\nIt can easily use the following code to do add a specified child module between **model.layer2** and **model.layer3**:\r\n``` python\r\nclass Perceptron_ADD(nn.Module):\r\n    def __init__(self):\r\n        super(Perceptron_ADD, self).__init__()\r\n        model = Perceptron(10, 5, 4)\r\n\r\n        model.add_module(\"arbitarylayer\", nn.Linear(4, 4), index=2)\r\n\r\n        self.model = model\r\n\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n"}