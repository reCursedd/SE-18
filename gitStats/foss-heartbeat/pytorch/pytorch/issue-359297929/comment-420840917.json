{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/420840917", "html_url": "https://github.com/pytorch/pytorch/pull/11565#issuecomment-420840917", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11565", "id": 420840917, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDg0MDkxNw==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-13T00:20:01Z", "updated_at": "2018-09-13T00:20:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> who has looked into related code</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20233731\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mingfeima\">@mingfeima</a> - Thank you for this! Could I also ask you to also look into smaller tensors to make sure that the constant overhead etc. stays the same and if this scales as expected. And what about single core performance and NUMA behavior, i.e. try binding memory and cpu to the same node?</p>\n<p>Also could you also (briefly) look at the behavior regarding different CPU capabilities using the environment variables [here].(<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/0b2e72ded0c14a776de76c29be7f11ea49572075/aten/src/ATen/native/DispatchStub.cpp#L14\">pytorch/aten/src/ATen/native/DispatchStub.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 14\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/0b2e72ded0c14a776de76c29be7f11ea49572075\">0b2e72d</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L14\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"14\"></td>\n          <td id=\"LC14\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> (<span class=\"pl-c1\">strcmp</span>(envar, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>avx2<span class=\"pl-pds\">\"</span></span>) == <span class=\"pl-c1\">0</span>) { </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n). Just in case that for capabilities the performance is much worse.</p>", "body_text": "cc @colesbury who has looked into related code\n@mingfeima - Thank you for this! Could I also ask you to also look into smaller tensors to make sure that the constant overhead etc. stays the same and if this scales as expected. And what about single core performance and NUMA behavior, i.e. try binding memory and cpu to the same node?\nAlso could you also (briefly) look at the behavior regarding different CPU capabilities using the environment variables [here].(\n  \n    \n      pytorch/aten/src/ATen/native/DispatchStub.cpp\n    \n    \n         Line 14\n      in\n      0b2e72d\n    \n    \n    \n    \n\n        \n          \n           if (strcmp(envar, \"avx2\") == 0) { \n        \n    \n  \n\n). Just in case that for capabilities the performance is much worse.", "body": "cc @colesbury who has looked into related code\r\n\r\n@mingfeima - Thank you for this! Could I also ask you to also look into smaller tensors to make sure that the constant overhead etc. stays the same and if this scales as expected. And what about single core performance and NUMA behavior, i.e. try binding memory and cpu to the same node?\r\n\r\nAlso could you also (briefly) look at the behavior regarding different CPU capabilities using the environment variables [here].(https://github.com/pytorch/pytorch/blob/0b2e72ded0c14a776de76c29be7f11ea49572075/aten/src/ATen/native/DispatchStub.cpp#L14). Just in case that for capabilities the performance is much worse."}