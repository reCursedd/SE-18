{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11565", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11565/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11565/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11565/events", "html_url": "https://github.com/pytorch/pytorch/pull/11565", "id": 359297929, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE0ODIyNzE4", "number": 11565, "title": "optimize norm on ATen CPU backend", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-09-12T03:05:16Z", "updated_at": "2018-11-23T15:51:21Z", "closed_at": "2018-09-14T02:41:58Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11565", "html_url": "https://github.com/pytorch/pytorch/pull/11565", "diff_url": "https://github.com/pytorch/pytorch/pull/11565.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11565.patch"}, "body_html": "<p>current torch.norm() runs sequentially on CPU. This PR did parallelization and vectorization of torch.norm() on ATen CPU path, roughly provide 2 order of magnitude performance boost.</p>\n<p>Performance is benchmarks on Xeon skylake 8180, 2*28 cores @2.5GHz, using the following script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> time <span class=\"pl-k\">import</span> time\n\ncount <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\nsize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">1000</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_norm</span>(<span class=\"pl-smi\">p</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>):\n    a <span class=\"pl-k\">=</span> torch.randn(size)\n    tstart <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(count):\n        torch.norm(a, p)\n    tend <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>norm on size <span class=\"pl-c1\">%d</span> tensor p = <span class=\"pl-c1\">%d</span>: <span class=\"pl-c1\">%f</span> s<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (size, p, (tend<span class=\"pl-k\">-</span>tstart)))\n\n<span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">4</span>):\n    test_norm(p)</pre></div>\n<p>without this optimization,</p>\n<pre><code>(intel-pytorch) [mingfeim@mlt-skx065 unit_tests]$ python test_norm.py\nnorm on size 1000000 tensor p = 0: 1.071235 s\nnorm on size 1000000 tensor p = 1: 1.069149 s\nnorm on size 1000000 tensor p = 2: 1.068212 s\nnorm on size 1000000 tensor p = 3: 69.735312 s\n</code></pre>\n<p>and with this optimization,</p>\n<pre><code>(pytorch-tf) [mingfeim@mlt-skx053 unit_tests]$ python test_norm.py\nnorm on size 1000000 tensor p = 0: 0.127507 s\nnorm on size 1000000 tensor p = 1: 0.011867 s\nnorm on size 1000000 tensor p = 2: 0.011907 s\nnorm on size 1000000 tensor p = 3: 0.014470 s\n</code></pre>", "body_text": "current torch.norm() runs sequentially on CPU. This PR did parallelization and vectorization of torch.norm() on ATen CPU path, roughly provide 2 order of magnitude performance boost.\nPerformance is benchmarks on Xeon skylake 8180, 2*28 cores @2.5GHz, using the following script:\nimport torch\nfrom time import time\n\ncount = 1000\nsize = 1000*1000\n\ndef test_norm(p=2):\n    a = torch.randn(size)\n    tstart = time()\n    for i in range(count):\n        torch.norm(a, p)\n    tend = time()\n    print(\"norm on size %d tensor p = %d: %f s\" % (size, p, (tend-tstart)))\n\nfor p in range(4):\n    test_norm(p)\nwithout this optimization,\n(intel-pytorch) [mingfeim@mlt-skx065 unit_tests]$ python test_norm.py\nnorm on size 1000000 tensor p = 0: 1.071235 s\nnorm on size 1000000 tensor p = 1: 1.069149 s\nnorm on size 1000000 tensor p = 2: 1.068212 s\nnorm on size 1000000 tensor p = 3: 69.735312 s\n\nand with this optimization,\n(pytorch-tf) [mingfeim@mlt-skx053 unit_tests]$ python test_norm.py\nnorm on size 1000000 tensor p = 0: 0.127507 s\nnorm on size 1000000 tensor p = 1: 0.011867 s\nnorm on size 1000000 tensor p = 2: 0.011907 s\nnorm on size 1000000 tensor p = 3: 0.014470 s", "body": "current torch.norm() runs sequentially on CPU. This PR did parallelization and vectorization of torch.norm() on ATen CPU path, roughly provide 2 order of magnitude performance boost.\r\n\r\nPerformance is benchmarks on Xeon skylake 8180, 2*28 cores @2.5GHz, using the following script:\r\n```python\r\nimport torch\r\nfrom time import time\r\n\r\ncount = 1000\r\nsize = 1000*1000\r\n\r\ndef test_norm(p=2):\r\n    a = torch.randn(size)\r\n    tstart = time()\r\n    for i in range(count):\r\n        torch.norm(a, p)\r\n    tend = time()\r\n    print(\"norm on size %d tensor p = %d: %f s\" % (size, p, (tend-tstart)))\r\n\r\nfor p in range(4):\r\n    test_norm(p)\r\n```\r\n\r\nwithout this optimization,\r\n```\r\n(intel-pytorch) [mingfeim@mlt-skx065 unit_tests]$ python test_norm.py\r\nnorm on size 1000000 tensor p = 0: 1.071235 s\r\nnorm on size 1000000 tensor p = 1: 1.069149 s\r\nnorm on size 1000000 tensor p = 2: 1.068212 s\r\nnorm on size 1000000 tensor p = 3: 69.735312 s\r\n```\r\n\r\nand with this optimization,\r\n```\r\n(pytorch-tf) [mingfeim@mlt-skx053 unit_tests]$ python test_norm.py\r\nnorm on size 1000000 tensor p = 0: 0.127507 s\r\nnorm on size 1000000 tensor p = 1: 0.011867 s\r\nnorm on size 1000000 tensor p = 2: 0.011907 s\r\nnorm on size 1000000 tensor p = 3: 0.014470 s\r\n```"}