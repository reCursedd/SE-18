{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217229212", "pull_request_review_id": 154891821, "id": 217229212, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzIyOTIxMg==", "diff_hunk": "@@ -183,9 +183,163 @@ static void prod_kernel_impl(Tensor& result, const Tensor& self, at::optional<in\n   });\n }\n \n+template<typename scalar_t>\n+struct NormReduction {\n+  // reduction width in number of scalar elements\n+  static constexpr int WIDTH = 128 / sizeof(scalar_t);\n+  using Vec = Vec256<scalar_t>;\n+\n+  static void apply(Tensor& res, const Tensor& self, Scalar p, at::optional<int64_t> dim) {\n+    auto out_ = res.data<scalar_t>();\n+    auto data_ = self.data<scalar_t>();\n+    auto numel = self.numel();\n+    float pval = 0.0;\n+    if (p.isIntegral()){\n+      pval = p.to<int64_t>();\n+    } else if (p.isFloatingPoint()) {\n+      pval = p.to<float>();\n+    }\n+    if (!dim.has_value()) {\n+      *out_ = reduce_all(data_, numel,  pval);\n+      return;\n+    }\n+    int64_t n = self.size(*dim);\n+    int64_t stride = self.stride(*dim);\n+    // A contiguous tensor does not need to hold a meaningful stride\n+    // if the corresponding size is 1\n+    if (n == 1) {\n+      stride = 1;\n+      for (int64_t i = self.ndimension() - 1; i > *dim; i--) {\n+        stride *= self.size(i);\n+      }\n+    }\n+    int64_t batch = numel / n;\n+    parallel_for(0, batch, 1, [=](int64_t begin, int64_t end) {\n+      for (int64_t bi = begin; bi < end; bi++) {\n+        int64_t b = bi / stride;\n+        int64_t i = bi % stride;\n+        const scalar_t* data = &data_[b * n * stride + i];\n+        out_[bi] = norm_reduce(data, n, stride, pval);\n+      }\n+    });\n+  }\n+\n+  static scalar_t reduce_all(const scalar_t* data_, int64_t size,  float pval) {\n+    scalar_t sum = parallel_reduce(\n+      0,\n+      size,\n+      internal::GRAIN_SIZE,\n+      (scalar_t)0,\n+      [=](int64_t begin, int64_t end, scalar_t init) {\n+        const scalar_t* data = &data_[begin];\n+        int64_t n = end - begin;\n+        scalar_t result = norm_reduce(data, n, 1, pval);\n+        return result;\n+      },\n+      std::plus<scalar_t>());\n+    return sum;\n+  }\n+\n+  static scalar_t norm_reduce(const scalar_t* data, int64_t n, int64_t stride, float pval) {\n+    scalar_t result = 0.0;\n+    if (stride == 1 && (pval == 1 || pval == 2 || pval == 3) && n >= WIDTH) {\n+      int64_t n_rounded = round_down(n, WIDTH);\n+      scalar_t result1 = norm_reduce128(data, n_rounded, pval);\n+      scalar_t result2 = norm_reduce_sequential(data + n_rounded, n - n_rounded, stride, pval);\n+      result = std::pow(std::pow(result1, pval) + std::pow(result2, pval), 1.0/pval);", "path": "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "position": 67, "original_position": 67, "commit_id": "0b2e72ded0c14a776de76c29be7f11ea49572075", "original_commit_id": "0b2e72ded0c14a776de76c29be7f11ea49572075", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "body": "There is a (miserable) [bug in glibc2.23](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cpu/vml.h#L36) - just in case something unexpected pops up the note might be useful.", "created_at": "2018-09-13T00:23:28Z", "updated_at": "2018-11-23T15:51:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/11565#discussion_r217229212", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11565", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217229212"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11565#discussion_r217229212"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11565"}}, "body_html": "<p>There is a (miserable) <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cpu/vml.h#L36\">bug in glibc2.23</a> - just in case something unexpected pops up the note might be useful.</p>", "body_text": "There is a (miserable) bug in glibc2.23 - just in case something unexpected pops up the note might be useful."}