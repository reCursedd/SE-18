{"url": "https://api.github.com/repos/pytorch/pytorch/issues/735", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/735/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/735/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/735/events", "html_url": "https://github.com/pytorch/pytorch/pull/735", "id": 207307698, "node_id": "MDExOlB1bGxSZXF1ZXN0MTA1OTU2NjE0", "number": 735, "title": "sparse tensor operations", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-02-13T18:57:34Z", "updated_at": "2018-11-23T15:32:44Z", "closed_at": "2017-03-03T17:37:04Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/735", "html_url": "https://github.com/pytorch/pytorch/pull/735", "diff_url": "https://github.com/pytorch/pytorch/pull/735.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/735.patch"}, "body_html": "<p>This pull request adds more support for sparse operations in Pytorch.</p>\n<p>The original goals:</p>\n<ul>\n<li>ability to propagate sparse updates in a network (e.g. for updating an embedding matrix)</li>\n<li>ability to efficiently compute \"bag-of-words\" sentence embeddings (e.g. weighted average of word embeddings)</li>\n</ul>\n<p>This request implements the following individual features to achieve those goals:</p>\n<ul>\n<li>enable backpropagation of sparse gradients without conversion to dense tensors. In most cases a runtime exception is thrown when mixing different gradient types for the same variable</li>\n<li>add some methods for <code>THSTensor</code>: <code>zero</code>, elementwise <code>add</code> and <code>mul</code>, scalar <code>mul</code> and <code>div</code></li>\n<li>make <code>addcmul</code> method of <code>THTensor</code> compatible with sparse operands</li>\n<li>make <code>spmm</code> method accessible from Python (I had to use the name <code>dsmm</code> since <code>smm</code> was already taken. Maybe we should rename the current <code>smm</code> to <code>ssmm</code> to follow the convention)</li>\n<li><code>sparse_mask</code> method on <code>THTensor</code>. This produces a sparse tensor from a dense tensor, by using a sparse tensor as a mask. A value is only present in the output sparse tensor if it also exists in the mask. See the changes to <code>adagrad.py</code> for an example of why this is needed.</li>\n<li>update Adagrad code to use sparse updates when possible. I was hoping the optimizers wouldn't require any modification, but it looks like they do (let me know if you see a better option). In addition, not every optimizer supports sparse updates easily. For example it looks like Adam is accumulating moments over time, which effectively makes the updates dense. For the same reason, Adagrad doesn't support having both sparse updates + weight decay.</li>\n<li>leave <code>Variable</code>'s gradient to <code>None</code> by default (required updating a few tests). This is because there is no canonical zero gradient anymore (it could be dense or sparse, and if it is sparse we don't know how many dimensions are sparse)</li>\n<li>I also added the basic glue code to hook up the existing THCS (cuda sparse) tensor implementation to Python (I did pretty much the same as for TH, THC, THS). I did that mainly so that existing tests keep working even when cuda sparse gradients are involved. Most of the THCS operations are still stubs and will throw an exception with an error message, but it means the only thing remaining for GPU sparse ops support is to fill in the appropriate functions.</li>\n</ul>\n<p>...and last but not least: N-dimensional values for sparse tensors. This one is a slightly bigger item. Basically for things like applying sparse updates to embedding matrices, only the first dimension (the one that corresponds to the word index) is sparse. The other dimension is always dense (only whole embedding vectors are updated). An elegant solution is to make the <code>values</code> tensor N-dimensional instead of 1-dimensional. For an embedding matrix, the sparse gradient will have a <code>values</code> tensor of size <code>nnz * embedding_size</code> instead of just <code>nnz</code>. I had to update a few existing functions to make that work, but otherwise the changes are actually not that big. Existing usecases with scalar values should all work as usual.</p>", "body_text": "This pull request adds more support for sparse operations in Pytorch.\nThe original goals:\n\nability to propagate sparse updates in a network (e.g. for updating an embedding matrix)\nability to efficiently compute \"bag-of-words\" sentence embeddings (e.g. weighted average of word embeddings)\n\nThis request implements the following individual features to achieve those goals:\n\nenable backpropagation of sparse gradients without conversion to dense tensors. In most cases a runtime exception is thrown when mixing different gradient types for the same variable\nadd some methods for THSTensor: zero, elementwise add and mul, scalar mul and div\nmake addcmul method of THTensor compatible with sparse operands\nmake spmm method accessible from Python (I had to use the name dsmm since smm was already taken. Maybe we should rename the current smm to ssmm to follow the convention)\nsparse_mask method on THTensor. This produces a sparse tensor from a dense tensor, by using a sparse tensor as a mask. A value is only present in the output sparse tensor if it also exists in the mask. See the changes to adagrad.py for an example of why this is needed.\nupdate Adagrad code to use sparse updates when possible. I was hoping the optimizers wouldn't require any modification, but it looks like they do (let me know if you see a better option). In addition, not every optimizer supports sparse updates easily. For example it looks like Adam is accumulating moments over time, which effectively makes the updates dense. For the same reason, Adagrad doesn't support having both sparse updates + weight decay.\nleave Variable's gradient to None by default (required updating a few tests). This is because there is no canonical zero gradient anymore (it could be dense or sparse, and if it is sparse we don't know how many dimensions are sparse)\nI also added the basic glue code to hook up the existing THCS (cuda sparse) tensor implementation to Python (I did pretty much the same as for TH, THC, THS). I did that mainly so that existing tests keep working even when cuda sparse gradients are involved. Most of the THCS operations are still stubs and will throw an exception with an error message, but it means the only thing remaining for GPU sparse ops support is to fill in the appropriate functions.\n\n...and last but not least: N-dimensional values for sparse tensors. This one is a slightly bigger item. Basically for things like applying sparse updates to embedding matrices, only the first dimension (the one that corresponds to the word index) is sparse. The other dimension is always dense (only whole embedding vectors are updated). An elegant solution is to make the values tensor N-dimensional instead of 1-dimensional. For an embedding matrix, the sparse gradient will have a values tensor of size nnz * embedding_size instead of just nnz. I had to update a few existing functions to make that work, but otherwise the changes are actually not that big. Existing usecases with scalar values should all work as usual.", "body": "This pull request adds more support for sparse operations in Pytorch.\r\n\r\nThe original goals:\r\n\r\n- ability to propagate sparse updates in a network (e.g. for updating an embedding matrix)\r\n- ability to efficiently compute \"bag-of-words\" sentence embeddings (e.g. weighted average of word embeddings)\r\n\r\nThis request implements the following individual features to achieve those goals:\r\n\r\n- enable backpropagation of sparse gradients without conversion to dense tensors. In most cases a runtime exception is thrown when mixing different gradient types for the same variable\r\n- add some methods for `THSTensor`: `zero`, elementwise `add` and `mul`, scalar `mul` and `div`\r\n- make `addcmul` method of `THTensor` compatible with sparse operands\r\n- make `spmm` method accessible from Python (I had to use the name `dsmm` since `smm` was already taken. Maybe we should rename the current `smm` to `ssmm` to follow the convention)\r\n- `sparse_mask` method on `THTensor`. This produces a sparse tensor from a dense tensor, by using a sparse tensor as a mask. A value is only present in the output sparse tensor if it also exists in the mask. See the changes to `adagrad.py` for an example of why this is needed.\r\n- update Adagrad code to use sparse updates when possible. I was hoping the optimizers wouldn't require any modification, but it looks like they do (let me know if you see a better option). In addition, not every optimizer supports sparse updates easily. For example it looks like Adam is accumulating moments over time, which effectively makes the updates dense. For the same reason, Adagrad doesn't support having both sparse updates + weight decay.\r\n- leave `Variable`'s gradient to `None` by default (required updating a few tests). This is because there is no canonical zero gradient anymore (it could be dense or sparse, and if it is sparse we don't know how many dimensions are sparse)\r\n- I also added the basic glue code to hook up the existing THCS (cuda sparse) tensor implementation to Python (I did pretty much the same as for TH, THC, THS). I did that mainly so that existing tests keep working even when cuda sparse gradients are involved. Most of the THCS operations are still stubs and will throw an exception with an error message, but it means the only thing remaining for GPU sparse ops support is to fill in the appropriate functions. \r\n\r\n...and last but not least: N-dimensional values for sparse tensors. This one is a slightly bigger item. Basically for things like applying sparse updates to embedding matrices, only the first dimension (the one that corresponds to the word index) is sparse. The other dimension is always dense (only whole embedding vectors are updated). An elegant solution is to make the `values` tensor N-dimensional instead of 1-dimensional. For an embedding matrix, the sparse gradient will have a `values` tensor of size `nnz * embedding_size` instead of just `nnz`. I had to update a few existing functions to make that work, but otherwise the changes are actually not that big. Existing usecases with scalar values should all work as usual."}