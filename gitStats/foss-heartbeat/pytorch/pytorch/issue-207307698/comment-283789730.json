{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/283789730", "html_url": "https://github.com/pytorch/pytorch/pull/735#issuecomment-283789730", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/735", "id": 283789730, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mzc4OTczMA==", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-02T21:36:33Z", "updated_at": "2017-03-03T09:52:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> for the feedback! I will address your various comments.</p>\n<p>Regarding the difference between <code>nDimensionI</code> and <code>nDimensionV</code>: the idea is to allow values of arbitrary dimension instead of just scalars. This happens for example with word embedding matrices. The matrix will be of size num_words x embedding_size, and updates to that matrix will be sparse, but only along the first dimension (only a few words are used in each batch, however whenever a word embedding changes, the whole embedding changes). In this specific case, nDimensionI=1 (num_words) and nDimensionV=1 (embedding_size). If I call <code>to_dense</code> on a sparse matrix, the result has dimension <code>nDimensionI + nDimensionV</code>. So the \"indices\" and \"values\" dimensions are complementary, and can of course be different (for example in the typical sparse matrix case, nDimensionI=2 and nDimensionV=0). Let me know if I'm still unclear.</p>", "body_text": "Thanks @apaszke for the feedback! I will address your various comments.\nRegarding the difference between nDimensionI and nDimensionV: the idea is to allow values of arbitrary dimension instead of just scalars. This happens for example with word embedding matrices. The matrix will be of size num_words x embedding_size, and updates to that matrix will be sparse, but only along the first dimension (only a few words are used in each batch, however whenever a word embedding changes, the whole embedding changes). In this specific case, nDimensionI=1 (num_words) and nDimensionV=1 (embedding_size). If I call to_dense on a sparse matrix, the result has dimension nDimensionI + nDimensionV. So the \"indices\" and \"values\" dimensions are complementary, and can of course be different (for example in the typical sparse matrix case, nDimensionI=2 and nDimensionV=0). Let me know if I'm still unclear.", "body": "Thanks @apaszke for the feedback! I will address your various comments.\r\n\r\nRegarding the difference between `nDimensionI` and `nDimensionV`: the idea is to allow values of arbitrary dimension instead of just scalars. This happens for example with word embedding matrices. The matrix will be of size num_words x embedding_size, and updates to that matrix will be sparse, but only along the first dimension (only a few words are used in each batch, however whenever a word embedding changes, the whole embedding changes). In this specific case, nDimensionI=1 (num_words) and nDimensionV=1 (embedding_size). If I call `to_dense` on a sparse matrix, the result has dimension `nDimensionI + nDimensionV`. So the \"indices\" and \"values\" dimensions are complementary, and can of course be different (for example in the typical sparse matrix case, nDimensionI=2 and nDimensionV=0). Let me know if I'm still unclear."}