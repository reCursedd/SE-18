{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104188547", "pull_request_review_id": 25011605, "id": 104188547, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDE4ODU0Nw==", "diff_hunk": "@@ -5,6 +5,222 @@\n #define ROW_PTR2(t, r) (THTensor_(data)(t) + (r) * (t)->stride[0])\n #define COL_PTR2(t, c) (THTensor_(data)(t) + (c) * (t)->stride[1])\n \n+void THSTensor_(zero)(THSTensor *self) {\n+  self->nnz = 0;\n+}\n+\n+void THSTensor_(mul)(THSTensor *r_, THSTensor *t, real value) {\n+  if (r_ == t) {\n+    THTensor *r_values_ = THSTensor_(values)(r_);\n+    THTensor_(mul)(r_values_, r_values_, value);\n+    THTensor_(free)(r_values_);\n+  } else {\n+    THSTensor_(resizeAs)(r_, t);\n+\n+    THLongTensor *r_indices_ = THSTensor_(indices)(r_);\n+    THTensor *r_values_ = THSTensor_(values)(r_);\n+    THLongTensor *t_indices_ = THSTensor_(indices)(t);\n+    THTensor *t_values_ = THSTensor_(values)(t);\n+\n+    THLongTensor_resizeAs(r_indices_, t_indices_);\n+    THLongTensor_copy(r_indices_, t_indices_);\n+    THTensor_(mul)(r_values_, t_values_, value);\n+    r_->nnz = t->nnz;\n+    r_->contiguous = t->contiguous;\n+\n+    THLongTensor_free(r_indices_);\n+    THTensor_(free)(r_values_);\n+    THLongTensor_free(t_indices_);\n+    THTensor_(free)(t_values_);\n+  }\n+}\n+\n+void THSTensor_(div)(THSTensor *r_, THSTensor *t, real value) {\n+  if (r_ == t) {\n+    THTensor *r_values_ = THSTensor_(values)(r_);\n+    THTensor_(div)(r_values_, r_values_, value);\n+    THTensor_(free)(r_values_);\n+  } else {\n+    THSTensor_(resizeAs)(r_, t);\n+\n+    THLongTensor *r_indices_ = THSTensor_(indices)(r_);\n+    THTensor *r_values_ = THSTensor_(values)(r_);\n+    THLongTensor *t_indices_ = THSTensor_(indices)(t);\n+    THTensor *t_values_ = THSTensor_(values)(t);\n+\n+    THLongTensor_resizeAs(r_indices_, t_indices_);\n+    THLongTensor_copy(r_indices_, t_indices_);\n+    THTensor_(div)(r_values_, t_values_, value);\n+    r_->nnz = t->nnz;\n+    r_->contiguous = t->contiguous;\n+\n+    THLongTensor_free(r_indices_);\n+    THTensor_(free)(r_values_);\n+    THLongTensor_free(t_indices_);\n+    THTensor_(free)(t_values_);\n+  }\n+}\n+\n+void THSTensor_(cadd)(THSTensor *r_, THSTensor *t, real value, THSTensor *src) {\n+  if(!THSTensor_(isSameSizeAs)(t, src)) {\n+    THError(\"cadd operands have incompatible sizes or dimension types\");\n+  }\n+  THSTensor_(contiguous)(t);\n+  THSTensor_(contiguous)(src);\n+\n+  if (src->nnz == 0) {\n+    THSTensor_(copy)(r_, t);\n+    return;\n+  }\n+  if (t->nnz == 0) {\n+    THSTensor_(mul)(r_, src, value);\n+    return;\n+  }\n+\n+  // saving those because they can be overwritten when doing in-place operations\n+  ptrdiff_t t_nnz = t->nnz, s_nnz = src->nnz, max_nnz = t_nnz + s_nnz;\n+  long nDimI = THSTensor_(nDimensionI)(src);\n+  long nDimV = THSTensor_(nDimensionV)(src);\n+  THLongTensor *t_indices_ = THSTensor_(indices)(t);\n+  THTensor *t_values_ = THSTensor_(values)(t);\n+  THLongTensor *src_indices_ = THSTensor_(indices)(src);\n+  THTensor *s_values_ = THSTensor_(values)(src);\n+  THLongTensor *r_indices_ = THLongTensor_newWithSize2d(nDimI, max_nnz);\n+  THTensor *r_values_ = THSTensor_(newValuesWithSizeOf)(s_values_, max_nnz);\n+  THTensor_(zero)(r_values_);\n+  // TODO handle case where src values is empty\n+  THSTensor_(resizeAs)(r_, src);\n+  THSTensor_(move)(r_, r_indices_, r_values_);\n+\n+  THTensor *srcBuffer = THTensor_(new)();\n+  THTensor *dstBuffer = THTensor_(new)();\n+  long cmp, d;\n+  long r_i = 0, t_i = 0, s_i = 0;\n+  while (t_i < t_nnz || s_i < s_nnz) {\n+    if (t_i >= t_nnz) {\n+      cmp = -1;\n+    } else if (s_i >= s_nnz) {\n+      cmp = 1;\n+    } else {\n+      cmp = 0;\n+      for (d = 0; d < nDimI; d++) {\n+        if (THTensor_fastGet2d(t_indices_, d, t_i) < THTensor_fastGet2d(src_indices_, d, s_i)) {\n+          cmp = 1;\n+          break;\n+        }\n+        if (THTensor_fastGet2d(t_indices_, d, t_i) > THTensor_fastGet2d(src_indices_, d, s_i)) {\n+          cmp = -1;\n+          break;\n+        }\n+      }\n+    }\n+    if (cmp >= 0) {\n+      for (d = 0; d < nDimI; d++) {\n+        THTensor_fastSet2d(r_indices_, d, r_i, THTensor_fastGet2d(t_indices_, d, t_i));\n+      }\n+      THSTensor_(addSlice)(dstBuffer, dstBuffer, srcBuffer, r_values_, r_values_, 1, t_values_, 0, r_i, r_i, t_i);\n+      t_i++;\n+    }\n+    if (cmp <= 0) {\n+      for (d = 0; d < nDimI; d++) {\n+        THTensor_fastSet2d(r_indices_, d, r_i, THTensor_fastGet2d(src_indices_, d, s_i));\n+      }\n+      THSTensor_(addSlice)(dstBuffer, dstBuffer, srcBuffer, r_values_, r_values_, value, s_values_, 0, r_i, r_i, s_i);\n+      s_i++;\n+    }\n+    r_i++;\n+  }\n+\n+  r_->nnz = r_i;\n+  r_->contiguous = 1;\n+\n+  THLongTensor_free(t_indices_);\n+  THTensor_(free)(t_values_);\n+  THLongTensor_free(src_indices_);\n+  THTensor_(free)(s_values_);\n+  THTensor_(free)(srcBuffer);\n+  THTensor_(free)(dstBuffer);\n+}\n+\n+void THSTensor_(csub)(THSTensor *r_, THSTensor *t, real value, THSTensor *src) {\n+  THSTensor_(cadd)(r_, t, -value, src);", "path": "torch/lib/THS/generic/THSTensorMath.c", "position": 148, "original_position": 142, "commit_id": "d5a99574d20676583112f982ab429a1ea1ae1a5b", "original_commit_id": "27e053e0c70da19a221e7b9829edde4be2aa6a45", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Yeah I haven't thought about that, it should work. They will wrap properly for sure, because for unsigned types it is guaranteed by the C and C++ standard.", "created_at": "2017-03-03T16:23:34Z", "updated_at": "2018-11-23T15:32:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/735#discussion_r104188547", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/735", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104188547"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/735#discussion_r104188547"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/735"}}, "body_html": "<p>Yeah I haven't thought about that, it should work. They will wrap properly for sure, because for unsigned types it is guaranteed by the C and C++ standard.</p>", "body_text": "Yeah I haven't thought about that, it should work. They will wrap properly for sure, because for unsigned types it is guaranteed by the C and C++ standard.", "in_reply_to_id": 104008708}