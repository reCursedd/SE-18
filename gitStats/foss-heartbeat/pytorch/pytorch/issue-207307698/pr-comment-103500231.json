{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103500231", "pull_request_review_id": 24288641, "id": 103500231, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMzUwMDIzMQ==", "diff_hunk": "@@ -224,12 +224,7 @@ PyObject *THPVariable_get_grad(THPVariable *self)\n {\n   auto& var = *self->cdata;\n   if (!var.grad) {\n-#ifdef WITH_CUDA\n-    THCPAutoGPU __guard(var.data->getDevice());\n-#endif\n-    auto grad = var.data->newTensor();\n-    grad->resizeAs(*var.data).zero();\n-    var.grad = std::make_shared<Variable>(std::move(grad), 0, 1);\n+    return Py_None;", "path": "torch/csrc/autograd/python_variable.cpp", "position": null, "original_position": 10, "commit_id": "d5a99574d20676583112f982ab429a1ea1ae1a5b", "original_commit_id": "e28206b9b3e50dafe6b7d7f2d127746c471b6682", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "body": "When gradients can be sparse, there is unfortunately no good default value for zero gradients (the gradient could be dense or sparse, and even for sparse gradients we don't know how many dimensions are actually sparse). I think the most elegant way to handle it is to leave it to `None` by default, though I understand this is a small breaking change. Let me know if you see a better way.", "created_at": "2017-02-28T16:49:46Z", "updated_at": "2018-11-23T15:32:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/735#discussion_r103500231", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/735", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103500231"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/735#discussion_r103500231"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/735"}}, "body_html": "<p>When gradients can be sparse, there is unfortunately no good default value for zero gradients (the gradient could be dense or sparse, and even for sparse gradients we don't know how many dimensions are actually sparse). I think the most elegant way to handle it is to leave it to <code>None</code> by default, though I understand this is a small breaking change. Let me know if you see a better way.</p>", "body_text": "When gradients can be sparse, there is unfortunately no good default value for zero gradients (the gradient could be dense or sparse, and even for sparse gradients we don't know how many dimensions are actually sparse). I think the most elegant way to handle it is to leave it to None by default, though I understand this is a small breaking change. Let me know if you see a better way.", "in_reply_to_id": 103495160}