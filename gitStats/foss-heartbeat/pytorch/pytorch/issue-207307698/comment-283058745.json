{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/283058745", "html_url": "https://github.com/pytorch/pytorch/pull/735#issuecomment-283058745", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/735", "id": 283058745, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzA1ODc0NQ==", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-28T14:48:41Z", "updated_at": "2017-02-28T14:48:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There are a few changes compared to the original PR description:</p>\n<ul>\n<li>there is no explicit <code>sparse_grad</code> flag anymore. Whether a <code>Variable</code>'s gradient is sparse is not an intrinsic property of the variable itself, instead it is determined by the functions that are applied to it. Therefore sparse gradients are propagated if possible, and an error message is shown if incompatible types of gradients are accumulated for the same variable (we could relax this constraint and fallback to dense gradients when accumulating but I'm not sure if that's better from a user perspective)</li>\n<li>I removed some functions that don't make much sense / have unclear semantics: scalar <code>add</code> (that would turn a sparse matrix into a dense one), elementwise <code>div</code> and <code>addcdiv</code> (causes division by zero errors unless the denominator index set is a subset of the numerator index set)</li>\n<li>I added some glue code to bridge THCS and Python to make existing tests pass and lay the groundwork for future sparse cuda tensor support (for the time being I didn't implement any additional sparse cuda ops)</li>\n</ul>", "body_text": "There are a few changes compared to the original PR description:\n\nthere is no explicit sparse_grad flag anymore. Whether a Variable's gradient is sparse is not an intrinsic property of the variable itself, instead it is determined by the functions that are applied to it. Therefore sparse gradients are propagated if possible, and an error message is shown if incompatible types of gradients are accumulated for the same variable (we could relax this constraint and fallback to dense gradients when accumulating but I'm not sure if that's better from a user perspective)\nI removed some functions that don't make much sense / have unclear semantics: scalar add (that would turn a sparse matrix into a dense one), elementwise div and addcdiv (causes division by zero errors unless the denominator index set is a subset of the numerator index set)\nI added some glue code to bridge THCS and Python to make existing tests pass and lay the groundwork for future sparse cuda tensor support (for the time being I didn't implement any additional sparse cuda ops)", "body": "There are a few changes compared to the original PR description:\r\n\r\n* there is no explicit `sparse_grad` flag anymore. Whether a `Variable`'s gradient is sparse is not an intrinsic property of the variable itself, instead it is determined by the functions that are applied to it. Therefore sparse gradients are propagated if possible, and an error message is shown if incompatible types of gradients are accumulated for the same variable (we could relax this constraint and fallback to dense gradients when accumulating but I'm not sure if that's better from a user perspective)\r\n* I removed some functions that don't make much sense / have unclear semantics: scalar `add` (that would turn a sparse matrix into a dense one), elementwise `div` and `addcdiv` (causes division by zero errors unless the denominator index set is a subset of the numerator index set)\r\n* I added some glue code to bridge THCS and Python to make existing tests pass and lay the groundwork for future sparse cuda tensor support (for the time being I didn't implement any additional sparse cuda ops)"}