{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4067", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4067/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4067/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4067/events", "html_url": "https://github.com/pytorch/pytorch/issues/4067", "id": 280027625, "node_id": "MDU6SXNzdWUyODAwMjc2MjU=", "number": 4067, "title": "arguments are located on different GPUs", "user": {"login": "a791702141", "id": 18284220, "node_id": "MDQ6VXNlcjE4Mjg0MjIw", "avatar_url": "https://avatars3.githubusercontent.com/u/18284220?v=4", "gravatar_id": "", "url": "https://api.github.com/users/a791702141", "html_url": "https://github.com/a791702141", "followers_url": "https://api.github.com/users/a791702141/followers", "following_url": "https://api.github.com/users/a791702141/following{/other_user}", "gists_url": "https://api.github.com/users/a791702141/gists{/gist_id}", "starred_url": "https://api.github.com/users/a791702141/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/a791702141/subscriptions", "organizations_url": "https://api.github.com/users/a791702141/orgs", "repos_url": "https://api.github.com/users/a791702141/repos", "events_url": "https://api.github.com/users/a791702141/events{/privacy}", "received_events_url": "https://api.github.com/users/a791702141/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-07T07:24:00Z", "updated_at": "2017-12-08T14:53:12Z", "closed_at": "2017-12-07T17:12:28Z", "author_association": "NONE", "body_html": "<p>train.lua</p>\n<p>if opt.gpu &gt; 0 then<br>\nrequire 'cunn'<br>\ncutorch.setDevice(opt.gpu)<br>\n--cutorch.setDevice(opt.gpu1)<br>\ninput_ctx_vis = input_ctx_vis:cuda(); input_ctx = input_ctx:cuda();  input_center = input_center:cuda()<br>\nnoise = noise:cuda();  label = label:cuda()<br>\nnetG = util.cudnn(netG);     netD = util.cudnn(netD)<br>\nnetD:cuda();           netG:cuda();           criterion:cuda();<br>\nif opt.wtl2~=0 then<br>\ncriterionMSE:cuda(); input_real_center = input_real_center:cuda();<br>\nend<br>\nend<br>\nprint('NetG:',netG)<br>\nprint('NetD:',netD)</p>\n<p>local parametersD, gradParametersD = netD:getParameters()<br>\nlocal parametersG, gradParametersG = netG:getParameters()</p>\n<p>if opt.display then disp = require 'display' end</p>\n<p>noise_vis = noise:clone()<br>\nif opt.noisetype == 'uniform' then<br>\nnoise_vis:uniform(-1, 1)<br>\nelseif opt.noisetype == 'normal' then<br>\nnoise_vis:normal(0, 1)<br>\nend</p>\n<hr>\n<h2>-- Define generator and adversary closures</h2>\n<p>-- create closure to evaluate f(X) and df/dX of discriminator<br>\nlocal fDx = function(x)<br>\nnetD:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)<br>\nnetG:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)</p>\n<p>gradParametersD:zero()</p>\n<p>-- train with real<br>\ndata_tm:reset(); data_tm:resume()<br>\nlocal real_ctx = data:getBatch()<br>\nlocal real_center = real_ctx[{{},{},{1 + opt.fineSize/4, opt.fineSize/2 + opt.fineSize/4},{1 + opt.fineSize/4, opt.fineSize/2 + opt.fineSize/4}}]:clone() -- copy by value<br>\nreal_ctx[{{},{1},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2<em>117.0/255.0 - 1.0<br>\nreal_ctx[{{},{2},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2</em>104.0/255.0 - 1.0<br>\nreal_ctx[{{},{3},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*123.0/255.0 - 1.0<br>\ninput_ctx:copy(real_ctx)<br>\ninput_center:copy(real_center)<br>\ndata_tm:stop()<br>\nif opt.wtl2~=0 then<br>\ninput_real_center:copy(real_center)<br>\nend<br>\nlabel:fill(real_label)</p>\n<p>local output<br>\nif opt.conditionAdv then<br>\noutput = netD:forward({input_ctx,input_center})<br>\nelse<br>\noutput = netD:forward(input_center)<br>\nend<br>\nlocal errD_real = criterion:forward(output, label)<br>\nlocal df_do = criterion:backward(output, label)<br>\nif opt.conditionAdv then<br>\nnetD:backward({input_ctx,input_center}, df_do)<br>\nelse<br>\nnetD:backward(input_center, df_do)<br>\nend</p>\n<p>and util.lua</p>\n<p>function util.cudnn(net)<br>\n-- assume cunn and cudnn are both avaliable<br>\n-- use cudnn convolution layer<br>\n--recursiveCudnn(net)<br>\ncudnn.convert(net, cudnn)</p>\n<pre><code>--local nGPU = cutorch.getDeviceCount()\nlocal nGPU = 2\n-- support data parallel for multi gpus\n\nif nGPU &gt; 1 then\n    local parallel_net = nn.DataParallelTable(1)\n    parallel_net:add(net,{5,6})--:cuda()\n    return parallel_net\n  --  local gpus = torch.range(5, nGPU):totable()\n   -- print(gpus)\n    --local fastest, benchmark = cudnn.fastest, cudnn.benchmark\n    --local parallel_net = nn.DataParallelTable(1, true, true)\n    --parallel_net:add(net, gpus):cuda\n            --:threads(function()\n               -- local cudnn = require 'cudnn' \n               -- cudnn.fastest, cudnn.benchmark = fastest, benchmark\n            --end)\n    --return parallel_net\nelse\n    return net\nend\n</code></pre>\n<p>end</p>\n<p>When I executed I got this</p>\n<p>/usr/local/torch/install/bin/luajit: train.lua:276: arguments are located on different GPUs at /usr/local/torch/extra/cutorch/lib/THC/generic/THCTensorMath.cu:21<br>\nstack traceback:<br>\n[C]: in function 'zero'<br>\ntrain.lua:276: in function 'callback'<br>\n/usr/local/torch/install/share/lua/5.1/nn/Module.lua:352: in function 'apply'<br>\n/usr/local/torch/install/share/lua/5.1/nn/Module.lua:356: in function 'apply'<br>\n/usr/local/torch/install/share/lua/5.1/nn/Module.lua:356: in function 'apply'<br>\n...l/torch/install/share/lua/5.1/cunn/DataParallelTable.lua:464: in function 'apply'<br>\ntrain.lua:276: in function 'opfunc'<br>\n/usr/local/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'adam'<br>\ntrain.lua:413: in main chunk<br>\n[C]: in function 'dofile'<br>\n...ocal/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk<br>\n[C]: at 0x004064f0</p>", "body_text": "train.lua\nif opt.gpu > 0 then\nrequire 'cunn'\ncutorch.setDevice(opt.gpu)\n--cutorch.setDevice(opt.gpu1)\ninput_ctx_vis = input_ctx_vis:cuda(); input_ctx = input_ctx:cuda();  input_center = input_center:cuda()\nnoise = noise:cuda();  label = label:cuda()\nnetG = util.cudnn(netG);     netD = util.cudnn(netD)\nnetD:cuda();           netG:cuda();           criterion:cuda();\nif opt.wtl2~=0 then\ncriterionMSE:cuda(); input_real_center = input_real_center:cuda();\nend\nend\nprint('NetG:',netG)\nprint('NetD:',netD)\nlocal parametersD, gradParametersD = netD:getParameters()\nlocal parametersG, gradParametersG = netG:getParameters()\nif opt.display then disp = require 'display' end\nnoise_vis = noise:clone()\nif opt.noisetype == 'uniform' then\nnoise_vis:uniform(-1, 1)\nelseif opt.noisetype == 'normal' then\nnoise_vis:normal(0, 1)\nend\n\n-- Define generator and adversary closures\n-- create closure to evaluate f(X) and df/dX of discriminator\nlocal fDx = function(x)\nnetD:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)\nnetG:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)\ngradParametersD:zero()\n-- train with real\ndata_tm:reset(); data_tm:resume()\nlocal real_ctx = data:getBatch()\nlocal real_center = real_ctx[{{},{},{1 + opt.fineSize/4, opt.fineSize/2 + opt.fineSize/4},{1 + opt.fineSize/4, opt.fineSize/2 + opt.fineSize/4}}]:clone() -- copy by value\nreal_ctx[{{},{1},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2117.0/255.0 - 1.0\nreal_ctx[{{},{2},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2104.0/255.0 - 1.0\nreal_ctx[{{},{3},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*123.0/255.0 - 1.0\ninput_ctx:copy(real_ctx)\ninput_center:copy(real_center)\ndata_tm:stop()\nif opt.wtl2~=0 then\ninput_real_center:copy(real_center)\nend\nlabel:fill(real_label)\nlocal output\nif opt.conditionAdv then\noutput = netD:forward({input_ctx,input_center})\nelse\noutput = netD:forward(input_center)\nend\nlocal errD_real = criterion:forward(output, label)\nlocal df_do = criterion:backward(output, label)\nif opt.conditionAdv then\nnetD:backward({input_ctx,input_center}, df_do)\nelse\nnetD:backward(input_center, df_do)\nend\nand util.lua\nfunction util.cudnn(net)\n-- assume cunn and cudnn are both avaliable\n-- use cudnn convolution layer\n--recursiveCudnn(net)\ncudnn.convert(net, cudnn)\n--local nGPU = cutorch.getDeviceCount()\nlocal nGPU = 2\n-- support data parallel for multi gpus\n\nif nGPU > 1 then\n    local parallel_net = nn.DataParallelTable(1)\n    parallel_net:add(net,{5,6})--:cuda()\n    return parallel_net\n  --  local gpus = torch.range(5, nGPU):totable()\n   -- print(gpus)\n    --local fastest, benchmark = cudnn.fastest, cudnn.benchmark\n    --local parallel_net = nn.DataParallelTable(1, true, true)\n    --parallel_net:add(net, gpus):cuda\n            --:threads(function()\n               -- local cudnn = require 'cudnn' \n               -- cudnn.fastest, cudnn.benchmark = fastest, benchmark\n            --end)\n    --return parallel_net\nelse\n    return net\nend\n\nend\nWhen I executed I got this\n/usr/local/torch/install/bin/luajit: train.lua:276: arguments are located on different GPUs at /usr/local/torch/extra/cutorch/lib/THC/generic/THCTensorMath.cu:21\nstack traceback:\n[C]: in function 'zero'\ntrain.lua:276: in function 'callback'\n/usr/local/torch/install/share/lua/5.1/nn/Module.lua:352: in function 'apply'\n/usr/local/torch/install/share/lua/5.1/nn/Module.lua:356: in function 'apply'\n/usr/local/torch/install/share/lua/5.1/nn/Module.lua:356: in function 'apply'\n...l/torch/install/share/lua/5.1/cunn/DataParallelTable.lua:464: in function 'apply'\ntrain.lua:276: in function 'opfunc'\n/usr/local/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'adam'\ntrain.lua:413: in main chunk\n[C]: in function 'dofile'\n...ocal/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk\n[C]: at 0x004064f0", "body": "train.lua\r\n\r\nif opt.gpu > 0 then\r\n   require 'cunn'\r\n   cutorch.setDevice(opt.gpu)\r\n   --cutorch.setDevice(opt.gpu1)\r\n   input_ctx_vis = input_ctx_vis:cuda(); input_ctx = input_ctx:cuda();  input_center = input_center:cuda()\r\n   noise = noise:cuda();  label = label:cuda()\r\n   netG = util.cudnn(netG);     netD = util.cudnn(netD)\r\n   netD:cuda();           netG:cuda();           criterion:cuda();      \r\n   if opt.wtl2~=0 then\r\n      criterionMSE:cuda(); input_real_center = input_real_center:cuda();\r\n   end\r\nend\r\nprint('NetG:',netG)\r\nprint('NetD:',netD)\r\n\r\nlocal parametersD, gradParametersD = netD:getParameters()\r\nlocal parametersG, gradParametersG = netG:getParameters()\r\n\r\nif opt.display then disp = require 'display' end\r\n\r\nnoise_vis = noise:clone()\r\nif opt.noisetype == 'uniform' then\r\n    noise_vis:uniform(-1, 1)\r\nelseif opt.noisetype == 'normal' then\r\n    noise_vis:normal(0, 1)\r\nend\r\n\r\n---------------------------------------------------------------------------\r\n-- Define generator and adversary closures\r\n---------------------------------------------------------------------------\r\n-- create closure to evaluate f(X) and df/dX of discriminator\r\nlocal fDx = function(x)\r\n   netD:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)\r\n   netG:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)\r\n\r\n   gradParametersD:zero()\r\n\r\n   -- train with real\r\n   data_tm:reset(); data_tm:resume()\r\n   local real_ctx = data:getBatch()\r\n   local real_center = real_ctx[{{},{},{1 + opt.fineSize/4, opt.fineSize/2 + opt.fineSize/4},{1 + opt.fineSize/4, opt.fineSize/2 + opt.fineSize/4}}]:clone() -- copy by value\r\n   real_ctx[{{},{1},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*117.0/255.0 - 1.0\r\n   real_ctx[{{},{2},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*104.0/255.0 - 1.0\r\n   real_ctx[{{},{3},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*123.0/255.0 - 1.0\r\n   input_ctx:copy(real_ctx)\r\n   input_center:copy(real_center)\r\n   data_tm:stop()\r\n   if opt.wtl2~=0 then\r\n      input_real_center:copy(real_center)\r\n   end\r\n   label:fill(real_label)\r\n\r\n   local output\r\n   if opt.conditionAdv then\r\n      output = netD:forward({input_ctx,input_center})\r\n   else\r\n      output = netD:forward(input_center)\r\n   end\r\n   local errD_real = criterion:forward(output, label)\r\n   local df_do = criterion:backward(output, label)\r\n   if opt.conditionAdv then\r\n      netD:backward({input_ctx,input_center}, df_do)\r\n   else\r\n      netD:backward(input_center, df_do)\r\n   end\r\n\r\nand util.lua\r\n\r\nfunction util.cudnn(net)\r\n    -- assume cunn and cudnn are both avaliable\r\n    -- use cudnn convolution layer\r\n    --recursiveCudnn(net)\r\n    cudnn.convert(net, cudnn)\r\n\r\n    --local nGPU = cutorch.getDeviceCount()\r\n    local nGPU = 2\r\n    -- support data parallel for multi gpus\r\n\r\n    if nGPU > 1 then\r\n        local parallel_net = nn.DataParallelTable(1)\r\n        parallel_net:add(net,{5,6})--:cuda()\r\n        return parallel_net\r\n      --  local gpus = torch.range(5, nGPU):totable()\r\n       -- print(gpus)\r\n        --local fastest, benchmark = cudnn.fastest, cudnn.benchmark\r\n        --local parallel_net = nn.DataParallelTable(1, true, true)\r\n        --parallel_net:add(net, gpus):cuda\r\n                --:threads(function()\r\n                   -- local cudnn = require 'cudnn' \r\n                   -- cudnn.fastest, cudnn.benchmark = fastest, benchmark\r\n                --end)\r\n        --return parallel_net\r\n    else\r\n        return net\r\n    end\r\nend\r\n\r\nWhen I executed I got this \r\n\r\n/usr/local/torch/install/bin/luajit: train.lua:276: arguments are located on different GPUs at /usr/local/torch/extra/cutorch/lib/THC/generic/THCTensorMath.cu:21\r\nstack traceback:\r\n\t[C]: in function 'zero'\r\n\ttrain.lua:276: in function 'callback'\r\n\t/usr/local/torch/install/share/lua/5.1/nn/Module.lua:352: in function 'apply'\r\n\t/usr/local/torch/install/share/lua/5.1/nn/Module.lua:356: in function 'apply'\r\n\t/usr/local/torch/install/share/lua/5.1/nn/Module.lua:356: in function 'apply'\r\n\t...l/torch/install/share/lua/5.1/cunn/DataParallelTable.lua:464: in function 'apply'\r\n\ttrain.lua:276: in function 'opfunc'\r\n\t/usr/local/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'adam'\r\n\ttrain.lua:413: in main chunk\r\n\t[C]: in function 'dofile'\r\n\t...ocal/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk\r\n\t[C]: at 0x004064f0\r\n"}