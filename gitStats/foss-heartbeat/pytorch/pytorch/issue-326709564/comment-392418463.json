{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392418463", "html_url": "https://github.com/pytorch/pytorch/pull/7873#issuecomment-392418463", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7873", "id": 392418463, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjQxODQ2Mw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-28T04:50:49Z", "updated_at": "2018-05-28T04:50:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Nice work!<br>\nPlease unify dimensions error checking for cuda and cpu versions (right now it's 50 lines of copy-pasted code).<br>\nFor cuda implementation, please run collapseDims pass on input (see in this file <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/detail/TensorInfo.cuh\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/detail/TensorInfo.cuh</a>), so that e.g. last-dimension flip of a multi-D contiguous tensor is the same as dimension flip for a 2d tensor.<br>\nAlso, instead of implementing specialized kernel for this, for the flipped tensor you can create TensorInfo object with the negative strides for flipped dimensions (negative strides are generally not supported, and TensorInfo IndexType is usually unsigned, but you can instantiate it with signed) and run kernelPointwiseApply2 from CUDAApplyUtils.cuh with CopyOp. That way, you don't have to reimplement indexToOffset and back functions (TensorInfo already has them), and don't have to materialize indices tensor (that's really bad for performance).</p>", "body_text": "Nice work!\nPlease unify dimensions error checking for cuda and cpu versions (right now it's 50 lines of copy-pasted code).\nFor cuda implementation, please run collapseDims pass on input (see in this file https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/detail/TensorInfo.cuh), so that e.g. last-dimension flip of a multi-D contiguous tensor is the same as dimension flip for a 2d tensor.\nAlso, instead of implementing specialized kernel for this, for the flipped tensor you can create TensorInfo object with the negative strides for flipped dimensions (negative strides are generally not supported, and TensorInfo IndexType is usually unsigned, but you can instantiate it with signed) and run kernelPointwiseApply2 from CUDAApplyUtils.cuh with CopyOp. That way, you don't have to reimplement indexToOffset and back functions (TensorInfo already has them), and don't have to materialize indices tensor (that's really bad for performance).", "body": "Nice work!\r\nPlease unify dimensions error checking for cuda and cpu versions (right now it's 50 lines of copy-pasted code). \r\nFor cuda implementation, please run collapseDims pass on input (see in this file https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/detail/TensorInfo.cuh), so that e.g. last-dimension flip of a multi-D contiguous tensor is the same as dimension flip for a 2d tensor.\r\nAlso, instead of implementing specialized kernel for this, for the flipped tensor you can create TensorInfo object with the negative strides for flipped dimensions (negative strides are generally not supported, and TensorInfo IndexType is usually unsigned, but you can instantiate it with signed) and run kernelPointwiseApply2 from CUDAApplyUtils.cuh with CopyOp. That way, you don't have to reimplement indexToOffset and back functions (TensorInfo already has them), and don't have to materialize indices tensor (that's really bad for performance). "}