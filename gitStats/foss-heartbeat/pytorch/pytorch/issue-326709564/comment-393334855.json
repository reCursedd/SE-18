{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393334855", "html_url": "https://github.com/pytorch/pytorch/pull/7873#issuecomment-393334855", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7873", "id": 393334855, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzMzNDg1NQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T22:04:28Z", "updated_at": "2018-05-30T22:04:28Z", "author_association": "MEMBER", "body_html": "<p>Nice, thanks for the benchmarks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38509346\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/weiyangfb\">@weiyangfb</a> ! Can you try also adding a <code>torch.cuda.synchronize()</code> when benchmarking then CUDA kernels? Also, I'd be curious to know which fraction of the time was spent on the indexing, and which one was spent on the <code>torch.arange</code>. Would it be possible to check that as well?</p>\n<p>Thanks!</p>", "body_text": "Nice, thanks for the benchmarks @weiyangfb ! Can you try also adding a torch.cuda.synchronize() when benchmarking then CUDA kernels? Also, I'd be curious to know which fraction of the time was spent on the indexing, and which one was spent on the torch.arange. Would it be possible to check that as well?\nThanks!", "body": "Nice, thanks for the benchmarks @weiyangfb ! Can you try also adding a `torch.cuda.synchronize()` when benchmarking then CUDA kernels? Also, I'd be curious to know which fraction of the time was spent on the indexing, and which one was spent on the `torch.arange`. Would it be possible to check that as well?\r\n\r\nThanks!"}