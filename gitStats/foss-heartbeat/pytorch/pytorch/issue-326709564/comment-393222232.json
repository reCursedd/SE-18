{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393222232", "html_url": "https://github.com/pytorch/pytorch/pull/7873#issuecomment-393222232", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7873", "id": 393222232, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzIyMjIzMg==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T16:14:24Z", "updated_at": "2018-05-30T16:14:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p><code>collapseDims</code> is important because it will reduce the amount of indexing math that you have to do. Suppose you have a contiguous 4d tensor, where you want to flip the last dimension. You can collapse the first 3 dims to view this tensor as 2d,  then your indexing math will be simpler (you have to loop over just 2 dimensions). If you are flipping multiple dimensions, applying <code>collapseDims</code> is much trickier (may be impossible, if your flip dimensions are not contiguous, say you want to flip 0 and 2), but for a single flipped dimension collapseDims should help.<br>\nNow, to negative strides. Suppose you want to flip a 1d tensor. You can create TensorInfo object with data pointer pointing to the end of your output tensor, and set a stride of the 0-th dimension to -1, copy your original tensor to the tensor described by this TensorInfo object (using standard pointwiseApply kernel that's already in ATen), and then view the result a contiguous tensor. Similarly for flips in other dimensions/multiple flipped dimensions - you'd have to move base pointer, and set negative strides for the dimensions you want to flip, but that will be CPU code, not GPU. Obviously, since you want negative strides, you can not use unsigned IndexType for those values.<br>\nThat said, it is quite possible that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a>'s implementation already achieves good fraction of peak bandwidth, you should benchmark it first (not the absolute time, but what bandwidth you achieve compared to maximum on your card), in which case you can just use it.</p>", "body_text": "collapseDims is important because it will reduce the amount of indexing math that you have to do. Suppose you have a contiguous 4d tensor, where you want to flip the last dimension. You can collapse the first 3 dims to view this tensor as 2d,  then your indexing math will be simpler (you have to loop over just 2 dimensions). If you are flipping multiple dimensions, applying collapseDims is much trickier (may be impossible, if your flip dimensions are not contiguous, say you want to flip 0 and 2), but for a single flipped dimension collapseDims should help.\nNow, to negative strides. Suppose you want to flip a 1d tensor. You can create TensorInfo object with data pointer pointing to the end of your output tensor, and set a stride of the 0-th dimension to -1, copy your original tensor to the tensor described by this TensorInfo object (using standard pointwiseApply kernel that's already in ATen), and then view the result a contiguous tensor. Similarly for flips in other dimensions/multiple flipped dimensions - you'd have to move base pointer, and set negative strides for the dimensions you want to flip, but that will be CPU code, not GPU. Obviously, since you want negative strides, you can not use unsigned IndexType for those values.\nThat said, it is quite possible that @fmassa's implementation already achieves good fraction of peak bandwidth, you should benchmark it first (not the absolute time, but what bandwidth you achieve compared to maximum on your card), in which case you can just use it.", "body": "`collapseDims` is important because it will reduce the amount of indexing math that you have to do. Suppose you have a contiguous 4d tensor, where you want to flip the last dimension. You can collapse the first 3 dims to view this tensor as 2d,  then your indexing math will be simpler (you have to loop over just 2 dimensions). If you are flipping multiple dimensions, applying `collapseDims` is much trickier (may be impossible, if your flip dimensions are not contiguous, say you want to flip 0 and 2), but for a single flipped dimension collapseDims should help. \r\nNow, to negative strides. Suppose you want to flip a 1d tensor. You can create TensorInfo object with data pointer pointing to the end of your output tensor, and set a stride of the 0-th dimension to -1, copy your original tensor to the tensor described by this TensorInfo object (using standard pointwiseApply kernel that's already in ATen), and then view the result a contiguous tensor. Similarly for flips in other dimensions/multiple flipped dimensions - you'd have to move base pointer, and set negative strides for the dimensions you want to flip, but that will be CPU code, not GPU. Obviously, since you want negative strides, you can not use unsigned IndexType for those values. \r\nThat said, it is quite possible that @fmassa's implementation already achieves good fraction of peak bandwidth, you should benchmark it first (not the absolute time, but what bandwidth you achieve compared to maximum on your card), in which case you can just use it. \r\n"}