{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396348077", "html_url": "https://github.com/pytorch/pytorch/pull/7873#issuecomment-396348077", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7873", "id": 396348077, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjM0ODA3Nw==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-11T18:53:25Z", "updated_at": "2018-06-11T18:53:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> Thanks a lot! Now it makes all sense! I am using <code>TensorInfo</code> and <code>collapseDims</code> to speed up the case where flip dim is the 1st or last dim. Here are some performance results:</p>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.flip(0)\n----------------------------------\n10000 loops, best of 3: 178 \u00b5s per loop\n</code></pre>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.flip(2)\n----------------------------------\n10000 loops, best of 3: 181 \u00b5s per loop\n</code></pre>\n<p>benchmark:</p>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.mul(2)\n----------------------------------\n10000 loops, best of 3: 86.3 \u00b5s per loop\n</code></pre>\n<p>And if I understand it correctly, <code>collapseDims</code> might not be able to squeeze nD to 2D tensor if flip dim is not the 1st or last dim, I am using the previous impl for these cases.</p>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.flip(1)\n----------------------------------\n1000 loops, best of 3: 364 \u00b5s per loop\n</code></pre>", "body_text": "@ngimel Thanks a lot! Now it makes all sense! I am using TensorInfo and collapseDims to speed up the case where flip dim is the 1st or last dim. Here are some performance results:\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.flip(0)\n----------------------------------\n10000 loops, best of 3: 178 \u00b5s per loop\n\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.flip(2)\n----------------------------------\n10000 loops, best of 3: 181 \u00b5s per loop\n\nbenchmark:\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.mul(2)\n----------------------------------\n10000 loops, best of 3: 86.3 \u00b5s per loop\n\nAnd if I understand it correctly, collapseDims might not be able to squeeze nD to 2D tensor if flip dim is not the 1st or last dim, I am using the previous impl for these cases.\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\n%timeit data_cuda.flip(1)\n----------------------------------\n1000 loops, best of 3: 364 \u00b5s per loop", "body": "@ngimel Thanks a lot! Now it makes all sense! I am using `TensorInfo` and `collapseDims` to speed up the case where flip dim is the 1st or last dim. Here are some performance results:\r\n\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\r\n%timeit data_cuda.flip(0)\r\n----------------------------------\r\n10000 loops, best of 3: 178 \u00b5s per loop\r\n```\r\n\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\r\n%timeit data_cuda.flip(2)\r\n----------------------------------\r\n10000 loops, best of 3: 181 \u00b5s per loop\r\n```\r\n\r\nbenchmark:\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\r\n%timeit data_cuda.mul(2)\r\n----------------------------------\r\n10000 loops, best of 3: 86.3 \u00b5s per loop\r\n```\r\n\r\nAnd if I understand it correctly, `collapseDims` might not be able to squeeze nD to 2D tensor if flip dim is not the 1st or last dim, I am using the previous impl for these cases.\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(100,100,100)\r\n%timeit data_cuda.flip(1)\r\n----------------------------------\r\n1000 loops, best of 3: 364 \u00b5s per loop\r\n```"}