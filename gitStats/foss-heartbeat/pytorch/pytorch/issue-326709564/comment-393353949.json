{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393353949", "html_url": "https://github.com/pytorch/pytorch/pull/7873#issuecomment-393353949", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7873", "id": 393353949, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzM1Mzk0OQ==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T23:42:05Z", "updated_at": "2018-05-30T23:42:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> Thanks a lot for the detailed instructions! Even though <code>collapseDims</code> might not help in the case of nD flip, I'd love to have it to speed up the case of flip in one dimension. So if I understand correctly, I probably will need to apply <code>collapseDims</code> on nD input tensor with dim to be flipped excluded - this gives a 2D tensor. Then I will need to use IndexToOffset along with <code>negative stride</code> to move elements from src to dst tensor. One quick question, <code>set a stride of the 0-th dimension to -1</code> works for 1D tensor, and so what formula works for the 2D tensor?</p>\n<p>Currently I removed the materialized indices in cuda kernel, and had it tested. I am still not quite sure how to test for GPU bandwidth, here I looked at some numbers from <code>nvidia-smi --query-gpu=gpu_name,gpu_bus_id,utilization.gpu,utilization.memory,memory.used --format=csv -l</code></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> implementation:</p>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\n%timeit flip_meshgrid(data_cuda, (0,1))\n-------------------------------------------------------------------\nTesla K40m, 00000000:28:00.0, 83 %, 29 %, 478 MiB\n\n1000 loops, best of 3: 1.72 ms per loop\n</code></pre>\n<p>My implementation with materialized indices:</p>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\n%timeit data_cuda.flip(0,1)\n-------------------------------------------------------------------\nTesla K40m, 00000000:28:00.0, 90 %, 73 %, 478 MiB\n\n1000 loops, best of 3: 637 \u00b5s per loop\n</code></pre>\n<p>My current implementation without materialized indices:</p>\n<pre><code>data_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\n%timeit data_cuda.flip(0,1)\n-------------------------------------------------------------------\nTesla K40m, 00000000:28:00.0, 85 %, 36 %, 463 MiB\n\n1000 loops, best of 3: 357 \u00b5s per loop\n</code></pre>", "body_text": "@ngimel Thanks a lot for the detailed instructions! Even though collapseDims might not help in the case of nD flip, I'd love to have it to speed up the case of flip in one dimension. So if I understand correctly, I probably will need to apply collapseDims on nD input tensor with dim to be flipped excluded - this gives a 2D tensor. Then I will need to use IndexToOffset along with negative stride to move elements from src to dst tensor. One quick question, set a stride of the 0-th dimension to -1 works for 1D tensor, and so what formula works for the 2D tensor?\nCurrently I removed the materialized indices in cuda kernel, and had it tested. I am still not quite sure how to test for GPU bandwidth, here I looked at some numbers from nvidia-smi --query-gpu=gpu_name,gpu_bus_id,utilization.gpu,utilization.memory,memory.used --format=csv -l\n@fmassa implementation:\ndata_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\n%timeit flip_meshgrid(data_cuda, (0,1))\n-------------------------------------------------------------------\nTesla K40m, 00000000:28:00.0, 83 %, 29 %, 478 MiB\n\n1000 loops, best of 3: 1.72 ms per loop\n\nMy implementation with materialized indices:\ndata_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\n%timeit data_cuda.flip(0,1)\n-------------------------------------------------------------------\nTesla K40m, 00000000:28:00.0, 90 %, 73 %, 478 MiB\n\n1000 loops, best of 3: 637 \u00b5s per loop\n\nMy current implementation without materialized indices:\ndata_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\n%timeit data_cuda.flip(0,1)\n-------------------------------------------------------------------\nTesla K40m, 00000000:28:00.0, 85 %, 36 %, 463 MiB\n\n1000 loops, best of 3: 357 \u00b5s per loop", "body": "@ngimel Thanks a lot for the detailed instructions! Even though `collapseDims` might not help in the case of nD flip, I'd love to have it to speed up the case of flip in one dimension. So if I understand correctly, I probably will need to apply `collapseDims` on nD input tensor with dim to be flipped excluded - this gives a 2D tensor. Then I will need to use IndexToOffset along with `negative stride` to move elements from src to dst tensor. One quick question, `set a stride of the 0-th dimension to -1` works for 1D tensor, and so what formula works for the 2D tensor? \r\n\r\nCurrently I removed the materialized indices in cuda kernel, and had it tested. I am still not quite sure how to test for GPU bandwidth, here I looked at some numbers from `nvidia-smi --query-gpu=gpu_name,gpu_bus_id,utilization.gpu,utilization.memory,memory.used --format=csv -l`\r\n\r\n@fmassa implementation:\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\r\n%timeit flip_meshgrid(data_cuda, (0,1))\r\n-------------------------------------------------------------------\r\nTesla K40m, 00000000:28:00.0, 83 %, 29 %, 478 MiB\r\n\r\n1000 loops, best of 3: 1.72 ms per loop\r\n```\r\n\r\nMy implementation with materialized indices:\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\r\n%timeit data_cuda.flip(0,1)\r\n-------------------------------------------------------------------\r\nTesla K40m, 00000000:28:00.0, 90 %, 73 %, 478 MiB\r\n\r\n1000 loops, best of 3: 637 \u00b5s per loop\r\n```\r\n\r\nMy current implementation without materialized indices:\r\n```\r\ndata_cuda = torch.arange(1000000, device=cuda).view(1000,1000)\r\n%timeit data_cuda.flip(0,1)\r\n-------------------------------------------------------------------\r\nTesla K40m, 00000000:28:00.0, 85 %, 36 %, 463 MiB\r\n\r\n1000 loops, best of 3: 357 \u00b5s per loop\r\n```"}