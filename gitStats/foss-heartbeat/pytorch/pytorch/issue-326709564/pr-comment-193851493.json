{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193851493", "pull_request_review_id": 126909845, "id": 193851493, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Mzg1MTQ5Mw==", "diff_hunk": "@@ -0,0 +1,25 @@\n+#include \"TensorTransformations.h\"\n+\n+#include \"ATen/NativeFunctions.h\"\n+\n+namespace at {\n+namespace native {\n+\n+Tensor flip_cpu(const Tensor& self, IntList dims) {\n+  const int64_t total_dims = self.dim(), flip_dims_size = dims.size();\n+  check_errors(total_dims, flip_dims_size, dims);\n+\n+  auto indices = std::vector<at::Tensor>(flip_dims_size);\n+  for (int64_t i = 0; i < flip_dims_size; i++) {\n+    indices[i] = at::arange(self.type().toScalarType(at::ScalarType::Long), self.size(i) - 1, -1, -1);\n+  }\n+  // creates a meshgrid\n+  for (int64_t i = 0; i < flip_dims_size; i++) {\n+    auto temp = std::vector<int64_t>(flip_dims_size, 1);\n+    temp[i] = indices[i].size(0);\n+    indices[i] = indices[i].view(IntList(temp));\n+  }\n+  return self.index(TensorList(indices));", "path": "aten/src/ATen/native/TensorTransformations.cpp", "position": null, "original_position": 22, "commit_id": "0709c30284942f0169b9b17ddd5c4e309fd8e1c1", "original_commit_id": "2173b886878ad9f8b1df34e6755791b6fa4276eb", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "body": "I am still missing something. In your code, how does `final_indices = [slice(i) for i in tensor.shape]` translate to ATen code? \r\n\r\n```python\r\ndef multi_meshgrid(*args):\r\n    \"\"\"\r\n    Creates a meshgrid from possibly many\r\n    elements (instead of only 2).\r\n    Returns a nd tensor with as many dimensions\r\n    as there are arguments\r\n    \"\"\"\r\n    args = list(args)\r\n    template = [1 for _ in args]\r\n    for i in range(len(args)):\r\n        n = args[i].shape[0]\r\n        template_copy = template.copy()\r\n        template_copy[i] = n\r\n        args[i] = args[i].view(*template_copy)\r\n        # there will be some broadcast magic going on\r\n    return tuple(args)\r\n\r\ndef flip(tensor, dims):\r\n    if not isinstance(dims, (tuple, list)):\r\n        dims = [dims]\r\n    indices = [torch.arange(tensor.shape[dim] - 1, -1, -1,\r\n        dtype=torch.int64) for dim in dims]\r\n    multi_indices = multi_meshgrid(*indices)\r\n    final_indices = [slice(i) for i in tensor.shape]\r\n    for i, dim in enumerate(dims):\r\n        final_indices[dim] = multi_indices[i]\r\n    flipped = tensor[final_indices]\r\n    # need to permute the final dimensions\r\n    # if dims is not consecutive, but I'm lazy\r\n    # now :-)\r\n    return flipped\r\n```", "created_at": "2018-06-07T18:44:33Z", "updated_at": "2018-11-23T15:45:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/7873#discussion_r193851493", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7873", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193851493"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7873#discussion_r193851493"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7873"}}, "body_html": "<p>I am still missing something. In your code, how does <code>final_indices = [slice(i) for i in tensor.shape]</code> translate to ATen code?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">multi_meshgrid</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Creates a meshgrid from possibly many</span>\n<span class=\"pl-s\">    elements (instead of only 2).</span>\n<span class=\"pl-s\">    Returns a nd tensor with as many dimensions</span>\n<span class=\"pl-s\">    as there are arguments</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    args <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(args)\n    template <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> args]\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(args)):\n        n <span class=\"pl-k\">=</span> args[i].shape[<span class=\"pl-c1\">0</span>]\n        template_copy <span class=\"pl-k\">=</span> template.copy()\n        template_copy[i] <span class=\"pl-k\">=</span> n\n        args[i] <span class=\"pl-k\">=</span> args[i].view(<span class=\"pl-k\">*</span>template_copy)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> there will be some broadcast magic going on</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">tuple</span>(args)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">flip</span>(<span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">dims</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(dims, (<span class=\"pl-c1\">tuple</span>, <span class=\"pl-c1\">list</span>)):\n        dims <span class=\"pl-k\">=</span> [dims]\n    indices <span class=\"pl-k\">=</span> [torch.arange(tensor.shape[dim] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,\n        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.int64) <span class=\"pl-k\">for</span> dim <span class=\"pl-k\">in</span> dims]\n    multi_indices <span class=\"pl-k\">=</span> multi_meshgrid(<span class=\"pl-k\">*</span>indices)\n    final_indices <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">slice</span>(i) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> tensor.shape]\n    <span class=\"pl-k\">for</span> i, dim <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(dims):\n        final_indices[dim] <span class=\"pl-k\">=</span> multi_indices[i]\n    flipped <span class=\"pl-k\">=</span> tensor[final_indices]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> need to permute the final dimensions</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> if dims is not consecutive, but I'm lazy</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> now :-)</span>\n    <span class=\"pl-k\">return</span> flipped</pre></div>", "body_text": "I am still missing something. In your code, how does final_indices = [slice(i) for i in tensor.shape] translate to ATen code?\ndef multi_meshgrid(*args):\n    \"\"\"\n    Creates a meshgrid from possibly many\n    elements (instead of only 2).\n    Returns a nd tensor with as many dimensions\n    as there are arguments\n    \"\"\"\n    args = list(args)\n    template = [1 for _ in args]\n    for i in range(len(args)):\n        n = args[i].shape[0]\n        template_copy = template.copy()\n        template_copy[i] = n\n        args[i] = args[i].view(*template_copy)\n        # there will be some broadcast magic going on\n    return tuple(args)\n\ndef flip(tensor, dims):\n    if not isinstance(dims, (tuple, list)):\n        dims = [dims]\n    indices = [torch.arange(tensor.shape[dim] - 1, -1, -1,\n        dtype=torch.int64) for dim in dims]\n    multi_indices = multi_meshgrid(*indices)\n    final_indices = [slice(i) for i in tensor.shape]\n    for i, dim in enumerate(dims):\n        final_indices[dim] = multi_indices[i]\n    flipped = tensor[final_indices]\n    # need to permute the final dimensions\n    # if dims is not consecutive, but I'm lazy\n    # now :-)\n    return flipped", "in_reply_to_id": 192048348}