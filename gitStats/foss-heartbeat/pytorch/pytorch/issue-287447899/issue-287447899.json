{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4578", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4578/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4578/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4578/events", "html_url": "https://github.com/pytorch/pytorch/issues/4578", "id": 287447899, "node_id": "MDU6SXNzdWUyODc0NDc4OTk=", "number": 4578, "title": "[feature request] Weight norm option for RNN cells", "user": {"login": "williamFalcon", "id": 3640001, "node_id": "MDQ6VXNlcjM2NDAwMDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3640001?v=4", "gravatar_id": "", "url": "https://api.github.com/users/williamFalcon", "html_url": "https://github.com/williamFalcon", "followers_url": "https://api.github.com/users/williamFalcon/followers", "following_url": "https://api.github.com/users/williamFalcon/following{/other_user}", "gists_url": "https://api.github.com/users/williamFalcon/gists{/gist_id}", "starred_url": "https://api.github.com/users/williamFalcon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/williamFalcon/subscriptions", "organizations_url": "https://api.github.com/users/williamFalcon/orgs", "repos_url": "https://api.github.com/users/williamFalcon/repos", "events_url": "https://api.github.com/users/williamFalcon/events{/privacy}", "received_events_url": "https://api.github.com/users/williamFalcon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-10T14:15:22Z", "updated_at": "2018-07-18T00:10:16Z", "closed_at": "2018-01-10T18:20:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So, looks like <a href=\"https://github.com/pytorch/pytorch/issues/1601\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1601/hovercard\">#1601</a> added weight norm to pytorch. However, what do you guys think about weightnorm within each RNN cell type that can be enabled through an option?</p>\n<p>Such as:</p>\n<div class=\"highlight highlight-source-python\"><pre> rnn <span class=\"pl-k\">=</span> nn.RNN(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">weight_norm</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>The implementation would add weight norm to each of the weights within the cell hidden output (which you can do now if you run your own loop and add weightnorm at the output of each step, but not possible if you feed in the full sequence).    Paper reference \"<a href=\"https://arxiv.org/pdf/1603.09025.pdf\" rel=\"nofollow\">RECURRENT BATCH NORMALIZATION</a>\"</p>\n<p>Or is there a way to do this already?    (happy to add if you guys think this is useful).</p>", "body_text": "So, looks like #1601 added weight norm to pytorch. However, what do you guys think about weightnorm within each RNN cell type that can be enabled through an option?\nSuch as:\n rnn = nn.RNN(10, 20, 2, weight_norm=True)\nThe implementation would add weight norm to each of the weights within the cell hidden output (which you can do now if you run your own loop and add weightnorm at the output of each step, but not possible if you feed in the full sequence).    Paper reference \"RECURRENT BATCH NORMALIZATION\"\nOr is there a way to do this already?    (happy to add if you guys think this is useful).", "body": "So, looks like [#1601](https://github.com/pytorch/pytorch/issues/1601) added weight norm to pytorch. However, what do you guys think about weightnorm within each RNN cell type that can be enabled through an option?   \r\n\r\nSuch as:    \r\n```python    \r\n rnn = nn.RNN(10, 20, 2, weight_norm=True)\r\n```   \r\n\r\nThe implementation would add weight norm to each of the weights within the cell hidden output (which you can do now if you run your own loop and add weightnorm at the output of each step, but not possible if you feed in the full sequence).    Paper reference \"[RECURRENT BATCH NORMALIZATION](https://arxiv.org/pdf/1603.09025.pdf)\"   \r\n\r\nOr is there a way to do this already?    (happy to add if you guys think this is useful).   "}