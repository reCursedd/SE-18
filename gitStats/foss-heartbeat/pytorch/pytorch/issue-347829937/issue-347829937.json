{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10255", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10255/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10255/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10255/events", "html_url": "https://github.com/pytorch/pytorch/issues/10255", "id": 347829937, "node_id": "MDU6SXNzdWUzNDc4Mjk5Mzc=", "number": 10255, "title": "CUDA error: out of memory - huge embedding layer", "user": {"login": "richielo", "id": 13329406, "node_id": "MDQ6VXNlcjEzMzI5NDA2", "avatar_url": "https://avatars3.githubusercontent.com/u/13329406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/richielo", "html_url": "https://github.com/richielo", "followers_url": "https://api.github.com/users/richielo/followers", "following_url": "https://api.github.com/users/richielo/following{/other_user}", "gists_url": "https://api.github.com/users/richielo/gists{/gist_id}", "starred_url": "https://api.github.com/users/richielo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/richielo/subscriptions", "organizations_url": "https://api.github.com/users/richielo/orgs", "repos_url": "https://api.github.com/users/richielo/repos", "events_url": "https://api.github.com/users/richielo/events{/privacy}", "received_events_url": "https://api.github.com/users/richielo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-06T08:39:07Z", "updated_at": "2018-08-06T16:18:54Z", "closed_at": "2018-08-06T16:18:53Z", "author_association": "NONE", "body_html": "<p>I am working on a CNN autoencoder for text with an architecture and loss function like this:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13329406/43705909-763d9628-9996-11e8-8a57-b8165369fe95.png\"><img src=\"https://user-images.githubusercontent.com/13329406/43705909-763d9628-9996-11e8-8a57-b8165369fe95.png\" alt=\"screen shot 2018-08-06 at 4 32 25 pm\" style=\"max-width:100%;\"></a></p>\n<pre><code>def computeCrossEntropy(log_prob, target):\n    loss = [F.nll_loss(sentence_emb_matrix, word_ids, size_average=False) for sentence_emb_matrix, word_ids in zip(log_prob, target)]\n    average_loss = sum([torch.sum(l) for l in loss]) / log_prob.size()[0]\n    return average_loss\n</code></pre>\n<p>I understand the embedding is huge and I can only have a batch size of 8. Otherwise, I would have memory issue (when loss.backward() is called). Apparently, the line below in the loss function occupies 60% of computation time:</p>\n<pre><code>[torch.sum(l) for l in loss]\n</code></pre>\n<p>are there any ways I can deal with this memory issue so I can increase my batch size? Like having the embedding layer processed somewhere else, assuming I have multiple GPUs? Sorry for my ignorance as I am pretty new to this. I would like to know the best practice in handling such situation to improve training time. Thanks a lot for the guidance in advance</p>", "body_text": "I am working on a CNN autoencoder for text with an architecture and loss function like this:\n\ndef computeCrossEntropy(log_prob, target):\n    loss = [F.nll_loss(sentence_emb_matrix, word_ids, size_average=False) for sentence_emb_matrix, word_ids in zip(log_prob, target)]\n    average_loss = sum([torch.sum(l) for l in loss]) / log_prob.size()[0]\n    return average_loss\n\nI understand the embedding is huge and I can only have a batch size of 8. Otherwise, I would have memory issue (when loss.backward() is called). Apparently, the line below in the loss function occupies 60% of computation time:\n[torch.sum(l) for l in loss]\n\nare there any ways I can deal with this memory issue so I can increase my batch size? Like having the embedding layer processed somewhere else, assuming I have multiple GPUs? Sorry for my ignorance as I am pretty new to this. I would like to know the best practice in handling such situation to improve training time. Thanks a lot for the guidance in advance", "body": "I am working on a CNN autoencoder for text with an architecture and loss function like this: \r\n![screen shot 2018-08-06 at 4 32 25 pm](https://user-images.githubusercontent.com/13329406/43705909-763d9628-9996-11e8-8a57-b8165369fe95.png)\r\n\r\n    def computeCrossEntropy(log_prob, target):\r\n        loss = [F.nll_loss(sentence_emb_matrix, word_ids, size_average=False) for sentence_emb_matrix, word_ids in zip(log_prob, target)]\r\n        average_loss = sum([torch.sum(l) for l in loss]) / log_prob.size()[0]\r\n        return average_loss\r\n\r\nI understand the embedding is huge and I can only have a batch size of 8. Otherwise, I would have memory issue (when loss.backward() is called). Apparently, the line below in the loss function occupies 60% of computation time:\r\n\r\n    [torch.sum(l) for l in loss]\r\n\r\nare there any ways I can deal with this memory issue so I can increase my batch size? Like having the embedding layer processed somewhere else, assuming I have multiple GPUs? Sorry for my ignorance as I am pretty new to this. I would like to know the best practice in handling such situation to improve training time. Thanks a lot for the guidance in advance"}