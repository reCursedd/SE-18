{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/320358341", "html_url": "https://github.com/pytorch/pytorch/issues/2293#issuecomment-320358341", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2293", "id": 320358341, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDM1ODM0MQ==", "user": {"login": "shreyassaxena", "id": 8330652, "node_id": "MDQ6VXNlcjgzMzA2NTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/8330652?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shreyassaxena", "html_url": "https://github.com/shreyassaxena", "followers_url": "https://api.github.com/users/shreyassaxena/followers", "following_url": "https://api.github.com/users/shreyassaxena/following{/other_user}", "gists_url": "https://api.github.com/users/shreyassaxena/gists{/gist_id}", "starred_url": "https://api.github.com/users/shreyassaxena/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shreyassaxena/subscriptions", "organizations_url": "https://api.github.com/users/shreyassaxena/orgs", "repos_url": "https://api.github.com/users/shreyassaxena/repos", "events_url": "https://api.github.com/users/shreyassaxena/events{/privacy}", "received_events_url": "https://api.github.com/users/shreyassaxena/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-04T21:34:31Z", "updated_at": "2017-08-04T21:34:31Z", "author_association": "NONE", "body_html": "<p>The running mean and variance of BatchNorm layer will change epoch to epoch, even when LR is 0. To get the same loss, you can set the momentum parameter of BatchNorm layer to 0 (in conjunction with LR being 0). This should fix the problem.</p>", "body_text": "The running mean and variance of BatchNorm layer will change epoch to epoch, even when LR is 0. To get the same loss, you can set the momentum parameter of BatchNorm layer to 0 (in conjunction with LR being 0). This should fix the problem.", "body": "The running mean and variance of BatchNorm layer will change epoch to epoch, even when LR is 0. To get the same loss, you can set the momentum parameter of BatchNorm layer to 0 (in conjunction with LR being 0). This should fix the problem."}