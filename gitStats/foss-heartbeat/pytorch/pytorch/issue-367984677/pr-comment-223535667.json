{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223535667", "pull_request_review_id": 162680377, "id": 223535667, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzUzNTY2Nw==", "diff_hunk": "@@ -0,0 +1,284 @@\n+#include \"torch/csrc/TypeInfo.h\"\n+\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/utils/object_ptr.h\"\n+#include \"torch/csrc/utils/pybind.h\"\n+#include \"torch/csrc/utils/python_arg_parser.h\"\n+#include \"torch/csrc/utils/python_numbers.h\"\n+#include \"torch/csrc/utils/python_strings.h\"\n+\n+#include <ATen/core/Error.h>\n+\n+#include <structmember.h>\n+#include <cstring>\n+#include <limits>\n+#include <sstream>\n+\n+PyObject* THPFInfo_New(const at::ScalarType& type) {\n+  auto finfo = (PyTypeObject*)&THPFInfoType;\n+  auto self = THPObjectPtr{finfo->tp_alloc(finfo, 0)};\n+  if (!self)\n+    throw python_error();\n+  auto self_ = reinterpret_cast<THPDTypeInfo*>(self.get());\n+  self_->type = type;\n+  return self.release();\n+}\n+\n+PyObject* THPIInfo_New(const at::ScalarType& type) {\n+  auto iinfo = (PyTypeObject*)&THPIInfoType;\n+  auto self = THPObjectPtr{iinfo->tp_alloc(iinfo, 0)};\n+  if (!self)\n+    throw python_error();\n+  auto self_ = reinterpret_cast<THPDTypeInfo*>(self.get());\n+  self_->type = type;\n+  return self.release();\n+}\n+\n+PyObject* THPDTypeInfo_repr(THPDTypeInfo* self) {\n+  std::ostringstream oss;\n+  oss << \"type_info(type=\" << self->type << \")\";\n+  return THPUtils_packString(oss.str().c_str());\n+}\n+\n+PyObject* THPDTypeInfo_str(THPDTypeInfo* self) {\n+  std::ostringstream oss;\n+  oss << \"type_info(type=\" << self->type << \")\";\n+  return THPUtils_packString(oss.str().c_str());\n+}\n+\n+PyObject* THPFInfo_pynew(PyTypeObject* type, PyObject* args, PyObject* kwargs) {\n+  HANDLE_TH_ERRORS\n+  static torch::PythonArgParser parser({\n+      \"ScalarType(ScalarType type)\",\n+  });\n+  torch::ParsedArgs<1> parsed_args;\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+  AT_CHECK(r.idx == 0, \"Not a type\");\n+  at::ScalarType scalar_type = r.scalartype(0);\n+  if (!at::isFloatingType(scalar_type)) {\n+    return PyErr_Format(\n+        PyExc_TypeError,\n+        \"torch.finfo() requires a floating point input type. Use torch.iinfo to handle '%s'\",\n+        type->tp_name);\n+  }\n+  return THPFInfo_New(scalar_type);\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+PyObject* THPIInfo_pynew(PyTypeObject* type, PyObject* args, PyObject* kwargs) {\n+  HANDLE_TH_ERRORS\n+  static torch::PythonArgParser parser({\n+      \"ScalarType(ScalarType type)\",\n+  });\n+  torch::ParsedArgs<1> parsed_args;\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+  AT_CHECK(r.idx == 0, \"Not a type\");\n+\n+  at::ScalarType scalar_type = r.scalartype(0);\n+  if (at::isFloatingType(scalar_type)) {\n+    return PyErr_Format(\n+        PyExc_TypeError,\n+        \"torch.iinfo() requires an integer input type. Use torch.finfo to handle '%s'\",\n+        type->tp_name);\n+  }\n+  return THPIInfo_New(scalar_type);\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+static PyObject* THPDTypeInfo_bits(THPDTypeInfo* self, void*) {\n+  int bits = elementSize(self->type) * 8;\n+  return PyLong_FromLong(bits);\n+}\n+\n+static PyObject* THPFInfo_eps(THPFInfo* self, void*) {\n+  switch (self->type) {\n+    case at::ScalarType::Float:\n+      return PyFloat_FromDouble(std::numeric_limits<float>::epsilon());\n+    case at::ScalarType::Double:\n+      return PyFloat_FromDouble(std::numeric_limits<double>::epsilon());\n+    case at::ScalarType::Half:\n+      return PyFloat_FromDouble(std::numeric_limits<at::Half>::epsilon());\n+    case at::ScalarType::ComplexFloat:\n+      return PyFloat_FromDouble(std::numeric_limits<float>::epsilon());\n+    case at::ScalarType::ComplexDouble:\n+      return PyFloat_FromDouble(std::numeric_limits<double>::epsilon());\n+    case at::ScalarType::ComplexHalf:\n+      return PyFloat_FromDouble(std::numeric_limits<at::Half>::epsilon());\n+    default:\n+      return Py_NotImplemented;\n+  }\n+}\n+\n+static PyObject* THPFInfo_max(THPFInfo* self, void*) {\n+  switch (self->type) {\n+    case at::ScalarType::Float:\n+      return PyFloat_FromDouble(std::numeric_limits<float>::max());\n+    case at::ScalarType::Double:\n+      return PyFloat_FromDouble(std::numeric_limits<double>::max());\n+    case at::ScalarType::Half:\n+      return PyFloat_FromDouble(std::numeric_limits<at::Half>::max());\n+    case at::ScalarType::ComplexFloat:\n+      return PyFloat_FromDouble(std::numeric_limits<float>::max());\n+    case at::ScalarType::ComplexDouble:\n+      return PyFloat_FromDouble(std::numeric_limits<double>::max());\n+    case at::ScalarType::ComplexHalf:\n+      return PyFloat_FromDouble(std::numeric_limits<at::Half>::max());\n+    default:\n+      return Py_NotImplemented;\n+  }\n+}\n+\n+static PyObject* THPIInfo_max(THPFInfo* self, void*) {\n+  switch (self->type) {\n+    case at::ScalarType::Byte:\n+      return PyLong_FromLong(std::numeric_limits<unsigned char>::max());\n+    case at::ScalarType::Char:\n+      return PyLong_FromLong(std::numeric_limits<char>::max());\n+    case at::ScalarType::Short:\n+      return PyLong_FromLong(std::numeric_limits<short>::max());\n+      break;\n+    case at::ScalarType::Int:\n+      return PyLong_FromLong(std::numeric_limits<int>::max());\n+    case at::ScalarType::Long:\n+      return PyLong_FromLong(std::numeric_limits<long>::max());", "path": "torch/csrc/TypeInfo.cpp", "position": null, "original_position": 143, "commit_id": "40738747edcecfbd4b6e95be2f07b9faea7c9b69", "original_commit_id": "2f98db9b7add8cd6d9382c267e1b525d68c157c9", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "You can probably just use the dispatch macros from `<ATen/Dispatch.h>`. e.g.:\r\n\r\n```c++\r\nreturn AT_DISPATCH_INTEGRAL_TYPES(at::CPU(self->type), \"max\", [] {\r\n  return THPUtils_packInt64(std::numeric_limits<scalar_t>::max());\r\n});\r\n```\r\n\r\nIt might be missing a case for floating-point + complex, which you would have to add.", "created_at": "2018-10-09T01:00:31Z", "updated_at": "2018-11-23T15:52:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/12472#discussion_r223535667", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12472", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223535667"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12472#discussion_r223535667"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12472"}}, "body_html": "<p>You can probably just use the dispatch macros from <code>&lt;ATen/Dispatch.h&gt;</code>. e.g.:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">return</span> AT_DISPATCH_INTEGRAL_TYPES(at::CPU(self-&gt;type), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>max<span class=\"pl-pds\">\"</span></span>, [] {\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">THPUtils_packInt64</span>(std::numeric_limits&lt;<span class=\"pl-c1\">scalar_t</span>&gt;::<span class=\"pl-c1\">max</span>());\n});</pre></div>\n<p>It might be missing a case for floating-point + complex, which you would have to add.</p>", "body_text": "You can probably just use the dispatch macros from <ATen/Dispatch.h>. e.g.:\nreturn AT_DISPATCH_INTEGRAL_TYPES(at::CPU(self->type), \"max\", [] {\n  return THPUtils_packInt64(std::numeric_limits<scalar_t>::max());\n});\nIt might be missing a case for floating-point + complex, which you would have to add.", "in_reply_to_id": 223532985}