{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227160303", "pull_request_review_id": 167136378, "id": 227160303, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzE2MDMwMw==", "diff_hunk": "@@ -1240,6 +1231,44 @@ inline Device Tensor::device() const {\n   return Device(type().device_type(), type().is_cuda() ? get_device() : -1);\n }\n \n+inline int64_t Tensor::get_device() const {\n+  // NB: This gets called a lot so we special case it here instead of dispatching\n+  // to a native function.\n+  const auto& tid = impl_->type_id();\n+  if (tid == CUDATensorId()) {\n+    // TODO: #12934 Investigate caching device on TensorImpl for performance\n+    return impl_->storage().device().index();\n+  } else if (tid == SparseCUDATensorId()) {\n+    return _values().get_device();\n+  } else {\n+    AT_ERROR(\"get_device is not implemented for type \", type());\n+  }\n+}\n+\n+inline int64_t get_device(Tensor self) {\n+  return self.get_device();\n+}\n+\n+inline bool Tensor::is_cuda() const {\n+  // NB: avoids dispatch for perf reasons\n+  const auto& tid = impl_->type_id();\n+  return tid == CUDATensorId() || tid == SparseCUDATensorId();\n+}\n+\n+inline bool is_cuda(Tensor self) {\n+  return self.is_cuda();\n+}\n+\n+inline bool Tensor::is_sparse() const {", "path": "aten/src/ATen/core/TensorMethods.h", "position": 52, "original_position": 62, "commit_id": "5cc7fac6ea3e8ceac89da60d6485ce6f53626213", "original_commit_id": "79f77c643d4f7be19ca5f24f2c0912ecbea275b2", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "This method would mean that sparse is no longer open, which could be problematic in practice because one could very reasonably want to add a `SparseHIPTensorId`, e.g.\r\n\r\nIf we are going to allow people to write code like this, it seems like a strong argument to make TensorTypeId as a closed enum, and then implement all of these \"attribute\" tests in the file (so we remember to update them when we add other TensorTypeIds).\r\n\r\nIf we *don't* want to allow this, then we had better have another, non-vdispatched way to check what the memory layout of a tensor is. Maybe some sort of \"Layout\" field in TensorImpl?", "created_at": "2018-10-22T22:28:51Z", "updated_at": "2018-11-23T15:53:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/12841#discussion_r227160303", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12841", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227160303"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12841#discussion_r227160303"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12841"}}, "body_html": "<p>This method would mean that sparse is no longer open, which could be problematic in practice because one could very reasonably want to add a <code>SparseHIPTensorId</code>, e.g.</p>\n<p>If we are going to allow people to write code like this, it seems like a strong argument to make TensorTypeId as a closed enum, and then implement all of these \"attribute\" tests in the file (so we remember to update them when we add other TensorTypeIds).</p>\n<p>If we <em>don't</em> want to allow this, then we had better have another, non-vdispatched way to check what the memory layout of a tensor is. Maybe some sort of \"Layout\" field in TensorImpl?</p>", "body_text": "This method would mean that sparse is no longer open, which could be problematic in practice because one could very reasonably want to add a SparseHIPTensorId, e.g.\nIf we are going to allow people to write code like this, it seems like a strong argument to make TensorTypeId as a closed enum, and then implement all of these \"attribute\" tests in the file (so we remember to update them when we add other TensorTypeIds).\nIf we don't want to allow this, then we had better have another, non-vdispatched way to check what the memory layout of a tensor is. Maybe some sort of \"Layout\" field in TensorImpl?"}