{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/311351061", "html_url": "https://github.com/pytorch/pytorch/issues/1318#issuecomment-311351061", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1318", "id": 311351061, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTM1MTA2MQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T13:02:00Z", "updated_at": "2017-06-27T13:02:00Z", "author_association": "COLLABORATOR", "body_html": "<p>An even smaller example: make the gc run between the forward and the backward cause this problem.<br>\nIn <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> example, traceback objects stack up until the gc automatically kicks in which make the whole thing crash if by chance it ran between  the forward and backward.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.autograd <span class=\"pl-k\">as</span> autograd\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> gc\n\nwill_crash <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\nemb_func_handle <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RnnLm</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(RnnLm, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.embedding <span class=\"pl-k\">=</span> nn.Embedding(<span class=\"pl-v\">num_embeddings</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10000</span>,\n                                      <span class=\"pl-v\">embedding_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        <span class=\"pl-k\">global</span> emb_func_handle\n        <span class=\"pl-k\">try</span>:\n            <span class=\"pl-c1\">self</span>.foobar\n        <span class=\"pl-k\">except</span> <span class=\"pl-c1\">AttributeError</span>:\n            t, v, b <span class=\"pl-k\">=</span> sys.exc_info()\n        emb_inputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embedding(inputs)\n        emb_func_handle <span class=\"pl-k\">=</span> emb_inputs.grad_fn\n        <span class=\"pl-k\">return</span> emb_inputs.view(<span class=\"pl-c1\">1280</span>, <span class=\"pl-c1\">200</span>)\n\nmodel <span class=\"pl-k\">=</span> RnnLm()\n\nx <span class=\"pl-k\">=</span> autograd.Variable(torch.LongTensor(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">20</span>).fill_(<span class=\"pl-c1\">1</span>))\n\ny_hat <span class=\"pl-k\">=</span> model(x)\n\n<span class=\"pl-k\">if</span> will_crash:\n    <span class=\"pl-k\">del</span> emb_func_handle\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this both collect the traceback and call clear on the embedding backward function python object</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this means that when backward is called, the function does not exist anymore.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> When this clear is called, the THPFunction has a refcount of 2.</span>\n    gc.collect()\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> traceback is collected here but embedding function stays alive due to the handle we have</span>\n    gc.collect()\n    <span class=\"pl-k\">del</span> emb_func_handle\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> embedding function is not cleared here even though we released our handle to it</span>\n    gc.collect()\n\ngrad <span class=\"pl-k\">=</span> y_hat.data.new(<span class=\"pl-k\">*</span>y_hat.data.size()).zero_()\ny_hat.backward(grad)</pre></div>\n<p>Note that the problem comes from the traceback object returned by <code>sys.exc_info()</code>: adding a <code>del b</code> in the forward function fix the issue.</p>\n<p>The question that remains is why the two cases for the <code>will_crash</code> flag do not have the same behavior? I would have expected they have.</p>", "body_text": "An even smaller example: make the gc run between the forward and the backward cause this problem.\nIn @soumith example, traceback objects stack up until the gc automatically kicks in which make the whole thing crash if by chance it ran between  the forward and backward.\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport sys\nimport gc\n\nwill_crash = False\n\nemb_func_handle = None\nclass RnnLm(nn.Module):\n    def __init__(self):\n        super(RnnLm, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=10000,\n                                      embedding_dim=200)\n\n    def forward(self, inputs):\n        global emb_func_handle\n        try:\n            self.foobar\n        except AttributeError:\n            t, v, b = sys.exc_info()\n        emb_inputs = self.embedding(inputs)\n        emb_func_handle = emb_inputs.grad_fn\n        return emb_inputs.view(1280, 200)\n\nmodel = RnnLm()\n\nx = autograd.Variable(torch.LongTensor(64, 20).fill_(1))\n\ny_hat = model(x)\n\nif will_crash:\n    del emb_func_handle\n    # this both collect the traceback and call clear on the embedding backward function python object\n    # this means that when backward is called, the function does not exist anymore.\n    # When this clear is called, the THPFunction has a refcount of 2.\n    gc.collect()\nelse:\n    # traceback is collected here but embedding function stays alive due to the handle we have\n    gc.collect()\n    del emb_func_handle\n    # embedding function is not cleared here even though we released our handle to it\n    gc.collect()\n\ngrad = y_hat.data.new(*y_hat.data.size()).zero_()\ny_hat.backward(grad)\nNote that the problem comes from the traceback object returned by sys.exc_info(): adding a del b in the forward function fix the issue.\nThe question that remains is why the two cases for the will_crash flag do not have the same behavior? I would have expected they have.", "body": "An even smaller example: make the gc run between the forward and the backward cause this problem.\r\nIn @soumith example, traceback objects stack up until the gc automatically kicks in which make the whole thing crash if by chance it ran between  the forward and backward.\r\n```python\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport sys\r\nimport gc\r\n\r\nwill_crash = False\r\n\r\nemb_func_handle = None\r\nclass RnnLm(nn.Module):\r\n    def __init__(self):\r\n        super(RnnLm, self).__init__()\r\n        self.embedding = nn.Embedding(num_embeddings=10000,\r\n                                      embedding_dim=200)\r\n\r\n    def forward(self, inputs):\r\n        global emb_func_handle\r\n        try:\r\n            self.foobar\r\n        except AttributeError:\r\n            t, v, b = sys.exc_info()\r\n        emb_inputs = self.embedding(inputs)\r\n        emb_func_handle = emb_inputs.grad_fn\r\n        return emb_inputs.view(1280, 200)\r\n\r\nmodel = RnnLm()\r\n\r\nx = autograd.Variable(torch.LongTensor(64, 20).fill_(1))\r\n\r\ny_hat = model(x)\r\n\r\nif will_crash:\r\n    del emb_func_handle\r\n    # this both collect the traceback and call clear on the embedding backward function python object\r\n    # this means that when backward is called, the function does not exist anymore.\r\n    # When this clear is called, the THPFunction has a refcount of 2.\r\n    gc.collect()\r\nelse:\r\n    # traceback is collected here but embedding function stays alive due to the handle we have\r\n    gc.collect()\r\n    del emb_func_handle\r\n    # embedding function is not cleared here even though we released our handle to it\r\n    gc.collect()\r\n\r\ngrad = y_hat.data.new(*y_hat.data.size()).zero_()\r\ny_hat.backward(grad)\r\n```\r\n\r\nNote that the problem comes from the traceback object returned by `sys.exc_info()`: adding a `del b` in the forward function fix the issue.\r\n\r\nThe question that remains is why the two cases for the `will_crash` flag do not have the same behavior? I would have expected they have."}