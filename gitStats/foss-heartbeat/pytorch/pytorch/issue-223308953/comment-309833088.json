{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/309833088", "html_url": "https://github.com/pytorch/pytorch/issues/1318#issuecomment-309833088", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1318", "id": 309833088, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTgzMzA4OA==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-20T17:38:40Z", "updated_at": "2017-06-20T17:39:43Z", "author_association": "MEMBER", "body_html": "<p>The problem (very strangely) is the line: <code>t, v, b = sys.exc_info()</code> (go figure).</p>\n<p>The minimum reproducible example that segfaults on both CPU and CUDA is:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.autograd <span class=\"pl-k\">as</span> autograd\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">import</span> sys\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RnnLm</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(RnnLm, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.embedding <span class=\"pl-k\">=</span> nn.Embedding(<span class=\"pl-v\">num_embeddings</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10000</span>,\n                                      <span class=\"pl-v\">embedding_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        <span class=\"pl-k\">try</span>:\n            <span class=\"pl-c1\">self</span>.foobar\n        <span class=\"pl-k\">except</span> <span class=\"pl-c1\">AttributeError</span>:\n            t, v, b <span class=\"pl-k\">=</span> sys.exc_info()\n        emb_inputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embedding(inputs)\n        <span class=\"pl-k\">return</span> emb_inputs.view(<span class=\"pl-c1\">1280</span>, <span class=\"pl-c1\">200</span>)\n\ncuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> segfaults for both cuda = True and cuda = False</span>\nmodel <span class=\"pl-k\">=</span> RnnLm()\n<span class=\"pl-k\">if</span> cuda:\n    model.cuda()\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">1000</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iter<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> torch.LongTensor(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">20</span>).fill_(<span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">if</span> cuda:\n        x <span class=\"pl-k\">=</span> x.cuda()\n    x <span class=\"pl-k\">=</span> autograd.Variable(x)\n\n    y_hat <span class=\"pl-k\">=</span> model(x)\n    y_hat.backward(y_hat.data.new(<span class=\"pl-k\">*</span>y_hat.data.size()))</pre></div>\n<p>If we just replace:</p>\n<pre><code>t, v, b = sys.exc_info()\n</code></pre>\n<p>with</p>\n<pre><code>sys.exc_info()\n</code></pre>\n<p>it doesn't segfault...</p>", "body_text": "The problem (very strangely) is the line: t, v, b = sys.exc_info() (go figure).\nThe minimum reproducible example that segfaults on both CPU and CUDA is:\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport sys\n\nclass RnnLm(nn.Module):\n    def __init__(self):\n        super(RnnLm, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=10000,\n                                      embedding_dim=200)\n\n    def forward(self, inputs):\n        try:\n            self.foobar\n        except AttributeError:\n            t, v, b = sys.exc_info()\n        emb_inputs = self.embedding(inputs)\n        return emb_inputs.view(1280, 200)\n\ncuda = False # segfaults for both cuda = True and cuda = False\nmodel = RnnLm()\nif cuda:\n    model.cuda()\n\nfor i in xrange(1000):\n    print('iter')\n    x = torch.LongTensor(64, 20).fill_(1)\n    if cuda:\n        x = x.cuda()\n    x = autograd.Variable(x)\n\n    y_hat = model(x)\n    y_hat.backward(y_hat.data.new(*y_hat.data.size()))\nIf we just replace:\nt, v, b = sys.exc_info()\n\nwith\nsys.exc_info()\n\nit doesn't segfault...", "body": "The problem (very strangely) is the line: ```t, v, b = sys.exc_info()``` (go figure).\r\n\r\nThe minimum reproducible example that segfaults on both CPU and CUDA is:\r\n\r\n```python\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport sys\r\n\r\nclass RnnLm(nn.Module):\r\n    def __init__(self):\r\n        super(RnnLm, self).__init__()\r\n        self.embedding = nn.Embedding(num_embeddings=10000,\r\n                                      embedding_dim=200)\r\n\r\n    def forward(self, inputs):\r\n        try:\r\n            self.foobar\r\n        except AttributeError:\r\n            t, v, b = sys.exc_info()\r\n        emb_inputs = self.embedding(inputs)\r\n        return emb_inputs.view(1280, 200)\r\n\r\ncuda = False # segfaults for both cuda = True and cuda = False\r\nmodel = RnnLm()\r\nif cuda:\r\n    model.cuda()\r\n\r\nfor i in xrange(1000):\r\n    print('iter')\r\n    x = torch.LongTensor(64, 20).fill_(1)\r\n    if cuda:\r\n        x = x.cuda()\r\n    x = autograd.Variable(x)\r\n\r\n    y_hat = model(x)\r\n    y_hat.backward(y_hat.data.new(*y_hat.data.size()))\r\n```\r\n\r\nIf we just replace:\r\n```\r\nt, v, b = sys.exc_info()\r\n```\r\nwith\r\n```\r\nsys.exc_info()\r\n```\r\nit doesn't segfault..."}