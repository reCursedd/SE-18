{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438879713", "html_url": "https://github.com/pytorch/pytorch/issues/14000#issuecomment-438879713", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/14000", "id": 438879713, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODg3OTcxMw==", "user": {"login": "Suhail", "id": 66225, "node_id": "MDQ6VXNlcjY2MjI1", "avatar_url": "https://avatars1.githubusercontent.com/u/66225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Suhail", "html_url": "https://github.com/Suhail", "followers_url": "https://api.github.com/users/Suhail/followers", "following_url": "https://api.github.com/users/Suhail/following{/other_user}", "gists_url": "https://api.github.com/users/Suhail/gists{/gist_id}", "starred_url": "https://api.github.com/users/Suhail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Suhail/subscriptions", "organizations_url": "https://api.github.com/users/Suhail/orgs", "repos_url": "https://api.github.com/users/Suhail/repos", "events_url": "https://api.github.com/users/Suhail/events{/privacy}", "received_events_url": "https://api.github.com/users/Suhail/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-15T01:17:16Z", "updated_at": "2018-11-15T01:21:59Z", "author_association": "NONE", "body_html": "<p>Thank you for being so speedy! Super helpful.</p>\n<p>Good to know it's just flat -- that means I don't know need json in the first place most likely.</p>\n<p>Regardless, I am wondering if there's something a bit off about the code above since I am getting different inference results on successive results with the same data.</p>\n<p>Sometimes I get a -nan, -nan result. Could be the model though:</p>\n<pre lang=\"paperspace@psokdwo8n:~/mnist-torch-script-test/build$\" data-meta=\"./example-app ../mnist-traced.pt\"><code>Model loaded.\n 1.9708e+26  1.1496e+26\n[ Variable[CPUFloatType]{1,2} ]\npaperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\nModel loaded.\n-nan -nan\n[ Variable[CPUFloatType]{1,2} ]\npaperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\nModel loaded.\n0.01 *\n-3.6975 -41.3416\n[ Variable[CPUFloatType]{1,2} ]\n</code></pre>\n<p>Here's my code:</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>fstream<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>memory<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>string<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>exception<span class=\"pl-pds\">&gt;</span></span>\n\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>nlohmann/json.hpp<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/script.h<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">std</span><span class=\"pl-k\">;</span>\n<span class=\"pl-k\">using</span> json = nlohmann::json;\n\n<span class=\"pl-k\">const</span> <span class=\"pl-k\">bool</span> DEBUG_VERBOSE = <span class=\"pl-c1\">false</span>;\n\njson <span class=\"pl-en\">get_img_convert_to_tensor</span>(string);\nstring <span class=\"pl-en\">get_json_from_file</span>(string);\n\n<span class=\"pl-k\">int</span> <span class=\"pl-en\">main</span>(<span class=\"pl-k\">int</span> argc, <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* argv[]) {\n    <span class=\"pl-k\">if</span> (argc != <span class=\"pl-c1\">2</span>) {\n        cerr &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>usage: example-app &lt;path-to-exported-script-module&gt;<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">return</span> -<span class=\"pl-c1\">1</span>;\n    }\n\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> Deserialize the ScriptModule from a file using torch::jit::load().</span>\n    shared_ptr&lt;torch::jit::script::Module&gt; module = <span class=\"pl-c1\">torch::jit::load</span>(argv[<span class=\"pl-c1\">1</span>]);\n\n    <span class=\"pl-c1\">assert</span>(module != <span class=\"pl-c1\">nullptr</span>);\n    cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Model loaded.<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>;\n\n    vector&lt;torch::jit::IValue&gt; inputs;;\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span>inputs.push_back(torch::ones({1, 3, 28, 28}));</span>\n\n    vector&lt;<span class=\"pl-c1\">int64_t</span>&gt; sizes = {<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>};\n    json j = <span class=\"pl-c1\">get_img_convert_to_tensor</span>(<span class=\"pl-c1\">get_json_from_file</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>../29300.png.json<span class=\"pl-pds\">\"</span></span>));\n\n    <span class=\"pl-k\">auto</span> blob = j.<span class=\"pl-smi\">get</span>&lt;vector&lt;vector&lt;vector&lt;vector&lt;<span class=\"pl-k\">float</span>&gt;&gt;&gt;&gt;&gt;();\n    <span class=\"pl-k\">auto</span> tensor = <span class=\"pl-c1\">torch::empty</span>(<span class=\"pl-c1\">1</span> * <span class=\"pl-c1\">3</span> * <span class=\"pl-c1\">28</span> * <span class=\"pl-c1\">28</span>);\n    <span class=\"pl-k\">float</span>* data = tensor.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-k\">float</span>&gt;();\n\n    <span class=\"pl-k\">for</span> (<span class=\"pl-k\">const</span> <span class=\"pl-k\">auto</span>&amp; i : blob) {\n        <span class=\"pl-k\">for</span> (<span class=\"pl-k\">const</span> <span class=\"pl-k\">auto</span>&amp; j : i) {\n            <span class=\"pl-k\">for</span> (<span class=\"pl-k\">const</span> <span class=\"pl-k\">auto</span>&amp; k : j) {\n                <span class=\"pl-k\">for</span> (<span class=\"pl-k\">const</span> <span class=\"pl-k\">auto</span>&amp; l : k) {\n                    *data++ = l;\n                }\n            }\n        }\n    }\n\n    at::TensorOptions <span class=\"pl-smi\">options</span>(at::ScalarType::<span class=\"pl-c1\">Byte</span>);\n\n    at::Tensor t = <span class=\"pl-c1\">torch::from_blob</span>(data, {<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>});\n    t = t.<span class=\"pl-c1\">toType</span>(at::<span class=\"pl-c1\">kFloat</span>);\n\n    inputs.<span class=\"pl-c1\">emplace_back</span>(t);\n\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> Run inference</span>\n    <span class=\"pl-k\">auto</span> output = module-&gt;<span class=\"pl-c1\">forward</span>(inputs).<span class=\"pl-c1\">toTensor</span>();\n    cout &lt;&lt; output.<span class=\"pl-c1\">slice</span>(<span class=\"pl-c\"><span class=\"pl-c\">/*</span>dim=<span class=\"pl-c\">*/</span></span><span class=\"pl-c1\">1</span>, <span class=\"pl-c\"><span class=\"pl-c\">/*</span>start=<span class=\"pl-c\">*/</span></span><span class=\"pl-c1\">0</span>, <span class=\"pl-c\"><span class=\"pl-c\">/*</span>end=<span class=\"pl-c\">*/</span></span><span class=\"pl-c1\">5</span>) &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>;\n\n}</pre></div>\n<p>Model: <a href=\"https://drive.google.com/file/d/14vzQV2UXigkrxAfcvqzQDsFg5AXBSgt7/view?usp=sharing\" rel=\"nofollow\">https://drive.google.com/file/d/14vzQV2UXigkrxAfcvqzQDsFg5AXBSgt7/view?usp=sharing</a></p>", "body_text": "Thank you for being so speedy! Super helpful.\nGood to know it's just flat -- that means I don't know need json in the first place most likely.\nRegardless, I am wondering if there's something a bit off about the code above since I am getting different inference results on successive results with the same data.\nSometimes I get a -nan, -nan result. Could be the model though:\nModel loaded.\n 1.9708e+26  1.1496e+26\n[ Variable[CPUFloatType]{1,2} ]\npaperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\nModel loaded.\n-nan -nan\n[ Variable[CPUFloatType]{1,2} ]\npaperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\nModel loaded.\n0.01 *\n-3.6975 -41.3416\n[ Variable[CPUFloatType]{1,2} ]\n\nHere's my code:\n#include <iostream>\n#include <fstream>\n#include <memory>\n#include <string>\n#include <exception>\n\n#include <nlohmann/json.hpp>\n#include <torch/script.h>\n\nusing namespace std;\nusing json = nlohmann::json;\n\nconst bool DEBUG_VERBOSE = false;\n\njson get_img_convert_to_tensor(string);\nstring get_json_from_file(string);\n\nint main(int argc, const char* argv[]) {\n    if (argc != 2) {\n        cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\n        return -1;\n    }\n\n    // Deserialize the ScriptModule from a file using torch::jit::load().\n    shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\n\n    assert(module != nullptr);\n    cout << \"Model loaded.\\n\";\n\n    vector<torch::jit::IValue> inputs;;\n    //inputs.push_back(torch::ones({1, 3, 28, 28}));\n\n    vector<int64_t> sizes = {1, 3, 28, 28};\n    json j = get_img_convert_to_tensor(get_json_from_file(\"../29300.png.json\"));\n\n    auto blob = j.get<vector<vector<vector<vector<float>>>>>();\n    auto tensor = torch::empty(1 * 3 * 28 * 28);\n    float* data = tensor.data<float>();\n\n    for (const auto& i : blob) {\n        for (const auto& j : i) {\n            for (const auto& k : j) {\n                for (const auto& l : k) {\n                    *data++ = l;\n                }\n            }\n        }\n    }\n\n    at::TensorOptions options(at::ScalarType::Byte);\n\n    at::Tensor t = torch::from_blob(data, {1, 3, 28, 28});\n    t = t.toType(at::kFloat);\n\n    inputs.emplace_back(t);\n\n    // Run inference\n    auto output = module->forward(inputs).toTensor();\n    cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\\n';\n\n}\nModel: https://drive.google.com/file/d/14vzQV2UXigkrxAfcvqzQDsFg5AXBSgt7/view?usp=sharing", "body": "Thank you for being so speedy! Super helpful.\r\n\r\nGood to know it's just flat -- that means I don't know need json in the first place most likely.\r\n\r\nRegardless, I am wondering if there's something a bit off about the code above since I am getting different inference results on successive results with the same data. \r\n\r\nSometimes I get a -nan, -nan result. Could be the model though:\r\n```paperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\r\nModel loaded.\r\n 1.9708e+26  1.1496e+26\r\n[ Variable[CPUFloatType]{1,2} ]\r\npaperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\r\nModel loaded.\r\n-nan -nan\r\n[ Variable[CPUFloatType]{1,2} ]\r\npaperspace@psokdwo8n:~/mnist-torch-script-test/build$ ./example-app ../mnist-traced.pt\r\nModel loaded.\r\n0.01 *\r\n-3.6975 -41.3416\r\n[ Variable[CPUFloatType]{1,2} ]\r\n```\r\n\r\nHere's my code:\r\n\r\n```c++\r\n#include <iostream>\r\n#include <fstream>\r\n#include <memory>\r\n#include <string>\r\n#include <exception>\r\n\r\n#include <nlohmann/json.hpp>\r\n#include <torch/script.h>\r\n\r\nusing namespace std;\r\nusing json = nlohmann::json;\r\n\r\nconst bool DEBUG_VERBOSE = false;\r\n\r\njson get_img_convert_to_tensor(string);\r\nstring get_json_from_file(string);\r\n\r\nint main(int argc, const char* argv[]) {\r\n    if (argc != 2) {\r\n        cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\r\n        return -1;\r\n    }\r\n\r\n    // Deserialize the ScriptModule from a file using torch::jit::load().\r\n    shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\r\n\r\n    assert(module != nullptr);\r\n    cout << \"Model loaded.\\n\";\r\n\r\n    vector<torch::jit::IValue> inputs;;\r\n    //inputs.push_back(torch::ones({1, 3, 28, 28}));\r\n\r\n    vector<int64_t> sizes = {1, 3, 28, 28};\r\n    json j = get_img_convert_to_tensor(get_json_from_file(\"../29300.png.json\"));\r\n\r\n    auto blob = j.get<vector<vector<vector<vector<float>>>>>();\r\n    auto tensor = torch::empty(1 * 3 * 28 * 28);\r\n    float* data = tensor.data<float>();\r\n\r\n    for (const auto& i : blob) {\r\n        for (const auto& j : i) {\r\n            for (const auto& k : j) {\r\n                for (const auto& l : k) {\r\n                    *data++ = l;\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    at::TensorOptions options(at::ScalarType::Byte);\r\n\r\n    at::Tensor t = torch::from_blob(data, {1, 3, 28, 28});\r\n    t = t.toType(at::kFloat);\r\n\r\n    inputs.emplace_back(t);\r\n\r\n    // Run inference\r\n    auto output = module->forward(inputs).toTensor();\r\n    cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\\n';\r\n\r\n}\r\n```\r\n\r\nModel: https://drive.google.com/file/d/14vzQV2UXigkrxAfcvqzQDsFg5AXBSgt7/view?usp=sharing"}