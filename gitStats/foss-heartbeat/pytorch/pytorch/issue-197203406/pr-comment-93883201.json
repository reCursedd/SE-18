{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/93883201", "pull_request_review_id": 14415448, "id": 93883201, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDkzODgzMjAx", "diff_hunk": "@@ -0,0 +1,127 @@\n+import torch\n+import os\n+import weakref\n+import multiprocessing\n+from multiprocessing.reduction import ForkingPickler\n+import sys\n+try:\n+    # Early load resource_sharer to prevent a partially initialized instance\n+    # from being inherited in a forked child process. The reduce_storage method\n+    # requires this module indirectly through DupFd(). The built-in mp.Queue\n+    # class pickles arguments in a background thread which may overlap with the\n+    # fork.\n+    import multiprocessing.resource_sharer\n+except ImportError:\n+    pass\n+\n+\n+class StorageRef(object):\n+    def __init__(self, ptr):\n+        self.cdata = ptr\n+\n+\n+shared_cache = weakref.WeakValueDictionary()\n+\n+\n+def rebuild_event(handle):\n+    return torch.cuda.Event(_handle=handle)\n+\n+\n+def reduce_event(event):\n+    return (rebuild_event, (event.ipc_handle(),))\n+\n+\n+def rebuild_tensor(cls, storage, metadata):\n+    storage_offset, size, stride = metadata\n+    new_tensor = cls()\n+    new_tensor.set_(storage, storage_offset, size, stride)\n+    return new_tensor\n+\n+\n+def reduce_tensor(tensor):\n+    metadata = (tensor.storage_offset(), tensor.size(), tensor.stride())\n+    storage = tensor.storage()\n+    return (rebuild_tensor, (type(tensor), storage, metadata))\n+\n+\n+def fd_id(fd):\n+    stat = os.fstat(fd)\n+    return (stat.st_ino, stat.st_dev)\n+\n+\n+def storage_from_cache(cls, key):\n+    storage_ref = shared_cache.get(key)\n+    if storage_ref is None:\n+        return None\n+    new_storage = cls._new_with_weak_ptr(storage_ref)\n+    if new_storage is None:\n+        return None\n+    new_storage._shared_decref()\n+    return new_storage\n+\n+\n+def rebuild_storage_fd(cls, df, size):\n+    if sys.version_info[0] == 2:\n+        fd = multiprocessing.reduction.rebuild_handle(df)\n+    else:\n+        fd = df.detach()\n+    storage = storage_from_cache(cls, fd_id(fd))\n+    if storage is not None:\n+        return storage\n+    storage = cls._new_shared_fd(fd, size)\n+    shared_cache[fd_id(fd)] = storage._weak_ref(StorageRef)\n+    return storage._shared_decref()\n+\n+\n+def rebuild_storage_filename(cls, manager, handle, size):\n+    storage = storage_from_cache(cls, handle)\n+    if storage is not None:\n+        return storage\n+    storage = cls._new_shared_filename(manager, handle, size)\n+    shared_cache[handle] = storage._weak_ref(StorageRef)\n+    return storage._shared_decref()\n+\n+\n+def reubild_storage_cuda(cls, device, handle, size, offset, view_size):\n+    storage = storage_from_cache(cls, handle)\n+    if storage is not None:\n+        return storage._new_view(offset, size)\n+    torch.cuda._lazy_init()\n+    storage = cls._new_shared_cuda(device, handle, size, offset, view_size)\n+    shared_cache[handle] = storage._weak_ref(StorageRef)\n+    return storage._shared_decref()\n+\n+\n+def reduce_storage(storage):\n+    from . import get_sharing_strategy\n+    if storage.is_cuda:\n+        metadata = storage._share_cuda_()\n+        cache_key = metadata[1]\n+        rebuild = reubild_storage_cuda\n+    elif get_sharing_strategy() == 'file_system':\n+        metadata = storage._share_filename_()\n+        cache_key = metadata[1]\n+        rebuild = rebuild_storage_filename\n+    else:\n+        fd, size = storage._share_fd_()\n+        if sys.version_info[0] == 2:\n+            df = multiprocessing.reduction.reduce_handle(fd)\n+        else:\n+            df = multiprocessing.reduction.DupFd(fd)", "path": "torch/multiprocessing/reductions.py", "position": 116, "original_position": 110, "commit_id": "fe300c504147d8947010c885b45d6ad7bca8a3ea", "original_commit_id": "f44dea548f411ab8fda601766b4e7a2c12642758", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "No way, all python versions have functions for duplicating fds? \ud83d\ude15 \r\nWhat's this object?", "created_at": "2016-12-26T20:09:30Z", "updated_at": "2018-11-23T15:32:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/344#discussion_r93883201", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/344", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/93883201"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/344#discussion_r93883201"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/344"}}, "body_html": "<p>No way, all python versions have functions for duplicating fds? <g-emoji class=\"g-emoji\" alias=\"confused\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f615.png\">\ud83d\ude15</g-emoji><br>\nWhat's this object?</p>", "body_text": "No way, all python versions have functions for duplicating fds? \ud83d\ude15\nWhat's this object?"}