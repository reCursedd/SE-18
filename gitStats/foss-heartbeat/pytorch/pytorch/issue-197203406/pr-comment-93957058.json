{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/93957058", "pull_request_review_id": 14486153, "id": 93957058, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDkzOTU3MDU4", "diff_hunk": "@@ -0,0 +1,127 @@\n+import torch\n+import os\n+import weakref\n+import multiprocessing\n+from multiprocessing.reduction import ForkingPickler\n+import sys\n+try:\n+    # Early load resource_sharer to prevent a partially initialized instance\n+    # from being inherited in a forked child process. The reduce_storage method\n+    # requires this module indirectly through DupFd(). The built-in mp.Queue\n+    # class pickles arguments in a background thread which may overlap with the\n+    # fork.\n+    import multiprocessing.resource_sharer\n+except ImportError:\n+    pass\n+\n+\n+class StorageRef(object):\n+    def __init__(self, ptr):\n+        self.cdata = ptr\n+\n+\n+shared_cache = weakref.WeakValueDictionary()\n+\n+\n+def rebuild_event(handle):\n+    return torch.cuda.Event(_handle=handle)\n+\n+\n+def reduce_event(event):\n+    return (rebuild_event, (event.ipc_handle(),))\n+\n+\n+def rebuild_tensor(cls, storage, metadata):\n+    storage_offset, size, stride = metadata\n+    new_tensor = cls()\n+    new_tensor.set_(storage, storage_offset, size, stride)\n+    return new_tensor\n+\n+\n+def reduce_tensor(tensor):\n+    metadata = (tensor.storage_offset(), tensor.size(), tensor.stride())\n+    storage = tensor.storage()\n+    return (rebuild_tensor, (type(tensor), storage, metadata))\n+\n+\n+def fd_id(fd):\n+    stat = os.fstat(fd)\n+    return (stat.st_ino, stat.st_dev)\n+\n+\n+def storage_from_cache(cls, key):\n+    storage_ref = shared_cache.get(key)\n+    if storage_ref is None:\n+        return None\n+    new_storage = cls._new_with_weak_ptr(storage_ref)\n+    if new_storage is None:\n+        return None\n+    new_storage._shared_decref()\n+    return new_storage\n+\n+\n+def rebuild_storage_fd(cls, df, size):\n+    if sys.version_info[0] == 2:\n+        fd = multiprocessing.reduction.rebuild_handle(df)\n+    else:\n+        fd = df.detach()\n+    storage = storage_from_cache(cls, fd_id(fd))\n+    if storage is not None:\n+        return storage\n+    storage = cls._new_shared_fd(fd, size)\n+    shared_cache[fd_id(fd)] = storage._weak_ref(StorageRef)\n+    return storage._shared_decref()\n+\n+\n+def rebuild_storage_filename(cls, manager, handle, size):\n+    storage = storage_from_cache(cls, handle)\n+    if storage is not None:\n+        return storage\n+    storage = cls._new_shared_filename(manager, handle, size)\n+    shared_cache[handle] = storage._weak_ref(StorageRef)\n+    return storage._shared_decref()\n+\n+\n+def reubild_storage_cuda(cls, device, handle, size, offset, view_size):\n+    storage = storage_from_cache(cls, handle)\n+    if storage is not None:\n+        return storage._new_view(offset, size)\n+    torch.cuda._lazy_init()\n+    storage = cls._new_shared_cuda(device, handle, size, offset, view_size)\n+    shared_cache[handle] = storage._weak_ref(StorageRef)\n+    return storage._shared_decref()\n+\n+\n+def reduce_storage(storage):\n+    from . import get_sharing_strategy\n+    if storage.is_cuda:\n+        metadata = storage._share_cuda_()\n+        cache_key = metadata[1]\n+        rebuild = reubild_storage_cuda\n+    elif get_sharing_strategy() == 'file_system':\n+        metadata = storage._share_filename_()\n+        cache_key = metadata[1]\n+        rebuild = rebuild_storage_filename\n+    else:\n+        fd, size = storage._share_fd_()\n+        if sys.version_info[0] == 2:\n+            df = multiprocessing.reduction.reduce_handle(fd)\n+        else:\n+            df = multiprocessing.reduction.DupFd(fd)", "path": "torch/multiprocessing/reductions.py", "position": 116, "original_position": 110, "commit_id": "fe300c504147d8947010c885b45d6ad7bca8a3ea", "original_commit_id": "f44dea548f411ab8fda601766b4e7a2c12642758", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Yeah, it's surprising. Especially since you still can't do some basic ops like sending a pipe through a pipe (at least in 2.7).\r\n\r\nThe sending of fds has some disadvantages vs. your implementation. It uses a listener pattern (see [connection.py](https://github.com/python/cpython/blob/2.7/Lib/multiprocessing/connection.py#L117)) which means that the sender has to be alive when the receiver tries to get the fd. That's why I added the extra try-except in dataloader.\r\n\r\nIn practice, I think it's still worth it since we can use all the built-in multiprocessing objects in 3.4+ which is way easier than trying to match the `get_context` based API.", "created_at": "2016-12-27T18:17:46Z", "updated_at": "2018-11-23T15:32:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/344#discussion_r93957058", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/344", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/93957058"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/344#discussion_r93957058"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/344"}}, "body_html": "<p>Yeah, it's surprising. Especially since you still can't do some basic ops like sending a pipe through a pipe (at least in 2.7).</p>\n<p>The sending of fds has some disadvantages vs. your implementation. It uses a listener pattern (see <a href=\"https://github.com/python/cpython/blob/2.7/Lib/multiprocessing/connection.py#L117\">connection.py</a>) which means that the sender has to be alive when the receiver tries to get the fd. That's why I added the extra try-except in dataloader.</p>\n<p>In practice, I think it's still worth it since we can use all the built-in multiprocessing objects in 3.4+ which is way easier than trying to match the <code>get_context</code> based API.</p>", "body_text": "Yeah, it's surprising. Especially since you still can't do some basic ops like sending a pipe through a pipe (at least in 2.7).\nThe sending of fds has some disadvantages vs. your implementation. It uses a listener pattern (see connection.py) which means that the sender has to be alive when the receiver tries to get the fd. That's why I added the extra try-except in dataloader.\nIn practice, I think it's still worth it since we can use all the built-in multiprocessing objects in 3.4+ which is way easier than trying to match the get_context based API.", "in_reply_to_id": 93883201}