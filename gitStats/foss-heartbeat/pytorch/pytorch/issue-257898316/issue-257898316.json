{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2746", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2746/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2746/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2746/events", "html_url": "https://github.com/pytorch/pytorch/pull/2746", "id": 257898316, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQxMjA4Nzg2", "number": 2746, "title": "Changes from jit branch", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-15T01:04:04Z", "updated_at": "2017-09-19T14:53:33Z", "closed_at": "2017-09-19T14:53:33Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2746", "html_url": "https://github.com/pytorch/pytorch/pull/2746", "diff_url": "https://github.com/pytorch/pytorch/pull/2746.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2746.patch"}, "body_html": "<p>This PR integrates progress from <a href=\"https://github.com/ezyang/pytorch/\">https://github.com/ezyang/pytorch/</a></p>\n<p><strong>ONNX improvements.</strong></p>\n<ul>\n<li>Support for SubConstant can be exported via ONNX (TODO: not standardized in ONNX, see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"257897138\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/onnx/onnx/issues/26\" data-hovercard-type=\"issue\" data-hovercard-url=\"/onnx/onnx/issues/26/hovercard\" href=\"https://github.com/onnx/onnx/issues/26\">onnx/onnx#26</a>)</li>\n<li>We now directly use the ONNX IR as the intermediate compiler IR which we optimize. Some current ONNX symbolic definitions are not suitable for this, so we've exported to a PyTorch specific op, which is then further translated when we export (most notably Index got this treatment.)</li>\n</ul>\n<p><strong>Tracing improvements.</strong></p>\n<ul>\n<li>Better support for undefined tensors, by saving a TensorMeta which records what the original input tensor was, so we can infer what the grad_input should look like (TODO: at the moment, this is buggy), and some more careful definedness checking</li>\n</ul>\n<p><strong>JIT improvements.</strong></p>\n<ul>\n<li>We can now execute JIT traces that contain ONNX symbolic ops</li>\n<li>We have a SimpleEval optimization for executing backwards on black-box autograd operators. This reduces JIT trace execution levels back to regular backwards.</li>\n<li>Fusion compiler is more robust; it only attempts to fuse CUDA operators, and checks that the device information is consistent</li>\n</ul>\n<p><strong>Misc code cleanup.</strong></p>\n<ul>\n<li>New JIT_EXPECTM macro for user failures (as opposed to assert failures)</li>\n<li>Some API improvements to the IR (not exposed from Python yet.) For example, there is now a resource guard for temporarily setting stage we trace into.</li>\n<li>New fmap variant that can apply constructors</li>\n<li>More user friendly names for trace dumps</li>\n</ul>", "body_text": "This PR integrates progress from https://github.com/ezyang/pytorch/\nONNX improvements.\n\nSupport for SubConstant can be exported via ONNX (TODO: not standardized in ONNX, see onnx/onnx#26)\nWe now directly use the ONNX IR as the intermediate compiler IR which we optimize. Some current ONNX symbolic definitions are not suitable for this, so we've exported to a PyTorch specific op, which is then further translated when we export (most notably Index got this treatment.)\n\nTracing improvements.\n\nBetter support for undefined tensors, by saving a TensorMeta which records what the original input tensor was, so we can infer what the grad_input should look like (TODO: at the moment, this is buggy), and some more careful definedness checking\n\nJIT improvements.\n\nWe can now execute JIT traces that contain ONNX symbolic ops\nWe have a SimpleEval optimization for executing backwards on black-box autograd operators. This reduces JIT trace execution levels back to regular backwards.\nFusion compiler is more robust; it only attempts to fuse CUDA operators, and checks that the device information is consistent\n\nMisc code cleanup.\n\nNew JIT_EXPECTM macro for user failures (as opposed to assert failures)\nSome API improvements to the IR (not exposed from Python yet.) For example, there is now a resource guard for temporarily setting stage we trace into.\nNew fmap variant that can apply constructors\nMore user friendly names for trace dumps", "body": "This PR integrates progress from https://github.com/ezyang/pytorch/\r\n\r\n**ONNX improvements.**\r\n- Support for SubConstant can be exported via ONNX (TODO: not standardized in ONNX, see https://github.com/onnx/onnx/issues/26)\r\n- We now directly use the ONNX IR as the intermediate compiler IR which we optimize. Some current ONNX symbolic definitions are not suitable for this, so we've exported to a PyTorch specific op, which is then further translated when we export (most notably Index got this treatment.)\r\n\r\n**Tracing improvements.**\r\n- Better support for undefined tensors, by saving a TensorMeta which records what the original input tensor was, so we can infer what the grad_input should look like (TODO: at the moment, this is buggy), and some more careful definedness checking\r\n\r\n**JIT improvements.**\r\n- We can now execute JIT traces that contain ONNX symbolic ops\r\n- We have a SimpleEval optimization for executing backwards on black-box autograd operators. This reduces JIT trace execution levels back to regular backwards.\r\n- Fusion compiler is more robust; it only attempts to fuse CUDA operators, and checks that the device information is consistent\r\n\r\n**Misc code cleanup.**\r\n- New JIT_EXPECTM macro for user failures (as opposed to assert failures)\r\n- Some API improvements to the IR (not exposed from Python yet.) For example, there is now a resource guard for temporarily setting stage we trace into.\r\n- New fmap variant that can apply constructors\r\n- More user friendly names for trace dumps"}