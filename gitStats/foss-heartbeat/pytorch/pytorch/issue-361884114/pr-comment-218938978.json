{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218938978", "pull_request_review_id": 156990516, "id": 218938978, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODkzODk3OA==", "diff_hunk": "@@ -2466,6 +2466,19 @@ def test_rnn_backward_to_input_but_not_parameters_cuda(self):\n         out.sum().backward()\n         self.assertFalse(s.grad is None or s.grad.abs().sum().item() == 0)\n \n+    @unittest.skipIf(not torch.cuda.is_available(), \"CUDA unavailable\")\n+    @skipIfRocm\n+    def test_lstmcell_backward_only_one_output_grad(self):\n+        # checks that undefined gradients doen't hamper the backward\n+        # see #11872\n+        dev = torch.device('cuda')\n+        l = torch.nn.LSTMCell(2, 3).to(dev).double()\n+        s = torch.randn(1, 2, device=dev, dtype=torch.double, requires_grad=True)\n+        for i in range(2):\n+            out = l(s)[i]\n+            out.sum().backward()\n+            self.assertFalse(s.grad is None or s.grad.abs().sum().item() == 0)", "path": "test/test_autograd.py", "position": 15, "original_position": 15, "commit_id": "bc367f1f395e403855d046cbbc815466eb7690cc", "original_commit_id": "c7b1f5c3e987305ccbe00dd1c2f27cd793d38915", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "This isn't that safe. You can very well get 0 grad if RNG doesn't treat you well. I think just checking for non-throwing would be fine (maybe with a comment). \r\n\r\nIf you really to, you can also make `s` positive, initialize lstm weights to identity, bias to zeros, and use relu, to make sure that there is positive gradient (you can check for exact values too). But that might be too much work.", "created_at": "2018-09-19T19:44:50Z", "updated_at": "2018-11-23T15:51:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/11872#discussion_r218938978", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11872", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218938978"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11872#discussion_r218938978"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11872"}}, "body_html": "<p>This isn't that safe. You can very well get 0 grad if RNG doesn't treat you well. I think just checking for non-throwing would be fine (maybe with a comment).</p>\n<p>If you really to, you can also make <code>s</code> positive, initialize lstm weights to identity, bias to zeros, and use relu, to make sure that there is positive gradient (you can check for exact values too). But that might be too much work.</p>", "body_text": "This isn't that safe. You can very well get 0 grad if RNG doesn't treat you well. I think just checking for non-throwing would be fine (maybe with a comment).\nIf you really to, you can also make s positive, initialize lstm weights to identity, bias to zeros, and use relu, to make sure that there is positive gradient (you can check for exact values too). But that might be too much work."}