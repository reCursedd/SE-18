{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349524403", "html_url": "https://github.com/pytorch/pytorch/pull/3740#issuecomment-349524403", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3740", "id": 349524403, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTUyNDQwMw==", "user": {"login": "TiRune", "id": 1436356, "node_id": "MDQ6VXNlcjE0MzYzNTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1436356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TiRune", "html_url": "https://github.com/TiRune", "followers_url": "https://api.github.com/users/TiRune/followers", "following_url": "https://api.github.com/users/TiRune/following{/other_user}", "gists_url": "https://api.github.com/users/TiRune/gists{/gist_id}", "starred_url": "https://api.github.com/users/TiRune/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TiRune/subscriptions", "organizations_url": "https://api.github.com/users/TiRune/orgs", "repos_url": "https://api.github.com/users/TiRune/repos", "events_url": "https://api.github.com/users/TiRune/events{/privacy}", "received_events_url": "https://api.github.com/users/TiRune/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-06T03:52:43Z", "updated_at": "2017-12-06T11:09:07Z", "author_association": "NONE", "body_html": "<p>I tested the current code on a simple SeNET network on CIFAR10. The current ADAMW code I found here works nearly identically to SGD with momentum 0.9, and does not seem to decay the learning rate at all. In contract to that, the original ADAM optimiser decays the learning rate properly and converges very differently from SGD.</p>\n<p>I found the reason for this, and will leave this comment here for other people who are looking for a solution. The initial model used learning rate decay in steps after 2 set periods of iterations. In the old ADAM/SGD settings, this means the weight decay also gets decayed by a factor (in my case 0.1) every set of iterations. This means that to get the same results as with the original methods, you have to take care that the weight_decay term now has a different interpretation! E.g. if the final decay was done by multiplication of a factor 0.01, your weight decay should be the original setting *0.01 for the same results.</p>", "body_text": "I tested the current code on a simple SeNET network on CIFAR10. The current ADAMW code I found here works nearly identically to SGD with momentum 0.9, and does not seem to decay the learning rate at all. In contract to that, the original ADAM optimiser decays the learning rate properly and converges very differently from SGD.\nI found the reason for this, and will leave this comment here for other people who are looking for a solution. The initial model used learning rate decay in steps after 2 set periods of iterations. In the old ADAM/SGD settings, this means the weight decay also gets decayed by a factor (in my case 0.1) every set of iterations. This means that to get the same results as with the original methods, you have to take care that the weight_decay term now has a different interpretation! E.g. if the final decay was done by multiplication of a factor 0.01, your weight decay should be the original setting *0.01 for the same results.", "body": "I tested the current code on a simple SeNET network on CIFAR10. The current ADAMW code I found here works nearly identically to SGD with momentum 0.9, and does not seem to decay the learning rate at all. In contract to that, the original ADAM optimiser decays the learning rate properly and converges very differently from SGD. \r\n \r\nI found the reason for this, and will leave this comment here for other people who are looking for a solution. The initial model used learning rate decay in steps after 2 set periods of iterations. In the old ADAM/SGD settings, this means the weight decay also gets decayed by a factor (in my case 0.1) every set of iterations. This means that to get the same results as with the original methods, you have to take care that the weight_decay term now has a different interpretation! E.g. if the final decay was done by multiplication of a factor 0.01, your weight decay should be the original setting *0.01 for the same results."}