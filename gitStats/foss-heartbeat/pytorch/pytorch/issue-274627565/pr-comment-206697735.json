{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206697735", "pull_request_review_id": 142162923, "id": 206697735, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjY5NzczNQ==", "diff_hunk": "@@ -0,0 +1,84 @@\n+import math\n+import torch\n+from .optimizer import Optimizer\n+\n+\n+class AdamW(Optimizer):\n+    \"\"\"Implements AdamW algorithm.\n+\n+    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n+\n+    Arguments:\n+        params (iterable): iterable of parameters to optimize or dicts defining\n+            parameter groups\n+        lr (float, optional): learning rate (default: 1e-3)\n+        betas (Tuple[float, float], optional): coefficients used for computing\n+            running averages of gradient and its square (default: (0.9, 0.999))\n+        eps (float, optional): term added to the denominator to improve\n+            numerical stability (default: 1e-8)\n+        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n+\n+    .. Fixing Weight Decay Regularization in Adam:\n+    https://arxiv.org/abs/1711.05101\n+    \"\"\"\n+\n+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n+                 weight_decay=0):\n+        defaults = dict(lr=lr, betas=betas, eps=eps,\n+                        weight_decay=weight_decay)\n+        super(AdamW, self).__init__(params, defaults)\n+\n+    def step(self, closure=None):\n+        \"\"\"Performs a single optimization step.\n+\n+        Arguments:\n+            closure (callable, optional): A closure that reevaluates the model\n+                and returns the loss.\n+        \"\"\"\n+        loss = None\n+        if closure is not None:\n+            loss = closure()\n+\n+        for group in self.param_groups:\n+            for p in group['params']:\n+                if p.grad is None:\n+                    continue\n+                grad = p.grad.data\n+                if grad.is_sparse:\n+                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n+\n+                state = self.state[p]\n+\n+                # State initialization\n+                if len(state) == 0:\n+                    state['step'] = 0\n+                    # Exponential moving average of gradient values\n+                    state['exp_avg'] = torch.zeros_like(p.data)\n+                    # Exponential moving average of squared gradient values\n+                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n+\n+                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n+                beta1, beta2 = group['betas']\n+\n+                state['step'] += 1\n+\n+                # according to the paper, this penalty should come after the bias correction\n+                # if group['weight_decay'] != 0:\n+                #     grad = grad.add(group['weight_decay'], p.data)\n+\n+                # Decay the first and second moment running average coefficient\n+                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n+                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n+\n+                denom = exp_avg_sq.sqrt().add_(group['eps'])\n+\n+                bias_correction1 = 1 - beta1 ** state['step']\n+                bias_correction2 = 1 - beta2 ** state['step']\n+                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n+\n+                p.data.addcdiv_(-step_size, exp_avg, denom)\n+\n+                if group['weight_decay'] != 0:\n+                    p.data.add_(-group['weight_decay'], p.data)", "path": "torch/optim/adamw.py", "position": 82, "original_position": 82, "commit_id": "b2640e5793b6461603d032b1d804b4a1d71410a9", "original_commit_id": "b2640e5793b6461603d032b1d804b4a1d71410a9", "user": {"login": "colllin", "id": 185923, "node_id": "MDQ6VXNlcjE4NTkyMw==", "avatar_url": "https://avatars1.githubusercontent.com/u/185923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colllin", "html_url": "https://github.com/colllin", "followers_url": "https://api.github.com/users/colllin/followers", "following_url": "https://api.github.com/users/colllin/following{/other_user}", "gists_url": "https://api.github.com/users/colllin/gists{/gist_id}", "starred_url": "https://api.github.com/users/colllin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colllin/subscriptions", "organizations_url": "https://api.github.com/users/colllin/orgs", "repos_url": "https://api.github.com/users/colllin/repos", "events_url": "https://api.github.com/users/colllin/events{/privacy}", "received_events_url": "https://api.github.com/users/colllin/received_events", "type": "User", "site_admin": false}, "body": "There might be a mistake here: See that `p.data` in L82 was already updated based on the gradient in L79.  Thus it appears that the weight decay is being multiplied by the weights *after* the gradient-based update, and instead it should be multiplied by the original weights (before the gradient-based update).  I think this can be fixed by simply moving L81-82 above L79, since the `step_size`, `exp_avg`, and `denom` are already computed.  This will compute the weight decay separately from the gradient-based update. \r\n\r\nI found this based on the AdamW description in http://www.fast.ai/2018/07/02/adam-weight-decay/ .  They also multiply the weight decay by the learning rate (`-wd * lr * w`), and ~I applied both of these updates~ (see **update** below) to your AdamW implementation here: https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3 . ~I'm not sure whether you intentionally computed `-wd * w` instead of `-wd * lr * w` \u2014 if so, could you explain why you decided that?~\r\n\r\n**Update**: I checked the paper's provided implementation ([adam.lua#L70](https://github.com/loshchil/AdamW-and-SGDW/blob/7e882457e25283a4e3bd26711dc47385680cefcf/UPDATETORCHFILES/adam.lua#L70)) and found that they do copy the params so that they apply the decay to the original weights rather than decaying them after the gradient-based update.  I also saw that they do not multiply the weight decay by the learning rate, which matches your implementation.  I have updated my implementation to reflect this.", "created_at": "2018-07-31T22:00:43Z", "updated_at": "2018-11-23T15:48:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/3740#discussion_r206697735", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3740", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206697735"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3740#discussion_r206697735"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3740"}}, "body_html": "<p>There might be a mistake here: See that <code>p.data</code> in L82 was already updated based on the gradient in L79.  Thus it appears that the weight decay is being multiplied by the weights <em>after</em> the gradient-based update, and instead it should be multiplied by the original weights (before the gradient-based update).  I think this can be fixed by simply moving L81-82 above L79, since the <code>step_size</code>, <code>exp_avg</code>, and <code>denom</code> are already computed.  This will compute the weight decay separately from the gradient-based update.</p>\n<p>I found this based on the AdamW description in <a href=\"http://www.fast.ai/2018/07/02/adam-weight-decay/\" rel=\"nofollow\">http://www.fast.ai/2018/07/02/adam-weight-decay/</a> .  They also multiply the weight decay by the learning rate (<code>-wd * lr * w</code>), and <del>I applied both of these updates</del> (see <strong>update</strong> below) to your AdamW implementation here: <a href=\"https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3\">https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3</a> . <del>I'm not sure whether you intentionally computed <code>-wd * w</code> instead of <code>-wd * lr * w</code> \u2014 if so, could you explain why you decided that?</del></p>\n<p><strong>Update</strong>: I checked the paper's provided implementation (<a href=\"https://github.com/loshchil/AdamW-and-SGDW/blob/7e882457e25283a4e3bd26711dc47385680cefcf/UPDATETORCHFILES/adam.lua#L70\">adam.lua#L70</a>) and found that they do copy the params so that they apply the decay to the original weights rather than decaying them after the gradient-based update.  I also saw that they do not multiply the weight decay by the learning rate, which matches your implementation.  I have updated my implementation to reflect this.</p>", "body_text": "There might be a mistake here: See that p.data in L82 was already updated based on the gradient in L79.  Thus it appears that the weight decay is being multiplied by the weights after the gradient-based update, and instead it should be multiplied by the original weights (before the gradient-based update).  I think this can be fixed by simply moving L81-82 above L79, since the step_size, exp_avg, and denom are already computed.  This will compute the weight decay separately from the gradient-based update.\nI found this based on the AdamW description in http://www.fast.ai/2018/07/02/adam-weight-decay/ .  They also multiply the weight decay by the learning rate (-wd * lr * w), and I applied both of these updates (see update below) to your AdamW implementation here: https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3 . I'm not sure whether you intentionally computed -wd * w instead of -wd * lr * w \u2014 if so, could you explain why you decided that?\nUpdate: I checked the paper's provided implementation (adam.lua#L70) and found that they do copy the params so that they apply the decay to the original weights rather than decaying them after the gradient-based update.  I also saw that they do not multiply the weight decay by the learning rate, which matches your implementation.  I have updated my implementation to reflect this."}