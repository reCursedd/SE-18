{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/381659944", "html_url": "https://github.com/pytorch/pytorch/issues/6622#issuecomment-381659944", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6622", "id": 381659944, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTY1OTk0NA==", "user": {"login": "Lan1991Xu", "id": 22507798, "node_id": "MDQ6VXNlcjIyNTA3Nzk4", "avatar_url": "https://avatars0.githubusercontent.com/u/22507798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lan1991Xu", "html_url": "https://github.com/Lan1991Xu", "followers_url": "https://api.github.com/users/Lan1991Xu/followers", "following_url": "https://api.github.com/users/Lan1991Xu/following{/other_user}", "gists_url": "https://api.github.com/users/Lan1991Xu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lan1991Xu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lan1991Xu/subscriptions", "organizations_url": "https://api.github.com/users/Lan1991Xu/orgs", "repos_url": "https://api.github.com/users/Lan1991Xu/repos", "events_url": "https://api.github.com/users/Lan1991Xu/events{/privacy}", "received_events_url": "https://api.github.com/users/Lan1991Xu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-16T16:09:56Z", "updated_at": "2018-04-16T16:09:56Z", "author_association": "NONE", "body_html": "<p>Actually, I re-implementation KL loss with my side without dividing the dimensions. It works in my case, the really confused me is some other guy using the KL loss (default implement by the Pytorch) it seems work.  But, Actually, I am not understand this  situation. Compared with the orginal version,  if there is two loss(Ep: KL(lossA) , and Cross entropy( loss B). The totall loss  loss A+ lossB. However when you divide the dimensions of KL, it comes to lossA/b +lossB. b is the number of class  in, which will make  large difference in totally loss.</p>", "body_text": "Actually, I re-implementation KL loss with my side without dividing the dimensions. It works in my case, the really confused me is some other guy using the KL loss (default implement by the Pytorch) it seems work.  But, Actually, I am not understand this  situation. Compared with the orginal version,  if there is two loss(Ep: KL(lossA) , and Cross entropy( loss B). The totall loss  loss A+ lossB. However when you divide the dimensions of KL, it comes to lossA/b +lossB. b is the number of class  in, which will make  large difference in totally loss.", "body": "Actually, I re-implementation KL loss with my side without dividing the dimensions. It works in my case, the really confused me is some other guy using the KL loss (default implement by the Pytorch) it seems work.  But, Actually, I am not understand this  situation. Compared with the orginal version,  if there is two loss(Ep: KL(lossA) , and Cross entropy( loss B). The totall loss  loss A+ lossB. However when you divide the dimensions of KL, it comes to lossA/b +lossB. b is the number of class  in, which will make  large difference in totally loss."}