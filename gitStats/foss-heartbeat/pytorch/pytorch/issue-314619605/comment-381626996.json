{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/381626996", "html_url": "https://github.com/pytorch/pytorch/issues/6622#issuecomment-381626996", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6622", "id": 381626996, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTYyNjk5Ng==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-16T14:52:16Z", "updated_at": "2018-04-16T14:52:16Z", "author_association": "MEMBER", "body_html": "<p>KL distance is usually defined as an operator on a pair of distributions, while when you compute a loss you really have multiple distances, and you need to decide how to combine them. PyTorch chooses to average them because this makes gradient magnitudes independent of the batch size, but summation is another valid option. I think our implementation is correct, as you can always switch to the other option.</p>", "body_text": "KL distance is usually defined as an operator on a pair of distributions, while when you compute a loss you really have multiple distances, and you need to decide how to combine them. PyTorch chooses to average them because this makes gradient magnitudes independent of the batch size, but summation is another valid option. I think our implementation is correct, as you can always switch to the other option.", "body": "KL distance is usually defined as an operator on a pair of distributions, while when you compute a loss you really have multiple distances, and you need to decide how to combine them. PyTorch chooses to average them because this makes gradient magnitudes independent of the batch size, but summation is another valid option. I think our implementation is correct, as you can always switch to the other option."}