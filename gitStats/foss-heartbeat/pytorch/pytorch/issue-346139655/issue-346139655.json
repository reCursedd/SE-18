{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10066", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10066/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10066/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10066/events", "html_url": "https://github.com/pytorch/pytorch/pull/10066", "id": 346139655, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA1MDc1Mzk3", "number": 10066, "title": "fixed a newly introduced regression in softmax", "user": {"login": "ktarplee", "id": 3164343, "node_id": "MDQ6VXNlcjMxNjQzNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3164343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ktarplee", "html_url": "https://github.com/ktarplee", "followers_url": "https://api.github.com/users/ktarplee/followers", "following_url": "https://api.github.com/users/ktarplee/following{/other_user}", "gists_url": "https://api.github.com/users/ktarplee/gists{/gist_id}", "starred_url": "https://api.github.com/users/ktarplee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ktarplee/subscriptions", "organizations_url": "https://api.github.com/users/ktarplee/orgs", "repos_url": "https://api.github.com/users/ktarplee/repos", "events_url": "https://api.github.com/users/ktarplee/events{/privacy}", "received_events_url": "https://api.github.com/users/ktarplee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-07-31T11:05:38Z", "updated_at": "2018-08-01T02:29:35Z", "closed_at": "2018-08-01T02:29:35Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10066", "html_url": "https://github.com/pytorch/pytorch/pull/10066", "diff_url": "https://github.com/pytorch/pytorch/pull/10066.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10066.patch"}, "body_html": "<p>There is a regression in softmin in 0.4.1 that was not present in 0.4.0.  The behavior of softmin(x) should match softmax(-x) however instead it is implemented (in v0.4.1) as -softmax(x).  These are not the same.  The fix is trivial because the bug is due to operator precedence.</p>\n<p>This is a major regression that broke my training.  I'm not sure how a unit test did not catch this.</p>\n<pre><code>x = torch.tensor([1, 2, 3.5, 4])\nprint(F.softmin(x, dim=0)) # this has the wrong output in 0.4.1 but correct in 0.4.0\nprint(F.softmax(-x, dim=0)) # this is what softmax should be\nprint(F.softmax(x, dim=0))\nprint(-F.softmax(x, dim=0)) # this is how softmax is implemented incorrectly\n</code></pre>\n<p>In 0.4.1 this produces<br>\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])<br>\ntensor([0.6668, 0.2453, 0.0547, 0.0332])<br>\ntensor([0.0278, 0.0755, 0.3385, 0.5581])<br>\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])</p>\n<p>In 0.4.0 this produces the correct values<br>\ntensor([ 0.6668,  0.2453,  0.0547,  0.0332])<br>\ntensor([ 0.6668,  0.2453,  0.0547,  0.0332])<br>\ntensor([ 0.0278,  0.0755,  0.3385,  0.5581])<br>\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])</p>", "body_text": "There is a regression in softmin in 0.4.1 that was not present in 0.4.0.  The behavior of softmin(x) should match softmax(-x) however instead it is implemented (in v0.4.1) as -softmax(x).  These are not the same.  The fix is trivial because the bug is due to operator precedence.\nThis is a major regression that broke my training.  I'm not sure how a unit test did not catch this.\nx = torch.tensor([1, 2, 3.5, 4])\nprint(F.softmin(x, dim=0)) # this has the wrong output in 0.4.1 but correct in 0.4.0\nprint(F.softmax(-x, dim=0)) # this is what softmax should be\nprint(F.softmax(x, dim=0))\nprint(-F.softmax(x, dim=0)) # this is how softmax is implemented incorrectly\n\nIn 0.4.1 this produces\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])\ntensor([0.6668, 0.2453, 0.0547, 0.0332])\ntensor([0.0278, 0.0755, 0.3385, 0.5581])\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])\nIn 0.4.0 this produces the correct values\ntensor([ 0.6668,  0.2453,  0.0547,  0.0332])\ntensor([ 0.6668,  0.2453,  0.0547,  0.0332])\ntensor([ 0.0278,  0.0755,  0.3385,  0.5581])\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])", "body": "There is a regression in softmin in 0.4.1 that was not present in 0.4.0.  The behavior of softmin(x) should match softmax(-x) however instead it is implemented (in v0.4.1) as -softmax(x).  These are not the same.  The fix is trivial because the bug is due to operator precedence.\r\n\r\nThis is a major regression that broke my training.  I'm not sure how a unit test did not catch this.\r\n\r\n```\r\nx = torch.tensor([1, 2, 3.5, 4])\r\nprint(F.softmin(x, dim=0)) # this has the wrong output in 0.4.1 but correct in 0.4.0\r\nprint(F.softmax(-x, dim=0)) # this is what softmax should be\r\nprint(F.softmax(x, dim=0))\r\nprint(-F.softmax(x, dim=0)) # this is how softmax is implemented incorrectly\r\n```\r\nIn 0.4.1 this produces\r\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])\r\ntensor([0.6668, 0.2453, 0.0547, 0.0332])\r\ntensor([0.0278, 0.0755, 0.3385, 0.5581])\r\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])\r\n\r\nIn 0.4.0 this produces the correct values\r\ntensor([ 0.6668,  0.2453,  0.0547,  0.0332])\r\ntensor([ 0.6668,  0.2453,  0.0547,  0.0332])\r\ntensor([ 0.0278,  0.0755,  0.3385,  0.5581])\r\ntensor([-0.0278, -0.0755, -0.3385, -0.5581])\r\n"}