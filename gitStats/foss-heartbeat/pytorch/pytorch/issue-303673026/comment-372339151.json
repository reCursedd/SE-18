{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/372339151", "html_url": "https://github.com/pytorch/pytorch/issues/5650#issuecomment-372339151", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5650", "id": 372339151, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjMzOTE1MQ==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-12T14:57:39Z", "updated_at": "2018-03-12T14:57:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I think we do handle None <code>.grad</code> attributes correctly in <code>clip_grad_norm</code>: <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py#L18\">https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py#L18</a>.</p>\n<p>The traceback <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1892917\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/monajalal\">@monajalal</a> posted implies that the code uses its own <code>clip_gradient</code> method:</p>\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 343, in &lt;module&gt;\n    exp.train()\n  File \"train.py\", line 326, in train\n    loss = self.train_batch(i)\n  File \"train.py\", line 303, in train_batch\n    coeff = clip_gradient(self.mdl, self.args.clip_norm)\n  File \"train.py\", line 35, in clip_gradient\n    modulenorm = p.grad.data.norm()\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1892917\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/monajalal\">@monajalal</a> If you replace <code>clip_gradient</code> with some usage of <a href=\"http://pytorch.org/docs/master/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm\" rel=\"nofollow\"><code>torch.nn.utils.clip_grad_norm</code></a> this particular error should go away.</p>", "body_text": "@apaszke I think we do handle None .grad attributes correctly in clip_grad_norm: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py#L18.\nThe traceback @monajalal posted implies that the code uses its own clip_gradient method:\nTraceback (most recent call last):\n  File \"train.py\", line 343, in <module>\n    exp.train()\n  File \"train.py\", line 326, in train\n    loss = self.train_batch(i)\n  File \"train.py\", line 303, in train_batch\n    coeff = clip_gradient(self.mdl, self.args.clip_norm)\n  File \"train.py\", line 35, in clip_gradient\n    modulenorm = p.grad.data.norm()\n\n@monajalal If you replace clip_gradient with some usage of torch.nn.utils.clip_grad_norm this particular error should go away.", "body": "@apaszke I think we do handle None `.grad` attributes correctly in `clip_grad_norm`: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py#L18.\r\n\r\nThe traceback @monajalal posted implies that the code uses its own `clip_gradient` method:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 343, in <module>\r\n    exp.train()\r\n  File \"train.py\", line 326, in train\r\n    loss = self.train_batch(i)\r\n  File \"train.py\", line 303, in train_batch\r\n    coeff = clip_gradient(self.mdl, self.args.clip_norm)\r\n  File \"train.py\", line 35, in clip_gradient\r\n    modulenorm = p.grad.data.norm()\r\n```\r\n@monajalal If you replace `clip_gradient` with some usage of [`torch.nn.utils.clip_grad_norm`](http://pytorch.org/docs/master/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm) this particular error should go away.\r\n"}