{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/180355031", "pull_request_review_id": 110759853, "id": 180355031, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDM1NTAzMQ==", "diff_hunk": "@@ -168,6 +168,25 @@ def _init_global_test(self):\n         rank = dist.get_rank()\n         return (group, group_id, rank)\n \n+    # HELPER FOR MULTIGPU TESTS\n+    def _init_multigpu_helper(self):\n+        \"\"\"Multigpu tests are designed to simulate the multi nodes with multi\n+        GPUs on each node. Nccl backend requires equal #GPUs in each process.\n+        On a single node, all visible GPUs are evenly\n+        divided to subsets, each process only uses a subset.\n+        \"\"\"\n+        nGPUs = torch.cuda.device_count()\n+        world_size = dist.get_world_size()\n+        visible_devices = range(nGPUs)\n+\n+        if BACKEND == 'nccl':\n+            apply_hack_for_nccl()\n+\n+        nGPUs_per_process = nGPUs // world_size\n+        rank_to_GPU = {i: visible_devices[i * nGPUs_per_process: (i + 1) * nGPUs_per_process]\n+                       for i in list(range(world_size))}", "path": "test/test_distributed.py", "position": null, "original_position": 50, "commit_id": "2819b8630970cbe1eb645b974fbc0fd937d52f06", "original_commit_id": "a5578f700b5584614d0148b61ab7f985ba3f829e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "That doesn't solve the issue, because the result of `visible_devices[...]` will be a `range` object. You need to do `rank_to_gpu = [list(visible_devices[...]) for i in range(world_size)]`", "created_at": "2018-04-10T09:37:19Z", "updated_at": "2018-11-23T15:42:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/6337#discussion_r180355031", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6337", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/180355031"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6337#discussion_r180355031"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6337"}}, "body_html": "<p>That doesn't solve the issue, because the result of <code>visible_devices[...]</code> will be a <code>range</code> object. You need to do <code>rank_to_gpu = [list(visible_devices[...]) for i in range(world_size)]</code></p>", "body_text": "That doesn't solve the issue, because the result of visible_devices[...] will be a range object. You need to do rank_to_gpu = [list(visible_devices[...]) for i in range(world_size)]"}