{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233315574", "pull_request_review_id": 174698884, "id": 233315574, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzMxNTU3NA==", "diff_hunk": "@@ -899,6 +848,140 @@ void dump(const onnx::ModelProto& model, std::ostream& stream, size_t indent) {\n   }\n   stream << idt(indent) << \"}\\n\";\n }\n+\n+class ScriptModuleSerializer final {\n+\n+ public:\n+  ScriptModuleSerializer(const std::string& filename) :\n+    ofs_(filename, std::ofstream::out | std::ofstream::trunc | std::ofstream::binary),\n+    writer_(&ofs_) {\n+      // TODO appropriate support for mmap, right now we still use stream writer\n+  }\n+  ScriptModuleSerializer(std::ostream* ofs) : ofs_(), writer_(ofs) {}\n+  void serialize(const script::Module& module) {\n+    torch::ModelDef model_def;\n+    convertToModel(module, &model_def);\n+    std::string output;\n+    // NB: cannot use MessageToJsonString, since fbcode's protobuf is too old\n+    // be consistent with MessageToJsonString\n+    std::string url_prefix = \"type.googleapis.com\";\n+    std::unique_ptr<::google::protobuf::util::TypeResolver> resolver(\n+        ::google::protobuf::util::NewTypeResolverForDescriptorPool(\n+          url_prefix, model_def.GetDescriptor()->file()->pool()));\n+    ::google::protobuf::util::Status convert_result =\n+      ::google::protobuf::util::BinaryToJsonString(resolver.get(),\n+          url_prefix + \"/\" + model_def.GetDescriptor()->full_name(),\n+          model_def.SerializeAsString(), &output);\n+    if (!convert_result.ok()) {\n+      std::stringstream ss;\n+      ss << convert_result;\n+      AT_ERROR(ss.str());\n+    }\n+    auto record_id = writer_.writeRecord(output.data(), output.size());\n+    writer_.writeEndOfFile();\n+\n+  }\n+\n+ private:\n+  void convertToModel(const script::Module& module,\n+      torch::ModelDef* model_def) {\n+    model_def->set_name(\"script-model\");\n+    model_def->set_producer_name(\"pytorch\");\n+    model_def->set_producer_version(\"1.0\"); // TODO: set the producer version using appropriate function call\n+    model_def->set_proto_version(torch::ProtoVersion::PROTO_VERSION_NEWEST);\n+    std::string main_module_name = \"\";\n+    collectParamsInfo(module, main_module_name);\n+    convertModule(module, main_module_name, model_def->mutable_main_module());\n+  }\n+  void collectParamsInfo(const script::Module& module, const std::string& prefix) {\n+    for (const auto& elem: module.get_parameters()) {\n+      const script::NamedParameter& param = elem.value();\n+      parameterMap_[param.slot()] = prefix + param.name;\n+    }\n+    for (const auto& elem: module.get_modules()) {\n+      collectParamsInfo(*elem->module, prefix + elem.key() + \".\");\n+    }\n+  }\n+  void convertModule(const script::Module& module, const std::string& name,\n+      torch::ModuleDef* module_def) {\n+    module_def->set_name(name);\n+    for (const auto& elem: module.get_parameters()) {\n+      torch::ParameterDef* param_def = module_def->add_parameters();\n+      convertParameter(elem.value(), param_def);\n+    }\n+    for (auto& elem: module.get_methods()) {\n+      torch::MethodDef* method_def = module_def->add_methods();\n+      convertMethod(*elem.value(), method_def);\n+    }\n+    for (const auto& elem: module.get_modules()) {\n+      torch::ModuleDef* sub_def = module_def->add_submodules();\n+      convertModule(*elem->module, elem.key(), sub_def);\n+    }\n+  }\n+  void convertParameter(const script::NamedParameter& param,\n+      torch::ParameterDef* param_def) {\n+    param_def->set_name(param.name);\n+    param_def->set_is_buffer(param.is_buffer);\n+    param_def->set_require_gradient(param.slot()->requires_grad());\n+    convertTensor(*(param.slot()), param_def->mutable_tensor());\n+  }\n+  void convertTensor(const at::Tensor& tensor,\n+      caffe2::TensorProto* tensor_proto) {\n+    for (auto d : tensor.sizes()) {\n+      tensor_proto->add_dims(d);\n+    }\n+    tensor_proto->set_data_type(\n+        caffe2::TypeMetaToDataType(at::scalarTypeToTypeMeta(\n+            tensor.type().scalarType())));\n+    tensor_proto->set_storage_type(caffe2::TensorProto_StorageType_EXTERNAL);\n+    caffe2::ExternalDataProto* external_data = tensor_proto->mutable_external_data();", "path": "torch/csrc/jit/export.cpp", "position": 330, "original_position": 330, "commit_id": "e186c1feb925618f88a7471a9b0d6b51ed454a08", "original_commit_id": "e186c1feb925618f88a7471a9b0d6b51ed454a08", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "So right now TensorProto also support putting the data inline. If the data is inline - there's not need to put strides as the storage is by definition not shared.", "created_at": "2018-11-14T05:07:11Z", "updated_at": "2018-11-23T15:54:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/13736#discussion_r233315574", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13736", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233315574"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13736#discussion_r233315574"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13736"}}, "body_html": "<p>So right now TensorProto also support putting the data inline. If the data is inline - there's not need to put strides as the storage is by definition not shared.</p>", "body_text": "So right now TensorProto also support putting the data inline. If the data is inline - there's not need to put strides as the storage is by definition not shared.", "in_reply_to_id": 233171922}