{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232413854", "pull_request_review_id": 173602893, "id": 232413854, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjQxMzg1NA==", "diff_hunk": "@@ -366,22 +344,151 @@ ModuleDecoder::ModuleDecoder(\n   }\n }\n \n+at::ScalarType tensorProtoTypeToATenType(caffe2::TensorProto_DataType data_type) {\n+  switch(data_type) {\n+    // NB: handle BOOL\n+    case caffe2::TensorProto_DataType_UINT8:\n+      return at::kByte;\n+    case caffe2::TensorProto_DataType_INT8:\n+      return at::kChar;\n+    case caffe2::TensorProto_DataType_INT16:\n+      return at::kShort;\n+    case caffe2::TensorProto_DataType_INT32:\n+      return at::kInt;\n+    case caffe2::TensorProto_DataType_INT64:\n+      return at::kLong;\n+    case caffe2::TensorProto_DataType_FLOAT16:\n+      return at::kHalf;\n+    case caffe2::TensorProto_DataType_FLOAT:\n+      return at::kFloat;\n+    case caffe2::TensorProto_DataType_DOUBLE:\n+      return at::kDouble;\n+    default:\n+      AT_ERROR(\"Unsupported TensorProto data type\");\n+  }\n+}\n+\n+class ScriptModuleDeserializer final {\n+ public:\n+  ScriptModuleDeserializer(const std::string& filename) :\n+    ifs_(filename, std::ifstream::in), reader_(&ifs_) {\n+      // TODO appropriate support for mmap, right now still use stream reader\n+      //std::cout << \"filename: \" << filename << std::endl;\n+  }\n+  ScriptModuleDeserializer(std::istream* is) : ifs_(), reader_(is) {}\n+  void deserialize(ModuleLookup module_lookup) {\n+    torch::ModelDef model_def;\n+    at::DataPtr data_ptr;\n+    size_t data_size;\n+    std::tie(data_ptr, data_size) = reader_.getLastRecord();\n+    AT_ASSERTM(model_def.ParseFromArray(data_ptr.get(), data_size),\n+        \"parse metadata (i.e., ModelDef) failed.\");;\n+    //std::cout << \"loaded model_def: \" << model_def.DebugString() << std::endl;\n+    moduleLookup_ = module_lookup;\n+    const auto& module_def = model_def.main_module();\n+    collectParamsInfo(module_def, module_def.name());\n+    std::shared_ptr<script::Module> module = moduleLookup_(moduleStack_);\n+    convertModule(module_def, module.get());\n+  }\n+ private:\n+  void collectParamsInfo(const torch::ModuleDef& module_def, const std::string& prefix) {\n+    for (int i = 0; i < module_def.parameters_size(); ++i) {\n+      std::shared_ptr<script::Module> module = moduleLookup_(moduleStack_);\n+      const torch::ParameterDef& param_def = module_def.parameters(i);\n+      at::Tensor tensor = createTensor(param_def);\n+      autograd::Variable variable = autograd::make_variable(tensor,\n+          param_def.require_gradient());\n+      module->register_parameter(param_def.name(), variable, param_def.is_buffer());\n+      parameterMap_[prefix + param_def.name()] = module->parameter_slot(param_def.name());\n+    }\n+    for (int i = 0; i < module_def.submodules_size(); ++i) {\n+      const torch::ModuleDef& sub_def = module_def.submodules(i);\n+      moduleStack_.push_back(sub_def.name());\n+      collectParamsInfo(sub_def, prefix + sub_def.name() + \".\");\n+      moduleStack_.pop_back();\n+    }\n+  }\n+  void convertModule(const torch::ModuleDef& module_def,\n+      script::Module* module) {\n+    //std::unordered_map<std::string, at::Tensor*> param_map;\n+    for (int i = 0; i < module_def.methods_size(); ++i ) {\n+      const torch::MethodDef& method_def = module_def.methods(i);\n+      // TODO read unhacked torch script, right now it's serialized onnx proto\n+      ::ONNX_NAMESPACE::ModelProto method_proto;\n+      AT_ASSERTM(method_proto.ParseFromString(method_def.onnx_proto()),\n+          \"cannot parse method proto (i.e., hacked onnx proto)\");\n+      MethodDecoder decoder(method_proto, parameterMap_, module,\n+          &storageMap_, &reader_);\n+      (void)decoder;\n+    }\n+    for (int i = 0; i < module_def.submodules_size(); ++i) {\n+      const torch::ModuleDef& sub_def = module_def.submodules(i);\n+      moduleStack_.push_back(sub_def.name());\n+      std::shared_ptr<script::Module> sub = moduleLookup_(moduleStack_);\n+      convertModule(sub_def, sub.get());\n+      moduleStack_.pop_back();\n+    }\n+  }\n+  at::Tensor createTensor(const torch::ParameterDef& param_def) {\n+    const caffe2::TensorProto& tensor_proto = param_def.tensor();", "path": "torch/csrc/jit/import.cpp", "position": null, "original_position": 304, "commit_id": "e186c1feb925618f88a7471a9b0d6b51ed454a08", "original_commit_id": "ed6ab9b76f080f9c8643a5e84bd4de1a820d77f6", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "do it outside and pass only TensorProto?", "created_at": "2018-11-09T22:35:43Z", "updated_at": "2018-11-23T15:54:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/13736#discussion_r232413854", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13736", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232413854"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13736#discussion_r232413854"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13736"}}, "body_html": "<p>do it outside and pass only TensorProto?</p>", "body_text": "do it outside and pass only TensorProto?"}