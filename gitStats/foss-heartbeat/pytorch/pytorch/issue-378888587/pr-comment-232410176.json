{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232410176", "pull_request_review_id": 173602893, "id": 232410176, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjQxMDE3Ng==", "diff_hunk": "@@ -883,6 +826,162 @@ void dump(const onnx::ModelProto& model, std::ostream& stream, size_t indent) {\n   }\n   stream << idt(indent) << \"}\\n\";\n }\n+\n+// aten type ==> caffe2::TensorProto type\n+caffe2::TensorProto_DataType atenTypeToTensorProtoType(at::ScalarType aten_type) {\n+  switch(aten_type) {\n+    case at::kDouble:\n+      return caffe2::TensorProto_DataType_DOUBLE;\n+    case at::kFloat:\n+      return caffe2::TensorProto_DataType_FLOAT;\n+    case at::kHalf:\n+      return caffe2::TensorProto_DataType_FLOAT16;\n+    case at::kByte:\n+      return caffe2::TensorProto_DataType_UINT8;\n+    case at::kChar:\n+      return caffe2::TensorProto_DataType_INT8;\n+    case at::kShort:\n+      return caffe2::TensorProto_DataType_INT16;\n+    case at::kInt:\n+      return caffe2::TensorProto_DataType_INT32;\n+    case at::kLong:\n+      return caffe2::TensorProto_DataType_INT64;\n+    default:\n+      AT_ERROR(\"unexpected aten scalar type\");\n+  }\n+}\n+\n+class ScriptModuleSerializer final {\n+\n+ public:\n+  ScriptModuleSerializer(const std::string& filename) :\n+    //ofs_(filename, std::ofstream::out | std::ofstream::trunc | std::ofstream::binary),\n+    ofs_(filename, std::ofstream::out),\n+    writer_(&ofs_) {\n+      //std::cout << \"filename: \" << filename << std::endl;\n+      // TODO appropriate support for mmap, right now we still use stream writer\n+  }\n+  ScriptModuleSerializer(std::ostream* ofs) : ofs_(), writer_(ofs) {}\n+  void serialize(const script::Module& module) {\n+    torch::ModelDef model_def;\n+    convertToModel(module, &model_def);\n+    std::string output;\n+    model_def.SerializeToString(&output);\n+    auto record_id = writer_.writeRecord(output.data(), output.size());\n+    //std::cout << \"===> last record_id: \" << record_id << std::endl;\n+    //std::cout << \"===> serialized proto size: \" << output.size() << std::endl;\n+    //std::cout << model_def.DebugString() << std::endl;\n+    writer_.writeEndOfFile();\n+\n+  }\n+\n+ private:\n+  void convertToModel(const script::Module& module,\n+      torch::ModelDef* model_def) {\n+    model_def->set_name(\"script-model\");\n+    model_def->set_producer_name(\"pytorch\");\n+    model_def->set_producer_version(\"1.0\"); // TODO: set the producer version using appropriate function call\n+    // TODO set proto version\n+    std::string main_module_name = \"\";\n+    collectParamsInfo(module, main_module_name);\n+    convertModule(module, main_module_name, model_def->mutable_main_module());\n+  }\n+  void collectParamsInfo(const script::Module& module, const std::string& prefix) {\n+    for (const auto& elem: module.get_parameters()) {\n+      const script::NamedParameter& param = elem.value();\n+      parameterMap_[param.slot()] = prefix + param.name;\n+    }\n+    for (const auto& elem: module.get_modules()) {\n+      collectParamsInfo(*elem->module, prefix + elem.key() + \".\");\n+    }\n+  }\n+  void convertModule(const script::Module& module, const std::string& name,\n+      torch::ModuleDef* module_def) {\n+    //std::cout << \"module.name: \" << name << std::endl;\n+    module_def->set_name(name);\n+    for (const auto& elem: module.get_parameters()) {\n+      torch::ParameterDef* param_def = module_def->add_parameters();\n+      convertParameter(elem.value(), param_def);\n+    }\n+    for (auto& elem: module.get_methods()) {\n+      torch::MethodDef* method_def = module_def->add_methods();\n+      convertMethod(*elem.value(), method_def);\n+    }\n+    for (const auto& elem: module.get_modules()) {\n+      torch::ModuleDef* sub_def = module_def->add_submodules();\n+      convertModule(*elem->module, elem.key(), sub_def);\n+    }\n+  }\n+  void convertParameter(const script::NamedParameter& param,\n+      torch::ParameterDef* param_def) {\n+    //std::cout << \"param.name: \" << param.name << \",\" << param.slot() << std::endl;\n+    param_def->set_name(param.name);\n+    param_def->set_is_buffer(param.is_buffer);\n+    param_def->set_require_gradient(param.slot()->requires_grad());\n+    //parameterMap_[param.slot()] = param.name;\n+    convertTensor(*(param.slot()), param_def->mutable_tensor());\n+  }\n+  void convertTensor(const at::Tensor& tensor,\n+      caffe2::TensorProto* tensor_proto) {\n+    for (auto d : tensor.sizes()) {\n+      tensor_proto->add_dims(d);\n+    }\n+    tensor_proto->set_data_type(\n+        atenTypeToTensorProtoType(tensor.type().scalarType()));\n+    tensor_proto->set_storage_type(caffe2::TensorProto_StorageType_EXTERNAL);\n+    caffe2::ExternalDataProto* external_data = tensor_proto->mutable_external_data();\n+    //std::cout << \"strides: \" << tensor.strides() << std::endl;\n+    for (auto s : tensor.strides()) {\n+      external_data->add_strides(s);\n+    }\n+    external_data->set_offset(tensor.storage_offset());\n+    uint64_t record_size = tensor.type().elementSizeInBytes() * tensor.storage().size();\n+    external_data->set_record_size(record_size);\n+    auto* key = tensor.storage().unsafeGetStorageImpl();\n+    auto it = storageMap_.find(key);\n+    if (it == storageMap_.end()) {\n+      // TODO HIP support\n+      uint64_t record_id;\n+      if (tensor.storage().device_type() == at::DeviceType::CUDA) {\n+        // NB: This new tensor is created to support cuda tensors.\n+        // Storages can be mutated when converting tensors from cuda to cpu,\n+        // and we need a cpu tensor to copy data from.\n+        at::Tensor t = at::getType(tensor)._th_tensor(\n+            tensor.storage(),\n+            /* storageOffset = */ 0,\n+            /* size = */ { static_cast<int64_t>(tensor.storage().size()) },\n+            /* stride = */ { 1 }).cpu();\n+        AT_ASSERT(t.type().elementSizeInBytes() * t.storage().size() == record_size);\n+        record_id = writer_.writeRecord(t.storage().data(),\n+            t.type().elementSizeInBytes() * t.storage().size());\n+        //std::cout << \"===> record_id: \" << record_id << std::endl;\n+      } else {\n+        record_id = writer_.writeRecord(tensor.storage().data(),\n+            record_size);\n+      }\n+      external_data->set_record_id(caffe2::to_string(record_id));\n+      storageMap_[key] = record_id;\n+    } else {\n+      external_data->set_record_id(caffe2::to_string(it->second));\n+    }\n+    // TODO handle device case, set the device_detail and load to CUDA device\n+  }\n+  void convertMethod(script::Method& method, torch::MethodDef* method_def) {\n+    // TODO encode the real torch script instead of ModelProto\n+    ::ONNX_NAMESPACE::ModelProto model_proto;\n+    model_proto.set_doc_string(\"THIS PROTO IS NOT STANDARD ONNX\");\n+    MethodEncoder encoder(method, model_proto.mutable_graph()->add_node(),\n+        &storageMap_, &parameterMap_, &writer_);\n+    std::string serialized_proto;\n+    model_proto.SerializeToString(&serialized_proto);", "path": "torch/csrc/jit/export.cpp", "position": null, "original_position": 368, "commit_id": "e186c1feb925618f88a7471a9b0d6b51ed454a08", "original_commit_id": "ed6ab9b76f080f9c8643a5e84bd4de1a820d77f6", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "check output value?", "created_at": "2018-11-09T22:17:37Z", "updated_at": "2018-11-23T15:54:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/13736#discussion_r232410176", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13736", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232410176"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13736#discussion_r232410176"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13736"}}, "body_html": "<p>check output value?</p>", "body_text": "check output value?"}