{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/188135407", "pull_request_review_id": 120052757, "id": 188135407, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4ODEzNTQwNw==", "diff_hunk": "@@ -0,0 +1,282 @@\n+#include \"ATen/native/cpu/SoftmaxKernel.h\"\n+\n+#include <algorithm>\n+#include <iterator>\n+#include <numeric>\n+\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/Parallel.h\"\n+#include \"ATen/cpu/vec256/functional.h\"\n+#include \"ATen/cpu/vec256/vec256.h\"\n+#include \"ATen/optional.h\"\n+\n+// [Note AVX-SSE transitions] In general we avoid calls into cmath for code\n+// compiled with AVX/AVX2 This is because of SSE-AVX transitions and a bug in\n+// Glibc2.23 See https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280\n+//\n+// On grainsize: The grainsize is chosen to roughly get TBB_GRAIN_SIZE number of\n+// computations per task. Each task works across dim_size elements. 16 should be\n+// a very rough approximation of the number of computations per dim_size element\n+// by counting simple computations (*, +, -) as 1 and exp or log as 4.\n+\n+namespace at { namespace native {\n+namespace {\n+\n+static tbb::affinity_partitioner ap;\n+\n+template <int64_t size>\n+inline int64_t _leftover(int64_t x, int64_t y) {\n+  if (x + size > y)\n+    return y - x;\n+  return size;\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_log_softmax_lastdim(\n+    scalar_t* input_data_base,\n+    scalar_t* output_data_base,\n+    int64_t outer_size,\n+    int64_t dim_size) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  static constexpr int64_t CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size;\n+  int64_t grain_size = internal::TBB_GRAIN_SIZE / (16 * dim_size * CHUNK_SIZE);\n+  if (grain_size < CHUNK_SIZE)\n+    grain_size = CHUNK_SIZE;\n+\n+  tbb::parallel_for(\n+      tbb::blocked_range<int64_t>(0, outer_size, grain_size),\n+      [&](const tbb::blocked_range<int64_t>& r) {\n+        for (int64_t ii = r.begin(); ii < r.end(); ii += CHUNK_SIZE) {\n+          scalar_t tmp_sum_scalar[CHUNK_SIZE];\n+          scalar_t max_input_arr[CHUNK_SIZE];\n+          int64_t loop_end = CHUNK_SIZE;\n+          if (ii + CHUNK_SIZE > r.end())\n+            loop_end = r.end() - ii;\n+          for (int64_t j = 0; j < loop_end; j++) {\n+            int64_t i = ii + j;\n+            scalar_t* input_data = input_data_base + i * dim_size;\n+            max_input_arr[j] = vec256::reduce_all<scalar_t>(\n+                [](Vec& x, Vec& y) { return vec256::max(x, y); },\n+                input_data,\n+                dim_size);\n+          }\n+          for (int64_t j = 0; j < loop_end; j++) {\n+            int64_t i = ii + j;\n+            scalar_t* input_data = input_data_base + i * dim_size;\n+            scalar_t max_input = max_input_arr[j];\n+            tmp_sum_scalar[j] = vec256::map_reduce_all<scalar_t>(\n+                [max_input](Vec x) { return (x - Vec(max_input)).exp(); },\n+                [](Vec x, Vec y) { return x + y; },\n+                input_data,\n+                dim_size);\n+          }\n+          // See [Note AVX-SSE transitions] for why this should call the\n+          // vectorized version (aside from perf improvements).\n+          int64_t j = 0;\n+          for (; j < (loop_end - loop_end % Vec::size); j += Vec::size) {\n+            Vec tmp_sum_scalar_vec = Vec::loadu(max_input_arr + j) +\n+                Vec::loadu(tmp_sum_scalar + j).log();\n+            tmp_sum_scalar_vec.store(tmp_sum_scalar + j);\n+          }\n+          if (loop_end - j > 0) {\n+            Vec tmp_sum_scalar_vec =\n+                Vec::loadu(max_input_arr + j, loop_end - j) +\n+                Vec::loadu(tmp_sum_scalar + j, loop_end - j).log();\n+            tmp_sum_scalar_vec.store(tmp_sum_scalar + j, loop_end - j);\n+          }\n+          for (int64_t j = 0; j < loop_end; j++) {\n+            int64_t i = ii + j;\n+            scalar_t* input_data = input_data_base + i * dim_size;\n+            scalar_t* output_data = output_data_base + i * dim_size;\n+            scalar_t tmp_sum = tmp_sum_scalar[j];\n+            vec256::map(\n+                [tmp_sum](Vec x) { return x - Vec(tmp_sum); },\n+                output_data,\n+                input_data,\n+                dim_size);\n+          }\n+        }\n+      },\n+      ap);\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_softmax_lastdim(\n+    scalar_t* input_data_base,\n+    scalar_t* output_data_base,\n+    int64_t outer_size,\n+    int64_t dim_size) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  int64_t grain_size = internal::TBB_GRAIN_SIZE / (16 * dim_size);\n+  if (grain_size < 1)\n+    grain_size = 1;\n+\n+  tbb::parallel_for(\n+      tbb::blocked_range<int64_t>(0, outer_size, grain_size),\n+      [&](const tbb::blocked_range<int64_t>& r) {\n+        for (int64_t i = r.begin(); i < r.end(); i++) {\n+          scalar_t* input_data = input_data_base + i * dim_size;\n+          scalar_t* output_data = output_data_base + i * dim_size;\n+          scalar_t max_input = vec256::reduce_all<scalar_t>(\n+              [](Vec& x, Vec& y) { return vec256::max(x, y); },\n+              input_data,\n+              dim_size);\n+          vec256::map(\n+              [max_input](Vec x) { return (x - Vec(max_input)).exp(); },\n+              output_data,\n+              input_data,\n+              dim_size);\n+          scalar_t tmp_sum = vec256::reduce_all<scalar_t>(\n+              [](Vec x, Vec y) { return x + y; }, output_data, dim_size);\n+          tmp_sum = 1 / tmp_sum;\n+          vec256::map(\n+              [tmp_sum](Vec x) { return x * Vec(tmp_sum); },\n+              output_data,\n+              output_data,\n+              dim_size);\n+        }\n+      },\n+      ap);\n+}\n+\n+template <typename scalar_t, bool log_softmax>\n+inline void _vec_host_softmax_backward_lastdim(\n+    scalar_t* grad_input_data_base,\n+    scalar_t* grad_data_base,\n+    scalar_t* output_data_base,\n+    int64_t outer_size,\n+    int64_t dim_size) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  int64_t grain_size = internal::TBB_GRAIN_SIZE / (16 * dim_size);\n+  if (grain_size < 1)\n+    grain_size = 1;\n+\n+  tbb::parallel_for(\n+      tbb::blocked_range<int64_t>(0, outer_size, grain_size),\n+      [&](const tbb::blocked_range<int64_t>& r) {\n+        for (int64_t i = r.begin(); i < r.end(); i++) {\n+          scalar_t* grad_input_data = grad_input_data_base + i * dim_size;\n+          scalar_t* grad_data = grad_data_base + i * dim_size;\n+          scalar_t* output_data = output_data_base + i * dim_size;\n+          scalar_t sum;\n+          if (log_softmax) {\n+            sum = vec256::reduce_all<scalar_t>(\n+                [](Vec& x, Vec& y) { return x + y; }, grad_data, dim_size);\n+          } else {\n+            sum = vec256::map2_reduce_all<scalar_t>(\n+                [](Vec x, Vec y) { return x * y; },\n+                [](Vec x, Vec y) { return x + y; },\n+                grad_data,\n+                output_data,\n+                dim_size);", "path": "aten/src/ATen/native/cpu/SoftMaxKernel.cpp", "position": 158, "original_position": 171, "commit_id": "b269b30289cf014a9bc3ce4924567ecb035a5fe1", "original_commit_id": "5daf54c358014219cfd16436f4db8f6fd8f45099", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "body": "I'm translating this from the existing code.", "created_at": "2018-05-15T00:20:47Z", "updated_at": "2018-11-23T15:44:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/7375#discussion_r188135407", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7375", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/188135407"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7375#discussion_r188135407"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7375"}}, "body_html": "<p>I'm translating this from the existing code.</p>", "body_text": "I'm translating this from the existing code.", "in_reply_to_id": 188133676}