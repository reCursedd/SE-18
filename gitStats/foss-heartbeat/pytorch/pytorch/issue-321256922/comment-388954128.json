{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388954128", "html_url": "https://github.com/pytorch/pytorch/pull/7375#issuecomment-388954128", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7375", "id": 388954128, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODk1NDEyOA==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-14T20:36:08Z", "updated_at": "2018-05-14T20:36:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><em>Deciding on whether to accumulate into doubles or floats from a perf perspective.</em></p>\n<p>I'm using the follow analysis to investigate whether this algorithm is compute-bound. If it is, we should accumulate into floats by default and advise users with numerical stability issues to upcast to doubles as that case will be rare.</p>\n<p>My development machine has two NUMA nodes with the following layout</p>\n<pre><code>$ numactl --hardware\navailable: 2 nodes (0-1)\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\nnode 0 size: 257863 MB\nnode 0 free: 222621 MB\nnode 1 cpus: 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79\nnode 1 size: 258041 MB\nnode 1 free: 248925 MB\nnode distances:\nnode   0   1\n  0:  10  21\n  1:  21  10\n</code></pre>\n<p>Any kernel that is to some extent memory bound will see heavy penalties from being scheduled on node 0 cpus, but have their memory bound to node 1 memory, since using the QPI is much slower than reading from local memory.</p>\n<p>I'm using the following code to get some measurements, which triggers the vectorized branches. The size of the input tensor is chosen to be roughly representative of a typical (larger) workload. It has a batchsize of 128 and 20k classes. There are two investigations: 1) Set the cpu frequency to different values 2) Bind memory to a different NUMA node.</p>\n<p>I'm using the following code</p>\n<pre><code>import torch\nimport time\nimport gc\n\nif __name__ == \"__main__\":\n    softmax = torch.nn.LogSoftmax(dim=1)\n    output = torch.randn(256, 20000).type('torch.FloatTensor')\n    gc.collect()\n    tstart = time.time()\n    for _ in range(250):\n        numbers = softmax(output)\n    print(\"elapsed: \" + str(time.time() - tstart))\n</code></pre>\n<p>For 1) we can observe</p>\n<pre><code>$ perf stat numactl --membind=0 taskset -c 0,40 python softmax_comp.py\nelapsed: 3.24645280838\n\n Performance counter stats for 'numactl --membind=0 taskset -c 0,40 python softmax_comp.py':\n\n       4034.933398      task-clock (msec)         #    0.940 CPUs utilized\n             1,637      context-switches          #    0.406 K/sec\n                20      cpu-migrations            #    0.005 K/sec\n            61,345      page-faults               #    0.015 M/sec\n     8,871,752,268      cycles                    #    2.199 GHz\n   &lt;not supported&gt;      stalled-cycles-frontend\n   &lt;not supported&gt;      stalled-cycles-backend\n    10,145,690,978      instructions              #    1.14  insns per cycle\n     1,373,296,620      branches                  #  340.352 M/sec\n         4,634,132      branch-misses             #    0.34% of all branches\n\n       4.294467340 seconds time elapsed\n</code></pre>\n<pre><code>$ perf stat numactl --membind=0 taskset -c 0,40 python softmax_comp.py\nelapsed: 6.08645606041\n\n Performance counter stats for 'numactl --membind=0 taskset -c 0,40 python softmax_comp.py':\n\n       7547.804302      task-clock (msec)         #    0.964 CPUs utilized\n             1,652      context-switches          #    0.219 K/sec\n                20      cpu-migrations            #    0.003 K/sec\n            92,522      page-faults               #    0.012 M/sec\n     9,048,895,951      cycles                    #    1.199 GHz\n   &lt;not supported&gt;      stalled-cycles-frontend\n   &lt;not supported&gt;      stalled-cycles-backend\n    10,227,923,023      instructions              #    1.13  insns per cycle\n     1,387,674,456      branches                  #  183.851 M/sec\n         4,650,287      branch-misses             #    0.34% of all branches\n\n       7.829675110 seconds time elapsed\n</code></pre>\n<p>For 2) we can observe</p>\n<pre><code>$ perf stat numactl --membind=1 taskset -c 0,40 python softmax_comp.py\nelapsed: 3.40651988983\n\n Performance counter stats for 'numactl --membind=1 taskset -c 0,40 python softmax_comp.py':\n\n       4187.342577      task-clock (msec)         #    0.971 CPUs utilized\n               786      context-switches          #    0.188 K/sec\n                12      cpu-migrations            #    0.003 K/sec\n            93,049      page-faults               #    0.022 M/sec\n     9,208,809,385      cycles                    #    2.199 GHz\n   &lt;not supported&gt;      stalled-cycles-frontend\n   &lt;not supported&gt;      stalled-cycles-backend\n    10,216,458,069      instructions              #    1.11  insns per cycle\n     1,385,948,756      branches                  #  330.985 M/sec\n         4,573,052      branch-misses             #    0.33% of all branches\n\n       4.314352108 seconds time elapsed\n</code></pre>\n<pre><code>$ perf stat numactl --membind=1 taskset -c 0,40 python softmax_comp.py\nelapsed: 6.14880990982\n\n Performance counter stats for 'numactl --membind=1 taskset -c 0,40 python softmax_comp.py':\n\n       7595.598948      task-clock (msec)         #    0.982 CPUs utilized\n               798      context-switches          #    0.105 K/sec\n                14      cpu-migrations            #    0.002 K/sec\n            61,877      page-faults               #    0.008 M/sec\n     9,108,585,035      cycles                    #    1.199 GHz\n   &lt;not supported&gt;      stalled-cycles-frontend\n   &lt;not supported&gt;      stalled-cycles-backend\n    10,139,876,776      instructions              #    1.11  insns per cycle\n     1,372,300,483      branches                  #  180.670 M/sec\n         4,619,897      branch-misses             #    0.34% of all branches\n\n       7.732898021 seconds time elapsed\n</code></pre>\n<p>In conclusion, we see a decrease in wall-time (3.2s to 6.0s and 3.4s to 6.1s on both memory nodes) proportional to a decrease in cpu frequency (2.2GHz to 1.2 GHz), however only a very small decrease in wall-time when allocating memory on a NUMA node away from the used CPUs.</p>\n<p>This leads me to believe that the kernel is compute bound on a single core and my particular computer.</p>\n<p>I've also compare the wall-time of this kernel with master. Even when using doubles, we still get a 2x improvement in runtime (on a single core) in comparison to the current implementation. With floats we get around a ~5.5x improvement.</p>", "body_text": "Deciding on whether to accumulate into doubles or floats from a perf perspective.\nI'm using the follow analysis to investigate whether this algorithm is compute-bound. If it is, we should accumulate into floats by default and advise users with numerical stability issues to upcast to doubles as that case will be rare.\nMy development machine has two NUMA nodes with the following layout\n$ numactl --hardware\navailable: 2 nodes (0-1)\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\nnode 0 size: 257863 MB\nnode 0 free: 222621 MB\nnode 1 cpus: 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79\nnode 1 size: 258041 MB\nnode 1 free: 248925 MB\nnode distances:\nnode   0   1\n  0:  10  21\n  1:  21  10\n\nAny kernel that is to some extent memory bound will see heavy penalties from being scheduled on node 0 cpus, but have their memory bound to node 1 memory, since using the QPI is much slower than reading from local memory.\nI'm using the following code to get some measurements, which triggers the vectorized branches. The size of the input tensor is chosen to be roughly representative of a typical (larger) workload. It has a batchsize of 128 and 20k classes. There are two investigations: 1) Set the cpu frequency to different values 2) Bind memory to a different NUMA node.\nI'm using the following code\nimport torch\nimport time\nimport gc\n\nif __name__ == \"__main__\":\n    softmax = torch.nn.LogSoftmax(dim=1)\n    output = torch.randn(256, 20000).type('torch.FloatTensor')\n    gc.collect()\n    tstart = time.time()\n    for _ in range(250):\n        numbers = softmax(output)\n    print(\"elapsed: \" + str(time.time() - tstart))\n\nFor 1) we can observe\n$ perf stat numactl --membind=0 taskset -c 0,40 python softmax_comp.py\nelapsed: 3.24645280838\n\n Performance counter stats for 'numactl --membind=0 taskset -c 0,40 python softmax_comp.py':\n\n       4034.933398      task-clock (msec)         #    0.940 CPUs utilized\n             1,637      context-switches          #    0.406 K/sec\n                20      cpu-migrations            #    0.005 K/sec\n            61,345      page-faults               #    0.015 M/sec\n     8,871,752,268      cycles                    #    2.199 GHz\n   <not supported>      stalled-cycles-frontend\n   <not supported>      stalled-cycles-backend\n    10,145,690,978      instructions              #    1.14  insns per cycle\n     1,373,296,620      branches                  #  340.352 M/sec\n         4,634,132      branch-misses             #    0.34% of all branches\n\n       4.294467340 seconds time elapsed\n\n$ perf stat numactl --membind=0 taskset -c 0,40 python softmax_comp.py\nelapsed: 6.08645606041\n\n Performance counter stats for 'numactl --membind=0 taskset -c 0,40 python softmax_comp.py':\n\n       7547.804302      task-clock (msec)         #    0.964 CPUs utilized\n             1,652      context-switches          #    0.219 K/sec\n                20      cpu-migrations            #    0.003 K/sec\n            92,522      page-faults               #    0.012 M/sec\n     9,048,895,951      cycles                    #    1.199 GHz\n   <not supported>      stalled-cycles-frontend\n   <not supported>      stalled-cycles-backend\n    10,227,923,023      instructions              #    1.13  insns per cycle\n     1,387,674,456      branches                  #  183.851 M/sec\n         4,650,287      branch-misses             #    0.34% of all branches\n\n       7.829675110 seconds time elapsed\n\nFor 2) we can observe\n$ perf stat numactl --membind=1 taskset -c 0,40 python softmax_comp.py\nelapsed: 3.40651988983\n\n Performance counter stats for 'numactl --membind=1 taskset -c 0,40 python softmax_comp.py':\n\n       4187.342577      task-clock (msec)         #    0.971 CPUs utilized\n               786      context-switches          #    0.188 K/sec\n                12      cpu-migrations            #    0.003 K/sec\n            93,049      page-faults               #    0.022 M/sec\n     9,208,809,385      cycles                    #    2.199 GHz\n   <not supported>      stalled-cycles-frontend\n   <not supported>      stalled-cycles-backend\n    10,216,458,069      instructions              #    1.11  insns per cycle\n     1,385,948,756      branches                  #  330.985 M/sec\n         4,573,052      branch-misses             #    0.33% of all branches\n\n       4.314352108 seconds time elapsed\n\n$ perf stat numactl --membind=1 taskset -c 0,40 python softmax_comp.py\nelapsed: 6.14880990982\n\n Performance counter stats for 'numactl --membind=1 taskset -c 0,40 python softmax_comp.py':\n\n       7595.598948      task-clock (msec)         #    0.982 CPUs utilized\n               798      context-switches          #    0.105 K/sec\n                14      cpu-migrations            #    0.002 K/sec\n            61,877      page-faults               #    0.008 M/sec\n     9,108,585,035      cycles                    #    1.199 GHz\n   <not supported>      stalled-cycles-frontend\n   <not supported>      stalled-cycles-backend\n    10,139,876,776      instructions              #    1.11  insns per cycle\n     1,372,300,483      branches                  #  180.670 M/sec\n         4,619,897      branch-misses             #    0.34% of all branches\n\n       7.732898021 seconds time elapsed\n\nIn conclusion, we see a decrease in wall-time (3.2s to 6.0s and 3.4s to 6.1s on both memory nodes) proportional to a decrease in cpu frequency (2.2GHz to 1.2 GHz), however only a very small decrease in wall-time when allocating memory on a NUMA node away from the used CPUs.\nThis leads me to believe that the kernel is compute bound on a single core and my particular computer.\nI've also compare the wall-time of this kernel with master. Even when using doubles, we still get a 2x improvement in runtime (on a single core) in comparison to the current implementation. With floats we get around a ~5.5x improvement.", "body": "*Deciding on whether to accumulate into doubles or floats from a perf perspective.*\r\n\r\nI'm using the follow analysis to investigate whether this algorithm is compute-bound. If it is, we should accumulate into floats by default and advise users with numerical stability issues to upcast to doubles as that case will be rare.\r\n\r\nMy development machine has two NUMA nodes with the following layout\r\n\r\n```\r\n$ numactl --hardware\r\navailable: 2 nodes (0-1)\r\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\r\nnode 0 size: 257863 MB\r\nnode 0 free: 222621 MB\r\nnode 1 cpus: 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79\r\nnode 1 size: 258041 MB\r\nnode 1 free: 248925 MB\r\nnode distances:\r\nnode   0   1\r\n  0:  10  21\r\n  1:  21  10\r\n```\r\n\r\nAny kernel that is to some extent memory bound will see heavy penalties from being scheduled on node 0 cpus, but have their memory bound to node 1 memory, since using the QPI is much slower than reading from local memory. \r\n\r\nI'm using the following code to get some measurements, which triggers the vectorized branches. The size of the input tensor is chosen to be roughly representative of a typical (larger) workload. It has a batchsize of 128 and 20k classes. There are two investigations: 1) Set the cpu frequency to different values 2) Bind memory to a different NUMA node.\r\n\r\nI'm using the following code\r\n```\r\nimport torch\r\nimport time\r\nimport gc\r\n\r\nif __name__ == \"__main__\":\r\n    softmax = torch.nn.LogSoftmax(dim=1)\r\n    output = torch.randn(256, 20000).type('torch.FloatTensor')\r\n    gc.collect()\r\n    tstart = time.time()\r\n    for _ in range(250):\r\n        numbers = softmax(output)\r\n    print(\"elapsed: \" + str(time.time() - tstart))\r\n```\r\n\r\n\r\nFor 1) we can observe\r\n\r\n```\r\n$ perf stat numactl --membind=0 taskset -c 0,40 python softmax_comp.py\r\nelapsed: 3.24645280838\r\n\r\n Performance counter stats for 'numactl --membind=0 taskset -c 0,40 python softmax_comp.py':\r\n\r\n       4034.933398      task-clock (msec)         #    0.940 CPUs utilized\r\n             1,637      context-switches          #    0.406 K/sec\r\n                20      cpu-migrations            #    0.005 K/sec\r\n            61,345      page-faults               #    0.015 M/sec\r\n     8,871,752,268      cycles                    #    2.199 GHz\r\n   <not supported>      stalled-cycles-frontend\r\n   <not supported>      stalled-cycles-backend\r\n    10,145,690,978      instructions              #    1.14  insns per cycle\r\n     1,373,296,620      branches                  #  340.352 M/sec\r\n         4,634,132      branch-misses             #    0.34% of all branches\r\n\r\n       4.294467340 seconds time elapsed\r\n```\r\n\r\n```\r\n$ perf stat numactl --membind=0 taskset -c 0,40 python softmax_comp.py\r\nelapsed: 6.08645606041\r\n\r\n Performance counter stats for 'numactl --membind=0 taskset -c 0,40 python softmax_comp.py':\r\n\r\n       7547.804302      task-clock (msec)         #    0.964 CPUs utilized\r\n             1,652      context-switches          #    0.219 K/sec\r\n                20      cpu-migrations            #    0.003 K/sec\r\n            92,522      page-faults               #    0.012 M/sec\r\n     9,048,895,951      cycles                    #    1.199 GHz\r\n   <not supported>      stalled-cycles-frontend\r\n   <not supported>      stalled-cycles-backend\r\n    10,227,923,023      instructions              #    1.13  insns per cycle\r\n     1,387,674,456      branches                  #  183.851 M/sec\r\n         4,650,287      branch-misses             #    0.34% of all branches\r\n\r\n       7.829675110 seconds time elapsed\r\n```\r\n\r\nFor 2) we can observe\r\n\r\n```\r\n$ perf stat numactl --membind=1 taskset -c 0,40 python softmax_comp.py\r\nelapsed: 3.40651988983\r\n\r\n Performance counter stats for 'numactl --membind=1 taskset -c 0,40 python softmax_comp.py':\r\n\r\n       4187.342577      task-clock (msec)         #    0.971 CPUs utilized\r\n               786      context-switches          #    0.188 K/sec\r\n                12      cpu-migrations            #    0.003 K/sec\r\n            93,049      page-faults               #    0.022 M/sec\r\n     9,208,809,385      cycles                    #    2.199 GHz\r\n   <not supported>      stalled-cycles-frontend\r\n   <not supported>      stalled-cycles-backend\r\n    10,216,458,069      instructions              #    1.11  insns per cycle\r\n     1,385,948,756      branches                  #  330.985 M/sec\r\n         4,573,052      branch-misses             #    0.33% of all branches\r\n\r\n       4.314352108 seconds time elapsed\r\n```\r\n\r\n```\r\n$ perf stat numactl --membind=1 taskset -c 0,40 python softmax_comp.py\r\nelapsed: 6.14880990982\r\n\r\n Performance counter stats for 'numactl --membind=1 taskset -c 0,40 python softmax_comp.py':\r\n\r\n       7595.598948      task-clock (msec)         #    0.982 CPUs utilized\r\n               798      context-switches          #    0.105 K/sec\r\n                14      cpu-migrations            #    0.002 K/sec\r\n            61,877      page-faults               #    0.008 M/sec\r\n     9,108,585,035      cycles                    #    1.199 GHz\r\n   <not supported>      stalled-cycles-frontend\r\n   <not supported>      stalled-cycles-backend\r\n    10,139,876,776      instructions              #    1.11  insns per cycle\r\n     1,372,300,483      branches                  #  180.670 M/sec\r\n         4,619,897      branch-misses             #    0.34% of all branches\r\n\r\n       7.732898021 seconds time elapsed\r\n```\r\n\r\nIn conclusion, we see a decrease in wall-time (3.2s to 6.0s and 3.4s to 6.1s on both memory nodes) proportional to a decrease in cpu frequency (2.2GHz to 1.2 GHz), however only a very small decrease in wall-time when allocating memory on a NUMA node away from the used CPUs.\r\n\r\nThis leads me to believe that the kernel is compute bound on a single core and my particular computer.\r\n\r\nI've also compare the wall-time of this kernel with master. Even when using doubles, we still get a 2x improvement in runtime (on a single core) in comparison to the current implementation. With floats we get around a ~5.5x improvement."}