{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186791735", "pull_request_review_id": 118441469, "id": 186791735, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Njc5MTczNQ==", "diff_hunk": "@@ -1,33 +1,241 @@\n #include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n #include \"ATen/NativeFunctions.h\"\n+#include \"ATen/TensorUtils.h\"\n+#include \"ATen/WrapDimUtils.h\"\n+#include \"ATen/native/cpu/ActivationKernel.h\"\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+#ifdef _MSC_VER\n+#define LOG_SOFTMAX_SIZE_TYPE int64_t\n+#define LOG_SOFTMAX_CAST_TYPE (int64_t)\n+#else\n+#define LOG_SOFTMAX_SIZE_TYPE uint64_t\n+#define LOG_SOFTMAX_CAST_TYPE\n+#endif\n \n namespace at { namespace native {\n \n static const double SELU_ALPHA = 1.6732632423543772848170429916717;\n static const double SELU_SCALE = 1.0507009873554804934193349852946;\n \n-Tensor relu(const Tensor & self) {\n+Tensor relu(const Tensor& self) {\n   return self.clamp_min(0.0);\n }\n \n-Tensor & relu_(Tensor & self) {\n+Tensor& relu_(Tensor& self) {\n   return self.clamp_min_(0.0);\n }\n \n-Tensor selu(const Tensor & self) {\n+Tensor selu(const Tensor& self) {\n   return at::elu(self, SELU_ALPHA, SELU_SCALE);\n }\n \n-Tensor & selu_(Tensor & self) {\n+Tensor& selu_(Tensor& self) {\n   return at::elu_(self, SELU_ALPHA, SELU_SCALE);\n }\n \n-Tensor rrelu(const Tensor & self, Scalar lower, Scalar upper, bool training, Generator* generator) {\n-  return at::rrelu_with_noise(self, self.type().tensor(), lower, upper, training, generator);\n+Tensor rrelu(\n+    const Tensor& self,\n+    Scalar lower,\n+    Scalar upper,\n+    bool training,\n+    Generator* generator) {\n+  return at::rrelu_with_noise(\n+      self, self.type().tensor(), lower, upper, training, generator);\n+}\n+\n+Tensor& rrelu_(\n+    Tensor& self,\n+    Scalar lower,\n+    Scalar upper,\n+    bool training,\n+    Generator* generator) {\n+  return at::rrelu_with_noise_(\n+      self, self.type().tensor(), lower, upper, training, generator);\n+}\n+\n+namespace {\n+\n+template <typename scalar_t, bool LogSoftMax>\n+void host_softmax(Tensor output, const Tensor& input, const int64_t dim) {\n+  uint64_t outer_size = 1;\n+  uint64_t dim_size = input.size(dim);\n+  uint64_t inner_size = 1;\n+  for (int64_t i = 0; i < dim; ++i)\n+    outer_size *= input.size(i);\n+  for (int64_t i = dim + 1; i < input.dim(); ++i)\n+    inner_size *= input.size(i);\n+  uint64_t dim_stride = inner_size;\n+  uint64_t outer_stride = dim_size * dim_stride;\n+  scalar_t* input_data_base = input.data<scalar_t>();\n+  scalar_t* output_data_base = output.data<scalar_t>();\n+  LOG_SOFTMAX_SIZE_TYPE i, d;\n+#pragma omp parallel for private(i, d)\n+  for (i = 0; i < LOG_SOFTMAX_CAST_TYPE(outer_size * inner_size); i++) {\n+    uint64_t outer_idx = i / inner_size;\n+    uint64_t inner_idx = i % inner_size;\n+    scalar_t* input_data =\n+        input_data_base + outer_idx * outer_stride + inner_idx;\n+    scalar_t* output_data =\n+        output_data_base + outer_idx * outer_stride + inner_idx;\n+\n+    scalar_t max_input = -std::numeric_limits<scalar_t>::max();\n+    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++)\n+      max_input = std::max(max_input, input_data[d * dim_stride]);\n+\n+    using accscalar_t = acc_type<scalar_t, false>;\n+    accscalar_t tmpsum = 0;\n+    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++) {", "path": "aten/src/ATen/native/Activation.cpp", "position": null, "original_position": 98, "commit_id": "b269b30289cf014a9bc3ce4924567ecb035a5fe1", "original_commit_id": "400ea798f33873ea27ef1e74167a0d7997fb16f1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can we just `static_cast<LONG_SOFTMAX_SIZE_TYPE>` instead of having this extra macro? We could shorten its name", "created_at": "2018-05-08T16:40:21Z", "updated_at": "2018-11-23T15:43:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/7375#discussion_r186791735", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7375", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186791735"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7375#discussion_r186791735"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7375"}}, "body_html": "<p>Can we just <code>static_cast&lt;LONG_SOFTMAX_SIZE_TYPE&gt;</code> instead of having this extra macro? We could shorten its name</p>", "body_text": "Can we just static_cast<LONG_SOFTMAX_SIZE_TYPE> instead of having this extra macro? We could shorten its name"}