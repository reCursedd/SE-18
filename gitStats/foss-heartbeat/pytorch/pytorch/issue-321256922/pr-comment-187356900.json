{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187356900", "pull_request_review_id": 118441469, "id": 187356900, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzM1NjkwMA==", "diff_hunk": "@@ -0,0 +1,484 @@\n+#include \"ATen/native/cpu/ActivationKernel.h\"\n+\n+#include <algorithm>\n+#include <iterator>\n+#include <limits>\n+#include <numeric>\n+\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/Parallel.h\"\n+#include \"ATen/cpu/vec256/vec256.h\"\n+#include \"ATen/optional.h\"\n+\n+// NOTE: In general we avoid calls into cmath for code compiled with AVX/AVX2\n+// This is because of SSE-AVX transitions and a bug in Glibc2.23\n+// See https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280\n+\n+namespace at { namespace native {\n+namespace {\n+\n+static tbb::affinity_partitioner ap;\n+\n+template <int64_t size>\n+inline int64_t _leftover(int64_t x, int64_t y) {\n+  if (x + size > y)\n+    return y - x;\n+  return size;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_sum(int64_t dim_size, scalar_t* input_data) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  for (int ii = 0; ii < dim_size; ii++) {\n+  }\n+  int64_t d = 0;\n+  Vec sum_vec(0);\n+  scalar_t sum = 0;\n+  Vec value(0);\n+  for (; d < dim_size; d += Vec::size) {\n+    if (d + Vec::size > dim_size) {\n+      // TODO: For some reason, when using partial loads, value isn't set to 0\n+      // on each iteration\n+      for (int64_t i = d; i < dim_size; i++) {\n+        sum += input_data[i];\n+      }\n+    } else {\n+      value.load(input_data + d);\n+      sum_vec = sum_vec + value;\n+    }\n+  }\n+  scalar_t sum_arr[Vec::size];\n+  sum_vec.store(sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    sum += sum_arr[i];\n+  }\n+  return sum;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t\n+_vec_sum_mul(int64_t dim_size, scalar_t* input_data, scalar_t* input_data2) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  int64_t d = 0;\n+  Vec sum_vec(0);\n+  scalar_t sum = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    if (d + Vec::size > dim_size) {\n+      for (int i = d; i < dim_size; i++) {\n+        sum += input_data[i] * input_data2[i];\n+      }\n+    } else {\n+      Vec value(0);\n+      Vec value2(0);\n+      value.load(input_data + d);\n+      value2.load(input_data2 + d);\n+      value = value * value2;\n+      sum_vec = sum_vec + value;\n+    }\n+  }\n+  scalar_t sum_arr[Vec::size];\n+  sum_vec.store(sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    sum += sum_arr[i];\n+  }\n+  return sum;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_max(int64_t dim_size, scalar_t* input_data) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec max_val_vec(std::numeric_limits<scalar_t>::lowest());\n+  scalar_t max_val = std::numeric_limits<scalar_t>::lowest();\n+  for (int64_t d = 0; d < dim_size; d += Vec::size) {\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    Vec value;\n+    value.load(input_data + d, leftover);\n+    if (leftover < Vec::size) {\n+      for (int i = 0; i < leftover; i++) {\n+        if (input_data[d + i] > max_val) {\n+          max_val = input_data[d + i];\n+        }\n+      }\n+    } else {\n+      max_val_vec = vec256::max(value, max_val_vec);\n+    }\n+  }\n+  scalar_t max_val_arr[Vec::size];\n+  max_val_vec.store(max_val_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    if (max_val_arr[i] > max_val) {\n+      max_val = max_val_arr[i];\n+    }\n+  }\n+  return max_val;\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_mul_scalarsub_write(\n+    scalar_t* grad_input_data,\n+    scalar_t* grad_data,\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t sum) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec sum_vec(sum);\n+  int64_t d = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    Vec output(0);\n+    Vec grad(0);\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    output.load(output_data + d, leftover);\n+    grad.load(grad_data + d, leftover);\n+    grad = grad - sum_vec;\n+    grad = grad * output;\n+\n+    grad.store(grad_input_data + d, leftover);\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_sub_exp_scalarmul_write(\n+    scalar_t* grad_input_data,\n+    scalar_t* grad_data,\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t sum) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec sum_vec(sum);\n+  int64_t d = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    Vec output(0);\n+    Vec grad(0);\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    output.load(output_data + d, leftover);\n+    grad.load(grad_data + d, leftover);\n+    output = output.exp();\n+    output = output * sum_vec;\n+    grad = grad - output;\n+\n+    grad.store(grad_input_data + d, leftover);\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_norm_exp_sum_write(\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t* input_data,\n+    scalar_t max_input) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+\n+  int64_t d = 0;\n+  Vec tmpsum(0);\n+  Vec max_input_vec(max_input);\n+  scalar_t tmp_sum_arr[Vec::size];\n+  scalar_t tmp_sum_scalar = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    Vec value(0);\n+    value.load(input_data + d, leftover);\n+    value = value - max_input_vec;\n+    value = value.exp();\n+    // Need to do a partial add\n+    if (d + Vec::size > dim_size) {\n+      scalar_t value_arr[Vec::size];\n+      value.store(value_arr);\n+      for (int64_t i = 0; i < dim_size - d; i++) {\n+        tmp_sum_scalar += value_arr[i];\n+      }\n+      value.store(output_data + d, leftover);\n+    } else {\n+      tmpsum = tmpsum + value;\n+      value.store(output_data + d);\n+    }\n+  }\n+  tmpsum.store(tmp_sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    tmp_sum_scalar += tmp_sum_arr[i];\n+  }\n+  return tmp_sum_scalar;\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_log_softmax_lastdim(\n+    scalar_t* input_data_base,\n+    scalar_t* output_data_base,\n+    int64_t outer_size,\n+    int64_t dim_size) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  static constexpr int64_t CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size;\n+  int64_t grain_size = internal::TBB_GRAIN_SIZE / (dim_size * CHUNK_SIZE);\n+  if (grain_size < CHUNK_SIZE)\n+    grain_size = CHUNK_SIZE;\n+  //std::cout << \"CHUNK_SIZE: \" << CHUNK_SIZE<< std::endl;\n+  //std::cout << \"Vec::size: \" << Vec::size << std::endl;\n+  //std::cout << \"outer_size: \" << outer_size<< std::endl;\n+\n+  tbb::parallel_for(\n+      tbb::blocked_range<int64_t>(0, outer_size, grain_size),\n+      [&](const tbb::blocked_range<int64_t>& r) {\n+        for (int64_t ii = r.begin(); ii < r.end(); ii += CHUNK_SIZE) {\n+          scalar_t tmp_sum_scalar[CHUNK_SIZE];\n+          scalar_t max_input_arr[CHUNK_SIZE];\n+          int64_t inner_loop_end = CHUNK_SIZE;\n+          if (ii + CHUNK_SIZE > r.end())\n+            inner_loop_end = r.end() - ii;\n+          //std::cout<< \"inner_loop_end: \" << inner_loop_end << std::endl;\n+          for (int64_t j = 0; j < inner_loop_end; j++) {\n+            int64_t i = ii + j;\n+            scalar_t* input_data = input_data_base + i * dim_size;\n+            max_input_arr[j] = _vec_max(dim_size, input_data);\n+            //std::cout << \"max_input_arr[\" << j << \"]: \" << max_input_arr[j] << std::endl;\n+          }\n+          for (int64_t j = 0; j < inner_loop_end; j++) {\n+            int64_t i = ii + j;\n+            tmp_sum_scalar[j] = 0;\n+            scalar_t* input_data = input_data_base + i * dim_size;\n+            Vec tmpsum(0);\n+            Vec max_input_vec(max_input_arr[j]);\n+            for (int64_t d = 0; d < dim_size; d += Vec::size) {\n+              Vec value(0);\n+              int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+              //std::cout << \"d: \" << d << \" - leftover: \" << leftover << std::endl;\n+              value.load(input_data + d, leftover);\n+              value = value - max_input_vec;\n+              value = value.exp();\n+              // Need to do a partial add\n+              if (leftover < Vec::size) {\n+                scalar_t value_arr[Vec::size];\n+                value.store(value_arr);\n+                for (int64_t i = 0; i < leftover; i++)\n+                  tmp_sum_scalar[j] += value_arr[i];\n+              } else {\n+                tmpsum = tmpsum + value;\n+              }\n+            }\n+            scalar_t tmp_sum_arr[Vec::size];\n+            tmpsum.store(tmp_sum_arr);\n+            for (int64_t i = 0; i < Vec::size; i++) \n+              tmp_sum_scalar[j] += tmp_sum_arr[i];\n+            //std::cout << \"tmp_sum_scalar[\" << j << \"]: \" << tmp_sum_scalar[j] << std::endl;\n+          }\n+          for (int64_t j = 0; j < inner_loop_end; j += Vec::size) {\n+            int64_t leftover = _leftover<Vec::size>(j, inner_loop_end);\n+            //std::cout << \"j: \" << j << \" - \" << leftover << std::endl;\n+            Vec tmp_sum_scalar_vec;\n+            tmp_sum_scalar_vec.load(tmp_sum_scalar + j, leftover);\n+            //std::cout << \"tmp_sum_scalar_vec1: \" << tmp_sum_scalar_vec << std::endl;\n+            tmp_sum_scalar_vec = tmp_sum_scalar_vec.log();\n+            //std::cout << \"tmp_sum_scalar_vec2: \" << tmp_sum_scalar_vec << std::endl;\n+            Vec max_input_vec;\n+            max_input_vec.load(max_input_arr + j, leftover);\n+            tmp_sum_scalar_vec = max_input_vec + tmp_sum_scalar_vec;\n+            tmp_sum_scalar_vec.store(tmp_sum_scalar + j, leftover);\n+          }\n+          for (int64_t j = 0; j < inner_loop_end; j++) {\n+            int64_t i = ii + j;\n+            scalar_t* input_data = input_data_base + i * dim_size;\n+            scalar_t* output_data = output_data_base + i * dim_size;\n+            Vec sub_val_vec(tmp_sum_scalar[j]);\n+            for (int64_t d = 0; d < dim_size; d += Vec::size) {\n+              Vec value;\n+              int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+              value.load(input_data + d, leftover);\n+              value = value - sub_val_vec;\n+              value.store(output_data + d, leftover);\n+            }\n+          }\n+        }\n+      },\n+      ap);\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_softmax_lastdim(\n+    scalar_t* input_data_base,\n+    scalar_t* output_data_base,\n+    int64_t outer_size,\n+    int64_t dim_size) {\n+  int64_t grain_size = internal::TBB_GRAIN_SIZE / dim_size;\n+  if (grain_size < 1)\n+    grain_size = 1;\n+\n+  using Vec = vec256::Vec256<scalar_t>;\n+  tbb::parallel_for(\n+      tbb::blocked_range<int64_t>(0, outer_size, grain_size),\n+      [&](const tbb::blocked_range<int64_t>& r) {\n+        for (int64_t i = r.begin(); i < r.end(); i++) {\n+          scalar_t* input_data = input_data_base + i * dim_size;\n+          scalar_t* output_data = output_data_base + i * dim_size;\n+\n+          scalar_t max_input = _vec_max(dim_size, input_data);\n+          scalar_t tmp_sum = _vec_norm_exp_sum_write(\n+              output_data, dim_size, input_data, max_input);\n+          tmp_sum = 1 / tmp_sum;\n+          Vec sub_val_vec(tmp_sum);\n+          for (int64_t d = 0; d < dim_size; d += Vec::size) {\n+            Vec value;\n+            int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+            value.load(output_data + d, leftover);\n+            value = value * sub_val_vec;\n+            value.store(output_data + d, leftover);\n+          }\n+        }\n+      },\n+      ap);", "path": "aten/src/ATen/native/cpu/ActivationKernel.cpp", "position": null, "original_position": 325, "commit_id": "b269b30289cf014a9bc3ce4924567ecb035a5fe1", "original_commit_id": "86cd5d1daee4eb5a2258769a99725d967b289dd1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Lol why is this one sooooooo much shorter than LogSoftmax? Why can't we have a templatized kernel like in the `host_softmax` case?", "created_at": "2018-05-10T14:59:04Z", "updated_at": "2018-11-23T15:43:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187356900", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7375", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187356900"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187356900"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7375"}}, "body_html": "<p>Lol why is this one sooooooo much shorter than LogSoftmax? Why can't we have a templatized kernel like in the <code>host_softmax</code> case?</p>", "body_text": "Lol why is this one sooooooo much shorter than LogSoftmax? Why can't we have a templatized kernel like in the host_softmax case?"}