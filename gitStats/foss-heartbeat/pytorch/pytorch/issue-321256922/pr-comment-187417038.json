{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187417038", "pull_request_review_id": 119194408, "id": 187417038, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzQxNzAzOA==", "diff_hunk": "@@ -0,0 +1,484 @@\n+#include \"ATen/native/cpu/ActivationKernel.h\"\n+\n+#include <algorithm>\n+#include <iterator>\n+#include <limits>\n+#include <numeric>\n+\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/Parallel.h\"\n+#include \"ATen/cpu/vec256/vec256.h\"\n+#include \"ATen/optional.h\"\n+\n+// NOTE: In general we avoid calls into cmath for code compiled with AVX/AVX2\n+// This is because of SSE-AVX transitions and a bug in Glibc2.23\n+// See https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280\n+\n+namespace at { namespace native {\n+namespace {\n+\n+static tbb::affinity_partitioner ap;\n+\n+template <int64_t size>\n+inline int64_t _leftover(int64_t x, int64_t y) {\n+  if (x + size > y)\n+    return y - x;\n+  return size;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_sum(int64_t dim_size, scalar_t* input_data) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  for (int ii = 0; ii < dim_size; ii++) {\n+  }\n+  int64_t d = 0;\n+  Vec sum_vec(0);\n+  scalar_t sum = 0;\n+  Vec value(0);\n+  for (; d < dim_size; d += Vec::size) {\n+    if (d + Vec::size > dim_size) {\n+      // TODO: For some reason, when using partial loads, value isn't set to 0\n+      // on each iteration\n+      for (int64_t i = d; i < dim_size; i++) {\n+        sum += input_data[i];\n+      }\n+    } else {\n+      value.load(input_data + d);\n+      sum_vec = sum_vec + value;\n+    }\n+  }\n+  scalar_t sum_arr[Vec::size];\n+  sum_vec.store(sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    sum += sum_arr[i];\n+  }\n+  return sum;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t\n+_vec_sum_mul(int64_t dim_size, scalar_t* input_data, scalar_t* input_data2) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  int64_t d = 0;\n+  Vec sum_vec(0);\n+  scalar_t sum = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    if (d + Vec::size > dim_size) {\n+      for (int i = d; i < dim_size; i++) {\n+        sum += input_data[i] * input_data2[i];\n+      }\n+    } else {\n+      Vec value(0);\n+      Vec value2(0);\n+      value.load(input_data + d);\n+      value2.load(input_data2 + d);\n+      value = value * value2;\n+      sum_vec = sum_vec + value;\n+    }\n+  }\n+  scalar_t sum_arr[Vec::size];\n+  sum_vec.store(sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    sum += sum_arr[i];\n+  }\n+  return sum;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_max(int64_t dim_size, scalar_t* input_data) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec max_val_vec(std::numeric_limits<scalar_t>::lowest());\n+  scalar_t max_val = std::numeric_limits<scalar_t>::lowest();\n+  for (int64_t d = 0; d < dim_size; d += Vec::size) {\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    Vec value;\n+    value.load(input_data + d, leftover);\n+    if (leftover < Vec::size) {\n+      for (int i = 0; i < leftover; i++) {\n+        if (input_data[d + i] > max_val) {\n+          max_val = input_data[d + i];\n+        }\n+      }\n+    } else {\n+      max_val_vec = vec256::max(value, max_val_vec);\n+    }\n+  }\n+  scalar_t max_val_arr[Vec::size];\n+  max_val_vec.store(max_val_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    if (max_val_arr[i] > max_val) {\n+      max_val = max_val_arr[i];\n+    }\n+  }\n+  return max_val;\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_mul_scalarsub_write(\n+    scalar_t* grad_input_data,\n+    scalar_t* grad_data,\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t sum) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec sum_vec(sum);\n+  int64_t d = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    Vec output(0);\n+    Vec grad(0);\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    output.load(output_data + d, leftover);\n+    grad.load(grad_data + d, leftover);\n+    grad = grad - sum_vec;\n+    grad = grad * output;\n+\n+    grad.store(grad_input_data + d, leftover);\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_sub_exp_scalarmul_write(\n+    scalar_t* grad_input_data,\n+    scalar_t* grad_data,\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t sum) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec sum_vec(sum);\n+  int64_t d = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    Vec output(0);\n+    Vec grad(0);\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    output.load(output_data + d, leftover);\n+    grad.load(grad_data + d, leftover);\n+    output = output.exp();\n+    output = output * sum_vec;\n+    grad = grad - output;\n+\n+    grad.store(grad_input_data + d, leftover);\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_norm_exp_sum_write(\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t* input_data,\n+    scalar_t max_input) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+\n+  int64_t d = 0;\n+  Vec tmpsum(0);\n+  Vec max_input_vec(max_input);\n+  scalar_t tmp_sum_arr[Vec::size];\n+  scalar_t tmp_sum_scalar = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    Vec value(0);\n+    value.load(input_data + d, leftover);\n+    value = value - max_input_vec;\n+    value = value.exp();\n+    // Need to do a partial add\n+    if (d + Vec::size > dim_size) {\n+      scalar_t value_arr[Vec::size];\n+      value.store(value_arr);\n+      for (int64_t i = 0; i < dim_size - d; i++) {\n+        tmp_sum_scalar += value_arr[i];\n+      }\n+      value.store(output_data + d, leftover);\n+    } else {\n+      tmpsum = tmpsum + value;\n+      value.store(output_data + d);\n+    }\n+  }\n+  tmpsum.store(tmp_sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    tmp_sum_scalar += tmp_sum_arr[i];\n+  }\n+  return tmp_sum_scalar;\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_log_softmax_lastdim(\n+    scalar_t* input_data_base,\n+    scalar_t* output_data_base,\n+    int64_t outer_size,\n+    int64_t dim_size) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  static constexpr int64_t CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size;\n+  int64_t grain_size = internal::TBB_GRAIN_SIZE / (dim_size * CHUNK_SIZE);\n+  if (grain_size < CHUNK_SIZE)\n+    grain_size = CHUNK_SIZE;\n+  //std::cout << \"CHUNK_SIZE: \" << CHUNK_SIZE<< std::endl;\n+  //std::cout << \"Vec::size: \" << Vec::size << std::endl;\n+  //std::cout << \"outer_size: \" << outer_size<< std::endl;\n+\n+  tbb::parallel_for(\n+      tbb::blocked_range<int64_t>(0, outer_size, grain_size),\n+      [&](const tbb::blocked_range<int64_t>& r) {\n+        for (int64_t ii = r.begin(); ii < r.end(); ii += CHUNK_SIZE) {\n+          scalar_t tmp_sum_scalar[CHUNK_SIZE];\n+          scalar_t max_input_arr[CHUNK_SIZE];\n+          int64_t inner_loop_end = CHUNK_SIZE;\n+          if (ii + CHUNK_SIZE > r.end())\n+            inner_loop_end = r.end() - ii;\n+          //std::cout<< \"inner_loop_end: \" << inner_loop_end << std::endl;\n+          for (int64_t j = 0; j < inner_loop_end; j++) {", "path": "aten/src/ATen/native/cpu/ActivationKernel.cpp", "position": null, "original_position": 227, "commit_id": "b269b30289cf014a9bc3ce4924567ecb035a5fe1", "original_commit_id": "86cd5d1daee4eb5a2258769a99725d967b289dd1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Uh, that actually makes sense. Thanks @colesbury!", "created_at": "2018-05-10T18:26:23Z", "updated_at": "2018-11-23T15:43:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187417038", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7375", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187417038"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187417038"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7375"}}, "body_html": "<p>Uh, that actually makes sense. Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a>!</p>", "body_text": "Uh, that actually makes sense. Thanks @colesbury!", "in_reply_to_id": 187355497}