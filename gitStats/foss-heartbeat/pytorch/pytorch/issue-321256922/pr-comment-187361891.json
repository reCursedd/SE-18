{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187361891", "pull_request_review_id": 119126961, "id": 187361891, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzM2MTg5MQ==", "diff_hunk": "@@ -0,0 +1,484 @@\n+#include \"ATen/native/cpu/ActivationKernel.h\"\n+\n+#include <algorithm>\n+#include <iterator>\n+#include <limits>\n+#include <numeric>\n+\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/Parallel.h\"\n+#include \"ATen/cpu/vec256/vec256.h\"\n+#include \"ATen/optional.h\"\n+\n+// NOTE: In general we avoid calls into cmath for code compiled with AVX/AVX2\n+// This is because of SSE-AVX transitions and a bug in Glibc2.23\n+// See https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280\n+\n+namespace at { namespace native {\n+namespace {\n+\n+static tbb::affinity_partitioner ap;\n+\n+template <int64_t size>\n+inline int64_t _leftover(int64_t x, int64_t y) {\n+  if (x + size > y)\n+    return y - x;\n+  return size;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_sum(int64_t dim_size, scalar_t* input_data) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  for (int ii = 0; ii < dim_size; ii++) {\n+  }\n+  int64_t d = 0;\n+  Vec sum_vec(0);\n+  scalar_t sum = 0;\n+  Vec value(0);\n+  for (; d < dim_size; d += Vec::size) {\n+    if (d + Vec::size > dim_size) {\n+      // TODO: For some reason, when using partial loads, value isn't set to 0\n+      // on each iteration\n+      for (int64_t i = d; i < dim_size; i++) {\n+        sum += input_data[i];\n+      }\n+    } else {\n+      value.load(input_data + d);\n+      sum_vec = sum_vec + value;\n+    }\n+  }\n+  scalar_t sum_arr[Vec::size];\n+  sum_vec.store(sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    sum += sum_arr[i];\n+  }\n+  return sum;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t\n+_vec_sum_mul(int64_t dim_size, scalar_t* input_data, scalar_t* input_data2) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  int64_t d = 0;\n+  Vec sum_vec(0);\n+  scalar_t sum = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    if (d + Vec::size > dim_size) {\n+      for (int i = d; i < dim_size; i++) {\n+        sum += input_data[i] * input_data2[i];\n+      }\n+    } else {\n+      Vec value(0);\n+      Vec value2(0);\n+      value.load(input_data + d);\n+      value2.load(input_data2 + d);\n+      value = value * value2;\n+      sum_vec = sum_vec + value;\n+    }\n+  }\n+  scalar_t sum_arr[Vec::size];\n+  sum_vec.store(sum_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    sum += sum_arr[i];\n+  }\n+  return sum;\n+}\n+\n+template <typename scalar_t>\n+inline scalar_t _vec_max(int64_t dim_size, scalar_t* input_data) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec max_val_vec(std::numeric_limits<scalar_t>::lowest());\n+  scalar_t max_val = std::numeric_limits<scalar_t>::lowest();\n+  for (int64_t d = 0; d < dim_size; d += Vec::size) {\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    Vec value;\n+    value.load(input_data + d, leftover);\n+    if (leftover < Vec::size) {\n+      for (int i = 0; i < leftover; i++) {\n+        if (input_data[d + i] > max_val) {\n+          max_val = input_data[d + i];\n+        }\n+      }\n+    } else {\n+      max_val_vec = vec256::max(value, max_val_vec);\n+    }\n+  }\n+  scalar_t max_val_arr[Vec::size];\n+  max_val_vec.store(max_val_arr);\n+  for (int64_t i = 0; i < Vec::size; i++) {\n+    if (max_val_arr[i] > max_val) {\n+      max_val = max_val_arr[i];\n+    }\n+  }\n+  return max_val;\n+}\n+\n+template <typename scalar_t>\n+inline void _vec_mul_scalarsub_write(\n+    scalar_t* grad_input_data,\n+    scalar_t* grad_data,\n+    scalar_t* output_data,\n+    int64_t dim_size,\n+    scalar_t sum) {\n+  using Vec = vec256::Vec256<scalar_t>;\n+  Vec sum_vec(sum);\n+  int64_t d = 0;\n+  for (; d < dim_size; d += Vec::size) {\n+    Vec output(0);\n+    Vec grad(0);\n+    int64_t leftover = _leftover<Vec::size>(d, dim_size);\n+    output.load(output_data + d, leftover);\n+    grad.load(grad_data + d, leftover);\n+    grad = grad - sum_vec;\n+    grad = grad * output;\n+\n+    grad.store(grad_input_data + d, leftover);\n+  }", "path": "aten/src/ATen/native/cpu/ActivationKernel.cpp", "position": null, "original_position": 136, "commit_id": "b269b30289cf014a9bc3ce4924567ecb035a5fe1", "original_commit_id": "86cd5d1daee4eb5a2258769a99725d967b289dd1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Ok nvm. It's using the `leftover` thing.", "created_at": "2018-05-10T15:15:19Z", "updated_at": "2018-11-23T15:43:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187361891", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7375", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187361891"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187361891"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7375"}}, "body_html": "<p>Ok nvm. It's using the <code>leftover</code> thing.</p>", "body_text": "Ok nvm. It's using the leftover thing.", "in_reply_to_id": 187353919}