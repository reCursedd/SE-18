{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187358704", "pull_request_review_id": 119122934, "id": 187358704, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzM1ODcwNA==", "diff_hunk": "@@ -1,33 +1,267 @@\n+#include <iostream>\n #include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n #include \"ATen/NativeFunctions.h\"\n+#include \"ATen/TensorUtils.h\"\n+#include \"ATen/WrapDimUtils.h\"\n+#include \"ATen/native/cpu/ActivationKernel.h\"\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n \n namespace at { namespace native {\n \n static const double SELU_ALPHA = 1.6732632423543772848170429916717;\n static const double SELU_SCALE = 1.0507009873554804934193349852946;\n \n-Tensor relu(const Tensor & self) {\n+Tensor relu(const Tensor& self) {\n   return self.clamp_min(0.0);\n }\n \n-Tensor & relu_(Tensor & self) {\n+Tensor& relu_(Tensor& self) {\n   return self.clamp_min_(0.0);\n }\n \n-Tensor selu(const Tensor & self) {\n+Tensor selu(const Tensor& self) {\n   return at::elu(self, SELU_ALPHA, SELU_SCALE);\n }\n \n-Tensor & selu_(Tensor & self) {\n+Tensor& selu_(Tensor& self) {\n   return at::elu_(self, SELU_ALPHA, SELU_SCALE);\n }\n \n-Tensor rrelu(const Tensor & self, Scalar lower, Scalar upper, bool training, Generator* generator) {\n-  return at::rrelu_with_noise(self, self.type().tensor(), lower, upper, training, generator);\n+Tensor rrelu(\n+    const Tensor& self,\n+    Scalar lower,\n+    Scalar upper,\n+    bool training,\n+    Generator* generator) {\n+  return at::rrelu_with_noise(\n+      self, self.type().tensor(), lower, upper, training, generator);\n+}\n+\n+Tensor& rrelu_(\n+    Tensor& self,\n+    Scalar lower,\n+    Scalar upper,\n+    bool training,\n+    Generator* generator) {\n+  return at::rrelu_with_noise_(\n+      self, self.type().tensor(), lower, upper, training, generator);\n }\n \n-Tensor & rrelu_(Tensor & self, Scalar lower, Scalar upper, bool training, Generator* generator) {\n-  return at::rrelu_with_noise_(self, self.type().tensor(), lower, upper, training, generator);\n+namespace {\n+\n+template <typename scalar_t, bool LogSoftMax>\n+void host_softmax(Tensor output, const Tensor& input, const int64_t dim) {\n+  int64_t outer_size = 1;\n+  int64_t dim_size = input.size(dim);\n+  int64_t inner_size = 1;\n+  for (int64_t i = 0; i < dim; ++i)\n+    outer_size *= input.size(i);\n+  for (int64_t i = dim + 1; i < input.dim(); ++i)\n+    inner_size *= input.size(i);\n+  int64_t dim_stride = inner_size;\n+  int64_t outer_stride = dim_size * dim_stride;\n+  scalar_t* input_data_base = input.data<scalar_t>();\n+  scalar_t* output_data_base = output.data<scalar_t>();\n+  int64_t i, d;\n+#pragma omp parallel for private(i, d)\n+  for (i = 0; i < (outer_size * inner_size); i++) {\n+    int64_t outer_idx = i / inner_size;\n+    int64_t inner_idx = i % inner_size;\n+    scalar_t* input_data =\n+        input_data_base + outer_idx * outer_stride + inner_idx;\n+    scalar_t* output_data =\n+        output_data_base + outer_idx * outer_stride + inner_idx;\n+\n+    scalar_t max_input = -std::numeric_limits<scalar_t>::max();\n+    for (d = 0; d <  dim_size; d++)\n+      max_input = std::max(max_input, input_data[d * dim_stride]);\n+\n+    scalar_t tmpsum = 0;\n+    for (d = 0; d <  dim_size; d++) {\n+      scalar_t z = std::exp(input_data[d * dim_stride] - max_input);\n+      tmpsum += z;\n+      if (!LogSoftMax) {\n+        output_data[d * dim_stride] = z;\n+      }\n+    }\n+\n+    if (LogSoftMax)\n+      tmpsum = max_input + std::log(tmpsum);\n+    else\n+      tmpsum = 1 / tmpsum;\n+\n+    for (d = 0; d <  dim_size; d++)\n+      if (LogSoftMax)\n+        output_data[d * dim_stride] = input_data[d * dim_stride] - tmpsum;\n+      else\n+        output_data[d * dim_stride] *= tmpsum;\n+  }\n+}\n+\n+template <typename scalar_t, bool LogSoftMax>\n+void host_softmax_backward(\n+    Tensor& gI,\n+    const Tensor& grad,\n+    const Tensor& output,\n+    int64_t dim) {\n+\n+  int64_t outer_size = 1;\n+  int64_t dim_size = grad.size(dim);\n+  int64_t inner_size = 1;\n+  for (int64_t i = 0; i < dim; ++i)\n+    outer_size *= grad.size(i);\n+  for (int64_t i = dim + 1; i < grad.dim(); ++i)\n+    inner_size *= grad.size(i);\n+  int64_t dim_stride = inner_size;\n+  int64_t outer_stride = dim_size * dim_stride;\n+  scalar_t* gradInput_data_base = gI.data<scalar_t>();\n+  scalar_t* output_data_base = output.data<scalar_t>();\n+  scalar_t* gradOutput_data_base = grad.data<scalar_t>();\n+  int64_t i, d;\n+\n+#pragma omp parallel for private(i, d)\n+  for (i = 0; i < (outer_size * inner_size); i++) {\n+    int64_t outer_idx = i / inner_size;\n+    int64_t inner_idx = i % inner_size;\n+    scalar_t* gradInput_data =\n+        gradInput_data_base + outer_idx * outer_stride + inner_idx;\n+    scalar_t* output_data =\n+        output_data_base + outer_idx * outer_stride + inner_idx;\n+    const scalar_t* gradOutput_data =\n+        gradOutput_data_base + outer_idx * outer_stride + inner_idx;\n+\n+    scalar_t sum = 0; // TODO was accreal here\n+  for (int ii = 0; ii < dim_size; ii++) {\n+  }\n+    for (d = 0; d <  dim_size; d++)\n+      if (LogSoftMax)\n+        sum += gradOutput_data[d * dim_stride];\n+      else\n+        sum += gradOutput_data[d * dim_stride] * output_data[d * dim_stride];\n+\n+\n+    for (d = 0; d < dim_size; d++) {\n+      if (LogSoftMax) {\n+        gradInput_data[d * dim_stride] = gradOutput_data[d * dim_stride] -\n+            std::exp(output_data[d * dim_stride]) * sum;\n+      } else {\n+        gradInput_data[d * dim_stride] = output_data[d * dim_stride] *\n+            (gradOutput_data[d * dim_stride] - sum);\n+      }\n+    }\n+  }\n+}\n+} // namespace\n+\n+Tensor softmax_cpu(const Tensor& input_, const int64_t dim_) {\n+  auto input = input_.contiguous();\n+  Tensor output = at::native::empty_like(input);\n+  Tensor output2 = at::native::empty_like(input);\n+  int64_t dim = maybe_wrap_dim(dim_, input.dim());\n+  if (input.dim() == 0)\n+    input = input.view(1);\n+  AT_CHECK(\n+      dim >= 0 && dim < input.dim(),\n+      \"dim must be non-negative and less than input dimensions\");\n+  if (input.ndimension()  > 0 && dim == input.ndimension() - 1) {\n+    softmax_lastdim_kernel(output, input);\n+  } else {\n+    AT_DISPATCH_FLOATING_TYPES(input.type(), \"softmax\", [&] {\n+      host_softmax<scalar_t, false>(output, input, dim);\n+    });\n+  }\n+  return output;\n+}\n+\n+Tensor log_softmax_cpu(const Tensor& input_, const int64_t dim_) {", "path": "aten/src/ATen/native/Activation.cpp", "position": null, "original_position": 188, "commit_id": "b269b30289cf014a9bc3ce4924567ecb035a5fe1", "original_commit_id": "86cd5d1daee4eb5a2258769a99725d967b289dd1", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "body": "Sure, it's only tricky for log_softmax since that needs to use another layer of looping to make use of the vectorization of .log. We can't call std::log from avx2 code because of the glibc bug.", "created_at": "2018-05-10T15:05:00Z", "updated_at": "2018-11-23T15:43:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187358704", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7375", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187358704"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7375#discussion_r187358704"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7375"}}, "body_html": "<p>Sure, it's only tricky for log_softmax since that needs to use another layer of looping to make use of the vectorization of .log. We can't call std::log from avx2 code because of the glibc bug.</p>", "body_text": "Sure, it's only tricky for log_softmax since that needs to use another layer of looping to make use of the vectorization of .log. We can't call std::log from avx2 code because of the glibc bug.", "in_reply_to_id": 187350454}