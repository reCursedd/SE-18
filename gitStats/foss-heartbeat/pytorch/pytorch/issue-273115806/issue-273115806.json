{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3638", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3638/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3638/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3638/events", "html_url": "https://github.com/pytorch/pytorch/issues/3638", "id": 273115806, "node_id": "MDU6SXNzdWUyNzMxMTU4MDY=", "number": 3638, "title": "EmbeddingBag inconsistent behaviour. Expects \"Variable\" even though I give it that...", "user": {"login": "zafarali", "id": 6295292, "node_id": "MDQ6VXNlcjYyOTUyOTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/6295292?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zafarali", "html_url": "https://github.com/zafarali", "followers_url": "https://api.github.com/users/zafarali/followers", "following_url": "https://api.github.com/users/zafarali/following{/other_user}", "gists_url": "https://api.github.com/users/zafarali/gists{/gist_id}", "starred_url": "https://api.github.com/users/zafarali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zafarali/subscriptions", "organizations_url": "https://api.github.com/users/zafarali/orgs", "repos_url": "https://api.github.com/users/zafarali/repos", "events_url": "https://api.github.com/users/zafarali/events{/privacy}", "received_events_url": "https://api.github.com/users/zafarali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-11T03:39:53Z", "updated_at": "2017-11-11T20:46:36Z", "closed_at": "2017-11-11T20:46:36Z", "author_association": "NONE", "body_html": "<p>I am trying to use <code>EmbeddingBag</code>. I just started using pytorch so I might have some knowledge gap, but I think there is an inconsistency in the way <code>EmbeddingBag</code> and <code>Embedding</code> is handled.</p>\n<p>I set up my embedding layers as follows:</p>\n<pre><code>import numpy as np\nimport torch\nfrom torch.autograd import Variable\nn_words = 4\nembed_dim = 2\nencoder_embedding_bag = torch.nn.EmbeddingBag(n_words, embed_dim, mode='sum')\nencoder_embedding = torch.nn.Embedding(n_words, embed_dim)\n</code></pre>\n<p>And i set up some \"words\" in the standard <code>BxN</code> way that torch requires inputs:</p>\n<pre><code>words_from_numpy = Variable(torch.from_numpy(np.array([[0, 1, 2, 3]])))\n</code></pre>\n<p>When I do the following, things work as expected:</p>\n<pre><code>torch.sum(encoder_embedding(words_from_numpy), dim=1)\n</code></pre>\n<p>and I get a \"Variable\" containing a <code>torch.FloatTensor</code> of size 1x2.</p>\n<p>However, when I do</p>\n<pre><code>encoder_embedding_bag(words_from_numpy, None)\n</code></pre>\n<p>I get an error <code>RuntimeError: expected a Variable argument, but got torch.LongTensor</code>:</p>\n<pre><code>.local/lib/python2.7/site-packages/torch/nn/modules/sparse.pyc in forward(self, input, offsets)\n    209             self.max_norm, self.norm_type,\n    210             self.scale_grad_by_freq, mode=self.mode\n--&gt; 211         )(self.weight, input, offsets)\n    212 \n    213     def __repr__(self):\n</code></pre>\n<p>I have been trying to trace the issue back to the backend but i'm having little luck due to my relatively new status with the code setup. Any help with debugging or am I making a logical mistake?</p>", "body_text": "I am trying to use EmbeddingBag. I just started using pytorch so I might have some knowledge gap, but I think there is an inconsistency in the way EmbeddingBag and Embedding is handled.\nI set up my embedding layers as follows:\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nn_words = 4\nembed_dim = 2\nencoder_embedding_bag = torch.nn.EmbeddingBag(n_words, embed_dim, mode='sum')\nencoder_embedding = torch.nn.Embedding(n_words, embed_dim)\n\nAnd i set up some \"words\" in the standard BxN way that torch requires inputs:\nwords_from_numpy = Variable(torch.from_numpy(np.array([[0, 1, 2, 3]])))\n\nWhen I do the following, things work as expected:\ntorch.sum(encoder_embedding(words_from_numpy), dim=1)\n\nand I get a \"Variable\" containing a torch.FloatTensor of size 1x2.\nHowever, when I do\nencoder_embedding_bag(words_from_numpy, None)\n\nI get an error RuntimeError: expected a Variable argument, but got torch.LongTensor:\n.local/lib/python2.7/site-packages/torch/nn/modules/sparse.pyc in forward(self, input, offsets)\n    209             self.max_norm, self.norm_type,\n    210             self.scale_grad_by_freq, mode=self.mode\n--> 211         )(self.weight, input, offsets)\n    212 \n    213     def __repr__(self):\n\nI have been trying to trace the issue back to the backend but i'm having little luck due to my relatively new status with the code setup. Any help with debugging or am I making a logical mistake?", "body": "I am trying to use `EmbeddingBag`. I just started using pytorch so I might have some knowledge gap, but I think there is an inconsistency in the way `EmbeddingBag` and `Embedding` is handled.\r\n\r\nI set up my embedding layers as follows:\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\nn_words = 4\r\nembed_dim = 2\r\nencoder_embedding_bag = torch.nn.EmbeddingBag(n_words, embed_dim, mode='sum')\r\nencoder_embedding = torch.nn.Embedding(n_words, embed_dim)\r\n```\r\n\r\nAnd i set up some \"words\" in the standard `BxN` way that torch requires inputs:\r\n\r\n```\r\nwords_from_numpy = Variable(torch.from_numpy(np.array([[0, 1, 2, 3]])))\r\n```\r\n\r\nWhen I do the following, things work as expected:\r\n\r\n```\r\ntorch.sum(encoder_embedding(words_from_numpy), dim=1)\r\n```\r\n\r\nand I get a \"Variable\" containing a `torch.FloatTensor` of size 1x2.\r\n\r\nHowever, when I do\r\n\r\n```\r\nencoder_embedding_bag(words_from_numpy, None)\r\n```\r\n\r\nI get an error `RuntimeError: expected a Variable argument, but got torch.LongTensor`:\r\n\r\n```\r\n.local/lib/python2.7/site-packages/torch/nn/modules/sparse.pyc in forward(self, input, offsets)\r\n    209             self.max_norm, self.norm_type,\r\n    210             self.scale_grad_by_freq, mode=self.mode\r\n--> 211         )(self.weight, input, offsets)\r\n    212 \r\n    213     def __repr__(self):\r\n```\r\n\r\nI have been trying to trace the issue back to the backend but i'm having little luck due to my relatively new status with the code setup. Any help with debugging or am I making a logical mistake?"}