{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11118", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11118/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11118/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11118/events", "html_url": "https://github.com/pytorch/pytorch/issues/11118", "id": 355789459, "node_id": "MDU6SXNzdWUzNTU3ODk0NTk=", "number": 11118, "title": "Some pointwise functions are not getting fused by fusion compiler", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-30T23:29:32Z", "updated_at": "2018-09-04T02:23:54Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li>log. It is currently in the list of fusable operations, but since there's no autodiff for it, if input requires grad log is not getting fused</li>\n<li>clamp  - not in the list of fusable operations, and no autodiff</li>\n<li>threshold - not in the list of fusable operations, and no autodiff. Since nn.ReLU is implemented via threshold, it means that networks that use nn.ReLU cannot fuse it</li>\n<li>relu (functional) is getting fused in forward, but is only half fused in backward (the comparison op is outside of fusion block), e.g. graph fuser pass gets the following graph for add-relu backward:</li>\n</ol>\n<pre><code>(%0 : Float(*, *)\n      %1 : Float(*, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %4 : int = prim::Constant[value=0]()\n  %5 : Byte(*, *) = aten::gt(%1, %4)\n  %6 : Float(*, *) = aten::type_as(%5, %1)\n  %7 : Float(*, *) = aten::mul(%0, %6)\n  %10 : Float(*, *) = aten::mul(%7, %2)\n  return (%7, %10);\n}\n</code></pre>\n<p>and returns the following (with gt outside of fusion group):</p>\n<pre><code>fusergraph(%0 : Float(*, *)\n      %1 : Float(*, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %4 : int = prim::Constant[value=0]()\n  %5 : Byte(*, *) = aten::gt(%1, %4)\n  %11 : Float(*, *), %12 : Float(*, *) = prim::FusionGroup_0[device=0](%0, %5, %1)\n  return (%12, %11);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)\n      %6 : Byte(*, *)\n      %7 : Float(*, *)) {\n  %8 : Float(*, *) = aten::type_as(%6, %7)\n  %5 : Float(*, *) = aten::mul(%3, %8)\n  %1 : int = prim::Constant[value=1]()\n  %2 : Float(*, *) = aten::mul(%5, %1)\n  return (%2, %5);\n}\n</code></pre>", "body_text": "log. It is currently in the list of fusable operations, but since there's no autodiff for it, if input requires grad log is not getting fused\nclamp  - not in the list of fusable operations, and no autodiff\nthreshold - not in the list of fusable operations, and no autodiff. Since nn.ReLU is implemented via threshold, it means that networks that use nn.ReLU cannot fuse it\nrelu (functional) is getting fused in forward, but is only half fused in backward (the comparison op is outside of fusion block), e.g. graph fuser pass gets the following graph for add-relu backward:\n\n(%0 : Float(*, *)\n      %1 : Float(*, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %4 : int = prim::Constant[value=0]()\n  %5 : Byte(*, *) = aten::gt(%1, %4)\n  %6 : Float(*, *) = aten::type_as(%5, %1)\n  %7 : Float(*, *) = aten::mul(%0, %6)\n  %10 : Float(*, *) = aten::mul(%7, %2)\n  return (%7, %10);\n}\n\nand returns the following (with gt outside of fusion group):\nfusergraph(%0 : Float(*, *)\n      %1 : Float(*, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %4 : int = prim::Constant[value=0]()\n  %5 : Byte(*, *) = aten::gt(%1, %4)\n  %11 : Float(*, *), %12 : Float(*, *) = prim::FusionGroup_0[device=0](%0, %5, %1)\n  return (%12, %11);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)\n      %6 : Byte(*, *)\n      %7 : Float(*, *)) {\n  %8 : Float(*, *) = aten::type_as(%6, %7)\n  %5 : Float(*, *) = aten::mul(%3, %8)\n  %1 : int = prim::Constant[value=1]()\n  %2 : Float(*, *) = aten::mul(%5, %1)\n  return (%2, %5);\n}", "body": "1) log. It is currently in the list of fusable operations, but since there's no autodiff for it, if input requires grad log is not getting fused\r\n2) clamp  - not in the list of fusable operations, and no autodiff\r\n3) threshold - not in the list of fusable operations, and no autodiff. Since nn.ReLU is implemented via threshold, it means that networks that use nn.ReLU cannot fuse it\r\n4) relu (functional) is getting fused in forward, but is only half fused in backward (the comparison op is outside of fusion block), e.g. graph fuser pass gets the following graph for add-relu backward:\r\n```\r\n(%0 : Float(*, *)\r\n      %1 : Float(*, *)) {\r\n  %2 : int = prim::Constant[value=1]()\r\n  %4 : int = prim::Constant[value=0]()\r\n  %5 : Byte(*, *) = aten::gt(%1, %4)\r\n  %6 : Float(*, *) = aten::type_as(%5, %1)\r\n  %7 : Float(*, *) = aten::mul(%0, %6)\r\n  %10 : Float(*, *) = aten::mul(%7, %2)\r\n  return (%7, %10);\r\n}\r\n```\r\nand returns the following (with gt outside of fusion group):\r\n```\r\nfusergraph(%0 : Float(*, *)\r\n      %1 : Float(*, *)) {\r\n  %2 : int = prim::Constant[value=1]()\r\n  %4 : int = prim::Constant[value=0]()\r\n  %5 : Byte(*, *) = aten::gt(%1, %4)\r\n  %11 : Float(*, *), %12 : Float(*, *) = prim::FusionGroup_0[device=0](%0, %5, %1)\r\n  return (%12, %11);\r\n}\r\nwith prim::FusionGroup_0 = graph(%3 : Float(*, *)\r\n      %6 : Byte(*, *)\r\n      %7 : Float(*, *)) {\r\n  %8 : Float(*, *) = aten::type_as(%6, %7)\r\n  %5 : Float(*, *) = aten::mul(%3, %8)\r\n  %1 : int = prim::Constant[value=1]()\r\n  %2 : Float(*, *) = aten::mul(%5, %1)\r\n  return (%2, %5);\r\n}\r\n```\r\n"}