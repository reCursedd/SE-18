{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5102", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5102/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5102/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5102/events", "html_url": "https://github.com/pytorch/pytorch/issues/5102", "id": 295037963, "node_id": "MDU6SXNzdWUyOTUwMzc5NjM=", "number": 5102, "title": "crash when backproping", "user": {"login": "duburlan", "id": 4682236, "node_id": "MDQ6VXNlcjQ2ODIyMzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/4682236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/duburlan", "html_url": "https://github.com/duburlan", "followers_url": "https://api.github.com/users/duburlan/followers", "following_url": "https://api.github.com/users/duburlan/following{/other_user}", "gists_url": "https://api.github.com/users/duburlan/gists{/gist_id}", "starred_url": "https://api.github.com/users/duburlan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/duburlan/subscriptions", "organizations_url": "https://api.github.com/users/duburlan/orgs", "repos_url": "https://api.github.com/users/duburlan/repos", "events_url": "https://api.github.com/users/duburlan/events{/privacy}", "received_events_url": "https://api.github.com/users/duburlan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-07T07:51:11Z", "updated_at": "2018-02-08T09:12:34Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I encountered this problem when was trying to concatenate features from the DenseNet after the average pooling layer and before the fully connected layer. Then I reduced the code into a minimal example reproducing the bug. The issue is when I'm calling the <code>backward</code> method of the loss variable it crashes. This is the example:</p>\n<pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ntorch.manual_seed(7)\n\nb = 4\nch = 3\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(ch)\n        self.affine = nn.Linear(ch + 1, 2)\n        \n    def forward(self, x, f):\n        x = self.bn(x)\n        x = F.avg_pool2d(x, (x.size(2), x.size(3)))\n        x = x.view(x.size(0), -1)\n        print('x:\\n',x)\n        print('f:\\n',f)\n        x = torch.cat([x, f], -1)\n        print('x(cat):\\n',x)\n        out = self.affine(x)\n        return out\n\nmodel = Model()\ncriterion = nn.CrossEntropyLoss()\n\nmodel.cuda().train()\n\nx = torch.FloatTensor(b, ch, 32, 32).uniform_()\nf = torch.FloatTensor(b).uniform_()\nx = Variable(x.float()).cuda()\nf = Variable(f).cuda()\n\ny = torch.LongTensor(np.random.randint(0, 2, b))\ny = Variable(y).cuda()\n\nout = model(x, f)\nprint('out:\\n%s' % out)\nloss = criterion(out, y)\nloss.backward()  # crash\n</code></pre>\n<p>The output would be:</p>\n<pre><code>x:\n Variable containing:\n1.00000e-02 *\n  0.8092 -0.0593  2.8019\n  1.6246  0.6555 -1.3738\n -2.9738 -0.8876 -1.0771\n  0.5400  0.2914 -0.3510\n[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\n\nf:\n Variable containing:\n 0.0835\n 0.6537\n 0.9912\n 0.2337\n[torch.cuda.FloatTensor of size 4 (GPU 0)]\n\nx(cat):\n Variable containing:\n 0.0081 -0.0006  0.0280  0.0835\n 0.0162  0.0066 -0.0137  0.6537\n-0.0297 -0.0089 -0.0108  0.9912\n 0.0054  0.0029 -0.0035  0.2337\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n\nout:\nVariable containing:\n 0.0258 -0.1725\n-0.1387  0.0153\n-0.2409  0.1322\n-0.0172 -0.1311\n[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-20-60a5fb918fc7&gt; in &lt;module&gt;()\n     47 print('out:\\n%s' % out)\n     48 loss = criterion(out, y)\n---&gt; 49 loss.backward()\n     50 print('grad:\\n',model.affine.weight.grad)\n\n~/projects/kaggle_camera/.env/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    165                 Variable.\n    166         \"\"\"\n--&gt; 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    168 \n    169     def register_hook(self, hook):\n\n~/projects/kaggle_camera/.env/lib/python3.5/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     97 \n     98     Variable._execution_engine.run_backward(\n---&gt; 99         variables, grad_variables, retain_graph)\n    100 \n    101 \n\nRuntimeError: invalid argument 5: out of range at /pytorch/torch/lib/THC/generic/THCTensor.c:451\n</code></pre>\n<p>However, if you disable the batchnorm layer (comment out <code>x = self.bn(x)</code> in <code>forward</code>) it doesn't crash.</p>\n<p>There workaround is to reshape the <code>f</code> variable to be 2-dimensional. Fix the line <code>f = torch.FloatTensor(b).uniform_()</code> to be <code>f = torch.FloatTensor(b, 1).uniform_()</code> and it doesn't crash. The output would be:</p>\n<pre><code>x:\n Variable containing:\n1.00000e-02 *\n  0.8092 -0.0593  2.8019\n  1.6246  0.6555 -1.3738\n -2.9738 -0.8876 -1.0771\n  0.5400  0.2914 -0.3510\n[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\n\nf:\n Variable containing:\n 0.0835\n 0.6537\n 0.9912\n 0.2337\n[torch.cuda.FloatTensor of size 4x1 (GPU 0)]\n\nx(cat):\n Variable containing:\n 0.0081 -0.0006  0.0280  0.0835\n 0.0162  0.0066 -0.0137  0.6537\n-0.0297 -0.0089 -0.0108  0.9912\n 0.0054  0.0029 -0.0035  0.2337\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n\nout:\nVariable containing:\n 0.0258 -0.1725\n-0.1387  0.0153\n-0.2409  0.1322\n-0.0172 -0.1311\n[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\n\ngrad:\n Variable containing:\n1.00000e-03 *\n -4.7437 -2.2122  5.0134 -3.0221\n  4.7437  2.2122 -5.0134  3.0222\n[torch.cuda.FloatTensor of size 2x4 (GPU 0)]\n</code></pre>\n<p>It doesn't depend whether you run it on GPU or CPU. Seems like a problem with the batchnorm layer.</p>\n<pre><code>OS Ubuntu 16.04.3 LTS\nPyTorch 0.3.0.post4 / pip\nPython 3.5.2\nCuda 8.0/cuDNN 6.0.21\nGeForce GTX 1070\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\n</code></pre>", "body_text": "I encountered this problem when was trying to concatenate features from the DenseNet after the average pooling layer and before the fully connected layer. Then I reduced the code into a minimal example reproducing the bug. The issue is when I'm calling the backward method of the loss variable it crashes. This is the example:\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ntorch.manual_seed(7)\n\nb = 4\nch = 3\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(ch)\n        self.affine = nn.Linear(ch + 1, 2)\n        \n    def forward(self, x, f):\n        x = self.bn(x)\n        x = F.avg_pool2d(x, (x.size(2), x.size(3)))\n        x = x.view(x.size(0), -1)\n        print('x:\\n',x)\n        print('f:\\n',f)\n        x = torch.cat([x, f], -1)\n        print('x(cat):\\n',x)\n        out = self.affine(x)\n        return out\n\nmodel = Model()\ncriterion = nn.CrossEntropyLoss()\n\nmodel.cuda().train()\n\nx = torch.FloatTensor(b, ch, 32, 32).uniform_()\nf = torch.FloatTensor(b).uniform_()\nx = Variable(x.float()).cuda()\nf = Variable(f).cuda()\n\ny = torch.LongTensor(np.random.randint(0, 2, b))\ny = Variable(y).cuda()\n\nout = model(x, f)\nprint('out:\\n%s' % out)\nloss = criterion(out, y)\nloss.backward()  # crash\n\nThe output would be:\nx:\n Variable containing:\n1.00000e-02 *\n  0.8092 -0.0593  2.8019\n  1.6246  0.6555 -1.3738\n -2.9738 -0.8876 -1.0771\n  0.5400  0.2914 -0.3510\n[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\n\nf:\n Variable containing:\n 0.0835\n 0.6537\n 0.9912\n 0.2337\n[torch.cuda.FloatTensor of size 4 (GPU 0)]\n\nx(cat):\n Variable containing:\n 0.0081 -0.0006  0.0280  0.0835\n 0.0162  0.0066 -0.0137  0.6537\n-0.0297 -0.0089 -0.0108  0.9912\n 0.0054  0.0029 -0.0035  0.2337\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n\nout:\nVariable containing:\n 0.0258 -0.1725\n-0.1387  0.0153\n-0.2409  0.1322\n-0.0172 -0.1311\n[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-20-60a5fb918fc7> in <module>()\n     47 print('out:\\n%s' % out)\n     48 loss = criterion(out, y)\n---> 49 loss.backward()\n     50 print('grad:\\n',model.affine.weight.grad)\n\n~/projects/kaggle_camera/.env/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    165                 Variable.\n    166         \"\"\"\n--> 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    168 \n    169     def register_hook(self, hook):\n\n~/projects/kaggle_camera/.env/lib/python3.5/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     97 \n     98     Variable._execution_engine.run_backward(\n---> 99         variables, grad_variables, retain_graph)\n    100 \n    101 \n\nRuntimeError: invalid argument 5: out of range at /pytorch/torch/lib/THC/generic/THCTensor.c:451\n\nHowever, if you disable the batchnorm layer (comment out x = self.bn(x) in forward) it doesn't crash.\nThere workaround is to reshape the f variable to be 2-dimensional. Fix the line f = torch.FloatTensor(b).uniform_() to be f = torch.FloatTensor(b, 1).uniform_() and it doesn't crash. The output would be:\nx:\n Variable containing:\n1.00000e-02 *\n  0.8092 -0.0593  2.8019\n  1.6246  0.6555 -1.3738\n -2.9738 -0.8876 -1.0771\n  0.5400  0.2914 -0.3510\n[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\n\nf:\n Variable containing:\n 0.0835\n 0.6537\n 0.9912\n 0.2337\n[torch.cuda.FloatTensor of size 4x1 (GPU 0)]\n\nx(cat):\n Variable containing:\n 0.0081 -0.0006  0.0280  0.0835\n 0.0162  0.0066 -0.0137  0.6537\n-0.0297 -0.0089 -0.0108  0.9912\n 0.0054  0.0029 -0.0035  0.2337\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n\nout:\nVariable containing:\n 0.0258 -0.1725\n-0.1387  0.0153\n-0.2409  0.1322\n-0.0172 -0.1311\n[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\n\ngrad:\n Variable containing:\n1.00000e-03 *\n -4.7437 -2.2122  5.0134 -3.0221\n  4.7437  2.2122 -5.0134  3.0222\n[torch.cuda.FloatTensor of size 2x4 (GPU 0)]\n\nIt doesn't depend whether you run it on GPU or CPU. Seems like a problem with the batchnorm layer.\nOS Ubuntu 16.04.3 LTS\nPyTorch 0.3.0.post4 / pip\nPython 3.5.2\nCuda 8.0/cuDNN 6.0.21\nGeForce GTX 1070\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)", "body": "I encountered this problem when was trying to concatenate features from the DenseNet after the average pooling layer and before the fully connected layer. Then I reduced the code into a minimal example reproducing the bug. The issue is when I'm calling the `backward` method of the loss variable it crashes. This is the example:\r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\ntorch.manual_seed(7)\r\n\r\nb = 4\r\nch = 3\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.bn = nn.BatchNorm2d(ch)\r\n        self.affine = nn.Linear(ch + 1, 2)\r\n        \r\n    def forward(self, x, f):\r\n        x = self.bn(x)\r\n        x = F.avg_pool2d(x, (x.size(2), x.size(3)))\r\n        x = x.view(x.size(0), -1)\r\n        print('x:\\n',x)\r\n        print('f:\\n',f)\r\n        x = torch.cat([x, f], -1)\r\n        print('x(cat):\\n',x)\r\n        out = self.affine(x)\r\n        return out\r\n\r\nmodel = Model()\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\nmodel.cuda().train()\r\n\r\nx = torch.FloatTensor(b, ch, 32, 32).uniform_()\r\nf = torch.FloatTensor(b).uniform_()\r\nx = Variable(x.float()).cuda()\r\nf = Variable(f).cuda()\r\n\r\ny = torch.LongTensor(np.random.randint(0, 2, b))\r\ny = Variable(y).cuda()\r\n\r\nout = model(x, f)\r\nprint('out:\\n%s' % out)\r\nloss = criterion(out, y)\r\nloss.backward()  # crash\r\n```\r\n\r\nThe output would be:\r\n```\r\nx:\r\n Variable containing:\r\n1.00000e-02 *\r\n  0.8092 -0.0593  2.8019\r\n  1.6246  0.6555 -1.3738\r\n -2.9738 -0.8876 -1.0771\r\n  0.5400  0.2914 -0.3510\r\n[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\r\n\r\nf:\r\n Variable containing:\r\n 0.0835\r\n 0.6537\r\n 0.9912\r\n 0.2337\r\n[torch.cuda.FloatTensor of size 4 (GPU 0)]\r\n\r\nx(cat):\r\n Variable containing:\r\n 0.0081 -0.0006  0.0280  0.0835\r\n 0.0162  0.0066 -0.0137  0.6537\r\n-0.0297 -0.0089 -0.0108  0.9912\r\n 0.0054  0.0029 -0.0035  0.2337\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n\r\nout:\r\nVariable containing:\r\n 0.0258 -0.1725\r\n-0.1387  0.0153\r\n-0.2409  0.1322\r\n-0.0172 -0.1311\r\n[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-20-60a5fb918fc7> in <module>()\r\n     47 print('out:\\n%s' % out)\r\n     48 loss = criterion(out, y)\r\n---> 49 loss.backward()\r\n     50 print('grad:\\n',model.affine.weight.grad)\r\n\r\n~/projects/kaggle_camera/.env/lib/python3.5/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    165                 Variable.\r\n    166         \"\"\"\r\n--> 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    168 \r\n    169     def register_hook(self, hook):\r\n\r\n~/projects/kaggle_camera/.env/lib/python3.5/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\r\n     97 \r\n     98     Variable._execution_engine.run_backward(\r\n---> 99         variables, grad_variables, retain_graph)\r\n    100 \r\n    101 \r\n\r\nRuntimeError: invalid argument 5: out of range at /pytorch/torch/lib/THC/generic/THCTensor.c:451\r\n```\r\n\r\nHowever, if you disable the batchnorm layer (comment out `x = self.bn(x)` in `forward`) it doesn't crash.\r\n\r\nThere workaround is to reshape the `f` variable to be 2-dimensional. Fix the line `f = torch.FloatTensor(b).uniform_()` to be `f = torch.FloatTensor(b, 1).uniform_()` and it doesn't crash. The output would be:\r\n```\r\nx:\r\n Variable containing:\r\n1.00000e-02 *\r\n  0.8092 -0.0593  2.8019\r\n  1.6246  0.6555 -1.3738\r\n -2.9738 -0.8876 -1.0771\r\n  0.5400  0.2914 -0.3510\r\n[torch.cuda.FloatTensor of size 4x3 (GPU 0)]\r\n\r\nf:\r\n Variable containing:\r\n 0.0835\r\n 0.6537\r\n 0.9912\r\n 0.2337\r\n[torch.cuda.FloatTensor of size 4x1 (GPU 0)]\r\n\r\nx(cat):\r\n Variable containing:\r\n 0.0081 -0.0006  0.0280  0.0835\r\n 0.0162  0.0066 -0.0137  0.6537\r\n-0.0297 -0.0089 -0.0108  0.9912\r\n 0.0054  0.0029 -0.0035  0.2337\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n\r\nout:\r\nVariable containing:\r\n 0.0258 -0.1725\r\n-0.1387  0.0153\r\n-0.2409  0.1322\r\n-0.0172 -0.1311\r\n[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\r\n\r\ngrad:\r\n Variable containing:\r\n1.00000e-03 *\r\n -4.7437 -2.2122  5.0134 -3.0221\r\n  4.7437  2.2122 -5.0134  3.0222\r\n[torch.cuda.FloatTensor of size 2x4 (GPU 0)]\r\n```\r\nIt doesn't depend whether you run it on GPU or CPU. Seems like a problem with the batchnorm layer.\r\n\r\n```\r\nOS Ubuntu 16.04.3 LTS\r\nPyTorch 0.3.0.post4 / pip\r\nPython 3.5.2\r\nCuda 8.0/cuDNN 6.0.21\r\nGeForce GTX 1070\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\r\n```"}