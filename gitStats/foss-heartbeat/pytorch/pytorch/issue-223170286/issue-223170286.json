{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1310", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1310/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1310/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1310/events", "html_url": "https://github.com/pytorch/pytorch/issues/1310", "id": 223170286, "node_id": "MDU6SXNzdWUyMjMxNzAyODY=", "number": 1310, "title": "Issue with torch.max() over dim 2", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-04-20T19:15:05Z", "updated_at": "2017-04-20T23:01:14Z", "closed_at": "2017-04-20T23:01:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When operating over tensor with 4 dimensions, the index returned by <code>torch.max()</code> are wrong and can go beyond the size of the dimension that was reduced over.</p>\n<p>Surprinsingly, this doesn't happen for other number of dimensions</p>\n<p>Code for demonstrating:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is fine and works as expected</span>\nt_2d <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">25</span>)\nmax_val_2d, max_idxs_2d <span class=\"pl-k\">=</span> torch.max(t_2d, <span class=\"pl-c1\">0</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> max_val_2d / max_idxs_2d is of size 1 x 25 -&gt; fine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The value of max_idxs_2d go from 0 to 4    -&gt; fine</span>\n<span class=\"pl-k\">assert</span>(max_idxs_2d.max() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is fine and works as expected</span>\nt_3d <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">26</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">25</span>)\nmax_val_3d, max_idxs_3d <span class=\"pl-k\">=</span> torch.max(t_3d, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> max_val_3d / max_idxs_3d is of size 26 x 1 x 25 -&gt; fine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The value of max_idxs_3d go from 0 to 4         -&gt; fine</span>\n<span class=\"pl-k\">assert</span>(max_idxs_3d.max() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is fine and works as expected</span>\nt_5d <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">26</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">25</span>)\nmax_val_5d, max_idxs_5d <span class=\"pl-k\">=</span> torch.max(t_5d, <span class=\"pl-c1\">3</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> max_val_5d / max_idxs_5d is of size 1 x 1 x 26 x 1 x 25 -&gt; fine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The value of max_idxs_5d go from 0 to 4                 -&gt; fine</span>\n<span class=\"pl-k\">assert</span>(max_idxs_5d.max() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is fine and works as expected</span>\nt_6d <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">26</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">25</span>)\nmax_val_6d, max_idxs_6d <span class=\"pl-k\">=</span> torch.max(t_6d, <span class=\"pl-c1\">4</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> max_val_6d / max_idxs_6d is of size 1 x 1 x 1 x 26 x 1 x 25 -&gt; fine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The value of max_idxs_6d go from 0 to 4                     -&gt; fine</span>\n<span class=\"pl-k\">assert</span>(max_idxs_6d.max() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is fine and works as expected</span>\nt_7d <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">26</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">25</span>)\nmax_val_7d, max_idxs_7d <span class=\"pl-k\">=</span> torch.max(t_7d, <span class=\"pl-c1\">5</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> max_val_7d / max_idxs_7d is of size 1 x 1 x 1 x 1 x 26 x 1 x 25 -&gt; fine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The value of max_idxs_7d go from 0 to 4                         -&gt; fine</span>\n<span class=\"pl-k\">assert</span>(max_idxs_7d.max() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>)\n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is not fine</span>\nt_4d <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">26</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">25</span>)\nmax_val_4d, max_idxs_4d <span class=\"pl-k\">=</span> torch.max(t_4d, <span class=\"pl-c1\">2</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> max_val_4d, max_idxs_4d is of size 1 x 26 x 1 x 25 -&gt; fine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The value of max_idxs_4d go from 0 to 24           -&gt; ???</span>\n<span class=\"pl-c1\">print</span>(max_idxs_4d)\n<span class=\"pl-k\">assert</span>(max_idxs_4d.max() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>)\n\n</pre></div>\n<p>Can you confirm this is a bug and not a misunderstanding on my part of what <code>torch.max()</code> should return?</p>", "body_text": "When operating over tensor with 4 dimensions, the index returned by torch.max() are wrong and can go beyond the size of the dimension that was reduced over.\nSurprinsingly, this doesn't happen for other number of dimensions\nCode for demonstrating:\nimport torch\nfrom torch.autograd import Variable\n\n# This is fine and works as expected\nt_2d = torch.randn(5, 25)\nmax_val_2d, max_idxs_2d = torch.max(t_2d, 0)\n# max_val_2d / max_idxs_2d is of size 1 x 25 -> fine\n# The value of max_idxs_2d go from 0 to 4    -> fine\nassert(max_idxs_2d.max() < 5)\n\n# This is fine and works as expected\nt_3d = torch.randn(26, 5, 25)\nmax_val_3d, max_idxs_3d = torch.max(t_3d, 1)\n# max_val_3d / max_idxs_3d is of size 26 x 1 x 25 -> fine\n# The value of max_idxs_3d go from 0 to 4         -> fine\nassert(max_idxs_3d.max() < 5)\n\n# This is fine and works as expected\nt_5d = torch.randn(1, 1, 26, 5, 25)\nmax_val_5d, max_idxs_5d = torch.max(t_5d, 3)\n# max_val_5d / max_idxs_5d is of size 1 x 1 x 26 x 1 x 25 -> fine\n# The value of max_idxs_5d go from 0 to 4                 -> fine\nassert(max_idxs_5d.max() < 5)\n\n# This is fine and works as expected\nt_6d = torch.randn(1, 1, 1, 26, 5, 25)\nmax_val_6d, max_idxs_6d = torch.max(t_6d, 4)\n# max_val_6d / max_idxs_6d is of size 1 x 1 x 1 x 26 x 1 x 25 -> fine\n# The value of max_idxs_6d go from 0 to 4                     -> fine\nassert(max_idxs_6d.max() < 5)\n\n# This is fine and works as expected\nt_7d = torch.randn(1, 1, 1, 1, 26, 5, 25)\nmax_val_7d, max_idxs_7d = torch.max(t_7d, 5)\n# max_val_7d / max_idxs_7d is of size 1 x 1 x 1 x 1 x 26 x 1 x 25 -> fine\n# The value of max_idxs_7d go from 0 to 4                         -> fine\nassert(max_idxs_7d.max() < 5)\n\n\n\n# This is not fine\nt_4d = torch.randn(1, 26, 5, 25)\nmax_val_4d, max_idxs_4d = torch.max(t_4d, 2)\n# max_val_4d, max_idxs_4d is of size 1 x 26 x 1 x 25 -> fine\n# The value of max_idxs_4d go from 0 to 24           -> ???\nprint(max_idxs_4d)\nassert(max_idxs_4d.max() < 5)\n\n\nCan you confirm this is a bug and not a misunderstanding on my part of what torch.max() should return?", "body": "When operating over tensor with 4 dimensions, the index returned by `torch.max()` are wrong and can go beyond the size of the dimension that was reduced over.\r\n\r\nSurprinsingly, this doesn't happen for other number of dimensions\r\n\r\nCode for demonstrating:\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n# This is fine and works as expected\r\nt_2d = torch.randn(5, 25)\r\nmax_val_2d, max_idxs_2d = torch.max(t_2d, 0)\r\n# max_val_2d / max_idxs_2d is of size 1 x 25 -> fine\r\n# The value of max_idxs_2d go from 0 to 4    -> fine\r\nassert(max_idxs_2d.max() < 5)\r\n\r\n# This is fine and works as expected\r\nt_3d = torch.randn(26, 5, 25)\r\nmax_val_3d, max_idxs_3d = torch.max(t_3d, 1)\r\n# max_val_3d / max_idxs_3d is of size 26 x 1 x 25 -> fine\r\n# The value of max_idxs_3d go from 0 to 4         -> fine\r\nassert(max_idxs_3d.max() < 5)\r\n\r\n# This is fine and works as expected\r\nt_5d = torch.randn(1, 1, 26, 5, 25)\r\nmax_val_5d, max_idxs_5d = torch.max(t_5d, 3)\r\n# max_val_5d / max_idxs_5d is of size 1 x 1 x 26 x 1 x 25 -> fine\r\n# The value of max_idxs_5d go from 0 to 4                 -> fine\r\nassert(max_idxs_5d.max() < 5)\r\n\r\n# This is fine and works as expected\r\nt_6d = torch.randn(1, 1, 1, 26, 5, 25)\r\nmax_val_6d, max_idxs_6d = torch.max(t_6d, 4)\r\n# max_val_6d / max_idxs_6d is of size 1 x 1 x 1 x 26 x 1 x 25 -> fine\r\n# The value of max_idxs_6d go from 0 to 4                     -> fine\r\nassert(max_idxs_6d.max() < 5)\r\n\r\n# This is fine and works as expected\r\nt_7d = torch.randn(1, 1, 1, 1, 26, 5, 25)\r\nmax_val_7d, max_idxs_7d = torch.max(t_7d, 5)\r\n# max_val_7d / max_idxs_7d is of size 1 x 1 x 1 x 1 x 26 x 1 x 25 -> fine\r\n# The value of max_idxs_7d go from 0 to 4                         -> fine\r\nassert(max_idxs_7d.max() < 5)\r\n\r\n\r\n\r\n# This is not fine\r\nt_4d = torch.randn(1, 26, 5, 25)\r\nmax_val_4d, max_idxs_4d = torch.max(t_4d, 2)\r\n# max_val_4d, max_idxs_4d is of size 1 x 26 x 1 x 25 -> fine\r\n# The value of max_idxs_4d go from 0 to 24           -> ???\r\nprint(max_idxs_4d)\r\nassert(max_idxs_4d.max() < 5)\r\n\r\n\r\n```\r\n\r\nCan you confirm this is a bug and not a misunderstanding on my part of what `torch.max()` should return?"}