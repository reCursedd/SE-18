{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295911302", "html_url": "https://github.com/pytorch/pytorch/issues/1310#issuecomment-295911302", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1310", "id": 295911302, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTkxMTMwMg==", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T21:01:31Z", "updated_at": "2017-04-20T21:49:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a> found that it is not actually limited to Tensor of 4D.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\ndims <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">8</span>)\na <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-k\">*</span>dims)\n<span class=\"pl-k\">for</span> dim <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(dims)):\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(a.size(dim)):\n        a.select(dim,i).fill_(i)\n    val, argmax <span class=\"pl-k\">=</span> a.max(dim)\n    <span class=\"pl-c1\">print</span>(argmax.max())\n</pre></div>\n<p>gives as output:</p>\n<pre><code>2\n3\n335\n5\n6\n7\n</code></pre>\n<p>It would seem like the problem only happen when the dimension given is 2. The value obtained as the argmax will in that case always be the product of the size of the remaining dimensions.</p>\n<p>Possible interpretation of what happens:<br>\nThe tensor is flattened and the argmax that is stored is the idx of the max in the flattened version, that is not properly converted to the reshaped version.</p>\n<p>Probable cause:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L1551\">https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L1551</a>,<br>\n<code>dimOffset</code> is only used by <code>max</code> and <code>min</code> and both are subject to this bug so it would seem likely that this is where the problem comes from.</p>", "body_text": "@albanD found that it is not actually limited to Tensor of 4D.\nimport torch\n\ndims = (3, 4, 5, 6, 7, 8)\na = torch.randn(*dims)\nfor dim in range(len(dims)):\n    for i in range(a.size(dim)):\n        a.select(dim,i).fill_(i)\n    val, argmax = a.max(dim)\n    print(argmax.max())\n\ngives as output:\n2\n3\n335\n5\n6\n7\n\nIt would seem like the problem only happen when the dimension given is 2. The value obtained as the argmax will in that case always be the product of the size of the remaining dimensions.\nPossible interpretation of what happens:\nThe tensor is flattened and the argmax that is stored is the idx of the max in the flattened version, that is not properly converted to the reshaped version.\nProbable cause:\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L1551,\ndimOffset is only used by max and min and both are subject to this bug so it would seem likely that this is where the problem comes from.", "body": "@albanD found that it is not actually limited to Tensor of 4D.\r\n\r\n```python\r\nimport torch\r\n\r\ndims = (3, 4, 5, 6, 7, 8)\r\na = torch.randn(*dims)\r\nfor dim in range(len(dims)):\r\n    for i in range(a.size(dim)):\r\n        a.select(dim,i).fill_(i)\r\n    val, argmax = a.max(dim)\r\n    print(argmax.max())\r\n\r\n```\r\n gives as output:\r\n\r\n```\r\n2\r\n3\r\n335\r\n5\r\n6\r\n7\r\n```\r\nIt would seem like the problem only happen when the dimension given is 2. The value obtained as the argmax will in that case always be the product of the size of the remaining dimensions.\r\n\r\nPossible interpretation of what happens:\r\nThe tensor is flattened and the argmax that is stored is the idx of the max in the flattened version, that is not properly converted to the reshaped version.\r\n\r\n\r\nProbable cause:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L1551,\r\n`dimOffset` is only used by `max` and `min` and both are subject to this bug so it would seem likely that this is where the problem comes from."}