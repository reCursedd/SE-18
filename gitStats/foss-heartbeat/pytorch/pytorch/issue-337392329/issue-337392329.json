{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9095", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9095/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9095/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9095/events", "html_url": "https://github.com/pytorch/pytorch/issues/9095", "id": 337392329, "node_id": "MDU6SXNzdWUzMzczOTIzMjk=", "number": 9095, "title": "RuntimeError: backward_input can only be called in training mode", "user": {"login": "EricAugust", "id": 16010978, "node_id": "MDQ6VXNlcjE2MDEwOTc4", "avatar_url": "https://avatars1.githubusercontent.com/u/16010978?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EricAugust", "html_url": "https://github.com/EricAugust", "followers_url": "https://api.github.com/users/EricAugust/followers", "following_url": "https://api.github.com/users/EricAugust/following{/other_user}", "gists_url": "https://api.github.com/users/EricAugust/gists{/gist_id}", "starred_url": "https://api.github.com/users/EricAugust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EricAugust/subscriptions", "organizations_url": "https://api.github.com/users/EricAugust/orgs", "repos_url": "https://api.github.com/users/EricAugust/repos", "events_url": "https://api.github.com/users/EricAugust/events{/privacy}", "received_events_url": "https://api.github.com/users/EricAugust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-02T07:00:53Z", "updated_at": "2018-07-02T17:32:36Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>This is my training code</p>\n<pre><code>logger.info('Training....')\nbest_val_loss = []\nstored_loss = 100000000\n\ndef train(train_dataloader=train_dataloader):\n    model.train()\n    start_time = datetime.now()\n    batch_losses = 0\n    batch_metrics = 0  \n    predict_proba = []\n    for questions,answers, targets in train_dataloader:\n\n        if args.use_gpu:\n            questions,answers , targets = Variable(questions.cuda()), Variable(answers.cuda()), Variable(targets.cuda())\n        else:\n            questions,answers, targets = Variable(questions), Variable(answers), Variable(targets)\n\n        optimizer.zero_grad()\n        model.zero_grad()\n        outputs = model(questions,answers)\n        batch_loss = criterion(outputs, targets)\n        batch_metric = accuracy(outputs, targets)\n        batch_loss.backward(retain_graph=True)\n        optimizer.step()\n\n        batch_losses += batch_loss.data\n        batch_metrics += batch_metric.data\n    train_data_size = len(train_dataloader.dataset)\n    epoch_loss = batch_losses / train_data_size\n    epoch_metric = batch_metrics / train_data_size\n    elapsed = datetime.now() - start_time\n    print('| epoch {:3d} | lr {:05.5f} | ms/epoch {} | '\n            'loss {:5.2f} | acc {:8.3f}  | ppl {:8.3f} | bpc {:8.3f}'.format(\n        epoch,  optimizer.param_groups[0]['lr'],\n        str(elapsed) , epoch_loss, epoch_metric , math.exp(epoch_loss), epoch_loss / math.log(2)))\n    \n\ndef evaluate(dataloader):\n    # validation\n    model.eval()\n    val_batch_losses = 0\n    val_batch_metrics = 0\n    predict_proba = []\n    for val_questions,val_answers, val_targets in dataloader:\n\n        if args.use_gpu:\n            val_questions,val_answers , val_targets = Variable(val_questions.cuda()), Variable(val_answers.cuda()), Variable(val_targets.cuda())\n        else:\n            val_questions,val_answers, val_targets = Variable(val_questions), Variable(val_answers), Variable(val_targets)\n\n        val_outputs = model(val_questions,val_answers)\n        val_batch_loss = criterion(val_outputs, val_targets)\n        val_batch_metric = accuracy(val_outputs, val_targets)\n        val_batch_losses += val_batch_loss.data\n        val_batch_metrics += val_batch_metric.data        \n        proba = [x[1] for x in torch.exp(val_outputs).cpu().data.numpy()]\n        predict_proba.extend(proba)\n    return val_batch_losses/len(dataloader.dataset), val_batch_metrics/len(dataloader.dataset),predict_proba\n\ndef ks_stat(outputs, labels,good_cnts,bad_cnts):\n    _, argmax = outputs.max(dim=1)\n    good = (argmax.data == 1).cpu().float()\n    bad = (argmax.data == 0).cpu().float()\n    good[0] += good_cnts\n    bad[0] += bad_cnts \n    good_cnt = torch.cumsum(good, dim=0)\n    bad_cnt = torch.cumsum(bad, dim=0)\n    val = torch.abs(bad_cnt/preprocessor.total_bad - good_cnt/preprocessor.total_good).max().cpu().float().data\n    \n    return good_cnt[-1],bad_cnt[-1],val\n\ndef accuracy(outputs, labels):\n    maximum, argmax = outputs.max(dim=1)\n    corrects = argmax == labels # ByteTensor\n    n_corrects = corrects.float().sum() # FloatTensor\n    return n_corrects\n\ndef model_save(fn):\n    with open(fn, 'wb') as f:\n        torch.save([model, criterion, optimizer], f)\n        \ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, 'rb') as f:\n        model, criterion, optimizer = torch.load(f)\n        \n        \nfrom scipy.stats import ks_2samp\n\nks_statistics = lambda y_pred, y_true: ks_2samp(y_pred[y_true==1],y_pred[y_true != 1]).statistic\n\nfor epoch in range(args.epochs + 1):\n    start_time = datetime.now()\n\n    train()\n    val_epoch_loss, val_epoch_metric,predict_proba = evaluate(test_dataloader)\n    print('-' * 130)\n    print('| end of epoch {:3d} | time: {} | valid loss {:5.3f} | valid acc {:5.3f} | '\n        'valid ppl {:8.3f} | valid bpc {:8.3f}'.format(\n      epoch, str(datetime.now() - start_time), val_epoch_loss,val_epoch_metric  ,math.exp(val_epoch_loss), val_epoch_loss / math.log(2)))\n    print('-' * 130)\n    \n    if val_epoch_loss &lt; stored_loss:\n        model_save('model.best')\n        print('Saving model (new best validation)')\n        stored_loss = val_epoch_loss\n\n    if epoch %2 ==0 :\n        print('Saving model before learning rate decreased')\n        model_save('{}.e{}'.format(start_time.strftime('%m%d-%H%M%S'), epoch))\n        print('recuce learning rate by multiply 0.7')\n        optimizer.param_groups[0]['lr'] * 0.7\n\n    best_val_loss.append(val_epoch_loss)\n</code></pre>\n<p>And I use Adam. The model code is :<br>\n`<br>\nimport torch<br>\nfrom torch.autograd import Variable<br>\nfrom torch import nn</p>\n<p>class LSTM(nn.Module):</p>\n<pre><code>def __init__(self, n_classes, dictionary, args):\n    super(LSTM, self).__init__()\n\n    self.hidden_dim = args.hidden_dim  \n    self.num_layers = args.num_layers\n    self.gpu = args.use_gpu\n    vocabulary_size = dictionary.vocabulary_size\n    vector_size = dictionary.vector_size\n    embedding_weight = dictionary.embedding\n    self.embedding = nn.Embedding(vocabulary_size, vector_size)\n    if embedding_weight is not None:\n        embedding_weight = nn.Parameter(torch.FloatTensor(embedding_weight), requires_grad=True)\n    self.bilstm = nn.LSTM(vector_size, self.hidden_dim // 2, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=False)\n    self.hidden1 = self.init_hidden(args.batch_size)\n    self.hidden2 = self.init_hidden(args.batch_size)\n    self.hidden2label1 = nn.Linear(2* self.hidden_dim, self.hidden_dim )\n    self.hidden2label2 = nn.Linear(self.hidden_dim , n_classes)\n   \ndef init_hidden(self, batch_size):\n    # the first is the hidden h\n    # the second is the cell  c\n    if self.gpu is True:\n        return (Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)).cuda(),\n                Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)).cuda())\n    else:\n        return (Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)),\n                Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)))    \n    \ndef forward(self, q, a):\n    \n    q_embed = self.embedding(q)\n    q = q_embed.view(q_embed.size(1), len(q), -1)\n    \n    q_lstm_out,self.hidden1 = self.bilstm(q, self.hidden1)\n\n    q_lstm_out = torch.transpose(q_lstm_out, 0, 1)\n    q_lstm_out = torch.transpose(q_lstm_out, 1, 2)\n    q_lstm_out = torch.mean(q_lstm_out,2)\n    a_embed = self.embedding(a)\n    a = a_embed.view(a_embed.size(1), len(a), -1)\n    a_lstm_out, self.hidden2 = self.bilstm(a, self.hidden2)\n\n    a_lstm_out = torch.transpose(a_lstm_out, 0, 1)\n    a_lstm_out = torch.transpose(a_lstm_out, 1, 2)\n    a_lstm_out = torch.mean(a_lstm_out,2)\n    final = torch.cat((q_lstm_out,a_lstm_out),1)\n    y = self.hidden2label1(final)\n    y = self.hidden2label2(y)\n    return y\n</code></pre>\n<p>`</p>\n<p>It gives me error:<br>\n[INFO] 0702-145451 &gt; Training....</p>\n<pre><code>| epoch   0 | lr 0.00100 | ms/epoch 0:00:42.024558 | loss  0.02 | acc    0.051  | ppl    1.023 | bpc    0.032\n----------------------------------------------------------------------------------------------------------------------------------\n| end of epoch   0 | time: 0:00:42.099358 | valid loss 0.022 | valid acc 0.000 | valid ppl    1.022 | valid bpc    0.031\n----------------------------------------------------------------------------------------------------------------------------------\nSaving model (new best validation)\nSaving model before learning rate decreased\nrecuce learning rate by multiply 0.7\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-f8fa20e0d1bd&gt; in &lt;module&gt;()\n    114     start_time = datetime.now()\n    115 \n--&gt; 116     train()\n    117     val_epoch_loss, val_epoch_metric,predict_proba = evaluate(test_dataloader)\n    118     print('-' * 130)\n\n&lt;ipython-input-9-f8fa20e0d1bd&gt; in train(train_dataloader)\n     24         batch_loss = criterion(outputs, targets)\n     25         batch_metric = accuracy(outputs, targets)\n---&gt; 26         batch_loss.backward(retain_graph=True)\n     27         optimizer.step()\n     28 \n\nd:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\nd:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---&gt; 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: backward_input can only be called in training mode\n</code></pre>\n<p>Pytorch is 0.4, Cuda is 9</p>", "body_text": "This is my training code\nlogger.info('Training....')\nbest_val_loss = []\nstored_loss = 100000000\n\ndef train(train_dataloader=train_dataloader):\n    model.train()\n    start_time = datetime.now()\n    batch_losses = 0\n    batch_metrics = 0  \n    predict_proba = []\n    for questions,answers, targets in train_dataloader:\n\n        if args.use_gpu:\n            questions,answers , targets = Variable(questions.cuda()), Variable(answers.cuda()), Variable(targets.cuda())\n        else:\n            questions,answers, targets = Variable(questions), Variable(answers), Variable(targets)\n\n        optimizer.zero_grad()\n        model.zero_grad()\n        outputs = model(questions,answers)\n        batch_loss = criterion(outputs, targets)\n        batch_metric = accuracy(outputs, targets)\n        batch_loss.backward(retain_graph=True)\n        optimizer.step()\n\n        batch_losses += batch_loss.data\n        batch_metrics += batch_metric.data\n    train_data_size = len(train_dataloader.dataset)\n    epoch_loss = batch_losses / train_data_size\n    epoch_metric = batch_metrics / train_data_size\n    elapsed = datetime.now() - start_time\n    print('| epoch {:3d} | lr {:05.5f} | ms/epoch {} | '\n            'loss {:5.2f} | acc {:8.3f}  | ppl {:8.3f} | bpc {:8.3f}'.format(\n        epoch,  optimizer.param_groups[0]['lr'],\n        str(elapsed) , epoch_loss, epoch_metric , math.exp(epoch_loss), epoch_loss / math.log(2)))\n    \n\ndef evaluate(dataloader):\n    # validation\n    model.eval()\n    val_batch_losses = 0\n    val_batch_metrics = 0\n    predict_proba = []\n    for val_questions,val_answers, val_targets in dataloader:\n\n        if args.use_gpu:\n            val_questions,val_answers , val_targets = Variable(val_questions.cuda()), Variable(val_answers.cuda()), Variable(val_targets.cuda())\n        else:\n            val_questions,val_answers, val_targets = Variable(val_questions), Variable(val_answers), Variable(val_targets)\n\n        val_outputs = model(val_questions,val_answers)\n        val_batch_loss = criterion(val_outputs, val_targets)\n        val_batch_metric = accuracy(val_outputs, val_targets)\n        val_batch_losses += val_batch_loss.data\n        val_batch_metrics += val_batch_metric.data        \n        proba = [x[1] for x in torch.exp(val_outputs).cpu().data.numpy()]\n        predict_proba.extend(proba)\n    return val_batch_losses/len(dataloader.dataset), val_batch_metrics/len(dataloader.dataset),predict_proba\n\ndef ks_stat(outputs, labels,good_cnts,bad_cnts):\n    _, argmax = outputs.max(dim=1)\n    good = (argmax.data == 1).cpu().float()\n    bad = (argmax.data == 0).cpu().float()\n    good[0] += good_cnts\n    bad[0] += bad_cnts \n    good_cnt = torch.cumsum(good, dim=0)\n    bad_cnt = torch.cumsum(bad, dim=0)\n    val = torch.abs(bad_cnt/preprocessor.total_bad - good_cnt/preprocessor.total_good).max().cpu().float().data\n    \n    return good_cnt[-1],bad_cnt[-1],val\n\ndef accuracy(outputs, labels):\n    maximum, argmax = outputs.max(dim=1)\n    corrects = argmax == labels # ByteTensor\n    n_corrects = corrects.float().sum() # FloatTensor\n    return n_corrects\n\ndef model_save(fn):\n    with open(fn, 'wb') as f:\n        torch.save([model, criterion, optimizer], f)\n        \ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, 'rb') as f:\n        model, criterion, optimizer = torch.load(f)\n        \n        \nfrom scipy.stats import ks_2samp\n\nks_statistics = lambda y_pred, y_true: ks_2samp(y_pred[y_true==1],y_pred[y_true != 1]).statistic\n\nfor epoch in range(args.epochs + 1):\n    start_time = datetime.now()\n\n    train()\n    val_epoch_loss, val_epoch_metric,predict_proba = evaluate(test_dataloader)\n    print('-' * 130)\n    print('| end of epoch {:3d} | time: {} | valid loss {:5.3f} | valid acc {:5.3f} | '\n        'valid ppl {:8.3f} | valid bpc {:8.3f}'.format(\n      epoch, str(datetime.now() - start_time), val_epoch_loss,val_epoch_metric  ,math.exp(val_epoch_loss), val_epoch_loss / math.log(2)))\n    print('-' * 130)\n    \n    if val_epoch_loss < stored_loss:\n        model_save('model.best')\n        print('Saving model (new best validation)')\n        stored_loss = val_epoch_loss\n\n    if epoch %2 ==0 :\n        print('Saving model before learning rate decreased')\n        model_save('{}.e{}'.format(start_time.strftime('%m%d-%H%M%S'), epoch))\n        print('recuce learning rate by multiply 0.7')\n        optimizer.param_groups[0]['lr'] * 0.7\n\n    best_val_loss.append(val_epoch_loss)\n\nAnd I use Adam. The model code is :\n`\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\nclass LSTM(nn.Module):\ndef __init__(self, n_classes, dictionary, args):\n    super(LSTM, self).__init__()\n\n    self.hidden_dim = args.hidden_dim  \n    self.num_layers = args.num_layers\n    self.gpu = args.use_gpu\n    vocabulary_size = dictionary.vocabulary_size\n    vector_size = dictionary.vector_size\n    embedding_weight = dictionary.embedding\n    self.embedding = nn.Embedding(vocabulary_size, vector_size)\n    if embedding_weight is not None:\n        embedding_weight = nn.Parameter(torch.FloatTensor(embedding_weight), requires_grad=True)\n    self.bilstm = nn.LSTM(vector_size, self.hidden_dim // 2, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=False)\n    self.hidden1 = self.init_hidden(args.batch_size)\n    self.hidden2 = self.init_hidden(args.batch_size)\n    self.hidden2label1 = nn.Linear(2* self.hidden_dim, self.hidden_dim )\n    self.hidden2label2 = nn.Linear(self.hidden_dim , n_classes)\n   \ndef init_hidden(self, batch_size):\n    # the first is the hidden h\n    # the second is the cell  c\n    if self.gpu is True:\n        return (Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)).cuda(),\n                Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)).cuda())\n    else:\n        return (Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)),\n                Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)))    \n    \ndef forward(self, q, a):\n    \n    q_embed = self.embedding(q)\n    q = q_embed.view(q_embed.size(1), len(q), -1)\n    \n    q_lstm_out,self.hidden1 = self.bilstm(q, self.hidden1)\n\n    q_lstm_out = torch.transpose(q_lstm_out, 0, 1)\n    q_lstm_out = torch.transpose(q_lstm_out, 1, 2)\n    q_lstm_out = torch.mean(q_lstm_out,2)\n    a_embed = self.embedding(a)\n    a = a_embed.view(a_embed.size(1), len(a), -1)\n    a_lstm_out, self.hidden2 = self.bilstm(a, self.hidden2)\n\n    a_lstm_out = torch.transpose(a_lstm_out, 0, 1)\n    a_lstm_out = torch.transpose(a_lstm_out, 1, 2)\n    a_lstm_out = torch.mean(a_lstm_out,2)\n    final = torch.cat((q_lstm_out,a_lstm_out),1)\n    y = self.hidden2label1(final)\n    y = self.hidden2label2(y)\n    return y\n\n`\nIt gives me error:\n[INFO] 0702-145451 > Training....\n| epoch   0 | lr 0.00100 | ms/epoch 0:00:42.024558 | loss  0.02 | acc    0.051  | ppl    1.023 | bpc    0.032\n----------------------------------------------------------------------------------------------------------------------------------\n| end of epoch   0 | time: 0:00:42.099358 | valid loss 0.022 | valid acc 0.000 | valid ppl    1.022 | valid bpc    0.031\n----------------------------------------------------------------------------------------------------------------------------------\nSaving model (new best validation)\nSaving model before learning rate decreased\nrecuce learning rate by multiply 0.7\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-f8fa20e0d1bd> in <module>()\n    114     start_time = datetime.now()\n    115 \n--> 116     train()\n    117     val_epoch_loss, val_epoch_metric,predict_proba = evaluate(test_dataloader)\n    118     print('-' * 130)\n\n<ipython-input-9-f8fa20e0d1bd> in train(train_dataloader)\n     24         batch_loss = criterion(outputs, targets)\n     25         batch_metric = accuracy(outputs, targets)\n---> 26         batch_loss.backward(retain_graph=True)\n     27         optimizer.step()\n     28 \n\nd:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\nd:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---> 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: backward_input can only be called in training mode\n\nPytorch is 0.4, Cuda is 9", "body": "This is my training code \r\n```\r\nlogger.info('Training....')\r\nbest_val_loss = []\r\nstored_loss = 100000000\r\n\r\ndef train(train_dataloader=train_dataloader):\r\n    model.train()\r\n    start_time = datetime.now()\r\n    batch_losses = 0\r\n    batch_metrics = 0  \r\n    predict_proba = []\r\n    for questions,answers, targets in train_dataloader:\r\n\r\n        if args.use_gpu:\r\n            questions,answers , targets = Variable(questions.cuda()), Variable(answers.cuda()), Variable(targets.cuda())\r\n        else:\r\n            questions,answers, targets = Variable(questions), Variable(answers), Variable(targets)\r\n\r\n        optimizer.zero_grad()\r\n        model.zero_grad()\r\n        outputs = model(questions,answers)\r\n        batch_loss = criterion(outputs, targets)\r\n        batch_metric = accuracy(outputs, targets)\r\n        batch_loss.backward(retain_graph=True)\r\n        optimizer.step()\r\n\r\n        batch_losses += batch_loss.data\r\n        batch_metrics += batch_metric.data\r\n    train_data_size = len(train_dataloader.dataset)\r\n    epoch_loss = batch_losses / train_data_size\r\n    epoch_metric = batch_metrics / train_data_size\r\n    elapsed = datetime.now() - start_time\r\n    print('| epoch {:3d} | lr {:05.5f} | ms/epoch {} | '\r\n            'loss {:5.2f} | acc {:8.3f}  | ppl {:8.3f} | bpc {:8.3f}'.format(\r\n        epoch,  optimizer.param_groups[0]['lr'],\r\n        str(elapsed) , epoch_loss, epoch_metric , math.exp(epoch_loss), epoch_loss / math.log(2)))\r\n    \r\n\r\ndef evaluate(dataloader):\r\n    # validation\r\n    model.eval()\r\n    val_batch_losses = 0\r\n    val_batch_metrics = 0\r\n    predict_proba = []\r\n    for val_questions,val_answers, val_targets in dataloader:\r\n\r\n        if args.use_gpu:\r\n            val_questions,val_answers , val_targets = Variable(val_questions.cuda()), Variable(val_answers.cuda()), Variable(val_targets.cuda())\r\n        else:\r\n            val_questions,val_answers, val_targets = Variable(val_questions), Variable(val_answers), Variable(val_targets)\r\n\r\n        val_outputs = model(val_questions,val_answers)\r\n        val_batch_loss = criterion(val_outputs, val_targets)\r\n        val_batch_metric = accuracy(val_outputs, val_targets)\r\n        val_batch_losses += val_batch_loss.data\r\n        val_batch_metrics += val_batch_metric.data        \r\n        proba = [x[1] for x in torch.exp(val_outputs).cpu().data.numpy()]\r\n        predict_proba.extend(proba)\r\n    return val_batch_losses/len(dataloader.dataset), val_batch_metrics/len(dataloader.dataset),predict_proba\r\n\r\ndef ks_stat(outputs, labels,good_cnts,bad_cnts):\r\n    _, argmax = outputs.max(dim=1)\r\n    good = (argmax.data == 1).cpu().float()\r\n    bad = (argmax.data == 0).cpu().float()\r\n    good[0] += good_cnts\r\n    bad[0] += bad_cnts \r\n    good_cnt = torch.cumsum(good, dim=0)\r\n    bad_cnt = torch.cumsum(bad, dim=0)\r\n    val = torch.abs(bad_cnt/preprocessor.total_bad - good_cnt/preprocessor.total_good).max().cpu().float().data\r\n    \r\n    return good_cnt[-1],bad_cnt[-1],val\r\n\r\ndef accuracy(outputs, labels):\r\n    maximum, argmax = outputs.max(dim=1)\r\n    corrects = argmax == labels # ByteTensor\r\n    n_corrects = corrects.float().sum() # FloatTensor\r\n    return n_corrects\r\n\r\ndef model_save(fn):\r\n    with open(fn, 'wb') as f:\r\n        torch.save([model, criterion, optimizer], f)\r\n        \r\ndef model_load(fn):\r\n    global model, criterion, optimizer\r\n    with open(fn, 'rb') as f:\r\n        model, criterion, optimizer = torch.load(f)\r\n        \r\n        \r\nfrom scipy.stats import ks_2samp\r\n\r\nks_statistics = lambda y_pred, y_true: ks_2samp(y_pred[y_true==1],y_pred[y_true != 1]).statistic\r\n\r\nfor epoch in range(args.epochs + 1):\r\n    start_time = datetime.now()\r\n\r\n    train()\r\n    val_epoch_loss, val_epoch_metric,predict_proba = evaluate(test_dataloader)\r\n    print('-' * 130)\r\n    print('| end of epoch {:3d} | time: {} | valid loss {:5.3f} | valid acc {:5.3f} | '\r\n        'valid ppl {:8.3f} | valid bpc {:8.3f}'.format(\r\n      epoch, str(datetime.now() - start_time), val_epoch_loss,val_epoch_metric  ,math.exp(val_epoch_loss), val_epoch_loss / math.log(2)))\r\n    print('-' * 130)\r\n    \r\n    if val_epoch_loss < stored_loss:\r\n        model_save('model.best')\r\n        print('Saving model (new best validation)')\r\n        stored_loss = val_epoch_loss\r\n\r\n    if epoch %2 ==0 :\r\n        print('Saving model before learning rate decreased')\r\n        model_save('{}.e{}'.format(start_time.strftime('%m%d-%H%M%S'), epoch))\r\n        print('recuce learning rate by multiply 0.7')\r\n        optimizer.param_groups[0]['lr'] * 0.7\r\n\r\n    best_val_loss.append(val_epoch_loss)\r\n```\r\nAnd I use Adam. The model code is :\r\n`\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch import nn\r\n\r\nclass LSTM(nn.Module):\r\n    \r\n    def __init__(self, n_classes, dictionary, args):\r\n        super(LSTM, self).__init__()\r\n\r\n        self.hidden_dim = args.hidden_dim  \r\n        self.num_layers = args.num_layers\r\n        self.gpu = args.use_gpu\r\n        vocabulary_size = dictionary.vocabulary_size\r\n        vector_size = dictionary.vector_size\r\n        embedding_weight = dictionary.embedding\r\n        self.embedding = nn.Embedding(vocabulary_size, vector_size)\r\n        if embedding_weight is not None:\r\n            embedding_weight = nn.Parameter(torch.FloatTensor(embedding_weight), requires_grad=True)\r\n        self.bilstm = nn.LSTM(vector_size, self.hidden_dim // 2, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=False)\r\n        self.hidden1 = self.init_hidden(args.batch_size)\r\n        self.hidden2 = self.init_hidden(args.batch_size)\r\n        self.hidden2label1 = nn.Linear(2* self.hidden_dim, self.hidden_dim )\r\n        self.hidden2label2 = nn.Linear(self.hidden_dim , n_classes)\r\n       \r\n    def init_hidden(self, batch_size):\r\n        # the first is the hidden h\r\n        # the second is the cell  c\r\n        if self.gpu is True:\r\n            return (Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)).cuda(),\r\n                    Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)).cuda())\r\n        else:\r\n            return (Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)),\r\n                    Variable(torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim // 2)))    \r\n        \r\n    def forward(self, q, a):\r\n        \r\n        q_embed = self.embedding(q)\r\n        q = q_embed.view(q_embed.size(1), len(q), -1)\r\n        \r\n        q_lstm_out,self.hidden1 = self.bilstm(q, self.hidden1)\r\n\r\n        q_lstm_out = torch.transpose(q_lstm_out, 0, 1)\r\n        q_lstm_out = torch.transpose(q_lstm_out, 1, 2)\r\n        q_lstm_out = torch.mean(q_lstm_out,2)\r\n        a_embed = self.embedding(a)\r\n        a = a_embed.view(a_embed.size(1), len(a), -1)\r\n        a_lstm_out, self.hidden2 = self.bilstm(a, self.hidden2)\r\n\r\n        a_lstm_out = torch.transpose(a_lstm_out, 0, 1)\r\n        a_lstm_out = torch.transpose(a_lstm_out, 1, 2)\r\n        a_lstm_out = torch.mean(a_lstm_out,2)\r\n        final = torch.cat((q_lstm_out,a_lstm_out),1)\r\n        y = self.hidden2label1(final)\r\n        y = self.hidden2label2(y)\r\n        return y\r\n`\r\n\r\nIt gives me error:\r\n[INFO] 0702-145451 > Training....\r\n```\r\n| epoch   0 | lr 0.00100 | ms/epoch 0:00:42.024558 | loss  0.02 | acc    0.051  | ppl    1.023 | bpc    0.032\r\n----------------------------------------------------------------------------------------------------------------------------------\r\n| end of epoch   0 | time: 0:00:42.099358 | valid loss 0.022 | valid acc 0.000 | valid ppl    1.022 | valid bpc    0.031\r\n----------------------------------------------------------------------------------------------------------------------------------\r\nSaving model (new best validation)\r\nSaving model before learning rate decreased\r\nrecuce learning rate by multiply 0.7\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-9-f8fa20e0d1bd> in <module>()\r\n    114     start_time = datetime.now()\r\n    115 \r\n--> 116     train()\r\n    117     val_epoch_loss, val_epoch_metric,predict_proba = evaluate(test_dataloader)\r\n    118     print('-' * 130)\r\n\r\n<ipython-input-9-f8fa20e0d1bd> in train(train_dataloader)\r\n     24         batch_loss = criterion(outputs, targets)\r\n     25         batch_metric = accuracy(outputs, targets)\r\n---> 26         batch_loss.backward(retain_graph=True)\r\n     27         optimizer.step()\r\n     28 \r\n\r\nd:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n     91                 products. Defaults to ``False``.\r\n     92         \"\"\"\r\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n     94 \r\n     95     def register_hook(self, hook):\r\n\r\nd:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     87     Variable._execution_engine.run_backward(\r\n     88         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 89         allow_unreachable=True)  # allow_unreachable flag\r\n     90 \r\n     91 \r\n\r\nRuntimeError: backward_input can only be called in training mode\r\n```\r\n\r\nPytorch is 0.4, Cuda is 9"}