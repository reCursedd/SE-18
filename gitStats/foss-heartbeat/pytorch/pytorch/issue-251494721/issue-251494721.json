{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2493", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2493/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2493/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2493/events", "html_url": "https://github.com/pytorch/pytorch/issues/2493", "id": 251494721, "node_id": "MDU6SXNzdWUyNTE0OTQ3MjE=", "number": 2493, "title": "Weird issues when Defining a large network?", "user": {"login": "DakshMiglani", "id": 23355449, "node_id": "MDQ6VXNlcjIzMzU1NDQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/23355449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DakshMiglani", "html_url": "https://github.com/DakshMiglani", "followers_url": "https://api.github.com/users/DakshMiglani/followers", "following_url": "https://api.github.com/users/DakshMiglani/following{/other_user}", "gists_url": "https://api.github.com/users/DakshMiglani/gists{/gist_id}", "starred_url": "https://api.github.com/users/DakshMiglani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DakshMiglani/subscriptions", "organizations_url": "https://api.github.com/users/DakshMiglani/orgs", "repos_url": "https://api.github.com/users/DakshMiglani/repos", "events_url": "https://api.github.com/users/DakshMiglani/events{/privacy}", "received_events_url": "https://api.github.com/users/DakshMiglani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-08-20T15:11:13Z", "updated_at": "2017-08-21T09:23:53Z", "closed_at": "2017-08-21T09:23:53Z", "author_association": "NONE", "body_html": "<p>Here's my network:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.pool <span class=\"pl-k\">=</span> nn.MaxPool2d(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">4</span>)\n        <span class=\"pl-c1\">self</span>.conv4 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">4</span>)\n        <span class=\"pl-c1\">self</span>.conv4_drop <span class=\"pl-k\">=</span> nn.Dropout2d()\n        <span class=\"pl-c1\">self</span>.conv4 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">4</span>)\n        <span class=\"pl-c1\">self</span>.conv5 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">4</span>)\n        <span class=\"pl-c1\">self</span>.conv6 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">4</span>)\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">64</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">512</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">256</span>)\n        <span class=\"pl-c1\">self</span>.fc3 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>)\n        <span class=\"pl-c1\">self</span>.fc4 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">64</span>)\n        <span class=\"pl-c1\">self</span>.fc4d <span class=\"pl-k\">=</span> nn.Dropout2d()\n        <span class=\"pl-c1\">self</span>.fc5 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">32</span>)\n        <span class=\"pl-c1\">self</span>.fc6 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">10</span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.conv1(x)))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv2(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv3(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.conv4_drop(<span class=\"pl-c1\">self</span>.conv4(x))))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv5(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv6(x))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span>)\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc2(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc3(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc4d(<span class=\"pl-c1\">self</span>.fc4(x)))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc5(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc6(x)\n        <span class=\"pl-k\">return</span> x</pre></div>\n<p>when I proceed to use <code>net.cuda()</code></p>\n<p>the output appears like this :</p>\n<div class=\"highlight highlight-source-python\"><pre>Net (\n  (conv1): Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (pool): MaxPool2d (<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (conv2): Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (conv3): Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (conv4): Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (conv4_drop): Dropout2d (<span class=\"pl-v\">p</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n  (conv5): Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (conv6): Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n  (fc1): Linear (<span class=\"pl-c1\">1024</span> <span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">512</span>)\n  (fc2): Linear (<span class=\"pl-c1\">512</span> <span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">256</span>)\n  (fc3): Linear (<span class=\"pl-c1\">256</span> <span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">128</span>)\n  (fc4): Linear (<span class=\"pl-c1\">128</span> <span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">64</span>)\n  (fc4d): Dropout2d (<span class=\"pl-v\">p</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n  (fc5): Linear (<span class=\"pl-c1\">64</span> <span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">32</span>)\n  (fc6): Linear (<span class=\"pl-c1\">32</span> <span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">10</span>)\n)</pre></div>\n<p>but the thing is the older network with less layers worked but this doesn't even tho I didn't change size of input and if you check conv3 and conv4 in the output and compare it to the input they are different.</p>\n<p>And that's where i get this error : <code>Need input.size[1] == 32 but got 64 instead.</code></p>", "body_text": "Here's my network:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(16, 32, 5)\n        self.conv3 = nn.Conv2d(32, 64, 4)\n        self.conv4 = nn.Conv2d(64, 32, 4)\n        self.conv4_drop = nn.Dropout2d()\n        self.conv4 = nn.Conv2d(32, 16, 4)\n        self.conv5 = nn.Conv2d(16, 32, 4)\n        self.conv6 = nn.Conv2d(32, 64, 4)\n        self.fc1 = nn.Linear(64 * 4 * 4,  512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc4d = nn.Dropout2d()\n        self.fc5 = nn.Linear(64, 32)\n        self.fc6 = nn.Linear(32, 10)\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = self.pool(F.relu(self.conv4_drop(self.conv4(x))))\n        x = F.relu(self.conv5(x))\n        x = F.relu(self.conv6(x))\n        x = x.view(-1, 64 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4d(self.fc4(x)))\n        x = F.relu(self.fc5(x))\n        x = self.fc6(x)\n        return x\nwhen I proceed to use net.cuda()\nthe output appears like this :\nNet (\n  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n  (conv4): Conv2d(32, 16, kernel_size=(4, 4), stride=(1, 1))\n  (conv4_drop): Dropout2d (p=0.5)\n  (conv5): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1))\n  (conv6): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n  (fc1): Linear (1024 -> 512)\n  (fc2): Linear (512 -> 256)\n  (fc3): Linear (256 -> 128)\n  (fc4): Linear (128 -> 64)\n  (fc4d): Dropout2d (p=0.5)\n  (fc5): Linear (64 -> 32)\n  (fc6): Linear (32 -> 10)\n)\nbut the thing is the older network with less layers worked but this doesn't even tho I didn't change size of input and if you check conv3 and conv4 in the output and compare it to the input they are different.\nAnd that's where i get this error : Need input.size[1] == 32 but got 64 instead.", "body": "Here's my network:\r\n\r\n```python\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 16, 5)\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(16, 32, 5)\r\n        self.conv3 = nn.Conv2d(32, 64, 4)\r\n        self.conv4 = nn.Conv2d(64, 32, 4)\r\n        self.conv4_drop = nn.Dropout2d()\r\n        self.conv4 = nn.Conv2d(32, 16, 4)\r\n        self.conv5 = nn.Conv2d(16, 32, 4)\r\n        self.conv6 = nn.Conv2d(32, 64, 4)\r\n        self.fc1 = nn.Linear(64 * 4 * 4,  512)\r\n        self.fc2 = nn.Linear(512, 256)\r\n        self.fc3 = nn.Linear(256, 128)\r\n        self.fc4 = nn.Linear(128, 64)\r\n        self.fc4d = nn.Dropout2d()\r\n        self.fc5 = nn.Linear(64, 32)\r\n        self.fc6 = nn.Linear(32, 10)\r\n    def forward(self, x):\r\n        x = self.pool(F.relu(self.conv1(x)))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.relu(self.conv3(x))\r\n        x = self.pool(F.relu(self.conv4_drop(self.conv4(x))))\r\n        x = F.relu(self.conv5(x))\r\n        x = F.relu(self.conv6(x))\r\n        x = x.view(-1, 64 * 4 * 4)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = F.relu(self.fc3(x))\r\n        x = F.relu(self.fc4d(self.fc4(x)))\r\n        x = F.relu(self.fc5(x))\r\n        x = self.fc6(x)\r\n        return x\r\n```\r\n\r\nwhen I proceed to use `net.cuda()`\r\n\r\nthe output appears like this : \r\n```python\r\nNet (\r\n  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\r\n  (pool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\r\n  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\r\n  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\r\n  (conv4): Conv2d(32, 16, kernel_size=(4, 4), stride=(1, 1))\r\n  (conv4_drop): Dropout2d (p=0.5)\r\n  (conv5): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1))\r\n  (conv6): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\r\n  (fc1): Linear (1024 -> 512)\r\n  (fc2): Linear (512 -> 256)\r\n  (fc3): Linear (256 -> 128)\r\n  (fc4): Linear (128 -> 64)\r\n  (fc4d): Dropout2d (p=0.5)\r\n  (fc5): Linear (64 -> 32)\r\n  (fc6): Linear (32 -> 10)\r\n)\r\n```\r\n\r\nbut the thing is the older network with less layers worked but this doesn't even tho I didn't change size of input and if you check conv3 and conv4 in the output and compare it to the input they are different.\r\n\r\nAnd that's where i get this error : `Need input.size[1] == 32 but got 64 instead.`"}