{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2328", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2328/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2328/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2328/events", "html_url": "https://github.com/pytorch/pytorch/pull/2328", "id": 248555662, "node_id": "MDExOlB1bGxSZXF1ZXN0MTM0NTQ0NzU4", "number": 2328, "title": "Fix BatchNorm double backwards memory leak (master)", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-08-07T23:06:16Z", "updated_at": "2018-11-23T15:34:18Z", "closed_at": "2017-08-10T18:04:54Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2328", "html_url": "https://github.com/pytorch/pytorch/pull/2328", "diff_url": "https://github.com/pytorch/pytorch/pull/2328.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2328.patch"}, "body_html": "<p>This fixes a couple of issues:</p>\n<ol>\n<li>Fixes python reference counts in BatchNormBackwardBackward; previously there were a couple of issues such as using initializing THPObjectPtr with Py_None, when THPVariableWrap already does the correct thing and returns Py_RETURN_NONE.</li>\n<li>Fixes the \"saved_for\" parameter in BatchNorm and Conv. Previously, these were passed as the object itself, rather than the Backwards object, i.e. ConvForward would pass ConvForward as the save for backward, which is wrong (you never save something for the forward). I'm not sure in these specific cases if it were possible to construct a graph where this was a problem (I wasn't able to with non-affine BatchNorm), but it's good to fix this anyway.</li>\n</ol>", "body_text": "This fixes a couple of issues:\n\nFixes python reference counts in BatchNormBackwardBackward; previously there were a couple of issues such as using initializing THPObjectPtr with Py_None, when THPVariableWrap already does the correct thing and returns Py_RETURN_NONE.\nFixes the \"saved_for\" parameter in BatchNorm and Conv. Previously, these were passed as the object itself, rather than the Backwards object, i.e. ConvForward would pass ConvForward as the save for backward, which is wrong (you never save something for the forward). I'm not sure in these specific cases if it were possible to construct a graph where this was a problem (I wasn't able to with non-affine BatchNorm), but it's good to fix this anyway.", "body": "This fixes a couple of issues:\r\n\r\n1) Fixes python reference counts in BatchNormBackwardBackward; previously there were a couple of issues such as using initializing THPObjectPtr with Py_None, when THPVariableWrap already does the correct thing and returns Py_RETURN_NONE.\r\n2) Fixes the \"saved_for\" parameter in BatchNorm and Conv. Previously, these were passed as the object itself, rather than the Backwards object, i.e. ConvForward would pass ConvForward as the save for backward, which is wrong (you never save something for the forward). I'm not sure in these specific cases if it were possible to construct a graph where this was a problem (I wasn't able to with non-affine BatchNorm), but it's good to fix this anyway."}