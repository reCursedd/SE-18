{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/428299390", "html_url": "https://github.com/pytorch/pytorch/issues/12241#issuecomment-428299390", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12241", "id": 428299390, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODI5OTM5MA==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-09T18:26:25Z", "updated_at": "2018-10-09T18:26:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1041752\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vadimkantorov\">@vadimkantorov</a> Prefixing operators for SparseTensor with <code>sparse_</code> in general means during the backward gradients at zero positions will be zeroed out. This deviates from canonical behaviors of operators, hence a different name. But yes, we can also provide <code>sum(SparseTensor)</code> without autograd support. On the other hand, for those operators with sparse gradients during backward, e.g., <code>max(SparseTensor)</code>, we will name it without <code>sparse_</code> prefix.</p>", "body_text": "@vadimkantorov Prefixing operators for SparseTensor with sparse_ in general means during the backward gradients at zero positions will be zeroed out. This deviates from canonical behaviors of operators, hence a different name. But yes, we can also provide sum(SparseTensor) without autograd support. On the other hand, for those operators with sparse gradients during backward, e.g., max(SparseTensor), we will name it without sparse_ prefix.", "body": "@vadimkantorov Prefixing operators for SparseTensor with `sparse_` in general means during the backward gradients at zero positions will be zeroed out. This deviates from canonical behaviors of operators, hence a different name. But yes, we can also provide `sum(SparseTensor)` without autograd support. On the other hand, for those operators with sparse gradients during backward, e.g., `max(SparseTensor)`, we will name it without `sparse_` prefix."}