{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5851", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5851/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5851/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5851/events", "html_url": "https://github.com/pytorch/pytorch/issues/5851", "id": 306161171, "node_id": "MDU6SXNzdWUzMDYxNjExNzE=", "number": 5851, "title": "Bug at the interaction of torch.no_grad() and torch.nn.parallel.data_parallel", "user": {"login": "aosokin", "id": 2099291, "node_id": "MDQ6VXNlcjIwOTkyOTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2099291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aosokin", "html_url": "https://github.com/aosokin", "followers_url": "https://api.github.com/users/aosokin/followers", "following_url": "https://api.github.com/users/aosokin/following{/other_user}", "gists_url": "https://api.github.com/users/aosokin/gists{/gist_id}", "starred_url": "https://api.github.com/users/aosokin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aosokin/subscriptions", "organizations_url": "https://api.github.com/users/aosokin/orgs", "repos_url": "https://api.github.com/users/aosokin/repos", "events_url": "https://api.github.com/users/aosokin/events{/privacy}", "received_events_url": "https://api.github.com/users/aosokin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-17T14:35:08Z", "updated_at": "2018-03-19T19:26:52Z", "closed_at": "2018-03-19T19:26:52Z", "author_association": "CONTRIBUTOR", "body_html": "<ul>\n<li>OS: ubuntu 16.04</li>\n<li>PyTorch version: 0.4.0a0+875925b</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version: 3.6.3</li>\n<li>CUDA/cuDNN version: v9.1, cudnn-9.1-linux-x64-v7</li>\n<li>GPU models and configuration: 4x Tesla M60</li>\n<li>GCC version (if compiling from source):  5.4.0</li>\n</ul>\n<p>When using context torch.no_grad() on a model with data_parallel operation (same with torch.nn.DataParallel) on &gt;1 GPU the flag requires_grad does not come back to True.<br>\nIf only one GPU is used then everything works fine.</p>\n<p>Script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\ndata <span class=\"pl-k\">=</span> torch.cuda.FloatTensor(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)\nnet <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>)\nnet.cuda()\n\ngpus <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>]\n\nout_par <span class=\"pl-k\">=</span> torch.nn.parallel.data_parallel(net, data, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>gpus, <span class=\"pl-v\">output_device</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Should be True:<span class=\"pl-pds\">'</span></span>, out_par.requires_grad)\n\n<span class=\"pl-k\">with</span> torch.no_grad():\n    out_par <span class=\"pl-k\">=</span> torch.nn.parallel.data_parallel(net, data, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>gpus, <span class=\"pl-v\">output_device</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Should be False:<span class=\"pl-pds\">'</span></span>, out_par.requires_grad)\n\nout_par <span class=\"pl-k\">=</span> torch.nn.parallel.data_parallel(net, data, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>gpus, <span class=\"pl-v\">output_device</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Should be True:<span class=\"pl-pds\">'</span></span>, out_par.requires_grad)</pre></div>\n<p>On 2 GPUs, this outputs</p>\n<pre><code>Should be True: True\nShould be False: False\nShould be True: False\n</code></pre>\n<p>which is wrong in the last line. On 1 GPU (set <code>gpus = [0]</code>) the output is correct.</p>", "body_text": "OS: ubuntu 16.04\nPyTorch version: 0.4.0a0+875925b\nHow you installed PyTorch (conda, pip, source): source\nPython version: 3.6.3\nCUDA/cuDNN version: v9.1, cudnn-9.1-linux-x64-v7\nGPU models and configuration: 4x Tesla M60\nGCC version (if compiling from source):  5.4.0\n\nWhen using context torch.no_grad() on a model with data_parallel operation (same with torch.nn.DataParallel) on >1 GPU the flag requires_grad does not come back to True.\nIf only one GPU is used then everything works fine.\nScript:\nimport torch\n\ndata = torch.cuda.FloatTensor(64, 3, 100, 100)\nnet = torch.nn.Conv2d(3, 1, 3)\nnet.cuda()\n\ngpus = [0, 1]\n\nout_par = torch.nn.parallel.data_parallel(net, data, device_ids=gpus, output_device=0)\nprint('Should be True:', out_par.requires_grad)\n\nwith torch.no_grad():\n    out_par = torch.nn.parallel.data_parallel(net, data, device_ids=gpus, output_device=0)\n    print('Should be False:', out_par.requires_grad)\n\nout_par = torch.nn.parallel.data_parallel(net, data, device_ids=gpus, output_device=0)\nprint('Should be True:', out_par.requires_grad)\nOn 2 GPUs, this outputs\nShould be True: True\nShould be False: False\nShould be True: False\n\nwhich is wrong in the last line. On 1 GPU (set gpus = [0]) the output is correct.", "body": "- OS: ubuntu 16.04\r\n- PyTorch version: 0.4.0a0+875925b\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: v9.1, cudnn-9.1-linux-x64-v7\r\n- GPU models and configuration: 4x Tesla M60\r\n- GCC version (if compiling from source):  5.4.0\r\n\r\nWhen using context torch.no_grad() on a model with data_parallel operation (same with torch.nn.DataParallel) on >1 GPU the flag requires_grad does not come back to True.\r\nIf only one GPU is used then everything works fine.\r\n\r\nScript:\r\n```python\r\nimport torch\r\n\r\ndata = torch.cuda.FloatTensor(64, 3, 100, 100)\r\nnet = torch.nn.Conv2d(3, 1, 3)\r\nnet.cuda()\r\n\r\ngpus = [0, 1]\r\n\r\nout_par = torch.nn.parallel.data_parallel(net, data, device_ids=gpus, output_device=0)\r\nprint('Should be True:', out_par.requires_grad)\r\n\r\nwith torch.no_grad():\r\n    out_par = torch.nn.parallel.data_parallel(net, data, device_ids=gpus, output_device=0)\r\n    print('Should be False:', out_par.requires_grad)\r\n\r\nout_par = torch.nn.parallel.data_parallel(net, data, device_ids=gpus, output_device=0)\r\nprint('Should be True:', out_par.requires_grad)\r\n```\r\nOn 2 GPUs, this outputs\r\n```\r\nShould be True: True\r\nShould be False: False\r\nShould be True: False\r\n```\r\nwhich is wrong in the last line. On 1 GPU (set `gpus = [0]`) the output is correct.\r\n\r\n\r\n\r\n"}