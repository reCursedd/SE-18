{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12576", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12576/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12576/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12576/events", "html_url": "https://github.com/pytorch/pytorch/issues/12576", "id": 369254814, "node_id": "MDU6SXNzdWUzNjkyNTQ4MTQ=", "number": 12576, "title": "[feature request] `ignore_label` argument in Caffe2 `SoftmaxWithLoss`", "user": {"login": "Feynman27", "id": 12432112, "node_id": "MDQ6VXNlcjEyNDMyMTEy", "avatar_url": "https://avatars1.githubusercontent.com/u/12432112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Feynman27", "html_url": "https://github.com/Feynman27", "followers_url": "https://api.github.com/users/Feynman27/followers", "following_url": "https://api.github.com/users/Feynman27/following{/other_user}", "gists_url": "https://api.github.com/users/Feynman27/gists{/gist_id}", "starred_url": "https://api.github.com/users/Feynman27/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Feynman27/subscriptions", "organizations_url": "https://api.github.com/users/Feynman27/orgs", "repos_url": "https://api.github.com/users/Feynman27/repos", "events_url": "https://api.github.com/users/Feynman27/events{/privacy}", "received_events_url": "https://api.github.com/users/Feynman27/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-11T18:42:56Z", "updated_at": "2018-10-23T19:39:13Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n\n<p>Similar to the Pytorch implementation of <a href=\"https://pytorch.org/docs/stable/nn.html#crossentropyloss\" rel=\"nofollow\">crossentropyloss</a>, it'd be nice to have an <code>ignore_index</code> or <code>ignore_label</code> argument in the Caffe2 <a href=\"https://github.com/pytorch/pytorch/blob/a4120fa132849028d2c84dee684b3dda1ef4e8b2/caffe2/operators/softmax_with_loss_op.cc#L134\">SoftmaxWithLoss</a> or <a href=\"https://github.com/pytorch/pytorch/blob/7035975508ed053b5f1ac08b96ac6d6b2bbb954e/caffe2/operators/softmax_ops.cu#L13\">softmax_ops.cu</a>.</p>\n<h2>Motivation</h2>\n<p>In practice, I often experience the need to include additional loss terms (e.g. additional classification losses) that are applicable to some classes but not others. For example, say that I would like to predict gender for only the <code>person</code> class, but would like to ignore the <code>gender</code> classification for other classes. I could simply set the gender <code>ignore_label = -1</code> for classes that don't apply. I'm currently finding it difficult to do this in Caffe2.</p>\n\n<h2>Pitch</h2>\n\n<h2>Alternatives</h2>\n\n<h2>Additional context</h2>\n\n<p>I suppose the cuda implementation in <a href=\"https://github.com/pytorch/pytorch/blob/7035975508ed053b5f1ac08b96ac6d6b2bbb954e/caffe2/operators/softmax_ops.cu#L13\">softmax_ops.cu</a> would look something like this:</p>\n<pre><code>#define DONTCARE (-1)\n__global__ void LabelCrossEntropyKernel(\n    const int N,\n    const int D,\n    const float* logPdata,\n    const int* labeldata,\n    const float* weights,\n    float* Ydata) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast&lt;int&gt;(labeldata[i]);\n    if (label != DONTCARE) {\n      CUDA_KERNEL_ASSERT(label &gt;= 0 &amp;&amp; label &lt; D);\n      float weight = weights ? weights[i] : 1.0;\n      Ydata[i] = -logPdata[i * D + label] * weight;\n    }\n    else {\n      Ydata[i] = 0;\n    }\n  }\n}\n\n__global__ void LabelCrossEntropyGradientKernel(\n    const int N,\n    const int D,\n    const float* Pdata,\n    const int* labeldata,\n    float* dXdata) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast&lt;int&gt;(labeldata[i]);\n    if (label != DONTCARE) {\n      int idx = i * D + label;\n      dXdata[idx] = Pdata[idx] - 1.;\n    }\n    else {\n      for (int j=0; j&lt;D; j++){\n        int idx = i * D + j;\n        dXdata[idx] = 0.0;\n      }\n    }\n  }\n}\n\n</code></pre>", "body_text": "\ud83d\ude80 Feature\n\nSimilar to the Pytorch implementation of crossentropyloss, it'd be nice to have an ignore_index or ignore_label argument in the Caffe2 SoftmaxWithLoss or softmax_ops.cu.\nMotivation\nIn practice, I often experience the need to include additional loss terms (e.g. additional classification losses) that are applicable to some classes but not others. For example, say that I would like to predict gender for only the person class, but would like to ignore the gender classification for other classes. I could simply set the gender ignore_label = -1 for classes that don't apply. I'm currently finding it difficult to do this in Caffe2.\n\nPitch\n\nAlternatives\n\nAdditional context\n\nI suppose the cuda implementation in softmax_ops.cu would look something like this:\n#define DONTCARE (-1)\n__global__ void LabelCrossEntropyKernel(\n    const int N,\n    const int D,\n    const float* logPdata,\n    const int* labeldata,\n    const float* weights,\n    float* Ydata) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast<int>(labeldata[i]);\n    if (label != DONTCARE) {\n      CUDA_KERNEL_ASSERT(label >= 0 && label < D);\n      float weight = weights ? weights[i] : 1.0;\n      Ydata[i] = -logPdata[i * D + label] * weight;\n    }\n    else {\n      Ydata[i] = 0;\n    }\n  }\n}\n\n__global__ void LabelCrossEntropyGradientKernel(\n    const int N,\n    const int D,\n    const float* Pdata,\n    const int* labeldata,\n    float* dXdata) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast<int>(labeldata[i]);\n    if (label != DONTCARE) {\n      int idx = i * D + label;\n      dXdata[idx] = Pdata[idx] - 1.;\n    }\n    else {\n      for (int j=0; j<D; j++){\n        int idx = i * D + j;\n        dXdata[idx] = 0.0;\n      }\n    }\n  }\n}", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nSimilar to the Pytorch implementation of [crossentropyloss](https://pytorch.org/docs/stable/nn.html#crossentropyloss), it'd be nice to have an `ignore_index` or `ignore_label` argument in the Caffe2 [SoftmaxWithLoss](https://github.com/pytorch/pytorch/blob/a4120fa132849028d2c84dee684b3dda1ef4e8b2/caffe2/operators/softmax_with_loss_op.cc#L134) or [softmax_ops.cu](https://github.com/pytorch/pytorch/blob/7035975508ed053b5f1ac08b96ac6d6b2bbb954e/caffe2/operators/softmax_ops.cu#L13).\r\n\r\n## Motivation\r\nIn practice, I often experience the need to include additional loss terms (e.g. additional classification losses) that are applicable to some classes but not others. For example, say that I would like to predict gender for only the `person` class, but would like to ignore the `gender` classification for other classes. I could simply set the gender `ignore_label = -1` for classes that don't apply. I'm currently finding it difficult to do this in Caffe2. \r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nI suppose the cuda implementation in [softmax_ops.cu](https://github.com/pytorch/pytorch/blob/7035975508ed053b5f1ac08b96ac6d6b2bbb954e/caffe2/operators/softmax_ops.cu#L13) would look something like this:\r\n```\r\n#define DONTCARE (-1)\r\n__global__ void LabelCrossEntropyKernel(\r\n    const int N,\r\n    const int D,\r\n    const float* logPdata,\r\n    const int* labeldata,\r\n    const float* weights,\r\n    float* Ydata) {\r\n  CUDA_1D_KERNEL_LOOP(i, N) {\r\n    const int label = static_cast<int>(labeldata[i]);\r\n    if (label != DONTCARE) {\r\n      CUDA_KERNEL_ASSERT(label >= 0 && label < D);\r\n      float weight = weights ? weights[i] : 1.0;\r\n      Ydata[i] = -logPdata[i * D + label] * weight;\r\n    }\r\n    else {\r\n      Ydata[i] = 0;\r\n    }\r\n  }\r\n}\r\n\r\n__global__ void LabelCrossEntropyGradientKernel(\r\n    const int N,\r\n    const int D,\r\n    const float* Pdata,\r\n    const int* labeldata,\r\n    float* dXdata) {\r\n  CUDA_1D_KERNEL_LOOP(i, N) {\r\n    const int label = static_cast<int>(labeldata[i]);\r\n    if (label != DONTCARE) {\r\n      int idx = i * D + label;\r\n      dXdata[idx] = Pdata[idx] - 1.;\r\n    }\r\n    else {\r\n      for (int j=0; j<D; j++){\r\n        int idx = i * D + j;\r\n        dXdata[idx] = 0.0;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```"}