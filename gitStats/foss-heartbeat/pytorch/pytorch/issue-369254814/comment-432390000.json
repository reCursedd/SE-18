{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/432390000", "html_url": "https://github.com/pytorch/pytorch/issues/12576#issuecomment-432390000", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12576", "id": 432390000, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjM5MDAwMA==", "user": {"login": "Feynman27", "id": 12432112, "node_id": "MDQ6VXNlcjEyNDMyMTEy", "avatar_url": "https://avatars1.githubusercontent.com/u/12432112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Feynman27", "html_url": "https://github.com/Feynman27", "followers_url": "https://api.github.com/users/Feynman27/followers", "following_url": "https://api.github.com/users/Feynman27/following{/other_user}", "gists_url": "https://api.github.com/users/Feynman27/gists{/gist_id}", "starred_url": "https://api.github.com/users/Feynman27/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Feynman27/subscriptions", "organizations_url": "https://api.github.com/users/Feynman27/orgs", "repos_url": "https://api.github.com/users/Feynman27/repos", "events_url": "https://api.github.com/users/Feynman27/events{/privacy}", "received_events_url": "https://api.github.com/users/Feynman27/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-23T19:39:13Z", "updated_at": "2018-10-23T19:39:13Z", "author_association": "NONE", "body_html": "<p>I was able to train this model by modifying <code>softmax_ops.cu</code> to the following:</p>\n<pre><code>#define DONTCARE (-1)\n__global__ void LabelCrossEntropyKernel(\n    const int N,\n    const int D,\n    const float* logPdata,\n    const int* labeldata,\n    const float* weights,\n    float* Ydata, \n    float* weight_data) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast&lt;int&gt;(labeldata[i]);\n    if (label != DONTCARE) {\n      CUDA_KERNEL_ASSERT(label &gt;= 0 &amp;&amp; label &lt; D);\n      float weight = weights ? weights[i] : 1.0;\n      Ydata[i] = -logPdata[i * D + label] * weight;\n      weight_data[i] = weight;\n    } else {\n      Ydata[i] = 0;\n      weight_data[i] = 0;\n    }\n  }\n}\n\n__global__ void LabelCrossEntropyGradientKernel(\n    const int N,\n    const int D,\n    const float* Pdata,\n    const int* labeldata,\n    float* dXdata, \n    float* weights_) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast&lt;int&gt;(labeldata[i]);\n    if (label != DONTCARE) {\n      int idx = i * D + label;\n      dXdata[idx] = Pdata[idx] - 1.;\n      weights_[i] = 1.0;\n    } else {\n      // Ignore-label, so set all gradients to zero\n      for (int c = 0; c &lt; D; ++c) {\n        int idx = i * D + c;\n        dXdata[idx] = 0.0;\n      }\n      weights_[i] = 0.0;\n    }\n  }\n}\n...\ntemplate&lt;&gt;\nbool SoftmaxWithLossOp&lt;float, CUDAContext&gt;::RunOnDevice() {\n  auto&amp; X = Input(0);  // Logits\n  auto&amp; T = Input(1);  // Labels / targets\n  auto* P = Output(0); // Probabilities from softmax\n  auto* avg_loss = Output(1); // Average loss\n  const float* weights = (InputSize() &gt; 2 ? Input(2).data&lt;float&gt;() : NULL);\n\n  const auto canonical_axis = X.canonical_axis_index(axis_);\n  int N, D;\n  N = X.size_to_dim(canonical_axis); // batch size\n  D = X.size_from_dim(canonical_axis);\n  P-&gt;ResizeLike(X);\n  total_weight_ptr_.Resize(1);\n\n  if (label_prob_mode_) {\n    CAFFE_ENFORCE_GE(T.ndim(), 2);\n    CAFFE_ENFORCE_EQ(T.size_to_dim(canonical_axis), N);\n    CAFFE_ENFORCE_EQ(T.size_from_dim(canonical_axis), D);\n  } else {\n    if (T.ndim() == canonical_axis) {\n      CAFFE_ENFORCE_EQ(T.size(), N);\n    } else {\n      CAFFE_ENFORCE_EQ(T.size_to_dim(canonical_axis), N);\n      CAFFE_ENFORCE_EQ(T.size_from_dim(canonical_axis), 1);\n    }\n  }\n\n  avg_loss-&gt;Resize(vector&lt;TIndex&gt;());\n  if (losses_.size() != N) {\n    losses_.Resize(N);\n  }\n  if (weights_.size() != N) {\n    weights_.Resize(N);\n  }\n  if (rowmax_.size() != N) {\n    rowmax_.Resize(N);\n  }\n  if (sum_multiplier_.size() != D) {\n    sum_multiplier_.Resize(D);\n    math::Set&lt;float, CUDAContext&gt;(\n        D, 1.f, sum_multiplier_.mutable_data&lt;float&gt;(), &amp;context_);\n  }\n  Softmax(\n      N,\n      D,\n      X.data&lt;float&gt;(),\n      sum_multiplier_.data&lt;float&gt;(),\n      losses_.mutable_data&lt;float&gt;(),\n      rowmax_.mutable_data&lt;float&gt;(),\n      P-&gt;mutable_data&lt;float&gt;(),\n      !label_prob_mode_, // logarithmic output\n      &amp;context_);\n  // Compute label xent loss per example\n  if (!label_prob_mode_) {\n    LabelCrossEntropyKernel&lt;&lt;&lt;\n        CAFFE_GET_BLOCKS(N),\n        CAFFE_CUDA_NUM_THREADS,\n        0,\n        context_.cuda_stream()&gt;&gt;&gt;(\n        N,\n        D,\n        P-&gt;data&lt;float&gt;(),\n        T.data&lt;int&gt;(),\n        weights,\n        losses_.mutable_data&lt;float&gt;(),\n        weights_.mutable_data&lt;float&gt;());\n    // Since we had logarithmic output, we need to exponentiate\n    // them again.\n    math::Exp&lt;float, CUDAContext&gt;(\n        N * D, P-&gt;data&lt;float&gt;(), P-&gt;mutable_data&lt;float&gt;(), &amp;context_);\n  } else {\n    ProbCrossEntropyKernel&lt;&lt;&lt;\n        std::min(N, CAFFE_MAXIMUM_NUM_BLOCKS),\n        CAFFE_CUDA_NUM_THREADS,\n        0,\n        context_.cuda_stream()&gt;&gt;&gt;(\n        N,\n        D,\n        P-&gt;data&lt;float&gt;(),\n        T.data&lt;float&gt;(),\n        weights,\n        losses_.mutable_data&lt;float&gt;());\n  }\n\n  float total_weight = N;\n  if (weights) {\n    // Sum weights\n    math::Sum&lt;float, CUDAContext&gt;(\n        N, weights, total_weight_ptr_.mutable_data&lt;float&gt;(), &amp;context_, &amp;scratch_);\n    CUDA_CHECK(cudaMemcpyAsync(\n        &amp;total_weight,\n        total_weight_ptr_.data&lt;float&gt;(),\n        sizeof(float),\n        cudaMemcpyDeviceToHost,\n        context_.cuda_stream()));\n  } else {\n    math::Sum&lt;float, CUDAContext&gt;(\n        weights_.size(),\n        weights_.data&lt;float&gt;(),\n        total_weight_ptr_.mutable_data&lt;float&gt;(),\n        &amp;context_,\n        &amp;scratch_);\n    CUDA_CHECK(cudaMemcpyAsync(\n        &amp;total_weight,\n        total_weight_ptr_.data&lt;float&gt;(),\n        sizeof(float),\n        cudaMemcpyDeviceToHost,\n        context_.cuda_stream()));\n  } \n  // Sum of all losses\n  float* avg_loss_data = avg_loss-&gt;mutable_data&lt;float&gt;();\n  math::Sum&lt;float, CUDAContext&gt;(\n      losses_.size(), losses_.data&lt;float&gt;(), avg_loss_data, &amp;context_, &amp;scratch_);\n  // Average of input batch size\n  if (total_weight &gt; 0) {\n    math::Scale&lt;float, CUDAContext&gt;(\n        1, scale_ / total_weight, avg_loss_data, avg_loss_data, &amp;context_);\n  }\n\n  return true;\n}\n\n</code></pre>\n<p>I also added the unit test to <code>softmax_ops_test.py</code>:</p>\n<pre><code>    @given(n=st.integers(1,2), D=st.integers(2,3), **hu.gcs_gpu_only)\n    def test_softmax_with_ignore_loss(self, n, D, gc, dc):\n        # n = number of examples, D = |labels|\n        # Initialize X and add 1e-2 for numerical stability\n        np.random.seed(2603)\n        X = np.random.rand(n, D).astype(np.float32)\n        X = X + 1e-2\n\n        # Initialize label. Some labels are (-1) i.e. \"DONT CARE\"\n        label = (np.random.rand(n) * (D + 1)).astype(np.int32) - 1\n        print(label)\n\n        # Reference implementation of cross entropy with soft labels\n        def label_softmax_crossent(X, label):\n            probs = np.zeros((n, D))\n            rowmax = np.zeros(n)\n            for i in range(n):\n                rowmax[i] = max(X[i, ])\n                # We need to subtract the max to avoid numerical issues\n                probs[i] = X[i] - rowmax[i]\n                exps = np.exp(probs[i, ])\n                norm = sum(exps)\n                probs[i, ] = exps / norm\n\n            label_xent = [-np.log(max(probs[i][label[i]], 1e-20))\n                          for i in range(n)]\n            #label_xent = [-np.log(max(probs[i][label[i]], 1e-20))\n            #              if label[i]!=-1 else 1e-20 for i in range(n)]\n            total_xent = 0.0\n            total_weight = 0.0\n            for i in range(n):\n                l = label[i]\n                if (l!=(-1)):\n                    total_xent += -np.log(max(probs[i,l], 1e-20))\n                    total_weight += 1.0\n            #avgloss = np.sum(label_xent) / float(n)\n            avgloss = total_xent / total_weight\n            print(avgloss)\n            return (probs, avgloss)\n\n        op = core.CreateOperator(\n            \"SoftmaxWithLoss\",\n            [\"X\", \"label\"],\n            [\"probs\", \"avgloss\"],\n        )\n\n        self.assertReferenceChecks(\n            device_option=gc,\n            op=op,\n            inputs=[X, label],\n            reference=label_softmax_crossent,\n        )\n\n        self.assertGradientChecks(\n            gc, op, [X, label], 0, [1], stepsize=1e-4, threshold=1e-2)\n\n    @given(n=st.sampled_from([0, 2, 4, 71, 103, 555, 751, 1201]),\n           D=st.sampled_from([4, 8, 64, 79, 256, 333, 1000]),\n           engine=st.sampled_from([None, 'CUDNN']),\n           **hu.gcs)\n</code></pre>\n<p>However, at the moment, this only works on the GPU and breaks any tests with <code>label_prob</code> ( <code>def test_softmax_with_loss_axis_2</code> and <code>test_softmax_with_loss_label_prob</code>). All other tests pass.</p>", "body_text": "I was able to train this model by modifying softmax_ops.cu to the following:\n#define DONTCARE (-1)\n__global__ void LabelCrossEntropyKernel(\n    const int N,\n    const int D,\n    const float* logPdata,\n    const int* labeldata,\n    const float* weights,\n    float* Ydata, \n    float* weight_data) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast<int>(labeldata[i]);\n    if (label != DONTCARE) {\n      CUDA_KERNEL_ASSERT(label >= 0 && label < D);\n      float weight = weights ? weights[i] : 1.0;\n      Ydata[i] = -logPdata[i * D + label] * weight;\n      weight_data[i] = weight;\n    } else {\n      Ydata[i] = 0;\n      weight_data[i] = 0;\n    }\n  }\n}\n\n__global__ void LabelCrossEntropyGradientKernel(\n    const int N,\n    const int D,\n    const float* Pdata,\n    const int* labeldata,\n    float* dXdata, \n    float* weights_) {\n  CUDA_1D_KERNEL_LOOP(i, N) {\n    const int label = static_cast<int>(labeldata[i]);\n    if (label != DONTCARE) {\n      int idx = i * D + label;\n      dXdata[idx] = Pdata[idx] - 1.;\n      weights_[i] = 1.0;\n    } else {\n      // Ignore-label, so set all gradients to zero\n      for (int c = 0; c < D; ++c) {\n        int idx = i * D + c;\n        dXdata[idx] = 0.0;\n      }\n      weights_[i] = 0.0;\n    }\n  }\n}\n...\ntemplate<>\nbool SoftmaxWithLossOp<float, CUDAContext>::RunOnDevice() {\n  auto& X = Input(0);  // Logits\n  auto& T = Input(1);  // Labels / targets\n  auto* P = Output(0); // Probabilities from softmax\n  auto* avg_loss = Output(1); // Average loss\n  const float* weights = (InputSize() > 2 ? Input(2).data<float>() : NULL);\n\n  const auto canonical_axis = X.canonical_axis_index(axis_);\n  int N, D;\n  N = X.size_to_dim(canonical_axis); // batch size\n  D = X.size_from_dim(canonical_axis);\n  P->ResizeLike(X);\n  total_weight_ptr_.Resize(1);\n\n  if (label_prob_mode_) {\n    CAFFE_ENFORCE_GE(T.ndim(), 2);\n    CAFFE_ENFORCE_EQ(T.size_to_dim(canonical_axis), N);\n    CAFFE_ENFORCE_EQ(T.size_from_dim(canonical_axis), D);\n  } else {\n    if (T.ndim() == canonical_axis) {\n      CAFFE_ENFORCE_EQ(T.size(), N);\n    } else {\n      CAFFE_ENFORCE_EQ(T.size_to_dim(canonical_axis), N);\n      CAFFE_ENFORCE_EQ(T.size_from_dim(canonical_axis), 1);\n    }\n  }\n\n  avg_loss->Resize(vector<TIndex>());\n  if (losses_.size() != N) {\n    losses_.Resize(N);\n  }\n  if (weights_.size() != N) {\n    weights_.Resize(N);\n  }\n  if (rowmax_.size() != N) {\n    rowmax_.Resize(N);\n  }\n  if (sum_multiplier_.size() != D) {\n    sum_multiplier_.Resize(D);\n    math::Set<float, CUDAContext>(\n        D, 1.f, sum_multiplier_.mutable_data<float>(), &context_);\n  }\n  Softmax(\n      N,\n      D,\n      X.data<float>(),\n      sum_multiplier_.data<float>(),\n      losses_.mutable_data<float>(),\n      rowmax_.mutable_data<float>(),\n      P->mutable_data<float>(),\n      !label_prob_mode_, // logarithmic output\n      &context_);\n  // Compute label xent loss per example\n  if (!label_prob_mode_) {\n    LabelCrossEntropyKernel<<<\n        CAFFE_GET_BLOCKS(N),\n        CAFFE_CUDA_NUM_THREADS,\n        0,\n        context_.cuda_stream()>>>(\n        N,\n        D,\n        P->data<float>(),\n        T.data<int>(),\n        weights,\n        losses_.mutable_data<float>(),\n        weights_.mutable_data<float>());\n    // Since we had logarithmic output, we need to exponentiate\n    // them again.\n    math::Exp<float, CUDAContext>(\n        N * D, P->data<float>(), P->mutable_data<float>(), &context_);\n  } else {\n    ProbCrossEntropyKernel<<<\n        std::min(N, CAFFE_MAXIMUM_NUM_BLOCKS),\n        CAFFE_CUDA_NUM_THREADS,\n        0,\n        context_.cuda_stream()>>>(\n        N,\n        D,\n        P->data<float>(),\n        T.data<float>(),\n        weights,\n        losses_.mutable_data<float>());\n  }\n\n  float total_weight = N;\n  if (weights) {\n    // Sum weights\n    math::Sum<float, CUDAContext>(\n        N, weights, total_weight_ptr_.mutable_data<float>(), &context_, &scratch_);\n    CUDA_CHECK(cudaMemcpyAsync(\n        &total_weight,\n        total_weight_ptr_.data<float>(),\n        sizeof(float),\n        cudaMemcpyDeviceToHost,\n        context_.cuda_stream()));\n  } else {\n    math::Sum<float, CUDAContext>(\n        weights_.size(),\n        weights_.data<float>(),\n        total_weight_ptr_.mutable_data<float>(),\n        &context_,\n        &scratch_);\n    CUDA_CHECK(cudaMemcpyAsync(\n        &total_weight,\n        total_weight_ptr_.data<float>(),\n        sizeof(float),\n        cudaMemcpyDeviceToHost,\n        context_.cuda_stream()));\n  } \n  // Sum of all losses\n  float* avg_loss_data = avg_loss->mutable_data<float>();\n  math::Sum<float, CUDAContext>(\n      losses_.size(), losses_.data<float>(), avg_loss_data, &context_, &scratch_);\n  // Average of input batch size\n  if (total_weight > 0) {\n    math::Scale<float, CUDAContext>(\n        1, scale_ / total_weight, avg_loss_data, avg_loss_data, &context_);\n  }\n\n  return true;\n}\n\n\nI also added the unit test to softmax_ops_test.py:\n    @given(n=st.integers(1,2), D=st.integers(2,3), **hu.gcs_gpu_only)\n    def test_softmax_with_ignore_loss(self, n, D, gc, dc):\n        # n = number of examples, D = |labels|\n        # Initialize X and add 1e-2 for numerical stability\n        np.random.seed(2603)\n        X = np.random.rand(n, D).astype(np.float32)\n        X = X + 1e-2\n\n        # Initialize label. Some labels are (-1) i.e. \"DONT CARE\"\n        label = (np.random.rand(n) * (D + 1)).astype(np.int32) - 1\n        print(label)\n\n        # Reference implementation of cross entropy with soft labels\n        def label_softmax_crossent(X, label):\n            probs = np.zeros((n, D))\n            rowmax = np.zeros(n)\n            for i in range(n):\n                rowmax[i] = max(X[i, ])\n                # We need to subtract the max to avoid numerical issues\n                probs[i] = X[i] - rowmax[i]\n                exps = np.exp(probs[i, ])\n                norm = sum(exps)\n                probs[i, ] = exps / norm\n\n            label_xent = [-np.log(max(probs[i][label[i]], 1e-20))\n                          for i in range(n)]\n            #label_xent = [-np.log(max(probs[i][label[i]], 1e-20))\n            #              if label[i]!=-1 else 1e-20 for i in range(n)]\n            total_xent = 0.0\n            total_weight = 0.0\n            for i in range(n):\n                l = label[i]\n                if (l!=(-1)):\n                    total_xent += -np.log(max(probs[i,l], 1e-20))\n                    total_weight += 1.0\n            #avgloss = np.sum(label_xent) / float(n)\n            avgloss = total_xent / total_weight\n            print(avgloss)\n            return (probs, avgloss)\n\n        op = core.CreateOperator(\n            \"SoftmaxWithLoss\",\n            [\"X\", \"label\"],\n            [\"probs\", \"avgloss\"],\n        )\n\n        self.assertReferenceChecks(\n            device_option=gc,\n            op=op,\n            inputs=[X, label],\n            reference=label_softmax_crossent,\n        )\n\n        self.assertGradientChecks(\n            gc, op, [X, label], 0, [1], stepsize=1e-4, threshold=1e-2)\n\n    @given(n=st.sampled_from([0, 2, 4, 71, 103, 555, 751, 1201]),\n           D=st.sampled_from([4, 8, 64, 79, 256, 333, 1000]),\n           engine=st.sampled_from([None, 'CUDNN']),\n           **hu.gcs)\n\nHowever, at the moment, this only works on the GPU and breaks any tests with label_prob ( def test_softmax_with_loss_axis_2 and test_softmax_with_loss_label_prob). All other tests pass.", "body": "I was able to train this model by modifying `softmax_ops.cu` to the following:\r\n\r\n```\r\n#define DONTCARE (-1)\r\n__global__ void LabelCrossEntropyKernel(\r\n    const int N,\r\n    const int D,\r\n    const float* logPdata,\r\n    const int* labeldata,\r\n    const float* weights,\r\n    float* Ydata, \r\n    float* weight_data) {\r\n  CUDA_1D_KERNEL_LOOP(i, N) {\r\n    const int label = static_cast<int>(labeldata[i]);\r\n    if (label != DONTCARE) {\r\n      CUDA_KERNEL_ASSERT(label >= 0 && label < D);\r\n      float weight = weights ? weights[i] : 1.0;\r\n      Ydata[i] = -logPdata[i * D + label] * weight;\r\n      weight_data[i] = weight;\r\n    } else {\r\n      Ydata[i] = 0;\r\n      weight_data[i] = 0;\r\n    }\r\n  }\r\n}\r\n\r\n__global__ void LabelCrossEntropyGradientKernel(\r\n    const int N,\r\n    const int D,\r\n    const float* Pdata,\r\n    const int* labeldata,\r\n    float* dXdata, \r\n    float* weights_) {\r\n  CUDA_1D_KERNEL_LOOP(i, N) {\r\n    const int label = static_cast<int>(labeldata[i]);\r\n    if (label != DONTCARE) {\r\n      int idx = i * D + label;\r\n      dXdata[idx] = Pdata[idx] - 1.;\r\n      weights_[i] = 1.0;\r\n    } else {\r\n      // Ignore-label, so set all gradients to zero\r\n      for (int c = 0; c < D; ++c) {\r\n        int idx = i * D + c;\r\n        dXdata[idx] = 0.0;\r\n      }\r\n      weights_[i] = 0.0;\r\n    }\r\n  }\r\n}\r\n...\r\ntemplate<>\r\nbool SoftmaxWithLossOp<float, CUDAContext>::RunOnDevice() {\r\n  auto& X = Input(0);  // Logits\r\n  auto& T = Input(1);  // Labels / targets\r\n  auto* P = Output(0); // Probabilities from softmax\r\n  auto* avg_loss = Output(1); // Average loss\r\n  const float* weights = (InputSize() > 2 ? Input(2).data<float>() : NULL);\r\n\r\n  const auto canonical_axis = X.canonical_axis_index(axis_);\r\n  int N, D;\r\n  N = X.size_to_dim(canonical_axis); // batch size\r\n  D = X.size_from_dim(canonical_axis);\r\n  P->ResizeLike(X);\r\n  total_weight_ptr_.Resize(1);\r\n\r\n  if (label_prob_mode_) {\r\n    CAFFE_ENFORCE_GE(T.ndim(), 2);\r\n    CAFFE_ENFORCE_EQ(T.size_to_dim(canonical_axis), N);\r\n    CAFFE_ENFORCE_EQ(T.size_from_dim(canonical_axis), D);\r\n  } else {\r\n    if (T.ndim() == canonical_axis) {\r\n      CAFFE_ENFORCE_EQ(T.size(), N);\r\n    } else {\r\n      CAFFE_ENFORCE_EQ(T.size_to_dim(canonical_axis), N);\r\n      CAFFE_ENFORCE_EQ(T.size_from_dim(canonical_axis), 1);\r\n    }\r\n  }\r\n\r\n  avg_loss->Resize(vector<TIndex>());\r\n  if (losses_.size() != N) {\r\n    losses_.Resize(N);\r\n  }\r\n  if (weights_.size() != N) {\r\n    weights_.Resize(N);\r\n  }\r\n  if (rowmax_.size() != N) {\r\n    rowmax_.Resize(N);\r\n  }\r\n  if (sum_multiplier_.size() != D) {\r\n    sum_multiplier_.Resize(D);\r\n    math::Set<float, CUDAContext>(\r\n        D, 1.f, sum_multiplier_.mutable_data<float>(), &context_);\r\n  }\r\n  Softmax(\r\n      N,\r\n      D,\r\n      X.data<float>(),\r\n      sum_multiplier_.data<float>(),\r\n      losses_.mutable_data<float>(),\r\n      rowmax_.mutable_data<float>(),\r\n      P->mutable_data<float>(),\r\n      !label_prob_mode_, // logarithmic output\r\n      &context_);\r\n  // Compute label xent loss per example\r\n  if (!label_prob_mode_) {\r\n    LabelCrossEntropyKernel<<<\r\n        CAFFE_GET_BLOCKS(N),\r\n        CAFFE_CUDA_NUM_THREADS,\r\n        0,\r\n        context_.cuda_stream()>>>(\r\n        N,\r\n        D,\r\n        P->data<float>(),\r\n        T.data<int>(),\r\n        weights,\r\n        losses_.mutable_data<float>(),\r\n        weights_.mutable_data<float>());\r\n    // Since we had logarithmic output, we need to exponentiate\r\n    // them again.\r\n    math::Exp<float, CUDAContext>(\r\n        N * D, P->data<float>(), P->mutable_data<float>(), &context_);\r\n  } else {\r\n    ProbCrossEntropyKernel<<<\r\n        std::min(N, CAFFE_MAXIMUM_NUM_BLOCKS),\r\n        CAFFE_CUDA_NUM_THREADS,\r\n        0,\r\n        context_.cuda_stream()>>>(\r\n        N,\r\n        D,\r\n        P->data<float>(),\r\n        T.data<float>(),\r\n        weights,\r\n        losses_.mutable_data<float>());\r\n  }\r\n\r\n  float total_weight = N;\r\n  if (weights) {\r\n    // Sum weights\r\n    math::Sum<float, CUDAContext>(\r\n        N, weights, total_weight_ptr_.mutable_data<float>(), &context_, &scratch_);\r\n    CUDA_CHECK(cudaMemcpyAsync(\r\n        &total_weight,\r\n        total_weight_ptr_.data<float>(),\r\n        sizeof(float),\r\n        cudaMemcpyDeviceToHost,\r\n        context_.cuda_stream()));\r\n  } else {\r\n    math::Sum<float, CUDAContext>(\r\n        weights_.size(),\r\n        weights_.data<float>(),\r\n        total_weight_ptr_.mutable_data<float>(),\r\n        &context_,\r\n        &scratch_);\r\n    CUDA_CHECK(cudaMemcpyAsync(\r\n        &total_weight,\r\n        total_weight_ptr_.data<float>(),\r\n        sizeof(float),\r\n        cudaMemcpyDeviceToHost,\r\n        context_.cuda_stream()));\r\n  } \r\n  // Sum of all losses\r\n  float* avg_loss_data = avg_loss->mutable_data<float>();\r\n  math::Sum<float, CUDAContext>(\r\n      losses_.size(), losses_.data<float>(), avg_loss_data, &context_, &scratch_);\r\n  // Average of input batch size\r\n  if (total_weight > 0) {\r\n    math::Scale<float, CUDAContext>(\r\n        1, scale_ / total_weight, avg_loss_data, avg_loss_data, &context_);\r\n  }\r\n\r\n  return true;\r\n}\r\n\r\n```\r\n\r\nI also added the unit test to `softmax_ops_test.py`:\r\n\r\n```\r\n    @given(n=st.integers(1,2), D=st.integers(2,3), **hu.gcs_gpu_only)\r\n    def test_softmax_with_ignore_loss(self, n, D, gc, dc):\r\n        # n = number of examples, D = |labels|\r\n        # Initialize X and add 1e-2 for numerical stability\r\n        np.random.seed(2603)\r\n        X = np.random.rand(n, D).astype(np.float32)\r\n        X = X + 1e-2\r\n\r\n        # Initialize label. Some labels are (-1) i.e. \"DONT CARE\"\r\n        label = (np.random.rand(n) * (D + 1)).astype(np.int32) - 1\r\n        print(label)\r\n\r\n        # Reference implementation of cross entropy with soft labels\r\n        def label_softmax_crossent(X, label):\r\n            probs = np.zeros((n, D))\r\n            rowmax = np.zeros(n)\r\n            for i in range(n):\r\n                rowmax[i] = max(X[i, ])\r\n                # We need to subtract the max to avoid numerical issues\r\n                probs[i] = X[i] - rowmax[i]\r\n                exps = np.exp(probs[i, ])\r\n                norm = sum(exps)\r\n                probs[i, ] = exps / norm\r\n\r\n            label_xent = [-np.log(max(probs[i][label[i]], 1e-20))\r\n                          for i in range(n)]\r\n            #label_xent = [-np.log(max(probs[i][label[i]], 1e-20))\r\n            #              if label[i]!=-1 else 1e-20 for i in range(n)]\r\n            total_xent = 0.0\r\n            total_weight = 0.0\r\n            for i in range(n):\r\n                l = label[i]\r\n                if (l!=(-1)):\r\n                    total_xent += -np.log(max(probs[i,l], 1e-20))\r\n                    total_weight += 1.0\r\n            #avgloss = np.sum(label_xent) / float(n)\r\n            avgloss = total_xent / total_weight\r\n            print(avgloss)\r\n            return (probs, avgloss)\r\n\r\n        op = core.CreateOperator(\r\n            \"SoftmaxWithLoss\",\r\n            [\"X\", \"label\"],\r\n            [\"probs\", \"avgloss\"],\r\n        )\r\n\r\n        self.assertReferenceChecks(\r\n            device_option=gc,\r\n            op=op,\r\n            inputs=[X, label],\r\n            reference=label_softmax_crossent,\r\n        )\r\n\r\n        self.assertGradientChecks(\r\n            gc, op, [X, label], 0, [1], stepsize=1e-4, threshold=1e-2)\r\n\r\n    @given(n=st.sampled_from([0, 2, 4, 71, 103, 555, 751, 1201]),\r\n           D=st.sampled_from([4, 8, 64, 79, 256, 333, 1000]),\r\n           engine=st.sampled_from([None, 'CUDNN']),\r\n           **hu.gcs)\r\n```\r\n\r\nHowever, at the moment, this only works on the GPU and breaks any tests with `label_prob` ( `def test_softmax_with_loss_axis_2` and `test_softmax_with_loss_label_prob`). All other tests pass. "}