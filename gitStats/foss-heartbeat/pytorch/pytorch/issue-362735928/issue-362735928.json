{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11945", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11945/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11945/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11945/events", "html_url": "https://github.com/pytorch/pytorch/issues/11945", "id": 362735928, "node_id": "MDU6SXNzdWUzNjI3MzU5Mjg=", "number": 11945, "title": "3X - 5X slowdown in torch.bernoulli and torch.normal on PyTorch master", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-09-21T18:54:01Z", "updated_at": "2018-09-21T19:41:18Z", "closed_at": "2018-09-21T19:41:17Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>While debugging some perf issues in Pyro using PyTorch master, there seems to be a perf regression in the torch samplers (I checked <code>torch.bernoulli</code> and <code>torch.normal</code>). cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>\n<h2>Code example</h2>\n<p><strong>Release</strong></p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">import</span> torch.distributions <span class=\"pl-k\">as</span> dist\n\nIn [<span class=\"pl-c1\">3</span>]: torch.<span class=\"pl-c1\">__version__</span>\nOut[<span class=\"pl-c1\">3</span>]: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.4.1<span class=\"pl-pds\">'</span></span>\n\nIn [<span class=\"pl-c1\">4</span>]: z, o, b <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>), torch.ones(<span class=\"pl-c1\">1000</span>), torch.ones(<span class=\"pl-c1\">1000</span>) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">0.5</span>\n\nIn [<span class=\"pl-c1\">5</span>]: <span class=\"pl-k\">%</span>timeit dist.Normal(z, o).sample(torch.Size([<span class=\"pl-c1\">100</span>]))\n<span class=\"pl-c1\">858</span> \u00b5s \u00b1 <span class=\"pl-c1\">10.8</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\nIn [<span class=\"pl-c1\">6</span>]: <span class=\"pl-k\">%</span>timeit torch.normal(z, o)\n<span class=\"pl-c1\">8.28</span> \u00b5s \u00b1 <span class=\"pl-c1\">198</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100000</span> loops each)\n\nIn [<span class=\"pl-c1\">7</span>]: <span class=\"pl-k\">%</span>timeit dist.Bernoulli(b).sample(torch.Size([<span class=\"pl-c1\">100</span>]))\n<span class=\"pl-c1\">1.47</span> ms \u00b1 <span class=\"pl-c1\">11.5</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\nIn [<span class=\"pl-c1\">8</span>]: <span class=\"pl-k\">%</span>timeit torch.bernoulli(b)\n<span class=\"pl-c1\">19.1</span> \u00b5s \u00b1 <span class=\"pl-c1\">230</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100000</span> loops each)</pre></div>\n<p><strong>Master</strong></p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">import</span> torch.distributions <span class=\"pl-k\">as</span> dist\n\nIn [<span class=\"pl-c1\">3</span>]: torch.<span class=\"pl-c1\">__version__</span>\nOut[<span class=\"pl-c1\">3</span>]: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1.0.0a0+9cd0ae5<span class=\"pl-pds\">'</span></span>\n\nIn [<span class=\"pl-c1\">4</span>]: z, o, b <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>), torch.ones(<span class=\"pl-c1\">1000</span>), torch.ones(<span class=\"pl-c1\">1000</span>) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">0.5</span>\n\nIn [<span class=\"pl-c1\">5</span>]: <span class=\"pl-k\">%</span>timeit dist.Normal(z, o).sample(torch.Size([<span class=\"pl-c1\">100</span>]))\n<span class=\"pl-c1\">3.7</span> ms \u00b1 <span class=\"pl-c1\">55.9</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)\n\nIn [<span class=\"pl-c1\">6</span>]: <span class=\"pl-k\">%</span>timeit torch.normal(z, o)\n<span class=\"pl-c1\">39.9</span> \u00b5s \u00b1 <span class=\"pl-c1\">737</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10000</span> loops each)\n\nIn [<span class=\"pl-c1\">7</span>]: <span class=\"pl-k\">%</span>timeit dist.Bernoulli(b).sample(torch.Size([<span class=\"pl-c1\">100</span>]))\n<span class=\"pl-c1\">4.25</span> ms \u00b1 <span class=\"pl-c1\">77.1</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)\n\nIn [<span class=\"pl-c1\">8</span>]: <span class=\"pl-k\">%</span>timeit torch.bernoulli(b)\n<span class=\"pl-c1\">56</span> \u00b5s \u00b1 <span class=\"pl-c1\">765</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10000</span> loops each)</pre></div>\n<h2>System Info</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 1.0.0a0+9cd0ae5\nIs debug build: Yes\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] torch (1.0.0a0+9cd0ae5, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] torch                     0.5.0a0+2431eac           &lt;pip&gt;\n[conda] torch                     0.5.0a0+6c3792b           &lt;pip&gt;\n[conda] torch                     0.5.0a0+6660a12           &lt;pip&gt;\n[conda] torch                     0.5.0a0+35d52db           &lt;pip&gt;\n[conda] torch                     1.0.0a0+9cd0ae5           &lt;pip&gt;\n[conda] torchfile                 0.1.0                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>", "body_text": "Issue description\nWhile debugging some perf issues in Pyro using PyTorch master, there seems to be a perf regression in the torch samplers (I checked torch.bernoulli and torch.normal). cc. @apaszke, @fritzo, @soumith\nCode example\nRelease\nIn [1]: import torch\n\nIn [2]: import torch.distributions as dist\n\nIn [3]: torch.__version__\nOut[3]: '0.4.1'\n\nIn [4]: z, o, b = torch.zeros(1000), torch.ones(1000), torch.ones(1000) * 0.5\n\nIn [5]: %timeit dist.Normal(z, o).sample(torch.Size([100]))\n858 \u00b5s \u00b1 10.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nIn [6]: %timeit torch.normal(z, o)\n8.28 \u00b5s \u00b1 198 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\nIn [7]: %timeit dist.Bernoulli(b).sample(torch.Size([100]))\n1.47 ms \u00b1 11.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nIn [8]: %timeit torch.bernoulli(b)\n19.1 \u00b5s \u00b1 230 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\nMaster\nIn [1]: import torch\n\nIn [2]: import torch.distributions as dist\n\nIn [3]: torch.__version__\nOut[3]: '1.0.0a0+9cd0ae5'\n\nIn [4]: z, o, b = torch.zeros(1000), torch.ones(1000), torch.ones(1000) * 0.5\n\nIn [5]: %timeit dist.Normal(z, o).sample(torch.Size([100]))\n3.7 ms \u00b1 55.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [6]: %timeit torch.normal(z, o)\n39.9 \u00b5s \u00b1 737 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [7]: %timeit dist.Bernoulli(b).sample(torch.Size([100]))\n4.25 ms \u00b1 77.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [8]: %timeit torch.bernoulli(b)\n56 \u00b5s \u00b1 765 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nSystem Info\nCollecting environment information...\nPyTorch version: 1.0.0a0+9cd0ae5\nIs debug build: Yes\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] torch (1.0.0a0+9cd0ae5, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] torch                     0.5.0a0+2431eac           <pip>\n[conda] torch                     0.5.0a0+6c3792b           <pip>\n[conda] torch                     0.5.0a0+6660a12           <pip>\n[conda] torch                     0.5.0a0+35d52db           <pip>\n[conda] torch                     1.0.0a0+9cd0ae5           <pip>\n[conda] torchfile                 0.1.0                     <pip>\n[conda] torchvision               0.2.1                     <pip>", "body": "## Issue description\r\n\r\nWhile debugging some perf issues in Pyro using PyTorch master, there seems to be a perf regression in the torch samplers (I checked `torch.bernoulli` and `torch.normal`). cc. @apaszke, @fritzo, @soumith\r\n\r\n## Code example\r\n\r\n**Release**\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: import torch.distributions as dist\r\n\r\nIn [3]: torch.__version__\r\nOut[3]: '0.4.1'\r\n\r\nIn [4]: z, o, b = torch.zeros(1000), torch.ones(1000), torch.ones(1000) * 0.5\r\n\r\nIn [5]: %timeit dist.Normal(z, o).sample(torch.Size([100]))\r\n858 \u00b5s \u00b1 10.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\nIn [6]: %timeit torch.normal(z, o)\r\n8.28 \u00b5s \u00b1 198 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n\r\nIn [7]: %timeit dist.Bernoulli(b).sample(torch.Size([100]))\r\n1.47 ms \u00b1 11.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\nIn [8]: %timeit torch.bernoulli(b)\r\n19.1 \u00b5s \u00b1 230 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n```\r\n\r\n**Master**\r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: import torch.distributions as dist\r\n\r\nIn [3]: torch.__version__\r\nOut[3]: '1.0.0a0+9cd0ae5'\r\n\r\nIn [4]: z, o, b = torch.zeros(1000), torch.ones(1000), torch.ones(1000) * 0.5\r\n\r\nIn [5]: %timeit dist.Normal(z, o).sample(torch.Size([100]))\r\n3.7 ms \u00b1 55.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [6]: %timeit torch.normal(z, o)\r\n39.9 \u00b5s \u00b1 737 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [7]: %timeit dist.Bernoulli(b).sample(torch.Size([100]))\r\n4.25 ms \u00b1 77.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [8]: %timeit torch.bernoulli(b)\r\n56 \u00b5s \u00b1 765 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\n## System Info\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+9cd0ae5\r\nIs debug build: Yes\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] torch (1.0.0a0+9cd0ae5, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     0.5.0a0+2431eac           <pip>\r\n[conda] torch                     0.5.0a0+6c3792b           <pip>\r\n[conda] torch                     0.5.0a0+6660a12           <pip>\r\n[conda] torch                     0.5.0a0+35d52db           <pip>\r\n[conda] torch                     1.0.0a0+9cd0ae5           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n"}