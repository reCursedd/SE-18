{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13594", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13594/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13594/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13594/events", "html_url": "https://github.com/pytorch/pytorch/pull/13594", "id": 377595981, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI4NTAxOTI1", "number": 13594, "title": " Give broadcast_coalesced tensors different version counters", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-05T21:45:42Z", "updated_at": "2018-11-23T15:54:29Z", "closed_at": "2018-11-08T05:51:06Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13594", "html_url": "https://github.com/pytorch/pytorch/pull/13594", "diff_url": "https://github.com/pytorch/pytorch/pull/13594.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13594.patch"}, "body_html": "<p>In <code>broadcast_coalesced</code>, since multiple variables can be \"views\" of a big flattened tensor, they can share the same version counter. However, this base flat tensor is not exposed and they don't share any memory locations, so this is not necessary. Furthermore, it can cause problems, e.g., when two buffers are broadcast together in <code>DataParallel</code> and one of them is modified in-place during <code>forward</code> but the other is needed in backward, autograd engine will complain.</p>\n<p>Fixing the bug discovered at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"375725430\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13350\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13350/hovercard?comment_id=436011370&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/pull/13350#issuecomment-436011370\">#13350 (comment)</a></p>\n<p>edit: This is a very real problem. E.g., consider using Spectral Norm + Batch Norm together.</p>", "body_text": "In broadcast_coalesced, since multiple variables can be \"views\" of a big flattened tensor, they can share the same version counter. However, this base flat tensor is not exposed and they don't share any memory locations, so this is not necessary. Furthermore, it can cause problems, e.g., when two buffers are broadcast together in DataParallel and one of them is modified in-place during forward but the other is needed in backward, autograd engine will complain.\nFixing the bug discovered at #13350 (comment)\nedit: This is a very real problem. E.g., consider using Spectral Norm + Batch Norm together.", "body": "In `broadcast_coalesced`, since multiple variables can be \"views\" of a big flattened tensor, they can share the same version counter. However, this base flat tensor is not exposed and they don't share any memory locations, so this is not necessary. Furthermore, it can cause problems, e.g., when two buffers are broadcast together in `DataParallel` and one of them is modified in-place during `forward` but the other is needed in backward, autograd engine will complain.\r\n\r\nFixing the bug discovered at https://github.com/pytorch/pytorch/pull/13350#issuecomment-436011370\r\n\r\nedit: This is a very real problem. E.g., consider using Spectral Norm + Batch Norm together."}