{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/194592531", "pull_request_review_id": 127793595, "id": 194592531, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU5MjUzMQ==", "diff_hunk": "@@ -111,6 +111,8 @@ class AT_API Context {\n   void setBenchmarkCuDNN(bool);\n   bool deterministicCuDNN() const;\n   void setDeterministicCuDNN(bool);\n+  bool cachePlanCuFFT() const;\n+  void setCachePlanCuFFT(bool);", "path": "aten/src/ATen/Context.h", "position": null, "original_position": 5, "commit_id": "98ecb3852ba4576f2969f1994d1982b94ffb0af1", "original_commit_id": "fa76ac4b6792557bf3e49f494d9d0c63b3958066", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Same reason as why (I think) we did `cudnn.benchmark` this way. Always caching when running FFT on various distinct configs wastes memory and some compute. So it would depend on the actual use case. \r\n\r\nProviding a caching/non-caching function is also tricky as this is specific to CUDA version, and it is hard to make it a general argument.", "created_at": "2018-06-12T01:23:09Z", "updated_at": "2018-11-23T15:45:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/8344#discussion_r194592531", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8344", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/194592531"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8344#discussion_r194592531"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8344"}}, "body_html": "<p>Same reason as why (I think) we did <code>cudnn.benchmark</code> this way. Always caching when running FFT on various distinct configs wastes memory and some compute. So it would depend on the actual use case.</p>\n<p>Providing a caching/non-caching function is also tricky as this is specific to CUDA version, and it is hard to make it a general argument.</p>", "body_text": "Same reason as why (I think) we did cudnn.benchmark this way. Always caching when running FFT on various distinct configs wastes memory and some compute. So it would depend on the actual use case.\nProviding a caching/non-caching function is also tricky as this is specific to CUDA version, and it is hard to make it a general argument.", "in_reply_to_id": 194592067}