{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430849497", "html_url": "https://github.com/pytorch/pytorch/issues/12498#issuecomment-430849497", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12498", "id": 430849497, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDg0OTQ5Nw==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-18T02:02:47Z", "updated_at": "2018-10-18T02:02:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ah, I finally see your point! I thought we don't have backward for <code>mm(S, D)</code>, it <a href=\"https://github.com/pytorch/pytorch/blob/ed5eb7196b355b2541d62676f33239a607cdacfb/tools/autograd/derivatives.yaml#L111-L114\">turns out</a> I was wrong! Here it returns a dense gradient. So back to the question why autograd works for <code>mm(A, x)</code> but not <code>mm(A, x.t())</code>. After digging a bit, it seems like <code>x.t()</code> has <code>stride = (1, 3)</code> and <code>size = (3, 3)</code>, and gets into <a href=\"https://github.com/pytorch/pytorch/blob/ed5eb7196b355b2541d62676f33239a607cdacfb/tools/autograd/templates/Functions.cpp#L474-L475\">this path</a>. But since we don't support <code>mm(D, S)</code>, it errors out, the error msg should be the same as if we do:</p>\n<pre><code>import torch\nx = torch.ones(3, 3, requires_grad=True)\ni = torch.LongTensor([ [0, 1, 2], [0, 1, 2] ])\nv = torch.FloatTensor([1,1,1])\nA = torch.sparse.FloatTensor(i, v, (3,3))\ny = torch.matmul(x, A)\n</code></pre>\n<p>RuntimeError: Expected object of backend CPU but got backend SparseCPU for argument <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"171402941\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2\">#2</a> 'mat2'</p>\n<p>We probably need to fix this by disallowing backward for <code>mm(S, D)</code></p>", "body_text": "Ah, I finally see your point! I thought we don't have backward for mm(S, D), it turns out I was wrong! Here it returns a dense gradient. So back to the question why autograd works for mm(A, x) but not mm(A, x.t()). After digging a bit, it seems like x.t() has stride = (1, 3) and size = (3, 3), and gets into this path. But since we don't support mm(D, S), it errors out, the error msg should be the same as if we do:\nimport torch\nx = torch.ones(3, 3, requires_grad=True)\ni = torch.LongTensor([ [0, 1, 2], [0, 1, 2] ])\nv = torch.FloatTensor([1,1,1])\nA = torch.sparse.FloatTensor(i, v, (3,3))\ny = torch.matmul(x, A)\n\nRuntimeError: Expected object of backend CPU but got backend SparseCPU for argument #2 'mat2'\nWe probably need to fix this by disallowing backward for mm(S, D)", "body": "Ah, I finally see your point! I thought we don't have backward for `mm(S, D)`, it [turns out](https://github.com/pytorch/pytorch/blob/ed5eb7196b355b2541d62676f33239a607cdacfb/tools/autograd/derivatives.yaml#L111-L114) I was wrong! Here it returns a dense gradient. So back to the question why autograd works for `mm(A, x)` but not `mm(A, x.t())`. After digging a bit, it seems like `x.t()` has `stride = (1, 3)` and `size = (3, 3)`, and gets into [this path](https://github.com/pytorch/pytorch/blob/ed5eb7196b355b2541d62676f33239a607cdacfb/tools/autograd/templates/Functions.cpp#L474-L475). But since we don't support `mm(D, S)`, it errors out, the error msg should be the same as if we do:\r\n```\r\nimport torch\r\nx = torch.ones(3, 3, requires_grad=True)\r\ni = torch.LongTensor([ [0, 1, 2], [0, 1, 2] ])\r\nv = torch.FloatTensor([1,1,1])\r\nA = torch.sparse.FloatTensor(i, v, (3,3))\r\ny = torch.matmul(x, A)\r\n```\r\nRuntimeError: Expected object of backend CPU but got backend SparseCPU for argument #2 'mat2'\r\n\r\nWe probably need to fix this by disallowing backward for `mm(S, D)`"}