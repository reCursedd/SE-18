{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430444416", "html_url": "https://github.com/pytorch/pytorch/issues/12498#issuecomment-430444416", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12498", "id": 430444416, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDQ0NDQxNg==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T00:10:42Z", "updated_at": "2018-10-17T00:11:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=31786013\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nkolot\">@nkolot</a> currently we don't support <code>matmul</code> for sparse tensor. The reason your code works is because both of A and x are 2D tensors, and so <code>mm</code> gets called in <code>matmul</code>(we do have <code>mm(S, D)</code> for sparse tensor), a 3D case will break as expected:</p>\n<pre><code>import torch\nx = torch.ones(3, 3, 3).requires_grad_(True)\ni = torch.LongTensor([[0, 1, 2], \n                      [0, 1, 2],\n                      [0, 1, 2]])\nv = torch.FloatTensor([1,1,1])\nA = torch.sparse.FloatTensor(i, v, (3, 3, 3))\ny = torch.matmul(A, x)\nloss = y.mean()\nloss.backward()\n</code></pre>\n<p>We are planing (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"366573981\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12308\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12308/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12308\">#12308</a>) to support <code>matmul</code> on the way to make <code>nn.Linear</code> work for sparse. Can I ask what's your use cases? Please use <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345996320\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10043\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10043/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10043\">#10043</a> for request on sparse. Ops in progress: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335497470\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8853\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8853/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8853\">#8853</a>. Current state of sparse: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"343294880\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9674\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9674/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9674\">#9674</a></p>", "body_text": "@nkolot currently we don't support matmul for sparse tensor. The reason your code works is because both of A and x are 2D tensors, and so mm gets called in matmul(we do have mm(S, D) for sparse tensor), a 3D case will break as expected:\nimport torch\nx = torch.ones(3, 3, 3).requires_grad_(True)\ni = torch.LongTensor([[0, 1, 2], \n                      [0, 1, 2],\n                      [0, 1, 2]])\nv = torch.FloatTensor([1,1,1])\nA = torch.sparse.FloatTensor(i, v, (3, 3, 3))\ny = torch.matmul(A, x)\nloss = y.mean()\nloss.backward()\n\nWe are planing (#12308) to support matmul on the way to make nn.Linear work for sparse. Can I ask what's your use cases? Please use #10043 for request on sparse. Ops in progress: #8853. Current state of sparse: #9674", "body": "@nkolot currently we don't support `matmul` for sparse tensor. The reason your code works is because both of A and x are 2D tensors, and so `mm` gets called in `matmul`(we do have `mm(S, D)` for sparse tensor), a 3D case will break as expected:\r\n```\r\nimport torch\r\nx = torch.ones(3, 3, 3).requires_grad_(True)\r\ni = torch.LongTensor([[0, 1, 2], \r\n                      [0, 1, 2],\r\n                      [0, 1, 2]])\r\nv = torch.FloatTensor([1,1,1])\r\nA = torch.sparse.FloatTensor(i, v, (3, 3, 3))\r\ny = torch.matmul(A, x)\r\nloss = y.mean()\r\nloss.backward()\r\n```\r\n\r\nWe are planing (https://github.com/pytorch/pytorch/issues/12308) to support `matmul` on the way to make `nn.Linear` work for sparse. Can I ask what's your use cases? Please use https://github.com/pytorch/pytorch/issues/10043 for request on sparse. Ops in progress: https://github.com/pytorch/pytorch/issues/8853. Current state of sparse: https://github.com/pytorch/pytorch/issues/9674"}