{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10800", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10800/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10800/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10800/events", "html_url": "https://github.com/pytorch/pytorch/issues/10800", "id": 353152605, "node_id": "MDU6SXNzdWUzNTMxNTI2MDU=", "number": 10800, "title": "[distributed] Synchronization on CUDA side with MPI backend", "user": {"login": "kose-y", "id": 8198142, "node_id": "MDQ6VXNlcjgxOTgxNDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8198142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kose-y", "html_url": "https://github.com/kose-y", "followers_url": "https://api.github.com/users/kose-y/followers", "following_url": "https://api.github.com/users/kose-y/following{/other_user}", "gists_url": "https://api.github.com/users/kose-y/gists{/gist_id}", "starred_url": "https://api.github.com/users/kose-y/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kose-y/subscriptions", "organizations_url": "https://api.github.com/users/kose-y/orgs", "repos_url": "https://api.github.com/users/kose-y/repos", "events_url": "https://api.github.com/users/kose-y/events{/privacy}", "received_events_url": "https://api.github.com/users/kose-y/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-22T22:41:41Z", "updated_at": "2018-08-27T17:30:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p><code>torch.distributed</code> with OpenMPI 3.0.0 as backend: when running \"blocking\" communication commands (e.g. <code>send</code>, <code>recv</code>, etc.  are blocking in MPI), CUDA side is not synchronized. OpenMPI does not do that automatically, PyTorch does not do that automatically. This incurred several timing issues in some of my applications. I think this should be either</p>\n<ul>\n<li>documented: so that let users add things like <code>torch.cuda.synchronize</code> properly, or</li>\n<li>synchronized: synchronize the device (<code>cudaDeviceSynchronize</code>) before/after MPI blocking communication call.</li>\n</ul>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Build command you used (if compiling from source): <code>python setup.py install</code></li>\n<li>OS: ubuntu 16.04</li>\n<li>PyTorch version: 0.4.0</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: 8.0</li>\n<li>GPU models and configuration: GTX 1080</li>\n<li>GCC version (if compiling from source): 5.4.0</li>\n<li>CMake version: 3.11.1</li>\n<li>Versions of any other relevant libraries: OpenMPI 3.0.0</li>\n</ul>", "body_text": "Issue description\ntorch.distributed with OpenMPI 3.0.0 as backend: when running \"blocking\" communication commands (e.g. send, recv, etc.  are blocking in MPI), CUDA side is not synchronized. OpenMPI does not do that automatically, PyTorch does not do that automatically. This incurred several timing issues in some of my applications. I think this should be either\n\ndocumented: so that let users add things like torch.cuda.synchronize properly, or\nsynchronized: synchronize the device (cudaDeviceSynchronize) before/after MPI blocking communication call.\n\nSystem Info\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): source\nBuild command you used (if compiling from source): python setup.py install\nOS: ubuntu 16.04\nPyTorch version: 0.4.0\nPython version: 3.6.4\nCUDA/cuDNN version: 8.0\nGPU models and configuration: GTX 1080\nGCC version (if compiling from source): 5.4.0\nCMake version: 3.11.1\nVersions of any other relevant libraries: OpenMPI 3.0.0", "body": "## Issue description\r\n\r\n`torch.distributed` with OpenMPI 3.0.0 as backend: when running \"blocking\" communication commands (e.g. `send`, `recv`, etc.  are blocking in MPI), CUDA side is not synchronized. OpenMPI does not do that automatically, PyTorch does not do that automatically. This incurred several timing issues in some of my applications. I think this should be either \r\n- documented: so that let users add things like `torch.cuda.synchronize` properly, or\r\n- synchronized: synchronize the device (`cudaDeviceSynchronize`) before/after MPI blocking communication call. \r\n\r\n## System Info\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Build command you used (if compiling from source): `python setup.py install`\r\n- OS: ubuntu 16.04\r\n- PyTorch version: 0.4.0\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: 8.0\r\n- GPU models and configuration: GTX 1080\r\n- GCC version (if compiling from source): 5.4.0\r\n- CMake version: 3.11.1\r\n- Versions of any other relevant libraries: OpenMPI 3.0.0"}