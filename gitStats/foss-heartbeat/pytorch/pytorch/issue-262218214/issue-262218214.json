{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2935", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2935/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2935/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2935/events", "html_url": "https://github.com/pytorch/pytorch/issues/2935", "id": 262218214, "node_id": "MDU6SXNzdWUyNjIyMTgyMTQ=", "number": 2935, "title": "__array__ and __array_wrap__", "user": {"login": "kohr-h", "id": 5030250, "node_id": "MDQ6VXNlcjUwMzAyNTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/5030250?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kohr-h", "html_url": "https://github.com/kohr-h", "followers_url": "https://api.github.com/users/kohr-h/followers", "following_url": "https://api.github.com/users/kohr-h/following{/other_user}", "gists_url": "https://api.github.com/users/kohr-h/gists{/gist_id}", "starred_url": "https://api.github.com/users/kohr-h/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kohr-h/subscriptions", "organizations_url": "https://api.github.com/users/kohr-h/orgs", "repos_url": "https://api.github.com/users/kohr-h/repos", "events_url": "https://api.github.com/users/kohr-h/events{/privacy}", "received_events_url": "https://api.github.com/users/kohr-h/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-02T20:32:16Z", "updated_at": "2017-10-03T14:27:57Z", "closed_at": "2017-10-03T14:27:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To improve Numpy interoperability, torch tensors could implement the array interface in a quite simple manner:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">__array__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">if</span> dtype <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.cpu().numpy()\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.cpu().numpy().astype(dtype, <span class=\"pl-v\">copy</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">__array_wrap__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">array</span>):\n    <span class=\"pl-k\">if</span> array.ndim <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">if</span> array.dtype.kind <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">bool</span>(array)\n        <span class=\"pl-k\">elif</span> array.dtype.kind <span class=\"pl-k\">in</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>i<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>u<span class=\"pl-pds\">'</span></span>):\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">int</span>(array)\n        <span class=\"pl-k\">elif</span> array.dtype.kind <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>f<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">float</span>(array)\n        <span class=\"pl-k\">elif</span> array.dtype.kind <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>c<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">complex</span>(array)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">if</span> array.dtype <span class=\"pl-k\">==</span> <span class=\"pl-c1\">bool</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Workaround, torch has no built-in bool tensor</span>\n            <span class=\"pl-c1\">cls</span> <span class=\"pl-k\">=</span> torch.ByteTensor\n            array <span class=\"pl-k\">=</span> array.astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>uint8<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">cls</span> <span class=\"pl-k\">=</span> _tensor_cls(array.dtype, <span class=\"pl-v\">use_cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.is_cuda)\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">cls</span>(array)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_tensor_cls</span>(<span class=\"pl-smi\">dtype</span>, <span class=\"pl-smi\">use_cuda</span>):\n    mapping <span class=\"pl-k\">=</span> {\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float16<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Half<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Float<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float64<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Double<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>int8<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Char<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>int16<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Short<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>int32<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Int<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>int64<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Long<span class=\"pl-pds\">'</span></span>,\n        np.dtype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>uint8<span class=\"pl-pds\">'</span></span>): <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Byte<span class=\"pl-pds\">'</span></span>,\n    }\n    <span class=\"pl-k\">try</span>:\n        name <span class=\"pl-k\">=</span> mapping[np.dtype(dtype)]\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">KeyError</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dtype <span class=\"pl-c1\">{}</span> not supported<span class=\"pl-pds\">'</span></span>.format(dtype))\n    <span class=\"pl-k\">if</span> use_cuda:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">getattr</span>(torch.cuda, name)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">getattr</span>(torch, name)</pre></div>\n<p>It would enable simple things like <code>numpy.asarray</code> to turn tensors into Numpy arrays, and with that make basically all of Numpy applicable to torch tensors (with some restrictions). Is something like that of interest?</p>", "body_text": "To improve Numpy interoperability, torch tensors could implement the array interface in a quite simple manner:\ndef __array__(self, dtype=None):\n    if dtype is None:\n        return self.cpu().numpy()\n    else:\n        return self.cpu().numpy().astype(dtype, copy=False)\n\n\ndef __array_wrap__(self, array):\n    if array.ndim == 0:\n        if array.dtype.kind == 'b':\n            return bool(array)\n        elif array.dtype.kind in ('i', 'u'):\n            return int(array)\n        elif array.dtype.kind == 'f':\n            return float(array)\n        elif array.dtype.kind == 'c':\n            return complex(array)\n        else:\n            raise RuntimeError\n    else:\n        if array.dtype == bool:\n            # Workaround, torch has no built-in bool tensor\n            cls = torch.ByteTensor\n            array = array.astype('uint8')\n        else:\n            cls = _tensor_cls(array.dtype, use_cuda=self.is_cuda)\n        return cls(array)\n\n\ndef _tensor_cls(dtype, use_cuda):\n    mapping = {\n        np.dtype('float16'): 'Half',\n        np.dtype('float32'): 'Float',\n        np.dtype('float64'): 'Double',\n        np.dtype('int8'): 'Char',\n        np.dtype('int16'): 'Short',\n        np.dtype('int32'): 'Int',\n        np.dtype('int64'): 'Long',\n        np.dtype('uint8'): 'Byte',\n    }\n    try:\n        name = mapping[np.dtype(dtype)]\n    except KeyError:\n        raise ValueError('dtype {} not supported'.format(dtype))\n    if use_cuda:\n        return getattr(torch.cuda, name)\n    else:\n        return getattr(torch, name)\nIt would enable simple things like numpy.asarray to turn tensors into Numpy arrays, and with that make basically all of Numpy applicable to torch tensors (with some restrictions). Is something like that of interest?", "body": "To improve Numpy interoperability, torch tensors could implement the array interface in a quite simple manner:\r\n```python\r\ndef __array__(self, dtype=None):\r\n    if dtype is None:\r\n        return self.cpu().numpy()\r\n    else:\r\n        return self.cpu().numpy().astype(dtype, copy=False)\r\n\r\n\r\ndef __array_wrap__(self, array):\r\n    if array.ndim == 0:\r\n        if array.dtype.kind == 'b':\r\n            return bool(array)\r\n        elif array.dtype.kind in ('i', 'u'):\r\n            return int(array)\r\n        elif array.dtype.kind == 'f':\r\n            return float(array)\r\n        elif array.dtype.kind == 'c':\r\n            return complex(array)\r\n        else:\r\n            raise RuntimeError\r\n    else:\r\n        if array.dtype == bool:\r\n            # Workaround, torch has no built-in bool tensor\r\n            cls = torch.ByteTensor\r\n            array = array.astype('uint8')\r\n        else:\r\n            cls = _tensor_cls(array.dtype, use_cuda=self.is_cuda)\r\n        return cls(array)\r\n\r\n\r\ndef _tensor_cls(dtype, use_cuda):\r\n    mapping = {\r\n        np.dtype('float16'): 'Half',\r\n        np.dtype('float32'): 'Float',\r\n        np.dtype('float64'): 'Double',\r\n        np.dtype('int8'): 'Char',\r\n        np.dtype('int16'): 'Short',\r\n        np.dtype('int32'): 'Int',\r\n        np.dtype('int64'): 'Long',\r\n        np.dtype('uint8'): 'Byte',\r\n    }\r\n    try:\r\n        name = mapping[np.dtype(dtype)]\r\n    except KeyError:\r\n        raise ValueError('dtype {} not supported'.format(dtype))\r\n    if use_cuda:\r\n        return getattr(torch.cuda, name)\r\n    else:\r\n        return getattr(torch, name)\r\n```\r\nIt would enable simple things like `numpy.asarray` to turn tensors into Numpy arrays, and with that make basically all of Numpy applicable to torch tensors (with some restrictions). Is something like that of interest? "}