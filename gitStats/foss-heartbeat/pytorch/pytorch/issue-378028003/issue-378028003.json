{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13638", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13638/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13638/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13638/events", "html_url": "https://github.com/pytorch/pytorch/issues/13638", "id": 378028003, "node_id": "MDU6SXNzdWUzNzgwMjgwMDM=", "number": 13638, "title": "VariableImpl/TensorImpl Merge Proposal", "user": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-06T20:51:37Z", "updated_at": "2018-11-21T00:23:05Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> TL;DR: High-level changes:</h2>\n<ol>\n<li>Common Tensor functions (e.g. <code>numel()</code>\u00a0/\u00a0<code>sizes()</code>\u00a0/\u00a0<code>dim()</code>) will be de-virtualized in TensorImpl and have their runtime reduced by 43%-86%.</li>\n<li>Autograd metadata (such as <code>grad_</code>/<code>grad_fn_</code>/<code>grad_accumulator_</code>) will be moved from <code>Variable::Impl</code> to a struct that TensorImpl has a pointer to</li>\n<li>Python API behavior change: changing shape/storage on <code>tensor.data</code> will no longer update <code>tensor</code></li>\n</ol>\n<h2>Proposal</h2>\n<p>Currently, common Tensor functions such as <code>numel()</code> / <code>sizes()</code> / <code>dim()</code> work on <code>Variable</code>, because <code>Variable</code> has an underlying <code>at::Tensor</code> (stored in <code>Variable::Impl</code>) that it can dispatch those functions to. I propose that we remove this <code>at::Tensor</code> data member in <code>Variable::Impl</code>: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/348867c10b14509518291fc7a03d17a087f9c7bf/torch/csrc/autograd/variable.h#L347\">pytorch/torch/csrc/autograd/variable.h</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 347\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/348867c10b14509518291fc7a03d17a087f9c7bf\">348867c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L347\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"347\"></td>\n          <td id=\"LC347\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> at::Tensor data_; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n and move all autograd-specific metadata from <code>Variable::Impl</code> to <code>TensorImpl</code>, with the following behavioral changes:</p>\n<h3>Tensor \u2192 Variable:</h3>\n<p>When C++ user wants to create a history-recording <code>Variable</code> from an <code>at::Tensor</code>, we make a shallow copy of the <code>at::Tensor</code>'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. <code>size</code> / <code>stride</code>), and then we add autograd-specific metadata into this new TensorImpl so that it knows how to record history.</p>\n<h3>Recording Variable \u2192 Detached Variable:</h3>\n<p>When C++ user wants to create a non-history-recording <code>Variable</code> from another <code>Variable</code>, we do the shallow copy of the original Variable's TensorImpl in the same way, and set the autograd-specific metadata to NULL.</p>\n<h2>Motivation</h2>\n<p>As part of the effort to unify the Tensor between PyTorch and Caffe2, we care about how much time it takes to execute common Tensor functions such as <code>numel()</code> / <code>sizes()</code> / <code>dim()</code>. Currently, these functions are <code>virtual</code> in TensorImpl, so that <code>Variable::Impl</code> (a subclass of TensorImpl) can override them and dispatch those calls to the <code>Variable::Impl</code>'s underlying <code>at::Tensor</code>. The current design has the following problems:</p>\n<ul>\n<li><strong>Virtual functions are slow</strong>: Virtual function calls are slow because they involve an extra vtable lookup. Specifically, we did the following comparison on the most common Tensor functions (all timings are in ns):</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Benchmark</th>\n<th>Time (no flush)</th>\n<th>Time (flush L1)</th>\n<th>Time (flush L1+L2)</th>\n<th>Time (flush L1+L2+L3)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Tensor.dim() - non-virtual</td>\n<td>1.3</td>\n<td>3.33</td>\n<td>7.6</td>\n<td>58</td>\n</tr>\n<tr>\n<td>Variable.dim() - virtual</td>\n<td>4.5</td>\n<td>24.4</td>\n<td>52</td>\n<td>173.67</td>\n</tr>\n<tr>\n<td><strong>Runtime Savings</strong></td>\n<td><strong>-71.11111%</strong></td>\n<td><strong>-86.35246%</strong></td>\n<td><strong>-85.38462%</strong></td>\n<td><strong>-66.60333%</strong></td>\n</tr>\n<tr>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>Tensor.numel() - non-virtual</td>\n<td>22.6</td>\n<td>63.89</td>\n<td>109.22</td>\n<td>294.5</td>\n</tr>\n<tr>\n<td>Variable.numel() - virtual</td>\n<td>80.33</td>\n<td>133.1</td>\n<td>192</td>\n<td>810.9</td>\n</tr>\n<tr>\n<td><strong>Runtime Savings</strong></td>\n<td><strong>-71.86605%</strong></td>\n<td><strong>-51.9985%</strong></td>\n<td><strong>-43.11458%</strong></td>\n<td><strong>-63.68233%</strong></td>\n</tr>\n<tr>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>Tensor.size(0) - non-virtual</td>\n<td>30.4</td>\n<td>60.1</td>\n<td>100.44</td>\n<td>384.3</td>\n</tr>\n<tr>\n<td>Variable.size(0) - virtual</td>\n<td>75.4</td>\n<td>127.67</td>\n<td>203.8</td>\n<td>875.9</td>\n</tr>\n<tr>\n<td><strong>Runtime Savings</strong></td>\n<td><strong>-59.6817%</strong></td>\n<td><strong>-52.92551%</strong></td>\n<td><strong>-50.71639%</strong></td>\n<td><strong>-56.12513%</strong></td>\n</tr>\n<tr>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>Tensor.sizes() - non-virtual</td>\n<td>2</td>\n<td>4.25</td>\n<td>13.25</td>\n<td>67.6</td>\n</tr>\n<tr>\n<td>Variable.sizes() - virtual</td>\n<td>5.2</td>\n<td>28.44</td>\n<td>62.1</td>\n<td>254.78</td>\n</tr>\n<tr>\n<td><strong>Runtime Savings</strong></td>\n<td><strong>-61.53846%</strong></td>\n<td><strong>-85.05626%</strong></td>\n<td><strong>-78.66345%</strong></td>\n<td><strong>-73.46731%</strong></td>\n</tr>\n<tr>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>Tensor.resize_({0}) no-op - non-virtual</td>\n<td>23.11</td>\n<td>86.44</td>\n<td>105.44</td>\n<td>332.33</td>\n</tr>\n<tr>\n<td>Variable.resize_({0}) no-op - virtual</td>\n<td>168.4</td>\n<td>254.22</td>\n<td>348.56</td>\n<td>890.9</td>\n</tr>\n<tr>\n<td><strong>Runtime Savings</strong></td>\n<td><strong>-86.27672%</strong></td>\n<td><strong>-65.99795%</strong></td>\n<td><strong>-69.74983%</strong></td>\n<td><strong>-62.69727%</strong></td>\n</tr>\n<tr>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>Tensor.resize_({64, 2048}) no-op - non-virtual</td>\n<td>33.4</td>\n<td>102.56</td>\n<td>129.56</td>\n<td>407.22</td>\n</tr>\n<tr>\n<td>Variable.resize_({64, 2048}) no-op - virtual</td>\n<td>193</td>\n<td>278.1</td>\n<td>364.9</td>\n<td>936.6</td>\n</tr>\n<tr>\n<td><strong>Runtime Savings</strong></td>\n<td><strong>-82.6943%</strong></td>\n<td><strong>-63.12118%</strong></td>\n<td><strong>-64.49438%</strong></td>\n<td><strong>-56.52146%</strong></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>Benchmarked commit: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/f000101b8139378f342b175c11072a925c9d7c7a/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/f000101b8139378f342b175c11072a925c9d7c7a\"><tt>f000101</tt></a><br>\nBenchmark script: <a href=\"https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp\">https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp</a><br>\nNon-virtual code: <a class=\"commit-link\" href=\"https://github.com/pytorch/pytorch/compare/master...yf225:nonvirtual_tensorimpl\"><tt>master...yf225:nonvirtual_tensorimpl</tt></a><br>\nVirtual code: <a class=\"commit-link\" href=\"https://github.com/pytorch/pytorch/compare/master...yf225:virtual_tensorimpl\"><tt>master...yf225:virtual_tensorimpl</tt></a></p>\n</blockquote>\n<p>Based on our current implementation, the runtime difference for <code>dim()</code>, <code>numel()</code>, <code>size()</code>, <code>sizes()</code>, and no-op <code>resize()</code> comes from the virtual function call overhead and the <code>at::Tensor</code> data member indirection in <code>Variable::Impl</code>. If we de-virtualize those functions, we would be able to cut the runtime by <strong>43%-86%</strong> on the most common Tensor functions.</p>\n<ul>\n<li>\n<p><strong><code>at::</code> namespace adds cognitive overhead</strong>: Right now, C++ users have to be very careful about choosing between <code>at::</code> and <code>torch::</code> functions, based on whether they want autograd behavior. (For example, a Tensor created from <code>at::ones()</code> can't record history, but a Variable created from <code>torch::ones()</code> can.) Ideally, we want to discourage use of the <code>at::</code> namespace and only use the <code>torch::</code> namespace, to be consistent with the Python API (which uses the <code>torch</code> module for everything). And we let people decide whether to enable history recording for a tensor in the following ways:</p>\n<ul>\n<li>Case 1: At tensor creation time:\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">auto</span> a = torch::ones({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>});  <span class=\"pl-c\"><span class=\"pl-c\">//</span> a is not recording history  (this already works)</span>\n<span class=\"pl-k\">auto</span> a = torch::ones({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>}, torch::requires_grad());  <span class=\"pl-c\"><span class=\"pl-c\">//</span> a is recording history  (this already works)</span></pre></div>\n</li>\n<li>Case 2: After tensor creation:\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">auto</span> a = torch::ones({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>});  <span class=\"pl-c\"><span class=\"pl-c\">//</span> a is not recording history (this already works)</span>\na.requires_grad_(<span class=\"pl-c1\">true</span>);  <span class=\"pl-c\"><span class=\"pl-c\">//</span> a is recording history now (new API)</span></pre></div>\n</li>\n</ul>\n<p>Note that the deprecation of <code>at::</code> namespace in our public API is easier to achieve if we successfully de-virtualize and speed up the common Tensor functions on Variables, so that users have one less reason to stick with <code>at::</code> APIs. (In an ideal world we don't need to expose the <code>at::</code> namespace at all, but for backward compatibility reasons we should still keep it around.)</p>\n</li>\n<li>\n<p><strong>Overly Complex OOP design</strong>: <code>Variable::Impl</code> is a subclass of TensorImpl, but it also has an <code>at::Tensor</code> data member which internally wraps another TensorImpl. This co-existence of \"is-a\" and \"has-a\" relationship makes the code complicated and adds cognitive overhead.  In particular, it's difficult to track which functions we have overridden in <code>Variable::Impl</code>, and which functions are applicable to Tensor vs. Variable (e.g. <code>is_wrapped_number()</code> is only valid on Tensor, not Variable) (for more context, also see note: <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/TensorImpl.h#L495-L514\">We regret making Variable hold a Tensor</a>).</p>\n</li>\n<li>\n<p><strong>Unused data members in <code>Variable::Impl</code> take up cache/memory space</strong>: Since <code>Variable::Impl</code> is a subclass of TensorImpl, it contains all of the data members that a normal TensorImpl would have (such as <code>sizes_</code> / <code>strides_</code> / etc.). However, the <code>Variable::Impl</code> functions always call into the underlying <code>at::Tensor</code> and ignores the rest of the fields, which causes a lot of wasted cache/memory space.</p>\n</li>\n</ul>\n<h2>Pitch</h2>\n<p>I propose that we remove the <code>at::Tensor</code> data member in <code>Variable::Impl</code> <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/348867c10b14509518291fc7a03d17a087f9c7bf/torch/csrc/autograd/variable.h#L347\">pytorch/torch/csrc/autograd/variable.h</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 347\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/348867c10b14509518291fc7a03d17a087f9c7bf\">348867c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L347\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"347\"></td>\n          <td id=\"LC347\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> at::Tensor data_; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n and move all autograd-specific metadata from <code>Variable::Impl</code> to TensorImpl. As a result, here are the new behaviors:</p>\n<ul>\n<li><strong>When C++ user wants to create a history-recording <code>Variable</code> from an <code>at::Tensor</code></strong>:<br>\nCurrent API (unchanged):</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">auto</span> var = make_variable(tensor, <span class=\"pl-c\"><span class=\"pl-c\">/*</span>requires_grad<span class=\"pl-c\">*/</span></span><span class=\"pl-c1\">true</span>)</pre></div>\n<p>Under the hood, we do the following steps:</p>\n<ol>\n<li>We make a shallow copy of <code>tensor</code>'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. <code>size</code> / <code>stride</code>).\n<ul>\n<li>Note that subclasses of TensorImpl (e.g. <code>SparseTensorImpl</code>) need to know how to make a shallow copy of themselves, and we dispatch this operation to each TensorImpl subclass' own <code>shallow_copy()</code> function (by making the <code>shallow_copy()</code> function virtual in TensorImpl and overriding it in TensorImpl subclasses).</li>\n</ul>\n</li>\n<li>We initialize a struct called <code>AutogradMeta</code>, which stores autograd-specific fields (such as <code>grad_</code>/<code>grad_fn_</code>/<code>grad_accumulator_</code>).</li>\n<li>We assign the struct to the <code>AutogradMeta</code> pointer in the new TensorImpl.</li>\n<li>We return a Variable that wraps the new TensorImpl.</li>\n</ol>\n<br>\n<ul>\n<li><strong>When C++ user wants to create a non-history-recording <code>Variable</code> from another <code>Variable</code>:</strong><br>\nCurrent API (unchanged):</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">auto</span> var = torch::ones({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>}, torch::requires_grad()); <span class=\"pl-c\"><span class=\"pl-c\">//</span> var is recording history</span>\n<span class=\"pl-k\">auto</span> var_detached = var.detach() <span class=\"pl-c\"><span class=\"pl-c\">//</span> var_detached is the non-history-recording version of var</span></pre></div>\n<p>When the user calls <code>var.detach()</code>, we do the following under the hood:</p>\n<ol>\n<li>We do the shallow copy of <code>var</code>'s TensorImpl in the same way as above.</li>\n<li>We set the <code>AutogradMeta</code> pointer to NULL, to indicate that it doesn't need to record history.</li>\n<li>We return a Variable that wraps the new TensorImpl.</li>\n</ol>\n<br>\n<ul>\n<li><strong>When C++ user wants to enable/disable history-recording for a <code>Variable</code>:</strong><br>\nProposed API:</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">auto</span> var = torch::ones({<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>});  <span class=\"pl-c\"><span class=\"pl-c\">//</span> var is not recording history (this already works)</span>\nvar.requires_grad_(<span class=\"pl-c1\">true</span>);  <span class=\"pl-c\"><span class=\"pl-c\">//</span> var is recording history now (new API)</span>\nvar.requires_grad_(<span class=\"pl-c1\">false</span>); <span class=\"pl-c\"><span class=\"pl-c\">//</span> var is not recording history anymore (new API)</span></pre></div>\n<p>When the user calls <code>var.requires_grad_(true)</code>, we do the following under the hood:</p>\n<ol>\n<li>We initialize a struct called <code>AutogradMeta</code>, which stores autograd-specific fields (such as <code>grad_</code>/<code>grad_fn_</code>/<code>grad_accumulator_</code>).</li>\n<li>We assign the struct to the <code>AutogradMeta</code> pointer in <code>var</code>'s TensorImpl.</li>\n</ol>\n<p>When the user calls <code>var.requires_grad_(false)</code>, we do the following under the hood:</p>\n<ol>\n<li>We set the <code>AutogradMeta</code> pointer in <code>var</code>'s TensorImpl to NULL.</li>\n</ol>\n<br>\n<ul>\n<li><strong>When C++ user wants to treat a Variable as a non-Variable <code>at::Tensor</code> when dispatching through <code>type()</code>:</strong><br>\nProposed API:</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre>{\n  <span class=\"pl-k\">auto</span> var_type = variable.<span class=\"pl-c1\">type</span>();  <span class=\"pl-c\"><span class=\"pl-c\">//</span> \"var_type\" is a Variable type</span>\n}\n{\n  at::AutoGradMode <span class=\"pl-smi\">grad_mode</span>(<span class=\"pl-c1\">false</span>);  <span class=\"pl-c\"><span class=\"pl-c\">//</span> thread-local guard (new API)</span>\n  <span class=\"pl-k\">auto</span> non_var_type = variable.<span class=\"pl-c1\">type</span>();  <span class=\"pl-c\"><span class=\"pl-c\">//</span> \"non_var_type\" is a non-Variable type</span>\n}\n{\n  at::AutoGradMode <span class=\"pl-smi\">grad_mode</span>(<span class=\"pl-c1\">true</span>);  <span class=\"pl-c\"><span class=\"pl-c\">//</span> thread-local guard (new API)</span>\n  <span class=\"pl-k\">auto</span> var_type = variable.<span class=\"pl-c1\">type</span>();  <span class=\"pl-c\"><span class=\"pl-c\">//</span> \"var_type\" is a Variable type</span>\n}</pre></div>\n<p>Under the hood, <code>type()</code> checks whether the <code>at::AutoGradMode</code> thread-local guard is enabled and takes its value into consideration when computing the type of the variable.</p>\n<p><strong>NOTE</strong>:  we will also remove the <code>Variable.data()</code> API (which returns a non-history-recording <code>at::Tensor</code> from the Variable), since now we can use the thread-local guard to decide whether we want to dispatch to the non-Variable code paths.</p>\n<h2>Breaking changes</h2>\n<p>Note that this change will break the current API in the following way:</p>\n<p>In the old world, whenever we want to create a <code>Variable</code> that shares the same data with another <code>Variable</code>, we simply do <code>auto var_new = make_variable(var.data())</code> or <code>auto var_new = var.detach()</code>, and any shape / data / storage pointer changes to <code>var_new</code> will be reflected in <code>var</code> automatically, because internally they share the same underlying <code>at::Tensor</code>.</p>\n<p>However, in the new world, there is no concept of the \"underlying <code>at::Tensor</code>\" of a Variable, since the Variable itself is the Tensor. When we want to create a <code>Variable</code> that shares the same data with another <code>Variable</code>, we can still call <code>auto var_new = var.detach()</code>, but in this case, only the tensor storage data is shared (via ref-counted pointer) between <code>var_new</code> and <code>var</code>, but not the tensor size/stride information (they are copied by value). In other words, changing anything (e.g. size / stride / storage_ptr ) in the detached Variable (<code>var_new</code>) that are not bits inside tensor storage won't update the original Variable (<code>var</code>), and we should no longer expect those data to be shared.</p>\n<p>This has implications for Python call sites that do</p>\n<div class=\"highlight highlight-source-python\"><pre>tensor.data.in_place_operation_()</pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-python\"><pre>tensor_detached <span class=\"pl-k\">=</span> tensor.detach()\ntensor_detached.in_place_operation_()</pre></div>\n<p>If <code>in_place_operation_()</code> only updates the data inside the tensor (such as <code>zeros_()</code>), such operation will still work properly; if the in-place operation changes the size, stride or the storage pointer inside the TensorImpl (e.g. <code>resize_</code> / <code>resize_as_</code> / <code>set_</code> / <code>transpose_</code>), such operation on <code>tensor.data</code> or <code>tensor_detached</code> will no longer update the <code>tensor</code>. We will address this inconsistency in the following ways:</p>\n<ol>\n<li>Add a flag to <code>TensorImpl</code> to disallow size/stride/storage_ptr changes from in-place operations such as <code>resize_</code> / <code>resize_as_</code> / <code>set_</code> / <code>transpose_</code>, and set this flag to true when people call <code>tensor.data</code> in Python.</li>\n<li>Write text in the docs to actively discourage changing the shape or storage of <code>tensor_detached</code> and expecting <code>tensor</code> to also be updated.</li>\n</ol>\n<h2>Upcoming PRs</h2>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 1st PR: (folded into 2nd PR: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379540592\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13827\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13827/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13827\">#13827</a>)</li>\n</ul>\n<ol>\n<li>Add a flag to <code>TensorImpl</code> to disallow size/stride/storage_ptr changes from in-place operations such as <code>resize_</code> / <code>resize_as_</code> / <code>set_</code> / <code>transpose_</code>, and set this flag to true when people call <code>tensor.data</code> in Python.</li>\n<li>Write text in the docs to actively discourage changing the shape or storage of <code>tensor_detached</code> and expecting <code>tensor</code> to also be updated.</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 2nd PR: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379540592\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13827\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13827/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13827\">#13827</a></li>\n</ul>\n<ol>\n<li>Move <code>Variable::Impl</code> data members into TensorImpl as <code>AutogradMeta</code> struct</li>\n<li>Change <code>Variable::Impl</code> functions to use data members in <code>AutogradMeta</code> struct</li>\n<li>Add <code>shallow_copy()</code> function to each subclass of TensorImpl</li>\n<li>Do shallow copy when the user calls <code>make_variable(tensor)</code> / <code>variable.detach()</code> (Reason: now that autograd metadata lives in TensorImpl, in order to create a new history for for the Variable returned from <code>variable.detach()</code> we not only need to create a new AutogradMeta struct, but we also need to create a new TensorImpl object that stores pointer to the new AutogradMeta struct (which we obtain by shallow-copying the original TensorImpl). Otherwise, changing history of the detached Variable will also change the history of the original Variable, which is not the correct behavior.)</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 3rd PR: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"380455531\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13932\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13932/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13932\">#13932</a></li>\n</ul>\n<ol>\n<li>Move <code>Variable::Impl</code> functions such as <code>backward()</code> / <code>rebase_history()</code> / <code>grad_accumulator()</code> / <code>grad_fn()</code> / <code>set_data()</code> out of <code>Variable::Impl</code> and into <code>Variable</code>. In those functions, we will use the getter to obtain the Variable's underlying <code>Variable::Impl</code> and perform the operations on it. (We need to make this change so that we can remove <code>Variable::Impl</code> class in the next PR.)</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 4th PR:</li>\n</ul>\n<ol>\n<li>Remove the <code>at::Tensor</code> data member from <code>Variable::Impl</code></li>\n<li>Remove the <code>Variable::Impl</code> class</li>\n<li>Remove all <code>Variable.data()</code> call sites, and use <code>Variable</code> directly</li>\n<li>Remove <code>Variable.data()</code> function</li>\n<li>In VariableType.cpp, use thread-local guard to make sure that operations on <code>baseType</code> still dispatch to non-Variable type, even if the parameters are now Variables</li>\n<li>Remove mentions of <code>Variable::Impl</code> and <code>DifferentiableViewImpl</code></li>\n<li>Fix comments in <code>[Tensor versus Variable in C++]</code>, <code>[We regret making Variable hold a Tensor]</code> and <code>[ Autograd View Variables ]</code></li>\n<li><strong>NOTE</strong>: we don't need to add <code>SparseVariableImpl</code> that handles how to copy <code>SparseTensorImpl</code>, because <code>SparseTensorImpl</code> already implements the <code>shallow_copy()</code> function that Variable factory functions can call.</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 5th PR</li>\n</ul>\n<ol>\n<li>Remove <code>is_variable()</code> and <code>is_variable_</code> from Tensor, use the AutogradMeta pointer instead</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 6th PR (ongoing: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357041581\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11259\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11259/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11259\">#11259</a>, will be rebased once we are at this step):</li>\n</ul>\n<ol>\n<li>Move the sparse-specific implementation of common Tensor functions such as <code>dim()</code> / <code>sizes()</code> from <code>SparseTensorImpl</code> to TensorImpl, by branching based on <code>is_sparse()</code> in TensorImpl</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 7th PR:</li>\n</ul>\n<ol>\n<li>Remove the <code>virtual</code> attribute from common Tensor functions in TensorImpl</li>\n<li>Verify the performance gain from de-virtualization.</li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 8th PR:</li>\n</ul>\n<ol>\n<li>Merge <code>at::AutoGradMode</code> and <code>autograd::AutoGradMode</code></li>\n</ol>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> 9th PR (not strictly related to the merge, but for improving consistency with Python API):</li>\n</ul>\n<ol>\n<li>Add <code>Tensor.requires_grad_(bool)</code> C++ API (which internally calls <code>Tensor.set_requires_grad(bool)</code>), to be consistent with Python API</li>\n</ol>", "body_text": "\ud83d\ude80 TL;DR: High-level changes:\n\nCommon Tensor functions (e.g. numel()\u00a0/\u00a0sizes()\u00a0/\u00a0dim()) will be de-virtualized in TensorImpl and have their runtime reduced by 43%-86%.\nAutograd metadata (such as grad_/grad_fn_/grad_accumulator_) will be moved from Variable::Impl to a struct that TensorImpl has a pointer to\nPython API behavior change: changing shape/storage on tensor.data will no longer update tensor\n\nProposal\nCurrently, common Tensor functions such as numel() / sizes() / dim() work on Variable, because Variable has an underlying at::Tensor (stored in Variable::Impl) that it can dispatch those functions to. I propose that we remove this at::Tensor data member in Variable::Impl: \n  \n    \n      pytorch/torch/csrc/autograd/variable.h\n    \n    \n         Line 347\n      in\n      348867c\n    \n    \n    \n    \n\n        \n          \n           at::Tensor data_; \n        \n    \n  \n\n and move all autograd-specific metadata from Variable::Impl to TensorImpl, with the following behavioral changes:\nTensor \u2192 Variable:\nWhen C++ user wants to create a history-recording Variable from an at::Tensor, we make a shallow copy of the at::Tensor's TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. size / stride), and then we add autograd-specific metadata into this new TensorImpl so that it knows how to record history.\nRecording Variable \u2192 Detached Variable:\nWhen C++ user wants to create a non-history-recording Variable from another Variable, we do the shallow copy of the original Variable's TensorImpl in the same way, and set the autograd-specific metadata to NULL.\nMotivation\nAs part of the effort to unify the Tensor between PyTorch and Caffe2, we care about how much time it takes to execute common Tensor functions such as numel() / sizes() / dim(). Currently, these functions are virtual in TensorImpl, so that Variable::Impl (a subclass of TensorImpl) can override them and dispatch those calls to the Variable::Impl's underlying at::Tensor. The current design has the following problems:\n\nVirtual functions are slow: Virtual function calls are slow because they involve an extra vtable lookup. Specifically, we did the following comparison on the most common Tensor functions (all timings are in ns):\n\n\n\n\nBenchmark\nTime (no flush)\nTime (flush L1)\nTime (flush L1+L2)\nTime (flush L1+L2+L3)\n\n\n\n\nTensor.dim() - non-virtual\n1.3\n3.33\n7.6\n58\n\n\nVariable.dim() - virtual\n4.5\n24.4\n52\n173.67\n\n\nRuntime Savings\n-71.11111%\n-86.35246%\n-85.38462%\n-66.60333%\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nTensor.numel() - non-virtual\n22.6\n63.89\n109.22\n294.5\n\n\nVariable.numel() - virtual\n80.33\n133.1\n192\n810.9\n\n\nRuntime Savings\n-71.86605%\n-51.9985%\n-43.11458%\n-63.68233%\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nTensor.size(0) - non-virtual\n30.4\n60.1\n100.44\n384.3\n\n\nVariable.size(0) - virtual\n75.4\n127.67\n203.8\n875.9\n\n\nRuntime Savings\n-59.6817%\n-52.92551%\n-50.71639%\n-56.12513%\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nTensor.sizes() - non-virtual\n2\n4.25\n13.25\n67.6\n\n\nVariable.sizes() - virtual\n5.2\n28.44\n62.1\n254.78\n\n\nRuntime Savings\n-61.53846%\n-85.05626%\n-78.66345%\n-73.46731%\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nTensor.resize_({0}) no-op - non-virtual\n23.11\n86.44\n105.44\n332.33\n\n\nVariable.resize_({0}) no-op - virtual\n168.4\n254.22\n348.56\n890.9\n\n\nRuntime Savings\n-86.27672%\n-65.99795%\n-69.74983%\n-62.69727%\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\nTensor.resize_({64, 2048}) no-op - non-virtual\n33.4\n102.56\n129.56\n407.22\n\n\nVariable.resize_({64, 2048}) no-op - virtual\n193\n278.1\n364.9\n936.6\n\n\nRuntime Savings\n-82.6943%\n-63.12118%\n-64.49438%\n-56.52146%\n\n\n\n\nBenchmarked commit: f000101\nBenchmark script: https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp\nNon-virtual code: master...yf225:nonvirtual_tensorimpl\nVirtual code: master...yf225:virtual_tensorimpl\n\nBased on our current implementation, the runtime difference for dim(), numel(), size(), sizes(), and no-op resize() comes from the virtual function call overhead and the at::Tensor data member indirection in Variable::Impl. If we de-virtualize those functions, we would be able to cut the runtime by 43%-86% on the most common Tensor functions.\n\n\nat:: namespace adds cognitive overhead: Right now, C++ users have to be very careful about choosing between at:: and torch:: functions, based on whether they want autograd behavior. (For example, a Tensor created from at::ones() can't record history, but a Variable created from torch::ones() can.) Ideally, we want to discourage use of the at:: namespace and only use the torch:: namespace, to be consistent with the Python API (which uses the torch module for everything). And we let people decide whether to enable history recording for a tensor in the following ways:\n\nCase 1: At tensor creation time:\nauto a = torch::ones({2, 2});  // a is not recording history  (this already works)\nauto a = torch::ones({2, 2}, torch::requires_grad());  // a is recording history  (this already works)\n\nCase 2: After tensor creation:\nauto a = torch::ones({2, 2});  // a is not recording history (this already works)\na.requires_grad_(true);  // a is recording history now (new API)\n\n\nNote that the deprecation of at:: namespace in our public API is easier to achieve if we successfully de-virtualize and speed up the common Tensor functions on Variables, so that users have one less reason to stick with at:: APIs. (In an ideal world we don't need to expose the at:: namespace at all, but for backward compatibility reasons we should still keep it around.)\n\n\nOverly Complex OOP design: Variable::Impl is a subclass of TensorImpl, but it also has an at::Tensor data member which internally wraps another TensorImpl. This co-existence of \"is-a\" and \"has-a\" relationship makes the code complicated and adds cognitive overhead.  In particular, it's difficult to track which functions we have overridden in Variable::Impl, and which functions are applicable to Tensor vs. Variable (e.g. is_wrapped_number() is only valid on Tensor, not Variable) (for more context, also see note: We regret making Variable hold a Tensor).\n\n\nUnused data members in Variable::Impl take up cache/memory space: Since Variable::Impl is a subclass of TensorImpl, it contains all of the data members that a normal TensorImpl would have (such as sizes_ / strides_ / etc.). However, the Variable::Impl functions always call into the underlying at::Tensor and ignores the rest of the fields, which causes a lot of wasted cache/memory space.\n\n\nPitch\nI propose that we remove the at::Tensor data member in Variable::Impl \n  \n    \n      pytorch/torch/csrc/autograd/variable.h\n    \n    \n         Line 347\n      in\n      348867c\n    \n    \n    \n    \n\n        \n          \n           at::Tensor data_; \n        \n    \n  \n\n and move all autograd-specific metadata from Variable::Impl to TensorImpl. As a result, here are the new behaviors:\n\nWhen C++ user wants to create a history-recording Variable from an at::Tensor:\nCurrent API (unchanged):\n\nauto var = make_variable(tensor, /*requires_grad*/true)\nUnder the hood, we do the following steps:\n\nWe make a shallow copy of tensor's TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. size / stride).\n\nNote that subclasses of TensorImpl (e.g. SparseTensorImpl) need to know how to make a shallow copy of themselves, and we dispatch this operation to each TensorImpl subclass' own shallow_copy() function (by making the shallow_copy() function virtual in TensorImpl and overriding it in TensorImpl subclasses).\n\n\nWe initialize a struct called AutogradMeta, which stores autograd-specific fields (such as grad_/grad_fn_/grad_accumulator_).\nWe assign the struct to the AutogradMeta pointer in the new TensorImpl.\nWe return a Variable that wraps the new TensorImpl.\n\n\n\nWhen C++ user wants to create a non-history-recording Variable from another Variable:\nCurrent API (unchanged):\n\nauto var = torch::ones({2, 2}, torch::requires_grad()); // var is recording history\nauto var_detached = var.detach() // var_detached is the non-history-recording version of var\nWhen the user calls var.detach(), we do the following under the hood:\n\nWe do the shallow copy of var's TensorImpl in the same way as above.\nWe set the AutogradMeta pointer to NULL, to indicate that it doesn't need to record history.\nWe return a Variable that wraps the new TensorImpl.\n\n\n\nWhen C++ user wants to enable/disable history-recording for a Variable:\nProposed API:\n\nauto var = torch::ones({2, 2});  // var is not recording history (this already works)\nvar.requires_grad_(true);  // var is recording history now (new API)\nvar.requires_grad_(false); // var is not recording history anymore (new API)\nWhen the user calls var.requires_grad_(true), we do the following under the hood:\n\nWe initialize a struct called AutogradMeta, which stores autograd-specific fields (such as grad_/grad_fn_/grad_accumulator_).\nWe assign the struct to the AutogradMeta pointer in var's TensorImpl.\n\nWhen the user calls var.requires_grad_(false), we do the following under the hood:\n\nWe set the AutogradMeta pointer in var's TensorImpl to NULL.\n\n\n\nWhen C++ user wants to treat a Variable as a non-Variable at::Tensor when dispatching through type():\nProposed API:\n\n{\n  auto var_type = variable.type();  // \"var_type\" is a Variable type\n}\n{\n  at::AutoGradMode grad_mode(false);  // thread-local guard (new API)\n  auto non_var_type = variable.type();  // \"non_var_type\" is a non-Variable type\n}\n{\n  at::AutoGradMode grad_mode(true);  // thread-local guard (new API)\n  auto var_type = variable.type();  // \"var_type\" is a Variable type\n}\nUnder the hood, type() checks whether the at::AutoGradMode thread-local guard is enabled and takes its value into consideration when computing the type of the variable.\nNOTE:  we will also remove the Variable.data() API (which returns a non-history-recording at::Tensor from the Variable), since now we can use the thread-local guard to decide whether we want to dispatch to the non-Variable code paths.\nBreaking changes\nNote that this change will break the current API in the following way:\nIn the old world, whenever we want to create a Variable that shares the same data with another Variable, we simply do auto var_new = make_variable(var.data()) or auto var_new = var.detach(), and any shape / data / storage pointer changes to var_new will be reflected in var automatically, because internally they share the same underlying at::Tensor.\nHowever, in the new world, there is no concept of the \"underlying at::Tensor\" of a Variable, since the Variable itself is the Tensor. When we want to create a Variable that shares the same data with another Variable, we can still call auto var_new = var.detach(), but in this case, only the tensor storage data is shared (via ref-counted pointer) between var_new and var, but not the tensor size/stride information (they are copied by value). In other words, changing anything (e.g. size / stride / storage_ptr ) in the detached Variable (var_new) that are not bits inside tensor storage won't update the original Variable (var), and we should no longer expect those data to be shared.\nThis has implications for Python call sites that do\ntensor.data.in_place_operation_()\nor\ntensor_detached = tensor.detach()\ntensor_detached.in_place_operation_()\nIf in_place_operation_() only updates the data inside the tensor (such as zeros_()), such operation will still work properly; if the in-place operation changes the size, stride or the storage pointer inside the TensorImpl (e.g. resize_ / resize_as_ / set_ / transpose_), such operation on tensor.data or tensor_detached will no longer update the tensor. We will address this inconsistency in the following ways:\n\nAdd a flag to TensorImpl to disallow size/stride/storage_ptr changes from in-place operations such as resize_ / resize_as_ / set_ / transpose_, and set this flag to true when people call tensor.data in Python.\nWrite text in the docs to actively discourage changing the shape or storage of tensor_detached and expecting tensor to also be updated.\n\nUpcoming PRs\n\n 1st PR: (folded into 2nd PR: #13827)\n\n\nAdd a flag to TensorImpl to disallow size/stride/storage_ptr changes from in-place operations such as resize_ / resize_as_ / set_ / transpose_, and set this flag to true when people call tensor.data in Python.\nWrite text in the docs to actively discourage changing the shape or storage of tensor_detached and expecting tensor to also be updated.\n\n\n 2nd PR: #13827\n\n\nMove Variable::Impl data members into TensorImpl as AutogradMeta struct\nChange Variable::Impl functions to use data members in AutogradMeta struct\nAdd shallow_copy() function to each subclass of TensorImpl\nDo shallow copy when the user calls make_variable(tensor) / variable.detach() (Reason: now that autograd metadata lives in TensorImpl, in order to create a new history for for the Variable returned from variable.detach() we not only need to create a new AutogradMeta struct, but we also need to create a new TensorImpl object that stores pointer to the new AutogradMeta struct (which we obtain by shallow-copying the original TensorImpl). Otherwise, changing history of the detached Variable will also change the history of the original Variable, which is not the correct behavior.)\n\n\n 3rd PR: #13932\n\n\nMove Variable::Impl functions such as backward() / rebase_history() / grad_accumulator() / grad_fn() / set_data() out of Variable::Impl and into Variable. In those functions, we will use the getter to obtain the Variable's underlying Variable::Impl and perform the operations on it. (We need to make this change so that we can remove Variable::Impl class in the next PR.)\n\n\n 4th PR:\n\n\nRemove the at::Tensor data member from Variable::Impl\nRemove the Variable::Impl class\nRemove all Variable.data() call sites, and use Variable directly\nRemove Variable.data() function\nIn VariableType.cpp, use thread-local guard to make sure that operations on baseType still dispatch to non-Variable type, even if the parameters are now Variables\nRemove mentions of Variable::Impl and DifferentiableViewImpl\nFix comments in [Tensor versus Variable in C++], [We regret making Variable hold a Tensor] and [ Autograd View Variables ]\nNOTE: we don't need to add SparseVariableImpl that handles how to copy SparseTensorImpl, because SparseTensorImpl already implements the shallow_copy() function that Variable factory functions can call.\n\n\n 5th PR\n\n\nRemove is_variable() and is_variable_ from Tensor, use the AutogradMeta pointer instead\n\n\n 6th PR (ongoing: #11259, will be rebased once we are at this step):\n\n\nMove the sparse-specific implementation of common Tensor functions such as dim() / sizes() from SparseTensorImpl to TensorImpl, by branching based on is_sparse() in TensorImpl\n\n\n 7th PR:\n\n\nRemove the virtual attribute from common Tensor functions in TensorImpl\nVerify the performance gain from de-virtualization.\n\n\n 8th PR:\n\n\nMerge at::AutoGradMode and autograd::AutoGradMode\n\n\n 9th PR (not strictly related to the merge, but for improving consistency with Python API):\n\n\nAdd Tensor.requires_grad_(bool) C++ API (which internally calls Tensor.set_requires_grad(bool)), to be consistent with Python API", "body": "## \ud83d\ude80 TL;DR: High-level changes:\r\n1. Common Tensor functions (e.g. `numel()`\u00a0/\u00a0`sizes()`\u00a0/\u00a0`dim()`) will be de-virtualized in TensorImpl and have their runtime reduced by 43%-86%.\r\n2. Autograd metadata (such as `grad_`/`grad_fn_`/`grad_accumulator_`) will be moved from `Variable::Impl` to a struct that TensorImpl has a pointer to\r\n3. Python API behavior change: changing shape/storage on `tensor.data` will no longer update `tensor`\r\n\r\n## Proposal\r\nCurrently, common Tensor functions such as `numel()` / `sizes()` / `dim()` work on `Variable`, because `Variable` has an underlying `at::Tensor` (stored in `Variable::Impl`) that it can dispatch those functions to. I propose that we remove this `at::Tensor` data member in `Variable::Impl`: https://github.com/pytorch/pytorch/blob/348867c10b14509518291fc7a03d17a087f9c7bf/torch/csrc/autograd/variable.h#L347 and move all autograd-specific metadata from `Variable::Impl` to `TensorImpl`, with the following behavioral changes: \r\n\r\n### Tensor \u2192 Variable:\r\n\r\nWhen C++ user wants to create a history-recording `Variable` from an `at::Tensor`, we make a shallow copy of the `at::Tensor`'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. `size` / `stride`), and then we add autograd-specific metadata into this new TensorImpl so that it knows how to record history.\r\n\r\n### Recording Variable \u2192 Detached Variable:\r\n\r\nWhen C++ user wants to create a non-history-recording `Variable` from another `Variable`, we do the shallow copy of the original Variable's TensorImpl in the same way, and set the autograd-specific metadata to NULL. \r\n\r\n\r\n## Motivation\r\n\r\nAs part of the effort to unify the Tensor between PyTorch and Caffe2, we care about how much time it takes to execute common Tensor functions such as `numel()` / `sizes()` / `dim()`. Currently, these functions are `virtual` in TensorImpl, so that `Variable::Impl` (a subclass of TensorImpl) can override them and dispatch those calls to the `Variable::Impl`'s underlying `at::Tensor`. The current design has the following problems:\r\n\r\n- **Virtual functions are slow**: Virtual function calls are slow because they involve an extra vtable lookup. Specifically, we did the following comparison on the most common Tensor functions (all timings are in ns):\r\n\r\nBenchmark | Time (no flush) | Time (flush L1) | Time (flush L1+L2) | Time (flush L1+L2+L3)\r\n-- | -- | -- | -- | --\r\nTensor.dim() - non-virtual | 1.3 | 3.33 | 7.6 | 58\r\nVariable.dim() - virtual | 4.5 | 24.4 | 52 | 173.67\r\n**Runtime Savings** | **-71.11111%** | **-86.35246%** | **-85.38462%** | **-66.60333%**\r\n\u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\r\nTensor.numel() - non-virtual | 22.6 | 63.89 | 109.22 | 294.5\r\nVariable.numel() - virtual | 80.33 | 133.1 | 192 | 810.9\r\n**Runtime Savings** | **-71.86605%** | **-51.9985%** | **-43.11458%** | **-63.68233%**\r\n\u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\r\nTensor.size(0) - non-virtual | 30.4 | 60.1 | 100.44 | 384.3\r\nVariable.size(0) - virtual | 75.4 | 127.67 | 203.8 | 875.9\r\n**Runtime Savings** | **-59.6817%** | **-52.92551%** | **-50.71639%** | **-56.12513%**\r\n\u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\r\nTensor.sizes() - non-virtual | 2 | 4.25 | 13.25 | 67.6\r\nVariable.sizes() - virtual | 5.2 | 28.44 | 62.1 | 254.78\r\n**Runtime Savings** | **-61.53846%** | **-85.05626%** | **-78.66345%** | **-73.46731%**\r\n\u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\r\nTensor.resize_({0}) no-op - non-virtual | 23.11 | 86.44 | 105.44 | 332.33\r\nVariable.resize_({0}) no-op - virtual | 168.4 | 254.22 | 348.56 | 890.9\r\n**Runtime Savings** | **-86.27672%** | **-65.99795%** | **-69.74983%** | **-62.69727%**\r\n\u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\r\nTensor.resize_({64, 2048}) no-op - non-virtual | 33.4 | 102.56 | 129.56 | 407.22\r\nVariable.resize_({64, 2048}) no-op - virtual | 193 | 278.1 | 364.9 | 936.6\r\n**Runtime Savings** | **-82.6943%** | **-63.12118%** | **-64.49438%** | **-56.52146%**\r\n\r\n> Benchmarked commit: https://github.com/pytorch/pytorch/commit/f000101b8139378f342b175c11072a925c9d7c7a\r\n> Benchmark script: https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp\r\n> Non-virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:nonvirtual_tensorimpl\r\n> Virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:virtual_tensorimpl\r\n\r\nBased on our current implementation, the runtime difference for `dim()`, `numel()`, `size()`, `sizes()`, and no-op `resize()` comes from the virtual function call overhead and the `at::Tensor` data member indirection in `Variable::Impl`. If we de-virtualize those functions, we would be able to cut the runtime by **43%-86%** on the most common Tensor functions.\r\n\r\n- **`at::` namespace adds cognitive overhead**: Right now, C++ users have to be very careful about choosing between `at::` and `torch::` functions, based on whether they want autograd behavior. (For example, a Tensor created from `at::ones()` can't record history, but a Variable created from `torch::ones()` can.) Ideally, we want to discourage use of the `at::` namespace and only use the `torch::` namespace, to be consistent with the Python API (which uses the `torch` module for everything). And we let people decide whether to enable history recording for a tensor in the following ways:\r\n    * Case 1: At tensor creation time:\r\n        ```cpp\r\n        auto a = torch::ones({2, 2});  // a is not recording history  (this already works)\r\n        auto a = torch::ones({2, 2}, torch::requires_grad());  // a is recording history  (this already works)\r\n        ```\r\n    * Case 2: After tensor creation:\r\n        ```cpp\r\n        auto a = torch::ones({2, 2});  // a is not recording history (this already works)\r\n        a.requires_grad_(true);  // a is recording history now (new API)\r\n        ```\r\n   Note that the deprecation of `at::` namespace in our public API is easier to achieve if we successfully de-virtualize and speed up the common Tensor functions on Variables, so that users have one less reason to stick with `at::` APIs. (In an ideal world we don't need to expose the `at::` namespace at all, but for backward compatibility reasons we should still keep it around.)\r\n\r\n- **Overly Complex OOP design**: `Variable::Impl` is a subclass of TensorImpl, but it also has an `at::Tensor` data member which internally wraps another TensorImpl. This co-existence of \"is-a\" and \"has-a\" relationship makes the code complicated and adds cognitive overhead.  In particular, it's difficult to track which functions we have overridden in `Variable::Impl`, and which functions are applicable to Tensor vs. Variable (e.g. `is_wrapped_number()` is only valid on Tensor, not Variable) (for more context, also see note: [We regret making Variable hold a Tensor](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/TensorImpl.h#L495-L514)).\r\n\r\n- **Unused data members in `Variable::Impl` take up cache/memory space**: Since `Variable::Impl` is a subclass of TensorImpl, it contains all of the data members that a normal TensorImpl would have (such as `sizes_` / `strides_` / etc.). However, the `Variable::Impl` functions always call into the underlying `at::Tensor` and ignores the rest of the fields, which causes a lot of wasted cache/memory space.\r\n\r\n## Pitch\r\n\r\nI propose that we remove the `at::Tensor` data member in `Variable::Impl` https://github.com/pytorch/pytorch/blob/348867c10b14509518291fc7a03d17a087f9c7bf/torch/csrc/autograd/variable.h#L347 and move all autograd-specific metadata from `Variable::Impl` to TensorImpl. As a result, here are the new behaviors: \r\n\r\n- **When C++ user wants to create a history-recording `Variable` from an `at::Tensor`**:\r\nCurrent API (unchanged):\r\n```cpp\r\nauto var = make_variable(tensor, /*requires_grad*/true)\r\n```\r\nUnder the hood, we do the following steps:\r\n1. We make a shallow copy of `tensor`'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. `size` / `stride`).\r\n    - Note that subclasses of TensorImpl (e.g. `SparseTensorImpl`) need to know how to make a shallow copy of themselves, and we dispatch this operation to each TensorImpl subclass' own `shallow_copy()` function (by making the `shallow_copy()` function virtual in TensorImpl and overriding it in TensorImpl subclasses).\r\n2. We initialize a struct called `AutogradMeta`, which stores autograd-specific fields (such as `grad_`/`grad_fn_`/`grad_accumulator_`).\r\n3. We assign the struct to the `AutogradMeta` pointer in the new TensorImpl.\r\n4. We return a Variable that wraps the new TensorImpl.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to create a non-history-recording `Variable` from another `Variable`:**\r\nCurrent API (unchanged):\r\n```cpp\r\nauto var = torch::ones({2, 2}, torch::requires_grad()); // var is recording history\r\nauto var_detached = var.detach() // var_detached is the non-history-recording version of var\r\n```\r\nWhen the user calls `var.detach()`, we do the following under the hood:\r\n1. We do the shallow copy of `var`'s TensorImpl in the same way as above.\r\n2. We set the `AutogradMeta` pointer to NULL, to indicate that it doesn't need to record history.\r\n3. We return a Variable that wraps the new TensorImpl.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to enable/disable history-recording for a `Variable`:**\r\nProposed API:\r\n```cpp\r\nauto var = torch::ones({2, 2});  // var is not recording history (this already works)\r\nvar.requires_grad_(true);  // var is recording history now (new API)\r\nvar.requires_grad_(false); // var is not recording history anymore (new API)\r\n```\r\nWhen the user calls `var.requires_grad_(true)`, we do the following under the hood:\r\n1. We initialize a struct called `AutogradMeta`, which stores autograd-specific fields (such as `grad_`/`grad_fn_`/`grad_accumulator_`).\r\n2. We assign the struct to the `AutogradMeta` pointer in `var`'s TensorImpl.\r\n\r\nWhen the user calls `var.requires_grad_(false)`, we do the following under the hood:\r\n1. We set the `AutogradMeta` pointer in `var`'s TensorImpl to NULL.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to treat a Variable as a non-Variable `at::Tensor` when dispatching through `type()`:**\r\nProposed API:\r\n```cpp\r\n{\r\n  auto var_type = variable.type();  // \"var_type\" is a Variable type\r\n}\r\n{\r\n  at::AutoGradMode grad_mode(false);  // thread-local guard (new API)\r\n  auto non_var_type = variable.type();  // \"non_var_type\" is a non-Variable type\r\n}\r\n{\r\n  at::AutoGradMode grad_mode(true);  // thread-local guard (new API)\r\n  auto var_type = variable.type();  // \"var_type\" is a Variable type\r\n}\r\n```\r\nUnder the hood, `type()` checks whether the `at::AutoGradMode` thread-local guard is enabled and takes its value into consideration when computing the type of the variable.\r\n\r\n**NOTE**:  we will also remove the `Variable.data()` API (which returns a non-history-recording `at::Tensor` from the Variable), since now we can use the thread-local guard to decide whether we want to dispatch to the non-Variable code paths.\r\n\r\n\r\n## Breaking changes\r\n\r\nNote that this change will break the current API in the following way:\r\n\r\nIn the old world, whenever we want to create a `Variable` that shares the same data with another `Variable`, we simply do `auto var_new = make_variable(var.data())` or `auto var_new = var.detach()`, and any shape / data / storage pointer changes to `var_new` will be reflected in `var` automatically, because internally they share the same underlying `at::Tensor`.\r\n\r\nHowever, in the new world, there is no concept of the \"underlying `at::Tensor`\" of a Variable, since the Variable itself is the Tensor. When we want to create a `Variable` that shares the same data with another `Variable`, we can still call `auto var_new = var.detach()`, but in this case, only the tensor storage data is shared (via ref-counted pointer) between `var_new` and `var`, but not the tensor size/stride information (they are copied by value). In other words, changing anything (e.g. size / stride / storage_ptr ) in the detached Variable (`var_new`) that are not bits inside tensor storage won't update the original Variable (`var`), and we should no longer expect those data to be shared.\r\n\r\nThis has implications for Python call sites that do\r\n```python\r\ntensor.data.in_place_operation_()\r\n```\r\nor\r\n```python\r\ntensor_detached = tensor.detach()\r\ntensor_detached.in_place_operation_()\r\n```\r\nIf `in_place_operation_()` only updates the data inside the tensor (such as `zeros_()`), such operation will still work properly; if the in-place operation changes the size, stride or the storage pointer inside the TensorImpl (e.g. `resize_` / `resize_as_` / `set_` / `transpose_`), such operation on `tensor.data` or `tensor_detached` will no longer update the `tensor`. We will address this inconsistency in the following ways:\r\n\r\n1. Add a flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n\r\n\r\n## Upcoming PRs\r\n- [ ] 1st PR: (folded into 2nd PR: https://github.com/pytorch/pytorch/pull/13827)\r\n1. Add a flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n\r\n- [ ] 2nd PR: https://github.com/pytorch/pytorch/pull/13827\r\n1. Move `Variable::Impl` data members into TensorImpl as `AutogradMeta` struct\r\n2. Change `Variable::Impl` functions to use data members in `AutogradMeta` struct\r\n3. Add `shallow_copy()` function to each subclass of TensorImpl\r\n4. Do shallow copy when the user calls `make_variable(tensor)` / `variable.detach()` (Reason: now that autograd metadata lives in TensorImpl, in order to create a new history for for the Variable returned from `variable.detach()` we not only need to create a new AutogradMeta struct, but we also need to create a new TensorImpl object that stores pointer to the new AutogradMeta struct (which we obtain by shallow-copying the original TensorImpl). Otherwise, changing history of the detached Variable will also change the history of the original Variable, which is not the correct behavior.)\r\n\r\n- [ ] 3rd PR: https://github.com/pytorch/pytorch/pull/13932\r\n1. Move `Variable::Impl` functions such as `backward()` / `rebase_history()` / `grad_accumulator()` / `grad_fn()` / `set_data()` out of `Variable::Impl` and into `Variable`. In those functions, we will use the getter to obtain the Variable's underlying `Variable::Impl` and perform the operations on it. (We need to make this change so that we can remove `Variable::Impl` class in the next PR.)\r\n\r\n- [ ] 4th PR:\r\n1. Remove the `at::Tensor` data member from `Variable::Impl`\r\n2. Remove the `Variable::Impl` class\r\n3. Remove all `Variable.data()` call sites, and use `Variable` directly\r\n4. Remove `Variable.data()` function\r\n6. In VariableType.cpp, use thread-local guard to make sure that operations on `baseType` still dispatch to non-Variable type, even if the parameters are now Variables\r\n7. Remove mentions of `Variable::Impl` and `DifferentiableViewImpl`\r\n8. Fix comments in `[Tensor versus Variable in C++]`, `[We regret making Variable hold a Tensor]` and `[ Autograd View Variables ]`\r\n9. **NOTE**: we don't need to add `SparseVariableImpl` that handles how to copy `SparseTensorImpl`, because `SparseTensorImpl` already implements the `shallow_copy()` function that Variable factory functions can call.\r\n\r\n- [ ] 5th PR\r\n1. Remove `is_variable()` and `is_variable_` from Tensor, use the AutogradMeta pointer instead\r\n\r\n- [ ] 6th PR (ongoing: https://github.com/pytorch/pytorch/pull/11259, will be rebased once we are at this step):\r\n1. Move the sparse-specific implementation of common Tensor functions such as `dim()` / `sizes()` from `SparseTensorImpl` to TensorImpl, by branching based on `is_sparse()` in TensorImpl\r\n\r\n- [ ] 7th PR:\r\n1. Remove the `virtual` attribute from common Tensor functions in TensorImpl\r\n2. Verify the performance gain from de-virtualization.\r\n\r\n- [ ] 8th PR:\r\n1. Merge `at::AutoGradMode` and `autograd::AutoGradMode`\r\n\r\n- [ ] 9th PR (not strictly related to the merge, but for improving consistency with Python API):\r\n1. Add `Tensor.requires_grad_(bool)` C++ API (which internally calls `Tensor.set_requires_grad(bool)`), to be consistent with Python API\r\n\r\n\r\n\r\n\r\n"}