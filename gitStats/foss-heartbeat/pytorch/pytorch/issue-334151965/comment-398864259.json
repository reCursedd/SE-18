{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398864259", "html_url": "https://github.com/pytorch/pytorch/issues/8692#issuecomment-398864259", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8692", "id": 398864259, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODg2NDI1OQ==", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-20T19:13:32Z", "updated_at": "2018-06-20T19:13:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Operations done in context of <code>no_grad</code> are not recorded in the autograd history for computing the gradients. This will not affect the initialization.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.nn.init.normal_(a)\ntensor([[ <span class=\"pl-c1\">1.6385</span>,  <span class=\"pl-c1\">1.0863</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.3725</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.9510</span>],\n        [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.5241</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.8865</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.0878</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.9455</span>],\n        [<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.3817</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1652</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.2502</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1927</span>]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.nn.init.normal_(a)\ntensor([[ <span class=\"pl-c1\">0.9640</span>,  <span class=\"pl-c1\">1.4952</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1739</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.4024</span>],\n        [ <span class=\"pl-c1\">0.7667</span>,  <span class=\"pl-c1\">1.1724</span>,  <span class=\"pl-c1\">0.2663</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.7854</span>],\n        [ <span class=\"pl-c1\">0.7183</span>,  <span class=\"pl-c1\">0.1480</span>,  <span class=\"pl-c1\">1.0115</span>,  <span class=\"pl-c1\">0.1128</span>]], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>", "body_text": "Operations done in context of no_grad are not recorded in the autograd history for computing the gradients. This will not affect the initialization.\n>>> import torch\n>>> a = torch.randn(3, 4)\n>>> torch.nn.init.normal_(a)\ntensor([[ 1.6385,  1.0863, -1.3725, -0.9510],\n        [-1.5241, -0.8865, -0.0878, -0.9455],\n        [-0.3817, -0.1652, -1.2502, -0.1927]])\n>>> a = torch.randn(3, 4, requires_grad=True)\n>>> torch.nn.init.normal_(a)\ntensor([[ 0.9640,  1.4952, -0.1739, -0.4024],\n        [ 0.7667,  1.1724,  0.2663, -0.7854],\n        [ 0.7183,  0.1480,  1.0115,  0.1128]], requires_grad=True)", "body": "Operations done in context of `no_grad` are not recorded in the autograd history for computing the gradients. This will not affect the initialization.\r\n\r\n```python\r\n>>> import torch\r\n>>> a = torch.randn(3, 4)\r\n>>> torch.nn.init.normal_(a)\r\ntensor([[ 1.6385,  1.0863, -1.3725, -0.9510],\r\n        [-1.5241, -0.8865, -0.0878, -0.9455],\r\n        [-0.3817, -0.1652, -1.2502, -0.1927]])\r\n>>> a = torch.randn(3, 4, requires_grad=True)\r\n>>> torch.nn.init.normal_(a)\r\ntensor([[ 0.9640,  1.4952, -0.1739, -0.4024],\r\n        [ 0.7667,  1.1724,  0.2663, -0.7854],\r\n        [ 0.7183,  0.1480,  1.0115,  0.1128]], requires_grad=True)\r\n```"}