{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7968", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7968/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7968/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7968/events", "html_url": "https://github.com/pytorch/pytorch/issues/7968", "id": 327921242, "node_id": "MDU6SXNzdWUzMjc5MjEyNDI=", "number": 7968, "title": "[JIT][script] Design for collecting results across iterations for RNNs", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-30T22:25:06Z", "updated_at": "2018-05-30T22:41:03Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Take the following example:</p>\n<pre><code>@torch.jit.compile(nderivs=1)\ndef lstm_block(input_, hx, cx, w_hh, b_hh):\n    output = []\n    for i in range(BLOCK_SIZE):\n        hx, cx = lstm_cell(input_[i], (hx, cx), w_hh, b_hh)\n        output.append(hx)\n    return output, cx\n</code></pre>\n<p>Note the collection of the outputs from each time-step of the RNN via a Python list and its <code>append()</code> method. After this list is returned, it is passed into <code>torch.stack</code> to provide a single tensor containing the results of applying the RNN over the input sequence.</p>\n<p>Previously, when this was compiled, the loop would be unrolled to BLOCK_SIZE time-steps, and each output would be given a new value within the IR. Then, all of these individual outputs were statically known and could be passed into the <code>stack</code> operator.</p>\n<p>This is not the case for the script. We need to design the interface for script in such a way that we can:</p>\n<ol>\n<li>Emulate this behavior in a user-friendly way</li>\n<li>Retain differentiability, particularly symbolic differentiability, for at least a subset of this operation</li>\n<li>Effectively optimize loops of this form</li>\n</ol>\n<p>Some alternatives we've considered have included:</p>\n<ol>\n<li>Add lists and <code>append()</code> to the script. An example is repeated <code>append()</code>s and then <code>stack()</code>, which would become something like <code>chunk()</code> and <code>pop()</code> in the backward pass</li>\n<li></li>\n</ol>\n<p>(TODO: finish this once I get a chance)</p>", "body_text": "Take the following example:\n@torch.jit.compile(nderivs=1)\ndef lstm_block(input_, hx, cx, w_hh, b_hh):\n    output = []\n    for i in range(BLOCK_SIZE):\n        hx, cx = lstm_cell(input_[i], (hx, cx), w_hh, b_hh)\n        output.append(hx)\n    return output, cx\n\nNote the collection of the outputs from each time-step of the RNN via a Python list and its append() method. After this list is returned, it is passed into torch.stack to provide a single tensor containing the results of applying the RNN over the input sequence.\nPreviously, when this was compiled, the loop would be unrolled to BLOCK_SIZE time-steps, and each output would be given a new value within the IR. Then, all of these individual outputs were statically known and could be passed into the stack operator.\nThis is not the case for the script. We need to design the interface for script in such a way that we can:\n\nEmulate this behavior in a user-friendly way\nRetain differentiability, particularly symbolic differentiability, for at least a subset of this operation\nEffectively optimize loops of this form\n\nSome alternatives we've considered have included:\n\nAdd lists and append() to the script. An example is repeated append()s and then stack(), which would become something like chunk() and pop() in the backward pass\n\n\n(TODO: finish this once I get a chance)", "body": "Take the following example:\r\n\r\n```\r\n@torch.jit.compile(nderivs=1)\r\ndef lstm_block(input_, hx, cx, w_hh, b_hh):\r\n    output = []\r\n    for i in range(BLOCK_SIZE):\r\n        hx, cx = lstm_cell(input_[i], (hx, cx), w_hh, b_hh)\r\n        output.append(hx)\r\n    return output, cx\r\n```\r\n\r\nNote the collection of the outputs from each time-step of the RNN via a Python list and its `append()` method. After this list is returned, it is passed into `torch.stack` to provide a single tensor containing the results of applying the RNN over the input sequence.\r\n\r\nPreviously, when this was compiled, the loop would be unrolled to BLOCK_SIZE time-steps, and each output would be given a new value within the IR. Then, all of these individual outputs were statically known and could be passed into the `stack` operator.\r\n\r\nThis is not the case for the script. We need to design the interface for script in such a way that we can:\r\n\r\n1) Emulate this behavior in a user-friendly way\r\n2) Retain differentiability, particularly symbolic differentiability, for at least a subset of this operation\r\n3) Effectively optimize loops of this form\r\n\r\nSome alternatives we've considered have included:\r\n\r\n1) Add lists and `append()` to the script. An example is repeated `append()`s and then `stack()`, which would become something like `chunk()` and `pop()` in the backward pass\r\n2) \r\n\r\n(TODO: finish this once I get a chance)\r\n"}