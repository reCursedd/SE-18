{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/277389616", "html_url": "https://github.com/pytorch/pytorch/issues/683#issuecomment-277389616", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/683", "id": 277389616, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NzM4OTYxNg==", "user": {"login": "bobbens", "id": 54677, "node_id": "MDQ6VXNlcjU0Njc3", "avatar_url": "https://avatars1.githubusercontent.com/u/54677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bobbens", "html_url": "https://github.com/bobbens", "followers_url": "https://api.github.com/users/bobbens/followers", "following_url": "https://api.github.com/users/bobbens/following{/other_user}", "gists_url": "https://api.github.com/users/bobbens/gists{/gist_id}", "starred_url": "https://api.github.com/users/bobbens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bobbens/subscriptions", "organizations_url": "https://api.github.com/users/bobbens/orgs", "repos_url": "https://api.github.com/users/bobbens/repos", "events_url": "https://api.github.com/users/bobbens/events{/privacy}", "received_events_url": "https://api.github.com/users/bobbens/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-03T23:19:07Z", "updated_at": "2017-02-03T23:19:07Z", "author_association": "NONE", "body_html": "<p>Yes, it is very expensive. Yes, I don't want the iterators to end. In fact, I am not using a single iterator, but multiple at the same time (from different sources), and have to have constant batch sizes, which is why if they reach the end it is troublesome.</p>\n<ol>\n<li>Yes, I am doing something like that. However, it does feel \"unclean\" from the code point of view.</li>\n<li>I have inserted a numpy.random.seed() in the generator itself that I trigger at the sampling start, which does work, but again is rather hackish.</li>\n</ol>\n<p>From my point of view, I have it all working, although it does feel hackish and I wouldn't be surprised to see it break when the DataLoader internals slightly change. So  if you deem this not an issue (or feature bloat), feel free to close the issue.</p>\n<p>Easiest solution would be to have a parameter to tell DataLoader it is a generator, not an iterator, and have it use for example <code>__call__</code> instead of <code>__iter__</code> in that case. That would just generate a new sample. On the random issue, either have all the processes automatically generate new random seeds, or add an option to give the worker threads a \"initialization\" function, which would be run when they are created. This would possibly allow other usages, while also allowing the user to randomize the seeds.</p>", "body_text": "Yes, it is very expensive. Yes, I don't want the iterators to end. In fact, I am not using a single iterator, but multiple at the same time (from different sources), and have to have constant batch sizes, which is why if they reach the end it is troublesome.\n\nYes, I am doing something like that. However, it does feel \"unclean\" from the code point of view.\nI have inserted a numpy.random.seed() in the generator itself that I trigger at the sampling start, which does work, but again is rather hackish.\n\nFrom my point of view, I have it all working, although it does feel hackish and I wouldn't be surprised to see it break when the DataLoader internals slightly change. So  if you deem this not an issue (or feature bloat), feel free to close the issue.\nEasiest solution would be to have a parameter to tell DataLoader it is a generator, not an iterator, and have it use for example __call__ instead of __iter__ in that case. That would just generate a new sample. On the random issue, either have all the processes automatically generate new random seeds, or add an option to give the worker threads a \"initialization\" function, which would be run when they are created. This would possibly allow other usages, while also allowing the user to randomize the seeds.", "body": "Yes, it is very expensive. Yes, I don't want the iterators to end. In fact, I am not using a single iterator, but multiple at the same time (from different sources), and have to have constant batch sizes, which is why if they reach the end it is troublesome.\r\n\r\n 1. Yes, I am doing something like that. However, it does feel \"unclean\" from the code point of view.\r\n 2. I have inserted a numpy.random.seed() in the generator itself that I trigger at the sampling start, which does work, but again is rather hackish.\r\n\r\nFrom my point of view, I have it all working, although it does feel hackish and I wouldn't be surprised to see it break when the DataLoader internals slightly change. So  if you deem this not an issue (or feature bloat), feel free to close the issue.\r\n\r\nEasiest solution would be to have a parameter to tell DataLoader it is a generator, not an iterator, and have it use for example ```__call__``` instead of ```__iter__``` in that case. That would just generate a new sample. On the random issue, either have all the processes automatically generate new random seeds, or add an option to give the worker threads a \"initialization\" function, which would be run when they are created. This would possibly allow other usages, while also allowing the user to randomize the seeds."}