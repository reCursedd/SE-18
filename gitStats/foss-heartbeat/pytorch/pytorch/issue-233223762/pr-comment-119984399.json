{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119984399", "pull_request_review_id": 41918047, "id": 119984399, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExOTk4NDM5OQ==", "diff_hunk": "@@ -130,51 +130,50 @@ class _SelectionFunction(Function):\n     # additional_args is prepended before dim when calling the tensor\n     # function. It's a no-op for subclasses other than kthvalue.\n     # kthvalue not only requires us to pass a dim, but also preceed it with k.\n-    additional_args = tuple()\n-\n-    def __init__(self, dim=None, keepdim=True):\n-        super(_SelectionFunction, self).__init__()\n-        self.dim = dim\n-        self.keepdim = keepdim\n-\n-    def forward(self, input):\n-        fn = getattr(input, type(self).__name__.lower())\n-        self.input_size = input.size()\n-        if self.dim is None and self.has_all_reduce:\n-            value = fn(*self.additional_args)\n-            self.indices = tuple(input.eq(value).nonzero()[0])\n+\n+    @classmethod\n+    def forward(cls, ctx, input, dim=None, keepdim=True, additional_args=tuple()):\n+        fn = getattr(input, cls.__name__.lower())\n+        ctx.dim = dim\n+        ctx.keepdim = keepdim\n+        ctx.additional_args = additional_args\n+        ctx.input_size = input.size()\n+        if ctx.dim is None and cls.has_all_reduce:\n+            value = fn(*additional_args)\n+            ctx.indices_tuple = tuple(input.eq(value).nonzero()[0])\n             return input.new((value,))\n         else:\n-            if self.dim is None:\n+            if ctx.dim is None:\n                 dim = input.dim() - 1\n             else:\n-                dim = self.dim\n-            args = (dim, self.keepdim)\n-            if self.additional_args:\n-                args = self.additional_args + args\n+                dim = ctx.dim\n+            args = (dim, keepdim)\n+            if additional_args:\n+                args = additional_args + args\n             output, indices = fn(*args)\n-            self.save_for_backward(indices)\n-            self.mark_non_differentiable(indices)\n+            ctx.save_for_backward(indices)\n+            ctx.mark_non_differentiable(indices)\n             return output, indices\n \n-    def backward(self, grad_output, grad_indices=None):\n-        grad_input = grad_output.new(*self.input_size).zero_()\n-        if self.dim is None and self.has_all_reduce:\n-            grad_input[self.indices] = grad_output[0]\n+    @classmethod\n+    def backward(cls, ctx, grad_output, grad_indices=None):\n+        grad_input = Variable(grad_output.data.new(*ctx.input_size).zero_())\n+        if ctx.dim is None and cls.has_all_reduce:\n+            grad_input[ctx.indices_tuple] = grad_output.data[0]", "path": "torch/autograd/_functions/reduce.py", "position": 57, "original_position": 57, "commit_id": "e002d5e094d14aba89c0988aec35bf773c5faf4e", "original_commit_id": "e002d5e094d14aba89c0988aec35bf773c5faf4e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is incorrect. `grad_input` won't be connected to `grad_output` in the graph in any way. You can't unpack `.data`", "created_at": "2017-06-03T10:49:01Z", "updated_at": "2018-11-23T15:33:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/1702#discussion_r119984399", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1702", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119984399"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1702#discussion_r119984399"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1702"}}, "body_html": "<p>This is incorrect. <code>grad_input</code> won't be connected to <code>grad_output</code> in the graph in any way. You can't unpack <code>.data</code></p>", "body_text": "This is incorrect. grad_input won't be connected to grad_output in the graph in any way. You can't unpack .data"}