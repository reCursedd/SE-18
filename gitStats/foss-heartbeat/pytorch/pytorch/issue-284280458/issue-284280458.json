{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4330", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4330/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4330/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4330/events", "html_url": "https://github.com/pytorch/pytorch/issues/4330", "id": 284280458, "node_id": "MDU6SXNzdWUyODQyODA0NTg=", "number": 4330, "title": "[Feature Request] Add kl divergence method for torch.distributions", "user": {"login": "p-morais", "id": 3759402, "node_id": "MDQ6VXNlcjM3NTk0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3759402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/p-morais", "html_url": "https://github.com/p-morais", "followers_url": "https://api.github.com/users/p-morais/followers", "following_url": "https://api.github.com/users/p-morais/following{/other_user}", "gists_url": "https://api.github.com/users/p-morais/gists{/gist_id}", "starred_url": "https://api.github.com/users/p-morais/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/p-morais/subscriptions", "organizations_url": "https://api.github.com/users/p-morais/orgs", "repos_url": "https://api.github.com/users/p-morais/repos", "events_url": "https://api.github.com/users/p-morais/events{/privacy}", "received_events_url": "https://api.github.com/users/p-morais/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-12-23T02:26:53Z", "updated_at": "2018-01-15T09:37:32Z", "closed_at": "2018-01-15T09:37:32Z", "author_association": "NONE", "body_html": "<p>This could be implemented as <code>dist1.kldiv(dist2)</code> (provided <code>type(dist1) == type(dist2)</code>) or something similar. KL divergence is a very useful diagnostic for policy gradient algorithms in general and some have a KL divergence penalty as part of their objective function (<a href=\"https://arxiv.org/pdf/1707.02286.pdf\" rel=\"nofollow\">e.g.</a>), so I think this would be useful to have.</p>\n<p><del>Also, the documentation mentions an <a href=\"http://pytorch.org/docs/master/distributions.html#torch.distributions.Distribution.entropy\" rel=\"nofollow\">entropy()</a> method but doing dir(torch.distributions.Distribution) on my installation shows no such method. Is that correct?</del></p>", "body_text": "This could be implemented as dist1.kldiv(dist2) (provided type(dist1) == type(dist2)) or something similar. KL divergence is a very useful diagnostic for policy gradient algorithms in general and some have a KL divergence penalty as part of their objective function (e.g.), so I think this would be useful to have.\nAlso, the documentation mentions an entropy() method but doing dir(torch.distributions.Distribution) on my installation shows no such method. Is that correct?", "body": "This could be implemented as ```dist1.kldiv(dist2)``` (provided ```type(dist1) == type(dist2)```) or something similar. KL divergence is a very useful diagnostic for policy gradient algorithms in general and some have a KL divergence penalty as part of their objective function ([e.g.](https://arxiv.org/pdf/1707.02286.pdf)), so I think this would be useful to have.\r\n\r\n~~Also, the documentation mentions an [entropy()](http://pytorch.org/docs/master/distributions.html#torch.distributions.Distribution.entropy) method but doing dir(torch.distributions.Distribution) on my installation shows no such method. Is that correct?~~\r\n"}