{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/325205661", "html_url": "https://github.com/pytorch/pytorch/issues/2021#issuecomment-325205661", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2021", "id": 325205661, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNTIwNTY2MQ==", "user": {"login": "matt-gardner", "id": 3291951, "node_id": "MDQ6VXNlcjMyOTE5NTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3291951?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt-gardner", "html_url": "https://github.com/matt-gardner", "followers_url": "https://api.github.com/users/matt-gardner/followers", "following_url": "https://api.github.com/users/matt-gardner/following{/other_user}", "gists_url": "https://api.github.com/users/matt-gardner/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt-gardner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt-gardner/subscriptions", "organizations_url": "https://api.github.com/users/matt-gardner/orgs", "repos_url": "https://api.github.com/users/matt-gardner/repos", "events_url": "https://api.github.com/users/matt-gardner/events{/privacy}", "received_events_url": "https://api.github.com/users/matt-gardner/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-27T15:34:12Z", "updated_at": "2017-08-27T15:34:12Z", "author_association": "NONE", "body_html": "<p>Thanks for the tip.  Yep, that would do it: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/5a0163cdee7d2071329d35ec44811abb572c74e8/torch/optim/adagrad.py#L31\">pytorch/torch/optim/adagrad.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 31\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/5a0163cdee7d2071329d35ec44811abb572c74e8\">5a0163c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L31\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"31\"></td>\n          <td id=\"LC31\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> state[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> p.data.new().resize_as_(p.data).zero_() </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Is there are reasonable way to provide something like <code>.cuda()</code> for optimizers?  Probably not...  At the least, the documentation needs to be updated to say that models must be moved to the GPU before the optimizer is created, either <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.Adagrad\" rel=\"nofollow\">here</a>, or <a href=\"http://pytorch.org/docs/master/optim.html#constructing-it\" rel=\"nofollow\">here</a> (probably the latter).  Happy to submit a PR for this if you think that's the best solution.</p>", "body_text": "Thanks for the tip.  Yep, that would do it: \n  \n    \n      pytorch/torch/optim/adagrad.py\n    \n    \n         Line 31\n      in\n      5a0163c\n    \n    \n    \n    \n\n        \n          \n           state['sum'] = p.data.new().resize_as_(p.data).zero_() \n        \n    \n  \n\n\nIs there are reasonable way to provide something like .cuda() for optimizers?  Probably not...  At the least, the documentation needs to be updated to say that models must be moved to the GPU before the optimizer is created, either here, or here (probably the latter).  Happy to submit a PR for this if you think that's the best solution.", "body": "Thanks for the tip.  Yep, that would do it: https://github.com/pytorch/pytorch/blob/5a0163cdee7d2071329d35ec44811abb572c74e8/torch/optim/adagrad.py#L31\r\n\r\nIs there are reasonable way to provide something like `.cuda()` for optimizers?  Probably not...  At the least, the documentation needs to be updated to say that models must be moved to the GPU before the optimizer is created, either [here](http://pytorch.org/docs/master/optim.html#torch.optim.Adagrad), or [here](http://pytorch.org/docs/master/optim.html#constructing-it) (probably the latter).  Happy to submit a PR for this if you think that's the best solution."}