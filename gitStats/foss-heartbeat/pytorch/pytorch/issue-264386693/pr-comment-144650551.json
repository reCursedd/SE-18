{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144650551", "pull_request_review_id": 69339437, "id": 144650551, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NDY1MDU1MQ==", "diff_hunk": "@@ -0,0 +1,191 @@\n+// updateOutput, updateGradInput Kernels ported from Sergey Zagoruyko's pyinn, which itself was a\n+// port from Caffe\n+\n+#include \"THCUNN.h\"\n+#include \"THCDeviceTensor.cuh\"\n+#include \"THCDeviceTensorUtils.cuh\"\n+#include \"THCNumerics.cuh\"\n+#include \"THCReduceApplyUtils.cuh\"\n+#include \"THCSortUtils.cuh\"\n+#include \"THCTensorMathReduce.cuh\"\n+#include \"SharedMem.cuh\"\n+#include \"common.h\"\n+\n+template <typename T, typename IndexType>\n+__global__ void spatialDepthwiseConvolutionUpdateOutput(\n+    const THCDeviceTensor<T, 4> input,\n+    THCDeviceTensor<T, 4> output,\n+    const THCDeviceTensor<T, 4> weight,\n+    const THCDeviceTensor<T, 1> bias,\n+    bool biasEnabled,\n+    IndexType totalElements,\n+    const int outputChannels,\n+    const int depthwiseMultiplier,\n+    const int inputWidth, const int inputHeight,\n+    const int outputWidth, const int outputHeight,\n+    const int kernelWidth, const int kernelHeight,\n+    const int strideWidth, const int strideHeight,\n+    const int padWidth, const int padHeight,\n+    const int dilationWidth, const int dilationHeight)\n+{\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+\n+    const int n = linearIndex / outputChannels / outputHeight / outputWidth;\n+    const int c = (linearIndex / outputHeight / outputWidth) % outputChannels;\n+    const int h = (linearIndex / outputWidth) % outputHeight;\n+    const int w = linearIndex % outputWidth;\n+\n+    const int inputChannel = c / depthwiseMultiplier;\n+    const int inputChannels = outputChannels / depthwiseMultiplier;\n+\n+    int weightOffset = c * kernelHeight * kernelWidth;\n+\n+    T value = biasEnabled ? bias.data()[c] : ScalarConvert<int, T>::to(0);\n+    for (int kH = 0; kH < kernelHeight; ++kH) {\n+      for (int kW = 0; kW < kernelWidth; ++kW) {\n+        const int h_in = -padHeight + h * strideHeight + kH * dilationHeight;\n+        const int w_in = -padWidth + w * strideWidth + kW * dilationWidth;\n+\n+        if ((h_in >= 0) && (h_in < inputHeight) && (w_in >= 0) && (w_in < inputWidth)) {\n+          const IndexType offset = ((n * inputChannels + inputChannel) * inputHeight + h_in) *\n+                                    inputWidth + w_in;\n+          value = THCNumerics<T>::add(\n+            value,\n+            THCNumerics<T>::mul(weight.data()[weightOffset], input.data()[offset]));\n+        }\n+        ++weightOffset;\n+      }\n+    }\n+    output.data()[linearIndex] = value;\n+  }\n+}\n+\n+template <typename T, typename IndexType>\n+__global__ void spatialDepthwiseConvolutionUpdateGradInput(\n+    const THCDeviceTensor<T, 4> gradOutput,\n+    THCDeviceTensor<T, 4> gradInput,\n+    const THCDeviceTensor<T, 4> weight,\n+    IndexType totalElements,\n+    const int inputChannels,\n+    const int depthwiseMultiplier,\n+    const int outputChannels,\n+    const int inputWidth, const int inputHeight,\n+    const int outputWidth, const int outputHeight,\n+    const int kernelWidth, const int kernelHeight,\n+    const int strideWidth, const int strideHeight,\n+    const int padWidth, const int padHeight,\n+    const int dilationWidth, const int dilationHeight)\n+{\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+\n+    const int n = linearIndex / inputChannels / inputHeight / inputWidth;\n+    const int c = (linearIndex / inputHeight / inputWidth) % inputChannels;\n+    const int h = (linearIndex / inputWidth) % inputHeight;\n+    const int w = linearIndex % inputWidth;\n+\n+    T value = ScalarConvert<int, T>::to(0);\n+    for (int multiplier = 0; multiplier < depthwiseMultiplier; ++multiplier) {\n+      int och = (c * depthwiseMultiplier) + multiplier;\n+      int weightOffset = och * kernelHeight * kernelWidth;\n+      for (int kh = 0; kh < kernelHeight; ++kh) {\n+        for (int kw = 0; kw < kernelWidth; ++kw) {\n+          const int h_out_s = h + padHeight - kh * dilationHeight;\n+          const int w_out_s = w + padWidth - kw * dilationWidth;\n+\n+          if (((h_out_s % strideHeight) == 0) && ((w_out_s % strideWidth) == 0)) {\n+            const int h_out = h_out_s / strideHeight;\n+            const int w_out = w_out_s / strideWidth;\n+\n+            if ((h_out >= 0) && (h_out < outputHeight)\n+                  && (w_out >= 0) && (w_out < outputWidth)) {\n+\n+              const int offset = ((n * outputChannels + och) * outputHeight + h_out)\n+                    * outputWidth + w_out;\n+              value = THCNumerics<T>::add(\n+                value,\n+                THCNumerics<T>::mul(weight.data()[weightOffset], gradOutput.data()[offset]));\n+            }\n+          }\n+          ++weightOffset;\n+        }\n+      }\n+    }\n+    gradInput.data()[linearIndex] = value;\n+  }\n+}\n+\n+template <typename T, typename IndexType>\n+__global__ void spatialDepthwiseConvolutionAccGradParameters(\n+    const THCDeviceTensor<T, 4> gradOutput,\n+    const THCDeviceTensor<T, 4> input,\n+    THCDeviceTensor<T, 4> gradWeight,\n+    const int batchSize,\n+    const int inputChannels,\n+    const int kernelChannels,\n+    const int depthwiseMultiplier,\n+    IndexType blockElements,\n+    const int inputWidth, const int inputHeight,\n+    const int outputWidth, const int outputHeight,\n+    const int kernelWidth, const int kernelHeight,\n+    const int strideWidth, const int strideHeight,\n+    const int padWidth, const int padHeight,\n+    const int dilationWidth, const int dilationHeight)\n+{\n+  // Have to use a statically typed Shared Memory pointer\n+  SharedMem<T> smem;\n+\n+  // Each Block is responsible for accumulating over a permutation of\n+  // (channels x kH x kW), use blockIdx to determine which one\n+  int bidx = blockIdx.x;\n+  int kW = bidx % kernelWidth;\n+  int kH = (bidx / kernelWidth) % kernelHeight;\n+  int ch = (bidx / kernelWidth / kernelHeight) % kernelChannels;\n+\n+  // Need to calculate which input channel is associated with this filter\n+  // channel\n+  int inputCh = ch / depthwiseMultiplier;\n+\n+  T grad = ScalarConvert<float, T>::to(0.0);\n+\n+  // Block-stride loop over the number of elements we need to reduce\n+  for (IndexType idx = threadIdx.x; idx < blockElements; idx += blockDim.x) {\n+    // Need to calculate the following: batch position, and offset into the gradOutput\n+    // in height, and width. We can intuit the corresponding position in the input from\n+    // the other parameters we have\n+    int go_w_offset = idx % outputWidth;\n+    int go_h_offset = (idx / outputWidth) % outputHeight;\n+    int batch = (idx / outputWidth / outputHeight) % batchSize;\n+\n+    int i_w_offset = (go_w_offset * strideWidth) + (kW * dilationWidth) - padWidth;\n+    int i_h_offset = (go_h_offset * strideHeight) + (kH * dilationHeight) - padHeight;\n+\n+    if (i_w_offset >= 0 && i_h_offset >= 0 && i_w_offset < inputWidth && i_h_offset < inputHeight) {\n+      int inputOffset = ((batch * inputChannels + inputCh) * inputHeight + i_h_offset) * inputWidth + i_w_offset;\n+      int outputOffset = ((batch * kernelChannels + ch) * outputHeight + go_h_offset) * outputWidth + go_w_offset;\n+      grad = THCNumerics<T>::add(\n+          grad,\n+          THCNumerics<T>::mul( input.data()[inputOffset], gradOutput.data()[outputOffset]));\n+    }\n+  }\n+  __syncthreads();\n+\n+  // At this point each thread in the block has a local gradient, which we need to\n+  // accumulate prior to writing the global value\n+  T *buf = smem.getPointer();\n+  T tval = reduceBlock<T, ReduceAdd<T, T>>(\n+      buf, blockDim.x, grad, ReduceAdd<T, T>(), ScalarConvert<float, T>::to(0));\n+", "path": "torch/lib/THCUNN/SpatialDepthwiseConvolution.cu", "position": null, "original_position": 181, "commit_id": "65cc6f18f7ad5eb23a300b5b7715dc6ea2282b5d", "original_commit_id": "e322bc9f0ca6241150ff49ab4141a67b98530b15", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "reduction through shared memory (in reduceBlock) is usually slower than through warp intrinsics, but it's likely not a bottleneck here. ", "created_at": "2017-10-13T20:31:00Z", "updated_at": "2018-11-23T15:35:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/3057#discussion_r144650551", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3057", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144650551"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3057#discussion_r144650551"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3057"}}, "body_html": "<p>reduction through shared memory (in reduceBlock) is usually slower than through warp intrinsics, but it's likely not a bottleneck here.</p>", "body_text": "reduction through shared memory (in reduceBlock) is usually slower than through warp intrinsics, but it's likely not a bottleneck here."}