{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144916432", "pull_request_review_id": 69640358, "id": 144916432, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NDkxNjQzMg==", "diff_hunk": "@@ -0,0 +1,191 @@\n+// updateOutput, updateGradInput Kernels ported from Sergey Zagoruyko's pyinn, which itself was a\n+// port from Caffe\n+\n+#include \"THCUNN.h\"\n+#include \"THCDeviceTensor.cuh\"\n+#include \"THCDeviceTensorUtils.cuh\"\n+#include \"THCNumerics.cuh\"\n+#include \"THCReduceApplyUtils.cuh\"\n+#include \"THCSortUtils.cuh\"\n+#include \"THCTensorMathReduce.cuh\"\n+#include \"SharedMem.cuh\"\n+#include \"common.h\"\n+\n+template <typename T, typename IndexType>\n+__global__ void spatialDepthwiseConvolutionUpdateOutput(\n+    const THCDeviceTensor<T, 4> input,\n+    THCDeviceTensor<T, 4> output,\n+    const THCDeviceTensor<T, 4> weight,\n+    const THCDeviceTensor<T, 1> bias,\n+    bool biasEnabled,\n+    IndexType totalElements,\n+    const int outputChannels,\n+    const int depthwiseMultiplier,\n+    const int inputWidth, const int inputHeight,\n+    const int outputWidth, const int outputHeight,\n+    const int kernelWidth, const int kernelHeight,\n+    const int strideWidth, const int strideHeight,\n+    const int padWidth, const int padHeight,\n+    const int dilationWidth, const int dilationHeight)\n+{\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+\n+    const int n = linearIndex / outputChannels / outputHeight / outputWidth;\n+    const int c = (linearIndex / outputHeight / outputWidth) % outputChannels;\n+    const int h = (linearIndex / outputWidth) % outputHeight;\n+    const int w = linearIndex % outputWidth;\n+\n+    const int inputChannel = c / depthwiseMultiplier;\n+    const int inputChannels = outputChannels / depthwiseMultiplier;\n+\n+    int weightOffset = c * kernelHeight * kernelWidth;\n+\n+    T value = biasEnabled ? bias.data()[c] : ScalarConvert<int, T>::to(0);\n+    for (int kH = 0; kH < kernelHeight; ++kH) {\n+      for (int kW = 0; kW < kernelWidth; ++kW) {\n+        const int h_in = -padHeight + h * strideHeight + kH * dilationHeight;\n+        const int w_in = -padWidth + w * strideWidth + kW * dilationWidth;\n+\n+        if ((h_in >= 0) && (h_in < inputHeight) && (w_in >= 0) && (w_in < inputWidth)) {\n+          const IndexType offset = ((n * inputChannels + inputChannel) * inputHeight + h_in) *\n+                                    inputWidth + w_in;\n+          value = THCNumerics<T>::add(\n+            value,", "path": "torch/lib/THCUNN/SpatialDepthwiseConvolution.cu", "position": null, "original_position": 55, "commit_id": "65cc6f18f7ad5eb23a300b5b7715dc6ea2282b5d", "original_commit_id": "b832c53242f272f5cc86fb1e25385985a02ef098", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "My recollection is ```accreal=double``` for ```float``` in THNN/TH, and ```accreal=float``` for ```float``` in THCUNN/THC, but I might be mistaken. We definitely don't want to accumulate in double for float, double performance is bad on many cards, but we do want float accumulation for halfs. ", "created_at": "2017-10-16T17:41:15Z", "updated_at": "2018-11-23T15:35:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/3057#discussion_r144916432", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3057", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144916432"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3057#discussion_r144916432"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3057"}}, "body_html": "<p>My recollection is <code>accreal=double</code> for <code>float</code> in THNN/TH, and <code>accreal=float</code> for <code>float</code> in THCUNN/THC, but I might be mistaken. We definitely don't want to accumulate in double for float, double performance is bad on many cards, but we do want float accumulation for halfs.</p>", "body_text": "My recollection is accreal=double for float in THNN/TH, and accreal=float for float in THCUNN/THC, but I might be mistaken. We definitely don't want to accumulate in double for float, double performance is bad on many cards, but we do want float accumulation for halfs.", "in_reply_to_id": 144681294}