{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144651684", "pull_request_review_id": 69339437, "id": 144651684, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NDY1MTY4NA==", "diff_hunk": "@@ -0,0 +1,201 @@\n+#ifndef THC_GENERIC_FILE\n+#define THC_GENERIC_FILE \"generic/SpatialDepthwiseConvolution.cu\"\n+#else\n+\n+void THNN_(SpatialDepthwiseConvolution_updateOutput)(\n+                  THCState *state,\n+                  THCTensor *input,\n+                  THCTensor *output,\n+                  THCTensor *weight,\n+                  THCTensor *bias,\n+                  int kW, int kH,\n+                  int dW, int dH,\n+                  int padW, int padH,\n+                  int dilationW, int dilationH)\n+{\n+  THCUNN_assertSameGPU(state, 3, input, output, weight);\n+\n+  // Only handle 4D Input Tensors for now\n+  assert(THCTensor_(nDimension)(state, input) == 4);\n+  assert(THCTensor_(nDimension)(state, weight) == 4);\n+\n+  // We assume that the input and weight Tensors are shaped properly by\n+  // the caller, so we verify that here to some extent\n+\n+  // Weight Tensor is shape (output_channels, 1, kH, kW)\n+  assert(weight->size[1] == 1);\n+\n+  // Input Tensor is shape (N, input_channels, H, W)\n+  // We verify that the # of output_channels is a multiple of input_channels\n+  assert(weight->size[0] % input->size[1] == 0);\n+\n+  // Bias has same # of channels as output\n+  if (bias) {\n+    assert(bias->size[0] == weight->size[0]);\n+  }\n+\n+  // Following the behvaior of other THCUNN functions, we shape the output\n+  // Tensor ourselves\n+\n+  int batchSize = input->size[0];\n+  int height = input->size[2];\n+  int width = input->size[3];\n+  int outputHeight = (height + 2 * padH - (dilationH * (kH - 1) + 1)) / dH + 1;\n+  int outputWidth = (width + 2 * padW - (dilationW * (kW - 1) + 1)) / dW + 1;\n+  int outputChannels = weight->size[0];\n+\n+  THCTensor_(resize4d)(state, output, batchSize, outputChannels, outputHeight, outputWidth);\n+\n+  THCDeviceTensor<real, 4> dInput = toDeviceTensor<real, 4>(state, input);\n+  THCDeviceTensor<real, 4> dWeight = toDeviceTensor<real, 4>(state, weight);\n+  THCDeviceTensor<real, 4> dOutput = toDeviceTensor<real, 4>(state, output);\n+  THCDeviceTensor<real, 1> dBias;\n+  if (bias) {\n+    dBias = toDeviceTensor<real, 1>(state, bias);\n+  }\n+\n+  // Kernel currently relies upon all the Tensors to be contiguous\n+  assert(dInput.isContiguous());\n+  assert(dWeight.isContiguous());\n+  assert(dOutput.isContiguous());\n+\n+  int inputChannels = input->size[1];\n+  int depthwiseMultiplier = outputChannels / inputChannels;\n+\n+  // One thread per output value\n+  int n = THCTensor_(nElement)(state, output);\n+  int blocks = GET_BLOCKS(n);\n+  dim3 grid(blocks);\n+  dim3 block(CUDA_NUM_THREADS);\n+\n+  spatialDepthwiseConvolutionUpdateOutput<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+    dInput, dOutput, dWeight, dBias, bias != NULL, n, outputChannels, depthwiseMultiplier,\n+    width, height, outputWidth, outputHeight,\n+    kW, kH, dW, dH, padW, padH, dilationW, dilationH);\n+\n+  THCudaCheck(cudaGetLastError());\n+}\n+\n+void THNN_(SpatialDepthwiseConvolution_updateGradInput)(\n+                  THCState *state,\n+                  THCTensor *input,\n+                  THCTensor *gradOutput,\n+                  THCTensor *gradInput,\n+                  THCTensor *weight,\n+                  int kW, int kH,\n+                  int dW, int dH,\n+                  int padW, int padH,\n+                  int dilationW, int dilationH)\n+{\n+  THCUNN_assertSameGPU(state, 4, input, gradOutput, gradInput, weight);", "path": "torch/lib/THCUNN/generic/SpatialDepthwiseConvolution.cu", "position": null, "original_position": 90, "commit_id": "65cc6f18f7ad5eb23a300b5b7715dc6ea2282b5d", "original_commit_id": "e322bc9f0ca6241150ff49ab4141a67b98530b15", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "input is not used in kernel, thus does not have to be on the same GPU. Also, same GPU asserts and shape checkes are the same in all 3 functions, save for argument names (updateOutput, updateGradInput, accGradParameters), so can be put into a separate helper function. ", "created_at": "2017-10-13T20:36:52Z", "updated_at": "2018-11-23T15:35:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/3057#discussion_r144651684", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3057", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/144651684"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3057#discussion_r144651684"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3057"}}, "body_html": "<p>input is not used in kernel, thus does not have to be on the same GPU. Also, same GPU asserts and shape checkes are the same in all 3 functions, save for argument names (updateOutput, updateGradInput, accGradParameters), so can be put into a separate helper function.</p>", "body_text": "input is not used in kernel, thus does not have to be on the same GPU. Also, same GPU asserts and shape checkes are the same in all 3 functions, save for argument names (updateOutput, updateGradInput, accGradParameters), so can be put into a separate helper function."}