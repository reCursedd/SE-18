{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/319448113", "html_url": "https://github.com/pytorch/pytorch/issues/2264#issuecomment-319448113", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2264", "id": 319448113, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTQ0ODExMw==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-01T17:58:29Z", "updated_at": "2017-08-01T17:58:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have a fix; the issue is BatchNormBackwards isn't reentrant.  I think what's going on with your first loss function causing a problem but the second not is that the backwards for sum doesn't involve the output, so you never hit the reentrant case, whereas with sum(exp) you do.  We should add tests for this to gradcheck...</p>", "body_text": "I have a fix; the issue is BatchNormBackwards isn't reentrant.  I think what's going on with your first loss function causing a problem but the second not is that the backwards for sum doesn't involve the output, so you never hit the reentrant case, whereas with sum(exp) you do.  We should add tests for this to gradcheck...", "body": "I have a fix; the issue is BatchNormBackwards isn't reentrant.  I think what's going on with your first loss function causing a problem but the second not is that the backwards for sum doesn't involve the output, so you never hit the reentrant case, whereas with sum(exp) you do.  We should add tests for this to gradcheck..."}