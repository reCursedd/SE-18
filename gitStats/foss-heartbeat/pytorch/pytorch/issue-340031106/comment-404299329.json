{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/404299329", "html_url": "https://github.com/pytorch/pytorch/issues/9326#issuecomment-404299329", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9326", "id": 404299329, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDI5OTMyOQ==", "user": {"login": "ybzhou", "id": 4228785, "node_id": "MDQ6VXNlcjQyMjg3ODU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4228785?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybzhou", "html_url": "https://github.com/ybzhou", "followers_url": "https://api.github.com/users/ybzhou/followers", "following_url": "https://api.github.com/users/ybzhou/following{/other_user}", "gists_url": "https://api.github.com/users/ybzhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybzhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybzhou/subscriptions", "organizations_url": "https://api.github.com/users/ybzhou/orgs", "repos_url": "https://api.github.com/users/ybzhou/repos", "events_url": "https://api.github.com/users/ybzhou/events{/privacy}", "received_events_url": "https://api.github.com/users/ybzhou/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-11T20:28:11Z", "updated_at": "2018-07-11T20:39:22Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a> Tested on pytorch master, problem remains. In addition, the same error happens when using cuda also. The key is to set <code>W</code> be an odd number.</p>\n<pre><code>import torch\nimport numpy as np\n\n\ndef test_conv():\n    print(\"test conv\")\n    with torch.no_grad():\n        W = 181\n        F = 20\n        B = 16\n        H = 110\n        x = torch.rand(B, 1, W, H).cuda()\n        conv_layers = [\n            torch.nn.Conv2d(1, F, (31, 11), (1, 1), ),\n        ]\n        conv = torch.nn.Sequential(*conv_layers).cuda()\n        conv.eval()\n        fx = conv(x)\n        print(fx.shape)\n        first_fx = conv(x[:, :, :, :100])\n        second_fx = conv(x[:, :, :, 10:])\n        for i in range(90):\n            print(i, np.array_equal(fx[:, :, :, i], first_fx[:, :, :, i]))\n        assert np.array_equal(fx[:, :, :, :90], first_fx),(\n                        \"first, diff:{}\".format(\n                            np.abs(fx[:, :, :, :90] - first_fx).max()\n                        ))\n        for i in range(90):\n            print(i, np.array_equal(fx[:, :, :, i + 10],\n                                    second_fx[:, :, :, i]))\n        assert np.array_equal(fx[:, :, :, 10:], second_fx), (\n                        \"second diff:{}\".format(\n                            np.abs(fx[:, :, :, 10:] - second_fx).max()\n                        ))\n\n\nif __name__ == '__main__':\n    # torch.manual_seed(123)\n    test_conv()\n</code></pre>", "body_text": "@albanD Tested on pytorch master, problem remains. In addition, the same error happens when using cuda also. The key is to set W be an odd number.\nimport torch\nimport numpy as np\n\n\ndef test_conv():\n    print(\"test conv\")\n    with torch.no_grad():\n        W = 181\n        F = 20\n        B = 16\n        H = 110\n        x = torch.rand(B, 1, W, H).cuda()\n        conv_layers = [\n            torch.nn.Conv2d(1, F, (31, 11), (1, 1), ),\n        ]\n        conv = torch.nn.Sequential(*conv_layers).cuda()\n        conv.eval()\n        fx = conv(x)\n        print(fx.shape)\n        first_fx = conv(x[:, :, :, :100])\n        second_fx = conv(x[:, :, :, 10:])\n        for i in range(90):\n            print(i, np.array_equal(fx[:, :, :, i], first_fx[:, :, :, i]))\n        assert np.array_equal(fx[:, :, :, :90], first_fx),(\n                        \"first, diff:{}\".format(\n                            np.abs(fx[:, :, :, :90] - first_fx).max()\n                        ))\n        for i in range(90):\n            print(i, np.array_equal(fx[:, :, :, i + 10],\n                                    second_fx[:, :, :, i]))\n        assert np.array_equal(fx[:, :, :, 10:], second_fx), (\n                        \"second diff:{}\".format(\n                            np.abs(fx[:, :, :, 10:] - second_fx).max()\n                        ))\n\n\nif __name__ == '__main__':\n    # torch.manual_seed(123)\n    test_conv()", "body": "@albanD Tested on pytorch master, problem remains. In addition, the same error happens when using cuda also. The key is to set `W` be an odd number.\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\ndef test_conv():\r\n    print(\"test conv\")\r\n    with torch.no_grad():\r\n        W = 181\r\n        F = 20\r\n        B = 16\r\n        H = 110\r\n        x = torch.rand(B, 1, W, H).cuda()\r\n        conv_layers = [\r\n            torch.nn.Conv2d(1, F, (31, 11), (1, 1), ),\r\n        ]\r\n        conv = torch.nn.Sequential(*conv_layers).cuda()\r\n        conv.eval()\r\n        fx = conv(x)\r\n        print(fx.shape)\r\n        first_fx = conv(x[:, :, :, :100])\r\n        second_fx = conv(x[:, :, :, 10:])\r\n        for i in range(90):\r\n            print(i, np.array_equal(fx[:, :, :, i], first_fx[:, :, :, i]))\r\n        assert np.array_equal(fx[:, :, :, :90], first_fx),(\r\n                        \"first, diff:{}\".format(\r\n                            np.abs(fx[:, :, :, :90] - first_fx).max()\r\n                        ))\r\n        for i in range(90):\r\n            print(i, np.array_equal(fx[:, :, :, i + 10],\r\n                                    second_fx[:, :, :, i]))\r\n        assert np.array_equal(fx[:, :, :, 10:], second_fx), (\r\n                        \"second diff:{}\".format(\r\n                            np.abs(fx[:, :, :, 10:] - second_fx).max()\r\n                        ))\r\n\r\n\r\nif __name__ == '__main__':\r\n    # torch.manual_seed(123)\r\n    test_conv()\r\n```"}