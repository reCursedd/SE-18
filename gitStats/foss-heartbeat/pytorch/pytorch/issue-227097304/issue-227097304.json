{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1512", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1512/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1512/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1512/events", "html_url": "https://github.com/pytorch/pytorch/issues/1512", "id": 227097304, "node_id": "MDU6SXNzdWUyMjcwOTczMDQ=", "number": 1512, "title": "[feature request] Support tensors of different sizes as batch elements in DataLoader", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-05-08T16:13:15Z", "updated_at": "2018-07-14T10:59:41Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Motivating example is returning bounding box annotation for images along with an image. An annotation list can contain variable number of boxes depending on an image, and padding them to a single length (and storing that length) may be nasty and unnecessarily complex.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.utils.data\nloader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(<span class=\"pl-v\">dataset</span> <span class=\"pl-k\">=</span> [(torch.zeros(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>), torch.zeros(i, <span class=\"pl-c1\">4</span>)) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>)], <span class=\"pl-v\">batch_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>)\n<span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> loader:\n    <span class=\"pl-c1\">print</span>(batch)</pre></div>\n<p>Currently this blows with a message below because collate wants to <code>torch.stack</code> batch elements, regardless if they have same size:</p>\n<pre><code>File \"...torch/utils/data/dataloader.py\", line 188, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \".../torch/utils/data/dataloader.py\", line 110, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \".../torch/utils/data/dataloader.py\", line 92, in default_collate\n    return torch.stack(batch, 0, out=out)\n  File \".../torch/functional.py\", line 56, in stack\n    inputs = [t.unsqueeze(dim) for t in sequence]\nRuntimeError: cannot unsqueeze empty tensor at .../torch/lib/TH/generic/THTensor.c:530\n</code></pre>\n<p>Returning a list instead of variable-sized tensor doesn't work either. Providing a custom collate isn't very nice either, since most of the default behavior needs to be copied, and the default collate doesn't allow hooks.</p>\n<p>A solution would be either adding an easy way to extend the default collate, or changing the first collate's branch to something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">all</span>(<span class=\"pl-c1\">map</span>(torch.is_tensor, batch)) <span class=\"pl-k\">and</span> <span class=\"pl-c1\">any</span>([tensor.size() <span class=\"pl-k\">!=</span> batch[<span class=\"pl-c1\">0</span>].size() <span class=\"pl-k\">for</span> tensor <span class=\"pl-k\">in</span> batch]):\n     <span class=\"pl-k\">return</span> batch</pre></div>\n<p>As a workaround, I'm currently monkey-patching the default collate like this:</p>\n<div class=\"highlight highlight-source-python\"><pre>collate_old <span class=\"pl-k\">=</span> torch.utils.data.dataloader.default_collate\ntorch.utils.data.dataloader.default_collate <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">batch</span>: batch <span class=\"pl-k\">if</span> <span class=\"pl-c1\">all</span>(<span class=\"pl-c1\">map</span>(torch.is_tensor, batch)) <span class=\"pl-k\">and</span> <span class=\"pl-c1\">any</span>([tensor.size() <span class=\"pl-k\">!=</span> batch[<span class=\"pl-c1\">0</span>].size() <span class=\"pl-k\">for</span> tensor <span class=\"pl-k\">in</span> batch]) <span class=\"pl-k\">else</span> collate_old(batch)</pre></div>", "body_text": "Motivating example is returning bounding box annotation for images along with an image. An annotation list can contain variable number of boxes depending on an image, and padding them to a single length (and storing that length) may be nasty and unnecessarily complex.\nimport torch.utils.data\nloader = torch.utils.data.DataLoader(dataset = [(torch.zeros(3, 128, 128), torch.zeros(i, 4)) for i in range(1, 3)], batch_size = 2)\nfor batch in loader:\n    print(batch)\nCurrently this blows with a message below because collate wants to torch.stack batch elements, regardless if they have same size:\nFile \"...torch/utils/data/dataloader.py\", line 188, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \".../torch/utils/data/dataloader.py\", line 110, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \".../torch/utils/data/dataloader.py\", line 92, in default_collate\n    return torch.stack(batch, 0, out=out)\n  File \".../torch/functional.py\", line 56, in stack\n    inputs = [t.unsqueeze(dim) for t in sequence]\nRuntimeError: cannot unsqueeze empty tensor at .../torch/lib/TH/generic/THTensor.c:530\n\nReturning a list instead of variable-sized tensor doesn't work either. Providing a custom collate isn't very nice either, since most of the default behavior needs to be copied, and the default collate doesn't allow hooks.\nA solution would be either adding an easy way to extend the default collate, or changing the first collate's branch to something like:\nif all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]):\n     return batch\nAs a workaround, I'm currently monkey-patching the default collate like this:\ncollate_old = torch.utils.data.dataloader.default_collate\ntorch.utils.data.dataloader.default_collate = lambda batch: batch if all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]) else collate_old(batch)", "body": "Motivating example is returning bounding box annotation for images along with an image. An annotation list can contain variable number of boxes depending on an image, and padding them to a single length (and storing that length) may be nasty and unnecessarily complex.\r\n\r\n```python\r\nimport torch.utils.data\r\nloader = torch.utils.data.DataLoader(dataset = [(torch.zeros(3, 128, 128), torch.zeros(i, 4)) for i in range(1, 3)], batch_size = 2)\r\nfor batch in loader:\r\n    print(batch)\r\n```\r\n\r\nCurrently this blows with a message below because collate wants to `torch.stack` batch elements, regardless if they have same size:\r\n```\r\nFile \"...torch/utils/data/dataloader.py\", line 188, in __next__\r\n    batch = self.collate_fn([self.dataset[i] for i in indices])\r\n  File \".../torch/utils/data/dataloader.py\", line 110, in default_collate\r\n    return [default_collate(samples) for samples in transposed]\r\n  File \".../torch/utils/data/dataloader.py\", line 92, in default_collate\r\n    return torch.stack(batch, 0, out=out)\r\n  File \".../torch/functional.py\", line 56, in stack\r\n    inputs = [t.unsqueeze(dim) for t in sequence]\r\nRuntimeError: cannot unsqueeze empty tensor at .../torch/lib/TH/generic/THTensor.c:530\r\n```\r\n\r\nReturning a list instead of variable-sized tensor doesn't work either. Providing a custom collate isn't very nice either, since most of the default behavior needs to be copied, and the default collate doesn't allow hooks.\r\n\r\nA solution would be either adding an easy way to extend the default collate, or changing the first collate's branch to something like:\r\n\r\n```python\r\nif all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]):\r\n     return batch\r\n```\r\n\r\nAs a workaround, I'm currently monkey-patching the default collate like this:\r\n```python\r\ncollate_old = torch.utils.data.dataloader.default_collate\r\ntorch.utils.data.dataloader.default_collate = lambda batch: batch if all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]) else collate_old(batch)\r\n```"}