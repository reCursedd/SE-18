{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/405015099", "html_url": "https://github.com/pytorch/pytorch/issues/1512#issuecomment-405015099", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1512", "id": 405015099, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTAxNTA5OQ==", "user": {"login": "snakers4", "id": 12515440, "node_id": "MDQ6VXNlcjEyNTE1NDQw", "avatar_url": "https://avatars0.githubusercontent.com/u/12515440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snakers4", "html_url": "https://github.com/snakers4", "followers_url": "https://api.github.com/users/snakers4/followers", "following_url": "https://api.github.com/users/snakers4/following{/other_user}", "gists_url": "https://api.github.com/users/snakers4/gists{/gist_id}", "starred_url": "https://api.github.com/users/snakers4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snakers4/subscriptions", "organizations_url": "https://api.github.com/users/snakers4/orgs", "repos_url": "https://api.github.com/users/snakers4/repos", "events_url": "https://api.github.com/users/snakers4/events{/privacy}", "received_events_url": "https://api.github.com/users/snakers4/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-14T10:44:14Z", "updated_at": "2018-07-14T10:59:41Z", "author_association": "NONE", "body_html": "<p><strong>Hi guys, I faced a similar issue and found an elegant and ingenious solution!</strong></p>\n<p>What if I have a multi-head model (in my case it is <strong>several image sizes and aspect ratios</strong>), and I would like to learn shared representations, but do not want to iterate over several datasets (in my case it caused sequential over-fitting and poorer performance)?<br>\nYeah, ConcatDataset does not help either in this case (it can work w/o shuffling and if your dataset exactly divides by batch-size, but it's not a proper solution).</p>\n<p><strong>You see, as far a I understand, PyTorch Dataset/Dataloader abstractions work the following way:</strong></p>\n<ul>\n<li>First you define an abstract <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py\">Dataset </a> with <code>len()</code> and <code>getitem()</code> methods;</li>\n<li>Then you define a sampling <a href=\"https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html\" rel=\"nofollow\">strategy </a>(usually people just opt for shuffle=True in 95% of cases);</li>\n<li>Then you just use this strategy with the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py\">Dataloader</a>;</li>\n</ul>\n<p>You see, the gist is contained in this <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L465-L491\">portion </a>of the Dataloader code!</p>\n<p>Translated in plain English it goes something like this:</p>\n<ul>\n<li>Has user re-defined standard batch-producing class? No?</li>\n<li>Has use re-defined standard sampling class? No?</li>\n<li>Then just shuffle randomly (or not) and use standard batch sampler</li>\n</ul>\n<p>If you read the standard batch <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L136-L144\">sampler</a> - you will see that it just sequentially samples from a sampling strategy and yields batches when they are full. As easy as that. So ... we can just re-write RandomSampler class a bit to support our multi-cluster behavior and we are good to go =)</p>\n<p><strong>My final logic is the following:</strong></p>\n<ul>\n<li>Somewhere in your Dataset class define cluster_indices property which says which index refers to which cluster of data</li>\n<li>Chunkify your data manually <code>batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]</code></li>\n<li>Drop the batches that are smaller than your batch size</li>\n<li>Concatenate all of your cluster batch lists, flatter the list, shuffle the batches</li>\n<li>Flatten list once again</li>\n<li>Feed to the standard BatchSampler and observe the multi-threaded goodness of PyTorch Dataloader  =)</li>\n</ul>\n<p><strong>New class</strong></p>\n<pre><code>import random\nfrom torch.utils.data.sampler import Sampler\n\nclass ClusterRandomSampler(Sampler):\n    r\"\"\"Takes a dataset with cluster_indices property, cuts it into batch-sized chunks\n    Drops the extra items, not fitting into exact batches\n    Arguments:\n        data_source (Dataset): a Dataset to sample from. Should have a cluster_indices property\n        batch_size (int): a batch size that you would like to use later with Dataloader class\n        shuffle (bool): whether to shuffle the data or not\n    \"\"\"\n\n    def __init__(self, data_source, batch_size, shuffle=True):\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n    def flatten_list(self, lst):\n        return [item for sublist in lst for item in sublist]\n\n    def __iter__(self):\n\n        batch_lists = []\n        for cluster_indices in train_dataset.cluster_indices:\n            batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]\n            # filter our the shorter batches\n            batches = [_ for _ in batches if len(_) == self.batch_size]\n            if self.shuffle:\n                random.shuffle(batches)\n            batch_lists.append(batches)       \n        \n        # flatten lists and shuffle the batches if necessary\n        # this works on batch level\n        lst = self.flatten_list(batch_lists)\n        if self.shuffle:\n            random.shuffle(lst)\n        # final flatten  - produce flat list of indexes\n        lst = self.flatten_list(lst)        \n        return iter(lst)\n\n    def __len__(self):\n        return len(self.data_source)\n</code></pre>\n<p><strong>Usage</strong></p>\n<pre><code>sampler = ClusterRandomSampler(train_dataset,512,True)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=512,        \n    sampler=sampler,\n    shuffle=False,\n    num_workers=1,\n    pin_memory=False,\n    drop_last=False)\n</code></pre>", "body_text": "Hi guys, I faced a similar issue and found an elegant and ingenious solution!\nWhat if I have a multi-head model (in my case it is several image sizes and aspect ratios), and I would like to learn shared representations, but do not want to iterate over several datasets (in my case it caused sequential over-fitting and poorer performance)?\nYeah, ConcatDataset does not help either in this case (it can work w/o shuffling and if your dataset exactly divides by batch-size, but it's not a proper solution).\nYou see, as far a I understand, PyTorch Dataset/Dataloader abstractions work the following way:\n\nFirst you define an abstract Dataset  with len() and getitem() methods;\nThen you define a sampling strategy (usually people just opt for shuffle=True in 95% of cases);\nThen you just use this strategy with the Dataloader;\n\nYou see, the gist is contained in this portion of the Dataloader code!\nTranslated in plain English it goes something like this:\n\nHas user re-defined standard batch-producing class? No?\nHas use re-defined standard sampling class? No?\nThen just shuffle randomly (or not) and use standard batch sampler\n\nIf you read the standard batch sampler - you will see that it just sequentially samples from a sampling strategy and yields batches when they are full. As easy as that. So ... we can just re-write RandomSampler class a bit to support our multi-cluster behavior and we are good to go =)\nMy final logic is the following:\n\nSomewhere in your Dataset class define cluster_indices property which says which index refers to which cluster of data\nChunkify your data manually batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]\nDrop the batches that are smaller than your batch size\nConcatenate all of your cluster batch lists, flatter the list, shuffle the batches\nFlatten list once again\nFeed to the standard BatchSampler and observe the multi-threaded goodness of PyTorch Dataloader  =)\n\nNew class\nimport random\nfrom torch.utils.data.sampler import Sampler\n\nclass ClusterRandomSampler(Sampler):\n    r\"\"\"Takes a dataset with cluster_indices property, cuts it into batch-sized chunks\n    Drops the extra items, not fitting into exact batches\n    Arguments:\n        data_source (Dataset): a Dataset to sample from. Should have a cluster_indices property\n        batch_size (int): a batch size that you would like to use later with Dataloader class\n        shuffle (bool): whether to shuffle the data or not\n    \"\"\"\n\n    def __init__(self, data_source, batch_size, shuffle=True):\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n    def flatten_list(self, lst):\n        return [item for sublist in lst for item in sublist]\n\n    def __iter__(self):\n\n        batch_lists = []\n        for cluster_indices in train_dataset.cluster_indices:\n            batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]\n            # filter our the shorter batches\n            batches = [_ for _ in batches if len(_) == self.batch_size]\n            if self.shuffle:\n                random.shuffle(batches)\n            batch_lists.append(batches)       \n        \n        # flatten lists and shuffle the batches if necessary\n        # this works on batch level\n        lst = self.flatten_list(batch_lists)\n        if self.shuffle:\n            random.shuffle(lst)\n        # final flatten  - produce flat list of indexes\n        lst = self.flatten_list(lst)        \n        return iter(lst)\n\n    def __len__(self):\n        return len(self.data_source)\n\nUsage\nsampler = ClusterRandomSampler(train_dataset,512,True)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=512,        \n    sampler=sampler,\n    shuffle=False,\n    num_workers=1,\n    pin_memory=False,\n    drop_last=False)", "body": "**Hi guys, I faced a similar issue and found an elegant and ingenious solution!**\r\n\r\nWhat if I have a multi-head model (in my case it is **several image sizes and aspect ratios**), and I would like to learn shared representations, but do not want to iterate over several datasets (in my case it caused sequential over-fitting and poorer performance)?\r\nYeah, ConcatDataset does not help either in this case (it can work w/o shuffling and if your dataset exactly divides by batch-size, but it's not a proper solution).\r\n\r\n**You see, as far a I understand, PyTorch Dataset/Dataloader abstractions work the following way:**\r\n- First you define an abstract [Dataset ](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py) with `len()` and `getitem()` methods;\r\n- Then you define a sampling [strategy ](https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html)(usually people just opt for shuffle=True in 95% of cases);\r\n- Then you just use this strategy with the [Dataloader](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py);\r\n\r\nYou see, the gist is contained in this [portion ](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L465-L491)of the Dataloader code!\r\n\r\nTranslated in plain English it goes something like this:\r\n- Has user re-defined standard batch-producing class? No?\r\n- Has use re-defined standard sampling class? No?\r\n- Then just shuffle randomly (or not) and use standard batch sampler\r\n\r\nIf you read the standard batch [sampler](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L136-L144) - you will see that it just sequentially samples from a sampling strategy and yields batches when they are full. As easy as that. So ... we can just re-write RandomSampler class a bit to support our multi-cluster behavior and we are good to go =)\r\n\r\n**My final logic is the following:**\r\n- Somewhere in your Dataset class define cluster_indices property which says which index refers to which cluster of data\r\n- Chunkify your data manually `batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]`\r\n- Drop the batches that are smaller than your batch size\r\n- Concatenate all of your cluster batch lists, flatter the list, shuffle the batches\r\n- Flatten list once again\r\n- Feed to the standard BatchSampler and observe the multi-threaded goodness of PyTorch Dataloader  =)\r\n\r\n**New class**\r\n```\r\nimport random\r\nfrom torch.utils.data.sampler import Sampler\r\n\r\nclass ClusterRandomSampler(Sampler):\r\n    r\"\"\"Takes a dataset with cluster_indices property, cuts it into batch-sized chunks\r\n    Drops the extra items, not fitting into exact batches\r\n    Arguments:\r\n        data_source (Dataset): a Dataset to sample from. Should have a cluster_indices property\r\n        batch_size (int): a batch size that you would like to use later with Dataloader class\r\n        shuffle (bool): whether to shuffle the data or not\r\n    \"\"\"\r\n\r\n    def __init__(self, data_source, batch_size, shuffle=True):\r\n        self.data_source = data_source\r\n        self.batch_size = batch_size\r\n        self.shuffle = shuffle\r\n        \r\n    def flatten_list(self, lst):\r\n        return [item for sublist in lst for item in sublist]\r\n\r\n    def __iter__(self):\r\n\r\n        batch_lists = []\r\n        for cluster_indices in train_dataset.cluster_indices:\r\n            batches = [cluster_indices[i:i + self.batch_size] for i in range(0, len(cluster_indices), self.batch_size)]\r\n            # filter our the shorter batches\r\n            batches = [_ for _ in batches if len(_) == self.batch_size]\r\n            if self.shuffle:\r\n                random.shuffle(batches)\r\n            batch_lists.append(batches)       \r\n        \r\n        # flatten lists and shuffle the batches if necessary\r\n        # this works on batch level\r\n        lst = self.flatten_list(batch_lists)\r\n        if self.shuffle:\r\n            random.shuffle(lst)\r\n        # final flatten  - produce flat list of indexes\r\n        lst = self.flatten_list(lst)        \r\n        return iter(lst)\r\n\r\n    def __len__(self):\r\n        return len(self.data_source)\r\n```\r\n\r\n**Usage**\r\n```\r\nsampler = ClusterRandomSampler(train_dataset,512,True)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    train_dataset,\r\n    batch_size=512,        \r\n    sampler=sampler,\r\n    shuffle=False,\r\n    num_workers=1,\r\n    pin_memory=False,\r\n    drop_last=False)\r\n```"}