{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343831891", "html_url": "https://github.com/pytorch/pytorch/issues/3662#issuecomment-343831891", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3662", "id": 343831891, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzgzMTg5MQ==", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T07:08:23Z", "updated_at": "2017-11-13T07:08:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think trailing zero vectors cause this.<br>\nIn the first case, the trailing zeros are fed to the reverse network before the real values are fed.<br>\nSince you're using a 2-layer bidirectional LSTM, the concatenation of the forward output and the backward output (whose values would be different in the two cases) of the first LSTM layer is passed as the second LSTM layer's input, and thus makes the difference.<br>\nIf you just use a single layer LSTM, <code>output1[0][2][1]</code> and <code>output2[0][2][1]</code> will be the same.</p>", "body_text": "I think trailing zero vectors cause this.\nIn the first case, the trailing zeros are fed to the reverse network before the real values are fed.\nSince you're using a 2-layer bidirectional LSTM, the concatenation of the forward output and the backward output (whose values would be different in the two cases) of the first LSTM layer is passed as the second LSTM layer's input, and thus makes the difference.\nIf you just use a single layer LSTM, output1[0][2][1] and output2[0][2][1] will be the same.", "body": "I think trailing zero vectors cause this.\r\nIn the first case, the trailing zeros are fed to the reverse network before the real values are fed.\r\nSince you're using a 2-layer bidirectional LSTM, the concatenation of the forward output and the backward output (whose values would be different in the two cases) of the first LSTM layer is passed as the second LSTM layer's input, and thus makes the difference.\r\nIf you just use a single layer LSTM, `output1[0][2][1]` and `output2[0][2][1]` will be the same."}