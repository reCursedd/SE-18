{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3662", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3662/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3662/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3662/events", "html_url": "https://github.com/pytorch/pytorch/issues/3662", "id": 273268064, "node_id": "MDU6SXNzdWUyNzMyNjgwNjQ=", "number": 3662, "title": "Unexpected results from the bidirectional LSTM", "user": {"login": "sheng-z", "id": 8541392, "node_id": "MDQ6VXNlcjg1NDEzOTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/8541392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sheng-z", "html_url": "https://github.com/sheng-z", "followers_url": "https://api.github.com/users/sheng-z/followers", "following_url": "https://api.github.com/users/sheng-z/following{/other_user}", "gists_url": "https://api.github.com/users/sheng-z/gists{/gist_id}", "starred_url": "https://api.github.com/users/sheng-z/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sheng-z/subscriptions", "organizations_url": "https://api.github.com/users/sheng-z/orgs", "repos_url": "https://api.github.com/users/sheng-z/repos", "events_url": "https://api.github.com/users/sheng-z/events{/privacy}", "received_events_url": "https://api.github.com/users/sheng-z/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-12T21:43:47Z", "updated_at": "2017-11-13T13:13:57Z", "closed_at": "2017-11-13T13:13:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, when I tested <code>pack_padded_sequence</code> in bidirectional LSTM, I surprisingly found that some values in the forward-pass output are not the same as the values I got when <code>pack_padded_sequence</code> is not used.<br>\nHere is the snippet I used:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.nn.utils.rnn <span class=\"pl-k\">import</span> pack_padded_sequence, pad_packed_sequence\n\ntorch.manual_seed(<span class=\"pl-c1\">7</span>)\nlstm <span class=\"pl-k\">=</span> torch.nn.LSTM(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nvec1 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>)\nvec2 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>)\nvec3 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>)\nbatch_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>)\nbatch_data[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> vec1\nbatch_data[<span class=\"pl-c1\">1</span>,:<span class=\"pl-c1\">5</span>] <span class=\"pl-k\">=</span> vec2\nbatch_data[<span class=\"pl-c1\">2</span>,:<span class=\"pl-c1\">3</span>] <span class=\"pl-k\">=</span> vec3\nbatch_data <span class=\"pl-k\">=</span> Variable(batch_data)\nlength_list <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>]\n\ninput1<span class=\"pl-k\">=</span> batch_data.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\noutput1, _ <span class=\"pl-k\">=</span> lstm(input1)\n<span class=\"pl-c1\">print</span> (output1)\n\ninput2 <span class=\"pl-k\">=</span> batch_data.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\ninput2 <span class=\"pl-k\">=</span> pack_padded_sequence(input2, [<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>])\noutput2, _ <span class=\"pl-k\">=</span> lstm(input2)\noutput2, _ <span class=\"pl-k\">=</span> pad_packed_sequence(output2)\n<span class=\"pl-c1\">print</span> (output2)</pre></div>\n<p>Here is the output:</p>\n<pre><code>Variable containing:\n(0 ,.,.) = \n -0.0535  0.0183 -0.1318 -0.1651  0.2408 -0.1308\n -0.0517  0.0391 -0.1486 -0.1147  0.2491 -0.1040\n -0.0551  0.0438 -0.1596 -0.1391  0.2550 -0.1024\n\n(1 ,.,.) = \n -0.0580  0.0514 -0.1555 -0.1506  0.2283 -0.1343\n -0.0509  0.0205 -0.2524 -0.1088  0.2454 -0.0765\n -0.0567  0.0642 -0.1850 -0.1468  0.2376 -0.1204\n\n(2 ,.,.) = \n -0.0565  0.0584 -0.2235 -0.1264  0.2172 -0.0929\n -0.0632  0.0522 -0.2529 -0.1272  0.2091 -0.0770\n -0.0541  0.0795 -0.1683 -0.1469  0.2124 -0.1291\n\n(3 ,.,.) = \n -0.0606  0.0412 -0.2019 -0.1358  0.1815 -0.0898\n -0.0558  0.0766 -0.2611 -0.1023  0.1985 -0.0708\n -0.0538  0.0904 -0.2353 -0.1197  0.1895 -0.0838\n\n(4 ,.,.) = \n -0.0616  0.0645 -0.2391 -0.1097  0.1471 -0.0659\n -0.0497  0.0771 -0.2751 -0.0912  0.1615 -0.0554\n -0.0523  0.0957 -0.2370 -0.1028  0.1517 -0.0668\n\n(5 ,.,.) = \n -0.0535  0.0638 -0.2357 -0.0722  0.0876 -0.0397\n -0.0512  0.0785 -0.2449 -0.0711  0.0904 -0.0386\n -0.0501  0.0960 -0.2376 -0.0696  0.0905 -0.0409\n[torch.FloatTensor of size 6x3x6]\n\nVariable containing:\n(0 ,.,.) = \n -0.0535  0.0183 -0.1318 -0.1651  0.2408 -0.1308\n -0.0517  0.0392 -0.1486 -0.1123  0.2388 -0.0997\n -0.0549  0.0435 -0.1586 -0.1181  0.2026 -0.0768\n\n(1 ,.,.) = \n -0.0580  0.0514 -0.1555 -0.1506  0.2283 -0.1343\n -0.0509  0.0205 -0.2522 -0.1042  0.2301 -0.0712\n -0.0565  0.0635 -0.1836 -0.1111  0.1552 -0.0771\n\n(2 ,.,.) = \n -0.0565  0.0584 -0.2235 -0.1264  0.2172 -0.0929\n -0.0627  0.0521 -0.2518 -0.1189  0.1867 -0.0692\n -0.0537  0.0772 -0.1648 -0.0828  0.0868 -0.0582\n\n(3 ,.,.) = \n -0.0606  0.0412 -0.2019 -0.1358  0.1815 -0.0898\n -0.0551  0.0762 -0.2591 -0.0877  0.1593 -0.0558\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\n(4 ,.,.) = \n -0.0616  0.0645 -0.2391 -0.1097  0.1471 -0.0659\n -0.0491  0.0746 -0.2700 -0.0603  0.0969 -0.0325\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\n(5 ,.,.) = \n -0.0535  0.0638 -0.2357 -0.0722  0.0876 -0.0397\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 6x3x6]\n</code></pre>\n<p>Apparently <code>output1[0][2][1]</code> should be the same as <code>output2[0][2][1]</code>, but they are not. This only happens when <code>bidirectional=True</code>.</p>", "body_text": "Hi, when I tested pack_padded_sequence in bidirectional LSTM, I surprisingly found that some values in the forward-pass output are not the same as the values I got when pack_padded_sequence is not used.\nHere is the snippet I used:\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\ntorch.manual_seed(7)\nlstm = torch.nn.LSTM(3, 3, 2, bidirectional=True)\nvec1 = torch.randn(6, 3)\nvec2 = torch.randn(5, 3)\nvec3 = torch.randn(3, 3)\nbatch_data = torch.zeros(3, 6, 3)\nbatch_data[0] = vec1\nbatch_data[1,:5] = vec2\nbatch_data[2,:3] = vec3\nbatch_data = Variable(batch_data)\nlength_list = [6, 3, 5]\n\ninput1= batch_data.transpose(0, 1)\noutput1, _ = lstm(input1)\nprint (output1)\n\ninput2 = batch_data.transpose(0, 1)\ninput2 = pack_padded_sequence(input2, [6, 5, 3])\noutput2, _ = lstm(input2)\noutput2, _ = pad_packed_sequence(output2)\nprint (output2)\nHere is the output:\nVariable containing:\n(0 ,.,.) = \n -0.0535  0.0183 -0.1318 -0.1651  0.2408 -0.1308\n -0.0517  0.0391 -0.1486 -0.1147  0.2491 -0.1040\n -0.0551  0.0438 -0.1596 -0.1391  0.2550 -0.1024\n\n(1 ,.,.) = \n -0.0580  0.0514 -0.1555 -0.1506  0.2283 -0.1343\n -0.0509  0.0205 -0.2524 -0.1088  0.2454 -0.0765\n -0.0567  0.0642 -0.1850 -0.1468  0.2376 -0.1204\n\n(2 ,.,.) = \n -0.0565  0.0584 -0.2235 -0.1264  0.2172 -0.0929\n -0.0632  0.0522 -0.2529 -0.1272  0.2091 -0.0770\n -0.0541  0.0795 -0.1683 -0.1469  0.2124 -0.1291\n\n(3 ,.,.) = \n -0.0606  0.0412 -0.2019 -0.1358  0.1815 -0.0898\n -0.0558  0.0766 -0.2611 -0.1023  0.1985 -0.0708\n -0.0538  0.0904 -0.2353 -0.1197  0.1895 -0.0838\n\n(4 ,.,.) = \n -0.0616  0.0645 -0.2391 -0.1097  0.1471 -0.0659\n -0.0497  0.0771 -0.2751 -0.0912  0.1615 -0.0554\n -0.0523  0.0957 -0.2370 -0.1028  0.1517 -0.0668\n\n(5 ,.,.) = \n -0.0535  0.0638 -0.2357 -0.0722  0.0876 -0.0397\n -0.0512  0.0785 -0.2449 -0.0711  0.0904 -0.0386\n -0.0501  0.0960 -0.2376 -0.0696  0.0905 -0.0409\n[torch.FloatTensor of size 6x3x6]\n\nVariable containing:\n(0 ,.,.) = \n -0.0535  0.0183 -0.1318 -0.1651  0.2408 -0.1308\n -0.0517  0.0392 -0.1486 -0.1123  0.2388 -0.0997\n -0.0549  0.0435 -0.1586 -0.1181  0.2026 -0.0768\n\n(1 ,.,.) = \n -0.0580  0.0514 -0.1555 -0.1506  0.2283 -0.1343\n -0.0509  0.0205 -0.2522 -0.1042  0.2301 -0.0712\n -0.0565  0.0635 -0.1836 -0.1111  0.1552 -0.0771\n\n(2 ,.,.) = \n -0.0565  0.0584 -0.2235 -0.1264  0.2172 -0.0929\n -0.0627  0.0521 -0.2518 -0.1189  0.1867 -0.0692\n -0.0537  0.0772 -0.1648 -0.0828  0.0868 -0.0582\n\n(3 ,.,.) = \n -0.0606  0.0412 -0.2019 -0.1358  0.1815 -0.0898\n -0.0551  0.0762 -0.2591 -0.0877  0.1593 -0.0558\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\n(4 ,.,.) = \n -0.0616  0.0645 -0.2391 -0.1097  0.1471 -0.0659\n -0.0491  0.0746 -0.2700 -0.0603  0.0969 -0.0325\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n\n(5 ,.,.) = \n -0.0535  0.0638 -0.2357 -0.0722  0.0876 -0.0397\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 6x3x6]\n\nApparently output1[0][2][1] should be the same as output2[0][2][1], but they are not. This only happens when bidirectional=True.", "body": "Hi, when I tested `pack_padded_sequence` in bidirectional LSTM, I surprisingly found that some values in the forward-pass output are not the same as the values I got when `pack_padded_sequence` is not used.\r\nHere is the snippet I used:\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n\r\ntorch.manual_seed(7)\r\nlstm = torch.nn.LSTM(3, 3, 2, bidirectional=True)\r\nvec1 = torch.randn(6, 3)\r\nvec2 = torch.randn(5, 3)\r\nvec3 = torch.randn(3, 3)\r\nbatch_data = torch.zeros(3, 6, 3)\r\nbatch_data[0] = vec1\r\nbatch_data[1,:5] = vec2\r\nbatch_data[2,:3] = vec3\r\nbatch_data = Variable(batch_data)\r\nlength_list = [6, 3, 5]\r\n\r\ninput1= batch_data.transpose(0, 1)\r\noutput1, _ = lstm(input1)\r\nprint (output1)\r\n\r\ninput2 = batch_data.transpose(0, 1)\r\ninput2 = pack_padded_sequence(input2, [6, 5, 3])\r\noutput2, _ = lstm(input2)\r\noutput2, _ = pad_packed_sequence(output2)\r\nprint (output2)\r\n```   \r\nHere is the output:\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.0535  0.0183 -0.1318 -0.1651  0.2408 -0.1308\r\n -0.0517  0.0391 -0.1486 -0.1147  0.2491 -0.1040\r\n -0.0551  0.0438 -0.1596 -0.1391  0.2550 -0.1024\r\n\r\n(1 ,.,.) = \r\n -0.0580  0.0514 -0.1555 -0.1506  0.2283 -0.1343\r\n -0.0509  0.0205 -0.2524 -0.1088  0.2454 -0.0765\r\n -0.0567  0.0642 -0.1850 -0.1468  0.2376 -0.1204\r\n\r\n(2 ,.,.) = \r\n -0.0565  0.0584 -0.2235 -0.1264  0.2172 -0.0929\r\n -0.0632  0.0522 -0.2529 -0.1272  0.2091 -0.0770\r\n -0.0541  0.0795 -0.1683 -0.1469  0.2124 -0.1291\r\n\r\n(3 ,.,.) = \r\n -0.0606  0.0412 -0.2019 -0.1358  0.1815 -0.0898\r\n -0.0558  0.0766 -0.2611 -0.1023  0.1985 -0.0708\r\n -0.0538  0.0904 -0.2353 -0.1197  0.1895 -0.0838\r\n\r\n(4 ,.,.) = \r\n -0.0616  0.0645 -0.2391 -0.1097  0.1471 -0.0659\r\n -0.0497  0.0771 -0.2751 -0.0912  0.1615 -0.0554\r\n -0.0523  0.0957 -0.2370 -0.1028  0.1517 -0.0668\r\n\r\n(5 ,.,.) = \r\n -0.0535  0.0638 -0.2357 -0.0722  0.0876 -0.0397\r\n -0.0512  0.0785 -0.2449 -0.0711  0.0904 -0.0386\r\n -0.0501  0.0960 -0.2376 -0.0696  0.0905 -0.0409\r\n[torch.FloatTensor of size 6x3x6]\r\n\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.0535  0.0183 -0.1318 -0.1651  0.2408 -0.1308\r\n -0.0517  0.0392 -0.1486 -0.1123  0.2388 -0.0997\r\n -0.0549  0.0435 -0.1586 -0.1181  0.2026 -0.0768\r\n\r\n(1 ,.,.) = \r\n -0.0580  0.0514 -0.1555 -0.1506  0.2283 -0.1343\r\n -0.0509  0.0205 -0.2522 -0.1042  0.2301 -0.0712\r\n -0.0565  0.0635 -0.1836 -0.1111  0.1552 -0.0771\r\n\r\n(2 ,.,.) = \r\n -0.0565  0.0584 -0.2235 -0.1264  0.2172 -0.0929\r\n -0.0627  0.0521 -0.2518 -0.1189  0.1867 -0.0692\r\n -0.0537  0.0772 -0.1648 -0.0828  0.0868 -0.0582\r\n\r\n(3 ,.,.) = \r\n -0.0606  0.0412 -0.2019 -0.1358  0.1815 -0.0898\r\n -0.0551  0.0762 -0.2591 -0.0877  0.1593 -0.0558\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\r\n\r\n(4 ,.,.) = \r\n -0.0616  0.0645 -0.2391 -0.1097  0.1471 -0.0659\r\n -0.0491  0.0746 -0.2700 -0.0603  0.0969 -0.0325\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\r\n\r\n(5 ,.,.) = \r\n -0.0535  0.0638 -0.2357 -0.0722  0.0876 -0.0397\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\r\n  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\r\n[torch.FloatTensor of size 6x3x6]\r\n```\r\n\r\nApparently `output1[0][2][1]` should be the same as `output2[0][2][1]`, but they are not. This only happens when `bidirectional=True`. "}