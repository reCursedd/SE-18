{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397716940", "html_url": "https://github.com/pytorch/pytorch/issues/8499#issuecomment-397716940", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8499", "id": 397716940, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzcxNjk0MA==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-15T19:14:12Z", "updated_at": "2018-06-15T19:16:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think the problem is that the script schema for addbmm doesn't take into account the other legacy schemas. For example,</p>\n<pre><code>import torch\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(x, w, 1.1, 0.9)\n\nb = torch.randn(2, 2)\nx = torch.randn(1, 2, 2)\nw = torch.randn(1, 2, 2)\n\n# The below three are all equivalent\nb.addbmm(x, w, beta=1.1, alpha=0.9)\naddbmm_script_test(b, x, w)\nb.addbmm(1.1, 0.9, x, w)\n</code></pre>\n<p>This happens because (in aten_schema), we only have<br>\n<code>addbmm(Tensor self, Tensor batch1, Tensor batch2, Tensor beta=&lt;default&gt;, Tensor alpha=&lt;devault&gt;)</code></p>\n<p>But in generated/python_variable_methods.cpp, python_arg_parser uses the following three schemas (in order):</p>\n<pre><code>addbmm(Scalar beta, Scalar alpha, Tensor batch1, Tensor batch2)|deprecated\"\naddbmm(Scalar beta, Tensor batch1, Tensor batch2)|deprecated\",\naddbmm(Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1)\",\n</code></pre>\n<p>So when we write</p>\n<pre><code>@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\n</code></pre>\n<p>it's interpreting <code>x</code> as beta, <code>w</code> as alpha, 1.1 as batch1, and 0.9 as batch2.</p>\n<p>if/when we deprecate the ones that need deprecating, everything will be okay?</p>", "body_text": "I think the problem is that the script schema for addbmm doesn't take into account the other legacy schemas. For example,\nimport torch\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(x, w, 1.1, 0.9)\n\nb = torch.randn(2, 2)\nx = torch.randn(1, 2, 2)\nw = torch.randn(1, 2, 2)\n\n# The below three are all equivalent\nb.addbmm(x, w, beta=1.1, alpha=0.9)\naddbmm_script_test(b, x, w)\nb.addbmm(1.1, 0.9, x, w)\n\nThis happens because (in aten_schema), we only have\naddbmm(Tensor self, Tensor batch1, Tensor batch2, Tensor beta=<default>, Tensor alpha=<devault>)\nBut in generated/python_variable_methods.cpp, python_arg_parser uses the following three schemas (in order):\naddbmm(Scalar beta, Scalar alpha, Tensor batch1, Tensor batch2)|deprecated\"\naddbmm(Scalar beta, Tensor batch1, Tensor batch2)|deprecated\",\naddbmm(Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1)\",\n\nSo when we write\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\n\nit's interpreting x as beta, w as alpha, 1.1 as batch1, and 0.9 as batch2.\nif/when we deprecate the ones that need deprecating, everything will be okay?", "body": "I think the problem is that the script schema for addbmm doesn't take into account the other legacy schemas. For example, \r\n\r\n```\r\nimport torch\r\n@torch.jit.script\r\ndef addbmm_script_test(b, x, w):\r\n    return b.addbmm(x, w, 1.1, 0.9)\r\n\r\nb = torch.randn(2, 2)\r\nx = torch.randn(1, 2, 2)\r\nw = torch.randn(1, 2, 2)\r\n\r\n# The below three are all equivalent\r\nb.addbmm(x, w, beta=1.1, alpha=0.9)\r\naddbmm_script_test(b, x, w)\r\nb.addbmm(1.1, 0.9, x, w)\r\n```\r\n\r\nThis happens because (in aten_schema), we only have\r\n`addbmm(Tensor self, Tensor batch1, Tensor batch2, Tensor beta=<default>, Tensor alpha=<devault>)`\r\n\r\nBut in generated/python_variable_methods.cpp, python_arg_parser uses the following three schemas (in order):\r\n```\r\naddbmm(Scalar beta, Scalar alpha, Tensor batch1, Tensor batch2)|deprecated\"\r\naddbmm(Scalar beta, Tensor batch1, Tensor batch2)|deprecated\",\r\naddbmm(Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1)\",\r\n```\r\n\r\nSo when we write\r\n```\r\n@torch.jit.script\r\ndef addbmm_script_test(b, x, w):\r\n    return b.addbmm(1.1, 0.9, x, w)\r\n```\r\nit's interpreting `x` as beta, `w` as alpha, 1.1 as batch1, and 0.9 as batch2.\r\n\r\nif/when we deprecate the ones that need deprecating, everything will be okay?\r\n"}