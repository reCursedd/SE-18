{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397714945", "html_url": "https://github.com/pytorch/pytorch/issues/8499#issuecomment-397714945", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8499", "id": 397714945, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzcxNDk0NQ==", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-15T19:05:36Z", "updated_at": "2018-06-15T19:12:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> I was a bit confused as well. Here's a quick demo i wrote:</p>\n<pre><code>import torch\n\n# First try to run an addmm broadcasting a scalar accumulation\n\nb = torch.rand(())\nx = torch.rand(3, 4)\nw = torch.rand(4, 5)\nbeta = 1.1\nalpha = 0.9\n\n# Python: works normally\nprint('=====Python addmm=====\\n', b.addmm(beta, alpha, x, w))\n\n# Tracing: works because of an expand that was inserted\n@torch.jit.trace(b, x, w)\ndef addmm_trace_test(b, x, w):\n    return b.addmm(beta, alpha, x, w)\nprint('=====Tracing addmm=====\\n', addmm_trace_test(b, x, w))\nprint('=====Tracing addmm graph=====\\n', addmm_trace_test.graph)\n\n# Scripting: fails because we don't have the expand inserted and shape prop\n# does not know how to handle addmm with coefficients\n\n@torch.jit.script\ndef addmm_script_test(b, x, w):\n    return b.addmm(1.1, 0.9, x, w)\ntry:\n    print('=====Scripting addmm=====\\n', addmm_script_test(b, x, w))\nexcept RuntimeError as e:\n    print('=====Scripting addmm=====\\nwe failed with exception', e)\n\n# Scripting: if we write out the op in the long form it works\n\n@torch.jit.script\ndef addmm_script_test_manual(b, x, w):\n    return 1.1 * b + 0.9 * x.mm(w)\n\nprint('=====Scripting addmm manual=====\\n', addmm_script_test_manual(b, x, w))\n\n\n# Same thing with addbmm, except now we have a reduction\n\nx = torch.rand(5, 3, 4)\nw = torch.rand(5, 4, 5)\n\n@torch.jit.trace(b, x, w)\ndef addbmm_tracing_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\n\nprint('=====Tracing addbmm graph=====\\n', addbmm_tracing_test.graph)\n\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\ntry:\n    print('=====Scripting addbmm=====\\n', addbmm_script_test(b, x, w))\nexcept RuntimeError as e:\n    print('=====Scripting addbmm=====\\nwe failed with exception', e)\n\n@torch.jit.script\ndef addbmm_script_test_manual(b, x, w):\n    return 1.1 * b + torch.sum(0.9 * x.bmm(w), dim=0)\n\nprint('=====Scripting addmm manual=====\\n', addbmm_script_test_manual(b, x, w))\nimport numpy as np\nnp.testing.assert_allclose(addbmm_tracing_test(b, x, w), addbmm_script_test_manual(b, x, w), atol=1e-5)\n</code></pre>\n<p>With output:</p>\n<pre><code>=====Python addmm=====\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\n=====Tracing addmm=====\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\n=====Tracing addmm graph=====\n graph(%0 : Float()\n      %1 : Float(3, 4)\n      %2 : Float(4, 5)) {\n  %3 : Float(3, 5!) = aten::expand[size=[3, 5], implicit=1](%0)\n  %4 : Float(3, 5) = aten::addmm[beta={1.1}, alpha={0.9}](%3, %1, %2)\n  return (%4);\n}\n\n=====Scripting addmm=====\nwe failed with exception \ninvalid argument 2: size '[1]' is invalid for input with 12 elements at ../aten/src/TH/THStorage.cpp:59:\noperation failed shape propagation:\n@torch.jit.script\ndef addmm_script_test(b, x, w):\n    return b.addmm(1.1, 0.9, x, w)\n           ~~~~~~~ &lt;--- HERE\n\n=====Scripting addmm manual=====\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\n=====Tracing addbmm graph=====\n graph(%0 : Float()\n      %1 : Float(5, 3, 4)\n      %2 : Float(5, 4, 5)) {\n  %3 : Float(3, 5!) = aten::expand[size=[3, 5], implicit=1](%0)\n  %4 : Float(3, 5) = aten::addbmm[beta={1.1}, alpha={0.9}](%3, %1, %2)\n  return (%4);\n}\n\n=====Scripting addbmm=====\nwe failed with exception \ninvalid argument 2: size '[1]' is invalid for input with 60 elements at ../aten/src/TH/THStorage.cpp:59:\noperation failed shape propagation:\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\n           ~~~~~~~~ &lt;--- HERE\n\n=====Scripting addmm manual=====\n tensor([[4.3806, 5.8912, 5.1491, 5.0702, 4.2510],\n        [3.6309, 4.8444, 4.5171, 4.2323, 3.2367],\n        [3.0597, 3.4274, 3.2279, 2.9622, 2.7127]])\n</code></pre>\n<p>So indeed those scalar arguments are getting correctly interpreted as the <code>alpha</code> and <code>beta</code> attributes. It's just that when running in Python (+scripting) there's an <code>expand</code> inserted on the accumulation input, but that's absent in shape propagation. Solution 2 above is basically:</p>\n<ol>\n<li>Break down all these CISC-y fused operations into smaller RISC-y operations (addmm =&gt; add + mm. addbmm =&gt; add + sum(mm).</li>\n<li>Fuse them back together after shape propagation and differentiation etc. Those passes already understand things like add, sum, mm, so they should work properly. Then later we just fuse all those patterns back together into the larger operations. The patterns will be more complicated than in the addmm case without alpha and beta (which is just a peephole). But this will probably prove to be more general and minimize the number of explicit autodiff and shape prop formulae we have to write. Also it'll fuse the cases in which the user explicitly wrote these ops, which is nice</li>\n</ol>\n<p>Note that more complexity is introduced into this proposal when we have alpha and beta coefficients</p>", "body_text": "@zou3519 I was a bit confused as well. Here's a quick demo i wrote:\nimport torch\n\n# First try to run an addmm broadcasting a scalar accumulation\n\nb = torch.rand(())\nx = torch.rand(3, 4)\nw = torch.rand(4, 5)\nbeta = 1.1\nalpha = 0.9\n\n# Python: works normally\nprint('=====Python addmm=====\\n', b.addmm(beta, alpha, x, w))\n\n# Tracing: works because of an expand that was inserted\n@torch.jit.trace(b, x, w)\ndef addmm_trace_test(b, x, w):\n    return b.addmm(beta, alpha, x, w)\nprint('=====Tracing addmm=====\\n', addmm_trace_test(b, x, w))\nprint('=====Tracing addmm graph=====\\n', addmm_trace_test.graph)\n\n# Scripting: fails because we don't have the expand inserted and shape prop\n# does not know how to handle addmm with coefficients\n\n@torch.jit.script\ndef addmm_script_test(b, x, w):\n    return b.addmm(1.1, 0.9, x, w)\ntry:\n    print('=====Scripting addmm=====\\n', addmm_script_test(b, x, w))\nexcept RuntimeError as e:\n    print('=====Scripting addmm=====\\nwe failed with exception', e)\n\n# Scripting: if we write out the op in the long form it works\n\n@torch.jit.script\ndef addmm_script_test_manual(b, x, w):\n    return 1.1 * b + 0.9 * x.mm(w)\n\nprint('=====Scripting addmm manual=====\\n', addmm_script_test_manual(b, x, w))\n\n\n# Same thing with addbmm, except now we have a reduction\n\nx = torch.rand(5, 3, 4)\nw = torch.rand(5, 4, 5)\n\n@torch.jit.trace(b, x, w)\ndef addbmm_tracing_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\n\nprint('=====Tracing addbmm graph=====\\n', addbmm_tracing_test.graph)\n\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\ntry:\n    print('=====Scripting addbmm=====\\n', addbmm_script_test(b, x, w))\nexcept RuntimeError as e:\n    print('=====Scripting addbmm=====\\nwe failed with exception', e)\n\n@torch.jit.script\ndef addbmm_script_test_manual(b, x, w):\n    return 1.1 * b + torch.sum(0.9 * x.bmm(w), dim=0)\n\nprint('=====Scripting addmm manual=====\\n', addbmm_script_test_manual(b, x, w))\nimport numpy as np\nnp.testing.assert_allclose(addbmm_tracing_test(b, x, w), addbmm_script_test_manual(b, x, w), atol=1e-5)\n\nWith output:\n=====Python addmm=====\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\n=====Tracing addmm=====\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\n=====Tracing addmm graph=====\n graph(%0 : Float()\n      %1 : Float(3, 4)\n      %2 : Float(4, 5)) {\n  %3 : Float(3, 5!) = aten::expand[size=[3, 5], implicit=1](%0)\n  %4 : Float(3, 5) = aten::addmm[beta={1.1}, alpha={0.9}](%3, %1, %2)\n  return (%4);\n}\n\n=====Scripting addmm=====\nwe failed with exception \ninvalid argument 2: size '[1]' is invalid for input with 12 elements at ../aten/src/TH/THStorage.cpp:59:\noperation failed shape propagation:\n@torch.jit.script\ndef addmm_script_test(b, x, w):\n    return b.addmm(1.1, 0.9, x, w)\n           ~~~~~~~ <--- HERE\n\n=====Scripting addmm manual=====\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\n=====Tracing addbmm graph=====\n graph(%0 : Float()\n      %1 : Float(5, 3, 4)\n      %2 : Float(5, 4, 5)) {\n  %3 : Float(3, 5!) = aten::expand[size=[3, 5], implicit=1](%0)\n  %4 : Float(3, 5) = aten::addbmm[beta={1.1}, alpha={0.9}](%3, %1, %2)\n  return (%4);\n}\n\n=====Scripting addbmm=====\nwe failed with exception \ninvalid argument 2: size '[1]' is invalid for input with 60 elements at ../aten/src/TH/THStorage.cpp:59:\noperation failed shape propagation:\n@torch.jit.script\ndef addbmm_script_test(b, x, w):\n    return b.addbmm(1.1, 0.9, x, w)\n           ~~~~~~~~ <--- HERE\n\n=====Scripting addmm manual=====\n tensor([[4.3806, 5.8912, 5.1491, 5.0702, 4.2510],\n        [3.6309, 4.8444, 4.5171, 4.2323, 3.2367],\n        [3.0597, 3.4274, 3.2279, 2.9622, 2.7127]])\n\nSo indeed those scalar arguments are getting correctly interpreted as the alpha and beta attributes. It's just that when running in Python (+scripting) there's an expand inserted on the accumulation input, but that's absent in shape propagation. Solution 2 above is basically:\n\nBreak down all these CISC-y fused operations into smaller RISC-y operations (addmm => add + mm. addbmm => add + sum(mm).\nFuse them back together after shape propagation and differentiation etc. Those passes already understand things like add, sum, mm, so they should work properly. Then later we just fuse all those patterns back together into the larger operations. The patterns will be more complicated than in the addmm case without alpha and beta (which is just a peephole). But this will probably prove to be more general and minimize the number of explicit autodiff and shape prop formulae we have to write. Also it'll fuse the cases in which the user explicitly wrote these ops, which is nice\n\nNote that more complexity is introduced into this proposal when we have alpha and beta coefficients", "body": "@zou3519 I was a bit confused as well. Here's a quick demo i wrote:\r\n\r\n```\r\nimport torch\r\n\r\n# First try to run an addmm broadcasting a scalar accumulation\r\n\r\nb = torch.rand(())\r\nx = torch.rand(3, 4)\r\nw = torch.rand(4, 5)\r\nbeta = 1.1\r\nalpha = 0.9\r\n\r\n# Python: works normally\r\nprint('=====Python addmm=====\\n', b.addmm(beta, alpha, x, w))\r\n\r\n# Tracing: works because of an expand that was inserted\r\n@torch.jit.trace(b, x, w)\r\ndef addmm_trace_test(b, x, w):\r\n    return b.addmm(beta, alpha, x, w)\r\nprint('=====Tracing addmm=====\\n', addmm_trace_test(b, x, w))\r\nprint('=====Tracing addmm graph=====\\n', addmm_trace_test.graph)\r\n\r\n# Scripting: fails because we don't have the expand inserted and shape prop\r\n# does not know how to handle addmm with coefficients\r\n\r\n@torch.jit.script\r\ndef addmm_script_test(b, x, w):\r\n    return b.addmm(1.1, 0.9, x, w)\r\ntry:\r\n    print('=====Scripting addmm=====\\n', addmm_script_test(b, x, w))\r\nexcept RuntimeError as e:\r\n    print('=====Scripting addmm=====\\nwe failed with exception', e)\r\n\r\n# Scripting: if we write out the op in the long form it works\r\n\r\n@torch.jit.script\r\ndef addmm_script_test_manual(b, x, w):\r\n    return 1.1 * b + 0.9 * x.mm(w)\r\n\r\nprint('=====Scripting addmm manual=====\\n', addmm_script_test_manual(b, x, w))\r\n\r\n\r\n# Same thing with addbmm, except now we have a reduction\r\n\r\nx = torch.rand(5, 3, 4)\r\nw = torch.rand(5, 4, 5)\r\n\r\n@torch.jit.trace(b, x, w)\r\ndef addbmm_tracing_test(b, x, w):\r\n    return b.addbmm(1.1, 0.9, x, w)\r\n\r\nprint('=====Tracing addbmm graph=====\\n', addbmm_tracing_test.graph)\r\n\r\n@torch.jit.script\r\ndef addbmm_script_test(b, x, w):\r\n    return b.addbmm(1.1, 0.9, x, w)\r\ntry:\r\n    print('=====Scripting addbmm=====\\n', addbmm_script_test(b, x, w))\r\nexcept RuntimeError as e:\r\n    print('=====Scripting addbmm=====\\nwe failed with exception', e)\r\n\r\n@torch.jit.script\r\ndef addbmm_script_test_manual(b, x, w):\r\n    return 1.1 * b + torch.sum(0.9 * x.bmm(w), dim=0)\r\n\r\nprint('=====Scripting addmm manual=====\\n', addbmm_script_test_manual(b, x, w))\r\nimport numpy as np\r\nnp.testing.assert_allclose(addbmm_tracing_test(b, x, w), addbmm_script_test_manual(b, x, w), atol=1e-5)\r\n```\r\n\r\nWith output:\r\n\r\n```\r\n=====Python addmm=====\r\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\r\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\r\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\r\n=====Tracing addmm=====\r\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\r\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\r\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\r\n=====Tracing addmm graph=====\r\n graph(%0 : Float()\r\n      %1 : Float(3, 4)\r\n      %2 : Float(4, 5)) {\r\n  %3 : Float(3, 5!) = aten::expand[size=[3, 5], implicit=1](%0)\r\n  %4 : Float(3, 5) = aten::addmm[beta={1.1}, alpha={0.9}](%3, %1, %2)\r\n  return (%4);\r\n}\r\n\r\n=====Scripting addmm=====\r\nwe failed with exception \r\ninvalid argument 2: size '[1]' is invalid for input with 12 elements at ../aten/src/TH/THStorage.cpp:59:\r\noperation failed shape propagation:\r\n@torch.jit.script\r\ndef addmm_script_test(b, x, w):\r\n    return b.addmm(1.1, 0.9, x, w)\r\n           ~~~~~~~ <--- HERE\r\n\r\n=====Scripting addmm manual=====\r\n tensor([[0.9177, 1.1336, 0.6198, 0.7214, 1.2803],\r\n        [0.5353, 0.6543, 0.7980, 0.7954, 1.0461],\r\n        [0.7949, 1.2400, 1.0233, 1.1356, 1.7397]])\r\n=====Tracing addbmm graph=====\r\n graph(%0 : Float()\r\n      %1 : Float(5, 3, 4)\r\n      %2 : Float(5, 4, 5)) {\r\n  %3 : Float(3, 5!) = aten::expand[size=[3, 5], implicit=1](%0)\r\n  %4 : Float(3, 5) = aten::addbmm[beta={1.1}, alpha={0.9}](%3, %1, %2)\r\n  return (%4);\r\n}\r\n\r\n=====Scripting addbmm=====\r\nwe failed with exception \r\ninvalid argument 2: size '[1]' is invalid for input with 60 elements at ../aten/src/TH/THStorage.cpp:59:\r\noperation failed shape propagation:\r\n@torch.jit.script\r\ndef addbmm_script_test(b, x, w):\r\n    return b.addbmm(1.1, 0.9, x, w)\r\n           ~~~~~~~~ <--- HERE\r\n\r\n=====Scripting addmm manual=====\r\n tensor([[4.3806, 5.8912, 5.1491, 5.0702, 4.2510],\r\n        [3.6309, 4.8444, 4.5171, 4.2323, 3.2367],\r\n        [3.0597, 3.4274, 3.2279, 2.9622, 2.7127]])\r\n```\r\n\r\nSo indeed those scalar arguments are getting correctly interpreted as the `alpha` and `beta` attributes. It's just that when running in Python (+scripting) there's an `expand` inserted on the accumulation input, but that's absent in shape propagation. Solution 2 above is basically:\r\n\r\n1. Break down all these CISC-y fused operations into smaller RISC-y operations (addmm => add + mm. addbmm => add + sum(mm).\r\n2. Fuse them back together after shape propagation and differentiation etc. Those passes already understand things like add, sum, mm, so they should work properly. Then later we just fuse all those patterns back together into the larger operations. The patterns will be more complicated than in the addmm case without alpha and beta (which is just a peephole). But this will probably prove to be more general and minimize the number of explicit autodiff and shape prop formulae we have to write. Also it'll fuse the cases in which the user explicitly wrote these ops, which is nice\r\n\r\nNote that more complexity is introduced into this proposal when we have alpha and beta coefficients"}