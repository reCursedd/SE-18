{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148915222", "pull_request_review_id": 74236661, "id": 148915222, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODkxNTIyMg==", "diff_hunk": "@@ -0,0 +1,74 @@\n+import torch\n+from torch.autograd import Variable\n+\n+from .module import Module\n+from .utils import _single, _pair, _triple\n+from .._functions.thnn.fold import Col2Im\n+\n+\n+class Fold(Module):\n+    \"\"\"\n+\n+    TODO: Finish this docstring...\n+\n+    Reshapes the input tensor into shape :math:`[N] + output_size` by\n+    de-interleaving vectors of length :math:`\\prod(kernel_size)` from the batch\n+    dimension and placing them into the output tensor via a sliding\n+    :math:`kernel_size` block of spatial dimensions.\n+\n+    The output has :math:`N` rows and :math:`output_size` spatial dimensions.\n+\n+    | If :attr:`padding` is non-zero, then the input is implicitly\n+    zero-padded on both sides by :attr:`padding` number of points\n+    | :attr:`dilation` controls the spacing between the kernel points.\n+    It is harder to describe, but this `link`_ has a nice visualization of what\n+    dilation does.\n+\n+    Args:\n+        kernel_size (int or tuple): the size of the sliding blocks to convert\n+                                    to columns. Default: 1\n+        stride (int or tuple): the stride of the sliding blocks in the input\n+                               spatial dimensions. Default: 1\n+        padding (int or tuple, optional): implicit zero padding to be added on\n+                                          both sides of input. Default: 0\n+        dilation (int or tuple, optional): a parameter that controls the\n+                                           stride of elements within the\n+                                           neighborhood. Default: 1\n+\n+    Shape:\n+        - Input: :math:`(N, C, L_{in})`\n+        - Output: :math:`(N * C * \\prod(kernel_size), L_{out},)` where\n+          :math:`L_{out} = floor((L_{in} + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1)\n+\n+    Examples::\n+\n+        >>> # kernel_size (2, 2), dilation (1, 1), padding (0, 0), stride (1, 1)\n+        >>> unfold = nn.Unfold((2, 2), (1, 1), (0, 0), (1, 1))\n+        >>> input = autograd.Variable(torch.randn(1, 256, 256))\n+        >>> output = unfold(input)\n+\n+    .. _link:\n+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n+\n+    \"\"\"\n+\n+    def __init__(self, output_size, kernel_size, dilation, padding, stride):\n+        super(Fold, self).__init__()\n+        self.output_size = output_size\n+        self.kernel_size = kernel_size\n+        self.dilation = dilation\n+        self.padding = padding\n+        self.stride = stride\n+\n+    def forward(self, input):\n+        return Col2Im.apply(input, self.output_size, self.kernel_size, self.dilation,", "path": "torch/nn/modules/fold.py", "position": null, "original_position": 64, "commit_id": "529d931a13ffc07d57b0da544adf5eef96a501d8", "original_commit_id": "217e47622aae90e32e000470926271b5243666c1", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "body": "This should call the `functional` version, which will handle all dimensions (see `upsample` in `functional.py` for an example):\r\n```python\r\ndef forward(self, input):\r\n    return F.fold(input, self.output_size, self.kernel_size, self.dilation, self.padding, self.stride)\r\n```", "created_at": "2017-11-04T00:01:09Z", "updated_at": "2018-11-23T15:36:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/2580#discussion_r148915222", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2580", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148915222"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2580#discussion_r148915222"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2580"}}, "body_html": "<p>This should call the <code>functional</code> version, which will handle all dimensions (see <code>upsample</code> in <code>functional.py</code> for an example):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n    <span class=\"pl-k\">return</span> F.fold(<span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">self</span>.output_size, <span class=\"pl-c1\">self</span>.kernel_size, <span class=\"pl-c1\">self</span>.dilation, <span class=\"pl-c1\">self</span>.padding, <span class=\"pl-c1\">self</span>.stride)</pre></div>", "body_text": "This should call the functional version, which will handle all dimensions (see upsample in functional.py for an example):\ndef forward(self, input):\n    return F.fold(input, self.output_size, self.kernel_size, self.dilation, self.padding, self.stride)"}