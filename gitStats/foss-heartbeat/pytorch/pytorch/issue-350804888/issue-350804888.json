{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10537", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10537/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10537/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10537/events", "html_url": "https://github.com/pytorch/pytorch/issues/10537", "id": 350804888, "node_id": "MDU6SXNzdWUzNTA4MDQ4ODg=", "number": 10537, "title": "the problem for RNN training with DataParallel", "user": {"login": "wqn628", "id": 17527574, "node_id": "MDQ6VXNlcjE3NTI3NTc0", "avatar_url": "https://avatars0.githubusercontent.com/u/17527574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wqn628", "html_url": "https://github.com/wqn628", "followers_url": "https://api.github.com/users/wqn628/followers", "following_url": "https://api.github.com/users/wqn628/following{/other_user}", "gists_url": "https://api.github.com/users/wqn628/gists{/gist_id}", "starred_url": "https://api.github.com/users/wqn628/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wqn628/subscriptions", "organizations_url": "https://api.github.com/users/wqn628/orgs", "repos_url": "https://api.github.com/users/wqn628/repos", "events_url": "https://api.github.com/users/wqn628/events{/privacy}", "received_events_url": "https://api.github.com/users/wqn628/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-08-15T13:15:16Z", "updated_at": "2018-09-22T01:38:33Z", "closed_at": "2018-08-15T17:47:46Z", "author_association": "NONE", "body_html": "<p>hello, today I train the nn.LSTM with multi-gpu by nn.DataParallel. I set the batch_size=12 in data.DataLoader, so the shape of input is (12, T, *), T represents the max frame in the 12 sentences(these 12 sentences has been padded into equal length), and * represents the dimension of the feature for each frame. the number of gpu is 3. But the error as follows: I confused by this error for a long time. Any help would be appreciated\u3002\u3002\u3002</p>\n<pre><code>File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 114, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 124, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 65, in parallel_apply\n    raise output\nValueError: Expected `len(lengths)` to be equal to batch_size, but got 12 (batch_size=4).\n</code></pre>", "body_text": "hello, today I train the nn.LSTM with multi-gpu by nn.DataParallel. I set the batch_size=12 in data.DataLoader, so the shape of input is (12, T, *), T represents the max frame in the 12 sentences(these 12 sentences has been padded into equal length), and * represents the dimension of the feature for each frame. the number of gpu is 3. But the error as follows: I confused by this error for a long time. Any help would be appreciated\u3002\u3002\u3002\nFile \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 114, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 124, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 65, in parallel_apply\n    raise output\nValueError: Expected `len(lengths)` to be equal to batch_size, but got 12 (batch_size=4).", "body": "hello, today I train the nn.LSTM with multi-gpu by nn.DataParallel. I set the batch_size=12 in data.DataLoader, so the shape of input is (12, T, *), T represents the max frame in the 12 sentences(these 12 sentences has been padded into equal length), and * represents the dimension of the feature for each frame. the number of gpu is 3. But the error as follows: I confused by this error for a long time. Any help would be appreciated\u3002\u3002\u3002\r\n\r\n```\r\nFile \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 114, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 124, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/search/speech/wangqingnan/Anaconda/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 65, in parallel_apply\r\n    raise output\r\nValueError: Expected `len(lengths)` to be equal to batch_size, but got 12 (batch_size=4).\r\n```"}