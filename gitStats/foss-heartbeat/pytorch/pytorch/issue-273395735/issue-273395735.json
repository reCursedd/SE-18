{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3665", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3665/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3665/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3665/events", "html_url": "https://github.com/pytorch/pytorch/issues/3665", "id": 273395735, "node_id": "MDU6SXNzdWUyNzMzOTU3MzU=", "number": 3665, "title": "Slight memory leak for LSTM ", "user": {"login": "jiesutd", "id": 9111828, "node_id": "MDQ6VXNlcjkxMTE4Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/9111828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiesutd", "html_url": "https://github.com/jiesutd", "followers_url": "https://api.github.com/users/jiesutd/followers", "following_url": "https://api.github.com/users/jiesutd/following{/other_user}", "gists_url": "https://api.github.com/users/jiesutd/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiesutd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiesutd/subscriptions", "organizations_url": "https://api.github.com/users/jiesutd/orgs", "repos_url": "https://api.github.com/users/jiesutd/repos", "events_url": "https://api.github.com/users/jiesutd/events{/privacy}", "received_events_url": "https://api.github.com/users/jiesutd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2017-11-13T11:24:09Z", "updated_at": "2018-07-27T03:39:57Z", "closed_at": "2018-06-25T20:11:28Z", "author_association": "NONE", "body_html": "<p>Hi everyone,<br>\nI find there is a slight memory leak during training my lstm networks.<br>\nHere is the running environment:</p>\n<blockquote>\n<p>torch: 0.2.0.3<br>\ncuda: 7.5<br>\nOS:ubuntu 16.04LTS</p>\n</blockquote>\n<p>This issue is similar to the discuss <a href=\"https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/9\" rel=\"nofollow\">here</a>.<br>\nFollowing with the discussion, I found to disable the cuDNN (adding <code>torch.backends.cudnn.enabled = False</code>) can solve this problem but the speed is affected either.</p>\n<p>I am quite confused about the memory leak when using cuDNN.</p>\n<pre><code># -*- coding: utf-8 -*-\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport os\ntorch.manual_seed(1)\n\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor).cuda()\n\nclass BiLSTM(nn.Module):\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n                            num_layers=1, bidirectional=True)\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n        self.gpu = True \n        if self.gpu:\n            self.word_embeds = self.word_embeds.cuda()\n            self.lstm = self.lstm.cuda()\n            self.hidden2tag = self.hidden2tag.cuda() \n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        (a,b) = (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n        if self.gpu:\n            a = a.cuda()\n            b = b.cuda()\n        return (a,b)\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def neg_log_likelihood(self, sentence, tags):\n        loss_function = nn.NLLLoss()\n        feats = self._get_lstm_features(sentence)\n        score = F.log_softmax(feats)\n        loss = loss_function(score, tags)\n        return loss\n\n    def forward(self, sentence):  \n        lstm_feats = self._get_lstm_features(sentence)\n        score = F.log_softmax(lstm_feats)\n        if self.gpu:\n            score = score.cpu()\n        tag_seq = score.data.numpy().argmax(axis=1).tolist()\n        return score, tag_seq\n#####################################################################\n# Run training\n## disable cudnn fixed the mem leak problem\n#torch.backends.cudnn.enabled = False\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n# Make up some training data\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n)] * 500 + [(\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]*500\n\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2}\nmodel = BiLSTM(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n## get process id\npid = os.getpid()\nprev_mem = 0\n# Make sure prepare_sequence from earlier in the LSTM section is loaded\nfor epoch in range(300):\n    print \"Epoch: %s\"%epoch\n    inst_id = 0\n    for sentence, tags in training_data:\n        inst_id += 1\n        model.zero_grad()\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n        neg_log_likelihood.backward()\n        optimizer.step()\n        if inst_id%100 == 0:\n            cur_mem = (int(open('/proc/%s/statm'%pid, 'r').read().split()[1])+0.0)/256\n            add_mem = cur_mem - prev_mem\n            prev_mem = cur_mem\n            print \"     train instances: %s, added mem: %sM\"%(inst_id, add_mem)\n\n# Check predictions after training\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprint(model(precheck_sent))\n</code></pre>\n<p>Results when cuDNN is enabled, the memory leak occurs in every epoch. Screenshot:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/9111828/32723137-558d6d6a-c8a7-11e7-8934-644aeb1aa762.png\"><img src=\"https://user-images.githubusercontent.com/9111828/32723137-558d6d6a-c8a7-11e7-8934-644aeb1aa762.png\" alt=\"cudnn_enable\" style=\"max-width:100%;\"></a></p>\n<p>Results when cuDNN is disabled, the memory leak is almost eleminated (although happens in first several epochs). Screenshot:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/9111828/32723151-638edee4-c8a7-11e7-825b-f6f604b7aaf2.png\"><img src=\"https://user-images.githubusercontent.com/9111828/32723151-638edee4-c8a7-11e7-825b-f6f604b7aaf2.png\" alt=\"cudnn_disable\" style=\"max-width:100%;\"></a></p>", "body_text": "Hi everyone,\nI find there is a slight memory leak during training my lstm networks.\nHere is the running environment:\n\ntorch: 0.2.0.3\ncuda: 7.5\nOS:ubuntu 16.04LTS\n\nThis issue is similar to the discuss here.\nFollowing with the discussion, I found to disable the cuDNN (adding torch.backends.cudnn.enabled = False) can solve this problem but the speed is affected either.\nI am quite confused about the memory leak when using cuDNN.\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport os\ntorch.manual_seed(1)\n\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor).cuda()\n\nclass BiLSTM(nn.Module):\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n                            num_layers=1, bidirectional=True)\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n        self.gpu = True \n        if self.gpu:\n            self.word_embeds = self.word_embeds.cuda()\n            self.lstm = self.lstm.cuda()\n            self.hidden2tag = self.hidden2tag.cuda() \n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        (a,b) = (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n        if self.gpu:\n            a = a.cuda()\n            b = b.cuda()\n        return (a,b)\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def neg_log_likelihood(self, sentence, tags):\n        loss_function = nn.NLLLoss()\n        feats = self._get_lstm_features(sentence)\n        score = F.log_softmax(feats)\n        loss = loss_function(score, tags)\n        return loss\n\n    def forward(self, sentence):  \n        lstm_feats = self._get_lstm_features(sentence)\n        score = F.log_softmax(lstm_feats)\n        if self.gpu:\n            score = score.cpu()\n        tag_seq = score.data.numpy().argmax(axis=1).tolist()\n        return score, tag_seq\n#####################################################################\n# Run training\n## disable cudnn fixed the mem leak problem\n#torch.backends.cudnn.enabled = False\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n# Make up some training data\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n)] * 500 + [(\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]*500\n\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2}\nmodel = BiLSTM(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n## get process id\npid = os.getpid()\nprev_mem = 0\n# Make sure prepare_sequence from earlier in the LSTM section is loaded\nfor epoch in range(300):\n    print \"Epoch: %s\"%epoch\n    inst_id = 0\n    for sentence, tags in training_data:\n        inst_id += 1\n        model.zero_grad()\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n        neg_log_likelihood.backward()\n        optimizer.step()\n        if inst_id%100 == 0:\n            cur_mem = (int(open('/proc/%s/statm'%pid, 'r').read().split()[1])+0.0)/256\n            add_mem = cur_mem - prev_mem\n            prev_mem = cur_mem\n            print \"     train instances: %s, added mem: %sM\"%(inst_id, add_mem)\n\n# Check predictions after training\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprint(model(precheck_sent))\n\nResults when cuDNN is enabled, the memory leak occurs in every epoch. Screenshot:\n\nResults when cuDNN is disabled, the memory leak is almost eleminated (although happens in first several epochs). Screenshot:", "body": "Hi everyone, \r\nI find there is a slight memory leak during training my lstm networks. \r\nHere is the running environment:\r\n\r\n> torch: 0.2.0.3\r\n> cuda: 7.5\r\n> OS:ubuntu 16.04LTS\r\n\r\nThis issue is similar to the discuss [here](https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/9). \r\nFollowing with the discussion, I found to disable the cuDNN (adding `torch.backends.cudnn.enabled = False`) can solve this problem but the speed is affected either. \r\n\r\nI am quite confused about the memory leak when using cuDNN. \r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nimport os\r\ntorch.manual_seed(1)\r\n\r\ndef prepare_sequence(seq, to_ix):\r\n    idxs = [to_ix[w] for w in seq]\r\n    tensor = torch.LongTensor(idxs)\r\n    return autograd.Variable(tensor).cuda()\r\n\r\nclass BiLSTM(nn.Module):\r\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\r\n        super(BiLSTM, self).__init__()\r\n        self.embedding_dim = embedding_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.vocab_size = vocab_size\r\n        self.tag_to_ix = tag_to_ix\r\n        self.tagset_size = len(tag_to_ix)\r\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\r\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\r\n                            num_layers=1, bidirectional=True)\r\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\r\n        self.gpu = True \r\n        if self.gpu:\r\n            self.word_embeds = self.word_embeds.cuda()\r\n            self.lstm = self.lstm.cuda()\r\n            self.hidden2tag = self.hidden2tag.cuda() \r\n        self.hidden = self.init_hidden()\r\n\r\n    def init_hidden(self):\r\n        (a,b) = (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\r\n                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\r\n        if self.gpu:\r\n            a = a.cuda()\r\n            b = b.cuda()\r\n        return (a,b)\r\n\r\n    def _get_lstm_features(self, sentence):\r\n        self.hidden = self.init_hidden()\r\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\r\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\r\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\r\n        lstm_feats = self.hidden2tag(lstm_out)\r\n        return lstm_feats\r\n\r\n    def neg_log_likelihood(self, sentence, tags):\r\n        loss_function = nn.NLLLoss()\r\n        feats = self._get_lstm_features(sentence)\r\n        score = F.log_softmax(feats)\r\n        loss = loss_function(score, tags)\r\n        return loss\r\n\r\n    def forward(self, sentence):  \r\n        lstm_feats = self._get_lstm_features(sentence)\r\n        score = F.log_softmax(lstm_feats)\r\n        if self.gpu:\r\n            score = score.cpu()\r\n        tag_seq = score.data.numpy().argmax(axis=1).tolist()\r\n        return score, tag_seq\r\n#####################################################################\r\n# Run training\r\n## disable cudnn fixed the mem leak problem\r\n#torch.backends.cudnn.enabled = False\r\nEMBEDDING_DIM = 5\r\nHIDDEN_DIM = 4\r\n# Make up some training data\r\ntraining_data = [(\r\n    \"the wall street journal reported today that apple corporation made money\".split(),\r\n    \"B I I I O O O B I O O\".split()\r\n)] * 500 + [(\r\n    \"georgia tech is a university in georgia\".split(),\r\n    \"B I O O O O B\".split()\r\n)]*500\r\n\r\nword_to_ix = {}\r\nfor sentence, tags in training_data:\r\n    for word in sentence:\r\n        if word not in word_to_ix:\r\n            word_to_ix[word] = len(word_to_ix)\r\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2}\r\nmodel = BiLSTM(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\r\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\r\n## get process id\r\npid = os.getpid()\r\nprev_mem = 0\r\n# Make sure prepare_sequence from earlier in the LSTM section is loaded\r\nfor epoch in range(300):\r\n    print \"Epoch: %s\"%epoch\r\n    inst_id = 0\r\n    for sentence, tags in training_data:\r\n        inst_id += 1\r\n        model.zero_grad()\r\n        sentence_in = prepare_sequence(sentence, word_to_ix)\r\n        targets = prepare_sequence(tags, tag_to_ix)\r\n        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\r\n        neg_log_likelihood.backward()\r\n        optimizer.step()\r\n        if inst_id%100 == 0:\r\n            cur_mem = (int(open('/proc/%s/statm'%pid, 'r').read().split()[1])+0.0)/256\r\n            add_mem = cur_mem - prev_mem\r\n            prev_mem = cur_mem\r\n            print \"     train instances: %s, added mem: %sM\"%(inst_id, add_mem)\r\n\r\n# Check predictions after training\r\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\r\nprint(model(precheck_sent))\r\n```\r\n\r\nResults when cuDNN is enabled, the memory leak occurs in every epoch. Screenshot:\r\n![cudnn_enable](https://user-images.githubusercontent.com/9111828/32723137-558d6d6a-c8a7-11e7-8934-644aeb1aa762.png)\r\n\r\nResults when cuDNN is disabled, the memory leak is almost eleminated (although happens in first several epochs). Screenshot:\r\n![cudnn_disable](https://user-images.githubusercontent.com/9111828/32723151-638edee4-c8a7-11e7-825b-f6f604b7aaf2.png)\r\n"}