{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393607435", "html_url": "https://github.com/pytorch/pytorch/issues/7965#issuecomment-393607435", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7965", "id": 393607435, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzYwNzQzNQ==", "user": {"login": "lzamparo", "id": 1569186, "node_id": "MDQ6VXNlcjE1NjkxODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1569186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lzamparo", "html_url": "https://github.com/lzamparo", "followers_url": "https://api.github.com/users/lzamparo/followers", "following_url": "https://api.github.com/users/lzamparo/following{/other_user}", "gists_url": "https://api.github.com/users/lzamparo/gists{/gist_id}", "starred_url": "https://api.github.com/users/lzamparo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lzamparo/subscriptions", "organizations_url": "https://api.github.com/users/lzamparo/orgs", "repos_url": "https://api.github.com/users/lzamparo/repos", "events_url": "https://api.github.com/users/lzamparo/events{/privacy}", "received_events_url": "https://api.github.com/users/lzamparo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-31T17:19:13Z", "updated_at": "2018-05-31T17:19:45Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>untied biases would also be tied to the height and width of the input, correct?</p>\n</blockquote>\n<p>Not quite.  <a href=\"https://datascience.stackexchange.com/questions/17671/correct-number-of-biases-in-cnn\" rel=\"nofollow\">This accounting</a> suggests the number of bias parameters depends on the size of the <em>filters</em>, not of the input.  As suggested in <a href=\"https://adbrebs.wordpress.com/2015/02/20/tied-weights-in-convolutions/\" rel=\"nofollow\">this</a> comparison from Blocks:</p>\n<ul>\n<li>Tied biases would have one bias parameter per feature map</li>\n<li>Untied biases would have one bias parameter per position per feature map.</li>\n</ul>\n<blockquote>\n<p>And user cannot pass a different shaped input after the first invocation.</p>\n</blockquote>\n<p>I think as long as the convolution is valid wrt the input, you can have either tied or untied biases.  If the user decided to change the shape of the kernels, that would cause a problem.</p>", "body_text": "untied biases would also be tied to the height and width of the input, correct?\n\nNot quite.  This accounting suggests the number of bias parameters depends on the size of the filters, not of the input.  As suggested in this comparison from Blocks:\n\nTied biases would have one bias parameter per feature map\nUntied biases would have one bias parameter per position per feature map.\n\n\nAnd user cannot pass a different shaped input after the first invocation.\n\nI think as long as the convolution is valid wrt the input, you can have either tied or untied biases.  If the user decided to change the shape of the kernels, that would cause a problem.", "body": "> untied biases would also be tied to the height and width of the input, correct?\r\n\r\nNot quite.  [This accounting](https://datascience.stackexchange.com/questions/17671/correct-number-of-biases-in-cnn) suggests the number of bias parameters depends on the size of the *filters*, not of the input.  As suggested in [this](https://adbrebs.wordpress.com/2015/02/20/tied-weights-in-convolutions/) comparison from Blocks:\r\n- Tied biases would have one bias parameter per feature map\r\n- Untied biases would have one bias parameter per position per feature map.  \r\n\r\n> And user cannot pass a different shaped input after the first invocation.\r\n\r\nI think as long as the convolution is valid wrt the input, you can have either tied or untied biases.  If the user decided to change the shape of the kernels, that would cause a problem."}