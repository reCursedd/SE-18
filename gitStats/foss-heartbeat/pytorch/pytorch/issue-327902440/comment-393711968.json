{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393711968", "html_url": "https://github.com/pytorch/pytorch/issues/7965#issuecomment-393711968", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7965", "id": 393711968, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzcxMTk2OA==", "user": {"login": "lzamparo", "id": 1569186, "node_id": "MDQ6VXNlcjE1NjkxODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1569186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lzamparo", "html_url": "https://github.com/lzamparo", "followers_url": "https://api.github.com/users/lzamparo/followers", "following_url": "https://api.github.com/users/lzamparo/following{/other_user}", "gists_url": "https://api.github.com/users/lzamparo/gists{/gist_id}", "starred_url": "https://api.github.com/users/lzamparo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lzamparo/subscriptions", "organizations_url": "https://api.github.com/users/lzamparo/orgs", "repos_url": "https://api.github.com/users/lzamparo/repos", "events_url": "https://api.github.com/users/lzamparo/events{/privacy}", "received_events_url": "https://api.github.com/users/lzamparo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-31T23:17:21Z", "updated_at": "2018-05-31T23:17:37Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> I'm speculating, since I have not used variable sized inputs in this (or other) models, but the size of the untied bias is dependent on the size of the feature map, so I think you are correct.  If the input changed in size, the tensor of untied bias values would also have to be resized (and some values either discarded or imputed, would be complicated).  Will this dependence prevent untied biases ever being supported?</p>", "body_text": "@fmassa @soumith I'm speculating, since I have not used variable sized inputs in this (or other) models, but the size of the untied bias is dependent on the size of the feature map, so I think you are correct.  If the input changed in size, the tensor of untied bias values would also have to be resized (and some values either discarded or imputed, would be complicated).  Will this dependence prevent untied biases ever being supported?", "body": "@fmassa @soumith I'm speculating, since I have not used variable sized inputs in this (or other) models, but the size of the untied bias is dependent on the size of the feature map, so I think you are correct.  If the input changed in size, the tensor of untied bias values would also have to be resized (and some values either discarded or imputed, would be complicated).  Will this dependence prevent untied biases ever being supported?"}