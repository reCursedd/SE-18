{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7965", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7965/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7965/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7965/events", "html_url": "https://github.com/pytorch/pytorch/issues/7965", "id": 327902440, "node_id": "MDU6SXNzdWUzMjc5MDI0NDA=", "number": 7965, "title": "[feature request] Untied biases for nn.Conv?d", "user": {"login": "lzamparo", "id": 1569186, "node_id": "MDQ6VXNlcjE1NjkxODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1569186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lzamparo", "html_url": "https://github.com/lzamparo", "followers_url": "https://api.github.com/users/lzamparo/followers", "following_url": "https://api.github.com/users/lzamparo/following{/other_user}", "gists_url": "https://api.github.com/users/lzamparo/gists{/gist_id}", "starred_url": "https://api.github.com/users/lzamparo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lzamparo/subscriptions", "organizations_url": "https://api.github.com/users/lzamparo/orgs", "repos_url": "https://api.github.com/users/lzamparo/repos", "events_url": "https://api.github.com/users/lzamparo/events{/privacy}", "received_events_url": "https://api.github.com/users/lzamparo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-05-30T21:16:22Z", "updated_at": "2018-06-01T13:45:33Z", "closed_at": "2018-06-01T02:47:15Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I'm trying to port a model from Lasagne to PyTorch, but am not sure I can do so since the convolutional layers have <a href=\"https://harmdevries89.wordpress.com/2015/03/27/tied-biases-vs-untied-biases/\" rel=\"nofollow\">untied biases</a>.</p>\n<p>From the Lasagne model:</p>\n<pre><code>l1a = Conv2DLayer(l0, num_filters=300, filter_size=(1, 19), W=nn.init.Orthogonal(gain='relu'), b=nn.init.Constant(0.1), nonlinearity=None, untie_biases=True)\n</code></pre>\n<p>What does it mean to untie the biases?.  From <a href=\"https://lasagne.readthedocs.io/en/latest/modules/layers/conv.html?highlight=untie#lasagne.layers.Conv2DLayer\" rel=\"nofollow\">the docs</a>:</p>\n<blockquote>\n<p>untie_biases : bool (default: False)<br>\nIf False, the layer will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D).</p>\n<p>If True, the layer will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.</p>\n</blockquote>\n<p>A cursory look at the source for _ConvNd shows that the biases  are currently 1D tensors:</p>\n<pre><code>if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n</code></pre>\n<p>it's not a bother to make this larger based on a different flag in the constructor, but would require a change to <code>F.conv2d</code> that would discern the different shape of <code>self.bias</code> and add accordingly.</p>", "body_text": "Issue description\nI'm trying to port a model from Lasagne to PyTorch, but am not sure I can do so since the convolutional layers have untied biases.\nFrom the Lasagne model:\nl1a = Conv2DLayer(l0, num_filters=300, filter_size=(1, 19), W=nn.init.Orthogonal(gain='relu'), b=nn.init.Constant(0.1), nonlinearity=None, untie_biases=True)\n\nWhat does it mean to untie the biases?.  From the docs:\n\nuntie_biases : bool (default: False)\nIf False, the layer will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D).\nIf True, the layer will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.\n\nA cursory look at the source for _ConvNd shows that the biases  are currently 1D tensors:\nif bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n\nit's not a bother to make this larger based on a different flag in the constructor, but would require a change to F.conv2d that would discern the different shape of self.bias and add accordingly.", "body": "## Issue description\r\n\r\nI'm trying to port a model from Lasagne to PyTorch, but am not sure I can do so since the convolutional layers have [untied biases](https://harmdevries89.wordpress.com/2015/03/27/tied-biases-vs-untied-biases/).  \r\n\r\nFrom the Lasagne model:\r\n\r\n```\r\nl1a = Conv2DLayer(l0, num_filters=300, filter_size=(1, 19), W=nn.init.Orthogonal(gain='relu'), b=nn.init.Constant(0.1), nonlinearity=None, untie_biases=True)\r\n```\r\n What does it mean to untie the biases?.  From [the docs](https://lasagne.readthedocs.io/en/latest/modules/layers/conv.html?highlight=untie#lasagne.layers.Conv2DLayer):\r\n\r\n> untie_biases : bool (default: False)\r\nIf False, the layer will have a bias parameter for each channel, which is shared across all positions in this channel. As a result, the b attribute will be a vector (1D).\r\n>\r\n>If True, the layer will have separate bias parameters for each position in each channel. As a result, the b attribute will be a 3D tensor.\r\n\r\nA cursory look at the source for _ConvNd shows that the biases  are currently 1D tensors:\r\n```\r\nif bias:\r\n            self.bias = Parameter(torch.Tensor(out_channels))\r\n        else:\r\n            self.register_parameter('bias', None)\r\n```\r\nit's not a bother to make this larger based on a different flag in the constructor, but would require a change to `F.conv2d` that would discern the different shape of `self.bias` and add accordingly."}