{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393742332", "html_url": "https://github.com/pytorch/pytorch/issues/7965#issuecomment-393742332", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7965", "id": 393742332, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mzc0MjMzMg==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-01T02:47:15Z", "updated_at": "2018-06-01T02:48:05Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1569186\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lzamparo\">@lzamparo</a> this should work for you for untied bias:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> torch.nn.modules.utils <span class=\"pl-k\">import</span> _pair\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Conv2dUntiedBias</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">height</span>, <span class=\"pl-smi\">width</span>, <span class=\"pl-smi\">in_channels</span>, <span class=\"pl-smi\">out_channels</span>, <span class=\"pl-smi\">kernel_size</span>, <span class=\"pl-smi\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-smi\">dilation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">groups</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        kernel_size <span class=\"pl-k\">=</span> _pair(kernel_size)\n        stride <span class=\"pl-k\">=</span> _pair(stride)\n        padding <span class=\"pl-k\">=</span> _pair(padding)\n        dilation <span class=\"pl-k\">=</span> _pair(dilation)\n\n        <span class=\"pl-k\">if</span> in_channels <span class=\"pl-k\">%</span> groups <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>in_channels must be divisible by groups<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">if</span> out_channels <span class=\"pl-k\">%</span> groups <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>out_channels must be divisible by groups<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c1\">self</span>.in_channels <span class=\"pl-k\">=</span> in_channels\n        <span class=\"pl-c1\">self</span>.out_channels <span class=\"pl-k\">=</span> out_channels\n        <span class=\"pl-c1\">self</span>.kernel_size <span class=\"pl-k\">=</span> kernel_size\n        <span class=\"pl-c1\">self</span>.stride <span class=\"pl-k\">=</span> stride\n        <span class=\"pl-c1\">self</span>.padding <span class=\"pl-k\">=</span> padding\n        <span class=\"pl-c1\">self</span>.dilation <span class=\"pl-k\">=</span> dilation\n        <span class=\"pl-c1\">self</span>.groups <span class=\"pl-k\">=</span> groups\n        <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> Parameter(torch.Tensor(\n                out_channels, in_channels <span class=\"pl-k\">//</span> groups, <span class=\"pl-k\">*</span>kernel_size))\n        <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">=</span> Parameter(torch.Tensor(out_channels, height, width))\n        <span class=\"pl-c1\">self</span>.reset_parameters()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">reset_parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.in_channels\n        <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.kernel_size:\n            n <span class=\"pl-k\">*=</span> k\n        stdv <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> math.sqrt(n)\n        <span class=\"pl-c1\">self</span>.weight.data.uniform_(<span class=\"pl-k\">-</span>stdv, stdv)\n        <span class=\"pl-c1\">self</span>.bias.data.uniform_(<span class=\"pl-k\">-</span>stdv, stdv)\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        output <span class=\"pl-k\">=</span> F.conv2d(<span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">self</span>.weight, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">self</span>.stride,\n                        <span class=\"pl-c1\">self</span>.padding, <span class=\"pl-c1\">self</span>.dilation, <span class=\"pl-c1\">self</span>.groups)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> add untied bias</span>\n        output <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">self</span>.bias.unsqueeze(<span class=\"pl-c1\">0</span>).repeat(<span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)</pre></div>", "body_text": "@lzamparo this should work for you for untied bias:\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair\n\nclass Conv2dUntiedBias(nn.Module):\n    def __init__(self, height, width, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n\n        if in_channels % groups != 0:\n            raise ValueError('in_channels must be divisible by groups')\n        if out_channels % groups != 0:\n            raise ValueError('out_channels must be divisible by groups')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.weight = Parameter(torch.Tensor(\n                out_channels, in_channels // groups, *kernel_size))\n        self.bias = Parameter(torch.Tensor(out_channels, height, width))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        self.bias.data.uniform_(-stdv, stdv)\n\n\n    def forward(self, input):\n        output = F.conv2d(input, self.weight, None, self.stride,\n                        self.padding, self.dilation, self.groups)\n        # add untied bias\n        output += self.bias.unsqueeze(0).repeat(input.size(0), 1, 1, 1)", "body": "\r\n@lzamparo this should work for you for untied bias:\r\n\r\n```python\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn.modules.utils import _pair\r\n\r\nclass Conv2dUntiedBias(nn.Module):\r\n    def __init__(self, height, width, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\r\n        kernel_size = _pair(kernel_size)\r\n        stride = _pair(stride)\r\n        padding = _pair(padding)\r\n        dilation = _pair(dilation)\r\n\r\n        if in_channels % groups != 0:\r\n            raise ValueError('in_channels must be divisible by groups')\r\n        if out_channels % groups != 0:\r\n            raise ValueError('out_channels must be divisible by groups')\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n        self.kernel_size = kernel_size\r\n        self.stride = stride\r\n        self.padding = padding\r\n        self.dilation = dilation\r\n        self.groups = groups\r\n        self.weight = Parameter(torch.Tensor(\r\n                out_channels, in_channels // groups, *kernel_size))\r\n        self.bias = Parameter(torch.Tensor(out_channels, height, width))\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        n = self.in_channels\r\n        for k in self.kernel_size:\r\n            n *= k\r\n        stdv = 1. / math.sqrt(n)\r\n        self.weight.data.uniform_(-stdv, stdv)\r\n        self.bias.data.uniform_(-stdv, stdv)\r\n\r\n\r\n    def forward(self, input):\r\n        output = F.conv2d(input, self.weight, None, self.stride,\r\n                        self.padding, self.dilation, self.groups)\r\n        # add untied bias\r\n        output += self.bias.unsqueeze(0).repeat(input.size(0), 1, 1, 1)\r\n```"}