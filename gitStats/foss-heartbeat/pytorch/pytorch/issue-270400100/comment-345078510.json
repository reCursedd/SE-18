{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/345078510", "html_url": "https://github.com/pytorch/pytorch/issues/3423#issuecomment-345078510", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3423", "id": 345078510, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTA3ODUxMA==", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-16T22:08:01Z", "updated_at": "2017-11-16T22:08:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> I'm not aware of a method better than Jacobi's formula unfortunately. (Anyone who has more experience with this please chime in!) For all intents and purposes I think it's fair to assume that the input is invertible in which case you can calculate the inverse transpose. Here's how tensorflow and autograd handle it:</p>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L46\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L46</a></li>\n<li><a href=\"https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L22\">https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L22</a></li>\n</ul>\n<p>It's also derived in section 2.2.4 here: <a href=\"https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\" rel=\"nofollow\">https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf</a>. The log-determinant gradient should be very similar but with a det(A) factor missing.</p>", "body_text": "@SsnL I'm not aware of a method better than Jacobi's formula unfortunately. (Anyone who has more experience with this please chime in!) For all intents and purposes I think it's fair to assume that the input is invertible in which case you can calculate the inverse transpose. Here's how tensorflow and autograd handle it:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L46\nhttps://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L22\n\nIt's also derived in section 2.2.4 here: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf. The log-determinant gradient should be very similar but with a det(A) factor missing.", "body": "@SsnL I'm not aware of a method better than Jacobi's formula unfortunately. (Anyone who has more experience with this please chime in!) For all intents and purposes I think it's fair to assume that the input is invertible in which case you can calculate the inverse transpose. Here's how tensorflow and autograd handle it:\r\n\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L46\r\n* https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L22\r\n\r\nIt's also derived in section 2.2.4 here: https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf. The log-determinant gradient should be very similar but with a det(A) factor missing."}