{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6470", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6470/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6470/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6470/events", "html_url": "https://github.com/pytorch/pytorch/pull/6470", "id": 312983494, "node_id": "MDExOlB1bGxSZXF1ZXN0MTgwNjQ3OTE1", "number": 6470, "title": "Separate cuda-ness from dtype.", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-10T15:48:33Z", "updated_at": "2018-11-23T15:42:10Z", "closed_at": "2018-04-12T18:05:45Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6470", "html_url": "https://github.com/pytorch/pytorch/pull/6470", "diff_url": "https://github.com/pytorch/pytorch/pull/6470.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6470.patch"}, "body_html": "<p>There are no longer torch.cuda.int64, etc; only torch.int64 that correspond to at::ScalarType.<br>\nAt the python arg parser level, the corresponding ATen type is selected from the combination of (ScalarType, Layout, Device).</p>\n<p>There is also currently unused code in here for support ScalarType in native_functions; this will be used for specifying aggregate types on reduction functions.</p>", "body_text": "There are no longer torch.cuda.int64, etc; only torch.int64 that correspond to at::ScalarType.\nAt the python arg parser level, the corresponding ATen type is selected from the combination of (ScalarType, Layout, Device).\nThere is also currently unused code in here for support ScalarType in native_functions; this will be used for specifying aggregate types on reduction functions.", "body": "There are no longer torch.cuda.int64, etc; only torch.int64 that correspond to at::ScalarType.\r\nAt the python arg parser level, the corresponding ATen type is selected from the combination of (ScalarType, Layout, Device).\r\n\r\nThere is also currently unused code in here for support ScalarType in native_functions; this will be used for specifying aggregate types on reduction functions.\r\n\r\n"}