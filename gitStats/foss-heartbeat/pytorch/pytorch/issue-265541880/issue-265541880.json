{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3123", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3123/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3123/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3123/events", "html_url": "https://github.com/pytorch/pytorch/issues/3123", "id": 265541880, "node_id": "MDU6SXNzdWUyNjU1NDE4ODA=", "number": 3123, "title": "Cannot compute gradients through variable reparameterizations", "user": {"login": "AtheMathmo", "id": 10150986, "node_id": "MDQ6VXNlcjEwMTUwOTg2", "avatar_url": "https://avatars0.githubusercontent.com/u/10150986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AtheMathmo", "html_url": "https://github.com/AtheMathmo", "followers_url": "https://api.github.com/users/AtheMathmo/followers", "following_url": "https://api.github.com/users/AtheMathmo/following{/other_user}", "gists_url": "https://api.github.com/users/AtheMathmo/gists{/gist_id}", "starred_url": "https://api.github.com/users/AtheMathmo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AtheMathmo/subscriptions", "organizations_url": "https://api.github.com/users/AtheMathmo/orgs", "repos_url": "https://api.github.com/users/AtheMathmo/repos", "events_url": "https://api.github.com/users/AtheMathmo/events{/privacy}", "received_events_url": "https://api.github.com/users/AtheMathmo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-15T03:11:59Z", "updated_at": "2017-10-15T03:22:02Z", "closed_at": "2017-10-15T03:21:02Z", "author_association": "NONE", "body_html": "<p>Hi!</p>\n<p>This is likely to be user error but I can't figure out why this fails and have reduced to a minimal example.</p>\n<p>I want to reparameterize some parameters to restrict their domain. In particular I have some variances I want to optimize which must remain positive. So I create some new variables to parameterize the variances and optimize over these instead. In particular, I use,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> rho is my dummy variable</span>\nrho_init <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">100</span>)\nrho <span class=\"pl-k\">=</span> Variable(rho_init, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Here the variances are reparameterized to always be positive</span>\nvariances <span class=\"pl-k\">=</span> torch.log(<span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">+</span> torch.exp(rho))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Do optimization</span>\noptimizer <span class=\"pl-k\">=</span> torch.optim.Adam([rho], <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    optimizer.zero_grad()\n    x <span class=\"pl-k\">=</span> torch.sum(variances)\n    x.backward()\n    optimizer.step()</pre></div>\n<p>However this loop will fail when it hits the second <code>backward()</code> call. My guess is that rho is updated and so the variances must be recomputed but the computational graph is not discarded. It is not obvious to me whether this is intended behaviour. If this is expected, how should I go about making use of such reparameterizations?</p>\n<p>The only issue I could find which may be related is <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"230239542\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1605\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1605/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1605\">#1605</a> but this seems to be resolved.</p>", "body_text": "Hi!\nThis is likely to be user error but I can't figure out why this fails and have reduced to a minimal example.\nI want to reparameterize some parameters to restrict their domain. In particular I have some variances I want to optimize which must remain positive. So I create some new variables to parameterize the variances and optimize over these instead. In particular, I use,\nimport torch\nfrom torch.autograd import Variable\n\n# rho is my dummy variable\nrho_init = torch.ones(100)\nrho = Variable(rho_init, requires_grad=True)\n# Here the variances are reparameterized to always be positive\nvariances = torch.log(1.0 + torch.exp(rho))\n\n# Do optimization\noptimizer = torch.optim.Adam([rho], lr=0.01)\n\nfor i in range(10):\n    optimizer.zero_grad()\n    x = torch.sum(variances)\n    x.backward()\n    optimizer.step()\nHowever this loop will fail when it hits the second backward() call. My guess is that rho is updated and so the variances must be recomputed but the computational graph is not discarded. It is not obvious to me whether this is intended behaviour. If this is expected, how should I go about making use of such reparameterizations?\nThe only issue I could find which may be related is #1605 but this seems to be resolved.", "body": "Hi!\r\n\r\nThis is likely to be user error but I can't figure out why this fails and have reduced to a minimal example.\r\n\r\nI want to reparameterize some parameters to restrict their domain. In particular I have some variances I want to optimize which must remain positive. So I create some new variables to parameterize the variances and optimize over these instead. In particular, I use,\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n# rho is my dummy variable\r\nrho_init = torch.ones(100)\r\nrho = Variable(rho_init, requires_grad=True)\r\n# Here the variances are reparameterized to always be positive\r\nvariances = torch.log(1.0 + torch.exp(rho))\r\n\r\n# Do optimization\r\noptimizer = torch.optim.Adam([rho], lr=0.01)\r\n\r\nfor i in range(10):\r\n    optimizer.zero_grad()\r\n    x = torch.sum(variances)\r\n    x.backward()\r\n    optimizer.step()\r\n```\r\n\r\nHowever this loop will fail when it hits the second `backward()` call. My guess is that rho is updated and so the variances must be recomputed but the computational graph is not discarded. It is not obvious to me whether this is intended behaviour. If this is expected, how should I go about making use of such reparameterizations?\r\n\r\nThe only issue I could find which may be related is #1605 but this seems to be resolved."}