{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9043", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9043/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9043/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9043/events", "html_url": "https://github.com/pytorch/pytorch/issues/9043", "id": 337090968, "node_id": "MDU6SXNzdWUzMzcwOTA5Njg=", "number": 9043, "title": "onnx RNN conversion has some unhygenic steps", "user": {"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-06-29T18:14:52Z", "updated_at": "2018-06-29T18:14:52Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>pytorch's nn.RNN (and analogously, nn.GRU and nn.LSTM) don't have a direct ONNX analog. When we export them, they produce a subgraph which is intended to fully match the semantics of the original nn operator.</p>\n<p>Another complication is that packed sequences do not exist in onnx. To get by this, we introduce a fictional onnx::PackPadded and onnx::PadPacked, which are defined to have exactly the same semantics as the corresponding pytorch operators, but which have no implementation. We eliminate this in an \"optimization\" pass.</p>\n<p>In order to maintain full correctness and cleanliness throughout all transformation stages, rather than directly produce the subgraph containing onnx::RNN, which isn't actually compatible with the fictional onnx::PackPadded, we should instead produce onnx::FakeRNN, which is, and then gradually convert all the fake ops in a way that maintains correctness at each stage.</p>", "body_text": "pytorch's nn.RNN (and analogously, nn.GRU and nn.LSTM) don't have a direct ONNX analog. When we export them, they produce a subgraph which is intended to fully match the semantics of the original nn operator.\nAnother complication is that packed sequences do not exist in onnx. To get by this, we introduce a fictional onnx::PackPadded and onnx::PadPacked, which are defined to have exactly the same semantics as the corresponding pytorch operators, but which have no implementation. We eliminate this in an \"optimization\" pass.\nIn order to maintain full correctness and cleanliness throughout all transformation stages, rather than directly produce the subgraph containing onnx::RNN, which isn't actually compatible with the fictional onnx::PackPadded, we should instead produce onnx::FakeRNN, which is, and then gradually convert all the fake ops in a way that maintains correctness at each stage.", "body": "pytorch's nn.RNN (and analogously, nn.GRU and nn.LSTM) don't have a direct ONNX analog. When we export them, they produce a subgraph which is intended to fully match the semantics of the original nn operator.\r\n\r\nAnother complication is that packed sequences do not exist in onnx. To get by this, we introduce a fictional onnx::PackPadded and onnx::PadPacked, which are defined to have exactly the same semantics as the corresponding pytorch operators, but which have no implementation. We eliminate this in an \"optimization\" pass.\r\n\r\nIn order to maintain full correctness and cleanliness throughout all transformation stages, rather than directly produce the subgraph containing onnx::RNN, which isn't actually compatible with the fictional onnx::PackPadded, we should instead produce onnx::FakeRNN, which is, and then gradually convert all the fake ops in a way that maintains correctness at each stage."}