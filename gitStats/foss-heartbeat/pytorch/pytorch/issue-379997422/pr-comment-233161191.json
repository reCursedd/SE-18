{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233161191", "pull_request_review_id": 174510711, "id": 233161191, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzE2MTE5MQ==", "diff_hunk": "@@ -33,4 +33,20 @@ static inline void flip_check_errors(int64_t total_dims, int64_t flip_dims_size,\n     \", but unique flip dims size=\", flip_dims_v.size());\n }\n \n+static inline Tensor roll_common(const Tensor& self, IntList shifts, IntList dims) {\n+  if (dims.size() == 0 && shifts.size() == 1) {\n+    auto flattened = self.contiguous().view(self.numel());\n+    return roll(flattened, shifts[0], 0).view(self.sizes());\n+  }\n+  AT_CHECK(\n+    shifts.size() == dims.size(),\n+    \"shifts and dimensions must align. shifts: \", shifts.size(), \", dims:\", dims.size()\n+  );\n+  AT_CHECK(dims.size() > 1, \"this code should only be reached for handling multi-dim rolling\" );\n+  auto tail_shifts = shifts.slice(1);\n+  auto tail_dims = dims.slice(1);\n+  auto first_dim_rolled = roll(self, shifts[0], dims[0]);", "path": "aten/src/ATen/native/TensorTransformations.h", "position": 17, "original_position": 16, "commit_id": "9f0df0489248d4e9128b7b512c8b5b71b28e8651", "original_commit_id": "e17dec6826c6d415f7a8300de0fada931db94eda", "user": {"login": "nairbv", "id": 582713, "node_id": "MDQ6VXNlcjU4MjcxMw==", "avatar_url": "https://avatars1.githubusercontent.com/u/582713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairbv", "html_url": "https://github.com/nairbv", "followers_url": "https://api.github.com/users/nairbv/followers", "following_url": "https://api.github.com/users/nairbv/following{/other_user}", "gists_url": "https://api.github.com/users/nairbv/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairbv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairbv/subscriptions", "organizations_url": "https://api.github.com/users/nairbv/orgs", "repos_url": "https://api.github.com/users/nairbv/repos", "events_url": "https://api.github.com/users/nairbv/events{/privacy}", "received_events_url": "https://api.github.com/users/nairbv/received_events", "type": "User", "site_admin": false}, "body": "It seems like all dimensions could be rolled at once by a single cuda kernel using the current kernel logic, just by iterating through the dimensions to update the shift amount. We'd need to pass the array of dims/shifts every time, which could be less efficient for the more common case of a single dimension. Maybe the optimal implementation will use two different kernels depending on number of dims being rolled?\r\n\r\nI haven't thought about optimizations to the CPU version.\r\n\r\nEither way, I'd prefer the functionality working, and possibly iterate on performance later.\r\n", "created_at": "2018-11-13T18:08:53Z", "updated_at": "2018-11-23T15:54:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/13874#discussion_r233161191", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13874", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233161191"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13874#discussion_r233161191"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13874"}}, "body_html": "<p>It seems like all dimensions could be rolled at once by a single cuda kernel using the current kernel logic, just by iterating through the dimensions to update the shift amount. We'd need to pass the array of dims/shifts every time, which could be less efficient for the more common case of a single dimension. Maybe the optimal implementation will use two different kernels depending on number of dims being rolled?</p>\n<p>I haven't thought about optimizations to the CPU version.</p>\n<p>Either way, I'd prefer the functionality working, and possibly iterate on performance later.</p>", "body_text": "It seems like all dimensions could be rolled at once by a single cuda kernel using the current kernel logic, just by iterating through the dimensions to update the shift amount. We'd need to pass the array of dims/shifts every time, which could be less efficient for the more common case of a single dimension. Maybe the optimal implementation will use two different kernels depending on number of dims being rolled?\nI haven't thought about optimizations to the CPU version.\nEither way, I'd prefer the functionality working, and possibly iterate on performance later.", "in_reply_to_id": 233159125}