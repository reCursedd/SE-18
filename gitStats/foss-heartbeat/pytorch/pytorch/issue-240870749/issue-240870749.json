{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1989", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1989/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1989/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1989/events", "html_url": "https://github.com/pytorch/pytorch/issues/1989", "id": 240870749, "node_id": "MDU6SXNzdWUyNDA4NzA3NDk=", "number": 1989, "title": "Reductions returning scalars cause implicit sync-point", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-07-06T07:43:26Z", "updated_at": "2017-07-07T07:43:08Z", "closed_at": "2017-07-07T07:43:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Scalar reduction results are currently being returned as host-side scalars, causing an implicit sync-point, and gpu-&gt;device copy.  See the following results:</p>\n<pre><code>=================\nit 0\n------ .sum(), returns a host-side scalar ---------\ntime for .sum() 0.040\ntime for synchronize 0.000\nres 2000001152.0\n---- .sum(1), returns proxy to gpu-side tensor -----\ntime for .sum(1) 0.000\ntime for synchronize 0.462\nres\n 2.0000e+09\n[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n\n=================\nit 1\n------ .sum(), returns a host-side scalar ---------\ntime for .sum() 0.039\ntime for synchronize 0.000\n---- .sum(1), returns proxy to gpu-side tensor -----\ntime for .sum(1) 0.000\ntime for synchronize 0.462\n=================\nit 2\n------ .sum(), returns a host-side scalar ---------\ntime for .sum() 0.040\ntime for synchronize 0.000\n---- .sum(1), returns proxy to gpu-side tensor -----\ntime for .sum(1) 0.000\ntime for synchronize 0.456\n</code></pre>\n<p>You can see that <code>.sum(1)</code> returns instantly, and the pain doesnt happen till the later <code>.synchronize()</code>. But <code>.sum()</code> waits for the entire gpu operation to finish, and returns a host-side scalar.  <code>.sum()</code> is implicitly, behind the scenes:</p>\n<ul>\n<li>waiting for the entire operation to finish</li>\n<li>sync'ing</li>\n<li>copying the result from the gpu back to the hostside</li>\n</ul>\n<p>I'm not sure this is desirable?  It's also inconsistent with how numpy works, which doesnt convert scalars into a non-numpy scalar until you call <code>.item()</code>.</p>\n<p>Test code for the implicit sync points:</p>\n<pre><code>import torch\nimport time\n\n\nN = 1000 * 1000 * 1000\n\ndef implicit_sync(print_res=False):\n    print('------ .sum(), returns a host-side scalar ---------')\n    # start = time.time()\n    # a = torch.ones(N).cuda()\n    # a += 1\n    # torch.cuda.synchronize()\n    start = time.time()\n    res = a.sum()\n    print('time for .sum() %.3f' % (time.time() - start))\n    start = time.time()\n    torch.cuda.synchronize()\n    print('time for synchronize %.3f' % (time.time() - start))\n    if print_res:\n        print('res', res)\n\ndef stays_on_gpu(print_res=False):\n    print('---- .sum(1), returns proxy to gpu-side tensor -----')\n    start = time.time()\n    res = a.sum(1)\n    print('time for .sum(1) %.3f' % (time.time() - start))\n    start = time.time()\n    torch.cuda.synchronize()\n    print('time for synchronize %.3f' % (time.time() - start))\n    if print_res:\n        print('res', res)\n\n\nif __name__ == '__main__':\n    a = torch.ones(1, N).cuda()\n    a += 1\n    torch.cuda.synchronize()\n    for it in range(3):\n        print('=================')\n        print('it', it)\n        implicit_sync(it == 0)\n        stays_on_gpu(it == 0)\n</code></pre>\n<p>Effect of <code>.item()</code> in numpy:</p>\n<pre><code>import numpy as np\n\na = np.zeros(3)\nprint('type(a[0])', type(a[0]))\nprint('type(a[0].item())', type(a[0].item()))\n</code></pre>\n<p>Result:</p>\n<pre><code>type(a[0]) &lt;class 'numpy.float64'&gt;\ntype(a[0].item()) &lt;class 'float'&gt;\n</code></pre>\n<p>I know this would need a ton of work behind the scenes, and for conv nets, there is only one reduce, in the loss function.  It seems likely that it could have a noticeable effect on python code that does some kind of normalization using fundamental pytorch operations though?  So, this issue means such kernels need to be written in CUDA, rather than in Python, in order to obtain good performance. Fixing this issues would make it easier for Python guys to write high performance kernels for novel research layers.</p>", "body_text": "Scalar reduction results are currently being returned as host-side scalars, causing an implicit sync-point, and gpu->device copy.  See the following results:\n=================\nit 0\n------ .sum(), returns a host-side scalar ---------\ntime for .sum() 0.040\ntime for synchronize 0.000\nres 2000001152.0\n---- .sum(1), returns proxy to gpu-side tensor -----\ntime for .sum(1) 0.000\ntime for synchronize 0.462\nres\n 2.0000e+09\n[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n\n=================\nit 1\n------ .sum(), returns a host-side scalar ---------\ntime for .sum() 0.039\ntime for synchronize 0.000\n---- .sum(1), returns proxy to gpu-side tensor -----\ntime for .sum(1) 0.000\ntime for synchronize 0.462\n=================\nit 2\n------ .sum(), returns a host-side scalar ---------\ntime for .sum() 0.040\ntime for synchronize 0.000\n---- .sum(1), returns proxy to gpu-side tensor -----\ntime for .sum(1) 0.000\ntime for synchronize 0.456\n\nYou can see that .sum(1) returns instantly, and the pain doesnt happen till the later .synchronize(). But .sum() waits for the entire gpu operation to finish, and returns a host-side scalar.  .sum() is implicitly, behind the scenes:\n\nwaiting for the entire operation to finish\nsync'ing\ncopying the result from the gpu back to the hostside\n\nI'm not sure this is desirable?  It's also inconsistent with how numpy works, which doesnt convert scalars into a non-numpy scalar until you call .item().\nTest code for the implicit sync points:\nimport torch\nimport time\n\n\nN = 1000 * 1000 * 1000\n\ndef implicit_sync(print_res=False):\n    print('------ .sum(), returns a host-side scalar ---------')\n    # start = time.time()\n    # a = torch.ones(N).cuda()\n    # a += 1\n    # torch.cuda.synchronize()\n    start = time.time()\n    res = a.sum()\n    print('time for .sum() %.3f' % (time.time() - start))\n    start = time.time()\n    torch.cuda.synchronize()\n    print('time for synchronize %.3f' % (time.time() - start))\n    if print_res:\n        print('res', res)\n\ndef stays_on_gpu(print_res=False):\n    print('---- .sum(1), returns proxy to gpu-side tensor -----')\n    start = time.time()\n    res = a.sum(1)\n    print('time for .sum(1) %.3f' % (time.time() - start))\n    start = time.time()\n    torch.cuda.synchronize()\n    print('time for synchronize %.3f' % (time.time() - start))\n    if print_res:\n        print('res', res)\n\n\nif __name__ == '__main__':\n    a = torch.ones(1, N).cuda()\n    a += 1\n    torch.cuda.synchronize()\n    for it in range(3):\n        print('=================')\n        print('it', it)\n        implicit_sync(it == 0)\n        stays_on_gpu(it == 0)\n\nEffect of .item() in numpy:\nimport numpy as np\n\na = np.zeros(3)\nprint('type(a[0])', type(a[0]))\nprint('type(a[0].item())', type(a[0].item()))\n\nResult:\ntype(a[0]) <class 'numpy.float64'>\ntype(a[0].item()) <class 'float'>\n\nI know this would need a ton of work behind the scenes, and for conv nets, there is only one reduce, in the loss function.  It seems likely that it could have a noticeable effect on python code that does some kind of normalization using fundamental pytorch operations though?  So, this issue means such kernels need to be written in CUDA, rather than in Python, in order to obtain good performance. Fixing this issues would make it easier for Python guys to write high performance kernels for novel research layers.", "body": "Scalar reduction results are currently being returned as host-side scalars, causing an implicit sync-point, and gpu->device copy.  See the following results:\r\n\r\n```\r\n=================\r\nit 0\r\n------ .sum(), returns a host-side scalar ---------\r\ntime for .sum() 0.040\r\ntime for synchronize 0.000\r\nres 2000001152.0\r\n---- .sum(1), returns proxy to gpu-side tensor -----\r\ntime for .sum(1) 0.000\r\ntime for synchronize 0.462\r\nres\r\n 2.0000e+09\r\n[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\r\n\r\n=================\r\nit 1\r\n------ .sum(), returns a host-side scalar ---------\r\ntime for .sum() 0.039\r\ntime for synchronize 0.000\r\n---- .sum(1), returns proxy to gpu-side tensor -----\r\ntime for .sum(1) 0.000\r\ntime for synchronize 0.462\r\n=================\r\nit 2\r\n------ .sum(), returns a host-side scalar ---------\r\ntime for .sum() 0.040\r\ntime for synchronize 0.000\r\n---- .sum(1), returns proxy to gpu-side tensor -----\r\ntime for .sum(1) 0.000\r\ntime for synchronize 0.456\r\n```\r\n\r\nYou can see that `.sum(1)` returns instantly, and the pain doesnt happen till the later `.synchronize()`. But `.sum()` waits for the entire gpu operation to finish, and returns a host-side scalar.  `.sum()` is implicitly, behind the scenes:\r\n- waiting for the entire operation to finish\r\n- sync'ing\r\n- copying the result from the gpu back to the hostside\r\n\r\nI'm not sure this is desirable?  It's also inconsistent with how numpy works, which doesnt convert scalars into a non-numpy scalar until you call `.item()`.\r\n\r\nTest code for the implicit sync points:\r\n\r\n```\r\nimport torch\r\nimport time\r\n\r\n\r\nN = 1000 * 1000 * 1000\r\n\r\ndef implicit_sync(print_res=False):\r\n    print('------ .sum(), returns a host-side scalar ---------')\r\n    # start = time.time()\r\n    # a = torch.ones(N).cuda()\r\n    # a += 1\r\n    # torch.cuda.synchronize()\r\n    start = time.time()\r\n    res = a.sum()\r\n    print('time for .sum() %.3f' % (time.time() - start))\r\n    start = time.time()\r\n    torch.cuda.synchronize()\r\n    print('time for synchronize %.3f' % (time.time() - start))\r\n    if print_res:\r\n        print('res', res)\r\n\r\ndef stays_on_gpu(print_res=False):\r\n    print('---- .sum(1), returns proxy to gpu-side tensor -----')\r\n    start = time.time()\r\n    res = a.sum(1)\r\n    print('time for .sum(1) %.3f' % (time.time() - start))\r\n    start = time.time()\r\n    torch.cuda.synchronize()\r\n    print('time for synchronize %.3f' % (time.time() - start))\r\n    if print_res:\r\n        print('res', res)\r\n\r\n\r\nif __name__ == '__main__':\r\n    a = torch.ones(1, N).cuda()\r\n    a += 1\r\n    torch.cuda.synchronize()\r\n    for it in range(3):\r\n        print('=================')\r\n        print('it', it)\r\n        implicit_sync(it == 0)\r\n        stays_on_gpu(it == 0)\r\n```\r\n\r\nEffect of `.item()` in numpy:\r\n```\r\nimport numpy as np\r\n\r\na = np.zeros(3)\r\nprint('type(a[0])', type(a[0]))\r\nprint('type(a[0].item())', type(a[0].item()))\r\n```\r\nResult:\r\n```\r\ntype(a[0]) <class 'numpy.float64'>\r\ntype(a[0].item()) <class 'float'>\r\n```\r\n\r\nI know this would need a ton of work behind the scenes, and for conv nets, there is only one reduce, in the loss function.  It seems likely that it could have a noticeable effect on python code that does some kind of normalization using fundamental pytorch operations though?  So, this issue means such kernels need to be written in CUDA, rather than in Python, in order to obtain good performance. Fixing this issues would make it easier for Python guys to write high performance kernels for novel research layers."}