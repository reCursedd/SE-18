{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10925", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10925/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10925/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10925/events", "html_url": "https://github.com/pytorch/pytorch/issues/10925", "id": 354526204, "node_id": "MDU6SXNzdWUzNTQ1MjYyMDQ=", "number": 10925, "title": "Certain extensions to the torch.distributions API", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 819357941, "node_id": "MDU6TGFiZWw4MTkzNTc5NDE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributions", "name": "distributions", "color": "39d651", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-28T00:33:07Z", "updated_at": "2018-09-19T18:51:25Z", "closed_at": "2018-09-19T18:51:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We have made certain extensions (see <a href=\"https://github.com/uber/pyro/blob/dev/pyro/distributions/torch.py\">patched torch distributions</a>) to <code>torch.distributions</code> classes within Pyro, which we thought may be of benefit to the larger community and can be moved upstream.</p>\n<ul>\n<li><strong>Having a <code>distribution.expand</code> method</strong>: This is to support a method that dynamically changes the distribution's batch shape. On the backend, this will expand all the distribution's parameters to match the expanded <code>batch_shape</code>, handling lazy parameters appropriately. Note that this is much cheaper than constructing new instances (with expanded distribution parameters) which needs to do additional validation. Inside of Pyro, this is used for automatic broadcasting of distribution samples. A couple of users on the torch distributions slack channel have also expressed a need for this feature.</li>\n<li><strong>Adding a <code>expand=False</code> kwarg to <code>distribution.enumerate_support()</code></strong>: Currently enumerate support builds up tensors of size <code>event_shape + batch_shape</code>, but the values are repeated over the <code>batch_shape</code>. This can lead to expensive matrix operations over large tensors when <code>batch_shape</code> is large, e.g. when enumerating over the state space in a Hidden Markov Model. <code>expand=False</code> collapses the batch dims, creating high dimensional sparse tensors. This allows for the possibility of using optimized matrix operations over these sparse tensors, e.g. using <a href=\"https://github.com/dgasmith/opt_einsum\">opt_einsum</a> over such sparse tensors without resulting in a OOM issue.</li>\n</ul>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> - What do you think about adding these distribution methods? Both of these should be backward compatible. cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1093846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alicanb\">@alicanb</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a></p>", "body_text": "We have made certain extensions (see patched torch distributions) to torch.distributions classes within Pyro, which we thought may be of benefit to the larger community and can be moved upstream.\n\nHaving a distribution.expand method: This is to support a method that dynamically changes the distribution's batch shape. On the backend, this will expand all the distribution's parameters to match the expanded batch_shape, handling lazy parameters appropriately. Note that this is much cheaper than constructing new instances (with expanded distribution parameters) which needs to do additional validation. Inside of Pyro, this is used for automatic broadcasting of distribution samples. A couple of users on the torch distributions slack channel have also expressed a need for this feature.\nAdding a expand=False kwarg to distribution.enumerate_support(): Currently enumerate support builds up tensors of size event_shape + batch_shape, but the values are repeated over the batch_shape. This can lead to expensive matrix operations over large tensors when batch_shape is large, e.g. when enumerating over the state space in a Hidden Markov Model. expand=False collapses the batch dims, creating high dimensional sparse tensors. This allows for the possibility of using optimized matrix operations over these sparse tensors, e.g. using opt_einsum over such sparse tensors without resulting in a OOM issue.\n\n@apaszke - What do you think about adding these distribution methods? Both of these should be backward compatible. cc. @alicanb, @fritzo", "body": "We have made certain extensions (see [patched torch distributions](https://github.com/uber/pyro/blob/dev/pyro/distributions/torch.py)) to `torch.distributions` classes within Pyro, which we thought may be of benefit to the larger community and can be moved upstream.\r\n - **Having a `distribution.expand` method**: This is to support a method that dynamically changes the distribution's batch shape. On the backend, this will expand all the distribution's parameters to match the expanded `batch_shape`, handling lazy parameters appropriately. Note that this is much cheaper than constructing new instances (with expanded distribution parameters) which needs to do additional validation. Inside of Pyro, this is used for automatic broadcasting of distribution samples. A couple of users on the torch distributions slack channel have also expressed a need for this feature.\r\n - **Adding a `expand=False` kwarg to `distribution.enumerate_support()`**: Currently enumerate support builds up tensors of size `event_shape + batch_shape`, but the values are repeated over the `batch_shape`. This can lead to expensive matrix operations over large tensors when `batch_shape` is large, e.g. when enumerating over the state space in a Hidden Markov Model. `expand=False` collapses the batch dims, creating high dimensional sparse tensors. This allows for the possibility of using optimized matrix operations over these sparse tensors, e.g. using [opt_einsum](https://github.com/dgasmith/opt_einsum) over such sparse tensors without resulting in a OOM issue.\r\n\r\n@apaszke - What do you think about adding these distribution methods? Both of these should be backward compatible. cc. @alicanb, @fritzo "}