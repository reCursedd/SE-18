{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/433156675", "html_url": "https://github.com/pytorch/pytorch/issues/3858#issuecomment-433156675", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3858", "id": 433156675, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzE1NjY3NQ==", "user": {"login": "Douphoton", "id": 15165859, "node_id": "MDQ6VXNlcjE1MTY1ODU5", "avatar_url": "https://avatars0.githubusercontent.com/u/15165859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Douphoton", "html_url": "https://github.com/Douphoton", "followers_url": "https://api.github.com/users/Douphoton/followers", "following_url": "https://api.github.com/users/Douphoton/following{/other_user}", "gists_url": "https://api.github.com/users/Douphoton/gists{/gist_id}", "starred_url": "https://api.github.com/users/Douphoton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Douphoton/subscriptions", "organizations_url": "https://api.github.com/users/Douphoton/orgs", "repos_url": "https://api.github.com/users/Douphoton/repos", "events_url": "https://api.github.com/users/Douphoton/events{/privacy}", "received_events_url": "https://api.github.com/users/Douphoton/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-25T18:25:33Z", "updated_at": "2018-10-25T18:25:33Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Here's a simple and pretty fast implementation for <code>[H, W]</code> shaped arrays:</p>\n<pre><code>def pad_circular(x, pad):\n    \"\"\"\n\n    :param x: shape [H, W]\n    :param pad: int &gt;= 0\n    :return:\n    \"\"\"\n    x = torch.cat([x, x[0:pad]], dim=0)\n    x = torch.cat([x, x[:, 0:pad]], dim=1)\n    x = torch.cat([x[-2 * pad:-pad], x], dim=0)\n    x = torch.cat([x[:, -2 * pad:-pad], x], dim=1)\n\n    return x\n</code></pre>\n<p>For a <code>1000x1000</code> array I get this:</p>\n<pre><code>N = 1000\n\nx = torch.randn(N, N).cuda()\n\n%timeit pad_circular(x, 1)\n\n&gt; 38 \u00b5s \u00b1 1.53 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre>\n<p>For the <code>repeat</code> way above I get CUDA OOM error...</p>\n</blockquote>\n<p>Is it autograd compatible? Do we need to implement backward function for circular padding?</p>", "body_text": "Here's a simple and pretty fast implementation for [H, W] shaped arrays:\ndef pad_circular(x, pad):\n    \"\"\"\n\n    :param x: shape [H, W]\n    :param pad: int >= 0\n    :return:\n    \"\"\"\n    x = torch.cat([x, x[0:pad]], dim=0)\n    x = torch.cat([x, x[:, 0:pad]], dim=1)\n    x = torch.cat([x[-2 * pad:-pad], x], dim=0)\n    x = torch.cat([x[:, -2 * pad:-pad], x], dim=1)\n\n    return x\n\nFor a 1000x1000 array I get this:\nN = 1000\n\nx = torch.randn(N, N).cuda()\n\n%timeit pad_circular(x, 1)\n\n> 38 \u00b5s \u00b1 1.53 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nFor the repeat way above I get CUDA OOM error...\n\nIs it autograd compatible? Do we need to implement backward function for circular padding?", "body": "> Here's a simple and pretty fast implementation for `[H, W]` shaped arrays:\r\n> \r\n> ```\r\n> def pad_circular(x, pad):\r\n>     \"\"\"\r\n> \r\n>     :param x: shape [H, W]\r\n>     :param pad: int >= 0\r\n>     :return:\r\n>     \"\"\"\r\n>     x = torch.cat([x, x[0:pad]], dim=0)\r\n>     x = torch.cat([x, x[:, 0:pad]], dim=1)\r\n>     x = torch.cat([x[-2 * pad:-pad], x], dim=0)\r\n>     x = torch.cat([x[:, -2 * pad:-pad], x], dim=1)\r\n> \r\n>     return x\r\n> ```\r\n> For a `1000x1000` array I get this:\r\n> \r\n> ```\r\n> N = 1000\r\n> \r\n> x = torch.randn(N, N).cuda()\r\n> \r\n> %timeit pad_circular(x, 1)\r\n> \r\n> > 38 \u00b5s \u00b1 1.53 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n> ```\r\n> For the `repeat` way above I get CUDA OOM error...\r\n\r\nIs it autograd compatible? Do we need to implement backward function for circular padding?\r\n"}