{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11362", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11362/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11362/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11362/events", "html_url": "https://github.com/pytorch/pytorch/issues/11362", "id": 357894895, "node_id": "MDU6SXNzdWUzNTc4OTQ4OTU=", "number": 11362, "title": "OnnxBackendNodeModelTest.test_dynamic_slice_cpu  is flaky on CircleCI", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-09-07T02:21:03Z", "updated_at": "2018-09-25T17:26:57Z", "closed_at": "2018-09-25T17:26:57Z", "author_association": "CONTRIBUTOR", "body_html": "<pre><code>\n=================================== FAILURES ===================================\n_______________ OnnxBackendNodeModelTest.test_dynamic_slice_cpu ________________\n\nargs = (&lt;caffe2.python.onnx.tests.onnx_backend_test.OnnxBackendNodeModelTest testMethod=test_dynamic_slice_cpu&gt;,)\nkwargs = {}\n\n    @unittest.skipIf(  # type: ignore\n        not self.backend.supports_device(device),\n        \"Backend doesn't support device {}\".format(device))\n    @functools.wraps(test_func)\n    def device_test_func(*args, **kwargs):  # type: (*Any, **Any) -&gt; Any\n        try:\n&gt;           return test_func(*args, device=device, **kwargs)\n\n../.local/lib/python2.7/site-packages/onnx/backend/test/runner/__init__.py:245: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../.local/lib/python2.7/site-packages/onnx/backend/test/runner/__init__.py:275: in run\n    prepared_model = self.backend.prepare(model, device)\n/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/onnx/backend.py:716: in prepare\n    init_net, predict_net = cls._onnx_model_to_caffe2_net(model, device, opset_version, False)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = &lt;class 'caffe2.python.onnx.backend.Caffe2Backend'&gt;\nonnx_model = ir_version: 3\nproducer_name: \"backend-test\"\ngraph {\n  node {\n    input: \"x\"\n  ...    }\n        }\n      }\n    }\n  }\n}\nopset_import {\n  domain: \"\"\n  version: 9\n}\n\ndevice = 'CPU', opset_version = 9L, include_initializers = False\n\n    @classmethod\n    def _onnx_model_to_caffe2_net(cls, onnx_model, device, opset_version, include_initializers):\n        device_option = get_device_option(Device(device))\n    \n        onnx_model = onnx.utils.polish_model(onnx_model)\n        init_model = cls.optimize_onnx(onnx_model, init=True)\n        pred_model = cls.optimize_onnx(onnx_model, predict=True)\n    \n        init_net = caffe2_pb2.NetDef()\n        pred_net = caffe2_pb2.NetDef()\n    \n        init_net.name = onnx_model.graph.name + '_init'\n        pred_net.name = onnx_model.graph.name + '_predict'\n    \n        if include_initializers:\n            init_net.op.extend(cls._create_tensor_filling_op(tp) for tp in onnx_model.graph.initializer)\n    \n        cls._dummy_name.reset(cls._all_names_in_graph(init_model.graph) | cls._all_names_in_graph(pred_model.graph))\n    \n        success = True\n        for net, model in ( (init_net, init_model), (pred_net, pred_model) ):\n            net.device_option.CopyFrom(device_option)\n            for node in model.graph.node:\n                try:\n                    c2ops = cls._onnx_node_to_caffe2_op(\n                        init_model, pred_model, node, opset_version)\n                except Exception as e:\n                    success = False\n                    print('ONNX FATAL:', e)\n                    continue\n                init_net.op.extend(c2ops.init_ops)\n                net.op.extend(c2ops.ops)\n                net.external_input.extend(c2ops.interface_blobs)\n            net.external_output.extend(\n                value_info.name for value_info in model.graph.output)\n            net.external_input.extend(\n                value_info.name for value_info in model.graph.input)\n    \n        if not success:\n&gt;           raise RuntimeError('ONNX conversion failed')\nE           RuntimeError: ONNX conversion failed\n</code></pre>\n<p>Sample log <a href=\"https://circleci.com/gh/pytorch/pytorch/13179?utm_campaign=vcs-integration-link&amp;utm_medium=referral&amp;utm_source=github-build-link\" rel=\"nofollow\">https://circleci.com/gh/pytorch/pytorch/13179?utm_campaign=vcs-integration-link&amp;utm_medium=referral&amp;utm_source=github-build-link</a></p>", "body_text": "=================================== FAILURES ===================================\n_______________ OnnxBackendNodeModelTest.test_dynamic_slice_cpu ________________\n\nargs = (<caffe2.python.onnx.tests.onnx_backend_test.OnnxBackendNodeModelTest testMethod=test_dynamic_slice_cpu>,)\nkwargs = {}\n\n    @unittest.skipIf(  # type: ignore\n        not self.backend.supports_device(device),\n        \"Backend doesn't support device {}\".format(device))\n    @functools.wraps(test_func)\n    def device_test_func(*args, **kwargs):  # type: (*Any, **Any) -> Any\n        try:\n>           return test_func(*args, device=device, **kwargs)\n\n../.local/lib/python2.7/site-packages/onnx/backend/test/runner/__init__.py:245: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../.local/lib/python2.7/site-packages/onnx/backend/test/runner/__init__.py:275: in run\n    prepared_model = self.backend.prepare(model, device)\n/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/onnx/backend.py:716: in prepare\n    init_net, predict_net = cls._onnx_model_to_caffe2_net(model, device, opset_version, False)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncls = <class 'caffe2.python.onnx.backend.Caffe2Backend'>\nonnx_model = ir_version: 3\nproducer_name: \"backend-test\"\ngraph {\n  node {\n    input: \"x\"\n  ...    }\n        }\n      }\n    }\n  }\n}\nopset_import {\n  domain: \"\"\n  version: 9\n}\n\ndevice = 'CPU', opset_version = 9L, include_initializers = False\n\n    @classmethod\n    def _onnx_model_to_caffe2_net(cls, onnx_model, device, opset_version, include_initializers):\n        device_option = get_device_option(Device(device))\n    \n        onnx_model = onnx.utils.polish_model(onnx_model)\n        init_model = cls.optimize_onnx(onnx_model, init=True)\n        pred_model = cls.optimize_onnx(onnx_model, predict=True)\n    \n        init_net = caffe2_pb2.NetDef()\n        pred_net = caffe2_pb2.NetDef()\n    \n        init_net.name = onnx_model.graph.name + '_init'\n        pred_net.name = onnx_model.graph.name + '_predict'\n    \n        if include_initializers:\n            init_net.op.extend(cls._create_tensor_filling_op(tp) for tp in onnx_model.graph.initializer)\n    \n        cls._dummy_name.reset(cls._all_names_in_graph(init_model.graph) | cls._all_names_in_graph(pred_model.graph))\n    \n        success = True\n        for net, model in ( (init_net, init_model), (pred_net, pred_model) ):\n            net.device_option.CopyFrom(device_option)\n            for node in model.graph.node:\n                try:\n                    c2ops = cls._onnx_node_to_caffe2_op(\n                        init_model, pred_model, node, opset_version)\n                except Exception as e:\n                    success = False\n                    print('ONNX FATAL:', e)\n                    continue\n                init_net.op.extend(c2ops.init_ops)\n                net.op.extend(c2ops.ops)\n                net.external_input.extend(c2ops.interface_blobs)\n            net.external_output.extend(\n                value_info.name for value_info in model.graph.output)\n            net.external_input.extend(\n                value_info.name for value_info in model.graph.input)\n    \n        if not success:\n>           raise RuntimeError('ONNX conversion failed')\nE           RuntimeError: ONNX conversion failed\n\nSample log https://circleci.com/gh/pytorch/pytorch/13179?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link", "body": "```\r\n\r\n=================================== FAILURES ===================================\r\n_______________ OnnxBackendNodeModelTest.test_dynamic_slice_cpu ________________\r\n\r\nargs = (<caffe2.python.onnx.tests.onnx_backend_test.OnnxBackendNodeModelTest testMethod=test_dynamic_slice_cpu>,)\r\nkwargs = {}\r\n\r\n    @unittest.skipIf(  # type: ignore\r\n        not self.backend.supports_device(device),\r\n        \"Backend doesn't support device {}\".format(device))\r\n    @functools.wraps(test_func)\r\n    def device_test_func(*args, **kwargs):  # type: (*Any, **Any) -> Any\r\n        try:\r\n>           return test_func(*args, device=device, **kwargs)\r\n\r\n../.local/lib/python2.7/site-packages/onnx/backend/test/runner/__init__.py:245: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../.local/lib/python2.7/site-packages/onnx/backend/test/runner/__init__.py:275: in run\r\n    prepared_model = self.backend.prepare(model, device)\r\n/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/onnx/backend.py:716: in prepare\r\n    init_net, predict_net = cls._onnx_model_to_caffe2_net(model, device, opset_version, False)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ncls = <class 'caffe2.python.onnx.backend.Caffe2Backend'>\r\nonnx_model = ir_version: 3\r\nproducer_name: \"backend-test\"\r\ngraph {\r\n  node {\r\n    input: \"x\"\r\n  ...    }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nopset_import {\r\n  domain: \"\"\r\n  version: 9\r\n}\r\n\r\ndevice = 'CPU', opset_version = 9L, include_initializers = False\r\n\r\n    @classmethod\r\n    def _onnx_model_to_caffe2_net(cls, onnx_model, device, opset_version, include_initializers):\r\n        device_option = get_device_option(Device(device))\r\n    \r\n        onnx_model = onnx.utils.polish_model(onnx_model)\r\n        init_model = cls.optimize_onnx(onnx_model, init=True)\r\n        pred_model = cls.optimize_onnx(onnx_model, predict=True)\r\n    \r\n        init_net = caffe2_pb2.NetDef()\r\n        pred_net = caffe2_pb2.NetDef()\r\n    \r\n        init_net.name = onnx_model.graph.name + '_init'\r\n        pred_net.name = onnx_model.graph.name + '_predict'\r\n    \r\n        if include_initializers:\r\n            init_net.op.extend(cls._create_tensor_filling_op(tp) for tp in onnx_model.graph.initializer)\r\n    \r\n        cls._dummy_name.reset(cls._all_names_in_graph(init_model.graph) | cls._all_names_in_graph(pred_model.graph))\r\n    \r\n        success = True\r\n        for net, model in ( (init_net, init_model), (pred_net, pred_model) ):\r\n            net.device_option.CopyFrom(device_option)\r\n            for node in model.graph.node:\r\n                try:\r\n                    c2ops = cls._onnx_node_to_caffe2_op(\r\n                        init_model, pred_model, node, opset_version)\r\n                except Exception as e:\r\n                    success = False\r\n                    print('ONNX FATAL:', e)\r\n                    continue\r\n                init_net.op.extend(c2ops.init_ops)\r\n                net.op.extend(c2ops.ops)\r\n                net.external_input.extend(c2ops.interface_blobs)\r\n            net.external_output.extend(\r\n                value_info.name for value_info in model.graph.output)\r\n            net.external_input.extend(\r\n                value_info.name for value_info in model.graph.input)\r\n    \r\n        if not success:\r\n>           raise RuntimeError('ONNX conversion failed')\r\nE           RuntimeError: ONNX conversion failed\r\n```\r\n\r\nSample log https://circleci.com/gh/pytorch/pytorch/13179?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link"}