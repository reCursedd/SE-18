{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10586", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10586/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10586/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10586/events", "html_url": "https://github.com/pytorch/pytorch/issues/10586", "id": 351303333, "node_id": "MDU6SXNzdWUzNTEzMDMzMzM=", "number": 10586, "title": "in-place operator \"+=\" breaks autograd", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-16T17:42:33Z", "updated_at": "2018-08-20T17:47:29Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>We found an autograd issue with in-place operator \"+=\" when a custom layer is present.<br>\nLooks like the autograd gets cut off after the in-place operator.</p>\n<h2>Code example</h2>\n<p>Runtime issue:<br>\nonly forward function of AddKernel is triggered.</p>\n<p>repro code:</p>\n<pre><code>  1 import torch\n  2\n  3 '''\n  4 The following model breaks autograd.\n  5\n  6 During execution, we run both forward (LINE:71) and backward (LINE:73)\n  7 we observe only the forward pass of our custom layer has been executed, while\n  8 the backward pass does not gets triggered by.\n  9 '''\n 10\n 11 class AddKernel(torch.autograd.Function):\n 12     @staticmethod\n 13     def forward(ctx, input, val):\n 14         print(\"forward\")\n 15         out = input.transpose(-1, 1)\n 16         ctx.save_for_backward(input, val)\n 17         out = out.add(val)\n 18         # For some reason, issue 1 only apears when we transpose the clone of a\n 19         # tensor in this very specific way\n 20         # Swapping it with any of the commented line below could make autograd\n 21         # functioning properly again.\n 22         out = out.clone().transpose(-1, 1)   #busted\n 23         #out = out.clone().transpose_(-1, 1) #working fine\n 24         #out = out.transpose(-1, 1).clone()  #working fine\n 25         return out\n 26\n 27     @staticmethod\n 28     def backward(ctx, grad_output):\n 29         print(\"backward\")\n 30         inp, val = ctx.saved_tensors\n 31         grad_input = torch.rand_like(inp)\n 32         grad_val = torch.rand_like(val)\n 33         return grad_input, grad_val\n 34\n 35\n 36 class AddLayer(torch.nn.Module):\n 37     def __init__(self, num_features):\n 38         super(AddLayer, self).__init__()\n 39         self.weight = torch.nn.parameter.Parameter(torch.Tensor(num_features))\n 40     def forward(self, input):\n 41         return AddKernel.apply(input, self.weight)\n 42\n 43\n 44 class ResidualAddLayer(torch.nn.Module):\n 45     def __init__(self):\n 46         super(ResidualAddLayer, self).__init__()\n 47         layers = []\n 48         layers.append(torch.nn.Sequential(\n 49             AddLayer(10),\n 50         ))\n 51         self.model = torch.nn.Sequential(*layers)\n 52\n 53     def forward(self, x):\n 54         residual = x\n 55         out = self.model(x)\n 56         # issue 1: in-place operator '+=' used here cut off autograd tracing\n 57         #          at run time, only AddKernel::forward function is executed.\n 58         # swap it with the commented line below would work fine.\n 59         # At run time, both AddKernel::forward &amp; AddKernel::backward would be\n 60         # executed.\n 61         out += residual       # busted\n 62         #out = out + residual # working fine\n 63         return out\n 64\n 65 dtype = torch.float\n 66 device = torch.device(\"cpu\")\n 67 N, C, H, W= 5, 10, 10, 10\n 68 x = torch.randn(N, C, H, W, device=device, dtype=dtype)\n 69 model = ResidualAddLayer()\n 70 optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n 71 y_pred = model(x)\n 72 loss = y_pred - 1.0\n 73 loss.backward(loss)\n</code></pre>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.5.0a0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.5 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: Tesla V100-DGXS-16GB<br>\nGPU 1: Tesla V100-DGXS-16GB<br>\nGPU 2: Tesla V100-DGXS-16GB<br>\nGPU 3: Tesla V100-DGXS-16GB</p>\n<p>Versions of relevant libraries:<br>\n[pip] msgpack-numpy (0.4.3)<br>\n[pip] numpy (1.15.0)<br>\n[pip] torch (0.5.0a0)<br>\n[pip] torchtext (0.3.0)<br>\n[pip] torchvision (0.2.1)</p>\n<ul>\n<li>PyTorch or Caffe2:<br>\nPyTorch</li>\n</ul>", "body_text": "Issue description\nWe found an autograd issue with in-place operator \"+=\" when a custom layer is present.\nLooks like the autograd gets cut off after the in-place operator.\nCode example\nRuntime issue:\nonly forward function of AddKernel is triggered.\nrepro code:\n  1 import torch\n  2\n  3 '''\n  4 The following model breaks autograd.\n  5\n  6 During execution, we run both forward (LINE:71) and backward (LINE:73)\n  7 we observe only the forward pass of our custom layer has been executed, while\n  8 the backward pass does not gets triggered by.\n  9 '''\n 10\n 11 class AddKernel(torch.autograd.Function):\n 12     @staticmethod\n 13     def forward(ctx, input, val):\n 14         print(\"forward\")\n 15         out = input.transpose(-1, 1)\n 16         ctx.save_for_backward(input, val)\n 17         out = out.add(val)\n 18         # For some reason, issue 1 only apears when we transpose the clone of a\n 19         # tensor in this very specific way\n 20         # Swapping it with any of the commented line below could make autograd\n 21         # functioning properly again.\n 22         out = out.clone().transpose(-1, 1)   #busted\n 23         #out = out.clone().transpose_(-1, 1) #working fine\n 24         #out = out.transpose(-1, 1).clone()  #working fine\n 25         return out\n 26\n 27     @staticmethod\n 28     def backward(ctx, grad_output):\n 29         print(\"backward\")\n 30         inp, val = ctx.saved_tensors\n 31         grad_input = torch.rand_like(inp)\n 32         grad_val = torch.rand_like(val)\n 33         return grad_input, grad_val\n 34\n 35\n 36 class AddLayer(torch.nn.Module):\n 37     def __init__(self, num_features):\n 38         super(AddLayer, self).__init__()\n 39         self.weight = torch.nn.parameter.Parameter(torch.Tensor(num_features))\n 40     def forward(self, input):\n 41         return AddKernel.apply(input, self.weight)\n 42\n 43\n 44 class ResidualAddLayer(torch.nn.Module):\n 45     def __init__(self):\n 46         super(ResidualAddLayer, self).__init__()\n 47         layers = []\n 48         layers.append(torch.nn.Sequential(\n 49             AddLayer(10),\n 50         ))\n 51         self.model = torch.nn.Sequential(*layers)\n 52\n 53     def forward(self, x):\n 54         residual = x\n 55         out = self.model(x)\n 56         # issue 1: in-place operator '+=' used here cut off autograd tracing\n 57         #          at run time, only AddKernel::forward function is executed.\n 58         # swap it with the commented line below would work fine.\n 59         # At run time, both AddKernel::forward & AddKernel::backward would be\n 60         # executed.\n 61         out += residual       # busted\n 62         #out = out + residual # working fine\n 63         return out\n 64\n 65 dtype = torch.float\n 66 device = torch.device(\"cpu\")\n 67 N, C, H, W= 5, 10, 10, 10\n 68 x = torch.randn(N, C, H, W, device=device, dtype=dtype)\n 69 model = ResidualAddLayer()\n 70 optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\n 71 y_pred = model(x)\n 72 loss = y_pred - 1.0\n 73 loss.backward(loss)\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.5.0a0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: Tesla V100-DGXS-16GB\nGPU 1: Tesla V100-DGXS-16GB\nGPU 2: Tesla V100-DGXS-16GB\nGPU 3: Tesla V100-DGXS-16GB\nVersions of relevant libraries:\n[pip] msgpack-numpy (0.4.3)\n[pip] numpy (1.15.0)\n[pip] torch (0.5.0a0)\n[pip] torchtext (0.3.0)\n[pip] torchvision (0.2.1)\n\nPyTorch or Caffe2:\nPyTorch", "body": "## Issue description\r\n\r\nWe found an autograd issue with in-place operator \"+=\" when a custom layer is present.\r\nLooks like the autograd gets cut off after the in-place operator.\r\n\r\n## Code example\r\n\r\nRuntime issue:\r\nonly forward function of AddKernel is triggered.\r\n \r\nrepro code:\r\n\r\n```\r\n  1 import torch\r\n  2\r\n  3 '''\r\n  4 The following model breaks autograd.\r\n  5\r\n  6 During execution, we run both forward (LINE:71) and backward (LINE:73)\r\n  7 we observe only the forward pass of our custom layer has been executed, while\r\n  8 the backward pass does not gets triggered by.\r\n  9 '''\r\n 10\r\n 11 class AddKernel(torch.autograd.Function):\r\n 12     @staticmethod\r\n 13     def forward(ctx, input, val):\r\n 14         print(\"forward\")\r\n 15         out = input.transpose(-1, 1)\r\n 16         ctx.save_for_backward(input, val)\r\n 17         out = out.add(val)\r\n 18         # For some reason, issue 1 only apears when we transpose the clone of a\r\n 19         # tensor in this very specific way\r\n 20         # Swapping it with any of the commented line below could make autograd\r\n 21         # functioning properly again.\r\n 22         out = out.clone().transpose(-1, 1)   #busted\r\n 23         #out = out.clone().transpose_(-1, 1) #working fine\r\n 24         #out = out.transpose(-1, 1).clone()  #working fine\r\n 25         return out\r\n 26\r\n 27     @staticmethod\r\n 28     def backward(ctx, grad_output):\r\n 29         print(\"backward\")\r\n 30         inp, val = ctx.saved_tensors\r\n 31         grad_input = torch.rand_like(inp)\r\n 32         grad_val = torch.rand_like(val)\r\n 33         return grad_input, grad_val\r\n 34\r\n 35\r\n 36 class AddLayer(torch.nn.Module):\r\n 37     def __init__(self, num_features):\r\n 38         super(AddLayer, self).__init__()\r\n 39         self.weight = torch.nn.parameter.Parameter(torch.Tensor(num_features))\r\n 40     def forward(self, input):\r\n 41         return AddKernel.apply(input, self.weight)\r\n 42\r\n 43\r\n 44 class ResidualAddLayer(torch.nn.Module):\r\n 45     def __init__(self):\r\n 46         super(ResidualAddLayer, self).__init__()\r\n 47         layers = []\r\n 48         layers.append(torch.nn.Sequential(\r\n 49             AddLayer(10),\r\n 50         ))\r\n 51         self.model = torch.nn.Sequential(*layers)\r\n 52\r\n 53     def forward(self, x):\r\n 54         residual = x\r\n 55         out = self.model(x)\r\n 56         # issue 1: in-place operator '+=' used here cut off autograd tracing\r\n 57         #          at run time, only AddKernel::forward function is executed.\r\n 58         # swap it with the commented line below would work fine.\r\n 59         # At run time, both AddKernel::forward & AddKernel::backward would be\r\n 60         # executed.\r\n 61         out += residual       # busted\r\n 62         #out = out + residual # working fine\r\n 63         return out\r\n 64\r\n 65 dtype = torch.float\r\n 66 device = torch.device(\"cpu\")\r\n 67 N, C, H, W= 5, 10, 10, 10\r\n 68 x = torch.randn(N, C, H, W, device=device, dtype=dtype)\r\n 69 model = ResidualAddLayer()\r\n 70 optimizer = torch.optim.SGD(model.parameters(), lr = 1e-1, momentum=0.9)\r\n 71 y_pred = model(x)\r\n 72 loss = y_pred - 1.0\r\n 73 loss.backward(loss)\r\n```\r\n\r\n## System Info\r\n\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-DGXS-16GB\r\nGPU 1: Tesla V100-DGXS-16GB\r\nGPU 2: Tesla V100-DGXS-16GB\r\nGPU 3: Tesla V100-DGXS-16GB\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy (0.4.3)\r\n[pip] numpy (1.15.0)\r\n[pip] torch (0.5.0a0)\r\n[pip] torchtext (0.3.0)\r\n[pip] torchvision (0.2.1)\r\n\r\n\r\n- PyTorch or Caffe2:\r\nPyTorch\r\n"}