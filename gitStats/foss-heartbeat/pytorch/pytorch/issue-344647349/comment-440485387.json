{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440485387", "html_url": "https://github.com/pytorch/pytorch/pull/9849#issuecomment-440485387", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9849", "id": 440485387, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDQ4NTM4Nw==", "user": {"login": "hayatoikoma", "id": 2889812, "node_id": "MDQ6VXNlcjI4ODk4MTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2889812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hayatoikoma", "html_url": "https://github.com/hayatoikoma", "followers_url": "https://api.github.com/users/hayatoikoma/followers", "following_url": "https://api.github.com/users/hayatoikoma/following{/other_user}", "gists_url": "https://api.github.com/users/hayatoikoma/gists{/gist_id}", "starred_url": "https://api.github.com/users/hayatoikoma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hayatoikoma/subscriptions", "organizations_url": "https://api.github.com/users/hayatoikoma/orgs", "repos_url": "https://api.github.com/users/hayatoikoma/repos", "events_url": "https://api.github.com/users/hayatoikoma/events{/privacy}", "received_events_url": "https://api.github.com/users/hayatoikoma/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-21T00:48:35Z", "updated_at": "2018-11-21T00:48:35Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9407960\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/driazati\">@driazati</a> I just tried out the current version and recognized that it works on the case I showed yesterday, but it seems to still have an issue for the batch dimension. FYI.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.interpolate(a, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bicubic<span class=\"pl-pds\">'</span></span>)\ntensor([[[[<span class=\"pl-c1\">0.3577</span>, <span class=\"pl-c1\">0.2687</span>, <span class=\"pl-c1\">0.2801</span>, <span class=\"pl-c1\">0.5409</span>, <span class=\"pl-c1\">0.8162</span>, <span class=\"pl-c1\">0.8665</span>],\n          [<span class=\"pl-c1\">0.2349</span>, <span class=\"pl-c1\">0.4151</span>, <span class=\"pl-c1\">0.5672</span>, <span class=\"pl-c1\">0.5236</span>, <span class=\"pl-c1\">0.4177</span>, <span class=\"pl-c1\">0.4037</span>],\n          [<span class=\"pl-c1\">0.1034</span>, <span class=\"pl-c1\">0.5432</span>, <span class=\"pl-c1\">0.8430</span>, <span class=\"pl-c1\">0.5395</span>, <span class=\"pl-c1\">0.0973</span>, <span class=\"pl-c1\">0.0273</span>],\n          [<span class=\"pl-c1\">0.0566</span>, <span class=\"pl-c1\">0.5201</span>, <span class=\"pl-c1\">0.8659</span>, <span class=\"pl-c1\">0.6279</span>, <span class=\"pl-c1\">0.2382</span>, <span class=\"pl-c1\">0.1794</span>],\n          [<span class=\"pl-c1\">0.0573</span>, <span class=\"pl-c1\">0.4455</span>, <span class=\"pl-c1\">0.7832</span>, <span class=\"pl-c1\">0.7166</span>, <span class=\"pl-c1\">0.5140</span>, <span class=\"pl-c1\">0.4887</span>],\n          [<span class=\"pl-c1\">0.0530</span>, <span class=\"pl-c1\">0.4364</span>, <span class=\"pl-c1\">0.7776</span>, <span class=\"pl-c1\">0.7332</span>, <span class=\"pl-c1\">0.5530</span>, <span class=\"pl-c1\">0.5320</span>]],\n\n         [[<span class=\"pl-c1\">0.4682</span>, <span class=\"pl-c1\">0.4246</span>, <span class=\"pl-c1\">0.4658</span>, <span class=\"pl-c1\">0.6916</span>, <span class=\"pl-c1\">0.9178</span>, <span class=\"pl-c1\">0.9602</span>],\n          [<span class=\"pl-c1\">0.3820</span>, <span class=\"pl-c1\">0.4956</span>, <span class=\"pl-c1\">0.5896</span>, <span class=\"pl-c1\">0.5566</span>, <span class=\"pl-c1\">0.4847</span>, <span class=\"pl-c1\">0.4748</span>],\n          [<span class=\"pl-c1\">0.4000</span>, <span class=\"pl-c1\">0.6286</span>, <span class=\"pl-c1\">0.7455</span>, <span class=\"pl-c1\">0.4800</span>, <span class=\"pl-c1\">0.1498</span>, <span class=\"pl-c1\">0.0940</span>],\n          [<span class=\"pl-c1\">0.6717</span>, <span class=\"pl-c1\">0.8128</span>, <span class=\"pl-c1\">0.8569</span>, <span class=\"pl-c1\">0.6158</span>, <span class=\"pl-c1\">0.3400</span>, <span class=\"pl-c1\">0.2916</span>],\n          [<span class=\"pl-c1\">0.9563</span>, <span class=\"pl-c1\">0.9588</span>, <span class=\"pl-c1\">0.9159</span>, <span class=\"pl-c1\">0.7913</span>, <span class=\"pl-c1\">0.6742</span>, <span class=\"pl-c1\">0.6516</span>],\n          [<span class=\"pl-c1\">1.0084</span>, <span class=\"pl-c1\">0.9897</span>, <span class=\"pl-c1\">0.9319</span>, <span class=\"pl-c1\">0.8205</span>, <span class=\"pl-c1\">0.7234</span>, <span class=\"pl-c1\">0.7038</span>]]]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.interpolate(a, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bicubic<span class=\"pl-pds\">'</span></span>)\ntensor([[[[<span class=\"pl-c1\">0.7897</span>, <span class=\"pl-c1\">0.8228</span>, <span class=\"pl-c1\">0.8356</span>, <span class=\"pl-c1\">0.7856</span>, <span class=\"pl-c1\">0.7270</span>, <span class=\"pl-c1\">0.7168</span>],\n          [<span class=\"pl-c1\">0.4667</span>, <span class=\"pl-c1\">0.6900</span>, <span class=\"pl-c1\">0.8496</span>, <span class=\"pl-c1\">0.7160</span>, <span class=\"pl-c1\">0.5106</span>, <span class=\"pl-c1\">0.4788</span>],\n          [<span class=\"pl-c1\">0.1671</span>, <span class=\"pl-c1\">0.5446</span>, <span class=\"pl-c1\">0.8217</span>, <span class=\"pl-c1\">0.6151</span>, <span class=\"pl-c1\">0.2859</span>, <span class=\"pl-c1\">0.2357</span>],\n          [<span class=\"pl-c1\">0.1708</span>, <span class=\"pl-c1\">0.4850</span>, <span class=\"pl-c1\">0.7082</span>, <span class=\"pl-c1\">0.5158</span>, <span class=\"pl-c1\">0.2227</span>, <span class=\"pl-c1\">0.1771</span>],\n          [<span class=\"pl-c1\">0.2914</span>, <span class=\"pl-c1\">0.4776</span>, <span class=\"pl-c1\">0.5973</span>, <span class=\"pl-c1\">0.4484</span>, <span class=\"pl-c1\">0.2421</span>, <span class=\"pl-c1\">0.2088</span>],\n          [<span class=\"pl-c1\">0.3030</span>, <span class=\"pl-c1\">0.4714</span>, <span class=\"pl-c1\">0.5763</span>, <span class=\"pl-c1\">0.4328</span>, <span class=\"pl-c1\">0.2380</span>, <span class=\"pl-c1\">0.2063</span>]]],\n\n\n        [[[<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>]]]])</pre></div>", "body_text": "@driazati I just tried out the current version and recognized that it works on the case I showed yesterday, but it seems to still have an issue for the batch dimension. FYI.\n>>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.rand(1,2,3,3)\n>>> F.interpolate(a, scale_factor=2, mode='bicubic')\ntensor([[[[0.3577, 0.2687, 0.2801, 0.5409, 0.8162, 0.8665],\n          [0.2349, 0.4151, 0.5672, 0.5236, 0.4177, 0.4037],\n          [0.1034, 0.5432, 0.8430, 0.5395, 0.0973, 0.0273],\n          [0.0566, 0.5201, 0.8659, 0.6279, 0.2382, 0.1794],\n          [0.0573, 0.4455, 0.7832, 0.7166, 0.5140, 0.4887],\n          [0.0530, 0.4364, 0.7776, 0.7332, 0.5530, 0.5320]],\n\n         [[0.4682, 0.4246, 0.4658, 0.6916, 0.9178, 0.9602],\n          [0.3820, 0.4956, 0.5896, 0.5566, 0.4847, 0.4748],\n          [0.4000, 0.6286, 0.7455, 0.4800, 0.1498, 0.0940],\n          [0.6717, 0.8128, 0.8569, 0.6158, 0.3400, 0.2916],\n          [0.9563, 0.9588, 0.9159, 0.7913, 0.6742, 0.6516],\n          [1.0084, 0.9897, 0.9319, 0.8205, 0.7234, 0.7038]]]])\n>>> a = torch.rand(2,1,3,3)\n>>> F.interpolate(a, scale_factor=2, mode='bicubic')\ntensor([[[[0.7897, 0.8228, 0.8356, 0.7856, 0.7270, 0.7168],\n          [0.4667, 0.6900, 0.8496, 0.7160, 0.5106, 0.4788],\n          [0.1671, 0.5446, 0.8217, 0.6151, 0.2859, 0.2357],\n          [0.1708, 0.4850, 0.7082, 0.5158, 0.2227, 0.1771],\n          [0.2914, 0.4776, 0.5973, 0.4484, 0.2421, 0.2088],\n          [0.3030, 0.4714, 0.5763, 0.4328, 0.2380, 0.2063]]],\n\n\n        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])", "body": "@driazati I just tried out the current version and recognized that it works on the case I showed yesterday, but it seems to still have an issue for the batch dimension. FYI.\r\n\r\n```python\r\n>>> import torch\r\n>>> import torch.nn.functional as F\r\n>>> a = torch.rand(1,2,3,3)\r\n>>> F.interpolate(a, scale_factor=2, mode='bicubic')\r\ntensor([[[[0.3577, 0.2687, 0.2801, 0.5409, 0.8162, 0.8665],\r\n          [0.2349, 0.4151, 0.5672, 0.5236, 0.4177, 0.4037],\r\n          [0.1034, 0.5432, 0.8430, 0.5395, 0.0973, 0.0273],\r\n          [0.0566, 0.5201, 0.8659, 0.6279, 0.2382, 0.1794],\r\n          [0.0573, 0.4455, 0.7832, 0.7166, 0.5140, 0.4887],\r\n          [0.0530, 0.4364, 0.7776, 0.7332, 0.5530, 0.5320]],\r\n\r\n         [[0.4682, 0.4246, 0.4658, 0.6916, 0.9178, 0.9602],\r\n          [0.3820, 0.4956, 0.5896, 0.5566, 0.4847, 0.4748],\r\n          [0.4000, 0.6286, 0.7455, 0.4800, 0.1498, 0.0940],\r\n          [0.6717, 0.8128, 0.8569, 0.6158, 0.3400, 0.2916],\r\n          [0.9563, 0.9588, 0.9159, 0.7913, 0.6742, 0.6516],\r\n          [1.0084, 0.9897, 0.9319, 0.8205, 0.7234, 0.7038]]]])\r\n>>> a = torch.rand(2,1,3,3)\r\n>>> F.interpolate(a, scale_factor=2, mode='bicubic')\r\ntensor([[[[0.7897, 0.8228, 0.8356, 0.7856, 0.7270, 0.7168],\r\n          [0.4667, 0.6900, 0.8496, 0.7160, 0.5106, 0.4788],\r\n          [0.1671, 0.5446, 0.8217, 0.6151, 0.2859, 0.2357],\r\n          [0.1708, 0.4850, 0.7082, 0.5158, 0.2227, 0.1771],\r\n          [0.2914, 0.4776, 0.5973, 0.4484, 0.2421, 0.2088],\r\n          [0.3030, 0.4714, 0.5763, 0.4328, 0.2380, 0.2063]]],\r\n\r\n\r\n        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\r\n```"}