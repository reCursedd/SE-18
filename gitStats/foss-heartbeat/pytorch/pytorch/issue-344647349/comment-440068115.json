{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440068115", "html_url": "https://github.com/pytorch/pytorch/pull/9849#issuecomment-440068115", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9849", "id": 440068115, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDA2ODExNQ==", "user": {"login": "hayatoikoma", "id": 2889812, "node_id": "MDQ6VXNlcjI4ODk4MTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2889812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hayatoikoma", "html_url": "https://github.com/hayatoikoma", "followers_url": "https://api.github.com/users/hayatoikoma/followers", "following_url": "https://api.github.com/users/hayatoikoma/following{/other_user}", "gists_url": "https://api.github.com/users/hayatoikoma/gists{/gist_id}", "starred_url": "https://api.github.com/users/hayatoikoma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hayatoikoma/subscriptions", "organizations_url": "https://api.github.com/users/hayatoikoma/orgs", "repos_url": "https://api.github.com/users/hayatoikoma/repos", "events_url": "https://api.github.com/users/hayatoikoma/events{/privacy}", "received_events_url": "https://api.github.com/users/hayatoikoma/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-19T22:39:32Z", "updated_at": "2018-11-19T22:39:32Z", "author_association": "NONE", "body_html": "<p>I just tried out this feature and found a couple of failure cases.</p>\n<p>If the channel dimension is more than 1, this implementation seem to interpolate only on the zero-th channel and fill out zero on the other channels.<br>\nI also found that it doesn't work for a CUDA tensor even though this PR have a CUDA code. (I didn't dig into what is causing this failure.)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a<span class=\"pl-k\">=</span>torch.rand(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> ac<span class=\"pl-k\">=</span>a.cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.interpolate(a, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bilinear<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>hayato<span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch_bicubic<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>functional.py:<span class=\"pl-c1\">2098</span>: <span class=\"pl-c1\">UserWarning</span>: Default upsampling behavior when mode<span class=\"pl-k\">=</span>bilinear <span class=\"pl-k\">is</span> changed to align_corners<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span> since <span class=\"pl-c1\">0.4</span>.0. Please specify align_corners<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span> <span class=\"pl-k\">if</span> the old behavior <span class=\"pl-k\">is</span> desired. See the documentation of nn.Upsample <span class=\"pl-k\">for</span> details.\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>See the documentation of nn.Upsample for details.<span class=\"pl-pds\">\"</span></span>.format(mode))\ntensor([[[[<span class=\"pl-c1\">0.2904</span>, <span class=\"pl-c1\">0.3275</span>, <span class=\"pl-c1\">0.4016</span>, <span class=\"pl-c1\">0.3507</span>, <span class=\"pl-c1\">0.1749</span>, <span class=\"pl-c1\">0.0869</span>],\n          [<span class=\"pl-c1\">0.4249</span>, <span class=\"pl-c1\">0.4506</span>, <span class=\"pl-c1\">0.5020</span>, <span class=\"pl-c1\">0.4467</span>, <span class=\"pl-c1\">0.2848</span>, <span class=\"pl-c1\">0.2039</span>],\n          [<span class=\"pl-c1\">0.6938</span>, <span class=\"pl-c1\">0.6968</span>, <span class=\"pl-c1\">0.7028</span>, <span class=\"pl-c1\">0.6388</span>, <span class=\"pl-c1\">0.5047</span>, <span class=\"pl-c1\">0.4377</span>],\n          [<span class=\"pl-c1\">0.7352</span>, <span class=\"pl-c1\">0.7531</span>, <span class=\"pl-c1\">0.7890</span>, <span class=\"pl-c1\">0.7512</span>, <span class=\"pl-c1\">0.6398</span>, <span class=\"pl-c1\">0.5841</span>],\n          [<span class=\"pl-c1\">0.5491</span>, <span class=\"pl-c1\">0.6196</span>, <span class=\"pl-c1\">0.7606</span>, <span class=\"pl-c1\">0.7841</span>, <span class=\"pl-c1\">0.6900</span>, <span class=\"pl-c1\">0.6430</span>],\n          [<span class=\"pl-c1\">0.4561</span>, <span class=\"pl-c1\">0.5529</span>, <span class=\"pl-c1\">0.7465</span>, <span class=\"pl-c1\">0.8005</span>, <span class=\"pl-c1\">0.7151</span>, <span class=\"pl-c1\">0.6724</span>]],\n\n         [[<span class=\"pl-c1\">0.6474</span>, <span class=\"pl-c1\">0.5409</span>, <span class=\"pl-c1\">0.3279</span>, <span class=\"pl-c1\">0.3598</span>, <span class=\"pl-c1\">0.6367</span>, <span class=\"pl-c1\">0.7751</span>],\n          [<span class=\"pl-c1\">0.6810</span>, <span class=\"pl-c1\">0.5968</span>, <span class=\"pl-c1\">0.4284</span>, <span class=\"pl-c1\">0.4429</span>, <span class=\"pl-c1\">0.6404</span>, <span class=\"pl-c1\">0.7391</span>],\n          [<span class=\"pl-c1\">0.7483</span>, <span class=\"pl-c1\">0.7087</span>, <span class=\"pl-c1\">0.6294</span>, <span class=\"pl-c1\">0.6091</span>, <span class=\"pl-c1\">0.6477</span>, <span class=\"pl-c1\">0.6670</span>],\n          [<span class=\"pl-c1\">0.8212</span>, <span class=\"pl-c1\">0.7853</span>, <span class=\"pl-c1\">0.7134</span>, <span class=\"pl-c1\">0.6556</span>, <span class=\"pl-c1\">0.6118</span>, <span class=\"pl-c1\">0.5899</span>],\n          [<span class=\"pl-c1\">0.8997</span>, <span class=\"pl-c1\">0.8266</span>, <span class=\"pl-c1\">0.6805</span>, <span class=\"pl-c1\">0.5825</span>, <span class=\"pl-c1\">0.5326</span>, <span class=\"pl-c1\">0.5076</span>],\n          [<span class=\"pl-c1\">0.9390</span>, <span class=\"pl-c1\">0.8473</span>, <span class=\"pl-c1\">0.6640</span>, <span class=\"pl-c1\">0.5459</span>, <span class=\"pl-c1\">0.4930</span>, <span class=\"pl-c1\">0.4665</span>]]]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.interpolate(a, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bicubic<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>hayato<span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch_bicubic<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>functional.py:<span class=\"pl-c1\">2098</span>: <span class=\"pl-c1\">UserWarning</span>: Default upsampling behavior when mode<span class=\"pl-k\">=</span>bicubic <span class=\"pl-k\">is</span> changed to align_corners<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span> since <span class=\"pl-c1\">0.4</span>.0. Please specify align_corners<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span> <span class=\"pl-k\">if</span> the old behavior <span class=\"pl-k\">is</span> desired. See the documentation of nn.Upsample <span class=\"pl-k\">for</span> details.\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>See the documentation of nn.Upsample for details.<span class=\"pl-pds\">\"</span></span>.format(mode))\ntensor([[[[<span class=\"pl-c1\">0.6474</span>, <span class=\"pl-c1\">0.3825</span>, <span class=\"pl-c1\">0.2214</span>, <span class=\"pl-c1\">0.4583</span>, <span class=\"pl-c1\">0.7751</span>, <span class=\"pl-c1\">0.8271</span>],\n          [<span class=\"pl-c1\">0.6999</span>, <span class=\"pl-c1\">0.5677</span>, <span class=\"pl-c1\">0.4802</span>, <span class=\"pl-c1\">0.5787</span>, <span class=\"pl-c1\">0.7185</span>, <span class=\"pl-c1\">0.7408</span>],\n          [<span class=\"pl-c1\">0.7819</span>, <span class=\"pl-c1\">0.7549</span>, <span class=\"pl-c1\">0.7126</span>, <span class=\"pl-c1\">0.6653</span>, <span class=\"pl-c1\">0.6310</span>, <span class=\"pl-c1\">0.6233</span>],\n          [<span class=\"pl-c1\">0.8730</span>, <span class=\"pl-c1\">0.7952</span>, <span class=\"pl-c1\">0.6885</span>, <span class=\"pl-c1\">0.5946</span>, <span class=\"pl-c1\">0.5352</span>, <span class=\"pl-c1\">0.5209</span>],\n          [<span class=\"pl-c1\">0.9390</span>, <span class=\"pl-c1\">0.7656</span>, <span class=\"pl-c1\">0.5724</span>, <span class=\"pl-c1\">0.4851</span>, <span class=\"pl-c1\">0.4665</span>, <span class=\"pl-c1\">0.4566</span>],\n          [<span class=\"pl-c1\">0.9537</span>, <span class=\"pl-c1\">0.7666</span>, <span class=\"pl-c1\">0.5592</span>, <span class=\"pl-c1\">0.4682</span>, <span class=\"pl-c1\">0.4511</span>, <span class=\"pl-c1\">0.4410</span>]],\n\n         [[<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>],\n          [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>]]]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.interpolate(ac, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bilinear<span class=\"pl-pds\">'</span></span>)\ntensor([[[[<span class=\"pl-c1\">0.2904</span>, <span class=\"pl-c1\">0.3275</span>, <span class=\"pl-c1\">0.4016</span>, <span class=\"pl-c1\">0.3507</span>, <span class=\"pl-c1\">0.1749</span>, <span class=\"pl-c1\">0.0869</span>],\n          [<span class=\"pl-c1\">0.4249</span>, <span class=\"pl-c1\">0.4506</span>, <span class=\"pl-c1\">0.5020</span>, <span class=\"pl-c1\">0.4467</span>, <span class=\"pl-c1\">0.2848</span>, <span class=\"pl-c1\">0.2039</span>],\n          [<span class=\"pl-c1\">0.6938</span>, <span class=\"pl-c1\">0.6968</span>, <span class=\"pl-c1\">0.7028</span>, <span class=\"pl-c1\">0.6388</span>, <span class=\"pl-c1\">0.5047</span>, <span class=\"pl-c1\">0.4377</span>],\n          [<span class=\"pl-c1\">0.7352</span>, <span class=\"pl-c1\">0.7531</span>, <span class=\"pl-c1\">0.7890</span>, <span class=\"pl-c1\">0.7512</span>, <span class=\"pl-c1\">0.6398</span>, <span class=\"pl-c1\">0.5841</span>],\n          [<span class=\"pl-c1\">0.5491</span>, <span class=\"pl-c1\">0.6196</span>, <span class=\"pl-c1\">0.7606</span>, <span class=\"pl-c1\">0.7841</span>, <span class=\"pl-c1\">0.6900</span>, <span class=\"pl-c1\">0.6430</span>],\n          [<span class=\"pl-c1\">0.4561</span>, <span class=\"pl-c1\">0.5529</span>, <span class=\"pl-c1\">0.7465</span>, <span class=\"pl-c1\">0.8005</span>, <span class=\"pl-c1\">0.7151</span>, <span class=\"pl-c1\">0.6724</span>]],\n\n         [[<span class=\"pl-c1\">0.6474</span>, <span class=\"pl-c1\">0.5409</span>, <span class=\"pl-c1\">0.3279</span>, <span class=\"pl-c1\">0.3598</span>, <span class=\"pl-c1\">0.6367</span>, <span class=\"pl-c1\">0.7751</span>],\n          [<span class=\"pl-c1\">0.6810</span>, <span class=\"pl-c1\">0.5968</span>, <span class=\"pl-c1\">0.4284</span>, <span class=\"pl-c1\">0.4429</span>, <span class=\"pl-c1\">0.6404</span>, <span class=\"pl-c1\">0.7391</span>],\n          [<span class=\"pl-c1\">0.7483</span>, <span class=\"pl-c1\">0.7087</span>, <span class=\"pl-c1\">0.6294</span>, <span class=\"pl-c1\">0.6091</span>, <span class=\"pl-c1\">0.6477</span>, <span class=\"pl-c1\">0.6670</span>],\n          [<span class=\"pl-c1\">0.8212</span>, <span class=\"pl-c1\">0.7853</span>, <span class=\"pl-c1\">0.7134</span>, <span class=\"pl-c1\">0.6556</span>, <span class=\"pl-c1\">0.6118</span>, <span class=\"pl-c1\">0.5899</span>],\n          [<span class=\"pl-c1\">0.8997</span>, <span class=\"pl-c1\">0.8266</span>, <span class=\"pl-c1\">0.6805</span>, <span class=\"pl-c1\">0.5825</span>, <span class=\"pl-c1\">0.5326</span>, <span class=\"pl-c1\">0.5076</span>],\n          [<span class=\"pl-c1\">0.9390</span>, <span class=\"pl-c1\">0.8473</span>, <span class=\"pl-c1\">0.6640</span>, <span class=\"pl-c1\">0.5459</span>, <span class=\"pl-c1\">0.4930</span>, <span class=\"pl-c1\">0.4665</span>]]]], <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.interpolate(ac, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bicubic<span class=\"pl-pds\">'</span></span>)\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;stdin&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2132</span>, <span class=\"pl-k\">in</span> interpolate\n    <span class=\"pl-k\">return</span> torch._C._nn.upsample_bicubic2d(<span class=\"pl-c1\">input</span>, _output_size(<span class=\"pl-c1\">2</span>), align_corners)\n<span class=\"pl-c1\">RuntimeError</span>: upsample_bicubic2d_forward <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> implemented <span class=\"pl-k\">for</span> <span class=\"pl-c1\">type</span> torch.cuda.FloatTensor</pre></div>\n<p>Is this PR WIP? I have been looking forward to this PR to be merged and am more than happy to help the integration of this feature if there is anything I can do.</p>", "body_text": "I just tried out this feature and found a couple of failure cases.\nIf the channel dimension is more than 1, this implementation seem to interpolate only on the zero-th channel and fill out zero on the other channels.\nI also found that it doesn't work for a CUDA tensor even though this PR have a CUDA code. (I didn't dig into what is causing this failure.)\n>>> import torch\n>>> import torch.nn.functional as F\n>>> a=torch.rand(1,2,3,3)\n>>> ac=a.cuda()\n>>> F.interpolate(a, scale_factor=2, mode='bilinear')\n/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py:2098: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n  \"See the documentation of nn.Upsample for details.\".format(mode))\ntensor([[[[0.2904, 0.3275, 0.4016, 0.3507, 0.1749, 0.0869],\n          [0.4249, 0.4506, 0.5020, 0.4467, 0.2848, 0.2039],\n          [0.6938, 0.6968, 0.7028, 0.6388, 0.5047, 0.4377],\n          [0.7352, 0.7531, 0.7890, 0.7512, 0.6398, 0.5841],\n          [0.5491, 0.6196, 0.7606, 0.7841, 0.6900, 0.6430],\n          [0.4561, 0.5529, 0.7465, 0.8005, 0.7151, 0.6724]],\n\n         [[0.6474, 0.5409, 0.3279, 0.3598, 0.6367, 0.7751],\n          [0.6810, 0.5968, 0.4284, 0.4429, 0.6404, 0.7391],\n          [0.7483, 0.7087, 0.6294, 0.6091, 0.6477, 0.6670],\n          [0.8212, 0.7853, 0.7134, 0.6556, 0.6118, 0.5899],\n          [0.8997, 0.8266, 0.6805, 0.5825, 0.5326, 0.5076],\n          [0.9390, 0.8473, 0.6640, 0.5459, 0.4930, 0.4665]]]])\n>>> F.interpolate(a, scale_factor=2, mode='bicubic')\n/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py:2098: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n  \"See the documentation of nn.Upsample for details.\".format(mode))\ntensor([[[[0.6474, 0.3825, 0.2214, 0.4583, 0.7751, 0.8271],\n          [0.6999, 0.5677, 0.4802, 0.5787, 0.7185, 0.7408],\n          [0.7819, 0.7549, 0.7126, 0.6653, 0.6310, 0.6233],\n          [0.8730, 0.7952, 0.6885, 0.5946, 0.5352, 0.5209],\n          [0.9390, 0.7656, 0.5724, 0.4851, 0.4665, 0.4566],\n          [0.9537, 0.7666, 0.5592, 0.4682, 0.4511, 0.4410]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n>>> F.interpolate(ac, scale_factor=2, mode='bilinear')\ntensor([[[[0.2904, 0.3275, 0.4016, 0.3507, 0.1749, 0.0869],\n          [0.4249, 0.4506, 0.5020, 0.4467, 0.2848, 0.2039],\n          [0.6938, 0.6968, 0.7028, 0.6388, 0.5047, 0.4377],\n          [0.7352, 0.7531, 0.7890, 0.7512, 0.6398, 0.5841],\n          [0.5491, 0.6196, 0.7606, 0.7841, 0.6900, 0.6430],\n          [0.4561, 0.5529, 0.7465, 0.8005, 0.7151, 0.6724]],\n\n         [[0.6474, 0.5409, 0.3279, 0.3598, 0.6367, 0.7751],\n          [0.6810, 0.5968, 0.4284, 0.4429, 0.6404, 0.7391],\n          [0.7483, 0.7087, 0.6294, 0.6091, 0.6477, 0.6670],\n          [0.8212, 0.7853, 0.7134, 0.6556, 0.6118, 0.5899],\n          [0.8997, 0.8266, 0.6805, 0.5825, 0.5326, 0.5076],\n          [0.9390, 0.8473, 0.6640, 0.5459, 0.4930, 0.4665]]]], device='cuda:0')\n>>> F.interpolate(ac, scale_factor=2, mode='bicubic')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py\", line 2132, in interpolate\n    return torch._C._nn.upsample_bicubic2d(input, _output_size(2), align_corners)\nRuntimeError: upsample_bicubic2d_forward is not implemented for type torch.cuda.FloatTensor\nIs this PR WIP? I have been looking forward to this PR to be merged and am more than happy to help the integration of this feature if there is anything I can do.", "body": "I just tried out this feature and found a couple of failure cases.\r\n\r\nIf the channel dimension is more than 1, this implementation seem to interpolate only on the zero-th channel and fill out zero on the other channels.\r\nI also found that it doesn't work for a CUDA tensor even though this PR have a CUDA code. (I didn't dig into what is causing this failure.)\r\n\r\n```python\r\n>>> import torch\r\n>>> import torch.nn.functional as F\r\n>>> a=torch.rand(1,2,3,3)\r\n>>> ac=a.cuda()\r\n>>> F.interpolate(a, scale_factor=2, mode='bilinear')\r\n/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py:2098: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\r\n  \"See the documentation of nn.Upsample for details.\".format(mode))\r\ntensor([[[[0.2904, 0.3275, 0.4016, 0.3507, 0.1749, 0.0869],\r\n          [0.4249, 0.4506, 0.5020, 0.4467, 0.2848, 0.2039],\r\n          [0.6938, 0.6968, 0.7028, 0.6388, 0.5047, 0.4377],\r\n          [0.7352, 0.7531, 0.7890, 0.7512, 0.6398, 0.5841],\r\n          [0.5491, 0.6196, 0.7606, 0.7841, 0.6900, 0.6430],\r\n          [0.4561, 0.5529, 0.7465, 0.8005, 0.7151, 0.6724]],\r\n\r\n         [[0.6474, 0.5409, 0.3279, 0.3598, 0.6367, 0.7751],\r\n          [0.6810, 0.5968, 0.4284, 0.4429, 0.6404, 0.7391],\r\n          [0.7483, 0.7087, 0.6294, 0.6091, 0.6477, 0.6670],\r\n          [0.8212, 0.7853, 0.7134, 0.6556, 0.6118, 0.5899],\r\n          [0.8997, 0.8266, 0.6805, 0.5825, 0.5326, 0.5076],\r\n          [0.9390, 0.8473, 0.6640, 0.5459, 0.4930, 0.4665]]]])\r\n>>> F.interpolate(a, scale_factor=2, mode='bicubic')\r\n/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py:2098: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\r\n  \"See the documentation of nn.Upsample for details.\".format(mode))\r\ntensor([[[[0.6474, 0.3825, 0.2214, 0.4583, 0.7751, 0.8271],\r\n          [0.6999, 0.5677, 0.4802, 0.5787, 0.7185, 0.7408],\r\n          [0.7819, 0.7549, 0.7126, 0.6653, 0.6310, 0.6233],\r\n          [0.8730, 0.7952, 0.6885, 0.5946, 0.5352, 0.5209],\r\n          [0.9390, 0.7656, 0.5724, 0.4851, 0.4665, 0.4566],\r\n          [0.9537, 0.7666, 0.5592, 0.4682, 0.4511, 0.4410]],\r\n\r\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\r\n>>> F.interpolate(ac, scale_factor=2, mode='bilinear')\r\ntensor([[[[0.2904, 0.3275, 0.4016, 0.3507, 0.1749, 0.0869],\r\n          [0.4249, 0.4506, 0.5020, 0.4467, 0.2848, 0.2039],\r\n          [0.6938, 0.6968, 0.7028, 0.6388, 0.5047, 0.4377],\r\n          [0.7352, 0.7531, 0.7890, 0.7512, 0.6398, 0.5841],\r\n          [0.5491, 0.6196, 0.7606, 0.7841, 0.6900, 0.6430],\r\n          [0.4561, 0.5529, 0.7465, 0.8005, 0.7151, 0.6724]],\r\n\r\n         [[0.6474, 0.5409, 0.3279, 0.3598, 0.6367, 0.7751],\r\n          [0.6810, 0.5968, 0.4284, 0.4429, 0.6404, 0.7391],\r\n          [0.7483, 0.7087, 0.6294, 0.6091, 0.6477, 0.6670],\r\n          [0.8212, 0.7853, 0.7134, 0.6556, 0.6118, 0.5899],\r\n          [0.8997, 0.8266, 0.6805, 0.5825, 0.5326, 0.5076],\r\n          [0.9390, 0.8473, 0.6640, 0.5459, 0.4930, 0.4665]]]], device='cuda:0')\r\n>>> F.interpolate(ac, scale_factor=2, mode='bicubic')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hayato/miniconda3/envs/pytorch_bicubic/lib/python3.6/site-packages/torch/nn/functional.py\", line 2132, in interpolate\r\n    return torch._C._nn.upsample_bicubic2d(input, _output_size(2), align_corners)\r\nRuntimeError: upsample_bicubic2d_forward is not implemented for type torch.cuda.FloatTensor\r\n```\r\n\r\nIs this PR WIP? I have been looking forward to this PR to be merged and am more than happy to help the integration of this feature if there is anything I can do.\r\n"}