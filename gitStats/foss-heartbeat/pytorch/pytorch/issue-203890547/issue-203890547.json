{"url": "https://api.github.com/repos/pytorch/pytorch/issues/635", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/635/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/635/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/635/events", "html_url": "https://github.com/pytorch/pytorch/issues/635", "id": 203890547, "node_id": "MDU6SXNzdWUyMDM4OTA1NDc=", "number": 635, "title": "Which backprop method is correct for RNN?", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-01-29T19:38:22Z", "updated_at": "2017-01-29T19:47:56Z", "closed_at": "2017-01-29T19:44:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Accumulating loss incrementally with timestep as in the tutorial, and sending all tiemsteps to RNN seem to produce the same output/hidden/loss but loss.backwards is calculating different parameter gradients. Is there a correct and incorrect method to do this? Which is right?</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ntorch.backends.cudnn.enabled=False\n\nmodel  = nn.LSTM(5, 5, 1).cuda()\nmodel2 = nn.LSTM(5, 5, 1).cuda()\n\nfor i in range(len(model2.all_weights)):\n    for j in range(len(model2.all_weights[i])):\n        model2.all_weights[i][j].data.copy_(model.all_weights[i][j].data)\n\ncrit = nn.MSELoss().cuda()\ncrit2 = nn.MSELoss().cuda()\n\ninput = Variable(torch.randn(2,1,5).cuda())\ntarget = Variable(torch.ones(2,1,5).cuda(), requires_grad=False)\nhidden = [ Variable(torch.randn(1,1,5).cuda().fill_(0.0)),\n            Variable(torch.randn(1,1,5).cuda().fill_(0.0))]\n\noutput, hidden = model(input, hidden)\nloss = crit(output, target)\nloss.backward(retain_variables=True)\n\nhidden2 = [ Variable(torch.randn(1,1,5).cuda().fill_(0.0)),\n            Variable(torch.randn(1,1,5).cuda().fill_(0.0))]\n\nloss2 = 0\nfor i in range(input.size(0)):\n    output2, hidden2 = model(input[i].view(1,1,-1), hidden2)\n    loss2 += crit2(output2[0], target[i])\n\nloss2 = loss2/2\nloss2.backward(retain_variables=True)\n\ndiff = 0\nmax_w = 0\nfor i in range(len(model2.all_weights)):\n    for j in range(len(model2.all_weights[i])):\n        diff = max(diff, (model2.all_weights[i][j].grad - model.all_weights[i][j].grad).abs().max().data[0])\n        \n        max_w = max(model2.all_weights[i][j].grad.max().data[0], max_w)\n        max_w = max(model.all_weights[i][j].grad.max().data[0], max_w)\n\ndh = (hidden[0]-hidden2[0]).abs().max().data[0]\ndc = (hidden[1]-hidden2[1]).abs().max().data[0]\ndo = (output[1]-output2).abs().max().data[0]\ndl = (loss-loss2).abs().max().data[0]\n\nprint(\"Diff in output : \" + str(do))\nprint(\"Diff in hidden states : \"+str(dh) +\", \"+str(dc))\nprint(\"Diff in loss : \" + str(dl))\n\nprint(\"Max weight grad found : \" +str(max_w))\nprint(\"Diff in weight grad : \" + str(diff))\n</code></pre>", "body_text": "Accumulating loss incrementally with timestep as in the tutorial, and sending all tiemsteps to RNN seem to produce the same output/hidden/loss but loss.backwards is calculating different parameter gradients. Is there a correct and incorrect method to do this? Which is right?\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ntorch.backends.cudnn.enabled=False\n\nmodel  = nn.LSTM(5, 5, 1).cuda()\nmodel2 = nn.LSTM(5, 5, 1).cuda()\n\nfor i in range(len(model2.all_weights)):\n    for j in range(len(model2.all_weights[i])):\n        model2.all_weights[i][j].data.copy_(model.all_weights[i][j].data)\n\ncrit = nn.MSELoss().cuda()\ncrit2 = nn.MSELoss().cuda()\n\ninput = Variable(torch.randn(2,1,5).cuda())\ntarget = Variable(torch.ones(2,1,5).cuda(), requires_grad=False)\nhidden = [ Variable(torch.randn(1,1,5).cuda().fill_(0.0)),\n            Variable(torch.randn(1,1,5).cuda().fill_(0.0))]\n\noutput, hidden = model(input, hidden)\nloss = crit(output, target)\nloss.backward(retain_variables=True)\n\nhidden2 = [ Variable(torch.randn(1,1,5).cuda().fill_(0.0)),\n            Variable(torch.randn(1,1,5).cuda().fill_(0.0))]\n\nloss2 = 0\nfor i in range(input.size(0)):\n    output2, hidden2 = model(input[i].view(1,1,-1), hidden2)\n    loss2 += crit2(output2[0], target[i])\n\nloss2 = loss2/2\nloss2.backward(retain_variables=True)\n\ndiff = 0\nmax_w = 0\nfor i in range(len(model2.all_weights)):\n    for j in range(len(model2.all_weights[i])):\n        diff = max(diff, (model2.all_weights[i][j].grad - model.all_weights[i][j].grad).abs().max().data[0])\n        \n        max_w = max(model2.all_weights[i][j].grad.max().data[0], max_w)\n        max_w = max(model.all_weights[i][j].grad.max().data[0], max_w)\n\ndh = (hidden[0]-hidden2[0]).abs().max().data[0]\ndc = (hidden[1]-hidden2[1]).abs().max().data[0]\ndo = (output[1]-output2).abs().max().data[0]\ndl = (loss-loss2).abs().max().data[0]\n\nprint(\"Diff in output : \" + str(do))\nprint(\"Diff in hidden states : \"+str(dh) +\", \"+str(dc))\nprint(\"Diff in loss : \" + str(dl))\n\nprint(\"Max weight grad found : \" +str(max_w))\nprint(\"Diff in weight grad : \" + str(diff))", "body": "Accumulating loss incrementally with timestep as in the tutorial, and sending all tiemsteps to RNN seem to produce the same output/hidden/loss but loss.backwards is calculating different parameter gradients. Is there a correct and incorrect method to do this? Which is right?\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\ntorch.backends.cudnn.enabled=False\r\n\r\nmodel  = nn.LSTM(5, 5, 1).cuda()\r\nmodel2 = nn.LSTM(5, 5, 1).cuda()\r\n\r\nfor i in range(len(model2.all_weights)):\r\n    for j in range(len(model2.all_weights[i])):\r\n        model2.all_weights[i][j].data.copy_(model.all_weights[i][j].data)\r\n\r\ncrit = nn.MSELoss().cuda()\r\ncrit2 = nn.MSELoss().cuda()\r\n\r\ninput = Variable(torch.randn(2,1,5).cuda())\r\ntarget = Variable(torch.ones(2,1,5).cuda(), requires_grad=False)\r\nhidden = [ Variable(torch.randn(1,1,5).cuda().fill_(0.0)),\r\n            Variable(torch.randn(1,1,5).cuda().fill_(0.0))]\r\n\r\noutput, hidden = model(input, hidden)\r\nloss = crit(output, target)\r\nloss.backward(retain_variables=True)\r\n\r\nhidden2 = [ Variable(torch.randn(1,1,5).cuda().fill_(0.0)),\r\n            Variable(torch.randn(1,1,5).cuda().fill_(0.0))]\r\n\r\nloss2 = 0\r\nfor i in range(input.size(0)):\r\n    output2, hidden2 = model(input[i].view(1,1,-1), hidden2)\r\n    loss2 += crit2(output2[0], target[i])\r\n\r\nloss2 = loss2/2\r\nloss2.backward(retain_variables=True)\r\n\r\ndiff = 0\r\nmax_w = 0\r\nfor i in range(len(model2.all_weights)):\r\n    for j in range(len(model2.all_weights[i])):\r\n        diff = max(diff, (model2.all_weights[i][j].grad - model.all_weights[i][j].grad).abs().max().data[0])\r\n        \r\n        max_w = max(model2.all_weights[i][j].grad.max().data[0], max_w)\r\n        max_w = max(model.all_weights[i][j].grad.max().data[0], max_w)\r\n\r\ndh = (hidden[0]-hidden2[0]).abs().max().data[0]\r\ndc = (hidden[1]-hidden2[1]).abs().max().data[0]\r\ndo = (output[1]-output2).abs().max().data[0]\r\ndl = (loss-loss2).abs().max().data[0]\r\n\r\nprint(\"Diff in output : \" + str(do))\r\nprint(\"Diff in hidden states : \"+str(dh) +\", \"+str(dc))\r\nprint(\"Diff in loss : \" + str(dl))\r\n\r\nprint(\"Max weight grad found : \" +str(max_w))\r\nprint(\"Diff in weight grad : \" + str(diff))\r\n```"}