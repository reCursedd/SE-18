{"url": "https://api.github.com/repos/pytorch/pytorch/issues/885", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/885/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/885/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/885/events", "html_url": "https://github.com/pytorch/pytorch/issues/885", "id": 211114087, "node_id": "MDU6SXNzdWUyMTExMTQwODc=", "number": 885, "title": "Computing the gradients for batch renormalization", "user": {"login": "linmx0130", "id": 975318, "node_id": "MDQ6VXNlcjk3NTMxOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/975318?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linmx0130", "html_url": "https://github.com/linmx0130", "followers_url": "https://api.github.com/users/linmx0130/followers", "following_url": "https://api.github.com/users/linmx0130/following{/other_user}", "gists_url": "https://api.github.com/users/linmx0130/gists{/gist_id}", "starred_url": "https://api.github.com/users/linmx0130/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linmx0130/subscriptions", "organizations_url": "https://api.github.com/users/linmx0130/orgs", "repos_url": "https://api.github.com/users/linmx0130/repos", "events_url": "https://api.github.com/users/linmx0130/events{/privacy}", "received_events_url": "https://api.github.com/users/linmx0130/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-01T15:26:02Z", "updated_at": "2017-03-01T17:12:46Z", "closed_at": "2017-03-01T17:12:46Z", "author_association": "NONE", "body_html": "<p>I tried to implement Batch Renormalization(arXiv 1702.03275) in PyTorch. The program stop when compute the gradients. Trackback information is attached below:</p>\n<pre><code>Traceback (most recent call last):\n  File \"cifar.py\", line 187, in &lt;module&gt;\n    loss.backward()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\", line 158, in backward\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\nRuntimeError: could not compute gradients for some functions (View, ConvNd)\n</code></pre>\n<p>My implementation of batch renormalization is shown below:</p>\n<pre><code>class BatchRenorm2d(nn.Module):\n    def __init__(self, channels, eps = 1e-5, rmax=3, dmax=5, lr=0.001):\n        super(BatchRenorm2d, self).__init__()\n        self.is_train = True\n        self.is_unlock = False\n        self.eps = eps \n        self.channels = channels\n        self.rmax = rmax\n        self.dmax = dmax\n        self.lr = lr\n        self.sigma = torch.from_numpy(np.zeros((1, channels, 1, 1), dtype=np.float32)).cuda()\n        self.mean = torch.from_numpy(np.zeros((1,channels), dtype=np.float32)).cuda()\n\n    def forward(self, x): \n        if self.is_train:\n            batch_size = x.size()[0]\n            feature_shape_size = x.size()[2] * x.size()[3]\n            sig_sqr_sum = Variable(torch.zeros(batch_size, self.channels)).cuda()\n            mu_b = x.mean(0).mean(2).mean(3).view(1, self.channels)\n            xview = x.view(batch_size, self.channels, feature_shape_size)\n\n            for j in range(self.channels):\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size)\n                for i in range(batch_size):\n                    sig_sqr_sum[i,j] = ((xview[i,j] - mu_b_0_j) ** 2).mean()\n            sigma_b = sig_sqr_sum.mean(0)\n            sigma_b += self.eps\n            sigma_b = torch.sqrt(sigma_b)\n            if self.is_unlock:\n                r = sigma_b.data / self.sigma\n                r.clamp_(1.0/rmax, rmax)\n                d = (mu_b.data - self.mean) / (self.sigma + torch.sqrt(eps) )\n                d.clamp_(-self.dmax, self.dmax)\n            else:\n                r = torch.zeros(1, self.channels) + 1.0 \n                d = torch.zeros(1, self.channels)\n            x_hat = Variable(torch.zeros(x.size()).cuda())\n            for j in range(self.channels):\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                sigma_b_0_j = sigma_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                for i in range(batch_size):\n                    x_hat_i_j = x[i,j,:,:].clone()\n                    x_hat_i_j -= mu_b_0_j\n                    x_hat_i_j /= sigma_b_0_j\n                    x_hat_i_j *= r[0, j]\n                    x_hat_i_j += d[0, j]\n                    x_hat[i,j,:,:] = x_hat_i_j\n                    self.mean += self.lr * (mu_b.data - self.mean)\n            self.sigma += self.lr * (sigma_b.data - self.sigma)\n        else:\n            mu_b = Variable(self.mean)\n            sigma_b = Variable(self.sigma)\n            for j in range(self.channels):\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                sigma_b_0_j = sigma_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                for i in range(batch_size):\n                    x_hat_i_j = x[i,j,:,:].clone()\n                    x_hat_i_j -= mu_b_0_j\n                    x_hat_i_j /= sigma_b_0_j\n                    x_hat_i_j *= r[0, j]\n                    x_hat_i_j += d[0, j]\n                    x_hat[i,j,:,:] = x_hat_i_j\n        return x_hat\n</code></pre>\n<p>What should I do to solve this problem? Thanks.</p>", "body_text": "I tried to implement Batch Renormalization(arXiv 1702.03275) in PyTorch. The program stop when compute the gradients. Trackback information is attached below:\nTraceback (most recent call last):\n  File \"cifar.py\", line 187, in <module>\n    loss.backward()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\", line 158, in backward\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\nRuntimeError: could not compute gradients for some functions (View, ConvNd)\n\nMy implementation of batch renormalization is shown below:\nclass BatchRenorm2d(nn.Module):\n    def __init__(self, channels, eps = 1e-5, rmax=3, dmax=5, lr=0.001):\n        super(BatchRenorm2d, self).__init__()\n        self.is_train = True\n        self.is_unlock = False\n        self.eps = eps \n        self.channels = channels\n        self.rmax = rmax\n        self.dmax = dmax\n        self.lr = lr\n        self.sigma = torch.from_numpy(np.zeros((1, channels, 1, 1), dtype=np.float32)).cuda()\n        self.mean = torch.from_numpy(np.zeros((1,channels), dtype=np.float32)).cuda()\n\n    def forward(self, x): \n        if self.is_train:\n            batch_size = x.size()[0]\n            feature_shape_size = x.size()[2] * x.size()[3]\n            sig_sqr_sum = Variable(torch.zeros(batch_size, self.channels)).cuda()\n            mu_b = x.mean(0).mean(2).mean(3).view(1, self.channels)\n            xview = x.view(batch_size, self.channels, feature_shape_size)\n\n            for j in range(self.channels):\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size)\n                for i in range(batch_size):\n                    sig_sqr_sum[i,j] = ((xview[i,j] - mu_b_0_j) ** 2).mean()\n            sigma_b = sig_sqr_sum.mean(0)\n            sigma_b += self.eps\n            sigma_b = torch.sqrt(sigma_b)\n            if self.is_unlock:\n                r = sigma_b.data / self.sigma\n                r.clamp_(1.0/rmax, rmax)\n                d = (mu_b.data - self.mean) / (self.sigma + torch.sqrt(eps) )\n                d.clamp_(-self.dmax, self.dmax)\n            else:\n                r = torch.zeros(1, self.channels) + 1.0 \n                d = torch.zeros(1, self.channels)\n            x_hat = Variable(torch.zeros(x.size()).cuda())\n            for j in range(self.channels):\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                sigma_b_0_j = sigma_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                for i in range(batch_size):\n                    x_hat_i_j = x[i,j,:,:].clone()\n                    x_hat_i_j -= mu_b_0_j\n                    x_hat_i_j /= sigma_b_0_j\n                    x_hat_i_j *= r[0, j]\n                    x_hat_i_j += d[0, j]\n                    x_hat[i,j,:,:] = x_hat_i_j\n                    self.mean += self.lr * (mu_b.data - self.mean)\n            self.sigma += self.lr * (sigma_b.data - self.sigma)\n        else:\n            mu_b = Variable(self.mean)\n            sigma_b = Variable(self.sigma)\n            for j in range(self.channels):\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                sigma_b_0_j = sigma_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\n                for i in range(batch_size):\n                    x_hat_i_j = x[i,j,:,:].clone()\n                    x_hat_i_j -= mu_b_0_j\n                    x_hat_i_j /= sigma_b_0_j\n                    x_hat_i_j *= r[0, j]\n                    x_hat_i_j += d[0, j]\n                    x_hat[i,j,:,:] = x_hat_i_j\n        return x_hat\n\nWhat should I do to solve this problem? Thanks.", "body": "I tried to implement Batch Renormalization(arXiv 1702.03275) in PyTorch. The program stop when compute the gradients. Trackback information is attached below:\r\n```\r\nTraceback (most recent call last):\r\n  File \"cifar.py\", line 187, in <module>\r\n    loss.backward()\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\", line 158, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\nRuntimeError: could not compute gradients for some functions (View, ConvNd)\r\n```\r\nMy implementation of batch renormalization is shown below:\r\n```\r\nclass BatchRenorm2d(nn.Module):\r\n    def __init__(self, channels, eps = 1e-5, rmax=3, dmax=5, lr=0.001):\r\n        super(BatchRenorm2d, self).__init__()\r\n        self.is_train = True\r\n        self.is_unlock = False\r\n        self.eps = eps \r\n        self.channels = channels\r\n        self.rmax = rmax\r\n        self.dmax = dmax\r\n        self.lr = lr\r\n        self.sigma = torch.from_numpy(np.zeros((1, channels, 1, 1), dtype=np.float32)).cuda()\r\n        self.mean = torch.from_numpy(np.zeros((1,channels), dtype=np.float32)).cuda()\r\n\r\n    def forward(self, x): \r\n        if self.is_train:\r\n            batch_size = x.size()[0]\r\n            feature_shape_size = x.size()[2] * x.size()[3]\r\n            sig_sqr_sum = Variable(torch.zeros(batch_size, self.channels)).cuda()\r\n            mu_b = x.mean(0).mean(2).mean(3).view(1, self.channels)\r\n            xview = x.view(batch_size, self.channels, feature_shape_size)\r\n\r\n            for j in range(self.channels):\r\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size)\r\n                for i in range(batch_size):\r\n                    sig_sqr_sum[i,j] = ((xview[i,j] - mu_b_0_j) ** 2).mean()\r\n            sigma_b = sig_sqr_sum.mean(0)\r\n            sigma_b += self.eps\r\n            sigma_b = torch.sqrt(sigma_b)\r\n            if self.is_unlock:\r\n                r = sigma_b.data / self.sigma\r\n                r.clamp_(1.0/rmax, rmax)\r\n                d = (mu_b.data - self.mean) / (self.sigma + torch.sqrt(eps) )\r\n                d.clamp_(-self.dmax, self.dmax)\r\n            else:\r\n                r = torch.zeros(1, self.channels) + 1.0 \r\n                d = torch.zeros(1, self.channels)\r\n            x_hat = Variable(torch.zeros(x.size()).cuda())\r\n            for j in range(self.channels):\r\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\r\n                sigma_b_0_j = sigma_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\r\n                for i in range(batch_size):\r\n                    x_hat_i_j = x[i,j,:,:].clone()\r\n                    x_hat_i_j -= mu_b_0_j\r\n                    x_hat_i_j /= sigma_b_0_j\r\n                    x_hat_i_j *= r[0, j]\r\n                    x_hat_i_j += d[0, j]\r\n                    x_hat[i,j,:,:] = x_hat_i_j\r\n                    self.mean += self.lr * (mu_b.data - self.mean)\r\n            self.sigma += self.lr * (sigma_b.data - self.sigma)\r\n        else:\r\n            mu_b = Variable(self.mean)\r\n            sigma_b = Variable(self.sigma)\r\n            for j in range(self.channels):\r\n                mu_b_0_j = mu_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\r\n                sigma_b_0_j = sigma_b[0, j].repeat(feature_shape_size).view(x.size()[2], x.size()[3])\r\n                for i in range(batch_size):\r\n                    x_hat_i_j = x[i,j,:,:].clone()\r\n                    x_hat_i_j -= mu_b_0_j\r\n                    x_hat_i_j /= sigma_b_0_j\r\n                    x_hat_i_j *= r[0, j]\r\n                    x_hat_i_j += d[0, j]\r\n                    x_hat[i,j,:,:] = x_hat_i_j\r\n        return x_hat\r\n```\r\n\r\nWhat should I do to solve this problem? Thanks."}