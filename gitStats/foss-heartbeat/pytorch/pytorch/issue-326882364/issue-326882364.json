{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7890", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7890/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7890/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7890/events", "html_url": "https://github.com/pytorch/pytorch/issues/7890", "id": 326882364, "node_id": "MDU6SXNzdWUzMjY4ODIzNjQ=", "number": 7890, "title": "[feature request] batch_first of RNN hidden weight for Multi GPU training", "user": {"login": "wangkenpu", "id": 15028609, "node_id": "MDQ6VXNlcjE1MDI4NjA5", "avatar_url": "https://avatars1.githubusercontent.com/u/15028609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangkenpu", "html_url": "https://github.com/wangkenpu", "followers_url": "https://api.github.com/users/wangkenpu/followers", "following_url": "https://api.github.com/users/wangkenpu/following{/other_user}", "gists_url": "https://api.github.com/users/wangkenpu/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangkenpu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangkenpu/subscriptions", "organizations_url": "https://api.github.com/users/wangkenpu/orgs", "repos_url": "https://api.github.com/users/wangkenpu/repos", "events_url": "https://api.github.com/users/wangkenpu/events{/privacy}", "received_events_url": "https://api.github.com/users/wangkenpu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-05-28T03:09:16Z", "updated_at": "2018-06-01T08:50:12Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>As we know, the input and output tensor support <code>batch_first</code> for RNN training, but as for hidden state, the tensor shape is forced to be <code>(num_layers * num_directions, batch, hidden_size)</code>, even we set <code>batch_first = True</code>. So there is a problem when we do multi GPU training by using <code>torch.nn.DataParallel</code>.</p>\n<p>For example, if we initialize a hidden state with shape <code>(num_layers * num_directions, batch, hidden_size)</code> and set <code>hidden = init_hidden(batch_size)</code> like <a href=\"https://github.com/pytorch/examples/blob/f9820471d615d848c14661b2d582417ca3aee8a3/word_language_model/main.py#L150\">word_language_model</a>\uff0cthen if we set <code>hidden = init_hidden(batch_size)</code> and use parallel training, the hidden size will be <code>(num_layers * num_directions / num_gpu, batch, hidden_size)</code>  while the correct shape is <code>(num_layers * num_directions, batch / num_gpu, hidden_size)</code>. So I was wondering if PyTorch Teams prepare to support <code>batch_first</code> for RNN hidden state.</p>\n<p>Another possible way to avoid this problem is to use the following codes:</p>\n<pre><code>def init_hidden(self, batch_size):\n    weight = next(self.parameters())\n    # The hidden weight format is not consisten with PyTorch's LSTM\n    # impelementation, so we will transpose it\n    hidden = weight.new_zeros(batch_size, self.num_layers * 2,\n                              self.hidden_size, requires_grad=False)\n    hidden = [hidden, hidden]\n    return hidden\n\ndef forward(self, inputs, hidden, length):\n    hidden[0] = hidden[0].permute(1, 0, 2).contiguous()\n    hidden[1] = hidden[1].permute(1, 0, 2).contiguous()\n    inputs_pack = pack_padded_sequence(inputs, length, batch_first=True)\n    # self.blstm.flatten_parameters()\n    output, hidden = self.blstm(inputs_pack, hidden)\n    ......\n    hidden = list(hidden)\n    hidden[0] = hidden[0].permute(1, 0, 2).contiguous()\n    hidden[1] = hidden[1].permute(1, 0, 2).contiguous()\n    return output, hidden\n</code></pre>\n<p>But this implementation is very ugly. Also, this implementation faces another problem like the following warning log:</p>\n<pre><code>/mnt/workspace/pytorch/deep_clustering/model/blstm_upit.py:65: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n</code></pre>\n<p>If we add <code>self.blstm.flatten_parameters()</code> before <code>output, hidden = self.blstm(inputs_pack, hidden)</code>, we will face another problem like <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"318834889\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7092\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7092/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7092\">#7092</a></p>", "body_text": "As we know, the input and output tensor support batch_first for RNN training, but as for hidden state, the tensor shape is forced to be (num_layers * num_directions, batch, hidden_size), even we set batch_first = True. So there is a problem when we do multi GPU training by using torch.nn.DataParallel.\nFor example, if we initialize a hidden state with shape (num_layers * num_directions, batch, hidden_size) and set hidden = init_hidden(batch_size) like word_language_model\uff0cthen if we set hidden = init_hidden(batch_size) and use parallel training, the hidden size will be (num_layers * num_directions / num_gpu, batch, hidden_size)  while the correct shape is (num_layers * num_directions, batch / num_gpu, hidden_size). So I was wondering if PyTorch Teams prepare to support batch_first for RNN hidden state.\nAnother possible way to avoid this problem is to use the following codes:\ndef init_hidden(self, batch_size):\n    weight = next(self.parameters())\n    # The hidden weight format is not consisten with PyTorch's LSTM\n    # impelementation, so we will transpose it\n    hidden = weight.new_zeros(batch_size, self.num_layers * 2,\n                              self.hidden_size, requires_grad=False)\n    hidden = [hidden, hidden]\n    return hidden\n\ndef forward(self, inputs, hidden, length):\n    hidden[0] = hidden[0].permute(1, 0, 2).contiguous()\n    hidden[1] = hidden[1].permute(1, 0, 2).contiguous()\n    inputs_pack = pack_padded_sequence(inputs, length, batch_first=True)\n    # self.blstm.flatten_parameters()\n    output, hidden = self.blstm(inputs_pack, hidden)\n    ......\n    hidden = list(hidden)\n    hidden[0] = hidden[0].permute(1, 0, 2).contiguous()\n    hidden[1] = hidden[1].permute(1, 0, 2).contiguous()\n    return output, hidden\n\nBut this implementation is very ugly. Also, this implementation faces another problem like the following warning log:\n/mnt/workspace/pytorch/deep_clustering/model/blstm_upit.py:65: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n\nIf we add self.blstm.flatten_parameters() before output, hidden = self.blstm(inputs_pack, hidden), we will face another problem like #7092", "body": "As we know, the input and output tensor support ```batch_first``` for RNN training, but as for hidden state, the tensor shape is forced to be ```(num_layers * num_directions, batch, hidden_size)```, even we set ```batch_first = True```. So there is a problem when we do multi GPU training by using ```torch.nn.DataParallel```. \r\n\r\nFor example, if we initialize a hidden state with shape ```(num_layers * num_directions, batch, hidden_size)``` and set ```hidden = init_hidden(batch_size)``` like [word_language_model](https://github.com/pytorch/examples/blob/f9820471d615d848c14661b2d582417ca3aee8a3/word_language_model/main.py#L150)\uff0cthen if we set ```hidden = init_hidden(batch_size)``` and use parallel training, the hidden size will be ```(num_layers * num_directions / num_gpu, batch, hidden_size)```  while the correct shape is ```(num_layers * num_directions, batch / num_gpu, hidden_size)```. So I was wondering if PyTorch Teams prepare to support ```batch_first``` for RNN hidden state.\r\n\r\nAnother possible way to avoid this problem is to use the following codes:\r\n```\r\ndef init_hidden(self, batch_size):\r\n    weight = next(self.parameters())\r\n    # The hidden weight format is not consisten with PyTorch's LSTM\r\n    # impelementation, so we will transpose it\r\n    hidden = weight.new_zeros(batch_size, self.num_layers * 2,\r\n                              self.hidden_size, requires_grad=False)\r\n    hidden = [hidden, hidden]\r\n    return hidden\r\n\r\ndef forward(self, inputs, hidden, length):\r\n    hidden[0] = hidden[0].permute(1, 0, 2).contiguous()\r\n    hidden[1] = hidden[1].permute(1, 0, 2).contiguous()\r\n    inputs_pack = pack_padded_sequence(inputs, length, batch_first=True)\r\n    # self.blstm.flatten_parameters()\r\n    output, hidden = self.blstm(inputs_pack, hidden)\r\n    ......\r\n    hidden = list(hidden)\r\n    hidden[0] = hidden[0].permute(1, 0, 2).contiguous()\r\n    hidden[1] = hidden[1].permute(1, 0, 2).contiguous()\r\n    return output, hidden\r\n```\r\n\r\nBut this implementation is very ugly. Also, this implementation faces another problem like the following warning log:\r\n```\r\n/mnt/workspace/pytorch/deep_clustering/model/blstm_upit.py:65: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\r\n```\r\nIf we add ```self.blstm.flatten_parameters()``` before ```output, hidden = self.blstm(inputs_pack, hidden)```, we will face another problem like #7092 \r\n"}