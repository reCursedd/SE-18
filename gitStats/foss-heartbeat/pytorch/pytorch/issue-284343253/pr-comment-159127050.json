{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159127050", "pull_request_review_id": 86034696, "id": 159127050, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTEyNzA1MA==", "diff_hunk": "@@ -2,6 +2,62 @@\n #define THC_GENERIC_FILE \"generic/THCTensorIndex.cu\"\n #else\n \n+// Check tensor dimensions for index operations, and return the slice size.\n+// src can be nullptr in case of indexFill: in that case it is ignored.\n+static ptrdiff_t THCTensor_(getSliceSize)(THCState *state, THCTensor *dst,\n+                                          int dim,\n+                                          THCudaLongTensor *index,\n+                                          THCTensor *src)\n+{\n+  int dstDims = THCTensor_(nDimension)(state, dst);\n+  int srcDims = (src == nullptr) ? dstDims : THCTensor_(nDimension)(state, src);\n+\n+  THArgCheck(THCudaLongTensor_nDimension(state, index) == 1, 4,\n+             \"expecting vector of indices\");\n+  THArgCheck(dim >= 0 && dim < dstDims, 2, \"Indexing dim is out of bounds\");\n+\n+  ptrdiff_t dstSliceSize = 1;\n+  for (int d = 0; d < dstDims; d++) {\n+    if (d != dim) {\n+      dstSliceSize *= dst->size[d];\n+    }\n+  }\n+\n+  if (src == nullptr) return dstSliceSize;\n+\n+  THArgCheck(dim < srcDims, 3, \"Indexing dim is out of bounds\");\n+  THArgCheck(THCudaLongTensor_nElement(state, index) == src->size[dim], 4,\n+             \"length of src.size[dim] is not equal to length of indices\");\n+\n+  ptrdiff_t srcSliceSize = 1;\n+  bool mismatch = false;\n+\n+  if (dstDims != srcDims) mismatch = true;\n+\n+  for (int d = 0; d < srcDims; d++) {\n+    if (d != dim) {\n+      srcSliceSize *= src->size[d];\n+      if (!mismatch && dst->size[d] != src->size[d]) mismatch = true;\n+    }\n+  }\n+\n+  THArgCheck(dstSliceSize == srcSliceSize, 2,\n+             \"Source/destination tensor have different slice sizes (%ld vs %ld)\",\n+             dstSliceSize, srcSliceSize);\n+\n+  if (mismatch) {", "path": "aten/src/THC/generic/THCTensorIndex.cu", "position": 47, "original_position": 47, "commit_id": "5906f3f9f632c10d02b8cacf00bb621e8f2149b3", "original_commit_id": "5906f3f9f632c10d02b8cacf00bb621e8f2149b3", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "body": "> (along with the code that reshapes the inputs correctly to fit into the strict constraints)\r\n\r\nHmm, I think it might be a bit difficult.  If the input or output is not contiguous, and the shapes do not match (say, (10, 5) vs (5, 10)), then the CUDA kernel *has* to know the dimensions of both input and output: the only way Python bindings could \"reshape\" input/output is by allocating a third buffer and copying data.\r\n\r\nSo I can think of several possible solutions:\r\n\r\n(A) Make these operations as strict as possible: *any* shape mismatch triggers THArgCheck, even when slice sizes are the same.  May break backward compatibility.\r\n\r\n(B) Migrate warning to Python bindings, which will merely print a warning and let the computation continue.  ATen still has to deal with the cases when shapes are different (as long as slice sizes are the same).\r\n\r\n(C) Migrate warning to Python bindings: in case of shape mismatch, it will print a warning, allocate a buffer, and copy the result back to the original output.  ATen now THArgCheck's that the shapes match exactly.  (Sounds like too much work for borderline crazy cases, though.)\r\n\r\n(D) Let's just remove `if (mismatch) { ... }` part here and add a TODO to deal with it in Python bindings.\r\n\r\nAny opinions?", "created_at": "2017-12-30T18:41:36Z", "updated_at": "2018-11-23T15:37:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/4342#discussion_r159127050", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4342", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159127050"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4342#discussion_r159127050"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4342"}}, "body_html": "<blockquote>\n<p>(along with the code that reshapes the inputs correctly to fit into the strict constraints)</p>\n</blockquote>\n<p>Hmm, I think it might be a bit difficult.  If the input or output is not contiguous, and the shapes do not match (say, (10, 5) vs (5, 10)), then the CUDA kernel <em>has</em> to know the dimensions of both input and output: the only way Python bindings could \"reshape\" input/output is by allocating a third buffer and copying data.</p>\n<p>So I can think of several possible solutions:</p>\n<p>(A) Make these operations as strict as possible: <em>any</em> shape mismatch triggers THArgCheck, even when slice sizes are the same.  May break backward compatibility.</p>\n<p>(B) Migrate warning to Python bindings, which will merely print a warning and let the computation continue.  ATen still has to deal with the cases when shapes are different (as long as slice sizes are the same).</p>\n<p>(C) Migrate warning to Python bindings: in case of shape mismatch, it will print a warning, allocate a buffer, and copy the result back to the original output.  ATen now THArgCheck's that the shapes match exactly.  (Sounds like too much work for borderline crazy cases, though.)</p>\n<p>(D) Let's just remove <code>if (mismatch) { ... }</code> part here and add a TODO to deal with it in Python bindings.</p>\n<p>Any opinions?</p>", "body_text": "(along with the code that reshapes the inputs correctly to fit into the strict constraints)\n\nHmm, I think it might be a bit difficult.  If the input or output is not contiguous, and the shapes do not match (say, (10, 5) vs (5, 10)), then the CUDA kernel has to know the dimensions of both input and output: the only way Python bindings could \"reshape\" input/output is by allocating a third buffer and copying data.\nSo I can think of several possible solutions:\n(A) Make these operations as strict as possible: any shape mismatch triggers THArgCheck, even when slice sizes are the same.  May break backward compatibility.\n(B) Migrate warning to Python bindings, which will merely print a warning and let the computation continue.  ATen still has to deal with the cases when shapes are different (as long as slice sizes are the same).\n(C) Migrate warning to Python bindings: in case of shape mismatch, it will print a warning, allocate a buffer, and copy the result back to the original output.  ATen now THArgCheck's that the shapes match exactly.  (Sounds like too much work for borderline crazy cases, though.)\n(D) Let's just remove if (mismatch) { ... } part here and add a TODO to deal with it in Python bindings.\nAny opinions?", "in_reply_to_id": 159121068}