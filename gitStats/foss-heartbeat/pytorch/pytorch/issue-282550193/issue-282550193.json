{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4200", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4200/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4200/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4200/events", "html_url": "https://github.com/pytorch/pytorch/pull/4200", "id": 282550193, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU4Njk1NTQw", "number": 4200, "title": "Expose node scopeName to python", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-15T21:17:02Z", "updated_at": "2017-12-17T01:00:22Z", "closed_at": "2017-12-17T01:00:22Z", "author_association": "COLLABORATOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4200", "html_url": "https://github.com/pytorch/pytorch/pull/4200", "diff_url": "https://github.com/pytorch/pytorch/pull/4200.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4200.patch"}, "body_html": "<p>This one-line PR addresses <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282462351\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4195\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4195/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4195\">#4195</a></p>\n<p>It just exposes Node <code>scopeName</code> to Python via pybind11.</p>\n<p>Usage:</p>\n<pre><code>class MyReLU(nn.Module):\n    def __init__(self):\n        super(MyReLU, self).__init__()\n        self.layer1 = nn.ReLU()\n    \n    def forward(self, x):\n        return self.layer1(x)\n    \nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer1 = nn.Sequential(nn.Linear(2,2), MyReLU())\n        self.layer2 = nn.Sequential(nn.Linear(2,2), nn.ReLU())\n\n    def forward(self, x):\n        return self.layer2(self.layer1(x))\n    \nnet = Net()\nt = Variable(torch.ones(2), requires_grad=True)\ntraced, _ = torch.jit.trace(net, (t, ))\ngraph = torch._C._jit_get_graph(traced)\n\nfor n in graph.nodes():\n    print('%s: %s' % (n.kind(), n.scopeName()))\n</code></pre>\n<p>prints</p>\n<pre><code>t: Net/Sequential[layer1]/Linear[0]\nunsqueeze: Net/Sequential[layer1]/Linear[0]\nmm: Net/Sequential[layer1]/Linear[0]\nsqueeze: Net/Sequential[layer1]/Linear[0]\nmatmul: Net/Sequential[layer1]/Linear[0]\nadd: Net/Sequential[layer1]/Linear[0]\nthreshold: Net/Sequential[layer1]/MyReLU[1]/ReLU[layer1]\nt: Net/Sequential[layer2]/Linear[0]\nunsqueeze: Net/Sequential[layer2]/Linear[0]\nmm: Net/Sequential[layer2]/Linear[0]\nsqueeze: Net/Sequential[layer2]/Linear[0]\nmatmul: Net/Sequential[layer2]/Linear[0]\nadd: Net/Sequential[layer2]/Linear[0]\nthreshold: Net/Sequential[layer2]/ReLU[1]\n</code></pre>", "body_text": "This one-line PR addresses #4195\nIt just exposes Node scopeName to Python via pybind11.\nUsage:\nclass MyReLU(nn.Module):\n    def __init__(self):\n        super(MyReLU, self).__init__()\n        self.layer1 = nn.ReLU()\n    \n    def forward(self, x):\n        return self.layer1(x)\n    \nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer1 = nn.Sequential(nn.Linear(2,2), MyReLU())\n        self.layer2 = nn.Sequential(nn.Linear(2,2), nn.ReLU())\n\n    def forward(self, x):\n        return self.layer2(self.layer1(x))\n    \nnet = Net()\nt = Variable(torch.ones(2), requires_grad=True)\ntraced, _ = torch.jit.trace(net, (t, ))\ngraph = torch._C._jit_get_graph(traced)\n\nfor n in graph.nodes():\n    print('%s: %s' % (n.kind(), n.scopeName()))\n\nprints\nt: Net/Sequential[layer1]/Linear[0]\nunsqueeze: Net/Sequential[layer1]/Linear[0]\nmm: Net/Sequential[layer1]/Linear[0]\nsqueeze: Net/Sequential[layer1]/Linear[0]\nmatmul: Net/Sequential[layer1]/Linear[0]\nadd: Net/Sequential[layer1]/Linear[0]\nthreshold: Net/Sequential[layer1]/MyReLU[1]/ReLU[layer1]\nt: Net/Sequential[layer2]/Linear[0]\nunsqueeze: Net/Sequential[layer2]/Linear[0]\nmm: Net/Sequential[layer2]/Linear[0]\nsqueeze: Net/Sequential[layer2]/Linear[0]\nmatmul: Net/Sequential[layer2]/Linear[0]\nadd: Net/Sequential[layer2]/Linear[0]\nthreshold: Net/Sequential[layer2]/ReLU[1]", "body": "This one-line PR addresses #4195\r\n\r\nIt just exposes Node `scopeName` to Python via pybind11.\r\n\r\nUsage:\r\n```\r\nclass MyReLU(nn.Module):\r\n    def __init__(self):\r\n        super(MyReLU, self).__init__()\r\n        self.layer1 = nn.ReLU()\r\n    \r\n    def forward(self, x):\r\n        return self.layer1(x)\r\n    \r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.layer1 = nn.Sequential(nn.Linear(2,2), MyReLU())\r\n        self.layer2 = nn.Sequential(nn.Linear(2,2), nn.ReLU())\r\n\r\n    def forward(self, x):\r\n        return self.layer2(self.layer1(x))\r\n    \r\nnet = Net()\r\nt = Variable(torch.ones(2), requires_grad=True)\r\ntraced, _ = torch.jit.trace(net, (t, ))\r\ngraph = torch._C._jit_get_graph(traced)\r\n\r\nfor n in graph.nodes():\r\n    print('%s: %s' % (n.kind(), n.scopeName()))\r\n```\r\n\r\nprints\r\n\r\n```\r\nt: Net/Sequential[layer1]/Linear[0]\r\nunsqueeze: Net/Sequential[layer1]/Linear[0]\r\nmm: Net/Sequential[layer1]/Linear[0]\r\nsqueeze: Net/Sequential[layer1]/Linear[0]\r\nmatmul: Net/Sequential[layer1]/Linear[0]\r\nadd: Net/Sequential[layer1]/Linear[0]\r\nthreshold: Net/Sequential[layer1]/MyReLU[1]/ReLU[layer1]\r\nt: Net/Sequential[layer2]/Linear[0]\r\nunsqueeze: Net/Sequential[layer2]/Linear[0]\r\nmm: Net/Sequential[layer2]/Linear[0]\r\nsqueeze: Net/Sequential[layer2]/Linear[0]\r\nmatmul: Net/Sequential[layer2]/Linear[0]\r\nadd: Net/Sequential[layer2]/Linear[0]\r\nthreshold: Net/Sequential[layer2]/ReLU[1]\r\n```"}