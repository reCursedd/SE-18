{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/336507523", "html_url": "https://github.com/pytorch/pytorch/issues/1637#issuecomment-336507523", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637", "id": 336507523, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjUwNzUyMw==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-13T16:52:00Z", "updated_at": "2017-10-13T16:52:10Z", "author_association": "MEMBER", "body_html": "<p>I suspect either a deadlock or that something is being jitted from ptx -&gt; sass.<br>\n<code>perf top -p [pid of the pytorch process]</code> will give hints on why the thing is just spinning if its a ptx-&gt;sass issue.</p>\n<p>If it's a deadlock issue, compiling pytorch with <code>DEBUG=1 python setup.py build develop</code> and getting gdb stack traces for all threads will help. A nicer way to figure out the deadlock is <a href=\"https://en.wikibooks.org/wiki/Linux_Applications_Debugging_Techniques/Deadlocks\" rel=\"nofollow\">https://en.wikibooks.org/wiki/Linux_Applications_Debugging_Techniques/Deadlocks</a></p>\n<p>It could also be botched CUDA install where some libraries are from CUDA8 and other libraries are from CUDA7.5 or CUDA9. To debug this, you can enter gdb with the process and run <code>gdb info shared</code> which will give the list of loaded shared libs. If some of them have 7.5 and some of them have 8, for example in their version number, that's suspect.</p>\n<p>I will be sparsely available for a week. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> can you co-ordinate with and help <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4086336\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Mctigger\">@Mctigger</a> isolate the issue?</p>", "body_text": "I suspect either a deadlock or that something is being jitted from ptx -> sass.\nperf top -p [pid of the pytorch process] will give hints on why the thing is just spinning if its a ptx->sass issue.\nIf it's a deadlock issue, compiling pytorch with DEBUG=1 python setup.py build develop and getting gdb stack traces for all threads will help. A nicer way to figure out the deadlock is https://en.wikibooks.org/wiki/Linux_Applications_Debugging_Techniques/Deadlocks\nIt could also be botched CUDA install where some libraries are from CUDA8 and other libraries are from CUDA7.5 or CUDA9. To debug this, you can enter gdb with the process and run gdb info shared which will give the list of loaded shared libs. If some of them have 7.5 and some of them have 8, for example in their version number, that's suspect.\nI will be sparsely available for a week. @zou3519 can you co-ordinate with and help @Mctigger isolate the issue?", "body": "I suspect either a deadlock or that something is being jitted from ptx -> sass.\r\n`perf top -p [pid of the pytorch process]` will give hints on why the thing is just spinning if its a ptx->sass issue.\r\n\r\nIf it's a deadlock issue, compiling pytorch with `DEBUG=1 python setup.py build develop` and getting gdb stack traces for all threads will help. A nicer way to figure out the deadlock is https://en.wikibooks.org/wiki/Linux_Applications_Debugging_Techniques/Deadlocks\r\n\r\nIt could also be botched CUDA install where some libraries are from CUDA8 and other libraries are from CUDA7.5 or CUDA9. To debug this, you can enter gdb with the process and run `gdb info shared` which will give the list of loaded shared libs. If some of them have 7.5 and some of them have 8, for example in their version number, that's suspect.\r\n\r\nI will be sparsely available for a week. @zou3519 can you co-ordinate with and help @Mctigger isolate the issue?"}