{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1637", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637/events", "html_url": "https://github.com/pytorch/pytorch/issues/1637", "id": 230857973, "node_id": "MDU6SXNzdWUyMzA4NTc5NzM=", "number": 1637, "title": "Multi-GPU K80s", "user": {"login": "davidmascharka", "id": 5611862, "node_id": "MDQ6VXNlcjU2MTE4NjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5611862?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidmascharka", "html_url": "https://github.com/davidmascharka", "followers_url": "https://api.github.com/users/davidmascharka/followers", "following_url": "https://api.github.com/users/davidmascharka/following{/other_user}", "gists_url": "https://api.github.com/users/davidmascharka/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidmascharka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidmascharka/subscriptions", "organizations_url": "https://api.github.com/users/davidmascharka/orgs", "repos_url": "https://api.github.com/users/davidmascharka/repos", "events_url": "https://api.github.com/users/davidmascharka/events{/privacy}", "received_events_url": "https://api.github.com/users/davidmascharka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 74, "created_at": "2017-05-23T21:52:20Z", "updated_at": "2018-07-25T14:36:51Z", "closed_at": "2017-10-22T10:27:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm having trouble getting multi-gpu via <code>DataParallel</code> across two Tesla K80 GPUs. The code I'm using is a modification of the MNIST example:</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom data_parallel import DataParallel\n\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = DataParallel(Net())\nmodel.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = nn.NLLLoss().cuda()\n\nmodel.train()\nfor batch_idx, (data, target) in enumerate(train_loader):\n    input_var = Variable(data.cuda())\n    target_var = Variable(target.cuda())\n\n    print('Getting model output')\n    output = model(input_var)\n    print('Got model output')\n\n    loss = criterion(output, target_var)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint('Finished')\n</code></pre>\n<p>This doesn't throw an error, but hangs after it prints \"Getting model output\" and never returns. I traced this down to the <code>parallel_apply</code> spawning threads that then never finish. The line that hangs is <a href=\"https://github.com/pytorch/pytorch/blob/e50a1f19b3dc735f0710929b97b0af384aafe09b/torch/nn/parallel/parallel_apply.py#L25\">here</a> where the threads are spawned using both GPU 0 and GPU 1, but never finish.</p>\n<p>This is only a problem when <code>CUDA_VISIBLE_DEVICES=0,1</code> as both GPU0 and GPU1 work perfectly well individually.</p>\n<p>Before running this, <code>nvidia-smi</code> shows</p>\n<pre><code>+------------------------------------------------------+\n| NVIDIA-SMI 352.68     Driver Version: 352.68         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\n| N/A   40C    P0    57W / 149W |     55MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\n| N/A   35C    P0    76W / 149W |     55MiB / 11519MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>after running (while it hangs), <code>nvidia-smi</code> gives</p>\n<pre><code>+------------------------------------------------------+\n| NVIDIA-SMI 352.68     Driver Version: 352.68         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\n| N/A   42C    P0    69W / 149W |    251MiB / 11519MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\n| N/A   36C    P0    90W / 149W |    249MiB / 11519MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      4785    C   python                                         194MiB |\n|    1      4785    C   python                                         192MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>and <code>top</code> shows the main python process and the two python subprocesses. Wondering if this could be something similar to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"202557169\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/554\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/554/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/554\">#554</a>.</p>\n<p>Using <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">this</a> TensorFlow example, I get linear speedup using multiple GPUs as I change <code>CUDA_VISIBLE_DEVICES</code> so multiple K80s should certainly be viable.</p>", "body_text": "I'm having trouble getting multi-gpu via DataParallel across two Tesla K80 GPUs. The code I'm using is a modification of the MNIST example:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom data_parallel import DataParallel\n\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = DataParallel(Net())\nmodel.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = nn.NLLLoss().cuda()\n\nmodel.train()\nfor batch_idx, (data, target) in enumerate(train_loader):\n    input_var = Variable(data.cuda())\n    target_var = Variable(target.cuda())\n\n    print('Getting model output')\n    output = model(input_var)\n    print('Got model output')\n\n    loss = criterion(output, target_var)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint('Finished')\n\nThis doesn't throw an error, but hangs after it prints \"Getting model output\" and never returns. I traced this down to the parallel_apply spawning threads that then never finish. The line that hangs is here where the threads are spawned using both GPU 0 and GPU 1, but never finish.\nThis is only a problem when CUDA_VISIBLE_DEVICES=0,1 as both GPU0 and GPU1 work perfectly well individually.\nBefore running this, nvidia-smi shows\n+------------------------------------------------------+\n| NVIDIA-SMI 352.68     Driver Version: 352.68         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\n| N/A   40C    P0    57W / 149W |     55MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\n| N/A   35C    P0    76W / 149W |     55MiB / 11519MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\nafter running (while it hangs), nvidia-smi gives\n+------------------------------------------------------+\n| NVIDIA-SMI 352.68     Driver Version: 352.68         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\n| N/A   42C    P0    69W / 149W |    251MiB / 11519MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\n| N/A   36C    P0    90W / 149W |    249MiB / 11519MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      4785    C   python                                         194MiB |\n|    1      4785    C   python                                         192MiB |\n+-----------------------------------------------------------------------------+\n\nand top shows the main python process and the two python subprocesses. Wondering if this could be something similar to #554.\nUsing this TensorFlow example, I get linear speedup using multiple GPUs as I change CUDA_VISIBLE_DEVICES so multiple K80s should certainly be viable.", "body": "I'm having trouble getting multi-gpu via `DataParallel` across two Tesla K80 GPUs. The code I'm using is a modification of the MNIST example:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable\r\nfrom data_parallel import DataParallel\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\nmodel = DataParallel(Net())\r\nmodel.cuda()\r\n\r\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\ncriterion = nn.NLLLoss().cuda()\r\n\r\nmodel.train()\r\nfor batch_idx, (data, target) in enumerate(train_loader):\r\n    input_var = Variable(data.cuda())\r\n    target_var = Variable(target.cuda())\r\n\r\n    print('Getting model output')\r\n    output = model(input_var)\r\n    print('Got model output')\r\n\r\n    loss = criterion(output, target_var)\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\nprint('Finished')\r\n```\r\n\r\nThis doesn't throw an error, but hangs after it prints \"Getting model output\" and never returns. I traced this down to the `parallel_apply` spawning threads that then never finish. The line that hangs is [here](https://github.com/pytorch/pytorch/blob/e50a1f19b3dc735f0710929b97b0af384aafe09b/torch/nn/parallel/parallel_apply.py#L25) where the threads are spawned using both GPU 0 and GPU 1, but never finish.\r\n\r\nThis is only a problem when `CUDA_VISIBLE_DEVICES=0,1` as both GPU0 and GPU1 work perfectly well individually.\r\n\r\nBefore running this, `nvidia-smi` shows\r\n\r\n```\r\n+------------------------------------------------------+\r\n| NVIDIA-SMI 352.68     Driver Version: 352.68         |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\r\n| N/A   40C    P0    57W / 149W |     55MiB / 11519MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\r\n| N/A   35C    P0    76W / 149W |     55MiB / 11519MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nafter running (while it hangs), `nvidia-smi` gives\r\n\r\n```\r\n+------------------------------------------------------+\r\n| NVIDIA-SMI 352.68     Driver Version: 352.68         |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\r\n| N/A   42C    P0    69W / 149W |    251MiB / 11519MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\r\n| N/A   36C    P0    90W / 149W |    249MiB / 11519MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      4785    C   python                                         194MiB |\r\n|    1      4785    C   python                                         192MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nand `top` shows the main python process and the two python subprocesses. Wondering if this could be something similar to #554.\r\n\r\n\r\nUsing [this](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) TensorFlow example, I get linear speedup using multiple GPUs as I change `CUDA_VISIBLE_DEVICES` so multiple K80s should certainly be viable."}