{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338268158", "html_url": "https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637", "id": 338268158, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODI2ODE1OA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-20T17:16:17Z", "updated_at": "2017-10-20T17:16:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To everyone facing hangs:</p>\n<ol>\n<li>Please make sure you can run p2pBandwithLatencyTest from cuda samples without errors, hangs, and timeouts. If you cannot, then the hangs you are facing are not a pytorch problem</li>\n<li>If p2pBandwidthLatencyTest fails, please check dmesg for IOMMU-related messages after reboot. If you see something like</li>\n</ol>\n<pre><code>[    1.217752] iommu: Adding device 0000:44:00.0 to group 18\n[    1.217762] iommu: Adding device 0000:44:00.2 to group 18\n[    1.217980] AMD-Vi: Found IOMMU at 0000:00:00.2 cap 0x40\n[    1.217984] AMD-Vi: Found IOMMU at 0000:40:00.2 cap 0x40\n[    1.220552] perf: amd_iommu: Detected. (0 banks, 0 counters/bank)\n</code></pre>\n<p>iommu is being enabled. P2P GPU traffic fails with enabled IOMMU, unless the cards are behind PLX switch.<br>\n3. Try disabling iommu via grub:</p>\n<pre><code>sudo vim /etc/default/grub\n</code></pre>\n<p>Make below changes in the file:</p>\n<pre><code>#GRUB_CMDLINE_LINUX=\"\"                           &lt;----- Original commented\nGRUB_CMDLINE_LINUX=\"iommu=soft\"           &lt;------ Change\n</code></pre>\n<pre><code>sudo update-grub\n</code></pre>\n<p>Reboot to see the change.</p>\n<ol start=\"4\">\n<li>Could also be an ACS-related issue <a href=\"https://devtalk.nvidia.com/default/topic/883054/multi-gpu-peer-to-peer-access-failing-on-tesla-k80-/?offset=26\" rel=\"nofollow\">https://devtalk.nvidia.com/default/topic/883054/multi-gpu-peer-to-peer-access-failing-on-tesla-k80-/?offset=26</a>, <a href=\"https://www.supermicro.com/support/faqs/faq.cfm?faq=20732\" rel=\"nofollow\">https://www.supermicro.com/support/faqs/faq.cfm?faq=20732</a></li>\n</ol>", "body_text": "To everyone facing hangs:\n\nPlease make sure you can run p2pBandwithLatencyTest from cuda samples without errors, hangs, and timeouts. If you cannot, then the hangs you are facing are not a pytorch problem\nIf p2pBandwidthLatencyTest fails, please check dmesg for IOMMU-related messages after reboot. If you see something like\n\n[    1.217752] iommu: Adding device 0000:44:00.0 to group 18\n[    1.217762] iommu: Adding device 0000:44:00.2 to group 18\n[    1.217980] AMD-Vi: Found IOMMU at 0000:00:00.2 cap 0x40\n[    1.217984] AMD-Vi: Found IOMMU at 0000:40:00.2 cap 0x40\n[    1.220552] perf: amd_iommu: Detected. (0 banks, 0 counters/bank)\n\niommu is being enabled. P2P GPU traffic fails with enabled IOMMU, unless the cards are behind PLX switch.\n3. Try disabling iommu via grub:\nsudo vim /etc/default/grub\n\nMake below changes in the file:\n#GRUB_CMDLINE_LINUX=\"\"                           <----- Original commented\nGRUB_CMDLINE_LINUX=\"iommu=soft\"           <------ Change\n\nsudo update-grub\n\nReboot to see the change.\n\nCould also be an ACS-related issue https://devtalk.nvidia.com/default/topic/883054/multi-gpu-peer-to-peer-access-failing-on-tesla-k80-/?offset=26, https://www.supermicro.com/support/faqs/faq.cfm?faq=20732", "body": "To everyone facing hangs:\r\n1) Please make sure you can run p2pBandwithLatencyTest from cuda samples without errors, hangs, and timeouts. If you cannot, then the hangs you are facing are not a pytorch problem\r\n2) If p2pBandwidthLatencyTest fails, please check dmesg for IOMMU-related messages after reboot. If you see something like \r\n```\r\n[    1.217752] iommu: Adding device 0000:44:00.0 to group 18\r\n[    1.217762] iommu: Adding device 0000:44:00.2 to group 18\r\n[    1.217980] AMD-Vi: Found IOMMU at 0000:00:00.2 cap 0x40\r\n[    1.217984] AMD-Vi: Found IOMMU at 0000:40:00.2 cap 0x40\r\n[    1.220552] perf: amd_iommu: Detected. (0 banks, 0 counters/bank)\r\n``` \r\niommu is being enabled. P2P GPU traffic fails with enabled IOMMU, unless the cards are behind PLX switch.\r\n3. Try disabling iommu via grub:\r\n```\r\nsudo vim /etc/default/grub\r\n```\r\nMake below changes in the file:\r\n```\r\n#GRUB_CMDLINE_LINUX=\"\"                           <----- Original commented\r\nGRUB_CMDLINE_LINUX=\"iommu=soft\"           <------ Change\r\n```\r\n```\r\nsudo update-grub\r\n```\r\nReboot to see the change.\r\n\r\n4. Could also be an ACS-related issue https://devtalk.nvidia.com/default/topic/883054/multi-gpu-peer-to-peer-access-failing-on-tesla-k80-/?offset=26, https://www.supermicro.com/support/faqs/faq.cfm?faq=20732"}