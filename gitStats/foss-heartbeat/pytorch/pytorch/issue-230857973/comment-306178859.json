{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/306178859", "html_url": "https://github.com/pytorch/pytorch/issues/1637#issuecomment-306178859", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637", "id": 306178859, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjE3ODg1OQ==", "user": {"login": "davidmascharka", "id": 5611862, "node_id": "MDQ6VXNlcjU2MTE4NjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5611862?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidmascharka", "html_url": "https://github.com/davidmascharka", "followers_url": "https://api.github.com/users/davidmascharka/followers", "following_url": "https://api.github.com/users/davidmascharka/following{/other_user}", "gists_url": "https://api.github.com/users/davidmascharka/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidmascharka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidmascharka/subscriptions", "organizations_url": "https://api.github.com/users/davidmascharka/orgs", "repos_url": "https://api.github.com/users/davidmascharka/repos", "events_url": "https://api.github.com/users/davidmascharka/events{/privacy}", "received_events_url": "https://api.github.com/users/davidmascharka/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-05T12:46:49Z", "updated_at": "2017-06-05T12:46:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It was my understanding that running <code>nvidia-smi</code> could cause the volatile usage to be nonzero. Anyway, I was able to get it to zero and run again.</p>\n<p>Before:</p>\n<pre><code>Mon Jun  5 08:44:00 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   41C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   36C    P0    85W / 149W |      0MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>It hangs on the same line, then <code>nvidia-smi</code> shows:</p>\n<pre><code>Mon Jun  5 08:45:01 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   41C    P0    69W / 149W |    195MiB / 11439MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   36C    P0    90W / 149W |    195MiB / 11439MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0    107462    C   python3                                        193MiB |\n|    1    107462    C   python3                                        193MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>", "body_text": "It was my understanding that running nvidia-smi could cause the volatile usage to be nonzero. Anyway, I was able to get it to zero and run again.\nBefore:\nMon Jun  5 08:44:00 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   41C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   36C    P0    85W / 149W |      0MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\nIt hangs on the same line, then nvidia-smi shows:\nMon Jun  5 08:45:01 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   41C    P0    69W / 149W |    195MiB / 11439MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   36C    P0    90W / 149W |    195MiB / 11439MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0    107462    C   python3                                        193MiB |\n|    1    107462    C   python3                                        193MiB |\n+-----------------------------------------------------------------------------+", "body": "It was my understanding that running `nvidia-smi` could cause the volatile usage to be nonzero. Anyway, I was able to get it to zero and run again.\r\n\r\nBefore:\r\n```\r\nMon Jun  5 08:44:00 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\r\n| N/A   41C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\r\n| N/A   36C    P0    85W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nIt hangs on the same line, then `nvidia-smi` shows:\r\n\r\n```\r\nMon Jun  5 08:45:01 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\r\n| N/A   41C    P0    69W / 149W |    195MiB / 11439MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\r\n| N/A   36C    P0    90W / 149W |    195MiB / 11439MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0    107462    C   python3                                        193MiB |\r\n|    1    107462    C   python3                                        193MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n"}