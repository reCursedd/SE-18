{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337746690", "html_url": "https://github.com/pytorch/pytorch/issues/1637#issuecomment-337746690", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1637", "id": 337746690, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzc0NjY5MA==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-18T22:30:35Z", "updated_at": "2017-10-18T22:31:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Debugging on <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4086336\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Mctigger\">@Mctigger</a>'s machine with 2 1080s, I found the following:</p>\n<ol>\n<li><code>python test/test-nccl.py</code> fails (rather, it just hangs).</li>\n<li>Calling ncclBcast in the backend causes a GPU to go into 100% and hang. It returns immediately, though, so DataParallel eventually calls it again on another GPU, causing that GPU to hang as well. The end result is that both GPUs are at 100% utilization after DataParallel calls code to split the input over multiple gpus.</li>\n<li>I wondered if this was a problem with the NCCL version. The one that is used by default seems to be 1.3.5. I built pytorch using the system nccl (2.0.5) and re-ran. Now, when ncclBcast is called, GPU usage is at 0% but the entire program hangs (and ncclBcast doesn't return).</li>\n</ol>\n<p>Any ideas?</p>", "body_text": "Debugging on @Mctigger's machine with 2 1080s, I found the following:\n\npython test/test-nccl.py fails (rather, it just hangs).\nCalling ncclBcast in the backend causes a GPU to go into 100% and hang. It returns immediately, though, so DataParallel eventually calls it again on another GPU, causing that GPU to hang as well. The end result is that both GPUs are at 100% utilization after DataParallel calls code to split the input over multiple gpus.\nI wondered if this was a problem with the NCCL version. The one that is used by default seems to be 1.3.5. I built pytorch using the system nccl (2.0.5) and re-ran. Now, when ncclBcast is called, GPU usage is at 0% but the entire program hangs (and ncclBcast doesn't return).\n\nAny ideas?", "body": "Debugging on @Mctigger's machine with 2 1080s, I found the following:\r\n\r\n1) `python test/test-nccl.py` fails (rather, it just hangs).\r\n2) Calling ncclBcast in the backend causes a GPU to go into 100% and hang. It returns immediately, though, so DataParallel eventually calls it again on another GPU, causing that GPU to hang as well. The end result is that both GPUs are at 100% utilization after DataParallel calls code to split the input over multiple gpus. \r\n3) I wondered if this was a problem with the NCCL version. The one that is used by default seems to be 1.3.5. I built pytorch using the system nccl (2.0.5) and re-ran. Now, when ncclBcast is called, GPU usage is at 0% but the entire program hangs (and ncclBcast doesn't return).\r\n\r\nAny ideas?"}