{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/420084340", "html_url": "https://github.com/pytorch/pytorch/issues/11335#issuecomment-420084340", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11335", "id": 420084340, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDA4NDM0MA==", "user": {"login": "jinserk", "id": 823222, "node_id": "MDQ6VXNlcjgyMzIyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/823222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinserk", "html_url": "https://github.com/jinserk", "followers_url": "https://api.github.com/users/jinserk/followers", "following_url": "https://api.github.com/users/jinserk/following{/other_user}", "gists_url": "https://api.github.com/users/jinserk/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinserk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinserk/subscriptions", "organizations_url": "https://api.github.com/users/jinserk/orgs", "repos_url": "https://api.github.com/users/jinserk/repos", "events_url": "https://api.github.com/users/jinserk/events{/privacy}", "received_events_url": "https://api.github.com/users/jinserk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-10T22:37:10Z", "updated_at": "2018-09-10T22:37:10Z", "author_association": "NONE", "body_html": "<p>I've resolved this as followings:</p>\n<pre><code>class LayerNormLSTMCell(nn.LSTMCell):\n\n    def __init__(self, input_size, hidden_size, bias=True):\n        super().__init__(input_size, hidden_size, bias)\n\n        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n        self.ln_ho = nn.LayerNorm(hidden_size)\n\n    def forward(self, input, hidden=None):\n        self.check_forward_input(input)\n        if hidden is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n        else:\n            hx, cx = hidden\n        self.check_forward_hidden(input, hx, '[0]')\n        self.check_forward_hidden(input, cx, '[1]')\n\n        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n                 + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n        g = gates[:, (3 * self.hidden_size):].tanh()\n\n        cy = (f * cx) + (i * g)\n        hy = o * self.ln_ho(cy).tanh()\n        return hy, cy\n\n\nclass LayerNormLSTM(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n\n        num_directions = 2 if bidirectional else 1\n        self.hidden0 = nn.ModuleList([\n            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n                              hidden_size=hidden_size, bias=bias)\n            for layer in range(num_layers)\n        ])\n\n        if self.bidirectional:\n            self.hidden1 = nn.ModuleList([\n                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n                                  hidden_size=hidden_size, bias=bias)\n                for layer in range(num_layers)\n            ])\n\n    def forward(self, input, hidden=None):\n        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n        num_directions = 2 if self.bidirectional else 1\n        if hidden is None:\n            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n        else:\n            hx, cx = hidden\n\n        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n\n        if self.bidirectional:\n            xs = input\n            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n                l0, l1 = 2 * l, 2 * l + 1\n                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n                    h0, c0 = ht[t][l0], ct[t][l0]\n                    t = seq_len - 1 - t\n                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n                    h1, c1 = ht[t][l1], ct[t][l1]\n                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n            y  = torch.stack(xs)\n            hy = torch.stack(ht[-1])\n            cy = torch.stack(ct[-1])\n        else:\n            h, c = hx, cx\n            for t, x in enumerate(input):\n                for l, layer in enumerate(self.hidden0):\n                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n                    x = ht[t][l]\n                h, c = ht[t], ct[t]\n            y  = torch.stack([h[-1] for h in ht])\n            hy = torch.stack(ht[-1])\n            cy = torch.stack(ct[-1])\n\n        return y, (hy, cy)\n</code></pre>\n<p>I'm leaving my code to get reviews from the others and to help someone who suffers from the same issue. Hope being helpful. :)</p>", "body_text": "I've resolved this as followings:\nclass LayerNormLSTMCell(nn.LSTMCell):\n\n    def __init__(self, input_size, hidden_size, bias=True):\n        super().__init__(input_size, hidden_size, bias)\n\n        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n        self.ln_ho = nn.LayerNorm(hidden_size)\n\n    def forward(self, input, hidden=None):\n        self.check_forward_input(input)\n        if hidden is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n        else:\n            hx, cx = hidden\n        self.check_forward_hidden(input, hx, '[0]')\n        self.check_forward_hidden(input, cx, '[1]')\n\n        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n                 + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n        g = gates[:, (3 * self.hidden_size):].tanh()\n\n        cy = (f * cx) + (i * g)\n        hy = o * self.ln_ho(cy).tanh()\n        return hy, cy\n\n\nclass LayerNormLSTM(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n\n        num_directions = 2 if bidirectional else 1\n        self.hidden0 = nn.ModuleList([\n            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n                              hidden_size=hidden_size, bias=bias)\n            for layer in range(num_layers)\n        ])\n\n        if self.bidirectional:\n            self.hidden1 = nn.ModuleList([\n                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n                                  hidden_size=hidden_size, bias=bias)\n                for layer in range(num_layers)\n            ])\n\n    def forward(self, input, hidden=None):\n        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n        num_directions = 2 if self.bidirectional else 1\n        if hidden is None:\n            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n        else:\n            hx, cx = hidden\n\n        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n\n        if self.bidirectional:\n            xs = input\n            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n                l0, l1 = 2 * l, 2 * l + 1\n                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n                    h0, c0 = ht[t][l0], ct[t][l0]\n                    t = seq_len - 1 - t\n                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n                    h1, c1 = ht[t][l1], ct[t][l1]\n                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n            y  = torch.stack(xs)\n            hy = torch.stack(ht[-1])\n            cy = torch.stack(ct[-1])\n        else:\n            h, c = hx, cx\n            for t, x in enumerate(input):\n                for l, layer in enumerate(self.hidden0):\n                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n                    x = ht[t][l]\n                h, c = ht[t], ct[t]\n            y  = torch.stack([h[-1] for h in ht])\n            hy = torch.stack(ht[-1])\n            cy = torch.stack(ct[-1])\n\n        return y, (hy, cy)\n\nI'm leaving my code to get reviews from the others and to help someone who suffers from the same issue. Hope being helpful. :)", "body": "I've resolved this as followings:\r\n```\r\nclass LayerNormLSTMCell(nn.LSTMCell):\r\n\r\n    def __init__(self, input_size, hidden_size, bias=True):\r\n        super().__init__(input_size, hidden_size, bias)\r\n\r\n        self.ln_ih = nn.LayerNorm(4 * hidden_size)\r\n        self.ln_hh = nn.LayerNorm(4 * hidden_size)\r\n        self.ln_ho = nn.LayerNorm(hidden_size)\r\n\r\n    def forward(self, input, hidden=None):\r\n        self.check_forward_input(input)\r\n        if hidden is None:\r\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n        else:\r\n            hx, cx = hidden\r\n        self.check_forward_hidden(input, hx, '[0]')\r\n        self.check_forward_hidden(input, cx, '[1]')\r\n\r\n        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\r\n                 + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\r\n        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\r\n        g = gates[:, (3 * self.hidden_size):].tanh()\r\n\r\n        cy = (f * cx) + (i * g)\r\n        hy = o * self.ln_ho(cy).tanh()\r\n        return hy, cy\r\n\r\n\r\nclass LayerNormLSTM(nn.Module):\r\n\r\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\r\n        super().__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.num_layers = num_layers\r\n        self.bidirectional = bidirectional\r\n\r\n        num_directions = 2 if bidirectional else 1\r\n        self.hidden0 = nn.ModuleList([\r\n            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\r\n                              hidden_size=hidden_size, bias=bias)\r\n            for layer in range(num_layers)\r\n        ])\r\n\r\n        if self.bidirectional:\r\n            self.hidden1 = nn.ModuleList([\r\n                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\r\n                                  hidden_size=hidden_size, bias=bias)\r\n                for layer in range(num_layers)\r\n            ])\r\n\r\n    def forward(self, input, hidden=None):\r\n        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\r\n        num_directions = 2 if self.bidirectional else 1\r\n        if hidden is None:\r\n            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\r\n            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\r\n        else:\r\n            hx, cx = hidden\r\n\r\n        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\r\n        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\r\n\r\n        if self.bidirectional:\r\n            xs = input\r\n            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\r\n                l0, l1 = 2 * l, 2 * l + 1\r\n                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\r\n                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\r\n                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\r\n                    h0, c0 = ht[t][l0], ct[t][l0]\r\n                    t = seq_len - 1 - t\r\n                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\r\n                    h1, c1 = ht[t][l1], ct[t][l1]\r\n                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\r\n            y  = torch.stack(xs)\r\n            hy = torch.stack(ht[-1])\r\n            cy = torch.stack(ct[-1])\r\n        else:\r\n            h, c = hx, cx\r\n            for t, x in enumerate(input):\r\n                for l, layer in enumerate(self.hidden0):\r\n                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\r\n                    x = ht[t][l]\r\n                h, c = ht[t], ct[t]\r\n            y  = torch.stack([h[-1] for h in ht])\r\n            hy = torch.stack(ht[-1])\r\n            cy = torch.stack(ct[-1])\r\n\r\n        return y, (hy, cy)\r\n```\r\nI'm leaving my code to get reviews from the others and to help someone who suffers from the same issue. Hope being helpful. :)\r\n"}