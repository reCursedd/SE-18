{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11335", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11335/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11335/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11335/events", "html_url": "https://github.com/pytorch/pytorch/issues/11335", "id": 357728243, "node_id": "MDU6SXNzdWUzNTc3MjgyNDM=", "number": 11335, "title": "Question for LayerNorm LSTM implementation", "user": {"login": "jinserk", "id": 823222, "node_id": "MDQ6VXNlcjgyMzIyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/823222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinserk", "html_url": "https://github.com/jinserk", "followers_url": "https://api.github.com/users/jinserk/followers", "following_url": "https://api.github.com/users/jinserk/following{/other_user}", "gists_url": "https://api.github.com/users/jinserk/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinserk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinserk/subscriptions", "organizations_url": "https://api.github.com/users/jinserk/orgs", "repos_url": "https://api.github.com/users/jinserk/repos", "events_url": "https://api.github.com/users/jinserk/events{/privacy}", "received_events_url": "https://api.github.com/users/jinserk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-06T16:13:53Z", "updated_at": "2018-09-10T22:37:10Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi all,<br>\nI'm trying to implement a LayerNorm applied multi-layered LSTM using LSTMCell, but stuck. For simplicity, I had tried unidirectional LSTM only. Here my code is:</p>\n<pre><code>class LayerNormLSTMCell(nn.LSTMCell):\n\n    def __init__(self, input_size, hidden_size, bias=True):\n        super().__init__(input_size, hidden_size, bias)\n\n        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n        self.ln_ho = nn.LayerNorm(hidden_size)\n\n    def forward(self, input, hidden=None):\n        self.check_forward_input(input)\n        if hidden is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            cx = hx\n        else:\n            hx, cx = hidden\n        self.check_forward_hidden(input, hx, '[0]')\n        self.check_forward_hidden(input, cx, '[1]')\n\n        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n                 + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n        g = gates[:, (3 * self.hidden_size):].tanh()\n\n        cy = (f * cx) + (i * g)\n        hy = o * self.ln_ho(cy).tanh()\n        return hy, cy\n\n\nclass LayerNormLSTM(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.hidden = nn.ModuleList([\n            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size),\n                              hidden_size=hidden_size)\n            for layer in range(num_layers)\n        ])\n\n    def forward(self, input, hidden=None):\n        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n        if hidden is None:\n            hx = input.new_zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\n            cx = input.new_zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\n            hidden = (hx, cx)\n        ht = input.new_zeros(seq_len, self.num_layers, batch_size, self.hidden_size)\n        ct = input.new_zeros(seq_len, self.num_layers, batch_size, self.hidden_size)\n\n        h, c = hidden\n        for t, x in enumerate(input):\n            for l, layer in enumerate(self.hidden):\n                ht[t, l], ct[t, l] = layer(x, (h[l], c[l]))\n                x = ht[t, l]\n            h, c = ht[t], ct[t]\n\n        y  = ht[:, -1, :, :].squeeze().contiguous()\n        hy = ht[-1, :, :, :].squeeze().contiguous()\n        cy = ct[-1, :, :, :].squeeze().contiguous()\n\n        return y, (hy, cy)\n</code></pre>\n<p>but I've got an error in runtime as:</p>\n<pre><code>  File \"/u3/jbaik/pytorch-asr/asr/models/trainer.py\", line 336, in unit_train\n    loss.backward()\n  File \"/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n</code></pre>\n<p>What should I fix for this? Thanks!</p>", "body_text": "Hi all,\nI'm trying to implement a LayerNorm applied multi-layered LSTM using LSTMCell, but stuck. For simplicity, I had tried unidirectional LSTM only. Here my code is:\nclass LayerNormLSTMCell(nn.LSTMCell):\n\n    def __init__(self, input_size, hidden_size, bias=True):\n        super().__init__(input_size, hidden_size, bias)\n\n        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n        self.ln_ho = nn.LayerNorm(hidden_size)\n\n    def forward(self, input, hidden=None):\n        self.check_forward_input(input)\n        if hidden is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            cx = hx\n        else:\n            hx, cx = hidden\n        self.check_forward_hidden(input, hx, '[0]')\n        self.check_forward_hidden(input, cx, '[1]')\n\n        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n                 + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n        g = gates[:, (3 * self.hidden_size):].tanh()\n\n        cy = (f * cx) + (i * g)\n        hy = o * self.ln_ho(cy).tanh()\n        return hy, cy\n\n\nclass LayerNormLSTM(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.hidden = nn.ModuleList([\n            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size),\n                              hidden_size=hidden_size)\n            for layer in range(num_layers)\n        ])\n\n    def forward(self, input, hidden=None):\n        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n        if hidden is None:\n            hx = input.new_zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\n            cx = input.new_zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\n            hidden = (hx, cx)\n        ht = input.new_zeros(seq_len, self.num_layers, batch_size, self.hidden_size)\n        ct = input.new_zeros(seq_len, self.num_layers, batch_size, self.hidden_size)\n\n        h, c = hidden\n        for t, x in enumerate(input):\n            for l, layer in enumerate(self.hidden):\n                ht[t, l], ct[t, l] = layer(x, (h[l], c[l]))\n                x = ht[t, l]\n            h, c = ht[t], ct[t]\n\n        y  = ht[:, -1, :, :].squeeze().contiguous()\n        hy = ht[-1, :, :, :].squeeze().contiguous()\n        cy = ct[-1, :, :, :].squeeze().contiguous()\n\n        return y, (hy, cy)\n\nbut I've got an error in runtime as:\n  File \"/u3/jbaik/pytorch-asr/asr/models/trainer.py\", line 336, in unit_train\n    loss.backward()\n  File \"/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n\nWhat should I fix for this? Thanks!", "body": "Hi all,\r\nI'm trying to implement a LayerNorm applied multi-layered LSTM using LSTMCell, but stuck. For simplicity, I had tried unidirectional LSTM only. Here my code is:\r\n```\r\nclass LayerNormLSTMCell(nn.LSTMCell):\r\n\r\n    def __init__(self, input_size, hidden_size, bias=True):\r\n        super().__init__(input_size, hidden_size, bias)\r\n\r\n        self.ln_ih = nn.LayerNorm(4 * hidden_size)\r\n        self.ln_hh = nn.LayerNorm(4 * hidden_size)\r\n        self.ln_ho = nn.LayerNorm(hidden_size)\r\n\r\n    def forward(self, input, hidden=None):\r\n        self.check_forward_input(input)\r\n        if hidden is None:\r\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n            cx = hx\r\n        else:\r\n            hx, cx = hidden\r\n        self.check_forward_hidden(input, hx, '[0]')\r\n        self.check_forward_hidden(input, cx, '[1]')\r\n\r\n        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\r\n                 + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\r\n        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\r\n        g = gates[:, (3 * self.hidden_size):].tanh()\r\n\r\n        cy = (f * cx) + (i * g)\r\n        hy = o * self.ln_ho(cy).tanh()\r\n        return hy, cy\r\n\r\n\r\nclass LayerNormLSTM(nn.Module):\r\n\r\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True):\r\n        super().__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.num_layers = num_layers\r\n\r\n        self.hidden = nn.ModuleList([\r\n            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size),\r\n                              hidden_size=hidden_size)\r\n            for layer in range(num_layers)\r\n        ])\r\n\r\n    def forward(self, input, hidden=None):\r\n        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\r\n        if hidden is None:\r\n            hx = input.new_zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\r\n            cx = input.new_zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\r\n            hidden = (hx, cx)\r\n        ht = input.new_zeros(seq_len, self.num_layers, batch_size, self.hidden_size)\r\n        ct = input.new_zeros(seq_len, self.num_layers, batch_size, self.hidden_size)\r\n\r\n        h, c = hidden\r\n        for t, x in enumerate(input):\r\n            for l, layer in enumerate(self.hidden):\r\n                ht[t, l], ct[t, l] = layer(x, (h[l], c[l]))\r\n                x = ht[t, l]\r\n            h, c = ht[t], ct[t]\r\n\r\n        y  = ht[:, -1, :, :].squeeze().contiguous()\r\n        hy = ht[-1, :, :, :].squeeze().contiguous()\r\n        cy = ct[-1, :, :, :].squeeze().contiguous()\r\n\r\n        return y, (hy, cy)\r\n```\r\nbut I've got an error in runtime as:\r\n```\r\n  File \"/u3/jbaik/pytorch-asr/asr/models/trainer.py\", line 336, in unit_train\r\n    loss.backward()\r\n  File \"/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\r\n```\r\nWhat should I fix for this? Thanks!\r\n"}