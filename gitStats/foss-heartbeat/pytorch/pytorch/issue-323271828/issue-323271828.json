{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7580", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7580/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7580/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7580/events", "html_url": "https://github.com/pytorch/pytorch/issues/7580", "id": 323271828, "node_id": "MDU6SXNzdWUzMjMyNzE4Mjg=", "number": 7580, "title": "[feature request]New pytorch op for something like combinatoric iterators in itertools", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2018-05-15T15:34:04Z", "updated_at": "2018-05-16T19:34:12Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, I want some ops that is similar to <a href=\"https://docs.python.org/3.6/library/itertools.html\" rel=\"nofollow\">combinatoric iterators in itertools</a>. These functions do the same thing as in itertools. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> A <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> B <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">6</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> cartesian_prod(A,B)\ntensor([[ <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">3</span>],\n        [ <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">5</span>,  <span class=\"pl-c1\">6</span>,  <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">5</span>,  <span class=\"pl-c1\">6</span>,  <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">5</span>,  <span class=\"pl-c1\">6</span>]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> combinations(A, <span class=\"pl-c1\">2</span>)\ntensor([[ <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">2</span>],\n        [ <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">3</span>]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> combinations_with_replacement(A, <span class=\"pl-c1\">2</span>)\ntensor([[ <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">3</span>],\n        [ <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">3</span>]])</pre></div>\n<p>I already have <a href=\"https://gist.github.com/zasdfgbnm/dfbe1c29ce8073dc2224c7286583fa43\">a python implementation of these ops</a>, supporting tensors of arbitrary rank, implemented using <code>torch.unbind</code>, <code>torch.stack</code>, and <code>torch.cat</code>. But this implementation is very slow, and is a bottleneck of my application.</p>\n<p>So I'm just asking, is pytorch interested in supporting these natively? If yes, I'm happy to contribute.</p>", "body_text": "Hi, I want some ops that is similar to combinatoric iterators in itertools. These functions do the same thing as in itertools. For example:\n>>> A = torch.tensor([1,2,3])\n>>> B = torch.tensor([4,5,6])\n>>> cartesian_prod(A,B)\ntensor([[ 1,  1,  1,  2,  2,  2,  3,  3,  3],\n        [ 4,  5,  6,  4,  5,  6,  4,  5,  6]])\n>>> combinations(A, 2)\ntensor([[ 1,  1,  2],\n        [ 2,  3,  3]])\n>>> combinations_with_replacement(A, 2)\ntensor([[ 1,  1,  1,  2,  2,  3],\n        [ 1,  2,  3,  2,  3,  3]])\nI already have a python implementation of these ops, supporting tensors of arbitrary rank, implemented using torch.unbind, torch.stack, and torch.cat. But this implementation is very slow, and is a bottleneck of my application.\nSo I'm just asking, is pytorch interested in supporting these natively? If yes, I'm happy to contribute.", "body": "Hi, I want some ops that is similar to [combinatoric iterators in itertools](https://docs.python.org/3.6/library/itertools.html). These functions do the same thing as in itertools. For example:\r\n```python\r\n>>> A = torch.tensor([1,2,3])\r\n>>> B = torch.tensor([4,5,6])\r\n>>> cartesian_prod(A,B)\r\ntensor([[ 1,  1,  1,  2,  2,  2,  3,  3,  3],\r\n        [ 4,  5,  6,  4,  5,  6,  4,  5,  6]])\r\n>>> combinations(A, 2)\r\ntensor([[ 1,  1,  2],\r\n        [ 2,  3,  3]])\r\n>>> combinations_with_replacement(A, 2)\r\ntensor([[ 1,  1,  1,  2,  2,  3],\r\n        [ 1,  2,  3,  2,  3,  3]])\r\n```\r\nI already have [a python implementation of these ops](https://gist.github.com/zasdfgbnm/dfbe1c29ce8073dc2224c7286583fa43), supporting tensors of arbitrary rank, implemented using `torch.unbind`, `torch.stack`, and `torch.cat`. But this implementation is very slow, and is a bottleneck of my application.\r\n\r\nSo I'm just asking, is pytorch interested in supporting these natively? If yes, I'm happy to contribute."}