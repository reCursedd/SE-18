{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/389234696", "html_url": "https://github.com/pytorch/pytorch/issues/7580#issuecomment-389234696", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7580", "id": 389234696, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTIzNDY5Ng==", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-15T16:44:33Z", "updated_at": "2018-05-15T16:45:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> Yes, the above example can be done easily by the following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> _A <span class=\"pl-k\">=</span> A.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>).expand(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>).reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> _B <span class=\"pl-k\">=</span> B.repeat(<span class=\"pl-c1\">3</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.stack([_A, _B])\ntensor([[ <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">2</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">3</span>,  <span class=\"pl-c1\">3</span>],\n        [ <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">5</span>,  <span class=\"pl-c1\">6</span>,  <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">5</span>,  <span class=\"pl-c1\">6</span>,  <span class=\"pl-c1\">4</span>,  <span class=\"pl-c1\">5</span>,  <span class=\"pl-c1\">6</span>]])</pre></div>\n<p>But if you have a high rank tensor, say (100, 100000, 10), and you want to do Cartesian product on <code>dim=1</code>, It requires much more effort on doing so.</p>\n<p>Also, I don't find a way to do <code>combinations</code> efficiently. The thing is, if the pytorch team don't think these ops are helpful enough to add it to pytorch, then there is nothing to worry about; but if the pytorch team do think these ops are helpful, since <code>combinations</code> and <code>combinations_with_replacement</code> are doing very similar job as <code>cartesian_prod</code>, then it might be better to make <code>cartesian_prod</code> come together rather than force the users to implement it by themselves.</p>", "body_text": "@fmassa Yes, the above example can be done easily by the following code:\n>>> _A = A.view(-1,1).expand(3,3).reshape(-1)\n>>> _B = B.repeat(3)\n>>> torch.stack([_A, _B])\ntensor([[ 1,  1,  1,  2,  2,  2,  3,  3,  3],\n        [ 4,  5,  6,  4,  5,  6,  4,  5,  6]])\nBut if you have a high rank tensor, say (100, 100000, 10), and you want to do Cartesian product on dim=1, It requires much more effort on doing so.\nAlso, I don't find a way to do combinations efficiently. The thing is, if the pytorch team don't think these ops are helpful enough to add it to pytorch, then there is nothing to worry about; but if the pytorch team do think these ops are helpful, since combinations and combinations_with_replacement are doing very similar job as cartesian_prod, then it might be better to make cartesian_prod come together rather than force the users to implement it by themselves.", "body": "@fmassa Yes, the above example can be done easily by the following code:\r\n```python\r\n>>> _A = A.view(-1,1).expand(3,3).reshape(-1)\r\n>>> _B = B.repeat(3)\r\n>>> torch.stack([_A, _B])\r\ntensor([[ 1,  1,  1,  2,  2,  2,  3,  3,  3],\r\n        [ 4,  5,  6,  4,  5,  6,  4,  5,  6]])\r\n```\r\nBut if you have a high rank tensor, say (100, 100000, 10), and you want to do Cartesian product on `dim=1`, It requires much more effort on doing so.\r\n\r\nAlso, I don't find a way to do `combinations` efficiently. The thing is, if the pytorch team don't think these ops are helpful enough to add it to pytorch, then there is nothing to worry about; but if the pytorch team do think these ops are helpful, since `combinations` and `combinations_with_replacement` are doing very similar job as `cartesian_prod`, then it might be better to make `cartesian_prod` come together rather than force the users to implement it by themselves."}