{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/273407990", "html_url": "https://github.com/pytorch/pytorch/issues/467#issuecomment-273407990", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/467", "id": 273407990, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzQwNzk5MA==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T07:58:23Z", "updated_at": "2017-01-18T07:58:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Well, in context the OpenNMT code was fooling directly with gradients (basically, what I was talking about earlier re: saving memory by backpropagating the softmax for each decoder timestep individually), effectively using undocumented parts of the autograd API. That API has changed, so any direct reference to <code>grad</code> should have become <code>grad.data</code>; this problem cleared up when we fixed those.<br>\nBut in principle there could still be something missing here? If Functions automatically wrapped non-Variable inputs in Variable that would definitely be enough to cover cases like this (were they likely to come up in non-user-error situations).</p>", "body_text": "Well, in context the OpenNMT code was fooling directly with gradients (basically, what I was talking about earlier re: saving memory by backpropagating the softmax for each decoder timestep individually), effectively using undocumented parts of the autograd API. That API has changed, so any direct reference to grad should have become grad.data; this problem cleared up when we fixed those.\nBut in principle there could still be something missing here? If Functions automatically wrapped non-Variable inputs in Variable that would definitely be enough to cover cases like this (were they likely to come up in non-user-error situations).", "body": "Well, in context the OpenNMT code was fooling directly with gradients (basically, what I was talking about earlier re: saving memory by backpropagating the softmax for each decoder timestep individually), effectively using undocumented parts of the autograd API. That API has changed, so any direct reference to `grad` should have become `grad.data`; this problem cleared up when we fixed those.\r\nBut in principle there could still be something missing here? If Functions automatically wrapped non-Variable inputs in Variable that would definitely be enough to cover cases like this (were they likely to come up in non-user-error situations)."}