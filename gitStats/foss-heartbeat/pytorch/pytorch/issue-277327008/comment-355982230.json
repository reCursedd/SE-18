{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355982230", "html_url": "https://github.com/pytorch/pytorch/issues/3917#issuecomment-355982230", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3917", "id": 355982230, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTk4MjIzMA==", "user": {"login": "chenyangh", "id": 8120212, "node_id": "MDQ6VXNlcjgxMjAyMTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenyangh", "html_url": "https://github.com/chenyangh", "followers_url": "https://api.github.com/users/chenyangh/followers", "following_url": "https://api.github.com/users/chenyangh/following{/other_user}", "gists_url": "https://api.github.com/users/chenyangh/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenyangh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenyangh/subscriptions", "organizations_url": "https://api.github.com/users/chenyangh/orgs", "repos_url": "https://api.github.com/users/chenyangh/repos", "events_url": "https://api.github.com/users/chenyangh/events{/privacy}", "received_events_url": "https://api.github.com/users/chenyangh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-08T14:33:26Z", "updated_at": "2018-01-08T14:33:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>  I think I just found out the reason. My code is still a mess and I am ashamed of sharing it right now. :(<br>\nThe unevenly GPU usage in my case is caused by some intermediate Tensors in the main loop. I don't quite know how you manage the variables that lost references, but they seem to be staying on the GPU memory unless specifically 'del' it.<br>\nMy main loop is like below.</p>\n<pre><code>        for i, (data, lable) in enumerate(data_loader):\n            # print('i=%d: ' % (i))\n            decoder_logit = model(Variable(data).cuda())\n            optimizer.zero_grad()\n            loss = loss_criterion(decoder_logit, lable )\n            train_loss_sum += loss.data[0]\n            loss.backward()\n            optimizer.step()\n           # del loss, decoder_logit\n</code></pre>\n<p>if I dont delete the  loss and decoder_logit, the memory usinage will increase in the main GPU until full( w/ or w/o DataParallel). But with DataParallel, the 'loss' and 'decoder_logit' are actually not on the 'slave gpus'. So the memory usage on 'slave GPUs' will be far less than that on the 'master GPU'</p>\n<p>I think I can say this issue is caused by the memory leak in the main thread.</p>", "body_text": "@apaszke  I think I just found out the reason. My code is still a mess and I am ashamed of sharing it right now. :(\nThe unevenly GPU usage in my case is caused by some intermediate Tensors in the main loop. I don't quite know how you manage the variables that lost references, but they seem to be staying on the GPU memory unless specifically 'del' it.\nMy main loop is like below.\n        for i, (data, lable) in enumerate(data_loader):\n            # print('i=%d: ' % (i))\n            decoder_logit = model(Variable(data).cuda())\n            optimizer.zero_grad()\n            loss = loss_criterion(decoder_logit, lable )\n            train_loss_sum += loss.data[0]\n            loss.backward()\n            optimizer.step()\n           # del loss, decoder_logit\n\nif I dont delete the  loss and decoder_logit, the memory usinage will increase in the main GPU until full( w/ or w/o DataParallel). But with DataParallel, the 'loss' and 'decoder_logit' are actually not on the 'slave gpus'. So the memory usage on 'slave GPUs' will be far less than that on the 'master GPU'\nI think I can say this issue is caused by the memory leak in the main thread.", "body": "@apaszke  I think I just found out the reason. My code is still a mess and I am ashamed of sharing it right now. :(\r\nThe unevenly GPU usage in my case is caused by some intermediate Tensors in the main loop. I don't quite know how you manage the variables that lost references, but they seem to be staying on the GPU memory unless specifically 'del' it.\r\nMy main loop is like below.\r\n```\r\n        for i, (data, lable) in enumerate(data_loader):\r\n            # print('i=%d: ' % (i))\r\n            decoder_logit = model(Variable(data).cuda())\r\n            optimizer.zero_grad()\r\n            loss = loss_criterion(decoder_logit, lable )\r\n            train_loss_sum += loss.data[0]\r\n            loss.backward()\r\n            optimizer.step()\r\n           # del loss, decoder_logit\r\n```\r\nif I dont delete the  loss and decoder_logit, the memory usinage will increase in the main GPU until full( w/ or w/o DataParallel). But with DataParallel, the 'loss' and 'decoder_logit' are actually not on the 'slave gpus'. So the memory usage on 'slave GPUs' will be far less than that on the 'master GPU'\r\n\r\nI think I can say this issue is caused by the memory leak in the main thread."}