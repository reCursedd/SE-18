{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/347482874", "html_url": "https://github.com/pytorch/pytorch/issues/3917#issuecomment-347482874", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3917", "id": 347482874, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzQ4Mjg3NA==", "user": {"login": "al3xsh", "id": 10220372, "node_id": "MDQ6VXNlcjEwMjIwMzcy", "avatar_url": "https://avatars1.githubusercontent.com/u/10220372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/al3xsh", "html_url": "https://github.com/al3xsh", "followers_url": "https://api.github.com/users/al3xsh/followers", "following_url": "https://api.github.com/users/al3xsh/following{/other_user}", "gists_url": "https://api.github.com/users/al3xsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/al3xsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/al3xsh/subscriptions", "organizations_url": "https://api.github.com/users/al3xsh/orgs", "repos_url": "https://api.github.com/users/al3xsh/repos", "events_url": "https://api.github.com/users/al3xsh/events{/privacy}", "received_events_url": "https://api.github.com/users/al3xsh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-28T10:39:48Z", "updated_at": "2017-11-28T10:39:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>\n<p>Thanks for the quick response!  I used a batch size of 64 as that is what they used in the torch implementation.  In fact Kaiming He has shown that, in their experiments, a minibatch size of 64 actually achieves better results than 128!</p>\n<p>(Which was obviously unexpected :)</p>\n<p>Increasing the batch size to 128 gives me roughly the same time to evaluate each batch (1.4s) as with a batch size of 64 (but obviously will result in half the time per epoch!).</p>\n<p>I was just a bit confused as to why the speed up was so much worse than the torch implementation - any ideas?</p>\n<p>Regards,</p>\n<p>Alex</p>", "body_text": "@apaszke\nThanks for the quick response!  I used a batch size of 64 as that is what they used in the torch implementation.  In fact Kaiming He has shown that, in their experiments, a minibatch size of 64 actually achieves better results than 128!\n(Which was obviously unexpected :)\nIncreasing the batch size to 128 gives me roughly the same time to evaluate each batch (1.4s) as with a batch size of 64 (but obviously will result in half the time per epoch!).\nI was just a bit confused as to why the speed up was so much worse than the torch implementation - any ideas?\nRegards,\nAlex", "body": "@apaszke \r\n\r\nThanks for the quick response!  I used a batch size of 64 as that is what they used in the torch implementation.  In fact Kaiming He has shown that, in their experiments, a minibatch size of 64 actually achieves better results than 128!\r\n\r\n(Which was obviously unexpected :) \r\n\r\nIncreasing the batch size to 128 gives me roughly the same time to evaluate each batch (1.4s) as with a batch size of 64 (but obviously will result in half the time per epoch!).  \r\n\r\nI was just a bit confused as to why the speed up was so much worse than the torch implementation - any ideas?\r\n\r\nRegards,\r\n\r\nAlex"}