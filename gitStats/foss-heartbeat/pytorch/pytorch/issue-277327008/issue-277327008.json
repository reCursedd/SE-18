{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3917", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3917/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3917/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3917/events", "html_url": "https://github.com/pytorch/pytorch/issues/3917", "id": 277327008, "node_id": "MDU6SXNzdWUyNzczMjcwMDg=", "number": 3917, "title": "Data Parallel slows things down - ResNet 1001", "user": {"login": "al3xsh", "id": 10220372, "node_id": "MDQ6VXNlcjEwMjIwMzcy", "avatar_url": "https://avatars1.githubusercontent.com/u/10220372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/al3xsh", "html_url": "https://github.com/al3xsh", "followers_url": "https://api.github.com/users/al3xsh/followers", "following_url": "https://api.github.com/users/al3xsh/following{/other_user}", "gists_url": "https://api.github.com/users/al3xsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/al3xsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/al3xsh/subscriptions", "organizations_url": "https://api.github.com/users/al3xsh/orgs", "repos_url": "https://api.github.com/users/al3xsh/repos", "events_url": "https://api.github.com/users/al3xsh/events{/privacy}", "received_events_url": "https://api.github.com/users/al3xsh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-11-28T10:12:05Z", "updated_at": "2018-10-09T13:48:20Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I am attempting to train ResNet 1001 on Cifar 10 using multiple GPUs (I have 4 x Titan X Maxwell cards) using DataParallel.  However, it seems to train faster on one GPU than on multiple GPUs.  I have attached screenshots below.</p>\n<p>4 GPUs:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10220372/33313461-9b8caaac-d422-11e7-882d-98c81c0fd2a4.png\"><img src=\"https://user-images.githubusercontent.com/10220372/33313461-9b8caaac-d422-11e7-882d-98c81c0fd2a4.png\" alt=\"4 GPUs\" style=\"max-width:100%;\"></a></p>\n<p>1 GPU:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10220372/33313474-a447228a-d422-11e7-9c53-88f3f9ade7c5.png\"><img src=\"https://user-images.githubusercontent.com/10220372/33313474-a447228a-d422-11e7-9c53-88f3f9ade7c5.png\" alt=\"Single GPU\" style=\"max-width:100%;\"></a></p>\n<p>(For reference, the torch implementation trains a batch in around 0.4 - 0.45s).</p>\n<p>Using nvidia-smi I can verify that it is using multiple GPUs:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10220372/33313993-35b7951e-d424-11e7-920b-cadc11e02045.png\"><img src=\"https://user-images.githubusercontent.com/10220372/33313993-35b7951e-d424-11e7-920b-cadc11e02045.png\" alt=\"nvidia-smi\" style=\"max-width:100%;\"></a></p>\n<p>But there isn't the speed up I was expecting ...</p>\n<p>My code uses:</p>\n<p><code>net = torch.nn.DataParallel(net).cuda()</code></p>\n<p>to set up the network across all the available GPUs, and</p>\n<p><code>input = input.cuda(async=True) target = target.cuda(async=True) input_var = torch.autograd.Variable(input) target_var = torch.autograd.Variable(target)</code></p>\n<p>to send the variables to the GPUS.</p>\n<p>The full code is here: <a href=\"https://github.com/al3xsh/pytorch-models\">https://github.com/al3xsh/pytorch-models</a></p>\n<p>Can anybody tell me what am I missing? (I'm sure it's something obvious :)</p>\n<p>Regards,</p>\n<p>Alex</p>\n<p>nb. I am using version 0.2.0_4 &amp; python 3.6 installed via the conda instructions on the installation page.</p>", "body_text": "Hi,\nI am attempting to train ResNet 1001 on Cifar 10 using multiple GPUs (I have 4 x Titan X Maxwell cards) using DataParallel.  However, it seems to train faster on one GPU than on multiple GPUs.  I have attached screenshots below.\n4 GPUs:\n\n1 GPU:\n\n(For reference, the torch implementation trains a batch in around 0.4 - 0.45s).\nUsing nvidia-smi I can verify that it is using multiple GPUs:\n\nBut there isn't the speed up I was expecting ...\nMy code uses:\nnet = torch.nn.DataParallel(net).cuda()\nto set up the network across all the available GPUs, and\ninput = input.cuda(async=True) target = target.cuda(async=True) input_var = torch.autograd.Variable(input) target_var = torch.autograd.Variable(target)\nto send the variables to the GPUS.\nThe full code is here: https://github.com/al3xsh/pytorch-models\nCan anybody tell me what am I missing? (I'm sure it's something obvious :)\nRegards,\nAlex\nnb. I am using version 0.2.0_4 & python 3.6 installed via the conda instructions on the installation page.", "body": "Hi,\r\n\r\nI am attempting to train ResNet 1001 on Cifar 10 using multiple GPUs (I have 4 x Titan X Maxwell cards) using DataParallel.  However, it seems to train faster on one GPU than on multiple GPUs.  I have attached screenshots below.\r\n\r\n4 GPUs:\r\n![4 GPUs](https://user-images.githubusercontent.com/10220372/33313461-9b8caaac-d422-11e7-882d-98c81c0fd2a4.png)\r\n\r\n1 GPU:\r\n![Single GPU](https://user-images.githubusercontent.com/10220372/33313474-a447228a-d422-11e7-9c53-88f3f9ade7c5.png)\r\n\r\n(For reference, the torch implementation trains a batch in around 0.4 - 0.45s).\r\n\r\nUsing nvidia-smi I can verify that it is using multiple GPUs:\r\n\r\n![nvidia-smi](https://user-images.githubusercontent.com/10220372/33313993-35b7951e-d424-11e7-920b-cadc11e02045.png)\r\n\r\n\r\nBut there isn't the speed up I was expecting ...\r\n\r\nMy code uses:\r\n\r\n`net = torch.nn.DataParallel(net).cuda()`\r\n\r\nto set up the network across all the available GPUs, and\r\n\r\n`\r\n        input = input.cuda(async=True)\r\n        target = target.cuda(async=True)\r\n        input_var = torch.autograd.Variable(input)\r\n        target_var = torch.autograd.Variable(target)\r\n`\r\n\r\nto send the variables to the GPUS.\r\n\r\nThe full code is here: https://github.com/al3xsh/pytorch-models\r\n\r\nCan anybody tell me what am I missing? (I'm sure it's something obvious :)\r\n\r\nRegards,\r\n\r\nAlex\r\n\r\nnb. I am using version 0.2.0_4 & python 3.6 installed via the conda instructions on the installation page."}