{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409778293", "html_url": "https://github.com/pytorch/pytorch/issues/4858#issuecomment-409778293", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4858", "id": 409778293, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTc3ODI5Mw==", "user": {"login": "rfejgin", "id": 345348, "node_id": "MDQ6VXNlcjM0NTM0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/345348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rfejgin", "html_url": "https://github.com/rfejgin", "followers_url": "https://api.github.com/users/rfejgin/followers", "following_url": "https://api.github.com/users/rfejgin/following{/other_user}", "gists_url": "https://api.github.com/users/rfejgin/gists{/gist_id}", "starred_url": "https://api.github.com/users/rfejgin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rfejgin/subscriptions", "organizations_url": "https://api.github.com/users/rfejgin/orgs", "repos_url": "https://api.github.com/users/rfejgin/repos", "events_url": "https://api.github.com/users/rfejgin/events{/privacy}", "received_events_url": "https://api.github.com/users/rfejgin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-02T01:44:50Z", "updated_at": "2018-08-02T01:44:50Z", "author_association": "NONE", "body_html": "<p>It looks like the problem is broader than we initially thought. Zero probability events can get selected anywhere in the distribution, not only just if they are the first or last bin. The script below reproduces the problem in bins in the middle of the distribution the in just a few seconds.</p>\n<p>...</p>\n<pre><code>import sys\nimport torch\n\nbatch_size = 1024\ndist_size = 2048\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\nweights = torch.zeros(batch_size, dist_size).cuda()\nwith torch.no_grad():\n    k = 0\n    while True:\n        if k % 10 == 0:\n            print(\"iteration %d\" % k)\n        # create a different unnormalized PDF each time.\n        weights.uniform_(0.4, 0.6) # use rather large weights\n\n        # zero out about half of the probabilities\n        cond = torch.rand(batch_size, dist_size).ge(0.5)\n        weights[cond] = 0\n        assert(weights.sum(dim=1).gt(0).all().item()) # make sure we didn't accidentally zero out all the weights in a distribution\n\n        # Sample\n        s = weights.multinomial(1).squeeze(1)\n\n        # Check if any selected samples had zero probability\n        selected_probs = weights[torch.arange(batch_size, dtype=torch.long), s]\n        if not selected_probs.eq(0).sum() == 0:\n            # Bug detected. Print out some debug info.\n            print(\"\\nError: Zero-probability event sampled!\")\n            for i in range(batch_size):\n                if selected_probs[i] == 0:\n                        print(\"bin %d out of %d in distribution %d was chosen even though its probability is %.9f\" %(s[i], dist_size, i, selected_probs[i]))\n            sys.exit(1)\n        k = k + 1\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>iteration 0\niteration 10\niteration 20\niteration 30\niteration 40\niteration 50\niteration 60\niteration 70\niteration 80\n\nError: Zero-probability event sampled!\nbin 1679 out of 2048 in distribution 461 was chosen even though its probability is 0.000000000\n</code></pre>", "body_text": "It looks like the problem is broader than we initially thought. Zero probability events can get selected anywhere in the distribution, not only just if they are the first or last bin. The script below reproduces the problem in bins in the middle of the distribution the in just a few seconds.\n...\nimport sys\nimport torch\n\nbatch_size = 1024\ndist_size = 2048\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\nweights = torch.zeros(batch_size, dist_size).cuda()\nwith torch.no_grad():\n    k = 0\n    while True:\n        if k % 10 == 0:\n            print(\"iteration %d\" % k)\n        # create a different unnormalized PDF each time.\n        weights.uniform_(0.4, 0.6) # use rather large weights\n\n        # zero out about half of the probabilities\n        cond = torch.rand(batch_size, dist_size).ge(0.5)\n        weights[cond] = 0\n        assert(weights.sum(dim=1).gt(0).all().item()) # make sure we didn't accidentally zero out all the weights in a distribution\n\n        # Sample\n        s = weights.multinomial(1).squeeze(1)\n\n        # Check if any selected samples had zero probability\n        selected_probs = weights[torch.arange(batch_size, dtype=torch.long), s]\n        if not selected_probs.eq(0).sum() == 0:\n            # Bug detected. Print out some debug info.\n            print(\"\\nError: Zero-probability event sampled!\")\n            for i in range(batch_size):\n                if selected_probs[i] == 0:\n                        print(\"bin %d out of %d in distribution %d was chosen even though its probability is %.9f\" %(s[i], dist_size, i, selected_probs[i]))\n            sys.exit(1)\n        k = k + 1\n\nOutput:\niteration 0\niteration 10\niteration 20\niteration 30\niteration 40\niteration 50\niteration 60\niteration 70\niteration 80\n\nError: Zero-probability event sampled!\nbin 1679 out of 2048 in distribution 461 was chosen even though its probability is 0.000000000", "body": "It looks like the problem is broader than we initially thought. Zero probability events can get selected anywhere in the distribution, not only just if they are the first or last bin. The script below reproduces the problem in bins in the middle of the distribution the in just a few seconds. \r\n\r\n... \r\n\r\n```\r\nimport sys\r\nimport torch\r\n\r\nbatch_size = 1024\r\ndist_size = 2048\r\ntorch.manual_seed(1)\r\ntorch.cuda.manual_seed(1)\r\nweights = torch.zeros(batch_size, dist_size).cuda()\r\nwith torch.no_grad():\r\n    k = 0\r\n    while True:\r\n        if k % 10 == 0:\r\n            print(\"iteration %d\" % k)\r\n        # create a different unnormalized PDF each time.\r\n        weights.uniform_(0.4, 0.6) # use rather large weights\r\n\r\n        # zero out about half of the probabilities\r\n        cond = torch.rand(batch_size, dist_size).ge(0.5)\r\n        weights[cond] = 0\r\n        assert(weights.sum(dim=1).gt(0).all().item()) # make sure we didn't accidentally zero out all the weights in a distribution\r\n\r\n        # Sample\r\n        s = weights.multinomial(1).squeeze(1)\r\n\r\n        # Check if any selected samples had zero probability\r\n        selected_probs = weights[torch.arange(batch_size, dtype=torch.long), s]\r\n        if not selected_probs.eq(0).sum() == 0:\r\n            # Bug detected. Print out some debug info.\r\n            print(\"\\nError: Zero-probability event sampled!\")\r\n            for i in range(batch_size):\r\n                if selected_probs[i] == 0:\r\n                        print(\"bin %d out of %d in distribution %d was chosen even though its probability is %.9f\" %(s[i], dist_size, i, selected_probs[i]))\r\n            sys.exit(1)\r\n        k = k + 1\r\n```\r\n**Output:**\r\n```\r\niteration 0\r\niteration 10\r\niteration 20\r\niteration 30\r\niteration 40\r\niteration 50\r\niteration 60\r\niteration 70\r\niteration 80\r\n\r\nError: Zero-probability event sampled!\r\nbin 1679 out of 2048 in distribution 461 was chosen even though its probability is 0.000000000\r\n```"}