{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/368793672", "html_url": "https://github.com/pytorch/pytorch/issues/4858#issuecomment-368793672", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4858", "id": 368793672, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODc5MzY3Mg==", "user": {"login": "wouterkool", "id": 1150455, "node_id": "MDQ6VXNlcjExNTA0NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1150455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wouterkool", "html_url": "https://github.com/wouterkool", "followers_url": "https://api.github.com/users/wouterkool/followers", "following_url": "https://api.github.com/users/wouterkool/following{/other_user}", "gists_url": "https://api.github.com/users/wouterkool/gists{/gist_id}", "starred_url": "https://api.github.com/users/wouterkool/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wouterkool/subscriptions", "organizations_url": "https://api.github.com/users/wouterkool/orgs", "repos_url": "https://api.github.com/users/wouterkool/repos", "events_url": "https://api.github.com/users/wouterkool/events{/privacy}", "received_events_url": "https://api.github.com/users/wouterkool/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-27T08:58:05Z", "updated_at": "2018-02-27T08:58:05Z", "author_association": "NONE", "body_html": "<p>I did not know this issue was open and also created a case <a href=\"https://discuss.pytorch.org/t/bad-behavior-of-multinomial-function/10232/3\" rel=\"nofollow\">here</a>. If it is any help, I found that it depends on the range of the values as well, if I change logits_range in the script below to 1 or 100 it does not happen.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef test(n, hot=False, logits_range=10):\n    torch.manual_seed(1234)\n\n    logits = Variable(torch.Tensor(128, 50).uniform_(-logits_range, logits_range).cuda())\n\n    # Set randomly 40 elements per row to 0\n    mask = torch.zeros_like(logits).byte()\n    _, idx_mask = Variable(torch.Tensor(128, 50).uniform_(0, 1).cuda()).topk(40, 1)\n    mask.scatter_(1, idx_mask, True)\n\n    logits[mask] = -np.inf\n\n    probs = F.softmax(logits, dim=1)\n\n    assert (probs[mask] == 0).all()\n    assert (torch.abs(probs.sum(1) - 1) &lt; 1e-6).all()\n\n    if hot:\n        with open('rng_state.pt', 'rb') as f:\n            rng_state = torch.load(f)\n        torch.cuda.set_rng_state(rng_state)\n\n    for j in tqdm(range(n)):\n\n        rng_state = torch.cuda.get_rng_state()\n\n        sample = probs.multinomial(1).squeeze(-1)\n        mask_sample = mask.gather(1, sample.unsqueeze(-1)).squeeze(-1)\n\n        if mask_sample.any():\n            print(\"Sampled value that was masked and had probability 0 in iteration {}\".format(j))\n            wrong = torch.nonzero(mask_sample).squeeze(-1)\n            print(\"Wrong samples: indices {}, sampled {}, probs {}\".format(\n                wrong.data.cpu().numpy().tolist(),\n                sample[wrong].data.cpu().numpy().tolist(),\n                probs[wrong, sample[wrong]].data.cpu().numpy().tolist()\n            ))\n\n            if hot:\n                break\n\n            with open('rng_state.pt', 'wb') as f:\n                torch.save(rng_state, f)\n\n\nif __name__ == \"__main__\":\n    with torch.cuda.device(0):\n        test(100000, hot=False)\n</code></pre>", "body_text": "I did not know this issue was open and also created a case here. If it is any help, I found that it depends on the range of the values as well, if I change logits_range in the script below to 1 or 100 it does not happen.\nimport torch\nfrom torch.autograd import Variable\n\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef test(n, hot=False, logits_range=10):\n    torch.manual_seed(1234)\n\n    logits = Variable(torch.Tensor(128, 50).uniform_(-logits_range, logits_range).cuda())\n\n    # Set randomly 40 elements per row to 0\n    mask = torch.zeros_like(logits).byte()\n    _, idx_mask = Variable(torch.Tensor(128, 50).uniform_(0, 1).cuda()).topk(40, 1)\n    mask.scatter_(1, idx_mask, True)\n\n    logits[mask] = -np.inf\n\n    probs = F.softmax(logits, dim=1)\n\n    assert (probs[mask] == 0).all()\n    assert (torch.abs(probs.sum(1) - 1) < 1e-6).all()\n\n    if hot:\n        with open('rng_state.pt', 'rb') as f:\n            rng_state = torch.load(f)\n        torch.cuda.set_rng_state(rng_state)\n\n    for j in tqdm(range(n)):\n\n        rng_state = torch.cuda.get_rng_state()\n\n        sample = probs.multinomial(1).squeeze(-1)\n        mask_sample = mask.gather(1, sample.unsqueeze(-1)).squeeze(-1)\n\n        if mask_sample.any():\n            print(\"Sampled value that was masked and had probability 0 in iteration {}\".format(j))\n            wrong = torch.nonzero(mask_sample).squeeze(-1)\n            print(\"Wrong samples: indices {}, sampled {}, probs {}\".format(\n                wrong.data.cpu().numpy().tolist(),\n                sample[wrong].data.cpu().numpy().tolist(),\n                probs[wrong, sample[wrong]].data.cpu().numpy().tolist()\n            ))\n\n            if hot:\n                break\n\n            with open('rng_state.pt', 'wb') as f:\n                torch.save(rng_state, f)\n\n\nif __name__ == \"__main__\":\n    with torch.cuda.device(0):\n        test(100000, hot=False)", "body": "I did not know this issue was open and also created a case [here](https://discuss.pytorch.org/t/bad-behavior-of-multinomial-function/10232/3). If it is any help, I found that it depends on the range of the values as well, if I change logits_range in the script below to 1 or 100 it does not happen.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\n\r\ndef test(n, hot=False, logits_range=10):\r\n    torch.manual_seed(1234)\r\n\r\n    logits = Variable(torch.Tensor(128, 50).uniform_(-logits_range, logits_range).cuda())\r\n\r\n    # Set randomly 40 elements per row to 0\r\n    mask = torch.zeros_like(logits).byte()\r\n    _, idx_mask = Variable(torch.Tensor(128, 50).uniform_(0, 1).cuda()).topk(40, 1)\r\n    mask.scatter_(1, idx_mask, True)\r\n\r\n    logits[mask] = -np.inf\r\n\r\n    probs = F.softmax(logits, dim=1)\r\n\r\n    assert (probs[mask] == 0).all()\r\n    assert (torch.abs(probs.sum(1) - 1) < 1e-6).all()\r\n\r\n    if hot:\r\n        with open('rng_state.pt', 'rb') as f:\r\n            rng_state = torch.load(f)\r\n        torch.cuda.set_rng_state(rng_state)\r\n\r\n    for j in tqdm(range(n)):\r\n\r\n        rng_state = torch.cuda.get_rng_state()\r\n\r\n        sample = probs.multinomial(1).squeeze(-1)\r\n        mask_sample = mask.gather(1, sample.unsqueeze(-1)).squeeze(-1)\r\n\r\n        if mask_sample.any():\r\n            print(\"Sampled value that was masked and had probability 0 in iteration {}\".format(j))\r\n            wrong = torch.nonzero(mask_sample).squeeze(-1)\r\n            print(\"Wrong samples: indices {}, sampled {}, probs {}\".format(\r\n                wrong.data.cpu().numpy().tolist(),\r\n                sample[wrong].data.cpu().numpy().tolist(),\r\n                probs[wrong, sample[wrong]].data.cpu().numpy().tolist()\r\n            ))\r\n\r\n            if hot:\r\n                break\r\n\r\n            with open('rng_state.pt', 'wb') as f:\r\n                torch.save(rng_state, f)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    with torch.cuda.device(0):\r\n        test(100000, hot=False)\r\n```"}