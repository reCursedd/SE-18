{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386546618", "html_url": "https://github.com/pytorch/pytorch/issues/7277#issuecomment-386546618", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7277", "id": 386546618, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjU0NjYxOA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-04T09:17:05Z", "updated_at": "2018-05-04T09:17:47Z", "author_association": "MEMBER", "body_html": "<p>Hum, I think this can get confusing.<br>\nDon't we have type inference for <code>torch.tensor</code>? Why can't we then have a <code>device</code> inference as well?</p>\n<p>If <code>torch.as_tensor</code> does device inference as well, then I'm ok with it, but we really need an option somewhere (either in <code>as_tensor</code>, or in <code>tensor</code> as well) that creates a tensor and infers the device from the data that was sent.</p>", "body_text": "Hum, I think this can get confusing.\nDon't we have type inference for torch.tensor? Why can't we then have a device inference as well?\nIf torch.as_tensor does device inference as well, then I'm ok with it, but we really need an option somewhere (either in as_tensor, or in tensor as well) that creates a tensor and infers the device from the data that was sent.", "body": "Hum, I think this can get confusing.\r\nDon't we have type inference for `torch.tensor`? Why can't we then have a `device` inference as well?\r\n\r\nIf `torch.as_tensor` does device inference as well, then I'm ok with it, but we really need an option somewhere (either in `as_tensor`, or in `tensor` as well) that creates a tensor and infers the device from the data that was sent."}