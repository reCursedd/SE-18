{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2017", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2017/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2017/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2017/events", "html_url": "https://github.com/pytorch/pytorch/issues/2017", "id": 241418464, "node_id": "MDU6SXNzdWUyNDE0MTg0NjQ=", "number": 2017, "title": "torch.cuda.current_device() is always 0 at backward in DataParallel", "user": {"login": "hzaskywalker", "id": 3241589, "node_id": "MDQ6VXNlcjMyNDE1ODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3241589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hzaskywalker", "html_url": "https://github.com/hzaskywalker", "followers_url": "https://api.github.com/users/hzaskywalker/followers", "following_url": "https://api.github.com/users/hzaskywalker/following{/other_user}", "gists_url": "https://api.github.com/users/hzaskywalker/gists{/gist_id}", "starred_url": "https://api.github.com/users/hzaskywalker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hzaskywalker/subscriptions", "organizations_url": "https://api.github.com/users/hzaskywalker/orgs", "repos_url": "https://api.github.com/users/hzaskywalker/repos", "events_url": "https://api.github.com/users/hzaskywalker/events{/privacy}", "received_events_url": "https://api.github.com/users/hzaskywalker/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2017-07-08T02:25:49Z", "updated_at": "2017-07-13T20:24:38Z", "closed_at": "2017-07-13T20:24:38Z", "author_association": "NONE", "body_html": "<p>Consider following code, and run it with multi gpus (e.g. 4):</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.autograd import Function\n\nclass Func(Function):\n    def forward(self, x):\n        print('forward devices', torch.cuda.current_device())\n        return x\n\n    def backward(self, grad):\n        print('backward devices', torch.cuda.current_device())\n        return grad\n\nclass Module(nn.Module):\n    def forward(self, x):\n        return Func()(x)\n\nf = nn.DataParallel(Module().cuda())\nx = torch.autograd.Variable( torch.zeros(4, 1).cuda(), requires_grad=True)\nloss = f(x)\nloss.sum().backward()\n</code></pre>\n<p>It will output:</p>\n<pre><code>forward devices 0\nforward devices 1\nforward devices 2\nforward devices 3\nbackward devices 0\nbackward devices 0\nbackward devices 0\nbackward devices 0\n</code></pre>\n<p>That is, the default device would always be zero even in DataParallel. My pytorch version is 0.1.12_2.</p>\n<p>I think this is not the desired behavior. It would cause some troubles. I tried to insert my own cuda kernel into backward to calculate the gradients, it become very slow, and I fixed it by torch.cuda.set_device(grad.get_device()).</p>\n<p>Anyway, I think current_device in forward and backward should be the same, or could anyone explain to me why they are different?</p>", "body_text": "Consider following code, and run it with multi gpus (e.g. 4):\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\n\nclass Func(Function):\n    def forward(self, x):\n        print('forward devices', torch.cuda.current_device())\n        return x\n\n    def backward(self, grad):\n        print('backward devices', torch.cuda.current_device())\n        return grad\n\nclass Module(nn.Module):\n    def forward(self, x):\n        return Func()(x)\n\nf = nn.DataParallel(Module().cuda())\nx = torch.autograd.Variable( torch.zeros(4, 1).cuda(), requires_grad=True)\nloss = f(x)\nloss.sum().backward()\n\nIt will output:\nforward devices 0\nforward devices 1\nforward devices 2\nforward devices 3\nbackward devices 0\nbackward devices 0\nbackward devices 0\nbackward devices 0\n\nThat is, the default device would always be zero even in DataParallel. My pytorch version is 0.1.12_2.\nI think this is not the desired behavior. It would cause some troubles. I tried to insert my own cuda kernel into backward to calculate the gradients, it become very slow, and I fixed it by torch.cuda.set_device(grad.get_device()).\nAnyway, I think current_device in forward and backward should be the same, or could anyone explain to me why they are different?", "body": "Consider following code, and run it with multi gpus (e.g. 4):\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Function\r\n\r\nclass Func(Function):\r\n    def forward(self, x):\r\n        print('forward devices', torch.cuda.current_device())\r\n        return x\r\n\r\n    def backward(self, grad):\r\n        print('backward devices', torch.cuda.current_device())\r\n        return grad\r\n\r\nclass Module(nn.Module):\r\n    def forward(self, x):\r\n        return Func()(x)\r\n\r\nf = nn.DataParallel(Module().cuda())\r\nx = torch.autograd.Variable( torch.zeros(4, 1).cuda(), requires_grad=True)\r\nloss = f(x)\r\nloss.sum().backward()\r\n```\r\n\r\nIt will output:\r\n```\r\nforward devices 0\r\nforward devices 1\r\nforward devices 2\r\nforward devices 3\r\nbackward devices 0\r\nbackward devices 0\r\nbackward devices 0\r\nbackward devices 0\r\n```\r\n\r\nThat is, the default device would always be zero even in DataParallel. My pytorch version is 0.1.12_2.\r\n\r\nI think this is not the desired behavior. It would cause some troubles. I tried to insert my own cuda kernel into backward to calculate the gradients, it become very slow, and I fixed it by torch.cuda.set_device(grad.get_device()).\r\n\r\nAnyway, I think current_device in forward and backward should be the same, or could anyone explain to me why they are different?"}