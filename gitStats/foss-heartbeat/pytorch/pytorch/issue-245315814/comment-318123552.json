{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/318123552", "html_url": "https://github.com/pytorch/pytorch/issues/2198#issuecomment-318123552", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2198", "id": 318123552, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODEyMzU1Mg==", "user": {"login": "rasoolfa", "id": 11698385, "node_id": "MDQ6VXNlcjExNjk4Mzg1", "avatar_url": "https://avatars2.githubusercontent.com/u/11698385?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rasoolfa", "html_url": "https://github.com/rasoolfa", "followers_url": "https://api.github.com/users/rasoolfa/followers", "following_url": "https://api.github.com/users/rasoolfa/following{/other_user}", "gists_url": "https://api.github.com/users/rasoolfa/gists{/gist_id}", "starred_url": "https://api.github.com/users/rasoolfa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rasoolfa/subscriptions", "organizations_url": "https://api.github.com/users/rasoolfa/orgs", "repos_url": "https://api.github.com/users/rasoolfa/repos", "events_url": "https://api.github.com/users/rasoolfa/events{/privacy}", "received_events_url": "https://api.github.com/users/rasoolfa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T17:23:00Z", "updated_at": "2017-07-26T17:24:35Z", "author_association": "NONE", "body_html": "<p>Thanks.<br>\nI guess it can be considered both. Yes, it does.<br>\nLook at the following code, it is not specific to LSTM/RNN, it happens  for any recurrent structure:</p>\n<pre><code>import numpy as np\nimport torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.output_size = output_size\n\n        self.i2h = nn.Linear( self.input_size + self.hidden_size, self.hidden_size)\n        self.i2o = nn.Linear( self.input_size + self.hidden_size, self.output_size)\n        self.softmax = nn.LogSoftmax()\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n\n        return output, hidden\n    def init_hidden(self):\n        return Variable(torch.zeros(16, self.hidden_size))\n\ncriterion = nn.NLLLoss()\nlearning_rate = 0.0005\ninput_size = 500\noutput_size = 10\nn_hidden =  1024\nrnn = RNN(input_size, n_hidden,output_size)\nrnn.cuda()\nhidden =  rnn.init_hidden().cuda()\nfor i in range(10000):\n    data = Variable(torch.rand(16,input_size).cuda())\n    out, hidden = rnn(data, hidden)\n</code></pre>\n<p>As I mentioned previously, It happens because it keeps \"all\" parameters at each time step which not are required to calculate the gradient. Take the following model as an example:</p>\n<p><code>h1 = WX1 + Uh0</code></p>\n<p>the gradient with respect to W, U, X, h0 would be:</p>\n<pre><code>dW  = dh_1*X1^T\ndU  = dh_1*h0^T\ndX  = W^T dh1\ndh0 = U^t dh1 \n</code></pre>\n<p>So the autogard is not required to save W, U if no gradient is required to be calculated for X, h0. However, in the current implementation, it saves everything regardless of  \"requires_grad\" content.</p>", "body_text": "Thanks.\nI guess it can be considered both. Yes, it does.\nLook at the following code, it is not specific to LSTM/RNN, it happens  for any recurrent structure:\nimport numpy as np\nimport torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.output_size = output_size\n\n        self.i2h = nn.Linear( self.input_size + self.hidden_size, self.hidden_size)\n        self.i2o = nn.Linear( self.input_size + self.hidden_size, self.output_size)\n        self.softmax = nn.LogSoftmax()\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n\n        return output, hidden\n    def init_hidden(self):\n        return Variable(torch.zeros(16, self.hidden_size))\n\ncriterion = nn.NLLLoss()\nlearning_rate = 0.0005\ninput_size = 500\noutput_size = 10\nn_hidden =  1024\nrnn = RNN(input_size, n_hidden,output_size)\nrnn.cuda()\nhidden =  rnn.init_hidden().cuda()\nfor i in range(10000):\n    data = Variable(torch.rand(16,input_size).cuda())\n    out, hidden = rnn(data, hidden)\n\nAs I mentioned previously, It happens because it keeps \"all\" parameters at each time step which not are required to calculate the gradient. Take the following model as an example:\nh1 = WX1 + Uh0\nthe gradient with respect to W, U, X, h0 would be:\ndW  = dh_1*X1^T\ndU  = dh_1*h0^T\ndX  = W^T dh1\ndh0 = U^t dh1 \n\nSo the autogard is not required to save W, U if no gradient is required to be calculated for X, h0. However, in the current implementation, it saves everything regardless of  \"requires_grad\" content.", "body": "Thanks.\r\nI guess it can be considered both. Yes, it does. \r\nLook at the following code, it is not specific to LSTM/RNN, it happens  for any recurrent structure:\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size, output_size):\r\n\r\n        super(RNN, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.input_size = input_size\r\n        self.output_size = output_size\r\n\r\n        self.i2h = nn.Linear( self.input_size + self.hidden_size, self.hidden_size)\r\n        self.i2o = nn.Linear( self.input_size + self.hidden_size, self.output_size)\r\n        self.softmax = nn.LogSoftmax()\r\n\r\n    def forward(self, input, hidden):\r\n        combined = torch.cat((input, hidden), 1)\r\n        hidden = self.i2h(combined)\r\n        output = self.i2o(combined)\r\n        output = self.softmax(output)\r\n\r\n        return output, hidden\r\n    def init_hidden(self):\r\n        return Variable(torch.zeros(16, self.hidden_size))\r\n\r\ncriterion = nn.NLLLoss()\r\nlearning_rate = 0.0005\r\ninput_size = 500\r\noutput_size = 10\r\nn_hidden =  1024\r\nrnn = RNN(input_size, n_hidden,output_size)\r\nrnn.cuda()\r\nhidden =  rnn.init_hidden().cuda()\r\nfor i in range(10000):\r\n    data = Variable(torch.rand(16,input_size).cuda())\r\n    out, hidden = rnn(data, hidden)\r\n```\r\n\r\n\r\nAs I mentioned previously, It happens because it keeps \"all\" parameters at each time step which not are required to calculate the gradient. Take the following model as an example:\r\n\r\n\r\n`h1 = WX1 + Uh0`\r\n\r\nthe gradient with respect to W, U, X, h0 would be:\r\n \r\n```\r\ndW  = dh_1*X1^T\r\ndU  = dh_1*h0^T\r\ndX  = W^T dh1\r\ndh0 = U^t dh1 \r\n```\r\n \r\nSo the autogard is not required to save W, U if no gradient is required to be calculated for X, h0. However, in the current implementation, it saves everything regardless of  \"requires_grad\" content.\r\n\r\n\r\n"}