{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2198", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2198/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2198/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2198/events", "html_url": "https://github.com/pytorch/pytorch/issues/2198", "id": 245315814, "node_id": "MDU6SXNzdWUyNDUzMTU4MTQ=", "number": 2198, "title": "Memory leak problem in LSTM and RNN ", "user": {"login": "rasoolfa", "id": 11698385, "node_id": "MDQ6VXNlcjExNjk4Mzg1", "avatar_url": "https://avatars2.githubusercontent.com/u/11698385?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rasoolfa", "html_url": "https://github.com/rasoolfa", "followers_url": "https://api.github.com/users/rasoolfa/followers", "following_url": "https://api.github.com/users/rasoolfa/following{/other_user}", "gists_url": "https://api.github.com/users/rasoolfa/gists{/gist_id}", "starred_url": "https://api.github.com/users/rasoolfa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rasoolfa/subscriptions", "organizations_url": "https://api.github.com/users/rasoolfa/orgs", "repos_url": "https://api.github.com/users/rasoolfa/repos", "events_url": "https://api.github.com/users/rasoolfa/events{/privacy}", "received_events_url": "https://api.github.com/users/rasoolfa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2017-07-25T07:56:29Z", "updated_at": "2018-07-27T06:50:51Z", "closed_at": "2017-08-07T00:35:19Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> ,</p>\n<p>I guess this problem still exists even on the master.<br>\nI followed the <a href=\"https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/42\" rel=\"nofollow\">https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/42</a> as well but I don't think so it is completely solved. My Pytorch version is '0.2.0+4a4d884' (got the latest version yesterday)</p>\n<p>Take the following as an example:</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport torch.autograd as autograd\n\ndef Variable(data, *args, **kwargs):\n    if torch.cuda.is_available():\n        return autograd.Variable(data.cuda(), *args, **kwargs)\n    else:\n        return autograd.Variable(data, *args, **kwargs)\n\nclass LSTM_MEM_LEAK(nn.Module):\n    \n    def __init__(self):\n \n        super(LSTM_MEM_LEAK, self).__init__()\n        self.l1 = nn.Linear(256, 300)\n        self.lstm =nn.LSTM(300, 400, batch_first = True, num_layers = 1)\n        self.l2 = nn.Linear(400, 300)\n            \n    def forward(self, input):\n        \n        hidden = (Variable(torch.zeros(1, 16, 400)),\n                Variable(torch.zeros(1, 16, 400))) \n        l1 = F.relu(self.l1(input.view(-1, 256)))\n        lstm_out, h = self.lstm(l1.view(16, -1, 300), hidden)\n        l2  = F.relu(self.l2(lstm_out.contiguous().view(-1, 400)))\n        \n        return l2    \n    \nnet = LSTM_MEM_LEAK()\nnet.cuda()\ninput = Variable(torch.rand(16, 6000, 256))\nprint(input.requires_grad)\nout = net(input)\n</code></pre>\n<p>The memory usage before out = net(input) is ~240MB but when this line runs, it goes up to 1708MB.</p>\n<p>Since no gradient will be computed for the input (i.e. requires_grad is False --&gt; so no need to save parameters at the each time steps to calculate the gradient of input), I am not sure why the memory keeps increasing.<br>\nAny comments would be highly appreciated.</p>", "body_text": "Hi @apaszke ,\nI guess this problem still exists even on the master.\nI followed the https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/42 as well but I don't think so it is completely solved. My Pytorch version is '0.2.0+4a4d884' (got the latest version yesterday)\nTake the following as an example:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport torch.autograd as autograd\n\ndef Variable(data, *args, **kwargs):\n    if torch.cuda.is_available():\n        return autograd.Variable(data.cuda(), *args, **kwargs)\n    else:\n        return autograd.Variable(data, *args, **kwargs)\n\nclass LSTM_MEM_LEAK(nn.Module):\n    \n    def __init__(self):\n \n        super(LSTM_MEM_LEAK, self).__init__()\n        self.l1 = nn.Linear(256, 300)\n        self.lstm =nn.LSTM(300, 400, batch_first = True, num_layers = 1)\n        self.l2 = nn.Linear(400, 300)\n            \n    def forward(self, input):\n        \n        hidden = (Variable(torch.zeros(1, 16, 400)),\n                Variable(torch.zeros(1, 16, 400))) \n        l1 = F.relu(self.l1(input.view(-1, 256)))\n        lstm_out, h = self.lstm(l1.view(16, -1, 300), hidden)\n        l2  = F.relu(self.l2(lstm_out.contiguous().view(-1, 400)))\n        \n        return l2    \n    \nnet = LSTM_MEM_LEAK()\nnet.cuda()\ninput = Variable(torch.rand(16, 6000, 256))\nprint(input.requires_grad)\nout = net(input)\n\nThe memory usage before out = net(input) is ~240MB but when this line runs, it goes up to 1708MB.\nSince no gradient will be computed for the input (i.e. requires_grad is False --> so no need to save parameters at the each time steps to calculate the gradient of input), I am not sure why the memory keeps increasing.\nAny comments would be highly appreciated.", "body": "Hi @apaszke ,\r\n\r\nI guess this problem still exists even on the master.\r\nI followed the https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/42 as well but I don't think so it is completely solved. My Pytorch version is '0.2.0+4a4d884' (got the latest version yesterday)\r\n\r\nTake the following as an example:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nimport torch.autograd as autograd\r\n\r\ndef Variable(data, *args, **kwargs):\r\n    if torch.cuda.is_available():\r\n        return autograd.Variable(data.cuda(), *args, **kwargs)\r\n    else:\r\n        return autograd.Variable(data, *args, **kwargs)\r\n\r\nclass LSTM_MEM_LEAK(nn.Module):\r\n    \r\n    def __init__(self):\r\n \r\n        super(LSTM_MEM_LEAK, self).__init__()\r\n        self.l1 = nn.Linear(256, 300)\r\n        self.lstm =nn.LSTM(300, 400, batch_first = True, num_layers = 1)\r\n        self.l2 = nn.Linear(400, 300)\r\n            \r\n    def forward(self, input):\r\n        \r\n        hidden = (Variable(torch.zeros(1, 16, 400)),\r\n                Variable(torch.zeros(1, 16, 400))) \r\n        l1 = F.relu(self.l1(input.view(-1, 256)))\r\n        lstm_out, h = self.lstm(l1.view(16, -1, 300), hidden)\r\n        l2  = F.relu(self.l2(lstm_out.contiguous().view(-1, 400)))\r\n        \r\n        return l2    \r\n    \r\nnet = LSTM_MEM_LEAK()\r\nnet.cuda()\r\ninput = Variable(torch.rand(16, 6000, 256))\r\nprint(input.requires_grad)\r\nout = net(input)\r\n```\r\nThe memory usage before out = net(input) is ~240MB but when this line runs, it goes up to 1708MB. \r\n\r\nSince no gradient will be computed for the input (i.e. requires_grad is False --> so no need to save parameters at the each time steps to calculate the gradient of input), I am not sure why the memory keeps increasing. \r\nAny comments would be highly appreciated."}