{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/318135942", "html_url": "https://github.com/pytorch/pytorch/issues/2198#issuecomment-318135942", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2198", "id": 318135942, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODEzNTk0Mg==", "user": {"login": "lolongcovas", "id": 3662732, "node_id": "MDQ6VXNlcjM2NjI3MzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3662732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lolongcovas", "html_url": "https://github.com/lolongcovas", "followers_url": "https://api.github.com/users/lolongcovas/followers", "following_url": "https://api.github.com/users/lolongcovas/following{/other_user}", "gists_url": "https://api.github.com/users/lolongcovas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lolongcovas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lolongcovas/subscriptions", "organizations_url": "https://api.github.com/users/lolongcovas/orgs", "repos_url": "https://api.github.com/users/lolongcovas/repos", "events_url": "https://api.github.com/users/lolongcovas/events{/privacy}", "received_events_url": "https://api.github.com/users/lolongcovas/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T18:07:03Z", "updated_at": "2017-07-26T18:07:03Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">For inference you have to:\n\nmodel.eval() and all the variables should have volatile=True,\nVariable(data, volatile =True)</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On 26 Jul 2017 7:23 p.m., \"Rasool Fakoor\" ***@***.***&gt; wrote:\n Thanks.\n I guess it can be considered both. Yes, it does. Look at the following\n code, it is not specific to LSTM/RNN, it happens for any recurrent\n structure:\n\n `\n import numpy as np\n import torch\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from torch.autograd import Variable\n\n class RNN(nn.Module):\n def *init*(self, input_size, hidden_size, output_size):\n\n     super(RNN, self).__init__()\n     self.hidden_size = hidden_size\n     self.input_size = input_size\n     self.output_size = output_size\n\n     self.i2h = nn.Linear( self.input_size + self.hidden_size, self.hidden_size)\n     self.i2o = nn.Linear( self.input_size + self.hidden_size, self.output_size)\n     self.softmax = nn.LogSoftmax()\n\n def forward(self, input, hidden):\n     combined = torch.cat((input, hidden), 1)\n     hidden = self.i2h(combined)\n     output = self.i2o(combined)\n     output = self.softmax(output)\n\n     return output, hidden\n def init_hidden(self):\n     return Variable(torch.zeros(16, self.hidden_size))\n\n criterion = nn.NLLLoss()\n learning_rate = 0.0005\n input_size = 500\n output_size = 10\n n_hidden = 1024\n rnn = RNN(input_size, n_hidden,output_size)\n rnn.cuda()\n hidden = rnn.init_hidden().cuda()\n for i in range(10000):\n data = Variable(torch.rand(16,input_size).cuda())\n out, hidden = rnn(data, hidden)\n `\n\n As I mentioned previously, It happens because it keeps \"all\" parameters at\n each time step which not are required to calculate the gradient. Take the\n following model as an example:\n\n h1 = WX1 + Uh0\n the gradient with respect to W, U, X, h0 would be:\n '\n dW = dh_1\n *X1^T dU = dh_1*h0^T\n\n *dX = W^T dh1 dh0 = U^t dh1*\n '\n So the autogard is not required to save W, U if no gradient is required to\n be calculated for X, h0. However, in the current implementation, it saves\n everything regardless of \"requires_grad\" content.\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"245315814\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2198\" href=\"https://github.com/pytorch/pytorch/issues/2198#issuecomment-318123552\">#2198 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ADfjjPgzx2OTtZQbz5CBhMSzWRNUt-Fsks5sR3X8gaJpZM4OiMTg\">https://github.com/notifications/unsubscribe-auth/ADfjjPgzx2OTtZQbz5CBhMSzWRNUt-Fsks5sR3X8gaJpZM4OiMTg</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "For inference you have to:\n\nmodel.eval() and all the variables should have volatile=True,\nVariable(data, volatile =True)\n\u2026\nOn 26 Jul 2017 7:23 p.m., \"Rasool Fakoor\" ***@***.***> wrote:\n Thanks.\n I guess it can be considered both. Yes, it does. Look at the following\n code, it is not specific to LSTM/RNN, it happens for any recurrent\n structure:\n\n `\n import numpy as np\n import torch\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from torch.autograd import Variable\n\n class RNN(nn.Module):\n def *init*(self, input_size, hidden_size, output_size):\n\n     super(RNN, self).__init__()\n     self.hidden_size = hidden_size\n     self.input_size = input_size\n     self.output_size = output_size\n\n     self.i2h = nn.Linear( self.input_size + self.hidden_size, self.hidden_size)\n     self.i2o = nn.Linear( self.input_size + self.hidden_size, self.output_size)\n     self.softmax = nn.LogSoftmax()\n\n def forward(self, input, hidden):\n     combined = torch.cat((input, hidden), 1)\n     hidden = self.i2h(combined)\n     output = self.i2o(combined)\n     output = self.softmax(output)\n\n     return output, hidden\n def init_hidden(self):\n     return Variable(torch.zeros(16, self.hidden_size))\n\n criterion = nn.NLLLoss()\n learning_rate = 0.0005\n input_size = 500\n output_size = 10\n n_hidden = 1024\n rnn = RNN(input_size, n_hidden,output_size)\n rnn.cuda()\n hidden = rnn.init_hidden().cuda()\n for i in range(10000):\n data = Variable(torch.rand(16,input_size).cuda())\n out, hidden = rnn(data, hidden)\n `\n\n As I mentioned previously, It happens because it keeps \"all\" parameters at\n each time step which not are required to calculate the gradient. Take the\n following model as an example:\n\n h1 = WX1 + Uh0\n the gradient with respect to W, U, X, h0 would be:\n '\n dW = dh_1\n *X1^T dU = dh_1*h0^T\n\n *dX = W^T dh1 dh0 = U^t dh1*\n '\n So the autogard is not required to save W, U if no gradient is required to\n be calculated for X, h0. However, in the current implementation, it saves\n everything regardless of \"requires_grad\" content.\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n <#2198 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ADfjjPgzx2OTtZQbz5CBhMSzWRNUt-Fsks5sR3X8gaJpZM4OiMTg>\n .", "body": "For inference you have to:\n\nmodel.eval() and all the variables should have volatile=True,\nVariable(data, volatile =True)\n\nOn 26 Jul 2017 7:23 p.m., \"Rasool Fakoor\" <notifications@github.com> wrote:\n\n> Thanks.\n> I guess it can be considered both. Yes, it does. Look at the following\n> code, it is not specific to LSTM/RNN, it happens for any recurrent\n> structure:\n>\n> `\n> import numpy as np\n> import torch\n> import torch\n> import torch.nn as nn\n> import torch.nn.functional as F\n> from torch.autograd import Variable\n>\n> class RNN(nn.Module):\n> def *init*(self, input_size, hidden_size, output_size):\n>\n>     super(RNN, self).__init__()\n>     self.hidden_size = hidden_size\n>     self.input_size = input_size\n>     self.output_size = output_size\n>\n>     self.i2h = nn.Linear( self.input_size + self.hidden_size, self.hidden_size)\n>     self.i2o = nn.Linear( self.input_size + self.hidden_size, self.output_size)\n>     self.softmax = nn.LogSoftmax()\n>\n> def forward(self, input, hidden):\n>     combined = torch.cat((input, hidden), 1)\n>     hidden = self.i2h(combined)\n>     output = self.i2o(combined)\n>     output = self.softmax(output)\n>\n>     return output, hidden\n> def init_hidden(self):\n>     return Variable(torch.zeros(16, self.hidden_size))\n>\n> criterion = nn.NLLLoss()\n> learning_rate = 0.0005\n> input_size = 500\n> output_size = 10\n> n_hidden = 1024\n> rnn = RNN(input_size, n_hidden,output_size)\n> rnn.cuda()\n> hidden = rnn.init_hidden().cuda()\n> for i in range(10000):\n> data = Variable(torch.rand(16,input_size).cuda())\n> out, hidden = rnn(data, hidden)\n> `\n>\n> As I mentioned previously, It happens because it keeps \"all\" parameters at\n> each time step which not are required to calculate the gradient. Take the\n> following model as an example:\n>\n> h1 = WX1 + Uh0\n> the gradient with respect to W, U, X, h0 would be:\n> '\n> dW = dh_1\n> *X1^T dU = dh_1*h0^T\n>\n> *dX = W^T dh1 dh0 = U^t dh1*\n> '\n> So the autogard is not required to save W, U if no gradient is required to\n> be calculated for X, h0. However, in the current implementation, it saves\n> everything regardless of \"requires_grad\" content.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/2198#issuecomment-318123552>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADfjjPgzx2OTtZQbz5CBhMSzWRNUt-Fsks5sR3X8gaJpZM4OiMTg>\n> .\n>\n"}