{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409284162", "html_url": "https://github.com/pytorch/pytorch/pull/10075#issuecomment-409284162", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10075", "id": 409284162, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTI4NDE2Mg==", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T16:30:57Z", "updated_at": "2018-07-31T16:34:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Nice! The additional behavior of <code>torch.distributions.utils.broadcast_all()</code> is to handle python floats. I believe we can update <code>broadcast_all()</code> to use your <code>broadcast_arrays()</code> as follows (in either this PR or a follow-up PR):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> in torch/distributions/utils.py</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">broadcast_all</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">values</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>docstring<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">all</span>(<span class=\"pl-c1\">map</span>(torch.is_tensor, values)):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> promote floats to tensors</span>\n        new_tensor <span class=\"pl-k\">=</span> torch.tensor\n        <span class=\"pl-k\">for</span> value <span class=\"pl-k\">in</span> values:\n            <span class=\"pl-k\">if</span> torch.is_tensor(value):\n                new_tensor <span class=\"pl-k\">=</span> value.new_tensor\n                <span class=\"pl-k\">break</span>\n        values <span class=\"pl-k\">=</span> [v <span class=\"pl-k\">if</span> torch.is_tensor(v) <span class=\"pl-k\">else</span> new_tensor(v) <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> values]\n    <span class=\"pl-k\">return</span> torch.broadcast_arrays(<span class=\"pl-k\">*</span>values)</pre></div>\n<p>This would also be a great test to see that distributions are compatible with the new version <g-emoji class=\"g-emoji\" alias=\"smile\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f604.png\">\ud83d\ude04</g-emoji></p>", "body_text": "Nice! The additional behavior of torch.distributions.utils.broadcast_all() is to handle python floats. I believe we can update broadcast_all() to use your broadcast_arrays() as follows (in either this PR or a follow-up PR):\n# in torch/distributions/utils.py\ndef broadcast_all(*values):\n    \"\"\"docstring\"\"\"\n    if not all(map(torch.is_tensor, values)):\n        # promote floats to tensors\n        new_tensor = torch.tensor\n        for value in values:\n            if torch.is_tensor(value):\n                new_tensor = value.new_tensor\n                break\n        values = [v if torch.is_tensor(v) else new_tensor(v) for v in values]\n    return torch.broadcast_arrays(*values)\nThis would also be a great test to see that distributions are compatible with the new version \ud83d\ude04", "body": "Nice! The additional behavior of `torch.distributions.utils.broadcast_all()` is to handle python floats. I believe we can update `broadcast_all()` to use your `broadcast_arrays()` as follows (in either this PR or a follow-up PR):\r\n```py\r\n# in torch/distributions/utils.py\r\ndef broadcast_all(*values):\r\n    \"\"\"docstring\"\"\"\r\n    if not all(map(torch.is_tensor, values)):\r\n        # promote floats to tensors\r\n        new_tensor = torch.tensor\r\n        for value in values:\r\n            if torch.is_tensor(value):\r\n                new_tensor = value.new_tensor\r\n                break\r\n        values = [v if torch.is_tensor(v) else new_tensor(v) for v in values]\r\n    return torch.broadcast_arrays(*values)\r\n```\r\nThis would also be a great test to see that distributions are compatible with the new version \ud83d\ude04 "}