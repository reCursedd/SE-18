{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10482", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10482/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10482/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10482/events", "html_url": "https://github.com/pytorch/pytorch/issues/10482", "id": 350163120, "node_id": "MDU6SXNzdWUzNTAxNjMxMjA=", "number": 10482, "title": "Reduce code duplication in interpolate and make it more generic", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-08-13T19:34:03Z", "updated_at": "2018-08-14T20:41:18Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>Currently, <code>torch.nn.functional.interpolate</code> has dedicated implementation for 1d, 2d and 3d data, as well as for nearest, bilinear and bicubic interpolation. The set of different kernels that are dispatched can be seen <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L2050-L2079\">here</a>.</p>\n<p>Those implementation are all very similar one to the other, and there is a lot of code duplication in there.<br>\nCompare for example <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialUpSamplingNearest.c#L31-L95\">nearest</a> with <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialUpSamplingBilinear.c#L33-L106\">bilinear</a>.</p>\n<p>I believe it is possible to refactor the underlying implementation so that we can have a single C++ / CUDA codepath, with minimal  code duplication.</p>\n<p>This could be achieved in two independent steps (that could be done at the same time):</p>\n<ul>\n<li>factor out the different kernel computations, so that we have a generic <code>filter</code> that is used to compute the interpolation, plus the size of the filter as a struct. For an example, <a href=\"https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L9-L83\">see how Pillow implements it</a>, and then the interpolation coefficients can be generically computed <a href=\"https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L236\">as in here</a>.</li>\n<li>make the interpolate kernel separable (it is already separable in most cases). This means that we can have the user specify the dimensions he wants to interpolate as a list, and in the C++/CUDA code we have a loop over the dimensions. Something like <code>F.interpolate(image, dim=[-2, -1])</code> for spatial interpolation, or <code>F.interpolate(volume, dim=[-3, -2, -1])</code> for volumetric data. We can have reasonable defaults if <code>dim</code> is not specified (that depends on the shape of the input), to keep backwards compatibility.</li>\n</ul>\n<p>The first point will allow us to fuse the <code>nearest</code> / <code>bilinear</code> / <code>bicubic</code> interpolation modes in a single file, while the second point will fuse <code>temporal</code> / <code>spatial</code> and <code>volumetric</code> into the same function.</p>", "body_text": "Currently, torch.nn.functional.interpolate has dedicated implementation for 1d, 2d and 3d data, as well as for nearest, bilinear and bicubic interpolation. The set of different kernels that are dispatched can be seen here.\nThose implementation are all very similar one to the other, and there is a lot of code duplication in there.\nCompare for example nearest with bilinear.\nI believe it is possible to refactor the underlying implementation so that we can have a single C++ / CUDA codepath, with minimal  code duplication.\nThis could be achieved in two independent steps (that could be done at the same time):\n\nfactor out the different kernel computations, so that we have a generic filter that is used to compute the interpolation, plus the size of the filter as a struct. For an example, see how Pillow implements it, and then the interpolation coefficients can be generically computed as in here.\nmake the interpolate kernel separable (it is already separable in most cases). This means that we can have the user specify the dimensions he wants to interpolate as a list, and in the C++/CUDA code we have a loop over the dimensions. Something like F.interpolate(image, dim=[-2, -1]) for spatial interpolation, or F.interpolate(volume, dim=[-3, -2, -1]) for volumetric data. We can have reasonable defaults if dim is not specified (that depends on the shape of the input), to keep backwards compatibility.\n\nThe first point will allow us to fuse the nearest / bilinear / bicubic interpolation modes in a single file, while the second point will fuse temporal / spatial and volumetric into the same function.", "body": "Currently, `torch.nn.functional.interpolate` has dedicated implementation for 1d, 2d and 3d data, as well as for nearest, bilinear and bicubic interpolation. The set of different kernels that are dispatched can be seen [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L2050-L2079).\r\n\r\nThose implementation are all very similar one to the other, and there is a lot of code duplication in there.\r\nCompare for example [nearest](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialUpSamplingNearest.c#L31-L95) with [bilinear](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialUpSamplingBilinear.c#L33-L106).\r\n\r\nI believe it is possible to refactor the underlying implementation so that we can have a single C++ / CUDA codepath, with minimal  code duplication.\r\n\r\nThis could be achieved in two independent steps (that could be done at the same time):\r\n- factor out the different kernel computations, so that we have a generic `filter` that is used to compute the interpolation, plus the size of the filter as a struct. For an example, [see how Pillow implements it](https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L9-L83), and then the interpolation coefficients can be generically computed [as in here](https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L236).\r\n- make the interpolate kernel separable (it is already separable in most cases). This means that we can have the user specify the dimensions he wants to interpolate as a list, and in the C++/CUDA code we have a loop over the dimensions. Something like `F.interpolate(image, dim=[-2, -1])` for spatial interpolation, or `F.interpolate(volume, dim=[-3, -2, -1])` for volumetric data. We can have reasonable defaults if `dim` is not specified (that depends on the shape of the input), to keep backwards compatibility.\r\n\r\nThe first point will allow us to fuse the `nearest` / `bilinear` / `bicubic` interpolation modes in a single file, while the second point will fuse `temporal` / `spatial` and `volumetric` into the same function."}