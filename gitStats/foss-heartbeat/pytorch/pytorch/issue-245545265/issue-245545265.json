{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2208", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2208/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2208/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2208/events", "html_url": "https://github.com/pytorch/pytorch/issues/2208", "id": 245545265, "node_id": "MDU6SXNzdWUyNDU1NDUyNjU=", "number": 2208, "title": "Forward pass OK, backward pass broken for cat + repeat", "user": {"login": "ezliu", "id": 8077207, "node_id": "MDQ6VXNlcjgwNzcyMDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8077207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezliu", "html_url": "https://github.com/ezliu", "followers_url": "https://api.github.com/users/ezliu/followers", "following_url": "https://api.github.com/users/ezliu/following{/other_user}", "gists_url": "https://api.github.com/users/ezliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezliu/subscriptions", "organizations_url": "https://api.github.com/users/ezliu/orgs", "repos_url": "https://api.github.com/users/ezliu/repos", "events_url": "https://api.github.com/users/ezliu/events{/privacy}", "received_events_url": "https://api.github.com/users/ezliu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-25T21:47:23Z", "updated_at": "2017-07-26T13:30:30Z", "closed_at": "2017-07-26T13:30:30Z", "author_association": "NONE", "body_html": "<p>When using cat followed by a repeat (which adds a dimension), the backprop throws the following errors:</p>\n<pre><code>Traceback (most recent call last):\n  File \"minimal.py\", line 73, in &lt;module&gt;\n    optimizer.step()\n  File \"/usr/local/lib/python2.7/site-packages/torch/optim/adam.py\", line 74, in step\n    p.data.addcdiv_(-step_size, exp_avg, denom)\nRuntimeError: inconsistent tensor size at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1127\n</code></pre>\n<p>AND</p>\n<pre><code>Traceback (most recent call last):\n  File \"minimal.py\", line 72, in &lt;module&gt;\n    loss.backward()\n  File \"/usr/local/lib/python2.7/site-packages/torch/autograd/variable.py\", line 146, in backward\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n  File \"/usr/local/lib/python2.7/site-packages/torch/autograd/_functions/blas.py\", line 40, in backward\n    grad_matrix1 = torch.mm(grad_output, matrix2.t())\nRuntimeError: matrices expected, got 3D, 2D tensors at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1232\n</code></pre>\n<p>Below is a minimal case that demonstrates both of these errors. In both cases, the forward pass succeeds in computing some sort of loss, while the backward pass fails. The two defined <code>forward</code> methods demonstrate different RuntimeErrors being thrown. These RuntimeErrors are not thrown when tile is achieved by <code>tile_via_unsqueeze_repeat</code> or <code>tile_via_expand</code>, as shown below (uncomment the other model lines).</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.autograd import Variable\n\n\nclass Model(nn.Module):\n    def __init__(self, tile_function):\n        super(Model, self).__init__()\n        self.weight1 = nn.Parameter(torch.rand(10, 5))\n        self.weight2 = nn.Parameter(torch.rand(10, 5))\n\n        self.cat = nn.Parameter(torch.rand(5, 1))\n\n        self.tile = tile_function\n\n    def forward(self, inp):\n        mult1 = torch.mm(self.weight1, inp)\n        mult2 = torch.mm(self.weight2, inp)\n        cat = torch.cat([mult1, mult2], 1)\n\n        tile = self.tile(cat)\n        \n        add = torch.sum(tile, 0)\n        add = torch.sum(add, 1)\n        add = torch.sum(add, 2)\n        return add\n\n    #def forward(self, inp):\n    #    cat = torch.cat([self.cat, inp], 1)\n    #    tile = self.tile(cat)\n    #    \n    #    add = torch.sum(tile, 0)\n    #    add = torch.sum(add, 1)\n    #    add = torch.sum(add, 2)\n    #    return add\n\n\ndef tile_via_expand(x):\n    a, b = x.size()\n    x = torch.unsqueeze(x, 0)\n    x = x.expand(100, a, b)\n    return x\n\ndef tile_via_repeat(x):\n    x = x.repeat(100, 1, 1)\n    return x\n\ndef tile_via_unsqueeze_repeat(x):\n    x = torch.unsqueeze(x, 0)\n    x = x.repeat(100, 1, 1)\n    return x\n\n# Works\n#model = Model(tile_via_expand)\n\n# Works\n#model = Model(tile_via_unsqueeze_repeat)\n\n# Broken (yields errors in backprop)\nmodel = Model(tile_via_repeat)\n\ninp = Variable(torch.rand(5, 1))\n\noptimizer = optim.Adam(model.parameters())\n\nfor i in xrange(10):\n    loss = model(inp)\n    print loss\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n</code></pre>", "body_text": "When using cat followed by a repeat (which adds a dimension), the backprop throws the following errors:\nTraceback (most recent call last):\n  File \"minimal.py\", line 73, in <module>\n    optimizer.step()\n  File \"/usr/local/lib/python2.7/site-packages/torch/optim/adam.py\", line 74, in step\n    p.data.addcdiv_(-step_size, exp_avg, denom)\nRuntimeError: inconsistent tensor size at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1127\n\nAND\nTraceback (most recent call last):\n  File \"minimal.py\", line 72, in <module>\n    loss.backward()\n  File \"/usr/local/lib/python2.7/site-packages/torch/autograd/variable.py\", line 146, in backward\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n  File \"/usr/local/lib/python2.7/site-packages/torch/autograd/_functions/blas.py\", line 40, in backward\n    grad_matrix1 = torch.mm(grad_output, matrix2.t())\nRuntimeError: matrices expected, got 3D, 2D tensors at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1232\n\nBelow is a minimal case that demonstrates both of these errors. In both cases, the forward pass succeeds in computing some sort of loss, while the backward pass fails. The two defined forward methods demonstrate different RuntimeErrors being thrown. These RuntimeErrors are not thrown when tile is achieved by tile_via_unsqueeze_repeat or tile_via_expand, as shown below (uncomment the other model lines).\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.autograd import Variable\n\n\nclass Model(nn.Module):\n    def __init__(self, tile_function):\n        super(Model, self).__init__()\n        self.weight1 = nn.Parameter(torch.rand(10, 5))\n        self.weight2 = nn.Parameter(torch.rand(10, 5))\n\n        self.cat = nn.Parameter(torch.rand(5, 1))\n\n        self.tile = tile_function\n\n    def forward(self, inp):\n        mult1 = torch.mm(self.weight1, inp)\n        mult2 = torch.mm(self.weight2, inp)\n        cat = torch.cat([mult1, mult2], 1)\n\n        tile = self.tile(cat)\n        \n        add = torch.sum(tile, 0)\n        add = torch.sum(add, 1)\n        add = torch.sum(add, 2)\n        return add\n\n    #def forward(self, inp):\n    #    cat = torch.cat([self.cat, inp], 1)\n    #    tile = self.tile(cat)\n    #    \n    #    add = torch.sum(tile, 0)\n    #    add = torch.sum(add, 1)\n    #    add = torch.sum(add, 2)\n    #    return add\n\n\ndef tile_via_expand(x):\n    a, b = x.size()\n    x = torch.unsqueeze(x, 0)\n    x = x.expand(100, a, b)\n    return x\n\ndef tile_via_repeat(x):\n    x = x.repeat(100, 1, 1)\n    return x\n\ndef tile_via_unsqueeze_repeat(x):\n    x = torch.unsqueeze(x, 0)\n    x = x.repeat(100, 1, 1)\n    return x\n\n# Works\n#model = Model(tile_via_expand)\n\n# Works\n#model = Model(tile_via_unsqueeze_repeat)\n\n# Broken (yields errors in backprop)\nmodel = Model(tile_via_repeat)\n\ninp = Variable(torch.rand(5, 1))\n\noptimizer = optim.Adam(model.parameters())\n\nfor i in xrange(10):\n    loss = model(inp)\n    print loss\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()", "body": "When using cat followed by a repeat (which adds a dimension), the backprop throws the following errors:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"minimal.py\", line 73, in <module>\r\n    optimizer.step()\r\n  File \"/usr/local/lib/python2.7/site-packages/torch/optim/adam.py\", line 74, in step\r\n    p.data.addcdiv_(-step_size, exp_avg, denom)\r\nRuntimeError: inconsistent tensor size at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1127\r\n```\r\n\r\nAND\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"minimal.py\", line 72, in <module>\r\n    loss.backward()\r\n  File \"/usr/local/lib/python2.7/site-packages/torch/autograd/variable.py\", line 146, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n  File \"/usr/local/lib/python2.7/site-packages/torch/autograd/_functions/blas.py\", line 40, in backward\r\n    grad_matrix1 = torch.mm(grad_output, matrix2.t())\r\nRuntimeError: matrices expected, got 3D, 2D tensors at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1232\r\n```\r\n\r\nBelow is a minimal case that demonstrates both of these errors. In both cases, the forward pass succeeds in computing some sort of loss, while the backward pass fails. The two defined `forward` methods demonstrate different RuntimeErrors being thrown. These RuntimeErrors are not thrown when tile is achieved by `tile_via_unsqueeze_repeat` or `tile_via_expand`, as shown below (uncomment the other model lines).\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, tile_function):\r\n        super(Model, self).__init__()\r\n        self.weight1 = nn.Parameter(torch.rand(10, 5))\r\n        self.weight2 = nn.Parameter(torch.rand(10, 5))\r\n\r\n        self.cat = nn.Parameter(torch.rand(5, 1))\r\n\r\n        self.tile = tile_function\r\n\r\n    def forward(self, inp):\r\n        mult1 = torch.mm(self.weight1, inp)\r\n        mult2 = torch.mm(self.weight2, inp)\r\n        cat = torch.cat([mult1, mult2], 1)\r\n\r\n        tile = self.tile(cat)\r\n        \r\n        add = torch.sum(tile, 0)\r\n        add = torch.sum(add, 1)\r\n        add = torch.sum(add, 2)\r\n        return add\r\n\r\n    #def forward(self, inp):\r\n    #    cat = torch.cat([self.cat, inp], 1)\r\n    #    tile = self.tile(cat)\r\n    #    \r\n    #    add = torch.sum(tile, 0)\r\n    #    add = torch.sum(add, 1)\r\n    #    add = torch.sum(add, 2)\r\n    #    return add\r\n\r\n\r\ndef tile_via_expand(x):\r\n    a, b = x.size()\r\n    x = torch.unsqueeze(x, 0)\r\n    x = x.expand(100, a, b)\r\n    return x\r\n\r\ndef tile_via_repeat(x):\r\n    x = x.repeat(100, 1, 1)\r\n    return x\r\n\r\ndef tile_via_unsqueeze_repeat(x):\r\n    x = torch.unsqueeze(x, 0)\r\n    x = x.repeat(100, 1, 1)\r\n    return x\r\n\r\n# Works\r\n#model = Model(tile_via_expand)\r\n\r\n# Works\r\n#model = Model(tile_via_unsqueeze_repeat)\r\n\r\n# Broken (yields errors in backprop)\r\nmodel = Model(tile_via_repeat)\r\n\r\ninp = Variable(torch.rand(5, 1))\r\n\r\noptimizer = optim.Adam(model.parameters())\r\n\r\nfor i in xrange(10):\r\n    loss = model(inp)\r\n    print loss\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n```"}