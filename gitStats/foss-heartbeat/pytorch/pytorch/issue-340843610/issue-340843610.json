{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9407", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9407/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9407/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9407/events", "html_url": "https://github.com/pytorch/pytorch/issues/9407", "id": 340843610, "node_id": "MDU6SXNzdWUzNDA4NDM2MTA=", "number": 9407, "title": "[feature request] Make `torch.gather` broadcastable.", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-13T00:40:31Z", "updated_at": "2018-07-14T11:16:20Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Currently, <code>torch.gather</code> does not broadcast. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre>t <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>],[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>]])\ntorch.gather(t, <span class=\"pl-c1\">1</span>, torch.tensor([[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>]]))</pre></div>\n<p>gives</p>\n<pre><code>tensor([[ 1,  2,  1],\n        [ 1,  2,  1]])\n</code></pre>\n<p>But</p>\n<div class=\"highlight highlight-source-python\"><pre>t <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>],[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>]])\ntorch.gather(t, <span class=\"pl-c1\">1</span>, torch.tensor([[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>]]))</pre></div>\n<p>and</p>\n<div class=\"highlight highlight-source-python\"><pre>t <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>]])\ntorch.gather(t, <span class=\"pl-c1\">1</span>, torch.tensor([[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>]]))</pre></div>\n<p>gives runtime errors.</p>\n<p>It would be more convenient to allow broadcasting on every dimension except the dim specified as the second argument in gather, so that the above two codes can give the same result as the first code.</p>\n<p>Does this proposal sound good? If yes, I can work on it.</p>", "body_text": "Currently, torch.gather does not broadcast. For example:\nt = torch.tensor([[1,2],[1,2]])\ntorch.gather(t, 1, torch.tensor([[0,1,0],[0,1,0]]))\ngives\ntensor([[ 1,  2,  1],\n        [ 1,  2,  1]])\n\nBut\nt = torch.tensor([[1,2],[1,2]])\ntorch.gather(t, 1, torch.tensor([[0,1,0]]))\nand\nt = torch.tensor([[1,2]])\ntorch.gather(t, 1, torch.tensor([[0,1,0],[0,1,0]]))\ngives runtime errors.\nIt would be more convenient to allow broadcasting on every dimension except the dim specified as the second argument in gather, so that the above two codes can give the same result as the first code.\nDoes this proposal sound good? If yes, I can work on it.", "body": "Currently, `torch.gather` does not broadcast. For example:\r\n```python\r\nt = torch.tensor([[1,2],[1,2]])\r\ntorch.gather(t, 1, torch.tensor([[0,1,0],[0,1,0]]))\r\n```\r\ngives\r\n```\r\ntensor([[ 1,  2,  1],\r\n        [ 1,  2,  1]])\r\n```\r\n\r\nBut\r\n\r\n```python\r\nt = torch.tensor([[1,2],[1,2]])\r\ntorch.gather(t, 1, torch.tensor([[0,1,0]]))\r\n```\r\nand\r\n```python\r\nt = torch.tensor([[1,2]])\r\ntorch.gather(t, 1, torch.tensor([[0,1,0],[0,1,0]]))\r\n```\r\ngives runtime errors.\r\n\r\nIt would be more convenient to allow broadcasting on every dimension except the dim specified as the second argument in gather, so that the above two codes can give the same result as the first code.\r\n\r\nDoes this proposal sound good? If yes, I can work on it."}