{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7788", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7788/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7788/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7788/events", "html_url": "https://github.com/pytorch/pytorch/issues/7788", "id": 325661930, "node_id": "MDU6SXNzdWUzMjU2NjE5MzA=", "number": 7788, "title": "Crashes when working with visual debugger", "user": {"login": "Elizaveta239", "id": 3789474, "node_id": "MDQ6VXNlcjM3ODk0NzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/3789474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Elizaveta239", "html_url": "https://github.com/Elizaveta239", "followers_url": "https://api.github.com/users/Elizaveta239/followers", "following_url": "https://api.github.com/users/Elizaveta239/following{/other_user}", "gists_url": "https://api.github.com/users/Elizaveta239/gists{/gist_id}", "starred_url": "https://api.github.com/users/Elizaveta239/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Elizaveta239/subscriptions", "organizations_url": "https://api.github.com/users/Elizaveta239/orgs", "repos_url": "https://api.github.com/users/Elizaveta239/repos", "events_url": "https://api.github.com/users/Elizaveta239/events{/privacy}", "received_events_url": "https://api.github.com/users/Elizaveta239/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-23T11:23:39Z", "updated_at": "2018-05-24T10:49:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi! I'm working on PyCharm's debugger and we have several users who have problems running PyCharm's debugger with PyTorch: <a href=\"https://youtrack.jetbrains.com/issue/PY-25513\" rel=\"nofollow\">https://youtrack.jetbrains.com/issue/PY-25513</a></p>\n<p>The have the following problems:<br>\n\"My program is a neural network training program using PyTorch, GPU computations are asynchronous, variables that are stored in GPU are transfered back to CPU memory only when required to print. Let's say A and B are very huge tensors, the low-level implementation is such that C=A+B will finish before the actual calculation is complete. The low-level optimization is even more aggressive such that when there are a series of operations, e.g., A=random(10000,10000), B=A<em>A+2</em>A+1, C=A+B, it will finish and return control to the debugger even before the C is allocated, and then PyCharm's auto variable display, i.e., \"collecting data...\", will try to display the not-yet allocated data object C, which will lead to corrupted output on slower computers, or even causing the program crash. The behavior is very random, i.e., sometimes it can display correctly, sometimes it cannot display, sometime it crashes subsequently. Moreover, the anomaly becomes more frequent when the object is large and the computer is slow.\"</p>\n<p>In PyCharm's debugger, when program execution is stopped at some breakpoint, we're evaluating all the local variables values, and sometimes it can lead to crashes.</p>\n<p>We would like to help our users and to create a workaround, or maybe a proper fix for the problem. Could you please help me with the following questions:</p>\n<ol>\n<li>Do you have any ability to distinguish, which variables are available for showing/printing? And which values are under optimization and can't be reached yet?</li>\n<li>Do you know about any other visual debuggers, which work fine with variables computed with PyTorch? I didn't manage to find here any issues about similar problems. Do your users use such debuggers at all?</li>\n</ol>\n<p>Thank you!</p>", "body_text": "Hi! I'm working on PyCharm's debugger and we have several users who have problems running PyCharm's debugger with PyTorch: https://youtrack.jetbrains.com/issue/PY-25513\nThe have the following problems:\n\"My program is a neural network training program using PyTorch, GPU computations are asynchronous, variables that are stored in GPU are transfered back to CPU memory only when required to print. Let's say A and B are very huge tensors, the low-level implementation is such that C=A+B will finish before the actual calculation is complete. The low-level optimization is even more aggressive such that when there are a series of operations, e.g., A=random(10000,10000), B=AA+2A+1, C=A+B, it will finish and return control to the debugger even before the C is allocated, and then PyCharm's auto variable display, i.e., \"collecting data...\", will try to display the not-yet allocated data object C, which will lead to corrupted output on slower computers, or even causing the program crash. The behavior is very random, i.e., sometimes it can display correctly, sometimes it cannot display, sometime it crashes subsequently. Moreover, the anomaly becomes more frequent when the object is large and the computer is slow.\"\nIn PyCharm's debugger, when program execution is stopped at some breakpoint, we're evaluating all the local variables values, and sometimes it can lead to crashes.\nWe would like to help our users and to create a workaround, or maybe a proper fix for the problem. Could you please help me with the following questions:\n\nDo you have any ability to distinguish, which variables are available for showing/printing? And which values are under optimization and can't be reached yet?\nDo you know about any other visual debuggers, which work fine with variables computed with PyTorch? I didn't manage to find here any issues about similar problems. Do your users use such debuggers at all?\n\nThank you!", "body": "Hi! I'm working on PyCharm's debugger and we have several users who have problems running PyCharm's debugger with PyTorch: https://youtrack.jetbrains.com/issue/PY-25513\r\n\r\nThe have the following problems:\r\n\"My program is a neural network training program using PyTorch, GPU computations are asynchronous, variables that are stored in GPU are transfered back to CPU memory only when required to print. Let's say A and B are very huge tensors, the low-level implementation is such that C=A+B will finish before the actual calculation is complete. The low-level optimization is even more aggressive such that when there are a series of operations, e.g., A=random(10000,10000), B=A*A+2*A+1, C=A+B, it will finish and return control to the debugger even before the C is allocated, and then PyCharm's auto variable display, i.e., \"collecting data...\", will try to display the not-yet allocated data object C, which will lead to corrupted output on slower computers, or even causing the program crash. The behavior is very random, i.e., sometimes it can display correctly, sometimes it cannot display, sometime it crashes subsequently. Moreover, the anomaly becomes more frequent when the object is large and the computer is slow.\"\r\n\r\nIn PyCharm's debugger, when program execution is stopped at some breakpoint, we're evaluating all the local variables values, and sometimes it can lead to crashes.\r\n\r\nWe would like to help our users and to create a workaround, or maybe a proper fix for the problem. Could you please help me with the following questions:\r\n\r\n1. Do you have any ability to distinguish, which variables are available for showing/printing? And which values are under optimization and can't be reached yet?\r\n2. Do you know about any other visual debuggers, which work fine with variables computed with PyTorch? I didn't manage to find here any issues about similar problems. Do your users use such debuggers at all?\r\n\r\nThank you!\r\n"}