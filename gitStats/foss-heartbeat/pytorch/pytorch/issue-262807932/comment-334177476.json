{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/334177476", "html_url": "https://github.com/pytorch/pytorch/issues/2973#issuecomment-334177476", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2973", "id": 334177476, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDE3NzQ3Ng==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-04T14:39:28Z", "updated_at": "2017-10-04T14:39:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4028979\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kyunghyuncho\">@kyunghyuncho</a>: these are expected behavior and consistent with NumPy broadcasting, see: <a href=\"http://pytorch.org/docs/master/notes/broadcasting.html\" rel=\"nofollow\">http://pytorch.org/docs/master/notes/broadcasting.html</a> for more information, but basically, the dimensions are matched in rightmost order, there is no inference for a matching axis size.</p>\n<p>I'm not sure I completely understand your scalar point about (c) and (d) -- could you provide an example?  But if you are doing reductions, you can control whether the (1) in the reduced dimension is preserved via the <code>keepdim</code> parameter, for example in <code>torch.sum</code>.</p>", "body_text": "Hi @kyunghyuncho: these are expected behavior and consistent with NumPy broadcasting, see: http://pytorch.org/docs/master/notes/broadcasting.html for more information, but basically, the dimensions are matched in rightmost order, there is no inference for a matching axis size.\nI'm not sure I completely understand your scalar point about (c) and (d) -- could you provide an example?  But if you are doing reductions, you can control whether the (1) in the reduced dimension is preserved via the keepdim parameter, for example in torch.sum.", "body": "Hi @kyunghyuncho: these are expected behavior and consistent with NumPy broadcasting, see: http://pytorch.org/docs/master/notes/broadcasting.html for more information, but basically, the dimensions are matched in rightmost order, there is no inference for a matching axis size.\r\n\r\nI'm not sure I completely understand your scalar point about (c) and (d) -- could you provide an example?  But if you are doing reductions, you can control whether the (1) in the reduced dimension is preserved via the `keepdim` parameter, for example in `torch.sum`."}