{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2973", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2973/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2973/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2973/events", "html_url": "https://github.com/pytorch/pytorch/issues/2973", "id": 262807932, "node_id": "MDU6SXNzdWUyNjI4MDc5MzI=", "number": 2973, "title": "broadcasting inconsistency?", "user": {"login": "kyunghyuncho", "id": 4028979, "node_id": "MDQ6VXNlcjQwMjg5Nzk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4028979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyunghyuncho", "html_url": "https://github.com/kyunghyuncho", "followers_url": "https://api.github.com/users/kyunghyuncho/followers", "following_url": "https://api.github.com/users/kyunghyuncho/following{/other_user}", "gists_url": "https://api.github.com/users/kyunghyuncho/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyunghyuncho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyunghyuncho/subscriptions", "organizations_url": "https://api.github.com/users/kyunghyuncho/orgs", "repos_url": "https://api.github.com/users/kyunghyuncho/repos", "events_url": "https://api.github.com/users/kyunghyuncho/events{/privacy}", "received_events_url": "https://api.github.com/users/kyunghyuncho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-10-04T14:26:59Z", "updated_at": "2017-10-04T15:37:30Z", "closed_at": "2017-10-04T15:37:30Z", "author_association": "NONE", "body_html": "<p>when we multiply (or any binary operator) two tensor variables, i've noticed the weird behaviour depending on the shapes of those variables:</p>\n<p>(a) when x.size()=(1,10) and y.size()=(10), (x*y).size()=(1,10) (expected)<br>\n(b) when x.size()=(10) and y.size()=(1,10), (x*y).size()=(1,10) (expected)<br>\n(c) when x.size()=(10,1) and y.size()=(10), (x*y).size()=(10,10) (<strong>unexpected</strong>)<br>\n(d) when x.size()=(10) and y.size()=(10,1), (x*y).size()=(10,10) (<strong>unexpected</strong>)<br>\n(e) when x.size()=(10,1) and y.size()=(1,10), (x*y).size()=(10,10) (expected)</p>\n<p>the cases (c) and (d) really easily throw off many (or perhaps only me) from debugging, when such cases happen in a loss function that reduces the output into a single scalar.</p>\n<p>my expectation is that broadcasting should first find a matching axis, and i wonder if the current implementation/behaviour is expected and was designed with a certain goal in mind.</p>", "body_text": "when we multiply (or any binary operator) two tensor variables, i've noticed the weird behaviour depending on the shapes of those variables:\n(a) when x.size()=(1,10) and y.size()=(10), (x*y).size()=(1,10) (expected)\n(b) when x.size()=(10) and y.size()=(1,10), (x*y).size()=(1,10) (expected)\n(c) when x.size()=(10,1) and y.size()=(10), (x*y).size()=(10,10) (unexpected)\n(d) when x.size()=(10) and y.size()=(10,1), (x*y).size()=(10,10) (unexpected)\n(e) when x.size()=(10,1) and y.size()=(1,10), (x*y).size()=(10,10) (expected)\nthe cases (c) and (d) really easily throw off many (or perhaps only me) from debugging, when such cases happen in a loss function that reduces the output into a single scalar.\nmy expectation is that broadcasting should first find a matching axis, and i wonder if the current implementation/behaviour is expected and was designed with a certain goal in mind.", "body": "when we multiply (or any binary operator) two tensor variables, i've noticed the weird behaviour depending on the shapes of those variables:\r\n\r\n(a) when x.size()=(1,10) and y.size()=(10), (x\\*y).size()=(1,10) (expected)\r\n(b) when x.size()=(10) and y.size()=(1,10), (x\\*y).size()=(1,10) (expected)\r\n(c) when x.size()=(10,1) and y.size()=(10), (x\\*y).size()=(10,10) (**unexpected**)\r\n(d) when x.size()=(10) and y.size()=(10,1), (x\\*y).size()=(10,10) (**unexpected**)\r\n(e) when x.size()=(10,1) and y.size()=(1,10), (x\\*y).size()=(10,10) (expected)\r\n\r\nthe cases (c) and (d) really easily throw off many (or perhaps only me) from debugging, when such cases happen in a loss function that reduces the output into a single scalar. \r\n\r\nmy expectation is that broadcasting should first find a matching axis, and i wonder if the current implementation/behaviour is expected and was designed with a certain goal in mind.\r\n\r\n"}