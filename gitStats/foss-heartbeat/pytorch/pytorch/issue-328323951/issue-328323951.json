{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8010", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8010/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8010/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8010/events", "html_url": "https://github.com/pytorch/pytorch/pull/8010", "id": 328323951, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkxODkzODg4", "number": 8010, "title": "Adding an implementation of the SVRG optimizer", "user": {"login": "AnesBenmerzoug", "id": 27914730, "node_id": "MDQ6VXNlcjI3OTE0NzMw", "avatar_url": "https://avatars3.githubusercontent.com/u/27914730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnesBenmerzoug", "html_url": "https://github.com/AnesBenmerzoug", "followers_url": "https://api.github.com/users/AnesBenmerzoug/followers", "following_url": "https://api.github.com/users/AnesBenmerzoug/following{/other_user}", "gists_url": "https://api.github.com/users/AnesBenmerzoug/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnesBenmerzoug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnesBenmerzoug/subscriptions", "organizations_url": "https://api.github.com/users/AnesBenmerzoug/orgs", "repos_url": "https://api.github.com/users/AnesBenmerzoug/repos", "events_url": "https://api.github.com/users/AnesBenmerzoug/events{/privacy}", "received_events_url": "https://api.github.com/users/AnesBenmerzoug/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 559719279, "node_id": "MDU6TGFiZWw1NTk3MTkyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ready%20for%20review", "name": "ready for review", "color": "b60205", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-05-31T22:51:56Z", "updated_at": "2018-09-23T09:56:58Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8010", "html_url": "https://github.com/pytorch/pytorch/pull/8010", "diff_url": "https://github.com/pytorch/pytorch/pull/8010.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8010.patch"}, "body_html": "<p>This commit, made for the issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"315060176\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6657\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6657/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6657\">#6657</a>, adds an implementation of the Stochastic Variance Reduced Gradient optimizer as described in:<br>\n<a href=\"https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf\" rel=\"nofollow\">https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf</a><br>\nwith added momentum, weight_decay and dampening just like for regular SGD.<br>\nThere is one thing about this optimizer that makes it a bit tougher than others. And that is the fact that it uses two sets of parameters ( which means two models in my implementation ): the main ones and a snapshot of the main taken every few epochs, and that are used to reduce the variance of the optimization.<br>\nSo far I only added tests for basic cases, but still no test for the average gradient computation and snapshot saving as that means having to change the test methods to call the update_snapshot() method of this optimizer or writing new ones and I would like to have feedback on this issue.</p>", "body_text": "This commit, made for the issue #6657, adds an implementation of the Stochastic Variance Reduced Gradient optimizer as described in:\nhttps://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf\nwith added momentum, weight_decay and dampening just like for regular SGD.\nThere is one thing about this optimizer that makes it a bit tougher than others. And that is the fact that it uses two sets of parameters ( which means two models in my implementation ): the main ones and a snapshot of the main taken every few epochs, and that are used to reduce the variance of the optimization.\nSo far I only added tests for basic cases, but still no test for the average gradient computation and snapshot saving as that means having to change the test methods to call the update_snapshot() method of this optimizer or writing new ones and I would like to have feedback on this issue.", "body": "This commit, made for the issue #6657, adds an implementation of the Stochastic Variance Reduced Gradient optimizer as described in:\r\nhttps://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf\r\nwith added momentum, weight_decay and dampening just like for regular SGD.\r\nThere is one thing about this optimizer that makes it a bit tougher than others. And that is the fact that it uses two sets of parameters ( which means two models in my implementation ): the main ones and a snapshot of the main taken every few epochs, and that are used to reduce the variance of the optimization.\r\nSo far I only added tests for basic cases, but still no test for the average gradient computation and snapshot saving as that means having to change the test methods to call the update_snapshot() method of this optimizer or writing new ones and I would like to have feedback on this issue."}