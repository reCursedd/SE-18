{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423727724", "html_url": "https://github.com/pytorch/pytorch/pull/8010#issuecomment-423727724", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8010", "id": 423727724, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzcyNzcyNA==", "user": {"login": "AnesBenmerzoug", "id": 27914730, "node_id": "MDQ6VXNlcjI3OTE0NzMw", "avatar_url": "https://avatars3.githubusercontent.com/u/27914730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnesBenmerzoug", "html_url": "https://github.com/AnesBenmerzoug", "followers_url": "https://api.github.com/users/AnesBenmerzoug/followers", "following_url": "https://api.github.com/users/AnesBenmerzoug/following{/other_user}", "gists_url": "https://api.github.com/users/AnesBenmerzoug/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnesBenmerzoug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnesBenmerzoug/subscriptions", "organizations_url": "https://api.github.com/users/AnesBenmerzoug/orgs", "repos_url": "https://api.github.com/users/AnesBenmerzoug/repos", "events_url": "https://api.github.com/users/AnesBenmerzoug/events{/privacy}", "received_events_url": "https://api.github.com/users/AnesBenmerzoug/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T08:37:03Z", "updated_at": "2018-09-22T08:37:03Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4994518\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/johncava\">@johncava</a> the optimizer currently works as follows:</p>\n<ul>\n<li>\n<p>At the beginning of each epoch, a snapshot is taken by making a copy of the main parameters and the new average gradient is computed ( this takes \"n\" steps, where \"n\" is the size of the dataset ). A mistake that I made here is that I use the update_frequency to count when to actually take the snapshot, i.e. if for example you choose an update_frequency = 2 then you would have to call update_snapshot twice for it to register and thus you would effectively do it only every two epochs. This comes from my misunderstanding of the paper. I will correct this part.</p>\n</li>\n<li>\n<p>Then, for each batch/sample the gradients using the main parameters and the snapshot parameters are computed and used in the step method of the optimizer. There it's a straightforward application of the equation described in the paper with the addition of momentum and dampening. I think this is where they use only \"m\" samples by picking uniformly from all samples. This part doesn't have to be taken care of by the optimizer as it can be changed from outside by the user.</p>\n</li>\n</ul>", "body_text": "@johncava the optimizer currently works as follows:\n\n\nAt the beginning of each epoch, a snapshot is taken by making a copy of the main parameters and the new average gradient is computed ( this takes \"n\" steps, where \"n\" is the size of the dataset ). A mistake that I made here is that I use the update_frequency to count when to actually take the snapshot, i.e. if for example you choose an update_frequency = 2 then you would have to call update_snapshot twice for it to register and thus you would effectively do it only every two epochs. This comes from my misunderstanding of the paper. I will correct this part.\n\n\nThen, for each batch/sample the gradients using the main parameters and the snapshot parameters are computed and used in the step method of the optimizer. There it's a straightforward application of the equation described in the paper with the addition of momentum and dampening. I think this is where they use only \"m\" samples by picking uniformly from all samples. This part doesn't have to be taken care of by the optimizer as it can be changed from outside by the user.", "body": "@johncava the optimizer currently works as follows:\r\n\r\n- At the beginning of each epoch, a snapshot is taken by making a copy of the main parameters and the new average gradient is computed ( this takes \"n\" steps, where \"n\" is the size of the dataset ). A mistake that I made here is that I use the update_frequency to count when to actually take the snapshot, i.e. if for example you choose an update_frequency = 2 then you would have to call update_snapshot twice for it to register and thus you would effectively do it only every two epochs. This comes from my misunderstanding of the paper. I will correct this part.\r\n\r\n- Then, for each batch/sample the gradients using the main parameters and the snapshot parameters are computed and used in the step method of the optimizer. There it's a straightforward application of the equation described in the paper with the addition of momentum and dampening. I think this is where they use only \"m\" samples by picking uniformly from all samples. This part doesn't have to be taken care of by the optimizer as it can be changed from outside by the user."}