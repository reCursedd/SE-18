{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423804810", "html_url": "https://github.com/pytorch/pytorch/pull/8010#issuecomment-423804810", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8010", "id": 423804810, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzgwNDgxMA==", "user": {"login": "AnesBenmerzoug", "id": 27914730, "node_id": "MDQ6VXNlcjI3OTE0NzMw", "avatar_url": "https://avatars3.githubusercontent.com/u/27914730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnesBenmerzoug", "html_url": "https://github.com/AnesBenmerzoug", "followers_url": "https://api.github.com/users/AnesBenmerzoug/followers", "following_url": "https://api.github.com/users/AnesBenmerzoug/following{/other_user}", "gists_url": "https://api.github.com/users/AnesBenmerzoug/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnesBenmerzoug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnesBenmerzoug/subscriptions", "organizations_url": "https://api.github.com/users/AnesBenmerzoug/orgs", "repos_url": "https://api.github.com/users/AnesBenmerzoug/repos", "events_url": "https://api.github.com/users/AnesBenmerzoug/events{/privacy}", "received_events_url": "https://api.github.com/users/AnesBenmerzoug/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-23T09:56:58Z", "updated_at": "2018-09-23T09:56:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4994518\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/johncava\">@johncava</a> It could be done using a single model via copying parameters and gradients inside the optimizer. I have thought about doing it, but never actually came to doing it. I could give it a try.<br>\nFor VITE it would work more or less the same. You would just have to sample two independent sets from the dataset outside the optimizer and maybe have three models instead of two ( one for the snapshot W<sup>~</sup>, one for W<sub>t</sub>, and one for W<sub>t+1</sub> ). In this case it would be better to use closures and copy the parameters inside the optimizer to make it simple and easy to use.</p>", "body_text": "@johncava It could be done using a single model via copying parameters and gradients inside the optimizer. I have thought about doing it, but never actually came to doing it. I could give it a try.\nFor VITE it would work more or less the same. You would just have to sample two independent sets from the dataset outside the optimizer and maybe have three models instead of two ( one for the snapshot W~, one for Wt, and one for Wt+1 ). In this case it would be better to use closures and copy the parameters inside the optimizer to make it simple and easy to use.", "body": "@johncava It could be done using a single model via copying parameters and gradients inside the optimizer. I have thought about doing it, but never actually came to doing it. I could give it a try.\r\nFor VITE it would work more or less the same. You would just have to sample two independent sets from the dataset outside the optimizer and maybe have three models instead of two ( one for the snapshot W<sup>~</sup>, one for W<sub>t</sub>, and one for W<sub>t+1</sub> ). In this case it would be better to use closures and copy the parameters inside the optimizer to make it simple and easy to use."}