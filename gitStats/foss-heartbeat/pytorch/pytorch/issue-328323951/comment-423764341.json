{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423764341", "html_url": "https://github.com/pytorch/pytorch/pull/8010#issuecomment-423764341", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8010", "id": 423764341, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzc2NDM0MQ==", "user": {"login": "johncava", "id": 4994518, "node_id": "MDQ6VXNlcjQ5OTQ1MTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4994518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johncava", "html_url": "https://github.com/johncava", "followers_url": "https://api.github.com/users/johncava/followers", "following_url": "https://api.github.com/users/johncava/following{/other_user}", "gists_url": "https://api.github.com/users/johncava/gists{/gist_id}", "starred_url": "https://api.github.com/users/johncava/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johncava/subscriptions", "organizations_url": "https://api.github.com/users/johncava/orgs", "repos_url": "https://api.github.com/users/johncava/repos", "events_url": "https://api.github.com/users/johncava/events{/privacy}", "received_events_url": "https://api.github.com/users/johncava/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T18:35:41Z", "updated_at": "2018-09-22T18:35:41Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=27914730\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AnesBenmerzoug\">@AnesBenmerzoug</a> Can you elaborate more on how the user can do the sampling outside the optimizer? From the paper, there are two for loops, one for each epoch, and one for each iteration till 'm'. I understand that the user from that can pick any arbitrary example which would be grad_phi_it(W(t-1)). However, there is grad_phi_it(W_tilda), which indicates that in the optimizer step, the optimizer has to pick the same index of that dataset but for the snapshot gradient.</p>\n<p>That is why I believe <code>for idx, p in enumerate(group['params']):</code> should be changed to <code>for iteration in range(0,m):</code> and then sample idx from (0,n) to index the snapshot gradients from there. Or maybe, let the user get the index of the example they chose, tell it to the optimizer, and the optimizer will match the index from the snapshot gradient.</p>", "body_text": "@AnesBenmerzoug Can you elaborate more on how the user can do the sampling outside the optimizer? From the paper, there are two for loops, one for each epoch, and one for each iteration till 'm'. I understand that the user from that can pick any arbitrary example which would be grad_phi_it(W(t-1)). However, there is grad_phi_it(W_tilda), which indicates that in the optimizer step, the optimizer has to pick the same index of that dataset but for the snapshot gradient.\nThat is why I believe for idx, p in enumerate(group['params']): should be changed to for iteration in range(0,m): and then sample idx from (0,n) to index the snapshot gradients from there. Or maybe, let the user get the index of the example they chose, tell it to the optimizer, and the optimizer will match the index from the snapshot gradient.", "body": "@AnesBenmerzoug Can you elaborate more on how the user can do the sampling outside the optimizer? From the paper, there are two for loops, one for each epoch, and one for each iteration till 'm'. I understand that the user from that can pick any arbitrary example which would be grad_phi_it(W(t-1)). However, there is grad_phi_it(W_tilda), which indicates that in the optimizer step, the optimizer has to pick the same index of that dataset but for the snapshot gradient.\r\n\r\nThat is why I believe `for idx, p in enumerate(group['params']):` should be changed to `for iteration in range(0,m):` and then sample idx from (0,n) to index the snapshot gradients from there. Or maybe, let the user get the index of the example they chose, tell it to the optimizer, and the optimizer will match the index from the snapshot gradient."}