{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3669", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3669/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3669/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3669/events", "html_url": "https://github.com/pytorch/pytorch/issues/3669", "id": 273511206, "node_id": "MDU6SXNzdWUyNzM1MTEyMDY=", "number": 3669, "title": "Bug in build scripts, outdated existing PyTorch headers are picked up", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2017-11-13T17:22:49Z", "updated_at": "2018-01-20T11:15:46Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>CUDA8, g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609, pytorch commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/25b166ed1f58fe1ba255cf218f8b44bf7cc9bb5b/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/25b166ed1f58fe1ba255cf218f8b44bf7cc9bb5b\"><tt>25b166e</tt></a></p>\n<p>TLDR, error is <code>.../pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)</code>, warnings are signed integer issues and uninitialized variable use.</p>\n<p>Problematic files: <code>THC/THCBlas.cu</code>, <code>THCUNN/generic/FusedRNNKernel.cu</code>, <code>THCUNN/generic/VolumetricAveragePooling.cu</code>, <code>THCUNN/generic/VolumetricDilatedMaxPooling.cu</code>, <code>THCS/generic/THCSTensor.cu</code></p>\n<details>\n<pre><code>[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\n.../pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n.../pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));\n                ^\n.../pytorch/aten/src/THC/THCBlas.cu:87:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n.../pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n.../pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));\n                ^\n.../pytorch/aten/src/THC/THCBlas.cu:117:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\n\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:437: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:443: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument list\n            argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)\n\n1 error detected in the compilation of \"/tmp/tmpxft_00000594_00000000-7_THCSTensor.cpp1.ii\".\n\n\n</code></pre>\n</details>", "body_text": "CUDA8, g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609, pytorch commit 25b166e\nTLDR, error is .../pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t), warnings are signed integer issues and uninitialized variable use.\nProblematic files: THC/THCBlas.cu, THCUNN/generic/FusedRNNKernel.cu, THCUNN/generic/VolumetricAveragePooling.cu, THCUNN/generic/VolumetricDilatedMaxPooling.cu, THCS/generic/THCSTensor.cu\n\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\n.../pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n.../pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n                ^\n.../pytorch/aten/src/THC/THCBlas.cu:87:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n.../pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n.../pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n                ^\n.../pytorch/aten/src/THC/THCBlas.cu:117:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\n\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:437: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:443: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument list\n            argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)\n\n1 error detected in the compilation of \"/tmp/tmpxft_00000594_00000000-7_THCSTensor.cpp1.ii\".", "body": "CUDA8, g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609, pytorch commit 25b166e\r\n\r\nTLDR, error is `.../pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)`, warnings are signed integer issues and uninitialized variable use.\r\n\r\nProblematic files: `THC/THCBlas.cu`, `THCUNN/generic/FusedRNNKernel.cu`, `THCUNN/generic/VolumetricAveragePooling.cu`, `THCUNN/generic/VolumetricDilatedMaxPooling.cu`, `THCS/generic/THCSTensor.cu`\r\n\r\n<details>\r\n\r\n```\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\r\n.../pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\r\n.../pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n                ^\r\n.../pytorch/aten/src/THC/THCBlas.cu:87:19: note: \u2018op\u2019 was declared here\r\n   cublasOperation_t op;\r\n                   ^\r\n.../pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\r\n.../pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n                ^\r\n.../pytorch/aten/src/THC/THCBlas.cu:117:19: note: \u2018op\u2019 was declared here\r\n   cublasOperation_t op;\r\n                   ^\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\r\n\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\r\n\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:437: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:443: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument list\r\n            argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)\r\n\r\n1 error detected in the compilation of \"/tmp/tmpxft_00000594_00000000-7_THCSTensor.cpp1.ii\".\r\n\r\n\r\n```\r\n</details>"}