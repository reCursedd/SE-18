{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/345487666", "html_url": "https://github.com/pytorch/pytorch/issues/3669#issuecomment-345487666", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3669", "id": 345487666, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTQ4NzY2Ng==", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-19T02:50:19Z", "updated_at": "2017-11-19T05:34:22Z", "author_association": "NONE", "body_html": "<details>\n<summary>\nFull log\n</summary>\n<pre><code>running install\nrunning build_deps\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"8.0\", minimum required is \"7.0\") \n-- Configuring done\n-- Generating done\n-- Build files have been written to: .../src/pytorch/torch/lib/build/nccl\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          &gt; .../src/pytorch/torch/lib/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      &gt; .../src/pytorch/torch/lib/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         &gt; .../src/pytorch/torch/lib/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   &gt; .../src/pytorch/torch/lib/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   &gt; .../src/pytorch/torch/lib/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    &gt; .../src/pytorch/torch/lib/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       &gt; .../src/pytorch/torch/lib/build/nccl/obj/reduce.o\nCompiling src/reduce_scatter.cu               &gt; .../src/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    &gt; .../src/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    &gt; .../src/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: .../src/pytorch/torch/lib/tmp_install/include/nccl.h\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"8.0\", minimum required is \"5.5\") \n-- Autodetected CUDA architecture(s): 6.1\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\n-- Try OpenMP C flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Try OpenMP CXX flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Found OpenMP: -fopenmp  \n-- Compiling with OpenMP support\n-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - False\n-- Compiling with MAGMA support\n-- MAGMA INCLUDE DIRECTORIES: .../prefix/include\n-- MAGMA LIBRARIES: .../prefix/lib/libmagma.so\n-- MAGMA V2 check: 0\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- Looking for cpuid.h\n-- Looking for cpuid.h - found\n-- Performing Test HAVE_GCC_GET_CPUID\n-- Performing Test HAVE_GCC_GET_CPUID - Success\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Success\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Success\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Success\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Success\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Success\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Success\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Success\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Success\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Success\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Success\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Success\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Success\n-- SSE2 Found\n-- SSE3 Found\n-- AVX Found\n-- AVX2 Found\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl - guide - pthread - m]\n--   Library mkl: not found\n-- MKL library not found\n-- Checking for [openblas]\n--   Library openblas: .../prefix/lib/libopenblas.so\n-- Looking for sgemm_\n-- Looking for sgemm_ - found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (open).\n-- Looking for cheev_\n-- Looking for cheev_ - found\n-- Found a library with LAPACK API. (open)\n-- Found CUDNN: /home/mscho/vadim/cudnnR7.0.3/include  \n-- Found cuDNN: v7.0.3  (include: /home/mscho/vadim/cudnnR7.0.3/include, library: /home/mscho/vadim/cudnnR7.0.3/lib64/libcudnn.so)\n-- Using python found in /usr/bin/python\n['.../src/pytorch/aten/src/THNN/generic/THNN.h', '.../src/pytorch/aten/src/THCUNN/generic/THCUNN.h', '.../src/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: set(['bernoulli', 'bernoulli_'])\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring done\n-- Generating done\n-- Build files have been written to: .../src/pytorch/torch/lib/build/aten\n[  1%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\n.../src/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n.../src/pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));\n                ^\n.../src/pytorch/aten/src/THC/THCBlas.cu:87:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n.../src/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n.../src/pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));\n                ^\n.../src/pytorch/aten/src/THC/THCBlas.cu:117:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\n['.../src/pytorch/aten/src/THNN/generic/THNN.h', '.../src/pytorch/aten/src/THCUNN/generic/THCUNN.h', '.../src/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: set(['bernoulli', 'bernoulli_'])\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &amp;&amp;\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:437: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:425: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:443: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o\n.../src/pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument list\n            argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)\n\n1 error detected in the compilation of \"/tmp/tmpxft_00007f45_00000000-7_THCSTensor.cpp1.ii\".\nCMake Error at ATen_generated_THCSTensor.cu.o.cmake:267 (message):\n  Error generating file\n  .../src/pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THCS/./ATen_generated_THCSTensor.cu.o\n\n\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1106: recipe for target 'src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\nMakefile:127: recipe for target 'all' failed\nmake: *** [all] Error 2\n</code></pre>\n</details>\n<p>The first error line is:</p>\n<div class=\"highlight highlight-source-shell\"><pre>/usr/local/cuda/bin/nvcc pytorch/aten/src/THCS/THCSTensor.cu -c -o pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THCS/./ATen_generated_THCSTensor.cu.o -ccbin /usr/bin/cc -m64 --std c++11 -DUSE_GCC_ATOMICS=1 -DTH_INDEX_BASE=0 -DAT_CUDA_ENABLED -DATen_EXPORTS -Xcompiler ,\\\"-fexceptions\\\",\\\"-fopenmp\\\",\\\"-Wall\\\",\\\"-Wno-vla\\\",\\\"-fPIC\\\",\\\"-O3\\\" -Wno-deprecated-gpu-targets -gencode arch=compute_61,code=sm_61 -Xcompiler -fPIC -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -DNVCC -I/usr/local/cuda/include -I/home/mscho/vadim/.wigwam/prefix/include -Ipytorch/aten/src/TH -Ipytorch/aten/src/THC -Ipytorch/torch/lib/build/aten/src/TH -Ipytorch/torch/lib/build/aten/src/THC -Ipytorch/aten/src/THS -Ipytorch/aten/src/THCS -Ipytorch/torch/lib/build/aten/src/THS -Ipytorch/torch/lib/build/aten/src/THCS -Ipytorch/aten/src -Ipytorch/torch/lib/build/aten/src -Ipytorch/aten/src/THNN -Ipytorch/aten/src/THCUNN -I/usr/local/cuda/include -I/TH -Ipytorch/aten/src/ATen/CUDA_SDK_ROOT_DIR-NOTFOUND/common/inc -Ipytorch/aten/src/ATen/cuda -Ipytorch/aten/src/ATen/.. -Ipytorch/torch/lib/build/aten/src/ATen</pre></div>\n<p>I can repro the error by running this line again.</p>\n<p>Verbose log attached: <a href=\"https://github.com/pytorch/pytorch/files/1485241/verbose.txt\">verbose.txt</a></p>", "body_text": "Full log\n\nrunning install\nrunning build_deps\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"8.0\", minimum required is \"7.0\") \n-- Configuring done\n-- Generating done\n-- Build files have been written to: .../src/pytorch/torch/lib/build/nccl\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          > .../src/pytorch/torch/lib/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      > .../src/pytorch/torch/lib/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         > .../src/pytorch/torch/lib/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   > .../src/pytorch/torch/lib/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   > .../src/pytorch/torch/lib/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    > .../src/pytorch/torch/lib/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       > .../src/pytorch/torch/lib/build/nccl/obj/reduce.o\nCompiling src/reduce_scatter.cu               > .../src/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    > .../src/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    > .../src/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: .../src/pytorch/torch/lib/tmp_install/include/nccl.h\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"8.0\", minimum required is \"5.5\") \n-- Autodetected CUDA architecture(s): 6.1\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\n-- Try OpenMP C flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Try OpenMP CXX flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Found OpenMP: -fopenmp  \n-- Compiling with OpenMP support\n-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - False\n-- Compiling with MAGMA support\n-- MAGMA INCLUDE DIRECTORIES: .../prefix/include\n-- MAGMA LIBRARIES: .../prefix/lib/libmagma.so\n-- MAGMA V2 check: 0\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- Looking for cpuid.h\n-- Looking for cpuid.h - found\n-- Performing Test HAVE_GCC_GET_CPUID\n-- Performing Test HAVE_GCC_GET_CPUID - Success\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Success\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Success\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Success\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Success\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Success\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Success\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Success\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Success\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Success\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Success\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Success\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Success\n-- SSE2 Found\n-- SSE3 Found\n-- AVX Found\n-- AVX2 Found\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl - guide - pthread - m]\n--   Library mkl: not found\n-- MKL library not found\n-- Checking for [openblas]\n--   Library openblas: .../prefix/lib/libopenblas.so\n-- Looking for sgemm_\n-- Looking for sgemm_ - found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (open).\n-- Looking for cheev_\n-- Looking for cheev_ - found\n-- Found a library with LAPACK API. (open)\n-- Found CUDNN: /home/mscho/vadim/cudnnR7.0.3/include  \n-- Found cuDNN: v7.0.3  (include: /home/mscho/vadim/cudnnR7.0.3/include, library: /home/mscho/vadim/cudnnR7.0.3/lib64/libcudnn.so)\n-- Using python found in /usr/bin/python\n['.../src/pytorch/aten/src/THNN/generic/THNN.h', '.../src/pytorch/aten/src/THCUNN/generic/THCUNN.h', '.../src/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: set(['bernoulli', 'bernoulli_'])\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring done\n-- Generating done\n-- Build files have been written to: .../src/pytorch/torch/lib/build/aten\n[  1%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\n.../src/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n.../src/pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n                ^\n.../src/pytorch/aten/src/THC/THCBlas.cu:87:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n.../src/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n.../src/pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n                ^\n.../src/pytorch/aten/src/THC/THCBlas.cu:117:19: note: \u2018op\u2019 was declared here\n   cublasOperation_t op;\n                   ^\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\n['.../src/pytorch/aten/src/THNN/generic/THNN.h', '.../src/pytorch/aten/src/THCUNN/generic/THCUNN.h', '.../src/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: set(['bernoulli', 'bernoulli_'])\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\n                            ^\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:437: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:425: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:443: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o\n.../src/pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument list\n            argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)\n\n1 error detected in the compilation of \"/tmp/tmpxft_00007f45_00000000-7_THCSTensor.cpp1.ii\".\nCMake Error at ATen_generated_THCSTensor.cu.o.cmake:267 (message):\n  Error generating file\n  .../src/pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THCS/./ATen_generated_THCSTensor.cu.o\n\n\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1106: recipe for target 'src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\nMakefile:127: recipe for target 'all' failed\nmake: *** [all] Error 2\n\n\nThe first error line is:\n/usr/local/cuda/bin/nvcc pytorch/aten/src/THCS/THCSTensor.cu -c -o pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THCS/./ATen_generated_THCSTensor.cu.o -ccbin /usr/bin/cc -m64 --std c++11 -DUSE_GCC_ATOMICS=1 -DTH_INDEX_BASE=0 -DAT_CUDA_ENABLED -DATen_EXPORTS -Xcompiler ,\\\"-fexceptions\\\",\\\"-fopenmp\\\",\\\"-Wall\\\",\\\"-Wno-vla\\\",\\\"-fPIC\\\",\\\"-O3\\\" -Wno-deprecated-gpu-targets -gencode arch=compute_61,code=sm_61 -Xcompiler -fPIC -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -DNVCC -I/usr/local/cuda/include -I/home/mscho/vadim/.wigwam/prefix/include -Ipytorch/aten/src/TH -Ipytorch/aten/src/THC -Ipytorch/torch/lib/build/aten/src/TH -Ipytorch/torch/lib/build/aten/src/THC -Ipytorch/aten/src/THS -Ipytorch/aten/src/THCS -Ipytorch/torch/lib/build/aten/src/THS -Ipytorch/torch/lib/build/aten/src/THCS -Ipytorch/aten/src -Ipytorch/torch/lib/build/aten/src -Ipytorch/aten/src/THNN -Ipytorch/aten/src/THCUNN -I/usr/local/cuda/include -I/TH -Ipytorch/aten/src/ATen/CUDA_SDK_ROOT_DIR-NOTFOUND/common/inc -Ipytorch/aten/src/ATen/cuda -Ipytorch/aten/src/ATen/.. -Ipytorch/torch/lib/build/aten/src/ATen\nI can repro the error by running this line again.\nVerbose log attached: verbose.txt", "body": "<details>\r\n<summary>\r\nFull log\r\n</summary>\r\n\r\n```\r\nrunning install\r\nrunning build_deps\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr/local/cuda (found suitable version \"8.0\", minimum required is \"7.0\") \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: .../src/pytorch/torch/lib/build/nccl\r\nScanning dependencies of target nccl\r\n[100%] Generating lib/libnccl.so\r\nGrabbing  src/nccl.h                          > .../src/pytorch/torch/lib/build/nccl/include/nccl.h\r\nCompiling src/libwrap.cu                      > .../src/pytorch/torch/lib/build/nccl/obj/libwrap.o\r\nCompiling src/core.cu                         > .../src/pytorch/torch/lib/build/nccl/obj/core.o\r\nCompiling src/all_gather.cu                   > .../src/pytorch/torch/lib/build/nccl/obj/all_gather.o\r\nCompiling src/all_reduce.cu                   > .../src/pytorch/torch/lib/build/nccl/obj/all_reduce.o\r\nCompiling src/broadcast.cu                    > .../src/pytorch/torch/lib/build/nccl/obj/broadcast.o\r\nCompiling src/reduce.cu                       > .../src/pytorch/torch/lib/build/nccl/obj/reduce.o\r\nCompiling src/reduce_scatter.cu               > .../src/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nLinking   libnccl.so.1.3.5                    > .../src/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\r\nArchiving libnccl_static.a                    > .../src/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\r\n[100%] Built target nccl\r\nInstall the project...\r\n-- Install configuration: \"Release\"\r\n-- Installing: .../src/pytorch/torch/lib/tmp_install/include/nccl.h\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr/local/cuda (found suitable version \"8.0\", minimum required is \"5.5\") \r\n-- Autodetected CUDA architecture(s): 6.1\r\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- Try OpenMP C flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Try OpenMP CXX flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Found OpenMP: -fopenmp  \r\n-- Compiling with OpenMP support\r\n-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - False\r\n-- Compiling with MAGMA support\r\n-- MAGMA INCLUDE DIRECTORIES: .../prefix/include\r\n-- MAGMA LIBRARIES: .../prefix/lib/libmagma.so\r\n-- MAGMA V2 check: 0\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_SSE1_1\r\n-- Performing Test C_HAS_SSE1_1 - Success\r\n-- Performing Test C_HAS_SSE2_1\r\n-- Performing Test C_HAS_SSE2_1 - Success\r\n-- Performing Test C_HAS_SSE3_1\r\n-- Performing Test C_HAS_SSE3_1 - Failed\r\n-- Performing Test C_HAS_SSE3_2\r\n-- Performing Test C_HAS_SSE3_2 - Success\r\n-- Performing Test C_HAS_SSE4_1_1\r\n-- Performing Test C_HAS_SSE4_1_1 - Failed\r\n-- Performing Test C_HAS_SSE4_1_2\r\n-- Performing Test C_HAS_SSE4_1_2 - Success\r\n-- Performing Test C_HAS_SSE4_2_1\r\n-- Performing Test C_HAS_SSE4_2_1 - Failed\r\n-- Performing Test C_HAS_SSE4_2_2\r\n-- Performing Test C_HAS_SSE4_2_2 - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Success\r\n-- Performing Test CXX_HAS_SSE1_1\r\n-- Performing Test CXX_HAS_SSE1_1 - Success\r\n-- Performing Test CXX_HAS_SSE2_1\r\n-- Performing Test CXX_HAS_SSE2_1 - Success\r\n-- Performing Test CXX_HAS_SSE3_1\r\n-- Performing Test CXX_HAS_SSE3_1 - Failed\r\n-- Performing Test CXX_HAS_SSE3_2\r\n-- Performing Test CXX_HAS_SSE3_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_1_1\r\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_2\r\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_2_1\r\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_2\r\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Success\r\n-- SSE2 Found\r\n-- SSE3 Found\r\n-- AVX Found\r\n-- AVX2 Found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Failed\r\n-- Performing Test HAS_MSC_ATOMICS\r\n-- Performing Test HAS_MSC_ATOMICS - Failed\r\n-- Performing Test HAS_GCC_ATOMICS\r\n-- Performing Test HAS_GCC_ATOMICS - Success\r\n-- Atomics: using GCC intrinsics\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl - guide - pthread - m]\r\n--   Library mkl: not found\r\n-- MKL library not found\r\n-- Checking for [openblas]\r\n--   Library openblas: .../prefix/lib/libopenblas.so\r\n-- Looking for sgemm_\r\n-- Looking for sgemm_ - found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (open).\r\n-- Looking for cheev_\r\n-- Looking for cheev_ - found\r\n-- Found a library with LAPACK API. (open)\r\n-- Found CUDNN: /home/mscho/vadim/cudnnR7.0.3/include  \r\n-- Found cuDNN: v7.0.3  (include: /home/mscho/vadim/cudnnR7.0.3/include, library: /home/mscho/vadim/cudnnR7.0.3/lib64/libcudnn.so)\r\n-- Using python found in /usr/bin/python\r\n['.../src/pytorch/aten/src/THNN/generic/THNN.h', '.../src/pytorch/aten/src/THCUNN/generic/THCUNN.h', '.../src/pytorch/aten/src/ATen/nn.yaml']\r\nATen Excluded: set(['bernoulli', 'bernoulli_'])\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\ndisable contrib because ATEN_NO_CONTRIB is set\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: .../src/pytorch/torch/lib/build/aten\r\n[  1%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\r\n.../src/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\r\n.../src/pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n                ^\r\n.../src/pytorch/aten/src/THC/THCBlas.cu:87:19: note: \u2018op\u2019 was declared here\r\n   cublasOperation_t op;\r\n                   ^\r\n.../src/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\r\n.../src/pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n                ^\r\n.../src/pytorch/aten/src/THC/THCBlas.cu:117:19: note: \u2018op\u2019 was declared here\r\n   cublasOperation_t op;\r\n                   ^\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\r\n['.../src/pytorch/aten/src/THNN/generic/THNN.h', '.../src/pytorch/aten/src/THCUNN/generic/THCUNN.h', '.../src/pytorch/aten/src/ATen/nn.yaml']\r\nATen Excluded: set(['bernoulli', 'bernoulli_'])\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                            ^\r\n.../src/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:437: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:425: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:443: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n.../src/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o\r\n.../src/pytorch/aten/src/THCS/generic/THCSTensor.cu(108): error: no instance of function template \"THCSTensor_coalesceValuesKernel\" matches the argument list\r\n            argument types are: (long *, long *, char *, char *, ptrdiff_t, int64_t, int64_t)\r\n\r\n1 error detected in the compilation of \"/tmp/tmpxft_00007f45_00000000-7_THCSTensor.cpp1.ii\".\r\nCMake Error at ATen_generated_THCSTensor.cu.o.cmake:267 (message):\r\n  Error generating file\r\n  .../src/pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THCS/./ATen_generated_THCSTensor.cu.o\r\n\r\n\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1106: recipe for target 'src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\r\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\r\nMakefile:127: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n</details>\r\n\r\nThe first error line is:\r\n```shell\r\n/usr/local/cuda/bin/nvcc pytorch/aten/src/THCS/THCSTensor.cu -c -o pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THCS/./ATen_generated_THCSTensor.cu.o -ccbin /usr/bin/cc -m64 --std c++11 -DUSE_GCC_ATOMICS=1 -DTH_INDEX_BASE=0 -DAT_CUDA_ENABLED -DATen_EXPORTS -Xcompiler ,\\\"-fexceptions\\\",\\\"-fopenmp\\\",\\\"-Wall\\\",\\\"-Wno-vla\\\",\\\"-fPIC\\\",\\\"-O3\\\" -Wno-deprecated-gpu-targets -gencode arch=compute_61,code=sm_61 -Xcompiler -fPIC -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -DNVCC -I/usr/local/cuda/include -I/home/mscho/vadim/.wigwam/prefix/include -Ipytorch/aten/src/TH -Ipytorch/aten/src/THC -Ipytorch/torch/lib/build/aten/src/TH -Ipytorch/torch/lib/build/aten/src/THC -Ipytorch/aten/src/THS -Ipytorch/aten/src/THCS -Ipytorch/torch/lib/build/aten/src/THS -Ipytorch/torch/lib/build/aten/src/THCS -Ipytorch/aten/src -Ipytorch/torch/lib/build/aten/src -Ipytorch/aten/src/THNN -Ipytorch/aten/src/THCUNN -I/usr/local/cuda/include -I/TH -Ipytorch/aten/src/ATen/CUDA_SDK_ROOT_DIR-NOTFOUND/common/inc -Ipytorch/aten/src/ATen/cuda -Ipytorch/aten/src/ATen/.. -Ipytorch/torch/lib/build/aten/src/ATen\r\n```\r\n\r\nI can repro the error by running this line again.\r\n\r\nVerbose log attached: [verbose.txt](https://github.com/pytorch/pytorch/files/1485241/verbose.txt)\r\n"}