{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440796324", "html_url": "https://github.com/pytorch/pytorch/issues/13807#issuecomment-440796324", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13807", "id": 440796324, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDc5NjMyNA==", "user": {"login": "CorentinJ", "id": 12038136, "node_id": "MDQ6VXNlcjEyMDM4MTM2", "avatar_url": "https://avatars0.githubusercontent.com/u/12038136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CorentinJ", "html_url": "https://github.com/CorentinJ", "followers_url": "https://api.github.com/users/CorentinJ/followers", "following_url": "https://api.github.com/users/CorentinJ/following{/other_user}", "gists_url": "https://api.github.com/users/CorentinJ/gists{/gist_id}", "starred_url": "https://api.github.com/users/CorentinJ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CorentinJ/subscriptions", "organizations_url": "https://api.github.com/users/CorentinJ/orgs", "repos_url": "https://api.github.com/users/CorentinJ/repos", "events_url": "https://api.github.com/users/CorentinJ/events{/privacy}", "received_events_url": "https://api.github.com/users/CorentinJ/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-21T20:16:36Z", "updated_at": "2018-11-21T20:17:09Z", "author_association": "NONE", "body_html": "<p>Here's my new code:</p>\n<pre><code>import numpy as np\nimport torch\nfrom timeit import default_timer as timer\nfrom torchvision.models import resnet50\n\ndef main():\n    # Define model and input data\n    resnet = resnet50().cuda()\n    x = torch.from_numpy(np.random.rand(1, 3, 224, 224).astype(np.float32)).cuda()\n\n    # The first pass is always slower, so run it once\n    resnet.forward(x)\n    \n    # Measure elapsed time\n    passes = 20\n    total_time = 0\n    for _ in range(passes):\n        torch.cuda.synchronize()\n        start = timer()\n        with torch.autograd.profiler.profile(use_cuda=True) as prof:\n            resnet(x)\n        torch.cuda.synchronize()\n        delta = timer() - start\n        \n        print('Forward pass: %.3fs' % delta)\n        total_time += delta\n    print('Average forward pass: %.3fs' % (total_time / passes))\n    \n    # Output the profiler log to a file\n    profiling_log = open(\"profiling.txt\", \"w\")\n    profiling_log.write(str(prof))\n    profiling_log.close()\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<p>And here are the outputs of the profiler (with and without CUDA enabled): <a href=\"https://github.com/pytorch/pytorch/files/2605658/profiling_autograd.zip\">profiling_autograd.zip</a></p>\n<p>The profiler performance hit was much harder on Unbuntu than on Windows, ironically. With CUDA enabled in the profiler, the execution speed on windows went from 60ms to 230ms but on Ubuntu from 11ms to 1386ms...</p>\n<p>(Also it appears I've closed the issue again by mistake, sorry about that)</p>", "body_text": "Here's my new code:\nimport numpy as np\nimport torch\nfrom timeit import default_timer as timer\nfrom torchvision.models import resnet50\n\ndef main():\n    # Define model and input data\n    resnet = resnet50().cuda()\n    x = torch.from_numpy(np.random.rand(1, 3, 224, 224).astype(np.float32)).cuda()\n\n    # The first pass is always slower, so run it once\n    resnet.forward(x)\n    \n    # Measure elapsed time\n    passes = 20\n    total_time = 0\n    for _ in range(passes):\n        torch.cuda.synchronize()\n        start = timer()\n        with torch.autograd.profiler.profile(use_cuda=True) as prof:\n            resnet(x)\n        torch.cuda.synchronize()\n        delta = timer() - start\n        \n        print('Forward pass: %.3fs' % delta)\n        total_time += delta\n    print('Average forward pass: %.3fs' % (total_time / passes))\n    \n    # Output the profiler log to a file\n    profiling_log = open(\"profiling.txt\", \"w\")\n    profiling_log.write(str(prof))\n    profiling_log.close()\n\nif __name__ == '__main__':\n    main()\n\nAnd here are the outputs of the profiler (with and without CUDA enabled): profiling_autograd.zip\nThe profiler performance hit was much harder on Unbuntu than on Windows, ironically. With CUDA enabled in the profiler, the execution speed on windows went from 60ms to 230ms but on Ubuntu from 11ms to 1386ms...\n(Also it appears I've closed the issue again by mistake, sorry about that)", "body": "Here's my new code:\r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom timeit import default_timer as timer\r\nfrom torchvision.models import resnet50\r\n\r\ndef main():\r\n    # Define model and input data\r\n    resnet = resnet50().cuda()\r\n    x = torch.from_numpy(np.random.rand(1, 3, 224, 224).astype(np.float32)).cuda()\r\n\r\n    # The first pass is always slower, so run it once\r\n    resnet.forward(x)\r\n    \r\n    # Measure elapsed time\r\n    passes = 20\r\n    total_time = 0\r\n    for _ in range(passes):\r\n        torch.cuda.synchronize()\r\n        start = timer()\r\n        with torch.autograd.profiler.profile(use_cuda=True) as prof:\r\n            resnet(x)\r\n        torch.cuda.synchronize()\r\n        delta = timer() - start\r\n        \r\n        print('Forward pass: %.3fs' % delta)\r\n        total_time += delta\r\n    print('Average forward pass: %.3fs' % (total_time / passes))\r\n    \r\n    # Output the profiler log to a file\r\n    profiling_log = open(\"profiling.txt\", \"w\")\r\n    profiling_log.write(str(prof))\r\n    profiling_log.close()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nAnd here are the outputs of the profiler (with and without CUDA enabled): [profiling_autograd.zip](https://github.com/pytorch/pytorch/files/2605658/profiling_autograd.zip)\r\n\r\nThe profiler performance hit was much harder on Unbuntu than on Windows, ironically. With CUDA enabled in the profiler, the execution speed on windows went from 60ms to 230ms but on Ubuntu from 11ms to 1386ms...\r\n\r\n(Also it appears I've closed the issue again by mistake, sorry about that)\r\n\r\n"}