{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8921", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8921/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8921/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8921/events", "html_url": "https://github.com/pytorch/pytorch/issues/8921", "id": 336014886, "node_id": "MDU6SXNzdWUzMzYwMTQ4ODY=", "number": 8921, "title": "[feature request] More methods for PackedSequence", "user": {"login": "dmitriy-serdyuk", "id": 2872615, "node_id": "MDQ6VXNlcjI4NzI2MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2872615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmitriy-serdyuk", "html_url": "https://github.com/dmitriy-serdyuk", "followers_url": "https://api.github.com/users/dmitriy-serdyuk/followers", "following_url": "https://api.github.com/users/dmitriy-serdyuk/following{/other_user}", "gists_url": "https://api.github.com/users/dmitriy-serdyuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmitriy-serdyuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmitriy-serdyuk/subscriptions", "organizations_url": "https://api.github.com/users/dmitriy-serdyuk/orgs", "repos_url": "https://api.github.com/users/dmitriy-serdyuk/repos", "events_url": "https://api.github.com/users/dmitriy-serdyuk/events{/privacy}", "received_events_url": "https://api.github.com/users/dmitriy-serdyuk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-06-26T22:32:21Z", "updated_at": "2018-08-31T09:17:08Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>As a person who works a lot with recurrent networks and sequences, I wish it was easier to work with <code>PackedSequence</code>. I frequently find myself packing/unpacking sequence to perform some simple operation such as</p>\n<pre><code>sequence, lengths = pad_packed_sequence(input, batch_first=True)\nsequence = elementwise_function(sequence)\npack_padded_sequence(sequence, lengths, batch_first=True)\n</code></pre>\n<p>This complicates code, introduces bugs (setting <code>batch_first</code> incorrectly), and makes me write two versions of code for tensors and sequences.</p>\n<p><code>PaddedSequence</code> can be extended to act more like <code>torch.Tensor</code>. More specifically, it can be extended with following methods (I tried to choose ones which make sense):</p>\n<h1>Element-wise unary operations (wrt to the tensor)</h1>\n<ul>\n<li>abs, acos, asin, atan, atan2, ceil, clamp, contiguous, cos, cosh,  exp, expm1, frac, log, log10, log1p, log2, mul, pow, reciprocal, neg, renorm, round, rsqrt, sigmoid, sign, sin, sinh, sqrt, tan, tanh, trunc</li>\n<li>fill_, pin_memory</li>\n</ul>\n<h1>Boolean functions</h1>\n<ul>\n<li>is_contiguous, is_pinned</li>\n</ul>\n<h1>Element-wise binary operations</h1>\n<p>Two sequences can be added, subtracted, compared, etc when they have same shape and lengths.</p>\n<ul>\n<li>add, div, eq, fmod, ge, gt, le, lt, ne, remainder, sub</li>\n<li>map_</li>\n</ul>\n<h1>Autograd functions</h1>\n<ul>\n<li>detach</li>\n</ul>", "body_text": "As a person who works a lot with recurrent networks and sequences, I wish it was easier to work with PackedSequence. I frequently find myself packing/unpacking sequence to perform some simple operation such as\nsequence, lengths = pad_packed_sequence(input, batch_first=True)\nsequence = elementwise_function(sequence)\npack_padded_sequence(sequence, lengths, batch_first=True)\n\nThis complicates code, introduces bugs (setting batch_first incorrectly), and makes me write two versions of code for tensors and sequences.\nPaddedSequence can be extended to act more like torch.Tensor. More specifically, it can be extended with following methods (I tried to choose ones which make sense):\nElement-wise unary operations (wrt to the tensor)\n\nabs, acos, asin, atan, atan2, ceil, clamp, contiguous, cos, cosh,  exp, expm1, frac, log, log10, log1p, log2, mul, pow, reciprocal, neg, renorm, round, rsqrt, sigmoid, sign, sin, sinh, sqrt, tan, tanh, trunc\nfill_, pin_memory\n\nBoolean functions\n\nis_contiguous, is_pinned\n\nElement-wise binary operations\nTwo sequences can be added, subtracted, compared, etc when they have same shape and lengths.\n\nadd, div, eq, fmod, ge, gt, le, lt, ne, remainder, sub\nmap_\n\nAutograd functions\n\ndetach", "body": "As a person who works a lot with recurrent networks and sequences, I wish it was easier to work with `PackedSequence`. I frequently find myself packing/unpacking sequence to perform some simple operation such as\r\n```\r\nsequence, lengths = pad_packed_sequence(input, batch_first=True)\r\nsequence = elementwise_function(sequence)\r\npack_padded_sequence(sequence, lengths, batch_first=True)\r\n```\r\n This complicates code, introduces bugs (setting `batch_first` incorrectly), and makes me write two versions of code for tensors and sequences.\r\n\r\n`PaddedSequence` can be extended to act more like `torch.Tensor`. More specifically, it can be extended with following methods (I tried to choose ones which make sense):\r\n\r\n# Element-wise unary operations (wrt to the tensor)\r\n\r\n- abs, acos, asin, atan, atan2, ceil, clamp, contiguous, cos, cosh,  exp, expm1, frac, log, log10, log1p, log2, mul, pow, reciprocal, neg, renorm, round, rsqrt, sigmoid, sign, sin, sinh, sqrt, tan, tanh, trunc \r\n- fill_, pin_memory\r\n\r\n# Boolean functions\r\n\r\n- is_contiguous, is_pinned\r\n\r\n# Element-wise binary operations\r\n\r\nTwo sequences can be added, subtracted, compared, etc when they have same shape and lengths.\r\n\r\n- add, div, eq, fmod, ge, gt, le, lt, ne, remainder, sub\r\n- map_\r\n\r\n# Autograd functions\r\n\r\n- detach"}