{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406978749", "html_url": "https://github.com/pytorch/pytorch/pull/9628#issuecomment-406978749", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9628", "id": 406978749, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjk3ODc0OQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-23T08:22:03Z", "updated_at": "2018-07-23T08:22:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So I experimented a bit.</p>\n<ul>\n<li>It seems beneficial to switch the t and s loop and to be able to cache the s target.</li>\n<li>For small alphabet, the performance is still noticeably worse than cudnn, but I might just be OK with that if I can avoid too many explicit optimizations.</li>\n<li>The naive \"index_log_add_\"-step is really, really terrible with large alphabets. Happily, there are two obvious ways to fix this.\n<ul>\n<li>One could move to non-log-space, but the numerical stability is problematic. the log_likelihood term is easily adjusted for before the exponentiation, but it is not as clear what to do with the prob term (take the min log_prob over the alphabet and adjust with that?). I would expect real activations to be more extremely spread out than unit random normal, so this is something to be careful about.</li>\n<li>Or one could do index_log_add_ properly. Yay! (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> Would you want <code>index_* </code> ported to native in the process?)</li>\n</ul>\n</li>\n<li>At any rate, separating the blank target and summing over that seems a good idea to no spin in atomic (log-) add.</li>\n</ul>\n<p>(Benchmark idea from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4767568\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/galv\">@galv</a> , <a href=\"http://danielgalvez.me/jekyll/update/2017/12/29/benchmarking-ctc-implementations.html\" rel=\"nofollow\">http://danielgalvez.me/jekyll/update/2017/12/29/benchmarking-ctc-implementations.html</a>, but with different random target lengths and my own errors, warpctc is unfair because it involves moving the result to cpu and gradientsback)</p>\n<table>\n<thead>\n<tr>\n<th>input_length</th>\n<th>batch_size</th>\n<th>num_labels</th>\n<th>cudnn</th>\n<th>cuda original PR</th>\n<th>warpctc (unfair)</th>\n<th>cuda with loop switched</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>150</td>\n<td>1</td>\n<td>28</td>\n<td>264</td>\n<td>633</td>\n<td>3597</td>\n<td>484</td>\n</tr>\n<tr>\n<td>150</td>\n<td>2</td>\n<td>28</td>\n<td>263</td>\n<td>772</td>\n<td>3591</td>\n<td>554</td>\n</tr>\n<tr>\n<td>150</td>\n<td>4</td>\n<td>28</td>\n<td>284</td>\n<td>859</td>\n<td>3741</td>\n<td>640</td>\n</tr>\n<tr>\n<td>150</td>\n<td>8</td>\n<td>28</td>\n<td>350</td>\n<td>856</td>\n<td>3758</td>\n<td>679</td>\n</tr>\n<tr>\n<td>150</td>\n<td>16</td>\n<td>28</td>\n<td>482</td>\n<td>901</td>\n<td>3784</td>\n<td>721</td>\n</tr>\n<tr>\n<td>150</td>\n<td>32</td>\n<td>28</td>\n<td>781</td>\n<td>1059</td>\n<td>3887</td>\n<td>853</td>\n</tr>\n<tr>\n<td>150</td>\n<td>64</td>\n<td>28</td>\n<td>1360</td>\n<td>2163</td>\n<td>3934</td>\n<td>1802</td>\n</tr>\n<tr>\n<td>150</td>\n<td>128</td>\n<td>28</td>\n<td>2646</td>\n<td>5236</td>\n<td>4139</td>\n<td>4712</td>\n</tr>\n<tr>\n<td>150</td>\n<td>256</td>\n<td>28</td>\n<td>5149</td>\n<td>10854</td>\n<td>4385</td>\n<td>9976</td>\n</tr>\n<tr>\n<td>150</td>\n<td>1</td>\n<td>5000</td>\n<td>340</td>\n<td>2737</td>\n<td>9460</td>\n<td>2627</td>\n</tr>\n<tr>\n<td>150</td>\n<td>2</td>\n<td>5000</td>\n<td>450</td>\n<td>4224</td>\n<td>10440</td>\n<td>4003</td>\n</tr>\n<tr>\n<td>150</td>\n<td>4</td>\n<td>5000</td>\n<td>589</td>\n<td>7589</td>\n<td>10659</td>\n<td>7394</td>\n</tr>\n<tr>\n<td>150</td>\n<td>8</td>\n<td>5000</td>\n<td>898</td>\n<td>7598</td>\n<td>11068</td>\n<td>7393</td>\n</tr>\n<tr>\n<td>150</td>\n<td>16</td>\n<td>5000</td>\n<td>1520</td>\n<td>8189</td>\n<td>11908</td>\n<td>7944</td>\n</tr>\n<tr>\n<td>150</td>\n<td>32</td>\n<td>5000</td>\n<td>2736</td>\n<td>9769</td>\n<td>13665</td>\n<td>9572</td>\n</tr>\n<tr>\n<td>150</td>\n<td>64</td>\n<td>5000</td>\n<td>5284</td>\n<td>23180</td>\n<td>17500</td>\n<td>22644</td>\n</tr>\n<tr>\n<td>150</td>\n<td>128</td>\n<td>5000</td>\n<td>10483</td>\n<td>120424</td>\n<td>24976</td>\n<td>117178</td>\n</tr>\n</tbody>\n</table>", "body_text": "So I experimented a bit.\n\nIt seems beneficial to switch the t and s loop and to be able to cache the s target.\nFor small alphabet, the performance is still noticeably worse than cudnn, but I might just be OK with that if I can avoid too many explicit optimizations.\nThe naive \"index_log_add_\"-step is really, really terrible with large alphabets. Happily, there are two obvious ways to fix this.\n\nOne could move to non-log-space, but the numerical stability is problematic. the log_likelihood term is easily adjusted for before the exponentiation, but it is not as clear what to do with the prob term (take the min log_prob over the alphabet and adjust with that?). I would expect real activations to be more extremely spread out than unit random normal, so this is something to be careful about.\nOr one could do index_log_add_ properly. Yay! (@soumith Would you want index_*  ported to native in the process?)\n\n\nAt any rate, separating the blank target and summing over that seems a good idea to no spin in atomic (log-) add.\n\n(Benchmark idea from @galv , http://danielgalvez.me/jekyll/update/2017/12/29/benchmarking-ctc-implementations.html, but with different random target lengths and my own errors, warpctc is unfair because it involves moving the result to cpu and gradientsback)\n\n\n\ninput_length\nbatch_size\nnum_labels\ncudnn\ncuda original PR\nwarpctc (unfair)\ncuda with loop switched\n\n\n\n\n150\n1\n28\n264\n633\n3597\n484\n\n\n150\n2\n28\n263\n772\n3591\n554\n\n\n150\n4\n28\n284\n859\n3741\n640\n\n\n150\n8\n28\n350\n856\n3758\n679\n\n\n150\n16\n28\n482\n901\n3784\n721\n\n\n150\n32\n28\n781\n1059\n3887\n853\n\n\n150\n64\n28\n1360\n2163\n3934\n1802\n\n\n150\n128\n28\n2646\n5236\n4139\n4712\n\n\n150\n256\n28\n5149\n10854\n4385\n9976\n\n\n150\n1\n5000\n340\n2737\n9460\n2627\n\n\n150\n2\n5000\n450\n4224\n10440\n4003\n\n\n150\n4\n5000\n589\n7589\n10659\n7394\n\n\n150\n8\n5000\n898\n7598\n11068\n7393\n\n\n150\n16\n5000\n1520\n8189\n11908\n7944\n\n\n150\n32\n5000\n2736\n9769\n13665\n9572\n\n\n150\n64\n5000\n5284\n23180\n17500\n22644\n\n\n150\n128\n5000\n10483\n120424\n24976\n117178", "body": "So I experimented a bit.\r\n- It seems beneficial to switch the t and s loop and to be able to cache the s target.\r\n- For small alphabet, the performance is still noticeably worse than cudnn, but I might just be OK with that if I can avoid too many explicit optimizations.\r\n- The naive \"index_log_add_\"-step is really, really terrible with large alphabets. Happily, there are two obvious ways to fix this.\r\n  - One could move to non-log-space, but the numerical stability is problematic. the log_likelihood term is easily adjusted for before the exponentiation, but it is not as clear what to do with the prob term (take the min log_prob over the alphabet and adjust with that?). I would expect real activations to be more extremely spread out than unit random normal, so this is something to be careful about.\r\n  - Or one could do index_log_add_ properly. Yay! (@soumith Would you want `index_* ` ported to native in the process?)\r\n- At any rate, separating the blank target and summing over that seems a good idea to no spin in atomic (log-) add.\r\n\r\n(Benchmark idea from @galv , http://danielgalvez.me/jekyll/update/2017/12/29/benchmarking-ctc-implementations.html, but with different random target lengths and my own errors, warpctc is unfair because it involves moving the result to cpu and gradientsback)\r\n\r\ninput_length | batch_size | num_labels |  cudnn | cuda original PR | warpctc (unfair) | cuda with loop switched\r\n-------------|------------|------------|--------|------------------|------------------|------------------------\r\n150  |  1  |  28 |  264 |  633 |  3597 |  484\r\n150  |  2  |  28 |  263 |  772 |  3591 |  554\r\n150  |  4  |  28 |  284 |  859 |  3741 |  640\r\n150  |  8  |  28 |  350 |  856 |  3758 |  679\r\n150  |  16  |  28 |  482 |  901 |  3784 |  721\r\n150  |  32  |  28 |  781 |  1059 |  3887 |  853\r\n150  |  64  |  28 |  1360 |  2163 |  3934 |  1802\r\n150  |  128  |  28 |  2646 |  5236 |  4139 |  4712\r\n150  |  256  |  28 |  5149 |  10854 |  4385 |  9976\r\n150  |  1  |  5000 |  340 |  2737 |  9460 |  2627\r\n150  |  2  |  5000 |  450 |  4224 |  10440 |  4003\r\n150  |  4  |  5000 |  589 |  7589 |  10659 |  7394\r\n150  |  8  |  5000 |  898 |  7598 |  11068 |  7393\r\n150  |  16  |  5000 |  1520 |  8189 |  11908 |  7944\r\n150  |  32  |  5000 |  2736 |  9769 |  13665 |  9572\r\n150  |  64  |  5000 |  5284 |  23180 |  17500 |  22644\r\n150  |  128  |  5000 |  10483 |  120424 |  24976 |  117178\r\n"}