{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206775218", "pull_request_review_id": 142251180, "id": 206775218, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjc3NTIxOA==", "diff_hunk": "@@ -1123,6 +1123,61 @@ def forward(self, anchor, positive, negative):\n         return F.triplet_margin_loss(anchor, positive, negative, margin=self.margin, p=self.p,\n                                      eps=self.eps, swap=self.swap, reduction=self.reduction)\n \n+\n+class CTCLoss(_Loss):\n+    r\"\"\"The Connectionist Temporal Classification loss.\n+\n+    Args:\n+        blank (int, optional): blank label. Default :math:`0`.\n+        reduction (string, optional): Specifies the reduction to apply to the output:\n+            'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n+            'elementwise_mean': the output losses will be divided by the target lengths and\n+            then the mean over the batch is taken. Default: 'elementwise_mean'\n+\n+    Inputs:\n+        log_probs: :math:`(T, N, C)` where `C = number of characters in alphabet including blank`,\n+            `T = input length`, and `N = batch size`.\n+            The logarithmized probabilities of the outputs\n+            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n+        targets: :math:`(N, S)` or `(sum(target_lenghts))`.\n+            Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.\n+        input_lengths: :math:`(N)`.\n+            Lengths of the inputs (must each be :math:`\\leq T`)\n+        target_lengths: :math:`(N)`.\n+            Lengths of the targets\n+\n+\n+    Example::\n+\n+        >>> ctc_loss = nn.CTCLoss()\n+        >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n+        >>> targets = torch.randint(1, 21, (16, 30), dtype=torch.long)\n+        >>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n+        >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n+        >>> loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n+        >>> loss.backward()\n+\n+    Reference:\n+        A. Graves et al.: Connectionist Temporal Classification:\n+        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:\n+        https://www.cs.toronto.edu/~graves/icml_2006.pdf\n+\n+    .. Note::\n+        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be\n+        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,\n+        :attr:`target_lengths` :math:`\\leq 256`, the integer arguments must be of\n+        :class:`torch.IntTensor`.", "path": "torch/nn/modules/loss.py", "position": 47, "original_position": 47, "commit_id": "11b97f7337172d5bfef0e2af792569597aafaee1", "original_commit_id": "11b97f7337172d5bfef0e2af792569597aafaee1", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Yeah, I got annoyed at requiring longints for cuda when it is the fallback for cudnn, so cuda now takes either.", "created_at": "2018-08-01T07:12:15Z", "updated_at": "2018-11-23T15:48:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/9628#discussion_r206775218", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9628", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206775218"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9628#discussion_r206775218"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9628"}}, "body_html": "<p>Yeah, I got annoyed at requiring longints for cuda when it is the fallback for cudnn, so cuda now takes either.</p>", "body_text": "Yeah, I got annoyed at requiring longints for cuda when it is the fallback for cudnn, so cuda now takes either.", "in_reply_to_id": 206624160}