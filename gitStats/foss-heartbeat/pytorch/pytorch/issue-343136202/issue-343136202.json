{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9628", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9628/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9628/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9628/events", "html_url": "https://github.com/pytorch/pytorch/pull/9628", "id": 343136202, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAyODcxNDk2", "number": 9628, "title": "Add CTC loss", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2018-07-20T14:47:38Z", "updated_at": "2018-11-23T15:48:31Z", "closed_at": "2018-07-31T18:10:56Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9628", "html_url": "https://github.com/pytorch/pytorch/pull/9628", "diff_url": "https://github.com/pytorch/pytorch/pull/9628.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9628.patch"}, "body_html": "<p>The CPU and CUDA variants are a direct transposition of Graves et al.'s description of the algorithm with the<br>\nmodification that is is in log space.<br>\nThe there also is a binding for the (much faster) CuDNN implementation.</p>\n<p>This could eventually <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #3420.\">fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"270365568\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3420\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3420/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3420\">#3420</a></p>\n<p>I still need to add tests (TestNN seems much more elaborate than the other testing) and fix the bugs than invariably turn up during the testing. Also, I want to add some more code comments.</p>\n<p>I could use feedback on all sorts of things, including:</p>\n<ul>\n<li>Type handling (cuda vs. cpu for the int tensors, dtype for the int tensors)</li>\n<li>Input convention. I use log probs because that is what the gradients are for.</li>\n<li>Launch parameters for the kernels</li>\n<li>Errors and obmissions and anything else I'm not even aware of.</li>\n</ul>\n<p>Thank you for looking!</p>\n<p>In terms of performance it looks like it is superficially comparable to WarpCTC (and thus, but I have not systematically investigated this).<br>\nI have read CuDNN is much faster than implementations because it does <em>not</em> use log-space, but also the gathering step is much much faster (but I avoided trying tricky things, it seems to contribute to warpctc's fragility). I might think some more which existing torch function (scatter or index..) I could learn from for that step.<br>\nAverage timings for the kernels from nvprof for some size:</p>\n<pre><code>CuDNN:\n60.464us compute_alphas_and_betas\n16.755us compute_grads_deterministic\nCuda:\n121.06us ctc_loss_backward_collect_gpu_kernel (= grads)\n109.88us ctc_loss_gpu_kernel (= alphas)\n98.517us ctc_loss_backward_betas_gpu_kernel (= betas)\nWarpCTC:\n299.74us compute_betas_and_grad_kernel\n66.977us compute_alpha_kernel\n</code></pre>\n<p>Of course, I still have the (silly) outer blocks loop rather than computing consecutive <code>s</code> in each thread which I might change, and there are a few other things where one could look for better implementations.</p>\n<p>Finally, it might not be unreasonable to start with these implementations, as the performance of the loss has to be seen in the context of the entire training computation, so this would likely dilute the relative speedup considerably.</p>\n<p>My performance measuring testing script:</p>\n<pre><code>import timeit\nimport sys\nimport torch\nnum_labels = 10\ntarget_length  = 30\ninput_length = 50\neps = 1e-5\nBLANK = 0#num_labels\nbatch_size = 16\n\n\ntorch.manual_seed(5)\nactivations = torch.randn(input_length, batch_size, num_labels + 1)\nlog_probs = torch.log_softmax(activations, 2)\nprobs = torch.exp(log_probs)\ntargets = torch.randint(1, num_labels+1, (batch_size * target_length,), dtype=torch.long)\ntargets_2d = targets.view(batch_size, target_length)\ntarget_lengths = torch.tensor(batch_size*[target_length])\ninput_lengths = torch.tensor(batch_size*[input_length])\nactivations = log_probs.detach()\n\n\ndef time_cuda_ctc_loss(grout, *args):\n    torch.cuda.synchronize()\n    culo, culog_alpha = torch._ctc_loss(*args)\n    g, = torch.autograd.grad(culo, args[0], grout)\n    torch.cuda.synchronize()\n\ndef time_cudnn_ctc_loss(groupt, *args):\n    torch.cuda.synchronize()\n    culo, cugra= torch._cudnn_ctc_loss(*args)\n    g, = torch.autograd.grad(culo, args[0], grout)\n    torch.cuda.synchronize()\n\ndef time_warp_ctc_loss(grout, *args):\n    torch.cuda.synchronize()\n    culo = warpctc.ctc_loss(*args, blank_label=BLANK, size_average=False, length_average=False, reduce=False)\n    g, = torch.autograd.grad(culo, args[0], grout)\n    torch.cuda.synchronize()\n\nif sys.argv[1] == 'cuda':\n    lpcu = log_probs.float().cuda().detach().requires_grad_()\n    args = [lpcu, targets_2d.cuda(), input_lengths.cuda(), target_lengths.cuda(), BLANK]\n    grout = lpcu.new_ones((batch_size,))\n    torch.cuda.synchronize()\n    print(timeit.repeat(\"time_cuda_ctc_loss(grout, *args)\", number=1000, globals=globals()))\nelif sys.argv[1] == 'cudnn':\n    lpcu = log_probs.float().cuda().detach().requires_grad_()\n    args = [lpcu, targets.int(), input_lengths.int(), target_lengths.int(), BLANK, True]\n    grout = lpcu.new_ones((batch_size,))\n    torch.cuda.synchronize()\n    print(timeit.repeat(\"time_cudnn_ctc_loss(grout, *args)\", number=1000, globals=globals()))\nelif sys.argv[1] == 'warpctc':\n    import warpctc\n    activations = activations.cuda().detach().requires_grad_()\n    args = [activations, input_lengths.int(), targets.int(), target_lengths.int()]\n    grout = activations.new_ones((batch_size,), device='cpu')\n    torch.cuda.synchronize()\n\n    print(timeit.repeat(\"time_warp_ctc_loss(grout, *args)\", number=1000, globals=globals()))\n</code></pre>\n<p>I'll also link to a notebook that I used for writing up the algorithm in simple form and then test the against implementations against it.</p>", "body_text": "The CPU and CUDA variants are a direct transposition of Graves et al.'s description of the algorithm with the\nmodification that is is in log space.\nThe there also is a binding for the (much faster) CuDNN implementation.\nThis could eventually fix #3420\nI still need to add tests (TestNN seems much more elaborate than the other testing) and fix the bugs than invariably turn up during the testing. Also, I want to add some more code comments.\nI could use feedback on all sorts of things, including:\n\nType handling (cuda vs. cpu for the int tensors, dtype for the int tensors)\nInput convention. I use log probs because that is what the gradients are for.\nLaunch parameters for the kernels\nErrors and obmissions and anything else I'm not even aware of.\n\nThank you for looking!\nIn terms of performance it looks like it is superficially comparable to WarpCTC (and thus, but I have not systematically investigated this).\nI have read CuDNN is much faster than implementations because it does not use log-space, but also the gathering step is much much faster (but I avoided trying tricky things, it seems to contribute to warpctc's fragility). I might think some more which existing torch function (scatter or index..) I could learn from for that step.\nAverage timings for the kernels from nvprof for some size:\nCuDNN:\n60.464us compute_alphas_and_betas\n16.755us compute_grads_deterministic\nCuda:\n121.06us ctc_loss_backward_collect_gpu_kernel (= grads)\n109.88us ctc_loss_gpu_kernel (= alphas)\n98.517us ctc_loss_backward_betas_gpu_kernel (= betas)\nWarpCTC:\n299.74us compute_betas_and_grad_kernel\n66.977us compute_alpha_kernel\n\nOf course, I still have the (silly) outer blocks loop rather than computing consecutive s in each thread which I might change, and there are a few other things where one could look for better implementations.\nFinally, it might not be unreasonable to start with these implementations, as the performance of the loss has to be seen in the context of the entire training computation, so this would likely dilute the relative speedup considerably.\nMy performance measuring testing script:\nimport timeit\nimport sys\nimport torch\nnum_labels = 10\ntarget_length  = 30\ninput_length = 50\neps = 1e-5\nBLANK = 0#num_labels\nbatch_size = 16\n\n\ntorch.manual_seed(5)\nactivations = torch.randn(input_length, batch_size, num_labels + 1)\nlog_probs = torch.log_softmax(activations, 2)\nprobs = torch.exp(log_probs)\ntargets = torch.randint(1, num_labels+1, (batch_size * target_length,), dtype=torch.long)\ntargets_2d = targets.view(batch_size, target_length)\ntarget_lengths = torch.tensor(batch_size*[target_length])\ninput_lengths = torch.tensor(batch_size*[input_length])\nactivations = log_probs.detach()\n\n\ndef time_cuda_ctc_loss(grout, *args):\n    torch.cuda.synchronize()\n    culo, culog_alpha = torch._ctc_loss(*args)\n    g, = torch.autograd.grad(culo, args[0], grout)\n    torch.cuda.synchronize()\n\ndef time_cudnn_ctc_loss(groupt, *args):\n    torch.cuda.synchronize()\n    culo, cugra= torch._cudnn_ctc_loss(*args)\n    g, = torch.autograd.grad(culo, args[0], grout)\n    torch.cuda.synchronize()\n\ndef time_warp_ctc_loss(grout, *args):\n    torch.cuda.synchronize()\n    culo = warpctc.ctc_loss(*args, blank_label=BLANK, size_average=False, length_average=False, reduce=False)\n    g, = torch.autograd.grad(culo, args[0], grout)\n    torch.cuda.synchronize()\n\nif sys.argv[1] == 'cuda':\n    lpcu = log_probs.float().cuda().detach().requires_grad_()\n    args = [lpcu, targets_2d.cuda(), input_lengths.cuda(), target_lengths.cuda(), BLANK]\n    grout = lpcu.new_ones((batch_size,))\n    torch.cuda.synchronize()\n    print(timeit.repeat(\"time_cuda_ctc_loss(grout, *args)\", number=1000, globals=globals()))\nelif sys.argv[1] == 'cudnn':\n    lpcu = log_probs.float().cuda().detach().requires_grad_()\n    args = [lpcu, targets.int(), input_lengths.int(), target_lengths.int(), BLANK, True]\n    grout = lpcu.new_ones((batch_size,))\n    torch.cuda.synchronize()\n    print(timeit.repeat(\"time_cudnn_ctc_loss(grout, *args)\", number=1000, globals=globals()))\nelif sys.argv[1] == 'warpctc':\n    import warpctc\n    activations = activations.cuda().detach().requires_grad_()\n    args = [activations, input_lengths.int(), targets.int(), target_lengths.int()]\n    grout = activations.new_ones((batch_size,), device='cpu')\n    torch.cuda.synchronize()\n\n    print(timeit.repeat(\"time_warp_ctc_loss(grout, *args)\", number=1000, globals=globals()))\n\nI'll also link to a notebook that I used for writing up the algorithm in simple form and then test the against implementations against it.", "body": "The CPU and CUDA variants are a direct transposition of Graves et al.'s description of the algorithm with the\r\nmodification that is is in log space.\r\nThe there also is a binding for the (much faster) CuDNN implementation.\r\n\r\nThis could eventually fix #3420 \r\n\r\nI still need to add tests (TestNN seems much more elaborate than the other testing) and fix the bugs than invariably turn up during the testing. Also, I want to add some more code comments.\r\n\r\nI could use feedback on all sorts of things, including:\r\n- Type handling (cuda vs. cpu for the int tensors, dtype for the int tensors)\r\n- Input convention. I use log probs because that is what the gradients are for.\r\n- Launch parameters for the kernels\r\n- Errors and obmissions and anything else I'm not even aware of.\r\n\r\nThank you for looking!\r\n\r\nIn terms of performance it looks like it is superficially comparable to WarpCTC (and thus, but I have not systematically investigated this).\r\nI have read CuDNN is much faster than implementations because it does *not* use log-space, but also the gathering step is much much faster (but I avoided trying tricky things, it seems to contribute to warpctc's fragility). I might think some more which existing torch function (scatter or index..) I could learn from for that step.\r\nAverage timings for the kernels from nvprof for some size:\r\n\r\n```\r\nCuDNN:\r\n60.464us compute_alphas_and_betas\r\n16.755us compute_grads_deterministic\r\nCuda:\r\n121.06us ctc_loss_backward_collect_gpu_kernel (= grads)\r\n109.88us ctc_loss_gpu_kernel (= alphas)\r\n98.517us ctc_loss_backward_betas_gpu_kernel (= betas)\r\nWarpCTC:\r\n299.74us compute_betas_and_grad_kernel\r\n66.977us compute_alpha_kernel\r\n```\r\n\r\nOf course, I still have the (silly) outer blocks loop rather than computing consecutive `s` in each thread which I might change, and there are a few other things where one could look for better implementations.\r\n\r\nFinally, it might not be unreasonable to start with these implementations, as the performance of the loss has to be seen in the context of the entire training computation, so this would likely dilute the relative speedup considerably.\r\n\r\nMy performance measuring testing script:\r\n```\r\nimport timeit\r\nimport sys\r\nimport torch\r\nnum_labels = 10\r\ntarget_length  = 30\r\ninput_length = 50\r\neps = 1e-5\r\nBLANK = 0#num_labels\r\nbatch_size = 16\r\n\r\n\r\ntorch.manual_seed(5)\r\nactivations = torch.randn(input_length, batch_size, num_labels + 1)\r\nlog_probs = torch.log_softmax(activations, 2)\r\nprobs = torch.exp(log_probs)\r\ntargets = torch.randint(1, num_labels+1, (batch_size * target_length,), dtype=torch.long)\r\ntargets_2d = targets.view(batch_size, target_length)\r\ntarget_lengths = torch.tensor(batch_size*[target_length])\r\ninput_lengths = torch.tensor(batch_size*[input_length])\r\nactivations = log_probs.detach()\r\n\r\n\r\ndef time_cuda_ctc_loss(grout, *args):\r\n    torch.cuda.synchronize()\r\n    culo, culog_alpha = torch._ctc_loss(*args)\r\n    g, = torch.autograd.grad(culo, args[0], grout)\r\n    torch.cuda.synchronize()\r\n\r\ndef time_cudnn_ctc_loss(groupt, *args):\r\n    torch.cuda.synchronize()\r\n    culo, cugra= torch._cudnn_ctc_loss(*args)\r\n    g, = torch.autograd.grad(culo, args[0], grout)\r\n    torch.cuda.synchronize()\r\n\r\ndef time_warp_ctc_loss(grout, *args):\r\n    torch.cuda.synchronize()\r\n    culo = warpctc.ctc_loss(*args, blank_label=BLANK, size_average=False, length_average=False, reduce=False)\r\n    g, = torch.autograd.grad(culo, args[0], grout)\r\n    torch.cuda.synchronize()\r\n\r\nif sys.argv[1] == 'cuda':\r\n    lpcu = log_probs.float().cuda().detach().requires_grad_()\r\n    args = [lpcu, targets_2d.cuda(), input_lengths.cuda(), target_lengths.cuda(), BLANK]\r\n    grout = lpcu.new_ones((batch_size,))\r\n    torch.cuda.synchronize()\r\n    print(timeit.repeat(\"time_cuda_ctc_loss(grout, *args)\", number=1000, globals=globals()))\r\nelif sys.argv[1] == 'cudnn':\r\n    lpcu = log_probs.float().cuda().detach().requires_grad_()\r\n    args = [lpcu, targets.int(), input_lengths.int(), target_lengths.int(), BLANK, True]\r\n    grout = lpcu.new_ones((batch_size,))\r\n    torch.cuda.synchronize()\r\n    print(timeit.repeat(\"time_cudnn_ctc_loss(grout, *args)\", number=1000, globals=globals()))\r\nelif sys.argv[1] == 'warpctc':\r\n    import warpctc\r\n    activations = activations.cuda().detach().requires_grad_()\r\n    args = [activations, input_lengths.int(), targets.int(), target_lengths.int()]\r\n    grout = activations.new_ones((batch_size,), device='cpu')\r\n    torch.cuda.synchronize()\r\n\r\n    print(timeit.repeat(\"time_warp_ctc_loss(grout, *args)\", number=1000, globals=globals()))\r\n```\r\nI'll also link to a notebook that I used for writing up the algorithm in simple form and then test the against implementations against it.\r\n"}