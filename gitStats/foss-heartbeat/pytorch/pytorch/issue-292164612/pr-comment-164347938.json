{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164347938", "pull_request_review_id": 92104702, "id": 164347938, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDM0NzkzOA==", "diff_hunk": "@@ -45,22 +47,95 @@ static std::vector<Value*> gradientForNode(Node* node, ArrayRef<Value*> grad_val\n   return fmap(sym_grads, [](const SymbolicVariable &v) { return v.value(); });\n }\n \n+static value_set findAllRequiresGradNodes(\n+        Graph& graph, const std::vector<bool>& input_requires_grad) {\n+  JIT_ASSERT(graph.inputs().size() == input_requires_grad.size());\n+\n+  std::unordered_set<Value*> requires_grad_set;\n+  const auto requires_grad = [&](Value *v) { return requires_grad_set.count(v) > 0; };\n+\n+  auto inputs = graph.inputs();\n+  for (std::size_t i = 0, num_inputs = inputs.size(); i < num_inputs; ++i) {\n+    if (!input_requires_grad[i]) continue;\n+    requires_grad_set.emplace(inputs[i]);\n+  }\n+\n+  for (Node * node : graph.nodes()) {\n+    if (std::none_of(node->inputs().begin(), node->inputs().end(), requires_grad)) continue;\n+    for (Value * output : node->outputs())\n+      requires_grad_set.emplace(output);\n+  }\n+\n+  return requires_grad_set;\n+}\n+\n+static Value* allocZerosLike(Value *v) {\n+  static const Symbol constant_sym = \"constant\"_sym;\n+  static const Symbol is_zero_sym  = \"is_zero\"_sym;\n+  static const Symbol value_sym    = \"value\"_sym;\n+  JIT_EXPECTM(v->hasType(), \"can't allocate zero gradient for a value without a type\");\n+  Graph *graph = v->owningGraph();\n+  auto type = v->type()->expect<TensorType>();\n+  AutoGPU gpu_guard(type->device());\n+\n+  auto & at_type = type->device() == -1 ? at::CPU(type->scalarType()) : at::CUDA(type->scalarType());\n+  auto zeros = at_type.zeros({1}).expand(type->sizes());\n+  Node *constant = graph->create(constant_sym)\n+                        ->t_(value_sym, zeros)\n+                        ->i_(is_zero_sym, 1);\n+  graph->appendNode(constant);\n+  return constant->output();\n+}\n+\n+struct ReverseDetails {\n+  ReverseDetails(value_map&& grad_map, value_set&& requires_grad_set)\n+    : grad_map(std::move(grad_map))\n+    , requires_grad_set(std::move(requires_grad_set)) {}\n+\n+  value_map grad_map;\n+  value_set requires_grad_set;\n+};\n+\n // Before:\n //   - graph has only stage 0\n //   - grad_desc doesn't have any fields initialized\n // After:\n //   - graph has stage 0 and stage 1 that computes its vjp\n //   - grad_desc has df_input_vjps and df_output_vjps set\n //     (but df_input_vjps will be modified later as well)\n-static value_map addReverseInline(Graph& graph, Gradient& grad_desc) {\n+static ReverseDetails addReverseInline(Graph& graph, Gradient& grad_desc,\n+                                  const std::vector<bool>& input_requires_grad) {\n   JIT_ASSERT(graph.stage() == 0);\n   graph.advanceStage();\n \n+  auto requires_grad_set = findAllRequiresGradNodes(graph, input_requires_grad);\n+  const auto requires_grad = [&](Value *v) { return requires_grad_set.count(v) > 0; };\n+\n   value_map grad_map; // x -> dx mapping\n-  const auto get_grad = [&](Value* v) { return grad_map.at(v); };\n+  const auto get_grad = [&](Value* v) -> Value* {\n+    auto it = grad_map.find(v);\n+    if (it == grad_map.end()) {\n+      std::tie(it, std::ignore) = grad_map.emplace(v, allocZerosLike(v));\n+    }\n+    return it->second;\n+  };\n   const auto set_grad = [&](Value *x, Value *dx) {\n     if (Value * prev_grad = grad_map[x]) {\n-      Value * new_grad = addAndPutAfter(prev_grad, dx, dx->node());\n+      // NB: unique() gives us correct topological ordering within values created inside", "path": "torch/csrc/jit/autodiff.cpp", "position": null, "original_position": 91, "commit_id": "eb029c49aa4d7129706aa94a1180c037ae0a7843", "original_commit_id": "b6b7ba22b9c8d6ca6cdd21ad55f41e97ab2eb067", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Relying on unique is pretty fragile to future changes to unique and to the implementation of this function. Is it every wrong to just append it to the end of the nodes list?", "created_at": "2018-01-29T07:05:48Z", "updated_at": "2018-11-23T15:38:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/4898#discussion_r164347938", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4898", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164347938"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4898#discussion_r164347938"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4898"}}, "body_html": "<p>Relying on unique is pretty fragile to future changes to unique and to the implementation of this function. Is it every wrong to just append it to the end of the nodes list?</p>", "body_text": "Relying on unique is pretty fragile to future changes to unique and to the implementation of this function. Is it every wrong to just append it to the end of the nodes list?"}