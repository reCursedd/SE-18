{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8897", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8897/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8897/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8897/events", "html_url": "https://github.com/pytorch/pytorch/issues/8897", "id": 335822980, "node_id": "MDU6SXNzdWUzMzU4MjI5ODA=", "number": 8897, "title": "[feature request] Expanding gradient function with variances", "user": {"login": "alexdepremia", "id": 8928344, "node_id": "MDQ6VXNlcjg5MjgzNDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/8928344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexdepremia", "html_url": "https://github.com/alexdepremia", "followers_url": "https://api.github.com/users/alexdepremia/followers", "following_url": "https://api.github.com/users/alexdepremia/following{/other_user}", "gists_url": "https://api.github.com/users/alexdepremia/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexdepremia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexdepremia/subscriptions", "organizations_url": "https://api.github.com/users/alexdepremia/orgs", "repos_url": "https://api.github.com/users/alexdepremia/repos", "events_url": "https://api.github.com/users/alexdepremia/events{/privacy}", "received_events_url": "https://api.github.com/users/alexdepremia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-26T13:35:31Z", "updated_at": "2018-06-26T18:16:30Z", "closed_at": "2018-06-26T18:16:30Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nin many applications will be useful to have available not only the gradients averaged on minibatches, but also its second momentum for each parameter.<br>\nI think developers can do it with a small computation overhead by modifying accumulate_grad.cpp (I was not able to understand the underlying class structure), accumulating also the square of the individual contribution to the gradient (which is much more simple that getting the individual contribution to the gradients as requested in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"325590018\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7786\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7786/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7786\">#7786</a>).<br>\nI'm aware that it will duplicate the memory requirements, but hopefully it can be done as optional.<br>\nThank you very much.</p>\n<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>Provide a short description.</p>\n<h2>Code example</h2>\n<p>Please try to provide a minimal example to repro the bug.<br>\nError messages and stack traces are also helpful.</p>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2:</li>\n<li>How you installed PyTorch (conda, pip, source):</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS:</li>\n<li>PyTorch version:</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Hi,\nin many applications will be useful to have available not only the gradients averaged on minibatches, but also its second momentum for each parameter.\nI think developers can do it with a small computation overhead by modifying accumulate_grad.cpp (I was not able to understand the underlying class structure), accumulating also the square of the individual contribution to the gradient (which is much more simple that getting the individual contribution to the gradients as requested in #7786).\nI'm aware that it will duplicate the memory requirements, but hopefully it can be done as optional.\nThank you very much.\nIf you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nProvide a short description.\nCode example\nPlease try to provide a minimal example to repro the bug.\nError messages and stack traces are also helpful.\nSystem Info\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch or Caffe2:\nHow you installed PyTorch (conda, pip, source):\nBuild command you used (if compiling from source):\nOS:\nPyTorch version:\nPython version:\nCUDA/cuDNN version:\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "Hi,\r\nin many applications will be useful to have available not only the gradients averaged on minibatches, but also its second momentum for each parameter.\r\nI think developers can do it with a small computation overhead by modifying accumulate_grad.cpp (I was not able to understand the underlying class structure), accumulating also the square of the individual contribution to the gradient (which is much more simple that getting the individual contribution to the gradients as requested in https://github.com/pytorch/pytorch/issues/7786). \r\nI'm aware that it will duplicate the memory requirements, but hopefully it can be done as optional.\r\nThank you very much.\r\n\r\nIf you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\nProvide a short description.\r\n\r\n## Code example\r\n\r\nPlease try to provide a minimal example to repro the bug.\r\nError messages and stack traces are also helpful.\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2:\r\n- How you installed PyTorch (conda, pip, source):\r\n- Build command you used (if compiling from source):\r\n- OS:\r\n- PyTorch version:\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}