{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200120180", "pull_request_review_id": 134372260, "id": 200120180, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDEyMDE4MA==", "diff_hunk": "@@ -1443,6 +1456,113 @@ def test_normal_sample(self):\n                                         scipy.stats.norm(loc=loc, scale=scale),\n                                         'Normal(mean={}, std={})'.format(loc, scale))\n \n+    def test_lowrank_multivariate_normal_shape(self):\n+        mean = torch.randn(5, 3, requires_grad=True)\n+        mean_no_batch = torch.randn(3, requires_grad=True)\n+        mean_multi_batch = torch.randn(6, 5, 3, requires_grad=True)\n+\n+        # construct PSD covariance\n+        scale_factor = torch.randn(3, 1, requires_grad=True)\n+        scale_diag = torch.tensor(torch.randn(3).abs(), requires_grad=True)\n+\n+        # construct batch of PSD covariances\n+        scale_factor_batched = torch.randn(6, 5, 3, 2, requires_grad=True)\n+        scale_diag_batched = torch.tensor(torch.randn(6, 5, 3).abs(), requires_grad=True)\n+\n+        # ensure that sample, batch, event shapes all handled correctly\n+        self.assertEqual(LowRankMultivariateNormal(mean, scale_factor, scale_diag)\n+                         .sample().size(), (5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_no_batch, scale_factor, scale_diag)\n+                         .sample().size(), (3,))\n+        self.assertEqual(LowRankMultivariateNormal(mean_multi_batch, scale_factor, scale_diag)\n+                         .sample().size(), (6, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean, scale_factor, scale_diag)\n+                         .sample((2,)).size(), (2, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_no_batch, scale_factor, scale_diag)\n+                         .sample((2,)).size(), (2, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_multi_batch, scale_factor, scale_diag)\n+                         .sample((2,)).size(), (2, 6, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean, scale_factor, scale_diag)\n+                         .sample((2, 7)).size(), (2, 7, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_no_batch, scale_factor, scale_diag)\n+                         .sample((2, 7)).size(), (2, 7, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_multi_batch, scale_factor, scale_diag)\n+                         .sample((2, 7)).size(), (2, 7, 6, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean, scale_factor_batched, scale_diag_batched)\n+                         .sample((2, 7)).size(), (2, 7, 6, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_no_batch, scale_factor_batched, scale_diag_batched)\n+                         .sample((2, 7)).size(), (2, 7, 6, 5, 3))\n+        self.assertEqual(LowRankMultivariateNormal(mean_multi_batch, scale_factor_batched, scale_diag_batched)\n+                         .sample((2, 7)).size(), (2, 7, 6, 5, 3))\n+\n+        # check gradients\n+        self._gradcheck_log_prob(LowRankMultivariateNormal,\n+                                 (mean, scale_factor, scale_diag))\n+        self._gradcheck_log_prob(LowRankMultivariateNormal,\n+                                 (mean_multi_batch, scale_factor, scale_diag))\n+        self._gradcheck_log_prob(LowRankMultivariateNormal,\n+                                 (mean_multi_batch, scale_factor_batched, scale_diag_batched))\n+\n+    @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n+    def test_lowrank_multivariate_normal_log_prob(self):\n+        mean = torch.randn(3, requires_grad=True)\n+        scale_factor = torch.randn(3, 1, requires_grad=True)\n+        scale_diag = torch.tensor(torch.randn(3).abs(), requires_grad=True)\n+        cov = scale_factor.matmul(scale_factor.t()) + scale_diag.diag()\n+\n+        # check that logprob values match scipy logpdf,\n+        # and that covariance and scale_tril parameters are equivalent\n+        dist1 = LowRankMultivariateNormal(mean, scale_factor, scale_diag)\n+        ref_dist = scipy.stats.multivariate_normal(mean.detach().numpy(), cov.detach().numpy())\n+\n+        x = dist1.sample((10,))\n+        expected = ref_dist.logpdf(x.numpy())\n+        print(dist1.log_prob(x), MultivariateNormal(mean, cov).log_prob(x))", "path": "test/test_distributions.py", "position": null, "original_position": 94, "commit_id": "36d6044c64e07c795b8f2b8c9a88df25a407e025", "original_commit_id": "465e3f155b7522370272851c1579e6494d077c88", "user": {"login": "fehiepsi", "id": 4736342, "node_id": "MDQ6VXNlcjQ3MzYzNDI=", "avatar_url": "https://avatars1.githubusercontent.com/u/4736342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fehiepsi", "html_url": "https://github.com/fehiepsi", "followers_url": "https://api.github.com/users/fehiepsi/followers", "following_url": "https://api.github.com/users/fehiepsi/following{/other_user}", "gists_url": "https://api.github.com/users/fehiepsi/gists{/gist_id}", "starred_url": "https://api.github.com/users/fehiepsi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fehiepsi/subscriptions", "organizations_url": "https://api.github.com/users/fehiepsi/orgs", "repos_url": "https://api.github.com/users/fehiepsi/repos", "events_url": "https://api.github.com/users/fehiepsi/events{/privacy}", "received_events_url": "https://api.github.com/users/fehiepsi/received_events", "type": "User", "site_admin": false}, "body": "remove this print line", "created_at": "2018-07-04T13:04:25Z", "updated_at": "2018-11-23T15:46:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/8635#discussion_r200120180", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8635", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200120180"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8635#discussion_r200120180"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8635"}}, "body_html": "<p>remove this print line</p>", "body_text": "remove this print line"}