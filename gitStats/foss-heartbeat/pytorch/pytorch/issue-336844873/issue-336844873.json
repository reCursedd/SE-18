{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9015", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9015/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9015/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9015/events", "html_url": "https://github.com/pytorch/pytorch/issues/9015", "id": 336844873, "node_id": "MDU6SXNzdWUzMzY4NDQ4NzM=", "number": 9015, "title": "Using a FastText model inside a PyTorch DataLoader", "user": {"login": "jaxony", "id": 12774152, "node_id": "MDQ6VXNlcjEyNzc0MTUy", "avatar_url": "https://avatars3.githubusercontent.com/u/12774152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaxony", "html_url": "https://github.com/jaxony", "followers_url": "https://api.github.com/users/jaxony/followers", "following_url": "https://api.github.com/users/jaxony/following{/other_user}", "gists_url": "https://api.github.com/users/jaxony/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaxony/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaxony/subscriptions", "organizations_url": "https://api.github.com/users/jaxony/orgs", "repos_url": "https://api.github.com/users/jaxony/repos", "events_url": "https://api.github.com/users/jaxony/events{/privacy}", "received_events_url": "https://api.github.com/users/jaxony/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-29T02:51:50Z", "updated_at": "2018-07-09T20:05:52Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm loading images and generating sentence vectors on-the-fly inside a PyTorch DataLoader. I am using FastText embedding model loaded with FastText's Python binding i.e., <code>fastText.load_model(bin_file_here)</code>.</p>\n<p>When I pass into <code>torch.utils.data.DataLoader</code>:</p>\n<ul>\n<li><code>num_workers &gt; 0</code>  and <code>pin_memory=True</code>, <code>DataLoader.__iter__()</code> <strong>never returns</strong> and just hangs.</li>\n<li><code>num_workers &gt; 0</code> and <code>pin_memory=False</code>, <code>DataLoader.__iter__()</code> <strong>returns successfully</strong>.</li>\n</ul>\n<p>I'm curious about why the vectors created by FastText in a multiprocessing environment can't be pinned onto CUDA memory. Thanks!</p>", "body_text": "I'm loading images and generating sentence vectors on-the-fly inside a PyTorch DataLoader. I am using FastText embedding model loaded with FastText's Python binding i.e., fastText.load_model(bin_file_here).\nWhen I pass into torch.utils.data.DataLoader:\n\nnum_workers > 0  and pin_memory=True, DataLoader.__iter__() never returns and just hangs.\nnum_workers > 0 and pin_memory=False, DataLoader.__iter__() returns successfully.\n\nI'm curious about why the vectors created by FastText in a multiprocessing environment can't be pinned onto CUDA memory. Thanks!", "body": "I'm loading images and generating sentence vectors on-the-fly inside a PyTorch DataLoader. I am using FastText embedding model loaded with FastText's Python binding i.e., `fastText.load_model(bin_file_here)`.\r\n\r\nWhen I pass into `torch.utils.data.DataLoader`:\r\n* `num_workers > 0`  and `pin_memory=True`, `DataLoader.__iter__()` **never returns** and just hangs.\r\n* `num_workers > 0` and `pin_memory=False`, `DataLoader.__iter__()` **returns successfully**.\r\n\r\nI'm curious about why the vectors created by FastText in a multiprocessing environment can't be pinned onto CUDA memory. Thanks!"}