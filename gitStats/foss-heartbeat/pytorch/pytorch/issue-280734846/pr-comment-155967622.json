{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155967622", "pull_request_review_id": 82370461, "id": 155967622, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTk2NzYyMg==", "diff_hunk": "@@ -22,7 +22,8 @@ def recv(self):\n         return pickle.loads(buf)\n \n     def __getattr__(self, name):\n-        return getattr(self.conn, name)\n+        conn = object.__getattribute__(self, 'conn')", "path": "torch/multiprocessing/queue.py", "position": 5, "original_position": 5, "commit_id": "826bfb7f43a7d27bfbc5984b31d22950ce3dd6f8", "original_commit_id": "826bfb7f43a7d27bfbc5984b31d22950ce3dd6f8", "user": {"login": "Giszy", "id": 24215899, "node_id": "MDQ6VXNlcjI0MjE1ODk5", "avatar_url": "https://avatars1.githubusercontent.com/u/24215899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Giszy", "html_url": "https://github.com/Giszy", "followers_url": "https://api.github.com/users/Giszy/followers", "following_url": "https://api.github.com/users/Giszy/following{/other_user}", "gists_url": "https://api.github.com/users/Giszy/gists{/gist_id}", "starred_url": "https://api.github.com/users/Giszy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Giszy/subscriptions", "organizations_url": "https://api.github.com/users/Giszy/orgs", "repos_url": "https://api.github.com/users/Giszy/repos", "events_url": "https://api.github.com/users/Giszy/events{/privacy}", "received_events_url": "https://api.github.com/users/Giszy/received_events", "type": "User", "site_admin": false}, "body": "@apaszke\r\nYour code is better, but the origin code is failed.\r\n\r\ntestcode.py\r\n```python\r\nimport torch\r\nimport torch.utils.data as Data\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\nimport matplotlib.pyplot as plt\r\n\r\nif __name__ == '__main__':\r\n    torch.manual_seed(1)    # reproducible\r\n    \r\n    LR = 0.01\r\n    BATCH_SIZE = 32\r\n    EPOCH = 12\r\n    \r\n    # fake dataset\r\n    x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\r\n    y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\r\n    \r\n    # plot dataset\r\n    plt.scatter(x.numpy(), y.numpy())\r\n    plt.show()\r\n    \r\n    # put dateset into torch dataset\r\n    torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)\r\n    loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\r\n    \r\n    \r\n    # default network\r\n    class Net(torch.nn.Module):\r\n        def __init__(self):\r\n            super(Net, self).__init__()\r\n            self.hidden = torch.nn.Linear(1, 20)   # hidden layer\r\n            self.predict = torch.nn.Linear(20, 1)   # output layer\r\n    \r\n        def forward(self, x):\r\n            x = F.relu(self.hidden(x))      # activation function for hidden layer\r\n            x = self.predict(x)             # linear output\r\n            return x\r\n    \r\n    # different nets\r\n    net_SGD         = Net()\r\n    net_Momentum    = Net()\r\n    net_RMSprop     = Net()\r\n    net_Adam        = Net()\r\n    nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\r\n    \r\n    # different optimizers\r\n    opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\r\n    opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\r\n    opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\r\n    opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\r\n    optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\r\n    \r\n    loss_func = torch.nn.MSELoss()\r\n    losses_his = [[], [], [], []]   # record loss\r\n    \r\n    # training\r\n    for epoch in range(EPOCH):\r\n        print('Epoch: ', epoch)\r\n        for step, (batch_x, batch_y) in enumerate(loader):          # for each training step\r\n            b_x = Variable(batch_x)\r\n            b_y = Variable(batch_y)\r\n    \r\n            for net, opt, l_his in zip(nets, optimizers, losses_his):\r\n                output = net(b_x)              # get output for every net\r\n                loss = loss_func(output, b_y)  # compute loss for every net\r\n                opt.zero_grad()                # clear gradients for next train\r\n                loss.backward()                # backpropagation, compute gradients\r\n                opt.step()                     # apply gradients\r\n                l_his.append(loss.data[0])     # loss recoder\r\n    \r\n    labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']\r\n    for i, l_his in enumerate(losses_his):\r\n        plt.plot(l_his, label=labels[i])\r\n    plt.legend(loc='best')\r\n    plt.xlabel('Steps')\r\n    plt.ylabel('Loss')\r\n    plt.ylim((0, 0.2))\r\n    plt.show()\r\n```", "created_at": "2017-12-11T01:03:21Z", "updated_at": "2018-11-23T15:37:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/4099#discussion_r155967622", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4099", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155967622"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4099#discussion_r155967622"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4099"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a><br>\nYour code is better, but the origin code is failed.</p>\n<p>testcode.py</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.utils.data <span class=\"pl-k\">as</span> Data\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    torch.manual_seed(<span class=\"pl-c1\">1</span>)    <span class=\"pl-c\"><span class=\"pl-c\">#</span> reproducible</span>\n    \n    <span class=\"pl-c1\">LR</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.01</span>\n    <span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n    <span class=\"pl-c1\">EPOCH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">12</span>\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> fake dataset</span>\n    x <span class=\"pl-k\">=</span> torch.unsqueeze(torch.linspace(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1000</span>), <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    y <span class=\"pl-k\">=</span> x.pow(<span class=\"pl-c1\">2</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">0.1</span><span class=\"pl-k\">*</span>torch.normal(torch.zeros(<span class=\"pl-k\">*</span>x.size()))\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> plot dataset</span>\n    plt.scatter(x.numpy(), y.numpy())\n    plt.show()\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> put dateset into torch dataset</span>\n    torch_dataset <span class=\"pl-k\">=</span> Data.TensorDataset(<span class=\"pl-v\">data_tensor</span><span class=\"pl-k\">=</span>x, <span class=\"pl-v\">target_tensor</span><span class=\"pl-k\">=</span>y)\n    loader <span class=\"pl-k\">=</span> Data.DataLoader(<span class=\"pl-v\">dataset</span><span class=\"pl-k\">=</span>torch_dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>,)\n    \n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> default network</span>\n    <span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n            <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n            <span class=\"pl-c1\">self</span>.hidden <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden layer</span>\n            <span class=\"pl-c1\">self</span>.predict <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">1</span>)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> output layer</span>\n    \n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n            x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.hidden(x))      <span class=\"pl-c\"><span class=\"pl-c\">#</span> activation function for hidden layer</span>\n            x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.predict(x)             <span class=\"pl-c\"><span class=\"pl-c\">#</span> linear output</span>\n            <span class=\"pl-k\">return</span> x\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> different nets</span>\n    net_SGD         <span class=\"pl-k\">=</span> Net()\n    net_Momentum    <span class=\"pl-k\">=</span> Net()\n    net_RMSprop     <span class=\"pl-k\">=</span> Net()\n    net_Adam        <span class=\"pl-k\">=</span> Net()\n    nets <span class=\"pl-k\">=</span> [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> different optimizers</span>\n    opt_SGD         <span class=\"pl-k\">=</span> torch.optim.SGD(net_SGD.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LR</span>)\n    opt_Momentum    <span class=\"pl-k\">=</span> torch.optim.SGD(net_Momentum.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LR</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.8</span>)\n    opt_RMSprop     <span class=\"pl-k\">=</span> torch.optim.RMSprop(net_RMSprop.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LR</span>, <span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n    opt_Adam        <span class=\"pl-k\">=</span> torch.optim.Adam(net_Adam.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LR</span>, <span class=\"pl-v\">betas</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0.9</span>, <span class=\"pl-c1\">0.99</span>))\n    optimizers <span class=\"pl-k\">=</span> [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n    \n    loss_func <span class=\"pl-k\">=</span> torch.nn.MSELoss()\n    losses_his <span class=\"pl-k\">=</span> [[], [], [], []]   <span class=\"pl-c\"><span class=\"pl-c\">#</span> record loss</span>\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> training</span>\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">EPOCH</span>):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Epoch: <span class=\"pl-pds\">'</span></span>, epoch)\n        <span class=\"pl-k\">for</span> step, (batch_x, batch_y) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(loader):          <span class=\"pl-c\"><span class=\"pl-c\">#</span> for each training step</span>\n            b_x <span class=\"pl-k\">=</span> Variable(batch_x)\n            b_y <span class=\"pl-k\">=</span> Variable(batch_y)\n    \n            <span class=\"pl-k\">for</span> net, opt, l_his <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(nets, optimizers, losses_his):\n                output <span class=\"pl-k\">=</span> net(b_x)              <span class=\"pl-c\"><span class=\"pl-c\">#</span> get output for every net</span>\n                loss <span class=\"pl-k\">=</span> loss_func(output, b_y)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute loss for every net</span>\n                opt.zero_grad()                <span class=\"pl-c\"><span class=\"pl-c\">#</span> clear gradients for next train</span>\n                loss.backward()                <span class=\"pl-c\"><span class=\"pl-c\">#</span> backpropagation, compute gradients</span>\n                opt.step()                     <span class=\"pl-c\"><span class=\"pl-c\">#</span> apply gradients</span>\n                l_his.append(loss.data[<span class=\"pl-c1\">0</span>])     <span class=\"pl-c\"><span class=\"pl-c\">#</span> loss recoder</span>\n    \n    labels <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>SGD<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Momentum<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>RMSprop<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adam<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-k\">for</span> i, l_his <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(losses_his):\n        plt.plot(l_his, <span class=\"pl-v\">label</span><span class=\"pl-k\">=</span>labels[i])\n    plt.legend(<span class=\"pl-v\">loc</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>best<span class=\"pl-pds\">'</span></span>)\n    plt.xlabel(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Steps<span class=\"pl-pds\">'</span></span>)\n    plt.ylabel(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loss<span class=\"pl-pds\">'</span></span>)\n    plt.ylim((<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.2</span>))\n    plt.show()</pre></div>", "body_text": "@apaszke\nYour code is better, but the origin code is failed.\ntestcode.py\nimport torch\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\nif __name__ == '__main__':\n    torch.manual_seed(1)    # reproducible\n    \n    LR = 0.01\n    BATCH_SIZE = 32\n    EPOCH = 12\n    \n    # fake dataset\n    x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\n    y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n    \n    # plot dataset\n    plt.scatter(x.numpy(), y.numpy())\n    plt.show()\n    \n    # put dateset into torch dataset\n    torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)\n    loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n    \n    \n    # default network\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.hidden = torch.nn.Linear(1, 20)   # hidden layer\n            self.predict = torch.nn.Linear(20, 1)   # output layer\n    \n        def forward(self, x):\n            x = F.relu(self.hidden(x))      # activation function for hidden layer\n            x = self.predict(x)             # linear output\n            return x\n    \n    # different nets\n    net_SGD         = Net()\n    net_Momentum    = Net()\n    net_RMSprop     = Net()\n    net_Adam        = Net()\n    nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n    \n    # different optimizers\n    opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\n    opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\n    opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\n    opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\n    optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n    \n    loss_func = torch.nn.MSELoss()\n    losses_his = [[], [], [], []]   # record loss\n    \n    # training\n    for epoch in range(EPOCH):\n        print('Epoch: ', epoch)\n        for step, (batch_x, batch_y) in enumerate(loader):          # for each training step\n            b_x = Variable(batch_x)\n            b_y = Variable(batch_y)\n    \n            for net, opt, l_his in zip(nets, optimizers, losses_his):\n                output = net(b_x)              # get output for every net\n                loss = loss_func(output, b_y)  # compute loss for every net\n                opt.zero_grad()                # clear gradients for next train\n                loss.backward()                # backpropagation, compute gradients\n                opt.step()                     # apply gradients\n                l_his.append(loss.data[0])     # loss recoder\n    \n    labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']\n    for i, l_his in enumerate(losses_his):\n        plt.plot(l_his, label=labels[i])\n    plt.legend(loc='best')\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.ylim((0, 0.2))\n    plt.show()", "in_reply_to_id": 155939336}