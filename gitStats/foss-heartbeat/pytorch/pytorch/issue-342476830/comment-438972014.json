{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438972014", "html_url": "https://github.com/pytorch/pytorch/issues/9560#issuecomment-438972014", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9560", "id": 438972014, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODk3MjAxNA==", "user": {"login": "BelBES", "id": 3617413, "node_id": "MDQ6VXNlcjM2MTc0MTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3617413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BelBES", "html_url": "https://github.com/BelBES", "followers_url": "https://api.github.com/users/BelBES/followers", "following_url": "https://api.github.com/users/BelBES/following{/other_user}", "gists_url": "https://api.github.com/users/BelBES/gists{/gist_id}", "starred_url": "https://api.github.com/users/BelBES/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BelBES/subscriptions", "organizations_url": "https://api.github.com/users/BelBES/orgs", "repos_url": "https://api.github.com/users/BelBES/repos", "events_url": "https://api.github.com/users/BelBES/events{/privacy}", "received_events_url": "https://api.github.com/users/BelBES/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-15T09:18:17Z", "updated_at": "2018-11-15T09:18:17Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Considering epoch is always used referring to iterating over the training set once, it is incredibly surprising to see the LR schedulers use it as the training step number (batch counter).</p>\n</blockquote>\n<p>So, there are some cases when we need to schedule learning rate along iterations:</p>\n<ol>\n<li>\n<p>Dataset size is too large (or it's have infinite size) and plateau of loss function was achieved when a lot of steps is yet to come.</p>\n</li>\n<li>\n<p>We need a smooth learning rate policy (for example exponential or polynomial learning rate policy) along iterations.</p>\n</li>\n</ol>\n<p>So, also I think that there are tho independent essences:</p>\n<ol>\n<li>Abstract policy which can schedul any parameter step by step along iterations (for exapmple learning rate in optimizer, pruning rate in some pruning algorithm etc.)</li>\n<li>Learning rate scheduler which use Policy and use it for schedule of learning rate.</li>\n</ol>\n<p>Currently we can use any existen policies only for learning rate.</p>", "body_text": "Considering epoch is always used referring to iterating over the training set once, it is incredibly surprising to see the LR schedulers use it as the training step number (batch counter).\n\nSo, there are some cases when we need to schedule learning rate along iterations:\n\n\nDataset size is too large (or it's have infinite size) and plateau of loss function was achieved when a lot of steps is yet to come.\n\n\nWe need a smooth learning rate policy (for example exponential or polynomial learning rate policy) along iterations.\n\n\nSo, also I think that there are tho independent essences:\n\nAbstract policy which can schedul any parameter step by step along iterations (for exapmple learning rate in optimizer, pruning rate in some pruning algorithm etc.)\nLearning rate scheduler which use Policy and use it for schedule of learning rate.\n\nCurrently we can use any existen policies only for learning rate.", "body": "> Considering epoch is always used referring to iterating over the training set once, it is incredibly surprising to see the LR schedulers use it as the training step number (batch counter).\r\n\r\nSo, there are some cases when we need to schedule learning rate along iterations:\r\n1) Dataset size is too large (or it's have infinite size) and plateau of loss function was achieved when a lot of steps is yet to come.\r\n\r\n2) We need a smooth learning rate policy (for example exponential or polynomial learning rate policy) along iterations.\r\n\r\nSo, also I think that there are tho independent essences:\r\n1) Abstract policy which can schedul any parameter step by step along iterations (for exapmple learning rate in optimizer, pruning rate in some pruning algorithm etc.)\r\n2) Learning rate scheduler which use Policy and use it for schedule of learning rate.\r\n\r\nCurrently we can use any existen policies only for learning rate."}