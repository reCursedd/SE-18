{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1875", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1875/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1875/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1875/events", "html_url": "https://github.com/pytorch/pytorch/issues/1875", "id": 237835089, "node_id": "MDU6SXNzdWUyMzc4MzUwODk=", "number": 1875, "title": "[Feature request] A few things that numpy does that pytorch could have", "user": {"login": "martinarjovsky", "id": 5272722, "node_id": "MDQ6VXNlcjUyNzI3MjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5272722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinarjovsky", "html_url": "https://github.com/martinarjovsky", "followers_url": "https://api.github.com/users/martinarjovsky/followers", "following_url": "https://api.github.com/users/martinarjovsky/following{/other_user}", "gists_url": "https://api.github.com/users/martinarjovsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinarjovsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinarjovsky/subscriptions", "organizations_url": "https://api.github.com/users/martinarjovsky/orgs", "repos_url": "https://api.github.com/users/martinarjovsky/repos", "events_url": "https://api.github.com/users/martinarjovsky/events{/privacy}", "received_events_url": "https://api.github.com/users/martinarjovsky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 424131850, "node_id": "MDU6TGFiZWw0MjQxMzE4NTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-06-22T12:50:27Z", "updated_at": "2018-11-14T10:12:39Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Hi! Small list of simple features that I used on a day to day basis both in numpy and theano and would be helpful to have now. They're all super dumb, but writing hack-arounds or debugging a few extra things is sometimes annoying, and they should be super easy to add.</p>\n<ul>\n<li>Add .mean() for ByteTensors so we can calculate accs easily.</li>\n</ul>\n<pre><code>x = torch.Tensor(3).normal_()\n(x &gt; 0).mean() # Throws error\n\ny = np.random.randn(3)\n(y &gt; 0).mean() # Works ok\n</code></pre>\n<ul>\n<li>It's weird that .random_() defaults to garbage and not U[0,1] or something like that. Seems very error prone (I myself had to debug this ~ 10 times). Maybe a warning at least?</li>\n</ul>\n<pre><code>x = torch.Tensor(2,3)\nx.random_() # Returns random ints from 0 to max_int?\n\ny = np.random.random((2,3)) # Returns U[0,1] on each entry\n</code></pre>\n<ul>\n<li>Having something like ones_like and zeros_like would be ideal! I used to use this all the time for masks and other things. Also the hack arounds are very prone to do a bad use of memory, as opposed to the expand_as [esp when the programmer is bad such as myself :) ].</li>\n</ul>\n<pre><code>y = np.random.rand(3)\nz = np.ones_like(y)\n</code></pre>\n<ul>\n<li>\n<p>It's very annoying that torch.arange returns a FloatTensor and index_select requires a LongTensor. I guess when advanced indexing is in this won't matter :)</p>\n</li>\n<li>\n<p>Edit: Adding that np.arange works just by specifying the end (as range) and torch.arange doesn't. Also the numpy version has dtypo int64 while the torch one is a FloatTensor, though similar things like torch.randperm give LongTensor (this might be useful, idk).</p>\n</li>\n</ul>\n<pre><code>y = np.arange(10) # [0, ..., 9]\nx = torch.arange(10) # Hell unleashes\n</code></pre>\n<p>Cheers!</p>", "body_text": "Hi! Small list of simple features that I used on a day to day basis both in numpy and theano and would be helpful to have now. They're all super dumb, but writing hack-arounds or debugging a few extra things is sometimes annoying, and they should be super easy to add.\n\nAdd .mean() for ByteTensors so we can calculate accs easily.\n\nx = torch.Tensor(3).normal_()\n(x > 0).mean() # Throws error\n\ny = np.random.randn(3)\n(y > 0).mean() # Works ok\n\n\nIt's weird that .random_() defaults to garbage and not U[0,1] or something like that. Seems very error prone (I myself had to debug this ~ 10 times). Maybe a warning at least?\n\nx = torch.Tensor(2,3)\nx.random_() # Returns random ints from 0 to max_int?\n\ny = np.random.random((2,3)) # Returns U[0,1] on each entry\n\n\nHaving something like ones_like and zeros_like would be ideal! I used to use this all the time for masks and other things. Also the hack arounds are very prone to do a bad use of memory, as opposed to the expand_as [esp when the programmer is bad such as myself :) ].\n\ny = np.random.rand(3)\nz = np.ones_like(y)\n\n\n\nIt's very annoying that torch.arange returns a FloatTensor and index_select requires a LongTensor. I guess when advanced indexing is in this won't matter :)\n\n\nEdit: Adding that np.arange works just by specifying the end (as range) and torch.arange doesn't. Also the numpy version has dtypo int64 while the torch one is a FloatTensor, though similar things like torch.randperm give LongTensor (this might be useful, idk).\n\n\ny = np.arange(10) # [0, ..., 9]\nx = torch.arange(10) # Hell unleashes\n\nCheers!", "body": "Hi! Small list of simple features that I used on a day to day basis both in numpy and theano and would be helpful to have now. They're all super dumb, but writing hack-arounds or debugging a few extra things is sometimes annoying, and they should be super easy to add.\r\n\r\n- Add .mean() for ByteTensors so we can calculate accs easily.\r\n```\r\nx = torch.Tensor(3).normal_()\r\n(x > 0).mean() # Throws error\r\n\r\ny = np.random.randn(3)\r\n(y > 0).mean() # Works ok\r\n```\r\n\r\n- It's weird that .random_() defaults to garbage and not U[0,1] or something like that. Seems very error prone (I myself had to debug this ~ 10 times). Maybe a warning at least? \r\n```\r\nx = torch.Tensor(2,3)\r\nx.random_() # Returns random ints from 0 to max_int?\r\n\r\ny = np.random.random((2,3)) # Returns U[0,1] on each entry\r\n```\r\n\r\n- Having something like ones_like and zeros_like would be ideal! I used to use this all the time for masks and other things. Also the hack arounds are very prone to do a bad use of memory, as opposed to the expand_as [esp when the programmer is bad such as myself :) ].\r\n```\r\ny = np.random.rand(3)\r\nz = np.ones_like(y)\r\n```\r\n\r\n- It's very annoying that torch.arange returns a FloatTensor and index_select requires a LongTensor. I guess when advanced indexing is in this won't matter :)\r\n\r\n- Edit: Adding that np.arange works just by specifying the end (as range) and torch.arange doesn't. Also the numpy version has dtypo int64 while the torch one is a FloatTensor, though similar things like torch.randperm give LongTensor (this might be useful, idk).\r\n```\r\ny = np.arange(10) # [0, ..., 9]\r\nx = torch.arange(10) # Hell unleashes\r\n```\r\n\r\nCheers!"}