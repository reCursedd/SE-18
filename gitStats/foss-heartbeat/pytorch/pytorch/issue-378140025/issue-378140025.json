{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13658", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13658/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13658/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13658/events", "html_url": "https://github.com/pytorch/pytorch/issues/13658", "id": 378140025, "node_id": "MDU6SXNzdWUzNzgxNDAwMjU=", "number": 13658, "title": "[Pytorch]Speeding up inference?", "user": {"login": "zwx8981", "id": 14050646, "node_id": "MDQ6VXNlcjE0MDUwNjQ2", "avatar_url": "https://avatars2.githubusercontent.com/u/14050646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zwx8981", "html_url": "https://github.com/zwx8981", "followers_url": "https://api.github.com/users/zwx8981/followers", "following_url": "https://api.github.com/users/zwx8981/following{/other_user}", "gists_url": "https://api.github.com/users/zwx8981/gists{/gist_id}", "starred_url": "https://api.github.com/users/zwx8981/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zwx8981/subscriptions", "organizations_url": "https://api.github.com/users/zwx8981/orgs", "repos_url": "https://api.github.com/users/zwx8981/repos", "events_url": "https://api.github.com/users/zwx8981/events{/privacy}", "received_events_url": "https://api.github.com/users/zwx8981/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-07T04:37:50Z", "updated_at": "2018-11-07T05:08:27Z", "closed_at": "2018-11-07T05:08:27Z", "author_association": "NONE", "body_html": "<p>Hi, as we know, we only need to calculate forward in inference process, hence it is expected that the computation complexity in inference should be much lower than that in training. However, it seems that in Pytorch there is no obvious difference between training and inference in terms of computation complexity (running time). I have already executed the inference under the context manager 'with torch.no_grad()\uff1a'. Is there any possible way to speed-up inference in Pytorch?</p>", "body_text": "Hi, as we know, we only need to calculate forward in inference process, hence it is expected that the computation complexity in inference should be much lower than that in training. However, it seems that in Pytorch there is no obvious difference between training and inference in terms of computation complexity (running time). I have already executed the inference under the context manager 'with torch.no_grad()\uff1a'. Is there any possible way to speed-up inference in Pytorch?", "body": "Hi, as we know, we only need to calculate forward in inference process, hence it is expected that the computation complexity in inference should be much lower than that in training. However, it seems that in Pytorch there is no obvious difference between training and inference in terms of computation complexity (running time). I have already executed the inference under the context manager 'with torch.no_grad()\uff1a'. Is there any possible way to speed-up inference in Pytorch?\r\n"}