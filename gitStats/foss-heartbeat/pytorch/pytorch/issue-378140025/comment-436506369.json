{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436506369", "html_url": "https://github.com/pytorch/pytorch/issues/13658#issuecomment-436506369", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13658", "id": 436506369, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjUwNjM2OQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T05:07:23Z", "updated_at": "2018-11-07T05:07:23Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>hence it is expected that the computation complexity in inference should be much lower than that in training.</p>\n</blockquote>\n<p>This is not true. Things like <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"376548152\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13470\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13470/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13470\">#13470</a> will allow this in some functions, but generally, for most ops, the values needed to compute backward are also naturally computed in forward. So generally, needing to do backprop or not doesn't affect speed, but memory usage.</p>", "body_text": "hence it is expected that the computation complexity in inference should be much lower than that in training.\n\nThis is not true. Things like #13470 will allow this in some functions, but generally, for most ops, the values needed to compute backward are also naturally computed in forward. So generally, needing to do backprop or not doesn't affect speed, but memory usage.", "body": "> hence it is expected that the computation complexity in inference should be much lower than that in training.\r\n\r\nThis is not true. Things like #13470 will allow this in some functions, but generally, for most ops, the values needed to compute backward are also naturally computed in forward. So generally, needing to do backprop or not doesn't affect speed, but memory usage."}