{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179961829", "pull_request_review_id": 110300181, "id": 179961829, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTk2MTgyOQ==", "diff_hunk": "@@ -0,0 +1,117 @@\n+import torch\n+\n+\n+def detach_variable(inputs):\n+    if isinstance(inputs, tuple):\n+        out = []\n+        for inp in inputs:\n+            x = inp.detach()\n+            x.requires_grad = inp.requires_grad\n+            out.append(x)\n+        return tuple(out)\n+    else:\n+        raise RuntimeError(\n+            \"Only tuple of tensors is supported. Got Unsupported input type: \", type(inputs).__name__)\n+\n+\n+class CheckpointFunction(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, run_function, *args):\n+        ctx.run_function = run_function\n+        ctx.save_for_backward(*args)\n+        with torch.no_grad():\n+            outputs = run_function(*args)\n+        return outputs\n+\n+    @staticmethod\n+    def backward(ctx, *args):\n+        if not torch.autograd.is_checkpoint_valid():\n+            raise RuntimeError(\"Checkpointing is not compatible with .grad(), please use .backward() if possible\")\n+        inputs = ctx.saved_tensors\n+        inputs_list = detach_variable(inputs)\n+        with torch.enable_grad():\n+            outputs = ctx.run_function(*inputs_list)\n+\n+        if isinstance(outputs, torch.Tensor):\n+            outputs = (outputs,)\n+        torch.autograd.backward(outputs, args)\n+\n+        input_grads = None\n+        if isinstance(inputs_list, tuple):\n+            input_grads = tuple(inp.grad for inp in inputs_list)\n+            return (None,) + input_grads\n+\n+\n+def checkpoint(run_function, *args):\n+    r\"\"\"Checkpoint a model or part of the model\n+\n+    Checkpoint works by trading compute for memory. It can be applied on any\n+    part of the model. In the forward pass, the model activations are not\n+    stored. The forward pass save the inputs tuple and the run_function\n+    parameter. In the backwards pass, the saved inputs and run_function is\n+    retreived, and the forward pass is done on the model again (non-volatile\n+    this time) since we need to get the activations values for calculating the\n+    gradient and then the gradients are calculated.\n+\n+    .. warning::\n+\n+        checkpointing doesn't work with torch.autograd.grad(), but only with\n+        torch.autograd.backward()\n+\n+    Args:\n+        run_function: describes what to run in the forward pass of the model or\n+            part of the model. It should also know how to handle the inputs\n+            passed as the tuple. For example, in LSTM, user passes (activation,\n+            hidden), run_function should correctly use first input as activation\n+            and second input as hidden\n+        args: tuple containing inputs to the run_function\n+\n+    Returns:\n+        Output of running the run_function on *args\n+    \"\"\"\n+    return CheckpointFunction.apply(run_function, *args)\n+\n+\n+def checkpoint_sequential(functions, segments, *inputs):\n+    r\"\"\"A helper function for checkpointing Sequential based models.\n+\n+    For models that are constructed using Sequential, they normally are built\n+    using various modules/functions. For such models, given a list of modules/functions\n+    it executes in order (sequentially), we can divide the model in various\n+    segments and checkpoint the segments. All segments except the last will be\n+    run in volatile manner i.e. the model activations are not stored. The inputs\n+    of each checkpointed segment will be saved for re-running the segment in the\n+    backward pass.\n+\n+    Args:\n+        functions: The list of modules or functions (comprising the model) to run in order.\n+        segments: Number of chunks to create in the model\n+        inputs: tuple containing the inputs to run_function\n+\n+    Returns:\n+        Output of running the modules/functions on *inputs\n+\n+    Example:\n+        >>> if isinstance(model, nn.Sequential):\n+        >>>     functions = list(model.children())", "path": "torch/utils/checkpoint.py", "position": null, "original_position": 97, "commit_id": "37ee79fe45bdcfd2fb70eccc146369d6c765137e", "original_commit_id": "6b37243f2c25d6f6f6bf633c3fa0862dff2c843b", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I didn't mean to put it in the example, I wanted this to be part of the code. There's no reason to not extend the API to sequential, and force people to write this out every time, since that's probably the most common use case for this function.", "created_at": "2018-04-08T21:22:41Z", "updated_at": "2018-11-23T15:42:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/4594#discussion_r179961829", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4594", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179961829"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4594#discussion_r179961829"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4594"}}, "body_html": "<p>I didn't mean to put it in the example, I wanted this to be part of the code. There's no reason to not extend the API to sequential, and force people to write this out every time, since that's probably the most common use case for this function.</p>", "body_text": "I didn't mean to put it in the example, I wanted this to be part of the code. There's no reason to not extend the API to sequential, and force people to write this out every time, since that's probably the most common use case for this function."}