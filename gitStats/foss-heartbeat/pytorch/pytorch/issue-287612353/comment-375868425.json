{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/375868425", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-375868425", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 375868425, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTg2ODQyNQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-24T10:35:23Z", "updated_at": "2018-03-24T10:35:23Z", "author_association": "MEMBER", "body_html": "<p>Actually I think there's one extra constraint we haven't taken into account so far, and our previous solution is incorrect and inefficient:</p>\n<ol>\n<li>We do want to have this leaky behavior (gradients going towards weights used inside this function) even when we have <code>torch.autograd.grad</code> enabled. If we just use <code>.grad</code> ourselves, then we will only send the gradients w.r.t. inputs, but the ones w.r.t. weights won't appear in the outer backward, as if the weights weren't used.</li>\n<li>If we let <code>torch.autograd.backward</code> follow the entire path to the weights (possibly from some non-leaf variables that were used inside the checkpointed function), then we can actually end up differentiating parts of the graph multiple times, giving us a O(n^2) runtime (instead of O(n) we usually have). This is because the gradients from this checkpointed function wouldn't get accumulated with those coming from other uses during differentiation. They would still both get accumulated into <code>.grad</code>s later, so it is valid, but it can be much slower.</li>\n</ol>\n<p>Thus I don't think that knowing what kind of backward we're in is sufficient.</p>\n<p>I think that if we want to solve this problem correctly, we need a list of all variables that the function is closed over. They should be added as inputs to the autograd <code>Function</code>, and its <code>backward</code> should just <em>always use <code>.grad</code></em>, and return the gradients for all variables that were <em>possibly</em> used. This would work with both modes without any need for hacking:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">CheckpointedFunction</span>(<span class=\"pl-e\">Function</span>)\n    @<span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-e\">ctx</span>, <span class=\"pl-e\">fn</span>, <span class=\"pl-e\">num_real_args</span>, <span class=\"pl-k\">*</span><span class=\"pl-e\">args</span>): <span class=\"pl-c\"><span class=\"pl-c\">#</span> args include real args and those that fn closes over</span>\n        <span class=\"pl-k\">with</span> torch.no_grad():\n            <span class=\"pl-k\">return</span> fn(<span class=\"pl-k\">*</span>args[:num_real_args])\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">grad_outputs</span>)\n        outputs <span class=\"pl-k\">=</span> fn(<span class=\"pl-k\">*</span>args[:num_real_args])\n        <span class=\"pl-k\">return</span> torch.autograd.grad(outputs, args, grad_outputs) <span class=\"pl-c\"><span class=\"pl-c\">#</span> as simple as that!</span></pre></div>\n<p>The only remaining question is how do we obtain a list of used variables. The simplest solution, but also one that makes for a pretty ugly API (and that makes it easy to make mistakes), is to ask people for a list (or perhaps use <code>.parameters()</code> if the callable is a module). The problem with this is that if someone forgets to add a variable, then it will appear as if it was unused in the forward computation. Not great.</p>\n<p>The alternative would be to do some kind of auto-detection, which would be more reliable, but I'm not sure if we have existing mechanisms that would let us do this (tracing perhaps? it might be too slow <g-emoji class=\"g-emoji\" alias=\"confused\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f615.png\">\ud83d\ude15</g-emoji>).</p>", "body_text": "Actually I think there's one extra constraint we haven't taken into account so far, and our previous solution is incorrect and inefficient:\n\nWe do want to have this leaky behavior (gradients going towards weights used inside this function) even when we have torch.autograd.grad enabled. If we just use .grad ourselves, then we will only send the gradients w.r.t. inputs, but the ones w.r.t. weights won't appear in the outer backward, as if the weights weren't used.\nIf we let torch.autograd.backward follow the entire path to the weights (possibly from some non-leaf variables that were used inside the checkpointed function), then we can actually end up differentiating parts of the graph multiple times, giving us a O(n^2) runtime (instead of O(n) we usually have). This is because the gradients from this checkpointed function wouldn't get accumulated with those coming from other uses during differentiation. They would still both get accumulated into .grads later, so it is valid, but it can be much slower.\n\nThus I don't think that knowing what kind of backward we're in is sufficient.\nI think that if we want to solve this problem correctly, we need a list of all variables that the function is closed over. They should be added as inputs to the autograd Function, and its backward should just always use .grad, and return the gradients for all variables that were possibly used. This would work with both modes without any need for hacking:\nclass CheckpointedFunction(Function)\n    @staticmethod\n    def forward(ctx, fn, num_real_args, *args): # args include real args and those that fn closes over\n        with torch.no_grad():\n            return fn(*args[:num_real_args])\n\n    @staticmethod\n    def backward(ctx, *grad_outputs)\n        outputs = fn(*args[:num_real_args])\n        return torch.autograd.grad(outputs, args, grad_outputs) # as simple as that!\nThe only remaining question is how do we obtain a list of used variables. The simplest solution, but also one that makes for a pretty ugly API (and that makes it easy to make mistakes), is to ask people for a list (or perhaps use .parameters() if the callable is a module). The problem with this is that if someone forgets to add a variable, then it will appear as if it was unused in the forward computation. Not great.\nThe alternative would be to do some kind of auto-detection, which would be more reliable, but I'm not sure if we have existing mechanisms that would let us do this (tracing perhaps? it might be too slow \ud83d\ude15).", "body": "Actually I think there's one extra constraint we haven't taken into account so far, and our previous solution is incorrect and inefficient:\r\n1. We do want to have this leaky behavior (gradients going towards weights used inside this function) even when we have `torch.autograd.grad` enabled. If we just use `.grad` ourselves, then we will only send the gradients w.r.t. inputs, but the ones w.r.t. weights won't appear in the outer backward, as if the weights weren't used.\r\n2. If we let `torch.autograd.backward` follow the entire path to the weights (possibly from some non-leaf variables that were used inside the checkpointed function), then we can actually end up differentiating parts of the graph multiple times, giving us a O(n^2) runtime (instead of O(n) we usually have). This is because the gradients from this checkpointed function wouldn't get accumulated with those coming from other uses during differentiation. They would still both get accumulated into `.grad`s later, so it is valid, but it can be much slower.\r\n\r\nThus I don't think that knowing what kind of backward we're in is sufficient.\r\n\r\nI think that if we want to solve this problem correctly, we need a list of all variables that the function is closed over. They should be added as inputs to the autograd `Function`, and its `backward` should just *always use `.grad`*, and return the gradients for all variables that were *possibly* used. This would work with both modes without any need for hacking:\r\n\r\n```python\r\nclass CheckpointedFunction(Function)\r\n    @staticmethod\r\n    def forward(ctx, fn, num_real_args, *args): # args include real args and those that fn closes over\r\n        with torch.no_grad():\r\n            return fn(*args[:num_real_args])\r\n\r\n    @staticmethod\r\n    def backward(ctx, *grad_outputs)\r\n        outputs = fn(*args[:num_real_args])\r\n        return torch.autograd.grad(outputs, args, grad_outputs) # as simple as that!\r\n```\r\n\r\nThe only remaining question is how do we obtain a list of used variables. The simplest solution, but also one that makes for a pretty ugly API (and that makes it easy to make mistakes), is to ask people for a list (or perhaps use `.parameters()` if the callable is a module). The problem with this is that if someone forgets to add a variable, then it will appear as if it was unused in the forward computation. Not great.\r\n\r\nThe alternative would be to do some kind of auto-detection, which would be more reliable, but I'm not sure if we have existing mechanisms that would let us do this (tracing perhaps? it might be too slow :confused:)."}