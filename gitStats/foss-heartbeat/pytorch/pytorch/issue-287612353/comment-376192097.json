{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/376192097", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-376192097", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 376192097, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjE5MjA5Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-26T14:47:06Z", "updated_at": "2018-03-26T14:47:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Tracking free variables is the morally right thing to do, but the tracking should be done automatically (no users manually stating free variables); otherwise, \"Checkpointing is only valid with mutable gradients\" is a much easier precondition to achieve and test for. <code>backward()</code> in <code>backward()</code> is a little too clever for its own good, but when there aren't any free non-leaf variables it does exactly the same work the morally correct implementation would have done, and you can manually optimize it by manifesting the free variables yourself.</p>\n<p>Free variable tracking is similar to tracing, but I suspect we'd want to give it its own codepath. But it's a pretty simple analysis to run (maintain a map of <code>Variable</code> to <code>bool</code>; every explicitly passed input is marked <code>true</code>, any variables we read which are not in the map are marked <code>false</code>, every freshly generated variable inside is marked <code>true</code>.) You'd also want to adjust checkpointing function to run the forwards <em>outside</em> of the checkpoint (so that you can invoke the autograd function itself with the free variable list, so dependencies get setup correctly.) I am not sure it pays its weight; are there other places where something like this would be helpful?</p>\n<blockquote>\n<p>I guess a good enough approximation for now would be to have a counter which says how many .grads in are you currently, and raise an error in the checkpointed function if it's &gt; 0.</p>\n</blockquote>\n<p>No, this doesn't work, reentrant calls to grad don't follow stack discipline since we have multithreaded workers.</p>\n<blockquote>\n<p>Thus I don't think that knowing what kind of backward we're in is sufficient.</p>\n</blockquote>\n<p>The point was never to get correct behavior for torch.autograd.grad; it was simply to detect when unsound behavior would have occurred and bailed out.</p>", "body_text": "Tracking free variables is the morally right thing to do, but the tracking should be done automatically (no users manually stating free variables); otherwise, \"Checkpointing is only valid with mutable gradients\" is a much easier precondition to achieve and test for. backward() in backward() is a little too clever for its own good, but when there aren't any free non-leaf variables it does exactly the same work the morally correct implementation would have done, and you can manually optimize it by manifesting the free variables yourself.\nFree variable tracking is similar to tracing, but I suspect we'd want to give it its own codepath. But it's a pretty simple analysis to run (maintain a map of Variable to bool; every explicitly passed input is marked true, any variables we read which are not in the map are marked false, every freshly generated variable inside is marked true.) You'd also want to adjust checkpointing function to run the forwards outside of the checkpoint (so that you can invoke the autograd function itself with the free variable list, so dependencies get setup correctly.) I am not sure it pays its weight; are there other places where something like this would be helpful?\n\nI guess a good enough approximation for now would be to have a counter which says how many .grads in are you currently, and raise an error in the checkpointed function if it's > 0.\n\nNo, this doesn't work, reentrant calls to grad don't follow stack discipline since we have multithreaded workers.\n\nThus I don't think that knowing what kind of backward we're in is sufficient.\n\nThe point was never to get correct behavior for torch.autograd.grad; it was simply to detect when unsound behavior would have occurred and bailed out.", "body": "Tracking free variables is the morally right thing to do, but the tracking should be done automatically (no users manually stating free variables); otherwise, \"Checkpointing is only valid with mutable gradients\" is a much easier precondition to achieve and test for. `backward()` in `backward()` is a little too clever for its own good, but when there aren't any free non-leaf variables it does exactly the same work the morally correct implementation would have done, and you can manually optimize it by manifesting the free variables yourself.\r\n\r\nFree variable tracking is similar to tracing, but I suspect we'd want to give it its own codepath. But it's a pretty simple analysis to run (maintain a map of `Variable` to `bool`; every explicitly passed input is marked `true`, any variables we read which are not in the map are marked `false`, every freshly generated variable inside is marked `true`.) You'd also want to adjust checkpointing function to run the forwards *outside* of the checkpoint (so that you can invoke the autograd function itself with the free variable list, so dependencies get setup correctly.) I am not sure it pays its weight; are there other places where something like this would be helpful?\r\n\r\n> I guess a good enough approximation for now would be to have a counter which says how many .grads in are you currently, and raise an error in the checkpointed function if it's > 0.\r\n\r\nNo, this doesn't work, reentrant calls to grad don't follow stack discipline since we have multithreaded workers.\r\n\r\n> Thus I don't think that knowing what kind of backward we're in is sufficient.\r\n\r\nThe point was never to get correct behavior for torch.autograd.grad; it was simply to detect when unsound behavior would have occurred and bailed out."}