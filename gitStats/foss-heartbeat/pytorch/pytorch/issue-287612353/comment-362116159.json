{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362116159", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-362116159", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 362116159, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjExNjE1OQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T00:22:00Z", "updated_at": "2018-02-01T00:22:00Z", "author_association": "MEMBER", "body_html": "<p>We should never use <code>backward</code> for this purpose, because this will have side effects visible outside checkpointed function (will basically backpropagate through the whole graph that was created before checkpointed function ran).</p>\n<p>The weights should be updated, but it shouldn't happen because of the <code>backward</code> call inside <code>CheckpointedFunction.backward</code>. That method should return the gradients it computed, and only then autograd should assign them as <code>.grad</code> attributes of the weights.</p>", "body_text": "We should never use backward for this purpose, because this will have side effects visible outside checkpointed function (will basically backpropagate through the whole graph that was created before checkpointed function ran).\nThe weights should be updated, but it shouldn't happen because of the backward call inside CheckpointedFunction.backward. That method should return the gradients it computed, and only then autograd should assign them as .grad attributes of the weights.", "body": "We should never use `backward` for this purpose, because this will have side effects visible outside checkpointed function (will basically backpropagate through the whole graph that was created before checkpointed function ran).\r\n\r\nThe weights should be updated, but it shouldn't happen because of the `backward` call inside `CheckpointedFunction.backward`. That method should return the gradients it computed, and only then autograd should assign them as `.grad` attributes of the weights."}