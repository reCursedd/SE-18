{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362149219", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-362149219", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 362149219, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjE0OTIxOQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T03:41:33Z", "updated_at": "2018-02-01T03:41:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There are a few things to point out.</p>\n<p><strong>If imperative backward() is used, checkpointing should use imperative backward().</strong> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, you stated:</p>\n<blockquote>\n<p>We should never use backward for this purpose, because this will have side effects visible outside checkpointed function (will basically backpropagate through the whole graph that was created before checkpointed function ran).</p>\n</blockquote>\n<p>But, on it's face, this statement is not true, because in Priya's original, backward-using variant, she repackages the inputs into leaf variables precisely to avoid this situation.</p>\n<pre><code>        inputs = repackage_inputs(saved_inputs)\n        with torch.enable_grad():\n            outputs = ctx.run_function(*inputs)\n</code></pre>\n<p>Under what circumstances could backpropagation go beyond these inputs? It must be any Variable used by <code>ctx.run_function</code> which was not in the <code>inputs</code> list. Prima facie, these are the weight leaf Variables bundled in the modules (as specified by <code>module.parameters()</code>); but if these are leaf variables, then it doesn't matter: we <em>wanted</em> these <code>grad</code> updates.  I do admit that an equivalent effect could be achieved by running <code>torch.autograd.grad</code> with all <code>module.parameters()</code> as inputs, and then post-facto perform the update.</p>\n<p>On a second look, the other situation could occur if a module captures a Variable with history that is unrelated. Let's make this concrete with an example module that does this.</p>\n<pre><code>class Escape(Module):\n  def __init__(self, var):\n    self.var = var  # not a parameter\n  def forward(self, input):\n    return input + var\n</code></pre>\n<p>If we run the following:</p>\n<pre><code># let e be an Escape module\ninput = Variable(..., requires_grad=True)\noutput = e(input)\noutput.backward()\n</code></pre>\n<p>Then it is indeed true that backwards propagation will proceed into the history of <code>e.var</code>. The key is that this is true <em>with or without</em> checkpointing. That is to say, even if we replace this with a checkpointed e:</p>\n<pre><code>input = Variable(..., requires_grad=True)\noutput = checkpoint(e, input)\noutput.backward()\n</code></pre>\n<p>The intended semantics is that this should behave observationally equivalently to the original code, which means we <em>must</em> back-propagate through <code>e.var</code>. Adam, in your suggested rewrite with <code>torch.autograd.grad</code>, I cannot see how it is possible to make this propagation happen, because there is no way for checkpoint to know what the leafs of <code>e.var</code>'s  history are. <code>torch.grad.backward</code> is the correct tool for the job.</p>\n<p><strong>Trouble in paradise.</strong> Now, the discussion above was predicated on the use of the imperative <code>backward</code> interface, and this discussion got me thinking about what would happen if instead I had used <code>torch.autograd.grad</code> instead. Now we are in trouble: a checkpointing implementation that uses <code>backward</code> internally will mutate <code>leaf.grad</code>, where the non-checkpointing version would not have mutated <code>grad</code>. In this case, a <code>torch.autograd.grad</code> based implementation would have done better, because at least it wouldn't have mutated <code>grad</code>, but you will <em>still</em> have the of ferrying the gradients you computed from the reentrant <code>torch.autograd.grad</code> back to the outputs of the outer invocation of <code>torch.autograd.grad</code>. Ick.</p>\n<p>I think there is literally nothing we can do with this implementation strategy to handle this case, as the interfaces stand today. My best ideas are to either (1) add extra APIs to backpropagate into an arbitrary variable (but not as a reentrant backpropagation; as a <em>dynamically</em> added new gradient into the parent backpropagation graph) or (2) rework the entire implementation of checkpointing so it doesn't do reentrant backward execution (I don't actually know how to do this.)</p>\n<p><strong>Conclusion.</strong> Given that the implementation strategy Priya has implemented only actually works for imperative backward(), it seems to me that the simple AND correct implementation is to use an imperative backward() internally.</p>", "body_text": "There are a few things to point out.\nIf imperative backward() is used, checkpointing should use imperative backward(). @apaszke, you stated:\n\nWe should never use backward for this purpose, because this will have side effects visible outside checkpointed function (will basically backpropagate through the whole graph that was created before checkpointed function ran).\n\nBut, on it's face, this statement is not true, because in Priya's original, backward-using variant, she repackages the inputs into leaf variables precisely to avoid this situation.\n        inputs = repackage_inputs(saved_inputs)\n        with torch.enable_grad():\n            outputs = ctx.run_function(*inputs)\n\nUnder what circumstances could backpropagation go beyond these inputs? It must be any Variable used by ctx.run_function which was not in the inputs list. Prima facie, these are the weight leaf Variables bundled in the modules (as specified by module.parameters()); but if these are leaf variables, then it doesn't matter: we wanted these grad updates.  I do admit that an equivalent effect could be achieved by running torch.autograd.grad with all module.parameters() as inputs, and then post-facto perform the update.\nOn a second look, the other situation could occur if a module captures a Variable with history that is unrelated. Let's make this concrete with an example module that does this.\nclass Escape(Module):\n  def __init__(self, var):\n    self.var = var  # not a parameter\n  def forward(self, input):\n    return input + var\n\nIf we run the following:\n# let e be an Escape module\ninput = Variable(..., requires_grad=True)\noutput = e(input)\noutput.backward()\n\nThen it is indeed true that backwards propagation will proceed into the history of e.var. The key is that this is true with or without checkpointing. That is to say, even if we replace this with a checkpointed e:\ninput = Variable(..., requires_grad=True)\noutput = checkpoint(e, input)\noutput.backward()\n\nThe intended semantics is that this should behave observationally equivalently to the original code, which means we must back-propagate through e.var. Adam, in your suggested rewrite with torch.autograd.grad, I cannot see how it is possible to make this propagation happen, because there is no way for checkpoint to know what the leafs of e.var's  history are. torch.grad.backward is the correct tool for the job.\nTrouble in paradise. Now, the discussion above was predicated on the use of the imperative backward interface, and this discussion got me thinking about what would happen if instead I had used torch.autograd.grad instead. Now we are in trouble: a checkpointing implementation that uses backward internally will mutate leaf.grad, where the non-checkpointing version would not have mutated grad. In this case, a torch.autograd.grad based implementation would have done better, because at least it wouldn't have mutated grad, but you will still have the of ferrying the gradients you computed from the reentrant torch.autograd.grad back to the outputs of the outer invocation of torch.autograd.grad. Ick.\nI think there is literally nothing we can do with this implementation strategy to handle this case, as the interfaces stand today. My best ideas are to either (1) add extra APIs to backpropagate into an arbitrary variable (but not as a reentrant backpropagation; as a dynamically added new gradient into the parent backpropagation graph) or (2) rework the entire implementation of checkpointing so it doesn't do reentrant backward execution (I don't actually know how to do this.)\nConclusion. Given that the implementation strategy Priya has implemented only actually works for imperative backward(), it seems to me that the simple AND correct implementation is to use an imperative backward() internally.", "body": "There are a few things to point out.\r\n\r\n**If imperative backward() is used, checkpointing should use imperative backward().** @apaszke, you stated:\r\n\r\n> We should never use backward for this purpose, because this will have side effects visible outside checkpointed function (will basically backpropagate through the whole graph that was created before checkpointed function ran).\r\n\r\nBut, on it's face, this statement is not true, because in Priya's original, backward-using variant, she repackages the inputs into leaf variables precisely to avoid this situation.\r\n\r\n```\r\n        inputs = repackage_inputs(saved_inputs)\r\n        with torch.enable_grad():\r\n            outputs = ctx.run_function(*inputs)\r\n```\r\n\r\nUnder what circumstances could backpropagation go beyond these inputs? It must be any Variable used by `ctx.run_function` which was not in the `inputs` list. Prima facie, these are the weight leaf Variables bundled in the modules (as specified by `module.parameters()`); but if these are leaf variables, then it doesn't matter: we *wanted* these `grad` updates.  I do admit that an equivalent effect could be achieved by running `torch.autograd.grad` with all `module.parameters()` as inputs, and then post-facto perform the update.\r\n\r\nOn a second look, the other situation could occur if a module captures a Variable with history that is unrelated. Let's make this concrete with an example module that does this.\r\n\r\n```\r\nclass Escape(Module):\r\n  def __init__(self, var):\r\n    self.var = var  # not a parameter\r\n  def forward(self, input):\r\n    return input + var\r\n```\r\n\r\nIf we run the following:\r\n\r\n```\r\n# let e be an Escape module\r\ninput = Variable(..., requires_grad=True)\r\noutput = e(input)\r\noutput.backward()\r\n```\r\n\r\nThen it is indeed true that backwards propagation will proceed into the history of `e.var`. The key is that this is true *with or without* checkpointing. That is to say, even if we replace this with a checkpointed e:\r\n\r\n```\r\ninput = Variable(..., requires_grad=True)\r\noutput = checkpoint(e, input)\r\noutput.backward()\r\n```\r\n\r\nThe intended semantics is that this should behave observationally equivalently to the original code, which means we *must* back-propagate through `e.var`. Adam, in your suggested rewrite with `torch.autograd.grad`, I cannot see how it is possible to make this propagation happen, because there is no way for checkpoint to know what the leafs of `e.var`'s  history are. `torch.grad.backward` is the correct tool for the job.\r\n\r\n**Trouble in paradise.** Now, the discussion above was predicated on the use of the imperative `backward` interface, and this discussion got me thinking about what would happen if instead I had used `torch.autograd.grad` instead. Now we are in trouble: a checkpointing implementation that uses `backward` internally will mutate `leaf.grad`, where the non-checkpointing version would not have mutated `grad`. In this case, a `torch.autograd.grad` based implementation would have done better, because at least it wouldn't have mutated `grad`, but you will *still* have the of ferrying the gradients you computed from the reentrant `torch.autograd.grad` back to the outputs of the outer invocation of `torch.autograd.grad`. Ick.\r\n\r\nI think there is literally nothing we can do with this implementation strategy to handle this case, as the interfaces stand today. My best ideas are to either (1) add extra APIs to backpropagate into an arbitrary variable (but not as a reentrant backpropagation; as a *dynamically* added new gradient into the parent backpropagation graph) or (2) rework the entire implementation of checkpointing so it doesn't do reentrant backward execution (I don't actually know how to do this.)\r\n\r\n**Conclusion.** Given that the implementation strategy Priya has implemented only actually works for imperative backward(), it seems to me that the simple AND correct implementation is to use an imperative backward() internally."}