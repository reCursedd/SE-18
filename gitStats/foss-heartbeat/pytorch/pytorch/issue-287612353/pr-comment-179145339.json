{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179145339", "pull_request_review_id": 109333024, "id": 179145339, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTE0NTMzOQ==", "diff_hunk": "@@ -0,0 +1,119 @@\n+import torch\n+\n+\n+def detach_variable(inputs):\n+    if torch.is_tensor(inputs):\n+        inp = inputs.detach()\n+        return inp\n+    elif isinstance(inputs, tuple):\n+        return tuple(detach_variable(v) for v in inputs)\n+    else:\n+        raise RuntimeError(\"Unsupported input type: \", type(inputs).__name__)\n+\n+\n+class CheckpointFunction(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, run_function, *args):\n+        ctx.run_function = run_function\n+        ctx.save_for_backward(*args)\n+        with torch.no_grad():\n+            outputs = run_function(*args)\n+        return outputs\n+\n+    @staticmethod\n+    def backward(ctx, *args):\n+        assert (torch.autograd.is_checkpoint_valid()), \\\n+            \"Checkpointing is not compatible with .grad(), please use .backward() if possible\"\n+        inputs = ctx.saved_tensors\n+        inputs_list = detach_variable(inputs)\n+        with torch.enable_grad():\n+            outputs = ctx.run_function(*inputs_list)\n+\n+        if isinstance(outputs, tuple):\n+            output_list = list(outputs)\n+        elif torch.is_tensor(outputs):\n+            output_list = [outputs]\n+        out_grads = [grad for grad in args]\n+        torch.autograd.backward(output_list, out_grads)\n+\n+        input_grads = None\n+        if isinstance(inputs_list, tuple):\n+            input_grads = tuple(inp.grad for inp in inputs_list)\n+            return (None,) + input_grads\n+        elif torch.is_tensor(inputs_list):\n+            input_grads = inputs_list.grad\n+            return None, input_grads\n+\n+\n+def checkpoint(run_function, *args):\n+    r\"\"\"Checkpoint a model or part of the model\n+\n+    Checkpoint works by trading compute for memory. It can be applied on any\n+    part of the model. In the forward pass, the model is run in volatile\n+    manner i.e. the activations are not stored. The forward pass save the\n+    inputs tuple and the run_function parameter. In the backwards pass, the\n+    saved inputs and run_function is retreived, and the forward pass is done\n+    on the model again (non-volatile this time) since we need to get the\n+    activations values for calculating the gradient and then the gradients are\n+    calculated.\n+\n+    Args:\n+        run_function : describes what to run in the forward pass of the model or\n+                       part of the model. It should also know how to handle\n+                       the inputs passed as the tuple. For example, in LSTM,\n+                       user passes (activation, hidden), run_function should\n+                       correctly use first input as activation and second input\n+                       as hidden\n+        args:         tuple containing inputs to the run_function\n+\n+    Returns:\n+        Output of running the run_function on *args\n+    \"\"\"\n+    return CheckpointFunction.apply(run_function, *args)\n+\n+\n+def checkpoint_sequential(modules, segments, *inputs):", "path": "torch/utils/checkpoint.py", "position": null, "original_position": 76, "commit_id": "37ee79fe45bdcfd2fb70eccc146369d6c765137e", "original_commit_id": "718c291acc8942ab8ffee2803b475372c1c300a3", "user": {"login": "prigoyal", "id": 13488275, "node_id": "MDQ6VXNlcjEzNDg4Mjc1", "avatar_url": "https://avatars0.githubusercontent.com/u/13488275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prigoyal", "html_url": "https://github.com/prigoyal", "followers_url": "https://api.github.com/users/prigoyal/followers", "following_url": "https://api.github.com/users/prigoyal/following{/other_user}", "gists_url": "https://api.github.com/users/prigoyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/prigoyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prigoyal/subscriptions", "organizations_url": "https://api.github.com/users/prigoyal/orgs", "repos_url": "https://api.github.com/users/prigoyal/repos", "events_url": "https://api.github.com/users/prigoyal/events{/privacy}", "received_events_url": "https://api.github.com/users/prigoyal/received_events", "type": "User", "site_admin": false}, "body": "can you elaborate a bit more on what does \"generic callables to be composed sequentially means\"?\r\n\r\n**For a bit of context**: These modules are left to user to pass and the modules themselves can be sequentials. We leave this to users so they can determine the granularity at which they want to checkpoint model. For example: in densenet, granularity can be denseblock or denselayer or something else. It highly depends on how the model is constructed by user. \r\n\r\n**NOTE:** This is a *convenience* method that's applicable to sequential type models like resnet, densenet etc. but this is not the only way for checkpointing. Check this out https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb for various ways. ", "created_at": "2018-04-04T13:47:38Z", "updated_at": "2018-11-23T15:41:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/4594#discussion_r179145339", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4594", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179145339"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4594#discussion_r179145339"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4594"}}, "body_html": "<p>can you elaborate a bit more on what does \"generic callables to be composed sequentially means\"?</p>\n<p><strong>For a bit of context</strong>: These modules are left to user to pass and the modules themselves can be sequentials. We leave this to users so they can determine the granularity at which they want to checkpoint model. For example: in densenet, granularity can be denseblock or denselayer or something else. It highly depends on how the model is constructed by user.</p>\n<p><strong>NOTE:</strong> This is a <em>convenience</em> method that's applicable to sequential type models like resnet, densenet etc. but this is not the only way for checkpointing. Check this out <a href=\"https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb\">https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb</a> for various ways.</p>", "body_text": "can you elaborate a bit more on what does \"generic callables to be composed sequentially means\"?\nFor a bit of context: These modules are left to user to pass and the modules themselves can be sequentials. We leave this to users so they can determine the granularity at which they want to checkpoint model. For example: in densenet, granularity can be denseblock or denselayer or something else. It highly depends on how the model is constructed by user.\nNOTE: This is a convenience method that's applicable to sequential type models like resnet, densenet etc. but this is not the only way for checkpointing. Check this out https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb for various ways.", "in_reply_to_id": 179104345}