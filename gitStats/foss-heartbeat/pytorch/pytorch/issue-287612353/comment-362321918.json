{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362321918", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-362321918", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 362321918, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjMyMTkxOA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T16:31:49Z", "updated_at": "2018-02-01T16:31:49Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>given the issues we are discussing here need to be addressed and out of the scope of adding this feature of checkpointing models.</p>\n</blockquote>\n<p>I wouldn't call all the issues here out of scope. This PR should work in a way that makes sense semantically and is <strong>safe</strong>, i.e. doesn't break any contracts of other features of our library. In this case, silently calling <code>backward()</code> even though someone is running a (supposedly) side-effect free <code>grad()</code> is not acceptable, because it breaks a very reasonable use case.</p>\n<p>The minimum work needed to get this PR to be safe should solve three problems:</p>\n<ol>\n<li>Have no sde-effects when <code>grad()</code> is used (need to assert that we're in <code>backward()</code> and not <code>grad()</code> inside <code>CheckpointedFunction.backward</code>)</li>\n<li>Check that <code>run_function</code> doesn't mutate any of the inputs (probably expose <code>.version</code> attribute of Variables, check it before and after <code>run_function</code> executes in forward). Note that this is also an incomplete check, because the variables that are captured implicitly be <code>run_function</code> can still be mutated and we will have no control over this (and this will in fact happen in cases as simple as using <code>BatchNorm</code>). Handling this in general would be hard, so let's at least put a note in the docs.</li>\n<li>Make inputs leaves in a way that doesn't break tracking of storage sharing. <code>var.detach().attach()</code> (i.e. adding <code>attach()</code>), would be sufficient. <code>Variable(var.data)</code> is not a correct solution.</li>\n</ol>", "body_text": "given the issues we are discussing here need to be addressed and out of the scope of adding this feature of checkpointing models.\n\nI wouldn't call all the issues here out of scope. This PR should work in a way that makes sense semantically and is safe, i.e. doesn't break any contracts of other features of our library. In this case, silently calling backward() even though someone is running a (supposedly) side-effect free grad() is not acceptable, because it breaks a very reasonable use case.\nThe minimum work needed to get this PR to be safe should solve three problems:\n\nHave no sde-effects when grad() is used (need to assert that we're in backward() and not grad() inside CheckpointedFunction.backward)\nCheck that run_function doesn't mutate any of the inputs (probably expose .version attribute of Variables, check it before and after run_function executes in forward). Note that this is also an incomplete check, because the variables that are captured implicitly be run_function can still be mutated and we will have no control over this (and this will in fact happen in cases as simple as using BatchNorm). Handling this in general would be hard, so let's at least put a note in the docs.\nMake inputs leaves in a way that doesn't break tracking of storage sharing. var.detach().attach() (i.e. adding attach()), would be sufficient. Variable(var.data) is not a correct solution.", "body": "> given the issues we are discussing here need to be addressed and out of the scope of adding this feature of checkpointing models.\r\n\r\nI wouldn't call all the issues here out of scope. This PR should work in a way that makes sense semantically and is **safe**, i.e. doesn't break any contracts of other features of our library. In this case, silently calling `backward()` even though someone is running a (supposedly) side-effect free `grad()` is not acceptable, because it breaks a very reasonable use case.\r\n\r\nThe minimum work needed to get this PR to be safe should solve three problems:\r\n1. Have no sde-effects when `grad()` is used (need to assert that we're in `backward()` and not `grad()` inside `CheckpointedFunction.backward`)\r\n2. Check that `run_function` doesn't mutate any of the inputs (probably expose `.version` attribute of Variables, check it before and after `run_function` executes in forward). Note that this is also an incomplete check, because the variables that are captured implicitly be `run_function` can still be mutated and we will have no control over this (and this will in fact happen in cases as simple as using `BatchNorm`). Handling this in general would be hard, so let's at least put a note in the docs.\r\n3. Make inputs leaves in a way that doesn't break tracking of storage sharing. `var.detach().attach()` (i.e. adding `attach()`), would be sufficient. `Variable(var.data)` is not a correct solution.\r\n"}