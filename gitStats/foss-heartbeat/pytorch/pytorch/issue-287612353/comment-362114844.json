{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362114844", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-362114844", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 362114844, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjExNDg0NA==", "user": {"login": "prigoyal", "id": 13488275, "node_id": "MDQ6VXNlcjEzNDg4Mjc1", "avatar_url": "https://avatars0.githubusercontent.com/u/13488275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prigoyal", "html_url": "https://github.com/prigoyal", "followers_url": "https://api.github.com/users/prigoyal/followers", "following_url": "https://api.github.com/users/prigoyal/following{/other_user}", "gists_url": "https://api.github.com/users/prigoyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/prigoyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prigoyal/subscriptions", "organizations_url": "https://api.github.com/users/prigoyal/orgs", "repos_url": "https://api.github.com/users/prigoyal/repos", "events_url": "https://api.github.com/users/prigoyal/events{/privacy}", "received_events_url": "https://api.github.com/users/prigoyal/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T00:14:45Z", "updated_at": "2018-02-01T00:19:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I sat down with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> and we looked into this issue further today.</p>\n<p>TL;DR, it looks like in case of checkpointing, using <code>torch.autograd.backwards</code> is the right thing to do (which I had originally done and worked fine)</p>\n<p>explanation below:</p>\n<p>There is a difference in behavior of <code>torch.autograd.backward</code> and <code>torch.autograd.grad</code>.<br>\nBasically, in <code>torch.autograd.grad</code>, since we pass the inputs list explicitly, it won't monkey around with any other leaf variables i.e. weights and won't compute the gradient for them. However, in <code>torch.autograd.backward</code>, it will compute the gradient for weights. I think in case of checkpoint, that's exactly the behavior that we want i.e. compute the grads of the inputs and the params. Can you comment if this makes sense to you <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> ?</p>\n<p>attaching a screenshot below to verify the behavior of <code>torch.autograd.grad</code> not updating the weights</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13488275/35654395-d879c66e-06ba-11e8-8947-1a759da0f62d.png\"><img width=\"656\" alt=\"screen shot 2018-01-31 at 7 02 44 pm\" src=\"https://user-images.githubusercontent.com/13488275/35654395-d879c66e-06ba-11e8-8947-1a759da0f62d.png\" style=\"max-width:100%;\"></a></p>\n<p>this corresponds to the test here <a href=\"https://github.com/pytorch/pytorch/pull/4594/files#diff-85c7da7d97a696e8017da5742a876475\">https://github.com/pytorch/pytorch/pull/4594/files#diff-85c7da7d97a696e8017da5742a876475</a></p>\n<p>if you are not convinced, can we schedule a VC with you, me and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> to resolve this? if you are convinced, I can update PR to use <code>torch.autograd.backwards</code> and let's land it. Thanks a bunch :)</p>\n<p>also cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> in case they have any comments.</p>", "body_text": "@apaszke I sat down with @ezyang and we looked into this issue further today.\nTL;DR, it looks like in case of checkpointing, using torch.autograd.backwards is the right thing to do (which I had originally done and worked fine)\nexplanation below:\nThere is a difference in behavior of torch.autograd.backward and torch.autograd.grad.\nBasically, in torch.autograd.grad, since we pass the inputs list explicitly, it won't monkey around with any other leaf variables i.e. weights and won't compute the gradient for them. However, in torch.autograd.backward, it will compute the gradient for weights. I think in case of checkpoint, that's exactly the behavior that we want i.e. compute the grads of the inputs and the params. Can you comment if this makes sense to you @apaszke ?\nattaching a screenshot below to verify the behavior of torch.autograd.grad not updating the weights\n\nthis corresponds to the test here https://github.com/pytorch/pytorch/pull/4594/files#diff-85c7da7d97a696e8017da5742a876475\nif you are not convinced, can we schedule a VC with you, me and @ezyang to resolve this? if you are convinced, I can update PR to use torch.autograd.backwards and let's land it. Thanks a bunch :)\nalso cc @zdevito , @soumith in case they have any comments.", "body": "@apaszke I sat down with @ezyang and we looked into this issue further today. \r\n\r\nTL;DR, it looks like in case of checkpointing, using `torch.autograd.backwards` is the right thing to do (which I had originally done and worked fine)\r\n\r\nexplanation below:\r\n\r\nThere is a difference in behavior of `torch.autograd.backward` and `torch.autograd.grad`.\r\nBasically, in `torch.autograd.grad`, since we pass the inputs list explicitly, it won't monkey around with any other leaf variables i.e. weights and won't compute the gradient for them. However, in `torch.autograd.backward`, it will compute the gradient for weights. I think in case of checkpoint, that's exactly the behavior that we want i.e. compute the grads of the inputs and the params. Can you comment if this makes sense to you @apaszke ?\r\n\r\nattaching a screenshot below to verify the behavior of `torch.autograd.grad` not updating the weights\r\n\r\n<img width=\"656\" alt=\"screen shot 2018-01-31 at 7 02 44 pm\" src=\"https://user-images.githubusercontent.com/13488275/35654395-d879c66e-06ba-11e8-8947-1a759da0f62d.png\">\r\n\r\n\r\nthis corresponds to the test here https://github.com/pytorch/pytorch/pull/4594/files#diff-85c7da7d97a696e8017da5742a876475\r\n\r\nif you are not convinced, can we schedule a VC with you, me and @ezyang to resolve this? if you are convinced, I can update PR to use `torch.autograd.backwards` and let's land it. Thanks a bunch :) \r\n\r\nalso cc @zdevito , @soumith in case they have any comments."}