{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362238948", "html_url": "https://github.com/pytorch/pytorch/pull/4594#issuecomment-362238948", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4594", "id": 362238948, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjIzODk0OA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T11:31:39Z", "updated_at": "2018-02-01T11:33:29Z", "author_association": "MEMBER", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>, your post has really highlighted some things I missed about this PR. I would still argue that rewrapping + backward isn't the solution here. In fact, by our joint arguments both solutions are wrong, and we should come up with something else (by either adding new APIs, or reworking the implementation, as you suggested).</p>\n<p><strong>If imperative backward() is used, checkpointing should use imperative backward()</strong>. I previously didn't realize that weights are actually captured variables in run_function, and we won't compute grads for them. However, (as you noted in the end) you can easily take a dual of this sentence: <strong>If functional grad() is used, checkpointing should use functional grad()</strong> - otherwise <code>grad()</code> starts to have visible side effects and its semantics completely change. <code>checkpoint</code> is no longer a semantically-equivalent drop-in as you requested. Writing a checkpoint that uses <code>backward()</code> internally is not a good solution, and is incorrect, unless we actually put asserts that we're not in the functional mode at the moment.</p>\n<p>My statement was true provided we don't re-wrap inputs and re-wrapping them is bad because <strong>manually creating multiple Variables that alias the same storage is not supported in autograd</strong>. This breaks all our in-place/version checks, and this is one of the reasons I changed the implementation. A better solution would be to have something like an \"attach()\" function and do this:</p>\n<div class=\"highlight highlight-source-python\"><pre>inputs <span class=\"pl-k\">=</span> [i <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> i.requires_grad <span class=\"pl-k\">else</span> i.detach().attach() <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> inputs]</pre></div>\n<p>But that's not the end of the problems here...</p>\n<p>In fact, in the way this checkpointing function is written today, if <code>run_function</code> modifies its input (because it has e.g. <code>nn.Sigmoid(inplace=True)</code> as the first input) then <strong>the backward will run on the mutated input, and it's not even an error, even with version counting</strong> (since the forward runs in <code>no_grad</code> mode). This is not a big deal for <code>ReLU</code> (like in the test), because relu is nilpotent in a way (i.e. <code>relu(relu(x)) == relu(x)</code>), however this is not true for a more general class of functions including <code>sigmoid</code>.</p>\n<p>Pretty much all of it boils down to the same problem, that we've been battling with over the last summer - separating/modifying subgraphs of the backward graph is <strong>HARD</strong>.</p>", "body_text": "Thanks @ezyang, your post has really highlighted some things I missed about this PR. I would still argue that rewrapping + backward isn't the solution here. In fact, by our joint arguments both solutions are wrong, and we should come up with something else (by either adding new APIs, or reworking the implementation, as you suggested).\nIf imperative backward() is used, checkpointing should use imperative backward(). I previously didn't realize that weights are actually captured variables in run_function, and we won't compute grads for them. However, (as you noted in the end) you can easily take a dual of this sentence: If functional grad() is used, checkpointing should use functional grad() - otherwise grad() starts to have visible side effects and its semantics completely change. checkpoint is no longer a semantically-equivalent drop-in as you requested. Writing a checkpoint that uses backward() internally is not a good solution, and is incorrect, unless we actually put asserts that we're not in the functional mode at the moment.\nMy statement was true provided we don't re-wrap inputs and re-wrapping them is bad because manually creating multiple Variables that alias the same storage is not supported in autograd. This breaks all our in-place/version checks, and this is one of the reasons I changed the implementation. A better solution would be to have something like an \"attach()\" function and do this:\ninputs = [i if not i.requires_grad else i.detach().attach() for i in inputs]\nBut that's not the end of the problems here...\nIn fact, in the way this checkpointing function is written today, if run_function modifies its input (because it has e.g. nn.Sigmoid(inplace=True) as the first input) then the backward will run on the mutated input, and it's not even an error, even with version counting (since the forward runs in no_grad mode). This is not a big deal for ReLU (like in the test), because relu is nilpotent in a way (i.e. relu(relu(x)) == relu(x)), however this is not true for a more general class of functions including sigmoid.\nPretty much all of it boils down to the same problem, that we've been battling with over the last summer - separating/modifying subgraphs of the backward graph is HARD.", "body": "Thanks @ezyang, your post has really highlighted some things I missed about this PR. I would still argue that rewrapping + backward isn't the solution here. In fact, by our joint arguments both solutions are wrong, and we should come up with something else (by either adding new APIs, or reworking the implementation, as you suggested).\r\n\r\n**If imperative backward() is used, checkpointing should use imperative backward()**. I previously didn't realize that weights are actually captured variables in run_function, and we won't compute grads for them. However, (as you noted in the end) you can easily take a dual of this sentence: **If functional grad() is used, checkpointing should use functional grad()** - otherwise `grad()` starts to have visible side effects and its semantics completely change. `checkpoint` is no longer a semantically-equivalent drop-in as you requested. Writing a checkpoint that uses `backward()` internally is not a good solution, and is incorrect, unless we actually put asserts that we're not in the functional mode at the moment.\r\n\r\nMy statement was true provided we don't re-wrap inputs and re-wrapping them is bad because **manually creating multiple Variables that alias the same storage is not supported in autograd**. This breaks all our in-place/version checks, and this is one of the reasons I changed the implementation. A better solution would be to have something like an \"attach()\" function and do this:\r\n```python\r\ninputs = [i if not i.requires_grad else i.detach().attach() for i in inputs]\r\n```\r\n\r\nBut that's not the end of the problems here...\r\n\r\nIn fact, in the way this checkpointing function is written today, if `run_function` modifies its input (because it has e.g. `nn.Sigmoid(inplace=True)` as the first input) then **the backward will run on the mutated input, and it's not even an error, even with version counting** (since the forward runs in `no_grad` mode). This is not a big deal for `ReLU` (like in the test), because relu is nilpotent in a way (i.e. `relu(relu(x)) == relu(x)`), however this is not true for a more general class of functions including `sigmoid`.\r\n\r\nPretty much all of it boils down to the same problem, that we've been battling with over the last summer - separating/modifying subgraphs of the backward graph is **HARD**."}