{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161370923", "pull_request_review_id": 88656195, "id": 161370923, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTM3MDkyMw==", "diff_hunk": "@@ -0,0 +1,118 @@\n+import torch\n+from torch.autograd import Variable, Function\n+\n+\n+def repackage_inputs(inputs):\n+    if torch.is_tensor(inputs):\n+        return Variable(inputs)\n+    elif isinstance(inputs, tuple):\n+        return tuple(repackage_inputs(v) for v in inputs)\n+    else:\n+        raise RuntimeError(\"Unknown input type\")\n+\n+\n+def unpack_variables(inputs):\n+    if type(inputs) == Variable:\n+        return inputs.data\n+    elif torch.is_tensor(inputs):\n+        return inputs\n+    elif isinstance(inputs, tuple):\n+        return tuple(unpack_variables(v) for v in inputs)\n+    else:\n+        raise RuntimeError(\"Unknown input type\")\n+\n+\n+class CheckpointFunction(Function):\n+\n+    # NOTE: *args is the flat inputs list, args is the tuple containing inputs\n+    @staticmethod\n+    def forward(ctx, run_function, *args):\n+        ctx.run_function = run_function\n+        ctx.save_for_backward(*args)\n+        inputs = repackage_inputs(args)\n+        with torch.no_grad():\n+            outputs = run_function(*inputs)     # the *inputs* is always a tuple\n+        return unpack_variables(outputs)\n+\n+    @staticmethod\n+    def backward(ctx, *args):\n+        saved_inputs = ctx.saved_tensors\n+        inputs = repackage_inputs(saved_inputs)\n+        with torch.enable_grad():\n+            outputs = ctx.run_function(*inputs)\n+\n+        if isinstance(outputs, tuple):\n+            output_list = list(outputs)\n+        elif isinstance(outputs, Variable) or torch.is_tensor(outputs):\n+            output_list = [outputs]\n+        out_grads = [grad for grad in args]\n+        torch.autograd.backward(output_list, out_grads)\n+\n+        input_grads = None\n+        if isinstance(inputs, tuple):\n+            input_grads = tuple(inp.grad for inp in inputs)\n+            return (None,) + input_grads\n+        elif isinstance(inputs, Variable) or torch.is_tensor(inputs):\n+            input_grads = inputs.grad\n+            return None, input_grads\n+\n+\n+def checkpoint(run_function, *args):\n+    r\"\"\"Checkpoint a model or part of the model\n+\n+    Checkpoint works by trading compute for memory. It can be applied on any\n+    part of the model. In the forward pass, the model is run in volatile\n+    manner i.e. the activations are not stored. The forward pass save the\n+    inputs tuple and the run_function parameter. In the backwards pass, the\n+    saved inputs and run_function is retreived, and the forward pass is done\n+    on the model again (non-volatile this time) since we need to get the\n+    activations values for calculating the gradient and then the gradients are\n+    calculated.\n+\n+    Args:\n+        run_function : describes what to run in the forward pass of the model or\n+                       part of the model. It should also know how to handle\n+                       the inputs passed as the tuple. For example, in LSTM,\n+                       user passes (activation, hidden), run_function should\n+                       correctly use first input as activation and second input\n+                       as hidden\n+        args:         tuple containing inputs to the run_function\n+\n+    Returns:\n+        Output of running the run_function on *args\n+    \"\"\"\n+    return CheckpointFunction.apply(run_function, *args)\n+\n+\n+def checkpoint_sequential(modules, segments, run_function, *inputs):", "path": "torch/utils/checkpoint.py", "position": null, "original_position": 87, "commit_id": "37ee79fe45bdcfd2fb70eccc146369d6c765137e", "original_commit_id": "44577945e1afc25b8eaa32b44b804864ca23be64", "user": {"login": "prigoyal", "id": 13488275, "node_id": "MDQ6VXNlcjEzNDg4Mjc1", "avatar_url": "https://avatars0.githubusercontent.com/u/13488275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prigoyal", "html_url": "https://github.com/prigoyal", "followers_url": "https://api.github.com/users/prigoyal/followers", "following_url": "https://api.github.com/users/prigoyal/following{/other_user}", "gists_url": "https://api.github.com/users/prigoyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/prigoyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prigoyal/subscriptions", "organizations_url": "https://api.github.com/users/prigoyal/orgs", "repos_url": "https://api.github.com/users/prigoyal/repos", "events_url": "https://api.github.com/users/prigoyal/events{/privacy}", "received_events_url": "https://api.github.com/users/prigoyal/received_events", "type": "User", "site_admin": false}, "body": "I wanted to have consistency with the checkpoint() call (which is what checkpoint_sequential is also making) and also leave it to users to determine how their inputs_tuple should be handled and what they want the checkpoint call to run. Executing \r\n```modules[start:end]``` here would still require wrapping it with some function so that we can use ```CheckpointFunction```. However, letting users do that is good so that things are less magical for them IMO\r\n", "created_at": "2018-01-13T12:15:45Z", "updated_at": "2018-11-23T15:38:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/4594#discussion_r161370923", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4594", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161370923"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4594#discussion_r161370923"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4594"}}, "body_html": "<p>I wanted to have consistency with the checkpoint() call (which is what checkpoint_sequential is also making) and also leave it to users to determine how their inputs_tuple should be handled and what they want the checkpoint call to run. Executing<br>\n<code>modules[start:end]</code> here would still require wrapping it with some function so that we can use <code>CheckpointFunction</code>. However, letting users do that is good so that things are less magical for them IMO</p>", "body_text": "I wanted to have consistency with the checkpoint() call (which is what checkpoint_sequential is also making) and also leave it to users to determine how their inputs_tuple should be handled and what they want the checkpoint call to run. Executing\nmodules[start:end] here would still require wrapping it with some function so that we can use CheckpointFunction. However, letting users do that is good so that things are less magical for them IMO", "in_reply_to_id": 161369711}