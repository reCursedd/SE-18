{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383926260", "html_url": "https://github.com/pytorch/pytorch/issues/6504#issuecomment-383926260", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6504", "id": 383926260, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzkyNjI2MA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-24T13:16:29Z", "updated_at": "2018-04-24T13:16:29Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22089061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/CharlesxrWu\">@CharlesxrWu</a> it seems that you are using a model specific for cifar on ImageNet, which is not adapted, as the model for Cifar has very few downsampling, and you end up using way to much memory.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6627179\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ccampell\">@ccampell</a> your case looks a bit related. In the first case you are running out of memory because a batch size of 256 is too much for a single GPU for InceptionV3.<br>\nFor the two other cases, it seems that your input images are smaller than what the model that you are passing expect. InceptionV3 expects inputs to be of size 299x299 if I recall correctly, and not 224x224, as is the default in the imagenet example</p>\n<p>Closing as this looks like user-error, and not a bug in PyTorch</p>", "body_text": "@CharlesxrWu it seems that you are using a model specific for cifar on ImageNet, which is not adapted, as the model for Cifar has very few downsampling, and you end up using way to much memory.\n@ccampell your case looks a bit related. In the first case you are running out of memory because a batch size of 256 is too much for a single GPU for InceptionV3.\nFor the two other cases, it seems that your input images are smaller than what the model that you are passing expect. InceptionV3 expects inputs to be of size 299x299 if I recall correctly, and not 224x224, as is the default in the imagenet example\nClosing as this looks like user-error, and not a bug in PyTorch", "body": "@CharlesxrWu it seems that you are using a model specific for cifar on ImageNet, which is not adapted, as the model for Cifar has very few downsampling, and you end up using way to much memory.\r\n\r\n@ccampell your case looks a bit related. In the first case you are running out of memory because a batch size of 256 is too much for a single GPU for InceptionV3.\r\nFor the two other cases, it seems that your input images are smaller than what the model that you are passing expect. InceptionV3 expects inputs to be of size 299x299 if I recall correctly, and not 224x224, as is the default in the imagenet example\r\n\r\nClosing as this looks like user-error, and not a bug in PyTorch\r\n"}