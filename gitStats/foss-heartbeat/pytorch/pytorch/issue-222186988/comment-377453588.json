{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377453588", "html_url": "https://github.com/pytorch/pytorch/issues/1274#issuecomment-377453588", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1274", "id": 377453588, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzQ1MzU4OA==", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T06:04:48Z", "updated_at": "2018-03-30T06:04:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>FWIW I've implemented a wrapped version of torch that does its best to emulate this sort of behavior: <a href=\"https://github.com/samuela/kindling/blob/master/kindling/nan_police.py\">https://github.com/samuela/kindling/blob/master/kindling/nan_police.py</a>.</p>\n<pre><code>In [1]: from kindling.nan_police import torch\n\nIn [2]: x = torch.ones(2) / 0 * 0\n\nIn [3]: x\nOut[3]:\n\nnan\nnan\n[torch.FloatTensor of size 2]\n\nIn [4]: torch.sum(x)\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-4-e7f45fec8fb4&gt; in &lt;module&gt;()\n----&gt; 1 torch.sum(x)\n\n~/Development/kindling/kindling/nan_police.py in __call__(self, *args, **kwargs)\n    147         if argnan_path == []:\n    148           raise Exception(\n--&gt; 149             f'Found a NaN at positional argument {i + 1} (of {len(args)}) when '\n    150             f'calling `{path}`!'\n    151           )\n\nException: Found a NaN at positional argument 1 (of 1) when calling `torch.sum`!\n</code></pre>\n<p>I've added it to my mini-toolkit of pytorch utilities, <a href=\"https://github.com/samuela/kindling\">kindling</a>.</p>", "body_text": "FWIW I've implemented a wrapped version of torch that does its best to emulate this sort of behavior: https://github.com/samuela/kindling/blob/master/kindling/nan_police.py.\nIn [1]: from kindling.nan_police import torch\n\nIn [2]: x = torch.ones(2) / 0 * 0\n\nIn [3]: x\nOut[3]:\n\nnan\nnan\n[torch.FloatTensor of size 2]\n\nIn [4]: torch.sum(x)\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n<ipython-input-4-e7f45fec8fb4> in <module>()\n----> 1 torch.sum(x)\n\n~/Development/kindling/kindling/nan_police.py in __call__(self, *args, **kwargs)\n    147         if argnan_path == []:\n    148           raise Exception(\n--> 149             f'Found a NaN at positional argument {i + 1} (of {len(args)}) when '\n    150             f'calling `{path}`!'\n    151           )\n\nException: Found a NaN at positional argument 1 (of 1) when calling `torch.sum`!\n\nI've added it to my mini-toolkit of pytorch utilities, kindling.", "body": "FWIW I've implemented a wrapped version of torch that does its best to emulate this sort of behavior: https://github.com/samuela/kindling/blob/master/kindling/nan_police.py. \r\n\r\n```\r\nIn [1]: from kindling.nan_police import torch\r\n\r\nIn [2]: x = torch.ones(2) / 0 * 0\r\n\r\nIn [3]: x\r\nOut[3]:\r\n\r\nnan\r\nnan\r\n[torch.FloatTensor of size 2]\r\n\r\nIn [4]: torch.sum(x)\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-4-e7f45fec8fb4> in <module>()\r\n----> 1 torch.sum(x)\r\n\r\n~/Development/kindling/kindling/nan_police.py in __call__(self, *args, **kwargs)\r\n    147         if argnan_path == []:\r\n    148           raise Exception(\r\n--> 149             f'Found a NaN at positional argument {i + 1} (of {len(args)}) when '\r\n    150             f'calling `{path}`!'\r\n    151           )\r\n\r\nException: Found a NaN at positional argument 1 (of 1) when calling `torch.sum`!\r\n```\r\n\r\nI've added it to my mini-toolkit of pytorch utilities, [kindling](https://github.com/samuela/kindling)."}