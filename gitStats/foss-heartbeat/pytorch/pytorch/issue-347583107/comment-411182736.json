{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/411182736", "html_url": "https://github.com/pytorch/pytorch/issues/10229#issuecomment-411182736", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10229", "id": 411182736, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMTE4MjczNg==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-07T20:00:16Z", "updated_at": "2018-08-07T20:18:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Talked to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> offline, the perf diff should be a false alarm that mxnet didn't really run the conv  kernels with the script above.</p>\n<p>For a single shape, if you print the output tensor, that actually enforce mxnet to run 1000 invocations of <code>maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt</code> as well, with a similar avg. duration.</p>\n<p>I'm not very familiar with mxnet syntax, adding sth like <code>torch.cuda.synchronize()</code> in mxnet part should fix the gap. Or the way I verified it is like, before I enforced the print of output tensor, there's simply no conv kernel called in the profiler(and it's very fast, the time is constant even when you increase the iteration number).  After I enforced the print of output tensor, <code>maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt</code> kernel is called 1000 times as Pytorch. These indicate that mxnet returns before the kernel was actually run. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26398121\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Johnccl\">@Johnccl</a> If you have an idea how to synchronize cuda in mxnet, please let us know. Otherwise from the profiling, it's safe to say this is a false alarm imho.</p>\n<p>Here's a hacky script I used to generate the nvprof result.<br>\n[EDIT] deleted this script since I found the right command to enforce kernel launch in MXnet. Posting the perf numbers below.</p>", "body_text": "Talked to @ngimel offline, the perf diff should be a false alarm that mxnet didn't really run the conv  kernels with the script above.\nFor a single shape, if you print the output tensor, that actually enforce mxnet to run 1000 invocations of maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt as well, with a similar avg. duration.\nI'm not very familiar with mxnet syntax, adding sth like torch.cuda.synchronize() in mxnet part should fix the gap. Or the way I verified it is like, before I enforced the print of output tensor, there's simply no conv kernel called in the profiler(and it's very fast, the time is constant even when you increase the iteration number).  After I enforced the print of output tensor, maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt kernel is called 1000 times as Pytorch. These indicate that mxnet returns before the kernel was actually run. @Johnccl If you have an idea how to synchronize cuda in mxnet, please let us know. Otherwise from the profiling, it's safe to say this is a false alarm imho.\nHere's a hacky script I used to generate the nvprof result.\n[EDIT] deleted this script since I found the right command to enforce kernel launch in MXnet. Posting the perf numbers below.", "body": "Talked to @ngimel offline, the perf diff should be a false alarm that mxnet didn't really run the conv  kernels with the script above. \r\n\r\nFor a single shape, if you print the output tensor, that actually enforce mxnet to run 1000 invocations of `maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt` as well, with a similar avg. duration. \r\n\r\nI'm not very familiar with mxnet syntax, adding sth like `torch.cuda.synchronize()` in mxnet part should fix the gap. Or the way I verified it is like, before I enforced the print of output tensor, there's simply no conv kernel called in the profiler(and it's very fast, the time is constant even when you increase the iteration number).  After I enforced the print of output tensor, `maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt` kernel is called 1000 times as Pytorch. These indicate that mxnet returns before the kernel was actually run. @Johnccl If you have an idea how to synchronize cuda in mxnet, please let us know. Otherwise from the profiling, it's safe to say this is a false alarm imho.\r\n\r\nHere's a hacky script I used to generate the nvprof result. \r\n[EDIT] deleted this script since I found the right command to enforce kernel launch in MXnet. Posting the perf numbers below. "}