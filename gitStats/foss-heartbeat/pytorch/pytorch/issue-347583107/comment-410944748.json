{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/410944748", "html_url": "https://github.com/pytorch/pytorch/issues/10229#issuecomment-410944748", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10229", "id": 410944748, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDk0NDc0OA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-07T06:11:29Z", "updated_at": "2018-08-07T06:11:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5248122\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ailzhang\">@ailzhang</a> for your convolution in pytorch on P100 I see</p>\n<pre><code>GPU activities:   93.86%  243.44ms      1001  243.20us  224.80us  259.40us  maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt\n</code></pre>\n<p>1001 invocations of a kernel, which is to be expected for 1000 convolutions run in a loop + warm up<br>\nI could not profile mxnet with the given script, it errored out, and the profiles that it did produce never had 1000 invocations of the same kernel (which is to be expected if the timing loop did run correctly), so I would not trust mxnet timings. Your results, where mxnet launches just 2 kernels, also are suspicious.<br>\nFor actual grouped convolutions (group !=1) <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26398121\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Johnccl\">@Johnccl</a> is correct that perf is much better for groups == input_channels, because in this case pytorch's own kernel is called. In all other cases, the call is sent to cudnn and the performance is whatever cudnn provides.<br>\nMy script, slightly modified from original, is below</p>\n<pre><code>import time\nimport math\nimport torch\ntorch.backends.cudnn.benchmark = True\ndef count(a, m):\n    torch.cuda.synchronize()\n    t0 = time.time()\n    for i in range(1000):\n        b = m(a)\n    torch.cuda.synchronize()\n    return time.time() - t0\n\ndef th_test(in_channel=1024, size=19, batch=1):\n    import torch as th\n    import torch.nn as nn\n    x = th.rand((batch, in_channel, size, size))\n    x = x.cuda()\n    out_channels_lsit = [1024] #256, 512, 1024, 2048]\n    for out_channels in out_channels_lsit:\n        n = int(math.log(min(in_channel, out_channels), 2))+1\n        for i in range(n):\n            g = int(math.pow(2, i))\n            m = nn.Conv2d(in_channel, out_channels=out_channels, kernel_size=3, padding=1, groups=g).cuda()\n            b = m(x)#warm up\n            t = count(x, m)\n            print('OutChannel:{}, Group:{}, Time:{}'.format(out_channels, g, t))\n\nif __name__=='__main__':\n    print('Pytorch testing:')\n    th_test(1024, 19, 1)\n</code></pre>", "body_text": "@ailzhang for your convolution in pytorch on P100 I see\nGPU activities:   93.86%  243.44ms      1001  243.20us  224.80us  259.40us  maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt\n\n1001 invocations of a kernel, which is to be expected for 1000 convolutions run in a loop + warm up\nI could not profile mxnet with the given script, it errored out, and the profiles that it did produce never had 1000 invocations of the same kernel (which is to be expected if the timing loop did run correctly), so I would not trust mxnet timings. Your results, where mxnet launches just 2 kernels, also are suspicious.\nFor actual grouped convolutions (group !=1) @Johnccl is correct that perf is much better for groups == input_channels, because in this case pytorch's own kernel is called. In all other cases, the call is sent to cudnn and the performance is whatever cudnn provides.\nMy script, slightly modified from original, is below\nimport time\nimport math\nimport torch\ntorch.backends.cudnn.benchmark = True\ndef count(a, m):\n    torch.cuda.synchronize()\n    t0 = time.time()\n    for i in range(1000):\n        b = m(a)\n    torch.cuda.synchronize()\n    return time.time() - t0\n\ndef th_test(in_channel=1024, size=19, batch=1):\n    import torch as th\n    import torch.nn as nn\n    x = th.rand((batch, in_channel, size, size))\n    x = x.cuda()\n    out_channels_lsit = [1024] #256, 512, 1024, 2048]\n    for out_channels in out_channels_lsit:\n        n = int(math.log(min(in_channel, out_channels), 2))+1\n        for i in range(n):\n            g = int(math.pow(2, i))\n            m = nn.Conv2d(in_channel, out_channels=out_channels, kernel_size=3, padding=1, groups=g).cuda()\n            b = m(x)#warm up\n            t = count(x, m)\n            print('OutChannel:{}, Group:{}, Time:{}'.format(out_channels, g, t))\n\nif __name__=='__main__':\n    print('Pytorch testing:')\n    th_test(1024, 19, 1)", "body": "@ailzhang for your convolution in pytorch on P100 I see \r\n```\r\nGPU activities:   93.86%  243.44ms      1001  243.20us  224.80us  259.40us  maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_nt\r\n```\r\n1001 invocations of a kernel, which is to be expected for 1000 convolutions run in a loop + warm up\r\nI could not profile mxnet with the given script, it errored out, and the profiles that it did produce never had 1000 invocations of the same kernel (which is to be expected if the timing loop did run correctly), so I would not trust mxnet timings. Your results, where mxnet launches just 2 kernels, also are suspicious.\r\nFor actual grouped convolutions (group !=1) @Johnccl is correct that perf is much better for groups == input_channels, because in this case pytorch's own kernel is called. In all other cases, the call is sent to cudnn and the performance is whatever cudnn provides. \r\nMy script, slightly modified from original, is below\r\n```\r\nimport time\r\nimport math\r\nimport torch\r\ntorch.backends.cudnn.benchmark = True\r\ndef count(a, m):\r\n    torch.cuda.synchronize()\r\n    t0 = time.time()\r\n    for i in range(1000):\r\n        b = m(a)\r\n    torch.cuda.synchronize()\r\n    return time.time() - t0\r\n\r\ndef th_test(in_channel=1024, size=19, batch=1):\r\n    import torch as th\r\n    import torch.nn as nn\r\n    x = th.rand((batch, in_channel, size, size))\r\n    x = x.cuda()\r\n    out_channels_lsit = [1024] #256, 512, 1024, 2048]\r\n    for out_channels in out_channels_lsit:\r\n        n = int(math.log(min(in_channel, out_channels), 2))+1\r\n        for i in range(n):\r\n            g = int(math.pow(2, i))\r\n            m = nn.Conv2d(in_channel, out_channels=out_channels, kernel_size=3, padding=1, groups=g).cuda()\r\n            b = m(x)#warm up\r\n            t = count(x, m)\r\n            print('OutChannel:{}, Group:{}, Time:{}'.format(out_channels, g, t))\r\n\r\nif __name__=='__main__':\r\n    print('Pytorch testing:')\r\n    th_test(1024, 19, 1)\r\n```"}