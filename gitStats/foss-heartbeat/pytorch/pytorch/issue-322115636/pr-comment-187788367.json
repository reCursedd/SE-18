{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187788367", "pull_request_review_id": 119642227, "id": 187788367, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Nzc4ODM2Nw==", "diff_hunk": "@@ -63,58 +63,68 @@ kernelReduceNoncontigDim_shared(TensorInfo<T, IndexType> out,\n     //for(IndexType i=threadIdx.y; i<reductionSize; i+=blockDim.y){\n     //  local_reg += in[inOffset + i * reductionStride];\n     //}\n-    for(IndexType i=threadIdx.y; i<reductionSize; i+=blockDim.y*4){\n-      if(i + blockDim.y * 3 < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n-        load_reg[2] = modifyOp(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n-        load_reg[3] = modifyOp(in.data[inOffset + (i + blockDim.y * 3) * reductionStride]);\n+    for(IndexType i = threadIdx.y; i < reductionSize; i += blockDim.y * 4) {\n+      if (i + blockDim.y * 3 < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n+        const AccT val2 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+        load_reg[2] = modifyOp(val2);\n+        const AccT val3 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 3) * reductionStride]);\n+        load_reg[3] = modifyOp(val3);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n         local_reg = reduceOp(local_reg, load_reg[2]);\n         local_reg = reduceOp(local_reg, load_reg[3]);\n-      }else if(i + blockDim.y * 2 < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n-        load_reg[2] = modifyOp(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+      } else if (i + blockDim.y * 2 < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n+        const AccT val2 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+        load_reg[2] = modifyOp(val2);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n         local_reg = reduceOp(local_reg, load_reg[2]);\n-      }else if( (i + blockDim.y) < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n+      } else if (i + blockDim.y < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n-      }else if(i + blockDim.y * 0 < reductionSize){\n-        local_reg = reduceOp(local_reg, modifyOp(in.data[inOffset + i * reductionStride]));\n+      } else if (i < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        local_reg = reduceOp(local_reg, modifyOp(val0));\n       }\n     }\n \n     *shmem = local_reg;\n     int dimy = blockDim.y;\n-    while(dimy > 1){\n+    while (dimy > 1) {\n       __syncthreads();\n-      if( threadIdx.y == 0 && (dimy%2 != 0) ){\n-        *shmem = reduceAccOp(*shmem, *(shmem + (dimy-1) * blockDim.x) );\n+      if (threadIdx.y == 0 && (dimy % 2 != 0) ) {\n+        *shmem = reduceOp(*shmem, *(shmem + (dimy - 1) * blockDim.x));", "path": "aten/src/THC/THCReduce.cuh", "position": 103, "original_position": 103, "commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "original_commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "Hmm... \r\n\r\nSo before this PR there was reduceOp and reduceAccOp. However, this PR simplifies things so there is now only a single reduceOp that takes in two AccT and returns one. This is because the new pattern is:\r\n\r\n1) Acquire the data.\r\n2) Convert it to the accumulate type.\r\n3) Perform operations (like modify and reduce and finalize) on it\r\n4) Convert the out value to the \"real\" type and write it out\r\n\r\nIf you're concerned about some implicit conversions here we have some added safety because half types cannot be implicitly converted to other types (like float), so unless we're very careful with our (explicit) conversions we will not be able to compile. \r\n\r\nDoes that make sense?", "created_at": "2018-05-13T03:56:20Z", "updated_at": "2018-11-23T15:43:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187788367", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7487", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187788367"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187788367"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7487"}}, "body_html": "<p>Hmm...</p>\n<p>So before this PR there was reduceOp and reduceAccOp. However, this PR simplifies things so there is now only a single reduceOp that takes in two AccT and returns one. This is because the new pattern is:</p>\n<ol>\n<li>Acquire the data.</li>\n<li>Convert it to the accumulate type.</li>\n<li>Perform operations (like modify and reduce and finalize) on it</li>\n<li>Convert the out value to the \"real\" type and write it out</li>\n</ol>\n<p>If you're concerned about some implicit conversions here we have some added safety because half types cannot be implicitly converted to other types (like float), so unless we're very careful with our (explicit) conversions we will not be able to compile.</p>\n<p>Does that make sense?</p>", "body_text": "Hmm...\nSo before this PR there was reduceOp and reduceAccOp. However, this PR simplifies things so there is now only a single reduceOp that takes in two AccT and returns one. This is because the new pattern is:\n\nAcquire the data.\nConvert it to the accumulate type.\nPerform operations (like modify and reduce and finalize) on it\nConvert the out value to the \"real\" type and write it out\n\nIf you're concerned about some implicit conversions here we have some added safety because half types cannot be implicitly converted to other types (like float), so unless we're very careful with our (explicit) conversions we will not be able to compile.\nDoes that make sense?", "in_reply_to_id": 187787624}