{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187809615", "pull_request_review_id": 119662950, "id": 187809615, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzgwOTYxNQ==", "diff_hunk": "@@ -63,58 +63,68 @@ kernelReduceNoncontigDim_shared(TensorInfo<T, IndexType> out,\n     //for(IndexType i=threadIdx.y; i<reductionSize; i+=blockDim.y){\n     //  local_reg += in[inOffset + i * reductionStride];\n     //}\n-    for(IndexType i=threadIdx.y; i<reductionSize; i+=blockDim.y*4){\n-      if(i + blockDim.y * 3 < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n-        load_reg[2] = modifyOp(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n-        load_reg[3] = modifyOp(in.data[inOffset + (i + blockDim.y * 3) * reductionStride]);\n+    for(IndexType i = threadIdx.y; i < reductionSize; i += blockDim.y * 4) {\n+      if (i + blockDim.y * 3 < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n+        const AccT val2 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+        load_reg[2] = modifyOp(val2);\n+        const AccT val3 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 3) * reductionStride]);\n+        load_reg[3] = modifyOp(val3);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n         local_reg = reduceOp(local_reg, load_reg[2]);\n         local_reg = reduceOp(local_reg, load_reg[3]);\n-      }else if(i + blockDim.y * 2 < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n-        load_reg[2] = modifyOp(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+      } else if (i + blockDim.y * 2 < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n+        const AccT val2 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+        load_reg[2] = modifyOp(val2);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n         local_reg = reduceOp(local_reg, load_reg[2]);\n-      }else if( (i + blockDim.y) < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n+      } else if (i + blockDim.y < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n-      }else if(i + blockDim.y * 0 < reductionSize){\n-        local_reg = reduceOp(local_reg, modifyOp(in.data[inOffset + i * reductionStride]));\n+      } else if (i < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        local_reg = reduceOp(local_reg, modifyOp(val0));\n       }\n     }\n \n     *shmem = local_reg;\n     int dimy = blockDim.y;\n-    while(dimy > 1){\n+    while (dimy > 1) {\n       __syncthreads();\n-      if( threadIdx.y == 0 && (dimy%2 != 0) ){\n-        *shmem = reduceAccOp(*shmem, *(shmem + (dimy-1) * blockDim.x) );\n+      if (threadIdx.y == 0 && (dimy % 2 != 0) ) {\n+        *shmem = reduceOp(*shmem, *(shmem + (dimy - 1) * blockDim.x));", "path": "aten/src/THC/THCReduce.cuh", "position": 103, "original_position": 103, "commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "original_commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "Speaking of implicit conversions, right now no implicit conversions and no half math hinges on a line in aten CMakeLists https://github.com/pytorch/pytorch/blob/f7c9580fd3699f54f48fbf5d0a182265e752ae18/aten/CMakeLists.txt#L138. With all the refactoring and c2 merge going on, it seems very easy to lose this line, or to have some parts of the code that don't use aten's CMakeLists be compiled with implicit conversions. @ezyang, is there a way to make disabling implicit conversions more robust?", "created_at": "2018-05-13T18:32:13Z", "updated_at": "2018-11-23T15:44:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187809615", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7487", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187809615"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187809615"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7487"}}, "body_html": "<p>Speaking of implicit conversions, right now no implicit conversions and no half math hinges on a line in aten CMakeLists <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/f7c9580fd3699f54f48fbf5d0a182265e752ae18/aten/CMakeLists.txt#L138\">pytorch/aten/CMakeLists.txt</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 138\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/f7c9580fd3699f54f48fbf5d0a182265e752ae18\">f7c9580</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L138\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"138\"></td>\n          <td id=\"LC138\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">LIST</span>(<span class=\"pl-k\">APPEND</span> CUDA_NVCC_FLAGS <span class=\"pl-s\">\"-DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__\"</span>) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n. With all the refactoring and c2 merge going on, it seems very easy to lose this line, or to have some parts of the code that don't use aten's CMakeLists be compiled with implicit conversions. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>, is there a way to make disabling implicit conversions more robust?</p>", "body_text": "Speaking of implicit conversions, right now no implicit conversions and no half math hinges on a line in aten CMakeLists \n  \n    \n      pytorch/aten/CMakeLists.txt\n    \n    \n         Line 138\n      in\n      f7c9580\n    \n    \n    \n    \n\n        \n          \n           LIST(APPEND CUDA_NVCC_FLAGS \"-DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__\") \n        \n    \n  \n\n. With all the refactoring and c2 merge going on, it seems very easy to lose this line, or to have some parts of the code that don't use aten's CMakeLists be compiled with implicit conversions. @ezyang, is there a way to make disabling implicit conversions more robust?", "in_reply_to_id": 187787624}