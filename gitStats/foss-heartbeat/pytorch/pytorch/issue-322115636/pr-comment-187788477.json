{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187788477", "pull_request_review_id": 119642337, "id": 187788477, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Nzc4ODQ3Nw==", "diff_hunk": "@@ -16,330 +16,232 @@\n #include <thrust/system/cuda/execution_policy.h>\n #endif\n \n-// Reduction operators that support `half`, unlike Thrust\n-template <typename InT, typename AccT>\n-struct ReduceAdd {\n-  inline __device__ AccT operator()(AccT a, InT b) const {\n-    return a + (AccT) b;\n-  }\n-};\n-\n-#ifdef CUDA_HALF_TENSOR\n-template <>\n-struct ReduceAdd<half, half> {\n-  inline __device__ half operator()(half a, half b) const {\n-#ifdef CUDA_HALF_INSTRUCTIONS\n-    return __hadd(a, b);\n-#else\n-    float fa = __half2float(a);\n-    float fb = __half2float(b);\n-    return __float2half(fa + fb);\n-#endif\n-  }\n-};\n+/*\n+Reductions that (only) operate on accumulate types. \n+*/\n \n-template <>\n-struct ReduceAdd<half, float> {\n-  inline __device__ float operator()(float a, half b) const {\n-    return a + __half2float(b);\n+template <typename T>\n+struct ReduceAdd {\n+  inline __device__ T operator()(const T a, const T b) const {", "path": "aten/src/THC/THCTensorMathReduce.cuh", "position": 35, "original_position": 35, "commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "original_commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "Here, for example, we can see that ReduceAdd now accepts two elements of the same type, and outputs that same type. Historically, however, it would take the input type and the accumulate type. In practice these operations all first \"upconverted\" the input type to the accumulate type. The updated kernels simply do this for them. ", "created_at": "2018-05-13T04:05:43Z", "updated_at": "2018-11-23T15:43:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187788477", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7487", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187788477"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187788477"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7487"}}, "body_html": "<p>Here, for example, we can see that ReduceAdd now accepts two elements of the same type, and outputs that same type. Historically, however, it would take the input type and the accumulate type. In practice these operations all first \"upconverted\" the input type to the accumulate type. The updated kernels simply do this for them.</p>", "body_text": "Here, for example, we can see that ReduceAdd now accepts two elements of the same type, and outputs that same type. Historically, however, it would take the input type and the accumulate type. In practice these operations all first \"upconverted\" the input type to the accumulate type. The updated kernels simply do this for them."}