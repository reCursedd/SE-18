{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388661150", "html_url": "https://github.com/pytorch/pytorch/pull/7487#issuecomment-388661150", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7487", "id": 388661150, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODY2MTE1MA==", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-13T22:37:41Z", "updated_at": "2018-05-13T22:37:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> is correct as usual. To expand slightly, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> you might be concerned about an increase in fp32 operations, since that's what appears to be happening. TensorNormOp is a good example. Previously that had a half specialization that would call THCNumerics::pow and ::mul. This was incorrect behavior, but let's ignore that for a moment. The question is: could it be faster? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a>'s point is <em>no</em>, because when we look at THCNumerics.cuh we see that half mul immediately upconverts both inputs to float, and the same with EVERY OTHER HALF OPERATION (so long as CUDA_HALF_INSTRUCTIONS is false, which as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> pointed out, it is). Worse, EVERY HALF OPERATION also \"downconverted\" back to half.</p>\n<p>So what was actually happening historically is that PyTorch was constantly upconverting and downconverting values, and with this PR it will upconvert once and downconvert once. This prevents the loss of precision from the in media res down conversions. The actual mathematical operations performed are the same.</p>\n<p>Cool, right?</p>", "body_text": "@ngimel is correct as usual. To expand slightly, @ezyang you might be concerned about an increase in fp32 operations, since that's what appears to be happening. TensorNormOp is a good example. Previously that had a half specialization that would call THCNumerics::pow and ::mul. This was incorrect behavior, but let's ignore that for a moment. The question is: could it be faster? @ngimel's point is no, because when we look at THCNumerics.cuh we see that half mul immediately upconverts both inputs to float, and the same with EVERY OTHER HALF OPERATION (so long as CUDA_HALF_INSTRUCTIONS is false, which as @ngimel pointed out, it is). Worse, EVERY HALF OPERATION also \"downconverted\" back to half.\nSo what was actually happening historically is that PyTorch was constantly upconverting and downconverting values, and with this PR it will upconvert once and downconvert once. This prevents the loss of precision from the in media res down conversions. The actual mathematical operations performed are the same.\nCool, right?", "body": "@ngimel is correct as usual. To expand slightly, @ezyang you might be concerned about an increase in fp32 operations, since that's what appears to be happening. TensorNormOp is a good example. Previously that had a half specialization that would call THCNumerics<half>::pow and ::mul. This was incorrect behavior, but let's ignore that for a moment. The question is: could it be faster? @ngimel's point is *no*, because when we look at THCNumerics.cuh we see that half mul immediately upconverts both inputs to float, and the same with EVERY OTHER HALF OPERATION (so long as CUDA_HALF_INSTRUCTIONS is false, which as @ngimel pointed out, it is). Worse, EVERY HALF OPERATION also \"downconverted\" back to half. \r\n\r\nSo what was actually happening historically is that PyTorch was constantly upconverting and downconverting values, and with this PR it will upconvert once and downconvert once. This prevents the loss of precision from the in media res down conversions. The actual mathematical operations performed are the same. \r\n\r\nCool, right? \r\n"}