{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388648630", "html_url": "https://github.com/pytorch/pytorch/pull/7487#issuecomment-388648630", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7487", "id": 388648630, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODY0ODYzMA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-13T19:05:36Z", "updated_at": "2018-05-13T19:05:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>, in cuda/THC the only type for which AccumT is different from T is half. Accumulation in double used to happen only for doubles, and still happens only for doubles, so unless I'm missing something, there's no increase in double-precision operations. As for halfs, since no operation was properly defined on halfs anyway, they all happened via conversion to float. If anything, this PR should reduce the number of conversions.</p>", "body_text": "@ezyang, in cuda/THC the only type for which AccumT is different from T is half. Accumulation in double used to happen only for doubles, and still happens only for doubles, so unless I'm missing something, there's no increase in double-precision operations. As for halfs, since no operation was properly defined on halfs anyway, they all happened via conversion to float. If anything, this PR should reduce the number of conversions.", "body": "@ezyang, in cuda/THC the only type for which AccumT is different from T is half. Accumulation in double used to happen only for doubles, and still happens only for doubles, so unless I'm missing something, there's no increase in double-precision operations. As for halfs, since no operation was properly defined on halfs anyway, they all happened via conversion to float. If anything, this PR should reduce the number of conversions. "}