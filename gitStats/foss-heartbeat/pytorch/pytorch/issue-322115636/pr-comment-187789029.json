{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187789029", "pull_request_review_id": 119643011, "id": 187789029, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Nzc4OTAyOQ==", "diff_hunk": "@@ -63,58 +63,68 @@ kernelReduceNoncontigDim_shared(TensorInfo<T, IndexType> out,\n     //for(IndexType i=threadIdx.y; i<reductionSize; i+=blockDim.y){\n     //  local_reg += in[inOffset + i * reductionStride];\n     //}\n-    for(IndexType i=threadIdx.y; i<reductionSize; i+=blockDim.y*4){\n-      if(i + blockDim.y * 3 < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n-        load_reg[2] = modifyOp(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n-        load_reg[3] = modifyOp(in.data[inOffset + (i + blockDim.y * 3) * reductionStride]);\n+    for(IndexType i = threadIdx.y; i < reductionSize; i += blockDim.y * 4) {\n+      if (i + blockDim.y * 3 < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n+        const AccT val2 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+        load_reg[2] = modifyOp(val2);\n+        const AccT val3 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 3) * reductionStride]);\n+        load_reg[3] = modifyOp(val3);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n         local_reg = reduceOp(local_reg, load_reg[2]);\n         local_reg = reduceOp(local_reg, load_reg[3]);\n-      }else if(i + blockDim.y * 2 < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n-        load_reg[2] = modifyOp(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+      } else if (i + blockDim.y * 2 < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n+        const AccT val2 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y * 2) * reductionStride]);\n+        load_reg[2] = modifyOp(val2);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n         local_reg = reduceOp(local_reg, load_reg[2]);\n-      }else if( (i + blockDim.y) < reductionSize){\n-        load_reg[0] = modifyOp(in.data[inOffset + (i + blockDim.y * 0) * reductionStride]);\n-        load_reg[1] = modifyOp(in.data[inOffset + (i + blockDim.y * 1) * reductionStride]);\n+      } else if (i + blockDim.y < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        load_reg[0] = modifyOp(val0);\n+        const AccT val1 = scalar_cast<AccT>(in.data[inOffset + (i + blockDim.y) * reductionStride]);\n+        load_reg[1] = modifyOp(val1);\n         local_reg = reduceOp(local_reg, load_reg[0]);\n         local_reg = reduceOp(local_reg, load_reg[1]);\n-      }else if(i + blockDim.y * 0 < reductionSize){\n-        local_reg = reduceOp(local_reg, modifyOp(in.data[inOffset + i * reductionStride]));\n+      } else if (i < reductionSize) {\n+        const AccT val0 = scalar_cast<AccT>(in.data[inOffset + i * reductionStride]);\n+        local_reg = reduceOp(local_reg, modifyOp(val0));\n       }\n     }\n \n     *shmem = local_reg;\n     int dimy = blockDim.y;\n-    while(dimy > 1){\n+    while (dimy > 1) {\n       __syncthreads();\n-      if( threadIdx.y == 0 && (dimy%2 != 0) ){\n-        *shmem = reduceAccOp(*shmem, *(shmem + (dimy-1) * blockDim.x) );\n+      if (threadIdx.y == 0 && (dimy % 2 != 0) ) {\n+        *shmem = reduceOp(*shmem, *(shmem + (dimy - 1) * blockDim.x));", "path": "aten/src/THC/THCReduce.cuh", "position": 103, "original_position": 103, "commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "original_commit_id": "bcac88dd70acbbbfffa5177228745f8daad589a9", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "Oh I see. We could change the name, but I think that'd be verbose. The advantage is that it would remind you that you were operating on the accum type (good), but that's the standard and ModifyOp and FinalizeOp both also work on the accum type. So then we'd have to change them to be ModifyAccOp and FinalizeAccOp, and maybe change from type T to be type AccT everywhere we expected the accum type vs. any type, etc.\r\n\r\nI'll do it if you like but if we're worried about clarifying what the operators are working on then a comment may be better. I put a brief one in THCTensorMathReduce.cuh, but could elaborate on it and/or add comments elsewhere, too. \r\n\r\n", "created_at": "2018-05-13T05:07:44Z", "updated_at": "2018-11-23T15:44:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187789029", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7487", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187789029"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7487#discussion_r187789029"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7487"}}, "body_html": "<p>Oh I see. We could change the name, but I think that'd be verbose. The advantage is that it would remind you that you were operating on the accum type (good), but that's the standard and ModifyOp and FinalizeOp both also work on the accum type. So then we'd have to change them to be ModifyAccOp and FinalizeAccOp, and maybe change from type T to be type AccT everywhere we expected the accum type vs. any type, etc.</p>\n<p>I'll do it if you like but if we're worried about clarifying what the operators are working on then a comment may be better. I put a brief one in THCTensorMathReduce.cuh, but could elaborate on it and/or add comments elsewhere, too.</p>", "body_text": "Oh I see. We could change the name, but I think that'd be verbose. The advantage is that it would remind you that you were operating on the accum type (good), but that's the standard and ModifyOp and FinalizeOp both also work on the accum type. So then we'd have to change them to be ModifyAccOp and FinalizeAccOp, and maybe change from type T to be type AccT everywhere we expected the accum type vs. any type, etc.\nI'll do it if you like but if we're worried about clarifying what the operators are working on then a comment may be better. I put a brief one in THCTensorMathReduce.cuh, but could elaborate on it and/or add comments elsewhere, too.", "in_reply_to_id": 187787624}