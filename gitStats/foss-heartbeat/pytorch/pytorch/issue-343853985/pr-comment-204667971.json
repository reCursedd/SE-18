{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204667971", "pull_request_review_id": 139781933, "id": 204667971, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDY2Nzk3MQ==", "diff_hunk": "@@ -16,222 +17,177 @@ namespace torch { namespace jit {\n \n namespace {\n \n-namespace onnx = ::torch::onnx;\n+namespace onnx_torch = ::torch::onnx;\n+namespace onnx = ::ONNX_NAMESPACE;\n \n-std::string value_name(Value* n) {\n-  return n->uniqueName();\n+std::string getNodeStackTraceString(const Node* n) {\n+  std::stringstream ss;\n+  if (n->getSourceLocation()) {\n+    n->getSourceLocation()->highlight(ss);\n+  } else {\n+    ss << \"<unknown location>\";\n+  }\n+  return ss.str();\n }\n \n-struct ExportContext {\n-  size_t num_blocks = 0;\n-  onnx::OperatorExportTypes operator_export_type;\n-};\n-\n-void encodeGraph(onnx::GraphProto * p_g, const std::shared_ptr<Graph> & g,\n-                 const std::vector<at::Tensor> & initializers,\n-                 ExportContext *ctx, RawDataExportMap* raw_data_export_map=nullptr);\n+void validateGraph(const std::shared_ptr<Graph>& graph, onnx_torch::OperatorExportTypes operator_export_type) {\n+  for (auto node : graph->nodes()) {\n+      // Macro'ed so we get a marginally better line number on failed export\n+#define FAIL_EXPORT(name) \\\n+      throw std::runtime_error(std::string(\"ONNX export failed: \") + name + \"\\n\\nGraph we tried to export:\\n\" + graph->toString());\n+    IR_IF(node, PythonOp)\n+      auto py_node = static_cast<torch::jit::PythonOp*>(value);\n+      FAIL_EXPORT(\n+          \"Couldn't export Python operator \" + py_node->name() +\n+          \"\\n\\nDefined at:\\n\" + getNodeStackTraceString(node))\n+    IR_ELSE()\n+      // Special error messages for certain types of operators\n+      if (node->kind() == aten::expand) {\n+        FAIL_EXPORT(\n+            \"Could not export a broadcasted operation; ONNX likely does not support this form of broadcasting.\\n\\nBroadcast occurred at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+      if (node->kind() == prim::PackPadded || node->kind() == prim::PadPacked) {\n+        FAIL_EXPORT(\n+            \"Cannot export individual pack_padded_sequence or pad_packed_sequence; these operations must occur in pairs.\\n\\nUsage of this operation occurred at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+      bool is_aten_fallback = operator_export_type == onnx_torch::OperatorExportTypes::ONNX_ATEN_FALLBACK;\n+      if (!node->kind().is_onnx() && !is_aten_fallback && node->kind() != prim::Undefined) {\n+        FAIL_EXPORT(\n+            \"Couldn't export operator \" + node->kind().toDisplayString() + \"\\n\\nDefined at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+    IR_END()\n+#undef FAIL_EXPORT\n+  }\n+}\n \n-void encodeBlock(onnx::GraphProto * p_g, Block *b,\n-                const std::vector<at::Tensor> & initializers,\n-                ExportContext *ctx, RawDataExportMap* raw_data_export_map);\n+class JitEncoder {\n+ public:\n+  JitEncoder(onnx::ModelProto *model_proto,\n+             int64_t onnx_opset_version,\n+             onnx_torch::OperatorExportTypes operator_export_type,\n+             bool defer_weight_export = false);\n \n-void encodeTensor(onnx::TensorProto * p, const at::Tensor & tensor,\n-                  at::optional<std::string> external_ref={},\n-                  RawDataExportMap* raw_data_export_map = nullptr) {\n-  for(auto d : tensor.sizes()) {\n-    p->add_dims(d);\n+  RawDataExportMap get_raw_data_export_map() {\n+    return raw_data_export_map_;\n   }\n-  onnx::DataType onnx_type;\n-  // Most integral types and float16 need to be serialized as int32\n-  at::ScalarType cast_type = tensor.type().scalarType();\n-  switch(tensor.type().scalarType()) {\n+\n+ protected:\n+  void EncodeGraph(onnx::GraphProto *graph_proto,\n+                   const std::shared_ptr<Graph> &graph,\n+                   const std::vector<at::Tensor> &initializers = {});\n+\n+  void EncodeBlock(onnx::GraphProto *graph_proto,\n+                   const Block *block,\n+                   const std::vector<at::Tensor> &initializers = {});\n+\n+  virtual void EncodeTensor(onnx::TensorProto *tensor_proto,\n+                    const at::Tensor &tensor,\n+                    const at::optional<std::string> external_ref = {});\n+\n+  void AddAttribute(onnx::NodeProto *node_proto, const jit::Node *node, const jit::Symbol name);\n+\n+  size_t num_blocks_;\n+  bool defer_weight_export_;\n+  onnx_torch::OperatorExportTypes operator_export_type_;\n+  RawDataExportMap raw_data_export_map_;\n+};\n+\n+onnx::TensorProto_DataType ATenTypeToOnnxType(at::ScalarType at_type) {\n+  switch(at_type) {\n     case at::kDouble:\n-      onnx_type = onnx::kDOUBLE;\n-      break;\n+      return onnx::TensorProto_DataType_DOUBLE;\n     case at::kFloat:\n-      onnx_type = onnx::kFLOAT;\n-      break;\n+      return onnx::TensorProto_DataType_FLOAT;\n     case at::kHalf:\n-      onnx_type = onnx::kFLOAT16;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_FLOAT16;\n     case at::kByte:\n+      return onnx::TensorProto_DataType_UINT8;\n     case at::kChar:\n-      onnx_type = onnx::kINT8;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_INT8;\n     case at::kShort:\n-      onnx_type = onnx::kINT16;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_INT16;\n     case at::kInt:\n-      onnx_type = onnx::kINT32;\n-      break;\n+      return onnx::TensorProto_DataType_INT32;\n     case at::kLong:\n-      onnx_type = onnx::kINT64;\n-      break;\n+      return onnx::TensorProto_DataType_INT64;\n     default:\n       torch::barf(\"unexpected tensor scalar type\");\n-      break;\n-  }\n-  p->set_data_type(onnx_type);\n-  // CPU's HalfTensor doesn't have contiguous(), so first calling contiguous()\n-  auto t = tensor.contiguous().toBackend(at::kCPU).toType(cast_type);\n-  // Add a buffer to the raw_data_export_map for the caller to dump into an\n-  // external data store. If external_ref is not specified, we instead dump\n-  // the contiguous data into the protobuf itself\n-  if (external_ref) {\n-    // For now, we use the name of the tensor as the external lookup name to\n-    // avoid ONNX protobuf changes.\n-    JIT_ASSERT(external_ref.value() == p->get_name());\n-    JIT_ASSERT(raw_data_export_map != nullptr);\n-    JIT_ASSERT(raw_data_export_map->count(external_ref.value()) == 0);\n-    (*raw_data_export_map)[external_ref.value()] = t;\n-    p->set_external_data_present();\n-  } else {\n-    p->set_raw_data(t);\n   }\n }\n \n-void addAttribute(onnx::NodeProto * n_p, jit::Node * n, jit::Symbol name, ExportContext *ctx) {\n-  auto attr = n_p->add_attribute();\n-  JIT_ASSERT(name.is_attr());\n-  attr->set_name(name.toUnqualString());\n-  switch(n->kindOf(name)) {\n-    case AttributeKind::f:\n-      attr->set_f(n->f(name));\n-      attr->set_type(onnx::aFLOAT);\n-      break;\n-    case AttributeKind::fs:\n-      attr->set_type(onnx::aFLOATS);\n-      for(auto & v : n->fs(name))\n-        attr->add_floats(v);\n-      break;\n-    case AttributeKind::i:\n-      attr->set_type(onnx::aINT);\n-      attr->set_i(n->i(name));\n-      break;\n-    case AttributeKind::is:\n-      attr->set_type(onnx::aINTS);\n-      for(auto & v : n->is(name))\n-        attr->add_ints(v);\n-      break;\n-    case AttributeKind::s:\n-      attr->set_type(onnx::aSTRING);\n-      attr->set_s(n->s(name));\n-      break;\n-    case AttributeKind::ss:\n-      attr->set_type(onnx::aSTRINGS);\n-      for(auto & v : n->ss(name))\n-        attr->add_strings(v);\n-      break;\n-    case AttributeKind::t: {\n-      attr->set_type(onnx::aTENSOR);\n-      auto t = attr->mutable_t();\n-      encodeTensor(t, n->t(name));\n-    } break;\n-    case AttributeKind::ts:\n-      attr->set_type(onnx::aTENSORS);\n-      for(auto & v : n->ts(name)) {\n-        auto t = attr->add_tensors();\n-        encodeTensor(t, v);\n-      }\n-      break;\n-    case AttributeKind::g: {\n-      attr->set_type(onnx::aGRAPH);\n-      auto g = attr->mutable_g();\n-      encodeGraph(g, n->g(name), {}, ctx, nullptr);\n-    } break;\n-    case AttributeKind::gs:\n-      attr->set_type(onnx::aGRAPHS);\n-      for(auto & v : n->gs(name)) {\n-        auto g = attr->add_graphs();\n-        encodeGraph(g, v, {}, ctx, nullptr);\n-      }\n-      break;\n-  }\n-}\n-\n-void encodeTypeProtoTensorType(onnx::TypeProtoTensor* tensor_type, Value* n) {\n+void EncodeTypeProtoTensorType(onnx::TypeProto_Tensor* tensor_type, const Value* n) {\n   onnx::TensorShapeProto* shape = tensor_type->mutable_shape();\n   if (TensorType* node_type = n->type()->cast<TensorType>()) {\n     const std::vector<std::int64_t>& sizes = node_type->sizes();\n-    for (std::int64_t s : sizes) {\n-      shape->add_dim(s);\n-    }\n-    onnx::DataType onnx_type;\n-    switch(node_type->scalarType()) {\n-      case at::kDouble:\n-        onnx_type = onnx::kDOUBLE;\n-        break;\n-      case at::kFloat:\n-        onnx_type = onnx::kFLOAT;\n-        break;\n-      case at::kHalf:\n-        onnx_type = onnx::kFLOAT16;\n-        break;\n-      case at::kByte:\n-      case at::kChar:\n-        onnx_type = onnx::kINT8;\n-        break;\n-      case at::kShort:\n-        onnx_type = onnx::kINT16;\n-        break;\n-      case at::kInt:\n-        onnx_type = onnx::kINT32;\n-        break;\n-      case at::kLong:\n-        onnx_type = onnx::kINT64;\n-        break;\n-      default:\n-        torch::barf(\"unexpected tensor scalar type\");\n-        break;\n+    for (size_t i = 0; i < sizes.size(); i++) {\n+      shape->add_dim();\n+      shape->mutable_dim(i)->set_dim_value(sizes[i]);\n     }\n-    tensor_type->set_data_type(onnx_type);\n+    tensor_type->set_elem_type(ATenTypeToOnnxType(node_type->scalarType()));\n   }\n }\n \n-void encodeValueInfo(onnx::ValueInfoProto* v, Value* n) {\n-  v->set_name(value_name(n));\n+void EncodeValueInfo(onnx::ValueInfoProto* v, const Value* n) {\n+  v->set_name(n->uniqueName());\n   onnx::TypeProto* t = v->mutable_type();\n-  onnx::TypeProtoTensor* tensor_type = t->mutable_tensor_type();\n-  encodeTypeProtoTensorType(tensor_type, n);\n+  onnx::TypeProto_Tensor* tensor_type = t->mutable_tensor_type();\n+  EncodeTypeProtoTensorType(tensor_type, n);\n+}\n+\n+JitEncoder::JitEncoder(\n+    onnx::ModelProto *model_proto,\n+    int64_t onnx_opset_version,\n+    onnx_torch::OperatorExportTypes operator_export_type,\n+    bool defer_weight_export)\n+    : num_blocks_(0),\n+      defer_weight_export_(defer_weight_export),\n+      operator_export_type_(operator_export_type) {\n+  model_proto->set_producer_name(\"pytorch\");\n+  model_proto->set_ir_version(3);", "path": "torch/csrc/jit/export.cpp", "position": null, "original_position": 295, "commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "original_commit_id": "dfe899efbacf40f945fd85e4d49e3d0748317f4d", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "there should be constant called IR_VERSION", "created_at": "2018-07-24T08:33:36Z", "updated_at": "2018-11-23T15:47:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/9746#discussion_r204667971", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9746", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204667971"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9746#discussion_r204667971"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9746"}}, "body_html": "<p>there should be constant called IR_VERSION</p>", "body_text": "there should be constant called IR_VERSION"}