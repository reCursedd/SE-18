{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204925080", "pull_request_review_id": 140034348, "id": 204925080, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDkyNTA4MA==", "diff_hunk": "@@ -292,85 +239,314 @@ void encodeBlock(onnx::GraphProto * p_g, Block *b,\n       true_branch->set_name(\"then_branch\");\n       true_branch->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n       auto true_g = true_branch->mutable_g();\n-      encodeBlock(true_g, node->blocks()[0], {}, ctx, raw_data_export_map);\n+      EncodeBlock(true_g, node->blocks()[0]);\n \n       auto false_branch = p_n->add_attribute();\n       false_branch->set_name(\"else_branch\");\n       false_branch->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n       auto false_g = false_branch->mutable_g();\n-      encodeBlock(false_g, node->blocks()[1], {}, ctx, raw_data_export_map);\n+      EncodeBlock(false_g, node->blocks()[1]);\n     }\n   }\n   auto num_initializers = initializers.size();\n-  JIT_ASSERT(b->inputs().size() >= num_initializers);\n-  size_t inputs_count = b->inputs().size() - num_initializers;\n+  JIT_ASSERT(block->inputs().size() >= num_initializers);\n+  size_t inputs_count = block->inputs().size() - num_initializers;\n   for (auto & tensor : initializers) {\n     // TODO: stop using positions to determine which initializers\n     // match to which inputs\n-    std::string name = p_g->input(inputs_count++).name();\n-    auto p = p_g->add_initializer();\n+    std::string name = graph_proto->input(inputs_count++).name();\n+    auto p = graph_proto->add_initializer();\n     p->set_name(name);\n-    if (raw_data_export_map) {\n-      encodeTensor(p, tensor, name, raw_data_export_map);\n-    } else {\n-      encodeTensor(p, tensor, {});\n-    }\n+    EncodeTensor(p, tensor, name);\n   }\n }\n \n-void encodeModel(onnx::ModelProto* p_m, const std::shared_ptr<Graph>& g,\n-                 const std::vector<at::Tensor>& initializers,\n-                 RawDataExportMap* raw_data_export_map = nullptr,\n-                 onnx_torch::OperatorExportTypes operator_export_type\n-                   = onnx_torch::OperatorExportTypes::ONNX) {\n-  onnx::GraphProto* p_g = p_m->mutable_graph();\n-  ExportContext ctx;\n-  ctx.operator_export_type = operator_export_type;\n-  encodeGraph(p_g, g, initializers, &ctx, raw_data_export_map);\n+void JitEncoder::AddAttribute(onnx::NodeProto *node_proto, const jit::Node *node, const jit::Symbol name) {\n+  auto attr = node_proto->add_attribute();\n+  JIT_ASSERT(name.is_attr());\n+  attr->set_name(name.toUnqualString());\n+  switch(node->kindOf(name)) {\n+    case AttributeKind::f:\n+      attr->set_f(node->f(name));\n+      attr->set_type(onnx::AttributeProto_AttributeType_FLOAT);\n+      break;\n+    case AttributeKind::fs:\n+      attr->set_type(onnx::AttributeProto_AttributeType_FLOATS);\n+      for(auto & v : node->fs(name))\n+        attr->add_floats(v);\n+      break;\n+    case AttributeKind::i:\n+      attr->set_type(onnx::AttributeProto_AttributeType_INT);\n+      attr->set_i(node->i(name));\n+      break;\n+    case AttributeKind::is:\n+      attr->set_type(onnx::AttributeProto_AttributeType_INTS);\n+      for(auto & v : node->is(name))\n+        attr->add_ints(v);\n+      break;\n+    case AttributeKind::s:\n+      attr->set_type(onnx::AttributeProto_AttributeType_STRING);\n+      attr->set_s(node->s(name));\n+      break;\n+    case AttributeKind::ss:\n+      attr->set_type(onnx::AttributeProto_AttributeType_STRINGS);\n+      for(auto & v : node->ss(name))\n+        attr->add_strings(v);\n+      break;\n+    case AttributeKind::t: {\n+      attr->set_type(onnx::AttributeProto_AttributeType_TENSOR);\n+      auto t = attr->mutable_t();\n+      EncodeTensor(t, node->t(name));\n+    } break;\n+    case AttributeKind::ts:\n+      attr->set_type(onnx::AttributeProto_AttributeType_TENSORS);\n+      for(auto & v : node->ts(name)) {\n+        auto t = attr->add_tensors();\n+        EncodeTensor(t, v);\n+      }\n+      break;\n+    case AttributeKind::g: {\n+      attr->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n+      auto g = attr->mutable_g();\n+      EncodeGraph(g, node->g(name));\n+    } break;\n+    case AttributeKind::gs:\n+      attr->set_type(onnx::AttributeProto_AttributeType_GRAPHS);\n+      for(auto & v : node->gs(name)) {\n+        auto g = attr->add_graphs();\n+        EncodeGraph(g, v);\n+      }\n+      break;\n+  }\n }\n \n-namespace {\n-std::string getNodeStackTraceString(Node* n) {\n-  std::stringstream ss;\n-  if (n->getSourceLocation()) {\n-    n->getSourceLocation()->highlight(ss);\n+void JitEncoder::EncodeTensor(\n+    onnx::TensorProto *tensor_proto,\n+    const at::Tensor &tensor,\n+    const at::optional<std::string> external_ref) {\n+  for(auto d : tensor.sizes()) {\n+    tensor_proto->add_dims(d);\n+  }\n+  tensor_proto->set_data_type(ATenTypeToOnnxType(tensor.type().scalarType()));\n+  // CPU's HalfTensor doesn't have contiguous(), so first calling contiguous()\n+  auto t = tensor.contiguous().toBackend(at::kCPU);\n+  // Add a buffer to the raw_data_export_map for the caller to dump into an\n+  // external data store. If external_ref is not specified, we instead dump\n+  // the contiguous data into the protobuf itself\n+  if (defer_weight_export_) {\n+    // For now, we use the name of the tensor as the external lookup name to\n+    // avoid ONNX protobuf changes.\n+    JIT_ASSERT(external_ref.value() == tensor_proto->name());\n+    JIT_ASSERT(raw_data_export_map_.count(external_ref.value()) == 0);\n+    raw_data_export_map_[external_ref.value()] = t;\n+    tensor_proto->set_raw_data(\"__EXTERNAL\");\n   } else {\n-    ss << \"<unknown location>\";\n+    JIT_ASSERT(t.is_contiguous());\n+    tensor_proto->set_raw_data(std::string(static_cast<char*>(t.data_ptr()),  t.type().elementSizeInBytes() * t.numel()));\n   }\n-  return ss.str();\n }\n-} // namespace\n \n-void validateGraph(const std::shared_ptr<Graph>& graph, onnx_torch::OperatorExportTypes operator_export_type) {\n-  for (auto node : graph->nodes()) {\n-      // Macro'ed so we get a marginally better line number on failed export\n-#define FAIL_EXPORT(name) \\\n-      throw std::runtime_error(std::string(\"ONNX export failed: \") + name + \"\\n\\nGraph we tried to export:\\n\" + graph->toString());\n+class GraphEncoder: public JitEncoder {\n+ public:\n+  GraphEncoder(onnx::ModelProto *model_proto,\n+               const std::shared_ptr<Graph> &graph,\n+               int64_t onnx_opset_version,\n+               onnx_torch::OperatorExportTypes operator_export_type,\n+               const std::vector<at::Tensor> &initializers,\n+               bool defer_weight_export);\n+\n+};\n+\n+GraphEncoder::GraphEncoder(\n+    onnx::ModelProto *model_proto,\n+    const std::shared_ptr<Graph> &graph,\n+    int64_t onnx_opset_version,\n+    onnx_torch::OperatorExportTypes operator_export_type,\n+    const std::vector<at::Tensor> &initializers,\n+    bool defer_weight_export)\n+    : JitEncoder(model_proto, onnx_opset_version, operator_export_type, defer_weight_export) {\n+  if (operator_export_type != onnx_torch::OperatorExportTypes::RAW) {\n+    validateGraph(graph, operator_export_type);\n+  }\n+\n+  EncodeGraph(model_proto->mutable_graph(), graph, initializers);\n+}\n+\n+class ModuleEncoder: public JitEncoder {\n+ public:\n+  ModuleEncoder(onnx::ModelProto *model_proto,\n+                const std::shared_ptr<script::Module> &module,\n+                int64_t onnx_opset_version,\n+                onnx_torch::OperatorExportTypes operator_export_type);\n+\n+ private:\n+  void EncodeModule(onnx::GraphProto *graph_proto, const std::shared_ptr<script::Module> &module);\n+\n+  void EncodeParameters(onnx::GraphProto *graph_proto,\n+                        const std::shared_ptr<script::Module> &module,\n+                        const std::string prefix);\n+\n+  void EncodeParameter(onnx::TensorProto *tensor_proto,\n+                       const script::NamedParameter &parameter,\n+                       const std::string prefix);\n+\n+  void EncodeMethods(onnx::GraphProto *graph_proto,\n+                     const std::shared_ptr<script::Module> &module,\n+                     const std::string prefix);\n+\n+  void EncodeMethod(onnx::NodeProto *node_proto,\n+                    const std::unique_ptr<script::Method> &method,\n+                    const std::string prefix);\n+\n+  void EncodeTensor(onnx::TensorProto *tensor_proto,\n+                    const at::Tensor &tensor,\n+                    const at::optional<std::string> external_ref);\n+\n+  // Used to deduplicate tensor storages\n+  std::unordered_map<const void*, std::string> storage_dedup_map_;\n+\n+  // Used to keep track of Parameter names so Methods can refer to them\n+  std::unordered_map<at::Tensor*, std::string> parameter_map_;\n+\n+  // Used to create sequential tensor storage names\n+  size_t storage_counter_ = 0;\n+};\n+\n+ModuleEncoder::ModuleEncoder(\n+    onnx::ModelProto *model_proto,\n+    const std::shared_ptr<script::Module> &module,\n+    int64_t onnx_opset_version,\n+    onnx_torch::OperatorExportTypes operator_export_type)\n+    : JitEncoder(model_proto, onnx_opset_version, operator_export_type,\n+                 /*defer_weight_export*/ true) {\n+  EncodeModule(model_proto->mutable_graph(), module);\n+}\n+\n+void ModuleEncoder::EncodeModule(\n+    onnx::GraphProto *graph_proto,\n+    const std::shared_ptr<script::Module> &module) {\n+  EncodeParameters(graph_proto, module, \"\");\n+  EncodeMethods(graph_proto, module, \"\");\n+}\n+\n+void ModuleEncoder::EncodeParameters(\n+    onnx::GraphProto *graph_proto,\n+    const std::shared_ptr<script::Module> &module,\n+    const std::string prefix) {\n+  // Encode each parameter as a initializer in the proto\n+  for (auto &parameter : module->get_parameters()) {\n+    auto tensor_proto = graph_proto->add_initializer();\n+    EncodeParameter(tensor_proto, parameter.value, prefix);\n+  }\n+\n+  for (auto &submodule : module->get_modules()) {\n+    EncodeParameters(graph_proto, submodule.value.module, prefix + submodule.key + \".\");\n+  }\n+}\n+\n+void ModuleEncoder::EncodeParameter(\n+    onnx::TensorProto *tensor_proto,\n+    const script::NamedParameter &parameter,\n+    const std::string prefix) {\n+  auto tensor = parameter.slot();\n+\n+  // Name will be prefixed by submodule. e.g. submodule_foo.parameter_bar\n+  tensor_proto->set_name(prefix + parameter.name);\n+  parameter_map_[tensor] = tensor_proto->name();\n+\n+  for (auto &d : tensor->sizes()) {\n+    tensor_proto->add_dims(d);\n+  }\n+  tensor_proto->set_data_type(ATenTypeToOnnxType(tensor->type().scalarType()));\n+\n+  // The int64_data field stores requires_grad, is_buffer, offset, and strides\n+  tensor_proto->add_int64_data(tensor->requires_grad());\n+  tensor_proto->add_int64_data(parameter.is_buffer);\n+  tensor_proto->add_int64_data(tensor->storage_offset());\n+  for (auto &d : tensor->strides()) {\n+    tensor_proto->add_int64_data(d);\n+  }\n+\n+  auto storage_ptr = tensor->data_ptr();", "path": "torch/csrc/jit/export.cpp", "position": null, "original_position": 661, "commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "original_commit_id": "dfe899efbacf40f945fd85e4d49e3d0748317f4d", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "`data_ptr()` is not what you want there. `data_ptr == start_of_storage + storage_offset()`. You want the `start_of_storage` pointer.  You can get that out of the storage itself.", "created_at": "2018-07-24T21:58:14Z", "updated_at": "2018-11-23T15:48:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/9746#discussion_r204925080", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9746", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204925080"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9746#discussion_r204925080"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9746"}}, "body_html": "<p><code>data_ptr()</code> is not what you want there. <code>data_ptr == start_of_storage + storage_offset()</code>. You want the <code>start_of_storage</code> pointer.  You can get that out of the storage itself.</p>", "body_text": "data_ptr() is not what you want there. data_ptr == start_of_storage + storage_offset(). You want the start_of_storage pointer.  You can get that out of the storage itself."}