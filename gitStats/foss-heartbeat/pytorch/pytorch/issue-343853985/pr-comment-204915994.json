{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204915994", "pull_request_review_id": 140034348, "id": 204915994, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDkxNTk5NA==", "diff_hunk": "@@ -20,227 +20,174 @@ namespace {\n namespace onnx_torch = ::torch::onnx;\n namespace onnx = ::ONNX_NAMESPACE;\n \n-std::string value_name(Value* n) {\n-  return n->uniqueName();\n+std::string getNodeStackTraceString(const Node* n) {\n+  std::stringstream ss;\n+  if (n->getSourceLocation()) {\n+    n->getSourceLocation()->highlight(ss);\n+  } else {\n+    ss << \"<unknown location>\";\n+  }\n+  return ss.str();\n }\n \n-struct ExportContext {\n-  size_t num_blocks = 0;\n-  onnx_torch::OperatorExportTypes operator_export_type;\n-};\n-\n-void encodeGraph(onnx::GraphProto * p_g, const std::shared_ptr<Graph> & g,\n-                 const std::vector<at::Tensor> & initializers,\n-                 ExportContext *ctx, RawDataExportMap* raw_data_export_map=nullptr);\n+void validateGraph(const std::shared_ptr<Graph>& graph, onnx_torch::OperatorExportTypes operator_export_type) {\n+  for (auto node : graph->nodes()) {\n+      // Macro'ed so we get a marginally better line number on failed export\n+#define FAIL_EXPORT(name) \\\n+      throw std::runtime_error(std::string(\"ONNX export failed: \") + name + \"\\n\\nGraph we tried to export:\\n\" + graph->toString());\n+    IR_IF(node, PythonOp)\n+      auto py_node = static_cast<torch::jit::PythonOp*>(value);\n+      FAIL_EXPORT(\n+          \"Couldn't export Python operator \" + py_node->name() +\n+          \"\\n\\nDefined at:\\n\" + getNodeStackTraceString(node))\n+    IR_ELSE()\n+      // Special error messages for certain types of operators\n+      if (node->kind() == aten::expand) {\n+        FAIL_EXPORT(\n+            \"Could not export a broadcasted operation; ONNX likely does not support this form of broadcasting.\\n\\nBroadcast occurred at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+      if (node->kind() == prim::PackPadded || node->kind() == prim::PadPacked) {\n+        FAIL_EXPORT(\n+            \"Cannot export individual pack_padded_sequence or pad_packed_sequence; these operations must occur in pairs.\\n\\nUsage of this operation occurred at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+      bool is_aten_fallback = operator_export_type == onnx_torch::OperatorExportTypes::ONNX_ATEN_FALLBACK;\n+      if (!node->kind().is_onnx() && !is_aten_fallback && node->kind() != prim::Undefined) {\n+        FAIL_EXPORT(\n+            \"Couldn't export operator \" + node->kind().toDisplayString() + \"\\n\\nDefined at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+    IR_END()\n+#undef FAIL_EXPORT\n+  }\n+}\n \n-void encodeBlock(onnx::GraphProto * p_g, Block *b,\n-                const std::vector<at::Tensor> & initializers,\n-                ExportContext *ctx, RawDataExportMap* raw_data_export_map);\n+class JitEncoder {\n+ public:\n+  JitEncoder(onnx::ModelProto *model_proto,\n+             int64_t onnx_opset_version,\n+             onnx_torch::OperatorExportTypes operator_export_type,\n+             bool defer_weight_export = false);\n \n-void encodeTensor(onnx::TensorProto * p, const at::Tensor & tensor,\n-                  at::optional<std::string> external_ref={},\n-                  RawDataExportMap* raw_data_export_map = nullptr) {\n-  for(auto d : tensor.sizes()) {\n-    p->add_dims(d);\n+  RawDataExportMap get_raw_data_export_map() {\n+    return raw_data_export_map_;\n   }\n-  onnx::TensorProto_DataType onnx_type;\n-  // Most integral types and float16 need to be serialized as int32\n-  at::ScalarType cast_type = tensor.type().scalarType();\n-  switch(tensor.type().scalarType()) {\n+\n+ protected:\n+  void EncodeGraph(onnx::GraphProto *graph_proto,\n+                   const std::shared_ptr<Graph> &graph,\n+                   const std::vector<at::Tensor> &initializers = {});\n+\n+  void EncodeBlock(onnx::GraphProto *graph_proto,\n+                   const Block *block,\n+                   const std::vector<at::Tensor> &initializers = {});\n+\n+  virtual void EncodeTensor(onnx::TensorProto *tensor_proto,\n+                    const at::Tensor &tensor,\n+                    const at::optional<std::string> external_ref = {});\n+\n+  void AddAttribute(onnx::NodeProto *node_proto, const jit::Node *node, const jit::Symbol name);\n+\n+  size_t num_blocks_;\n+  bool defer_weight_export_;\n+  onnx_torch::OperatorExportTypes operator_export_type_;\n+  RawDataExportMap raw_data_export_map_;\n+};\n+\n+onnx::TensorProto_DataType ATenTypeToOnnxType(at::ScalarType at_type) {\n+  switch(at_type) {\n     case at::kDouble:\n-      onnx_type = onnx::TensorProto_DataType_DOUBLE;\n-      break;\n+      return onnx::TensorProto_DataType_DOUBLE;\n     case at::kFloat:\n-      onnx_type = onnx::TensorProto_DataType_FLOAT;\n-      break;\n+      return onnx::TensorProto_DataType_FLOAT;\n     case at::kHalf:\n-      onnx_type = onnx::TensorProto_DataType_FLOAT16;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_FLOAT16;\n     case at::kByte:\n-      onnx_type = onnx::TensorProto_DataType_UINT8;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_UINT8;\n     case at::kChar:\n-      onnx_type = onnx::TensorProto_DataType_INT8;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_INT8;\n     case at::kShort:\n-      onnx_type = onnx::TensorProto_DataType_INT16;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_INT16;\n     case at::kInt:\n-      onnx_type = onnx::TensorProto_DataType_INT32;\n-      break;\n+      return onnx::TensorProto_DataType_INT32;\n     case at::kLong:\n-      onnx_type = onnx::TensorProto_DataType_INT64;\n-      break;\n+      return onnx::TensorProto_DataType_INT64;\n     default:\n       torch::barf(\"unexpected tensor scalar type\");\n-      break;\n-  }\n-  p->set_data_type(onnx_type);\n-  // CPU's HalfTensor doesn't have contiguous(), so first calling contiguous()\n-  auto t = tensor.contiguous().toBackend(at::kCPU).toType(cast_type);\n-  // Add a buffer to the raw_data_export_map for the caller to dump into an\n-  // external data store. If external_ref is not specified, we instead dump\n-  // the contiguous data into the protobuf itself\n-  if (external_ref) {\n-    // For now, we use the name of the tensor as the external lookup name to\n-    // avoid ONNX protobuf changes.\n-    JIT_ASSERT(external_ref.value() == p->name());\n-    JIT_ASSERT(raw_data_export_map != nullptr);\n-    JIT_ASSERT(raw_data_export_map->count(external_ref.value()) == 0);\n-    (*raw_data_export_map)[external_ref.value()] = t;\n-    p->set_raw_data(\"__EXTERNAL\");\n-  } else {\n-    JIT_ASSERT(t.is_contiguous());\n-    p->set_raw_data(std::string(static_cast<char*>(t.data_ptr()),  t.type().elementSizeInBytes() * t.numel()));\n   }\n }\n \n-void addAttribute(onnx::NodeProto * n_p, jit::Node * n, jit::Symbol name, ExportContext *ctx) {\n-  auto attr = n_p->add_attribute();\n-  JIT_ASSERT(name.is_attr());\n-  attr->set_name(name.toUnqualString());\n-  switch(n->kindOf(name)) {\n-    case AttributeKind::f:\n-      attr->set_f(n->f(name));\n-      attr->set_type(onnx::AttributeProto_AttributeType_FLOAT);\n-      break;\n-    case AttributeKind::fs:\n-      attr->set_type(onnx::AttributeProto_AttributeType_FLOATS);\n-      for(auto & v : n->fs(name))\n-        attr->add_floats(v);\n-      break;\n-    case AttributeKind::i:\n-      attr->set_type(onnx::AttributeProto_AttributeType_INT);\n-      attr->set_i(n->i(name));\n-      break;\n-    case AttributeKind::is:\n-      attr->set_type(onnx::AttributeProto_AttributeType_INTS);\n-      for(auto & v : n->is(name))\n-        attr->add_ints(v);\n-      break;\n-    case AttributeKind::s:\n-      attr->set_type(onnx::AttributeProto_AttributeType_STRING);\n-      attr->set_s(n->s(name));\n-      break;\n-    case AttributeKind::ss:\n-      attr->set_type(onnx::AttributeProto_AttributeType_STRINGS);\n-      for(auto & v : n->ss(name))\n-        attr->add_strings(v);\n-      break;\n-    case AttributeKind::t: {\n-      attr->set_type(onnx::AttributeProto_AttributeType_TENSOR);\n-      auto t = attr->mutable_t();\n-      encodeTensor(t, n->t(name));\n-    } break;\n-    case AttributeKind::ts:\n-      attr->set_type(onnx::AttributeProto_AttributeType_TENSORS);\n-      for(auto & v : n->ts(name)) {\n-        auto t = attr->add_tensors();\n-        encodeTensor(t, v);\n-      }\n-      break;\n-    case AttributeKind::g: {\n-      attr->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n-      auto g = attr->mutable_g();\n-      encodeGraph(g, n->g(name), {}, ctx, nullptr);\n-    } break;\n-    case AttributeKind::gs:\n-      attr->set_type(onnx::AttributeProto_AttributeType_GRAPHS);\n-      for(auto & v : n->gs(name)) {\n-        auto g = attr->add_graphs();\n-        encodeGraph(g, v, {}, ctx, nullptr);\n-      }\n-      break;\n-  }\n-}\n-\n-void encodeTypeProtoTensorType(onnx::TypeProto_Tensor* tensor_type, Value* n) {\n+void EncodeTypeProtoTensorType(onnx::TypeProto_Tensor* tensor_type, const Value* n) {", "path": "torch/csrc/jit/export.cpp", "position": null, "original_position": 223, "commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "original_commit_id": "dfe899efbacf40f945fd85e4d49e3d0748317f4d", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "`const Value* n` --> `TypePtr t`, this is never using `n` it is always using `n->type()`", "created_at": "2018-07-24T21:23:04Z", "updated_at": "2018-11-23T15:48:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/9746#discussion_r204915994", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9746", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204915994"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9746#discussion_r204915994"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9746"}}, "body_html": "<p><code>const Value* n</code> --&gt; <code>TypePtr t</code>, this is never using <code>n</code> it is always using <code>n-&gt;type()</code></p>", "body_text": "const Value* n --> TypePtr t, this is never using n it is always using n->type()"}