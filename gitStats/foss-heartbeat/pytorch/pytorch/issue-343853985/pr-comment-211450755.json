{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/211450755", "pull_request_review_id": 147872871, "id": 211450755, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMTQ1MDc1NQ==", "diff_hunk": "@@ -22,227 +22,176 @@ namespace {\n namespace onnx_torch = ::torch::onnx;\n namespace onnx = ::ONNX_NAMESPACE;\n \n-std::string value_name(Value* n) {\n-  return n->uniqueName();\n+std::string getNodeStackTraceString(const Node* n) {\n+  std::stringstream ss;\n+  if (n->getSourceLocation()) {\n+    n->getSourceLocation()->highlight(ss);\n+  } else {\n+    ss << \"<unknown location>\";\n+  }\n+  return ss.str();\n }\n \n-struct ExportContext {\n-  size_t num_blocks = 0;\n-  onnx_torch::OperatorExportTypes operator_export_type;\n-};\n-\n-void encodeGraph(onnx::GraphProto * p_g, const std::shared_ptr<Graph> & g,\n-                 const std::vector<at::Tensor> & initializers,\n-                 ExportContext *ctx, RawDataExportMap* raw_data_export_map=nullptr);\n+void validateGraph(const std::shared_ptr<Graph>& graph, onnx_torch::OperatorExportTypes operator_export_type) {\n+  for (auto node : graph->nodes()) {\n+      // Macro'ed so we get a marginally better line number on failed export\n+#define FAIL_EXPORT(name) \\\n+      throw std::runtime_error(std::string(\"ONNX export failed: \") + name + \"\\n\\nGraph we tried to export:\\n\" + graph->toString());\n+    IR_IF(node, PythonOp)\n+      auto py_node = static_cast<torch::jit::PythonOp*>(value);\n+      FAIL_EXPORT(\n+          \"Couldn't export Python operator \" + py_node->name() +\n+          \"\\n\\nDefined at:\\n\" + getNodeStackTraceString(node))\n+    IR_ELSE()\n+      // Special error messages for certain types of operators\n+      if (node->kind() == aten::expand) {\n+        FAIL_EXPORT(\n+            \"Could not export a broadcasted operation; ONNX likely does not support this form of broadcasting.\\n\\nBroadcast occurred at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+      if (node->kind() == prim::PackPadded || node->kind() == prim::PadPacked) {\n+        FAIL_EXPORT(\n+            \"Cannot export individual pack_padded_sequence or pad_packed_sequence; these operations must occur in pairs.\\n\\nUsage of this operation occurred at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+      bool is_aten_fallback = operator_export_type == onnx_torch::OperatorExportTypes::ONNX_ATEN_FALLBACK;\n+      if (!node->kind().is_onnx() && !is_aten_fallback && node->kind() != prim::Undefined) {\n+        FAIL_EXPORT(\n+            \"Couldn't export operator \" + node->kind().toDisplayString() + \"\\n\\nDefined at:\\n\" +\n+            getNodeStackTraceString(node));\n+      }\n+    IR_END()\n+#undef FAIL_EXPORT\n+  }\n+}\n \n-void encodeBlock(onnx::GraphProto * p_g, Block *b,\n-                const std::vector<at::Tensor> & initializers,\n-                ExportContext *ctx, RawDataExportMap* raw_data_export_map);\n+class EncoderBase {\n+ public:\n+  EncoderBase(onnx::ModelProto *model_proto,\n+             onnx_torch::OperatorExportTypes operator_export_type,\n+             bool defer_weight_export = false);\n \n-void encodeTensor(onnx::TensorProto * p, const at::Tensor & tensor,\n-                  at::optional<std::string> external_ref={},\n-                  RawDataExportMap* raw_data_export_map = nullptr) {\n-  for(auto d : tensor.sizes()) {\n-    p->add_dims(d);\n+  RawDataExportMap get_raw_data_export_map() {\n+    return raw_data_export_map_;\n   }\n-  onnx::TensorProto_DataType onnx_type;\n-  // Most integral types and float16 need to be serialized as int32\n-  at::ScalarType cast_type = tensor.type().scalarType();\n-  switch(tensor.type().scalarType()) {\n+\n+ protected:\n+  void EncodeGraph(onnx::GraphProto *graph_proto,\n+                   const std::shared_ptr<Graph> &graph,\n+                   const std::vector<at::Tensor> &initializers = {});\n+\n+  void EncodeBlock(onnx::GraphProto *graph_proto,\n+                   const Block *block,\n+                   const std::vector<at::Tensor> &initializers = {});\n+\n+  virtual void EncodeTensor(onnx::TensorProto *tensor_proto,\n+                            const at::Tensor &tensor,\n+                            const at::optional<std::string> external_ref = {});\n+\n+  virtual void EncodeIntermediateValueInfo(onnx::GraphProto *graph_proto,\n+                                           const Value* n) {};\n+\n+  virtual void EncodeValueInfo(onnx::GraphProto *graph_proto,\n+                               onnx::ValueInfoProto* v,\n+                               const Value* n);\n+\n+  void AddAttribute(onnx::NodeProto *node_proto, const jit::Node *node, const jit::Symbol name);\n+\n+  size_t num_blocks_;\n+  bool defer_weight_export_;\n+  onnx_torch::OperatorExportTypes operator_export_type_;\n+  RawDataExportMap raw_data_export_map_;\n+};\n+\n+onnx::TensorProto_DataType ATenTypeToOnnxType(at::ScalarType at_type) {\n+  switch(at_type) {\n     case at::kDouble:\n-      onnx_type = onnx::TensorProto_DataType_DOUBLE;\n-      break;\n+      return onnx::TensorProto_DataType_DOUBLE;\n     case at::kFloat:\n-      onnx_type = onnx::TensorProto_DataType_FLOAT;\n-      break;\n+      return onnx::TensorProto_DataType_FLOAT;\n     case at::kHalf:\n-      onnx_type = onnx::TensorProto_DataType_FLOAT16;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_FLOAT16;\n     case at::kByte:\n-      onnx_type = onnx::TensorProto_DataType_UINT8;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_UINT8;\n     case at::kChar:\n-      onnx_type = onnx::TensorProto_DataType_INT8;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_INT8;\n     case at::kShort:\n-      onnx_type = onnx::TensorProto_DataType_INT16;\n-      cast_type = at::kInt;\n-      break;\n+      return onnx::TensorProto_DataType_INT16;\n     case at::kInt:\n-      onnx_type = onnx::TensorProto_DataType_INT32;\n-      break;\n+      return onnx::TensorProto_DataType_INT32;\n     case at::kLong:\n-      onnx_type = onnx::TensorProto_DataType_INT64;\n-      break;\n+      return onnx::TensorProto_DataType_INT64;\n     default:\n       AT_ERROR(\"unexpected tensor scalar type\");\n-      break;\n-  }\n-  p->set_data_type(onnx_type);\n-  // CPU's HalfTensor doesn't have contiguous(), so first calling contiguous()\n-  auto t = tensor.contiguous().toBackend(at::kCPU).toType(cast_type);", "path": "torch/csrc/jit/export.cpp", "position": 151, "original_position": 151, "commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "original_commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "body": "Why did you remove the type casting? ONNX export for integral types != int32 is now broken", "created_at": "2018-08-21T00:54:32Z", "updated_at": "2018-11-23T15:49:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/9746#discussion_r211450755", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9746", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/211450755"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9746#discussion_r211450755"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9746"}}, "body_html": "<p>Why did you remove the type casting? ONNX export for integral types != int32 is now broken</p>", "body_text": "Why did you remove the type casting? ONNX export for integral types != int32 is now broken"}