{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206699192", "pull_request_review_id": 142158193, "id": 206699192, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjY5OTE5Mg==", "diff_hunk": "@@ -553,20 +222,225 @@ void reconstructOutputTypes(Block *b) {\n   }\n }\n \n-} // anonymous namespace\n+std::shared_ptr<Graph> GraphDecoder::decode(\n+    const std::string& serialized_graph,\n+    std::vector<at::Tensor>& initializers) {\n+  auto model_proto = onnx_torch::ModelProto();\n+  model_proto.ParseFromString(serialized_graph);\n \n-std::shared_ptr<Graph> ImportIRGraph(const std::string& serialized_graph,\n-                                     std::vector<at::Tensor>& initializers) {\n+  auto graph_proto = model_proto.graph();\n+  auto graph = buildGraph(graph_proto);\n+  for (auto &tensor_ : graph_proto.initializer()) {\n+    initializers.push_back(buildTensor(tensor_));\n+  }\n+  reconstructOutputTypes(graph->block());\n+  return graph;\n+}\n \n-  pb_istream_t istream = pb_istream_from_buffer(reinterpret_cast<const pb_byte_t *>(serialized_graph.data()), serialized_graph.size());\n+class ModuleDecoder : DecoderBase {\n+ public:\n+  std::shared_ptr<script::Module> decode(\n+      std::shared_ptr<script::Module> root_module,\n+      const std::string& serialized_module,\n+      const std::unordered_map<std::string, std::string>& storage_map);\n \n-  auto model = Reader<Model_>::read(&istream);\n+ private:\n+  virtual std::shared_ptr<Graph> buildGraph(const onnx_torch::GraphProto& graph_proto) override;\n \n-  auto graph = buildGraph(model.graph, initializers);\n+  virtual at::Tensor buildTensor(const onnx_torch::TensorProto& tensor_proto) override;\n \n-  reconstructOutputTypes(graph->block());\n+  TypePtr buildType(const onnx_torch::TypeProto& type_proto);\n \n-  return graph;\n+  virtual void buildValue(Value* value, const onnx_torch::ValueInfoProto& valueinfo_proto) override;\n+\n+  virtual void buildIntermediateValue(Value* value, const std::string& name) override;\n+\n+  at::Tensor buildParameter(const onnx_torch::TensorProto& tensor_proto);\n+\n+  at::Tensor buildTensorCommon(const onnx_torch::TensorProto& tensor_proto,\n+                               const int64_t storage_offset,\n+                               const std::vector<int64_t>& strides);\n+\n+  std::pair<std::shared_ptr<script::Module>, std::string> parseFullName(\n+      std::shared_ptr<script::Module> root_module,\n+      const std::string fullname);\n+\n+  const std::unordered_map<std::string, std::string> *storage_export_map_;\n+  std::unordered_map<std::string, std::shared_ptr<at::Tensor>> storage_map_;\n+  std::unordered_map<std::string, const onnx_torch::TypeProto*> value_type_map_;\n+};\n+\n+std::shared_ptr<Graph> ModuleDecoder::buildGraph(const onnx_torch::GraphProto& graph_proto) {\n+  for (auto &subtype : graph_proto.value_info()) {\n+    value_type_map_[subtype.name()] = &subtype.type();\n+  }\n+  return DecoderBase::buildGraph(graph_proto);\n+}\n+\n+TypePtr ModuleDecoder::buildType(const onnx_torch::TypeProto& type_proto) {\n+  auto tensortype_proto = type_proto.tensor_type();\n+  auto shape_proto = tensortype_proto.shape();\n+  auto kind = type_proto.denotation();\n+  if (kind == \"DynamicType\") {\n+    return DynamicType::get();\n+  } else if (kind == \"TensorType\") {\n+    // TODO: Don't use DynamicType here\n+    return DynamicType::get();\n+  } else if (kind == \"TupleType\") {\n+    std::vector<TypePtr> elems;\n+    for (auto &subkind : shape_proto.dim()) {\n+      auto it = value_type_map_.find(subkind.dim_param());\n+      JIT_ASSERT(it != value_type_map_.end());\n+      elems.push_back(buildType(*it->second));\n+    }\n+    return TupleType::create(elems);\n+  } else if (kind == \"ListType\") {\n+    auto subkind = shape_proto.dim(0);\n+    auto it = value_type_map_.find(subkind.dim_param());\n+    JIT_ASSERT(it != value_type_map_.end());\n+    return ListType::create(buildType(*it->second));\n+  } else if (kind == \"NumberType\") {\n+    return NumberType::get();\n+  } else if (kind == \"FloatType\") {\n+    return FloatType::get();\n+  } else if (kind == \"IntType\") {\n+    return IntType::get();\n+  } else if (kind == \"NoneType\") {\n+    return NoneType::get();\n+  } else {\n+    throw std::runtime_error(\"unexpected string for type kind\");\n+  }\n+}\n+\n+void ModuleDecoder::buildValue(Value* value, const onnx_torch::ValueInfoProto& valueinfo_proto) {\n+  value->setType(buildType(valueinfo_proto.type()));\n+}\n+\n+void ModuleDecoder::buildIntermediateValue(Value* value, const std::string& name) {\n+  auto it = value_type_map_.find(name);\n+  JIT_ASSERT(it != value_type_map_.end());\n+  value->setType(buildType(*it->second));\n+}\n+\n+at::Tensor ModuleDecoder::buildParameter(const onnx_torch::TensorProto& tensor_proto) {\n+  std::vector<int64_t> strides;\n+  std::move(tensor_proto.int64_data().begin() + 3, tensor_proto.int64_data().end(), std::back_inserter(strides));", "path": "torch/csrc/jit/import.cpp", "position": null, "original_position": 724, "commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "original_commit_id": "c2bfe1029a6faab499e4bf6709a44c71c1924003", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Why are parameter tensors different than other tensors? We should at least have a comment. The +1 and the +3 are completely magical here.", "created_at": "2018-07-31T22:07:35Z", "updated_at": "2018-11-23T15:48:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/9746#discussion_r206699192", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9746", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206699192"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9746#discussion_r206699192"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9746"}}, "body_html": "<p>Why are parameter tensors different than other tensors? We should at least have a comment. The +1 and the +3 are completely magical here.</p>", "body_text": "Why are parameter tensors different than other tensors? We should at least have a comment. The +1 and the +3 are completely magical here."}