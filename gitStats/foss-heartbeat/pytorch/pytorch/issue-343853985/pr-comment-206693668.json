{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206693668", "pull_request_review_id": 142158193, "id": 206693668, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjY5MzY2OA==", "diff_hunk": "@@ -243,180 +201,634 @@ void encodeBlock(onnx::GraphProto * p_g, Block *b,\n       if (input->node()->kind() == prim::Undefined && !is_raw_export) {\n         p_n->add_input(\"\");\n       } else {\n-        p_n->add_input(value_name(input));\n+        p_n->add_input(input->uniqueName());\n       }\n     }\n     for(auto output : node->outputs()) {\n-      p_n->add_output(value_name(output));\n+      p_n->add_output(output->uniqueName());\n+      EncodeIntermediateValueInfo(graph_proto, output);\n     }\n     if (is_raw_export) {\n       JIT_ASSERT(!node->kind().is_onnx());\n       p_n->set_domain(node->kind().domainString());\n     }\n-    else if (ctx->operator_export_type != onnx::OperatorExportTypes::ONNX_ATEN_FALLBACK) {\n+    else if (operator_export_type_ == onnx_torch::OperatorExportTypes::ONNX) {\n       JIT_ASSERT(node->kind().is_onnx());\n     }\n     p_n->set_op_type(node->kind().toUnqualString());\n     for(auto attr_name : node->attributeNames()) {\n-      addAttribute(p_n, node, attr_name, ctx);\n+      AddAttribute(p_n, node, attr_name);\n     }\n     if (is_raw_export && node->blocks().size() > 0) {\n       auto blocks = p_n->add_attribute();\n       blocks->set_name(\"_blocks\");\n-      blocks->set_type(onnx::aGRAPHS);\n+      blocks->set_type(onnx::AttributeProto_AttributeType_GRAPHS);\n       for (auto block : node->blocks()) {\n         auto graph = blocks->add_graphs();\n-        encodeBlock(graph, block, initializers, ctx, raw_data_export_map);\n+        EncodeBlock(graph, block, initializers);\n       }\n     }\n     if (node->kind() == torch::jit::onnx::Loop) {\n       JIT_ASSERT(node->blocks().size() == 1);\n \n       auto body = p_n->add_attribute();\n       body->set_name(\"body\");\n-      body->set_type(onnx::aGRAPH);\n+      body->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n       auto g = body->mutable_g();\n-      encodeBlock(g, node->blocks()[0], {}, ctx, raw_data_export_map);\n+      EncodeBlock(g, node->blocks()[0]);\n     }\n     if (node->kind() == torch::jit::onnx::If) {\n       JIT_ASSERT(node->blocks().size() == 2);\n \n       auto true_branch = p_n->add_attribute();\n       true_branch->set_name(\"then_branch\");\n-      true_branch->set_type(onnx::aGRAPH);\n+      true_branch->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n       auto true_g = true_branch->mutable_g();\n-      encodeBlock(true_g, node->blocks()[0], {}, ctx, raw_data_export_map);\n+      EncodeBlock(true_g, node->blocks()[0]);\n \n       auto false_branch = p_n->add_attribute();\n       false_branch->set_name(\"else_branch\");\n-      false_branch->set_type(onnx::aGRAPH);\n+      false_branch->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n       auto false_g = false_branch->mutable_g();\n-      encodeBlock(false_g, node->blocks()[1], {}, ctx, raw_data_export_map);\n+      EncodeBlock(false_g, node->blocks()[1]);\n     }\n   }\n   auto num_initializers = initializers.size();\n-  JIT_ASSERT(b->inputs().size() >= num_initializers);\n-  size_t inputs_count = b->inputs().size() - num_initializers;\n+  JIT_ASSERT(block->inputs().size() >= num_initializers);\n+  size_t inputs_count = block->inputs().size() - num_initializers;\n   for (auto & tensor : initializers) {\n     // TODO: stop using positions to determine which initializers\n     // match to which inputs\n-    std::string name = p_g->get_input_name(inputs_count++);\n-    auto p = p_g->add_initializer();\n+    std::string name = graph_proto->input(inputs_count++).name();\n+    auto p = graph_proto->add_initializer();\n     p->set_name(name);\n-    if (raw_data_export_map) {\n-      encodeTensor(p, tensor, name, raw_data_export_map);\n-    } else {\n-      encodeTensor(p, tensor, {});\n-    }\n+    EncodeTensor(p, tensor, name);\n   }\n }\n \n-void encodeModel(onnx::ModelProto* p_m, const std::shared_ptr<Graph>& g,\n-                 const std::vector<at::Tensor>& initializers,\n-                 RawDataExportMap* raw_data_export_map = nullptr,\n-                 onnx::OperatorExportTypes operator_export_type\n-                   = onnx::OperatorExportTypes::ONNX) {\n-  onnx::GraphProto* p_g = p_m->mutable_graph();\n-  ExportContext ctx;\n-  ctx.operator_export_type = operator_export_type;\n-  encodeGraph(p_g, g, initializers, &ctx, raw_data_export_map);\n+void EncoderBase::AddAttribute(onnx::NodeProto *node_proto, const jit::Node *node, const jit::Symbol name) {\n+  auto attr = node_proto->add_attribute();\n+  JIT_ASSERT(name.is_attr());\n+  attr->set_name(name.toUnqualString());\n+  switch(node->kindOf(name)) {\n+    case AttributeKind::f:\n+      attr->set_f(node->f(name));\n+      attr->set_type(onnx::AttributeProto_AttributeType_FLOAT);\n+      break;\n+    case AttributeKind::fs:\n+      attr->set_type(onnx::AttributeProto_AttributeType_FLOATS);\n+      for(auto & v : node->fs(name))\n+        attr->add_floats(v);\n+      break;\n+    case AttributeKind::i:\n+      attr->set_type(onnx::AttributeProto_AttributeType_INT);\n+      attr->set_i(node->i(name));\n+      break;\n+    case AttributeKind::is:\n+      attr->set_type(onnx::AttributeProto_AttributeType_INTS);\n+      for(auto & v : node->is(name))\n+        attr->add_ints(v);\n+      break;\n+    case AttributeKind::s:\n+      attr->set_type(onnx::AttributeProto_AttributeType_STRING);\n+      attr->set_s(node->s(name));\n+      break;\n+    case AttributeKind::ss:\n+      attr->set_type(onnx::AttributeProto_AttributeType_STRINGS);\n+      for(auto & v : node->ss(name))\n+        attr->add_strings(v);\n+      break;\n+    case AttributeKind::t: {\n+      attr->set_type(onnx::AttributeProto_AttributeType_TENSOR);\n+      auto t = attr->mutable_t();\n+      EncodeTensor(t, node->t(name));\n+    } break;\n+    case AttributeKind::ts:\n+      attr->set_type(onnx::AttributeProto_AttributeType_TENSORS);\n+      for(auto & v : node->ts(name)) {\n+        auto t = attr->add_tensors();\n+        EncodeTensor(t, v);\n+      }\n+      break;\n+    case AttributeKind::g: {\n+      attr->set_type(onnx::AttributeProto_AttributeType_GRAPH);\n+      auto g = attr->mutable_g();\n+      EncodeGraph(g, node->g(name));\n+    } break;\n+    case AttributeKind::gs:\n+      attr->set_type(onnx::AttributeProto_AttributeType_GRAPHS);\n+      for(auto & v : node->gs(name)) {\n+        auto g = attr->add_graphs();\n+        EncodeGraph(g, v);\n+      }\n+      break;\n+    default:\n+      throw std::runtime_error(\"unexpected attribute kind\");\n+  }\n }\n \n-namespace {\n-std::string getNodeStackTraceString(Node* n) {\n-  std::stringstream ss;\n-  if (n->getSourceLocation()) {\n-    n->getSourceLocation()->highlight(ss);\n+void EncoderBase::EncodeTensor(\n+    onnx::TensorProto *tensor_proto,\n+    const at::Tensor &tensor,\n+    const at::optional<std::string> external_ref) {\n+  for(auto d : tensor.sizes()) {\n+    tensor_proto->add_dims(d);\n+  }\n+  tensor_proto->set_data_type(ATenTypeToOnnxType(tensor.type().scalarType()));\n+  // CPU's HalfTensor doesn't have contiguous(), so first calling contiguous()\n+  auto t = tensor.contiguous().toBackend(at::kCPU);\n+  // Add a buffer to the raw_data_export_map for the caller to dump into an\n+  // external data store. If external_ref is not specified, we instead dump\n+  // the contiguous data into the protobuf itself\n+  if (defer_weight_export_) {\n+    // For now, we use the name of the tensor as the external lookup name to\n+    // avoid ONNX protobuf changes.\n+    JIT_ASSERT(external_ref.value() == tensor_proto->name());\n+    JIT_ASSERT(raw_data_export_map_.count(external_ref.value()) == 0);\n+    raw_data_export_map_[external_ref.value()] = t;\n+    tensor_proto->set_raw_data(\"__EXTERNAL\");\n   } else {\n-    ss << \"<unknown location>\";\n+    JIT_ASSERT(t.is_contiguous());\n+    tensor_proto->set_raw_data(std::string(static_cast<char*>(t.data_ptr()),  t.type().elementSizeInBytes() * t.numel()));\n   }\n-  return ss.str();\n }\n-} // namespace\n \n-void validateGraph(const std::shared_ptr<Graph>& graph, onnx::OperatorExportTypes operator_export_type) {\n-  for (auto node : graph->nodes()) {\n-      // Macro'ed so we get a marginally better line number on failed export\n-#define FAIL_EXPORT(name) \\\n-      throw std::runtime_error(std::string(\"ONNX export failed: \") + name + \"\\n\\nGraph we tried to export:\\n\" + graph->toString());\n-    IR_IF(node, PythonOp)\n-      auto py_node = static_cast<torch::jit::PythonOp*>(value);\n-      FAIL_EXPORT(\n+class GraphEncoder: public EncoderBase {\n+ public:\n+  GraphEncoder(onnx::ModelProto *model_proto,\n+               const std::shared_ptr<Graph> &graph,\n+               int64_t onnx_opset_version,\n+               onnx_torch::OperatorExportTypes operator_export_type,\n+               const std::vector<at::Tensor> &initializers,\n+               bool defer_weight_export);\n+\n+};\n+\n+GraphEncoder::GraphEncoder(\n+    onnx::ModelProto *model_proto,\n+    const std::shared_ptr<Graph> &graph,\n+    int64_t onnx_opset_version,\n+    onnx_torch::OperatorExportTypes operator_export_type,\n+    const std::vector<at::Tensor> &initializers,\n+    bool defer_weight_export)\n+    : EncoderBase(model_proto, operator_export_type, defer_weight_export) {\n+  if (operator_export_type != onnx_torch::OperatorExportTypes::RAW) {\n+    validateGraph(graph, operator_export_type);\n+  }\n+\n+  auto* imp = model_proto->add_opset_import();\n+  // This is the version of ONNX operator set we are targeting\n+  imp->set_version(onnx_opset_version);\n+\n+  EncodeGraph(model_proto->mutable_graph(), graph, initializers);\n+}\n+\n+class ModuleEncoder: public EncoderBase {\n+ public:\n+  ModuleEncoder(onnx::ModelProto *model_proto,\n+                const std::shared_ptr<script::Module> &module);\n+\n+ private:\n+  void EncodeModule(onnx::GraphProto *graph_proto, const std::shared_ptr<script::Module> &module);\n+\n+  void EncodeParameters(onnx::GraphProto *graph_proto,\n+                        const std::shared_ptr<script::Module> &module,\n+                        const std::string prefix);\n+\n+  void EncodeParameter(onnx::TensorProto *tensor_proto,\n+                       const script::NamedParameter &parameter,\n+                       const std::string prefix);\n+\n+  void EncodeMethods(onnx::GraphProto *graph_proto,\n+                     const std::shared_ptr<script::Module> &module,\n+                     const std::string prefix);\n+\n+  void EncodeMethod(onnx::NodeProto *node_proto,\n+                    const std::unique_ptr<script::Method> &method,\n+                    const std::string prefix);\n+\n+  virtual void EncodeTensor(onnx::TensorProto *tensor_proto,\n+                            const at::Tensor &tensor,\n+                            const at::optional<std::string> external_ref) override;\n+\n+  virtual void EncodeIntermediateValueInfo(onnx::GraphProto *graph_proto,\n+                                           const Value* n) override;\n+\n+  virtual void EncodeValueInfo(onnx::GraphProto *graph_proto,\n+                               onnx::ValueInfoProto* v,\n+                               const Value* n) override;\n+\n+  void EncodeTypeInfo(onnx::GraphProto *graph_proto,\n+                      onnx::ValueInfoProto* v,\n+                      const TypePtr& type,\n+                      const std::string& name);\n+\n+  // Used to deduplicate tensor storages\n+  std::unordered_map<const void*, std::string> storage_dedup_map_;\n+\n+  // Used to keep track of Parameter names so Methods can refer to them\n+  std::unordered_map<at::Tensor*, std::string> parameter_map_;\n+\n+  // Used to create sequential tensor storage names\n+  size_t storage_counter_ = 0;\n+\n+  // Used to create sequential dummy names for node types\n+  size_t type_counter_ = 0;\n+};\n+\n+ModuleEncoder::ModuleEncoder(\n+    onnx::ModelProto *model_proto,\n+    const std::shared_ptr<script::Module> &module)\n+    : EncoderBase(model_proto,\n+                  onnx_torch::OperatorExportTypes::RAW,\n+                  /*defer_weight_export*/ true) {\n+  model_proto->set_doc_string(\"THIS PROTO IS NOT STANDARD ONNX\");\n+  EncodeModule(model_proto->mutable_graph(), module);\n+}\n+\n+void ModuleEncoder::EncodeIntermediateValueInfo(onnx::GraphProto *graph_proto, const Value *n) {\n+  auto v = graph_proto->add_value_info();\n+  EncodeTypeInfo(graph_proto, v, n->type(), n->uniqueName());\n+}\n+\n+void ModuleEncoder::EncodeTypeInfo(", "path": "torch/csrc/jit/export.cpp", "position": 648, "original_position": 664, "commit_id": "f622bcc6b1e23e942cca8615b87321ebc91e4273", "original_commit_id": "c2bfe1029a6faab499e4bf6709a44c71c1924003", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "This looks right and is the format we discussed! But please document the format, otherwise no one else will know how this encoding works.", "created_at": "2018-07-31T21:44:30Z", "updated_at": "2018-11-23T15:48:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/9746#discussion_r206693668", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9746", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/206693668"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9746#discussion_r206693668"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9746"}}, "body_html": "<p>This looks right and is the format we discussed! But please document the format, otherwise no one else will know how this encoding works.</p>", "body_text": "This looks right and is the format we discussed! But please document the format, otherwise no one else will know how this encoding works."}