{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274337983", "html_url": "https://github.com/pytorch/pytorch/issues/542#issuecomment-274337983", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/542", "id": 274337983, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDMzNzk4Mw==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-22T15:36:27Z", "updated_at": "2017-01-22T15:36:27Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5618407\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rasbt\">@rasbt</a></p>\n<p>The reason we explicitly named it <code>.cuda()</code> is because of how it is named in Torch itself.</p>\n<p>The origin of the API naming in Torch is untraceable, but it worked out well to keep it as <code>.cuda()</code> rather than change it to <code>.gpu()</code>. How did it work out well?</p>\n<ul>\n<li>the opencl package is <a href=\"https://github.com/hughperkins/cltorch\">a separate package and repository in torch</a> that doesn't need to sit in the core and has a <code>cl()</code> function. It can be installed and imported separately, after which a <code>.cl()</code> function will be available to users.</li>\n<li>In the same process, the OpenCL and CUDA packages can coexist (for example if you have a GPU that's AMD and another NVIDIA for example).</li>\n</ul>\n<p>Saying that, we dont plan to add any official OpenCL support (see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"201713458\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/488\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/488/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/488\">#488</a>) but instead will wait for AMD's HIP technology that has a much better chance of actually being (a) successful (b) on performance parity with CUDA at the software level. (c) has a CUDA to HIP transpiler</p>\n<p>Now that I've given you context, I kinda like the naming <code>.gpu()</code> more, and we'll think about if we can depreceate <code>.cuda()</code> in the favor of <code>.gpu()</code>, I'll get back to you on a decision.</p>", "body_text": "Hi @rasbt\nThe reason we explicitly named it .cuda() is because of how it is named in Torch itself.\nThe origin of the API naming in Torch is untraceable, but it worked out well to keep it as .cuda() rather than change it to .gpu(). How did it work out well?\n\nthe opencl package is a separate package and repository in torch that doesn't need to sit in the core and has a cl() function. It can be installed and imported separately, after which a .cl() function will be available to users.\nIn the same process, the OpenCL and CUDA packages can coexist (for example if you have a GPU that's AMD and another NVIDIA for example).\n\nSaying that, we dont plan to add any official OpenCL support (see #488) but instead will wait for AMD's HIP technology that has a much better chance of actually being (a) successful (b) on performance parity with CUDA at the software level. (c) has a CUDA to HIP transpiler\nNow that I've given you context, I kinda like the naming .gpu() more, and we'll think about if we can depreceate .cuda() in the favor of .gpu(), I'll get back to you on a decision.", "body": "Hi @rasbt \r\n\r\nThe reason we explicitly named it `.cuda()` is because of how it is named in Torch itself. \r\n\r\nThe origin of the API naming in Torch is untraceable, but it worked out well to keep it as `.cuda()` rather than change it to `.gpu()`. How did it work out well?\r\n- the opencl package is [a separate package and repository in torch](https://github.com/hughperkins/cltorch) that doesn't need to sit in the core and has a `cl()` function. It can be installed and imported separately, after which a `.cl()` function will be available to users.\r\n- In the same process, the OpenCL and CUDA packages can coexist (for example if you have a GPU that's AMD and another NVIDIA for example).\r\n\r\nSaying that, we dont plan to add any official OpenCL support (see #488) but instead will wait for AMD's HIP technology that has a much better chance of actually being (a) successful (b) on performance parity with CUDA at the software level. (c) has a CUDA to HIP transpiler\r\n\r\nNow that I've given you context, I kinda like the naming `.gpu()` more, and we'll think about if we can depreceate `.cuda()` in the favor of `.gpu()`, I'll get back to you on a decision.\r\n\r\n\r\n\r\n"}