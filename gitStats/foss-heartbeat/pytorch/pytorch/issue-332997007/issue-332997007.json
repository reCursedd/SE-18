{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8582", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8582/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8582/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8582/events", "html_url": "https://github.com/pytorch/pytorch/issues/8582", "id": 332997007, "node_id": "MDU6SXNzdWUzMzI5OTcwMDc=", "number": 8582, "title": "Layer freezing doesn't work with model.train() in v0.4.0", "user": {"login": "nabihach", "id": 6519168, "node_id": "MDQ6VXNlcjY1MTkxNjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6519168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nabihach", "html_url": "https://github.com/nabihach", "followers_url": "https://api.github.com/users/nabihach/followers", "following_url": "https://api.github.com/users/nabihach/following{/other_user}", "gists_url": "https://api.github.com/users/nabihach/gists{/gist_id}", "starred_url": "https://api.github.com/users/nabihach/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nabihach/subscriptions", "organizations_url": "https://api.github.com/users/nabihach/orgs", "repos_url": "https://api.github.com/users/nabihach/repos", "events_url": "https://api.github.com/users/nabihach/events{/privacy}", "received_events_url": "https://api.github.com/users/nabihach/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-06-16T16:04:41Z", "updated_at": "2018-11-01T13:33:30Z", "closed_at": "2018-11-01T13:33:30Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I just upgraded my Pytorch version from 0.3.0 to v0.4.0, and I'm running into the following issue.</p>\n<p>I want to freeze the parameters of a layer (i.e. set <code>requires_grad to False</code> for that layer) and train. This worked fine of v0.3.0 but now, I get the following error at the line <code>loss.backward()</code>:<br>\n<code>RuntimeError: backward_input can only be called in training mode</code></p>\n<p>Based on what I found online, I added <code>model.train()</code> to the code, but now I get the following error on <code>loss.backward()</code>:<br>\n<code>RuntimeError: inconsistent range for TensorList output</code></p>\n<p>When I comment out the layer freezing, things work fine. It seems that model.train() assumes <code>requires.grad=True</code> for all the parameters. How can I make training work properly with freezing?</p>\n<p>Giving the code example below.</p>\n<h2>Code example</h2>\n<p>The following code ran fine on v0.3.0</p>\n<pre><code>for name, param in model.named_parameters():\n    if name == 'encoder.embedding.weight':\n        param.requires_grad = False\nloss = model.forward()\nloss.backward()\n</code></pre>\n<p>Running it on v0.4.0 yields to following error:</p>\n<pre><code>  File \"/mnt/fs_default/Conversational-Agents/model.py\", line 86, in train_batch\n    loss.backward()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: backward_input can only be called in training mode\n</code></pre>\n<p>So then I added <code>model.train()</code> to my code as follows:</p>\n<pre><code>for name, param in model.named_parameters():\n    if name == 'encoder.embedding.weight':\n        param.requires_grad = False\nmodel.train()\nloss = model.forward()\nloss.backward()\n</code></pre>\n<p>but now I get the error:</p>\n<pre><code>  File \"/mnt/fs_default/kaselby/Conversational-Agents/model.py\", line 88, in train_batch\n    loss.backward()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: inconsistent range for TensorList output\n</code></pre>\n<p>Is this a bug? If not, can someone tell me how to make layer freezing work properly with model.train()?</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.5<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: Tesla P100-PCIE-16GB<br>\nGPU 1: Tesla P100-PCIE-16GB<br>\nGPU 2: Tesla P100-PCIE-16GB<br>\nGPU 3: Tesla P100-PCIE-16GB</p>\n<p>Nvidia driver version: 384.111<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip3] msgpack-numpy (0.4.1)<br>\n[pip3] numpy (1.13.3)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchtext (0.2.3)<br>\n[pip3] torchvision (0.1.9)<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nI just upgraded my Pytorch version from 0.3.0 to v0.4.0, and I'm running into the following issue.\nI want to freeze the parameters of a layer (i.e. set requires_grad to False for that layer) and train. This worked fine of v0.3.0 but now, I get the following error at the line loss.backward():\nRuntimeError: backward_input can only be called in training mode\nBased on what I found online, I added model.train() to the code, but now I get the following error on loss.backward():\nRuntimeError: inconsistent range for TensorList output\nWhen I comment out the layer freezing, things work fine. It seems that model.train() assumes requires.grad=True for all the parameters. How can I make training work properly with freezing?\nGiving the code example below.\nCode example\nThe following code ran fine on v0.3.0\nfor name, param in model.named_parameters():\n    if name == 'encoder.embedding.weight':\n        param.requires_grad = False\nloss = model.forward()\nloss.backward()\n\nRunning it on v0.4.0 yields to following error:\n  File \"/mnt/fs_default/Conversational-Agents/model.py\", line 86, in train_batch\n    loss.backward()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: backward_input can only be called in training mode\n\nSo then I added model.train() to my code as follows:\nfor name, param in model.named_parameters():\n    if name == 'encoder.embedding.weight':\n        param.requires_grad = False\nmodel.train()\nloss = model.forward()\nloss.backward()\n\nbut now I get the error:\n  File \"/mnt/fs_default/kaselby/Conversational-Agents/model.py\", line 88, in train_batch\n    loss.backward()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: inconsistent range for TensorList output\n\nIs this a bug? If not, can someone tell me how to make layer freezing work properly with model.train()?\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: Tesla P100-PCIE-16GB\nGPU 1: Tesla P100-PCIE-16GB\nGPU 2: Tesla P100-PCIE-16GB\nGPU 3: Tesla P100-PCIE-16GB\nNvidia driver version: 384.111\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip3] msgpack-numpy (0.4.1)\n[pip3] numpy (1.13.3)\n[pip3] torch (0.4.0)\n[pip3] torchtext (0.2.3)\n[pip3] torchvision (0.1.9)\n[conda] Could not collect", "body": "## Issue description\r\n\r\nI just upgraded my Pytorch version from 0.3.0 to v0.4.0, and I'm running into the following issue.\r\n\r\nI want to freeze the parameters of a layer (i.e. set `requires_grad to False` for that layer) and train. This worked fine of v0.3.0 but now, I get the following error at the line `loss.backward()`:\r\n`RuntimeError: backward_input can only be called in training mode`\r\n\r\nBased on what I found online, I added `model.train()` to the code, but now I get the following error on `loss.backward()`:\r\n`RuntimeError: inconsistent range for TensorList output`\r\n\r\nWhen I comment out the layer freezing, things work fine. It seems that model.train() assumes `requires.grad=True` for all the parameters. How can I make training work properly with freezing?\r\n\r\nGiving the code example below.\r\n\r\n## Code example\r\nThe following code ran fine on v0.3.0\r\n```\r\nfor name, param in model.named_parameters():\r\n    if name == 'encoder.embedding.weight':\r\n        param.requires_grad = False\r\nloss = model.forward()\r\nloss.backward()\r\n```\r\nRunning it on v0.4.0 yields to following error:\r\n```\r\n  File \"/mnt/fs_default/Conversational-Agents/model.py\", line 86, in train_batch\r\n    loss.backward()\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: backward_input can only be called in training mode\r\n```\r\nSo then I added `model.train()` to my code as follows:\r\n```\r\nfor name, param in model.named_parameters():\r\n    if name == 'encoder.embedding.weight':\r\n        param.requires_grad = False\r\nmodel.train()\r\nloss = model.forward()\r\nloss.backward()\r\n```\r\nbut now I get the error:\r\n```\r\n  File \"/mnt/fs_default/kaselby/Conversational-Agents/model.py\", line 88, in train_batch\r\n    loss.backward()\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: inconsistent range for TensorList output\r\n```\r\n\r\nIs this a bug? If not, can someone tell me how to make layer freezing work properly with model.train()?\r\n\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\nGPU 2: Tesla P100-PCIE-16GB\r\nGPU 3: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy (0.4.1)\r\n[pip3] numpy (1.13.3)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchtext (0.2.3)\r\n[pip3] torchvision (0.1.9)\r\n[conda] Could not collect\r\n"}