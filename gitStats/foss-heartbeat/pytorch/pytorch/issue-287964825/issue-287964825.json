{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620/events", "html_url": "https://github.com/pytorch/pytorch/issues/4620", "id": 287964825, "node_id": "MDU6SXNzdWUyODc5NjQ4MjU=", "number": 4620, "title": "`torch.normal` accepts Variables but does not propagate gradients", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2018-01-11T23:53:07Z", "updated_at": "2018-05-02T01:14:06Z", "closed_at": "2018-05-02T01:14:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Simple example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\nmu <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nsigma <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nloss <span class=\"pl-k\">=</span> torch.pow(torch.normal(mu, sigma), <span class=\"pl-c1\">2</span>)\nloss.backward()</pre></div>\n<p>produces:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-3-52a0569421b1&gt; in &lt;module&gt;()\n----&gt; 1 loss.backward()\n\n~/stuff/venv/lib/python3.6/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    165                 Variable.\n    166         \"\"\"\n--&gt; 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    168 \n    169     def register_hook(self, hook):\n\n~/stuff/venv/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     97 \n     98     Variable._execution_engine.run_backward(\n---&gt; 99         variables, grad_variables, retain_graph)\n    100 \n    101 \n\nRuntimeError: element 0 of variables does not require grad and does not have a grad_fn\n</code></pre>", "body_text": "Simple example:\nimport torch\nfrom torch.autograd import Variable\n\n\nmu = Variable(torch.Tensor([1]), requires_grad=True)\nsigma = Variable(torch.Tensor([1]), requires_grad=True)\nloss = torch.pow(torch.normal(mu, sigma), 2)\nloss.backward()\nproduces:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-3-52a0569421b1> in <module>()\n----> 1 loss.backward()\n\n~/stuff/venv/lib/python3.6/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    165                 Variable.\n    166         \"\"\"\n--> 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    168 \n    169     def register_hook(self, hook):\n\n~/stuff/venv/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     97 \n     98     Variable._execution_engine.run_backward(\n---> 99         variables, grad_variables, retain_graph)\n    100 \n    101 \n\nRuntimeError: element 0 of variables does not require grad and does not have a grad_fn", "body": "Simple example:\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n\r\nmu = Variable(torch.Tensor([1]), requires_grad=True)\r\nsigma = Variable(torch.Tensor([1]), requires_grad=True)\r\nloss = torch.pow(torch.normal(mu, sigma), 2)\r\nloss.backward()\r\n```\r\nproduces:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-52a0569421b1> in <module>()\r\n----> 1 loss.backward()\r\n\r\n~/stuff/venv/lib/python3.6/site-packages/torch/autograd/variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    165                 Variable.\r\n    166         \"\"\"\r\n--> 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    168 \r\n    169     def register_hook(self, hook):\r\n\r\n~/stuff/venv/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\r\n     97 \r\n     98     Variable._execution_engine.run_backward(\r\n---> 99         variables, grad_variables, retain_graph)\r\n    100 \r\n    101 \r\n\r\nRuntimeError: element 0 of variables does not require grad and does not have a grad_fn\r\n```"}