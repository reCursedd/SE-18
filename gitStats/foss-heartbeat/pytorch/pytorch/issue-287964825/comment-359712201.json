{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/359712201", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-359712201", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 359712201, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTcxMjIwMQ==", "user": {"login": "Kelym", "id": 4526957, "node_id": "MDQ6VXNlcjQ1MjY5NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/4526957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kelym", "html_url": "https://github.com/Kelym", "followers_url": "https://api.github.com/users/Kelym/followers", "following_url": "https://api.github.com/users/Kelym/following{/other_user}", "gists_url": "https://api.github.com/users/Kelym/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kelym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kelym/subscriptions", "organizations_url": "https://api.github.com/users/Kelym/orgs", "repos_url": "https://api.github.com/users/Kelym/repos", "events_url": "https://api.github.com/users/Kelym/events{/privacy}", "received_events_url": "https://api.github.com/users/Kelym/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-23T08:22:23Z", "updated_at": "2018-01-23T08:22:23Z", "author_association": "NONE", "body_html": "<p>Just encountered this trap. I came from tensorflow and is hoping to write a policy gradient agent for reinforcement learning, where I need to sample an action tensor from a normal distribution where the mean is the output of a network and the deviation is a variable. I need to propagate to both the mean and the variance.</p>\n<ol>\n<li>Would vote for back-prop able distribution because otherwise it is hard to write RL agent in torch.</li>\n<li>Can anyone point me to place to check \"what vars are affected by loss.backward()\"? It can be easily assumed that a lot of things carry gradient (list of variables, etc) and resulted in painful debugging.</li>\n</ol>", "body_text": "Just encountered this trap. I came from tensorflow and is hoping to write a policy gradient agent for reinforcement learning, where I need to sample an action tensor from a normal distribution where the mean is the output of a network and the deviation is a variable. I need to propagate to both the mean and the variance.\n\nWould vote for back-prop able distribution because otherwise it is hard to write RL agent in torch.\nCan anyone point me to place to check \"what vars are affected by loss.backward()\"? It can be easily assumed that a lot of things carry gradient (list of variables, etc) and resulted in painful debugging.", "body": "Just encountered this trap. I came from tensorflow and is hoping to write a policy gradient agent for reinforcement learning, where I need to sample an action tensor from a normal distribution where the mean is the output of a network and the deviation is a variable. I need to propagate to both the mean and the variance. \r\n\r\n1) Would vote for back-prop able distribution because otherwise it is hard to write RL agent in torch. \r\n2) Can anyone point me to place to check \"what vars are affected by loss.backward()\"? It can be easily assumed that a lot of things carry gradient (list of variables, etc) and resulted in painful debugging.\r\n"}