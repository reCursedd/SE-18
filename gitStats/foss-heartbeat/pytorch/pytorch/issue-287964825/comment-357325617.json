{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357325617", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-357325617", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 357325617, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzMyNTYxNw==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T19:03:00Z", "updated_at": "2018-01-12T19:03:00Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>But I've also marked mu and sigma variables as requiring a grad in the first example. What makes them different than nn.Linear in the eyes of backward()?</p>\n</blockquote>\n<p>I believe <code>torch.normal</code> sets its output to not requiring grad.</p>\n<blockquote>\n<p>Personally I would like to lobby for torch.normal (and friends) to be backprop-able, since it's convenient for cases where one wants to use a reparameterization trick.</p>\n</blockquote>\n<p>It is useful in case of the reparameterization trick. However, it still doesn't quite make any sense for backprop to work on sampling methods. For example, what would you say is the \"gradient\" for discrete distributions? And we should definitely not allow backprop through things like entire MCMC trace, naive RL rewards, etc. When users want noisy gradients, the best way is to let them manually write out the thing like N(0, 1) in my opinion.</p>\n<p>I'm not sure that we should throw an error. A warning might be a good solution. But I definitely agree that we should make the doc clearer.</p>", "body_text": "But I've also marked mu and sigma variables as requiring a grad in the first example. What makes them different than nn.Linear in the eyes of backward()?\n\nI believe torch.normal sets its output to not requiring grad.\n\nPersonally I would like to lobby for torch.normal (and friends) to be backprop-able, since it's convenient for cases where one wants to use a reparameterization trick.\n\nIt is useful in case of the reparameterization trick. However, it still doesn't quite make any sense for backprop to work on sampling methods. For example, what would you say is the \"gradient\" for discrete distributions? And we should definitely not allow backprop through things like entire MCMC trace, naive RL rewards, etc. When users want noisy gradients, the best way is to let them manually write out the thing like N(0, 1) in my opinion.\nI'm not sure that we should throw an error. A warning might be a good solution. But I definitely agree that we should make the doc clearer.", "body": "> But I've also marked mu and sigma variables as requiring a grad in the first example. What makes them different than nn.Linear in the eyes of backward()?\r\n\r\nI believe `torch.normal` sets its output to not requiring grad.\r\n\r\n> Personally I would like to lobby for torch.normal (and friends) to be backprop-able, since it's convenient for cases where one wants to use a reparameterization trick. \r\n\r\nIt is useful in case of the reparameterization trick. However, it still doesn't quite make any sense for backprop to work on sampling methods. For example, what would you say is the \"gradient\" for discrete distributions? And we should definitely not allow backprop through things like entire MCMC trace, naive RL rewards, etc. When users want noisy gradients, the best way is to let them manually write out the thing like N(0, 1) in my opinion.\r\n\r\nI'm not sure that we should throw an error. A warning might be a good solution. But I definitely agree that we should make the doc clearer."}