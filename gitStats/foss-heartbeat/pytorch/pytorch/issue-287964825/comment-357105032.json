{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357105032", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-357105032", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 357105032, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzEwNTAzMg==", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T00:21:20Z", "updated_at": "2018-01-12T00:21:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ok, here's another example of the behavior described in the previous comment: <code>backward()</code> runs without error, but the gradients are not actually calculated.</p>\n<p>It's something like a VAE. It demonstrates using <code>torch.normal</code> for the reparameterization trick vs. a hand-written version.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">import</span> math\n\n\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-c1\">LOG2PI</span> <span class=\"pl-k\">=</span> torch.log(torch.FloatTensor([<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> math.pi]))[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DiagonalMVN</span>(<span class=\"pl-c1\">object</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">mean</span>, <span class=\"pl-smi\">log_stddev</span>):\n    <span class=\"pl-k\">assert</span> mean.size() <span class=\"pl-k\">==</span> log_stddev.size()\n    <span class=\"pl-c1\">self</span>.mean <span class=\"pl-k\">=</span> mean\n    <span class=\"pl-c1\">self</span>.log_stddev <span class=\"pl-k\">=</span> log_stddev\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_bad</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> torch.normal(<span class=\"pl-c1\">self</span>.mean, torch.exp(<span class=\"pl-c1\">self</span>.log_stddev))\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_good</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.mean <span class=\"pl-k\">+</span> torch.exp(<span class=\"pl-c1\">self</span>.log_stddev) <span class=\"pl-k\">*</span> Variable(torch.randn(<span class=\"pl-c1\">self</span>.mean.size()))\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">logprob</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.5</span> <span class=\"pl-k\">*</span> (\n      <span class=\"pl-c1\">self</span>.mean.numel() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">LOG2PI</span>\n      <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> torch.sum(<span class=\"pl-c1\">self</span>.log_stddev)\n      <span class=\"pl-k\">+</span> torch.sum((x <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.mean) <span class=\"pl-k\">*</span> torch.exp(<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.log_stddev) <span class=\"pl-k\">*</span> (x <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.mean))\n    )\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">FixedVarianceNet</span>(<span class=\"pl-c1\">object</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">mean_net</span>, <span class=\"pl-smi\">log_stddev</span>):\n    <span class=\"pl-c1\">self</span>.mean_net <span class=\"pl-k\">=</span> mean_net\n    <span class=\"pl-c1\">self</span>.log_stddev <span class=\"pl-k\">=</span> log_stddev\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> DiagonalMVN(<span class=\"pl-c1\">self</span>.mean_net(x), <span class=\"pl-c1\">self</span>.log_stddev)\n\ninference_net <span class=\"pl-k\">=</span> FixedVarianceNet(torch.nn.Linear(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>), Variable(torch.zeros(<span class=\"pl-c1\">1</span>)))\ngenerative_net <span class=\"pl-k\">=</span> FixedVarianceNet(torch.nn.Linear(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>), Variable(torch.zeros(<span class=\"pl-c1\">2</span>)))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>### torch.normal broken!<span class=\"pl-pds\">'</span></span>)\nXvar <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>))\nvi_posterior <span class=\"pl-k\">=</span> inference_net(Xvar)\nloss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>generative_net(vi_posterior.sample_bad()).logprob(Xvar)\nloss.backward()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inference_net.mean_net.bias.grad ==<span class=\"pl-pds\">'</span></span>, inference_net.mean_net.bias.grad)\n<span class=\"pl-c1\">print</span>()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>### Custom sample() works...<span class=\"pl-pds\">'</span></span>)\nXvar <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>))\nvi_posterior <span class=\"pl-k\">=</span> inference_net(Xvar)\nloss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>generative_net(vi_posterior.sample_good()).logprob(Xvar)\nloss.backward()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inference_net.mean_net.bias.grad ==<span class=\"pl-pds\">'</span></span>, inference_net.mean_net.bias.grad)</pre></div>\n<p>It produces:</p>\n<pre><code>### torch.normal broken!\ninference_net.mean_net.bias.grad == None\n\n### Custom sample() works...\ninference_net.mean_net.bias.grad == Variable containing:\n 0.4637\n[torch.FloatTensor of size 1]\n\n</code></pre>", "body_text": "Ok, here's another example of the behavior described in the previous comment: backward() runs without error, but the gradients are not actually calculated.\nIt's something like a VAE. It demonstrates using torch.normal for the reparameterization trick vs. a hand-written version.\nimport torch\nfrom torch.autograd import Variable\n\nimport math\n\n\ntorch.manual_seed(0)\n\nLOG2PI = torch.log(torch.FloatTensor([2 * math.pi]))[0]\n\nclass DiagonalMVN(object):\n  def __init__(self, mean, log_stddev):\n    assert mean.size() == log_stddev.size()\n    self.mean = mean\n    self.log_stddev = log_stddev\n\n  def sample_bad(self):\n    return torch.normal(self.mean, torch.exp(self.log_stddev))\n\n  def sample_good(self):\n    return self.mean + torch.exp(self.log_stddev) * Variable(torch.randn(self.mean.size()))\n\n  def logprob(self, x):\n    return -0.5 * (\n      self.mean.numel() * LOG2PI\n      + 2 * torch.sum(self.log_stddev)\n      + torch.sum((x - self.mean) * torch.exp(-2 * self.log_stddev) * (x - self.mean))\n    )\n\nclass FixedVarianceNet(object):\n  def __init__(self, mean_net, log_stddev):\n    self.mean_net = mean_net\n    self.log_stddev = log_stddev\n\n  def __call__(self, x):\n    return DiagonalMVN(self.mean_net(x), self.log_stddev)\n\ninference_net = FixedVarianceNet(torch.nn.Linear(2, 1), Variable(torch.zeros(1)))\ngenerative_net = FixedVarianceNet(torch.nn.Linear(1, 2), Variable(torch.zeros(2)))\n\nprint('### torch.normal broken!')\nXvar = Variable(torch.randn(2))\nvi_posterior = inference_net(Xvar)\nloss = -generative_net(vi_posterior.sample_bad()).logprob(Xvar)\nloss.backward()\n\nprint('inference_net.mean_net.bias.grad ==', inference_net.mean_net.bias.grad)\nprint()\n\nprint('### Custom sample() works...')\nXvar = Variable(torch.randn(2))\nvi_posterior = inference_net(Xvar)\nloss = -generative_net(vi_posterior.sample_good()).logprob(Xvar)\nloss.backward()\n\nprint('inference_net.mean_net.bias.grad ==', inference_net.mean_net.bias.grad)\nIt produces:\n### torch.normal broken!\ninference_net.mean_net.bias.grad == None\n\n### Custom sample() works...\ninference_net.mean_net.bias.grad == Variable containing:\n 0.4637\n[torch.FloatTensor of size 1]", "body": "Ok, here's another example of the behavior described in the previous comment: `backward()` runs without error, but the gradients are not actually calculated.\r\n\r\nIt's something like a VAE. It demonstrates using `torch.normal` for the reparameterization trick vs. a hand-written version.\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nimport math\r\n\r\n\r\ntorch.manual_seed(0)\r\n\r\nLOG2PI = torch.log(torch.FloatTensor([2 * math.pi]))[0]\r\n\r\nclass DiagonalMVN(object):\r\n  def __init__(self, mean, log_stddev):\r\n    assert mean.size() == log_stddev.size()\r\n    self.mean = mean\r\n    self.log_stddev = log_stddev\r\n\r\n  def sample_bad(self):\r\n    return torch.normal(self.mean, torch.exp(self.log_stddev))\r\n\r\n  def sample_good(self):\r\n    return self.mean + torch.exp(self.log_stddev) * Variable(torch.randn(self.mean.size()))\r\n\r\n  def logprob(self, x):\r\n    return -0.5 * (\r\n      self.mean.numel() * LOG2PI\r\n      + 2 * torch.sum(self.log_stddev)\r\n      + torch.sum((x - self.mean) * torch.exp(-2 * self.log_stddev) * (x - self.mean))\r\n    )\r\n\r\nclass FixedVarianceNet(object):\r\n  def __init__(self, mean_net, log_stddev):\r\n    self.mean_net = mean_net\r\n    self.log_stddev = log_stddev\r\n\r\n  def __call__(self, x):\r\n    return DiagonalMVN(self.mean_net(x), self.log_stddev)\r\n\r\ninference_net = FixedVarianceNet(torch.nn.Linear(2, 1), Variable(torch.zeros(1)))\r\ngenerative_net = FixedVarianceNet(torch.nn.Linear(1, 2), Variable(torch.zeros(2)))\r\n\r\nprint('### torch.normal broken!')\r\nXvar = Variable(torch.randn(2))\r\nvi_posterior = inference_net(Xvar)\r\nloss = -generative_net(vi_posterior.sample_bad()).logprob(Xvar)\r\nloss.backward()\r\n\r\nprint('inference_net.mean_net.bias.grad ==', inference_net.mean_net.bias.grad)\r\nprint()\r\n\r\nprint('### Custom sample() works...')\r\nXvar = Variable(torch.randn(2))\r\nvi_posterior = inference_net(Xvar)\r\nloss = -generative_net(vi_posterior.sample_good()).logprob(Xvar)\r\nloss.backward()\r\n\r\nprint('inference_net.mean_net.bias.grad ==', inference_net.mean_net.bias.grad)\r\n```\r\nIt produces:\r\n```\r\n### torch.normal broken!\r\ninference_net.mean_net.bias.grad == None\r\n\r\n### Custom sample() works...\r\ninference_net.mean_net.bias.grad == Variable containing:\r\n 0.4637\r\n[torch.FloatTensor of size 1]\r\n\r\n```"}