{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357362015", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-357362015", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 357362015, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzM2MjAxNQ==", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T21:41:10Z", "updated_at": "2018-01-12T21:41:10Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>nn.Linear has no special treatment. Here is what happens when it is involved.<br>\nmu, sigma both require grad, but torch.normal(mu, sigma) doesn't<br>\ntorch.normal(mu, sigma), linear.weight and linear.bias interact, produce out. Since although torch.normal(mu, sigma) doesn't need grad, linear.weight and linear.bias are nn.Parameter and naturally require grad, so does out.</p>\n</blockquote>\n<p>Ok, so this all makes sense to me. But I don't see why</p>\n<div class=\"highlight highlight-source-python\"><pre>mu <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nsigma <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nx <span class=\"pl-k\">=</span> torch.normal(mu, sigma)\nloss <span class=\"pl-k\">=</span> torch.pow(x, <span class=\"pl-c1\">2</span>)\nloss.backward()</pre></div>\n<p>should produce a RuntimeError.</p>", "body_text": "nn.Linear has no special treatment. Here is what happens when it is involved.\nmu, sigma both require grad, but torch.normal(mu, sigma) doesn't\ntorch.normal(mu, sigma), linear.weight and linear.bias interact, produce out. Since although torch.normal(mu, sigma) doesn't need grad, linear.weight and linear.bias are nn.Parameter and naturally require grad, so does out.\n\nOk, so this all makes sense to me. But I don't see why\nmu = Variable(torch.Tensor([1]), requires_grad=True)\nsigma = Variable(torch.Tensor([1]), requires_grad=True)\nx = torch.normal(mu, sigma)\nloss = torch.pow(x, 2)\nloss.backward()\nshould produce a RuntimeError.", "body": "> nn.Linear has no special treatment. Here is what happens when it is involved.\r\nmu, sigma both require grad, but torch.normal(mu, sigma) doesn't\r\ntorch.normal(mu, sigma), linear.weight and linear.bias interact, produce out. Since although torch.normal(mu, sigma) doesn't need grad, linear.weight and linear.bias are nn.Parameter and naturally require grad, so does out.\r\n\r\nOk, so this all makes sense to me. But I don't see why \r\n```python\r\nmu = Variable(torch.Tensor([1]), requires_grad=True)\r\nsigma = Variable(torch.Tensor([1]), requires_grad=True)\r\nx = torch.normal(mu, sigma)\r\nloss = torch.pow(x, 2)\r\nloss.backward()\r\n```\r\nshould produce a RuntimeError."}