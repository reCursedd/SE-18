{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357344366", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-357344366", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 357344366, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzM0NDM2Ng==", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T20:23:00Z", "updated_at": "2018-01-12T20:23:00Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I believe torch.normal sets its output to not requiring grad.</p>\n</blockquote>\n<p>I'm still confused as to why one would error and the other would not. Shouldn't I be able to write my own linear layer <code>W @ x + b</code> and have it do the same thing? It looks like <code>nn.Linear</code> is receiving some special treatment here.</p>\n<blockquote>\n<p>It is useful in case of the reparameterization trick. However, it still doesn't quite make any sense for backprop to work on sampling methods. For example, what would you say is the \"gradient\" for discrete distributions? And we should definitely not allow backprop through things like entire MCMC trace, naive RL rewards, etc. When users want noisy gradients, the best way is to let them manually write out the thing like N(0, 1) in my opinion.</p>\n</blockquote>\n<p>That makes sense. In that case <code>torch.normal</code> should not accept Variables as input, correct?</p>", "body_text": "I believe torch.normal sets its output to not requiring grad.\n\nI'm still confused as to why one would error and the other would not. Shouldn't I be able to write my own linear layer W @ x + b and have it do the same thing? It looks like nn.Linear is receiving some special treatment here.\n\nIt is useful in case of the reparameterization trick. However, it still doesn't quite make any sense for backprop to work on sampling methods. For example, what would you say is the \"gradient\" for discrete distributions? And we should definitely not allow backprop through things like entire MCMC trace, naive RL rewards, etc. When users want noisy gradients, the best way is to let them manually write out the thing like N(0, 1) in my opinion.\n\nThat makes sense. In that case torch.normal should not accept Variables as input, correct?", "body": "> I believe torch.normal sets its output to not requiring grad.\r\n\r\nI'm still confused as to why one would error and the other would not. Shouldn't I be able to write my own linear layer `W @ x + b` and have it do the same thing? It looks like `nn.Linear` is receiving some special treatment here.\r\n\r\n> It is useful in case of the reparameterization trick. However, it still doesn't quite make any sense for backprop to work on sampling methods. For example, what would you say is the \"gradient\" for discrete distributions? And we should definitely not allow backprop through things like entire MCMC trace, naive RL rewards, etc. When users want noisy gradients, the best way is to let them manually write out the thing like N(0, 1) in my opinion.\r\n\r\nThat makes sense. In that case `torch.normal` should not accept Variables as input, correct?"}