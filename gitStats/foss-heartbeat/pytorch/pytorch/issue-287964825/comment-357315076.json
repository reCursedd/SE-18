{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357315076", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-357315076", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 357315076, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzMxNTA3Ng==", "user": {"login": "samuela", "id": 226872, "node_id": "MDQ6VXNlcjIyNjg3Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/226872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuela", "html_url": "https://github.com/samuela", "followers_url": "https://api.github.com/users/samuela/followers", "following_url": "https://api.github.com/users/samuela/following{/other_user}", "gists_url": "https://api.github.com/users/samuela/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuela/subscriptions", "organizations_url": "https://api.github.com/users/samuela/orgs", "repos_url": "https://api.github.com/users/samuela/repos", "events_url": "https://api.github.com/users/samuela/events{/privacy}", "received_events_url": "https://api.github.com/users/samuela/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T18:19:45Z", "updated_at": "2018-01-12T18:19:45Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>It becomes backprop-able because an nn.Linear participates in the the computation. Its parameter requires gradient, so the autograd engine can execute in your second example.</p>\n</blockquote>\n<p>But I've also marked <code>mu</code> and <code>sigma</code> variables as requiring a grad in the first example. What makes them different than <code>nn.Linear</code> in the eyes of <code>backward()</code>?</p>\n<p>Personally I would like to lobby for <code>torch.normal</code> (and friends) to be backprop-able, since it's convenient for cases where one wants to use a reparameterization trick. But I would also understand if they were deemed to not be backprop-able. In that case though, shouldn't pytorch throw an error any time it tries to backprop through <code>torch.normal</code>?</p>", "body_text": "It becomes backprop-able because an nn.Linear participates in the the computation. Its parameter requires gradient, so the autograd engine can execute in your second example.\n\nBut I've also marked mu and sigma variables as requiring a grad in the first example. What makes them different than nn.Linear in the eyes of backward()?\nPersonally I would like to lobby for torch.normal (and friends) to be backprop-able, since it's convenient for cases where one wants to use a reparameterization trick. But I would also understand if they were deemed to not be backprop-able. In that case though, shouldn't pytorch throw an error any time it tries to backprop through torch.normal?", "body": "> It becomes backprop-able because an nn.Linear participates in the the computation. Its parameter requires gradient, so the autograd engine can execute in your second example.\r\n\r\nBut I've also marked `mu` and `sigma` variables as requiring a grad in the first example. What makes them different than `nn.Linear` in the eyes of `backward()`?\r\n\r\nPersonally I would like to lobby for `torch.normal` (and friends) to be backprop-able, since it's convenient for cases where one wants to use a reparameterization trick. But I would also understand if they were deemed to not be backprop-able. In that case though, shouldn't pytorch throw an error any time it tries to backprop through `torch.normal`?"}