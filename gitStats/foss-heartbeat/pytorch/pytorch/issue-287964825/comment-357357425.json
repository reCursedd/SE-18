{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357357425", "html_url": "https://github.com/pytorch/pytorch/issues/4620#issuecomment-357357425", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4620", "id": 357357425, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzM1NzQyNQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T21:21:24Z", "updated_at": "2018-01-12T21:43:37Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I'm still confused as to why one would error and the other would not. Shouldn't I be able to write my own linear layer W @ x + b and have it do the same thing? It looks like nn.Linear is receiving some special treatment here.</p>\n</blockquote>\n<p>nn.Linear has no special treatment. Here is what happens when it is involved.</p>\n<ol>\n<li><code>mu</code>, <code>sigma</code> both require grad, but <code>torch.normal(mu, sigma)</code> doesn't</li>\n<li><code>torch.normal(mu, sigma)</code>, <code>linear.weight</code> and <code>linear.bias</code> interact, produce <code>out</code>. Although <code>torch.normal(mu, sigma)</code> doesn't need grad, <code>linear.weight</code> and <code>linear.bias</code> are <code>nn.Parameter</code> and naturally require grad, so <code>out</code> also does.</li>\n</ol>", "body_text": "I'm still confused as to why one would error and the other would not. Shouldn't I be able to write my own linear layer W @ x + b and have it do the same thing? It looks like nn.Linear is receiving some special treatment here.\n\nnn.Linear has no special treatment. Here is what happens when it is involved.\n\nmu, sigma both require grad, but torch.normal(mu, sigma) doesn't\ntorch.normal(mu, sigma), linear.weight and linear.bias interact, produce out. Although torch.normal(mu, sigma) doesn't need grad, linear.weight and linear.bias are nn.Parameter and naturally require grad, so out also does.", "body": "> I'm still confused as to why one would error and the other would not. Shouldn't I be able to write my own linear layer W @ x + b and have it do the same thing? It looks like nn.Linear is receiving some special treatment here.\r\n\r\nnn.Linear has no special treatment. Here is what happens when it is involved.\r\n1. `mu`, `sigma` both require grad, but `torch.normal(mu, sigma)` doesn't\r\n2. `torch.normal(mu, sigma)`, `linear.weight` and `linear.bias` interact, produce `out`. Although `torch.normal(mu, sigma)` doesn't need grad, `linear.weight` and `linear.bias` are `nn.Parameter` and naturally require grad, so `out` also does."}