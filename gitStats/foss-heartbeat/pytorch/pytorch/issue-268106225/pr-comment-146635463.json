{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146635463", "pull_request_review_id": 71606094, "id": 146635463, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjYzNTQ2Mw==", "diff_hunk": "@@ -164,25 +198,31 @@ __global__ void spatialDepthwiseConvolutionAccGradParameters(\n   AccT grad = ScalarConvert<float, AccT>::to(0.0);\n \n   // Block-stride loop over the number of elements we need to reduce\n-  for (IndexType idx = threadIdx.x; idx < blockElements; idx += blockDim.x) {\n+  const int laneId = threadIdx.x % WARP_SIZE;\n+  const int batch = threadIdx.x / WARP_SIZE;\n+  const int nwarps = blockDim.x / WARP_SIZE;\n+  const int imageElements = outputWidth * outputHeight;\n+  //use warp per item\n+  for (int batchIdx = batch; batchIdx < batchSize; batchIdx += nwarps){  ", "path": "torch/lib/THCUNN/SpatialDepthwiseConvolution.cu", "position": null, "original_position": 162, "commit_id": "ea8eddbfb6fdd3ce06dee2ae89e0a7119a4b4ee0", "original_commit_id": "0d806fdf519d90c0e5c6073c70826d4007069779", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "In the original kernel, a threadblock was used to sum over NHW. I'm using a warp to sum values over HW dimension, and if batchSize is larger than the number of warps, a warp would loop over remaining batch items (e.g. if there are 8 warps, warp 0 would go over 0-8-16 etc image, warp 1 over 1-9-17 etc). Later in blockReduce all the warps will be reduced anyway, thus the full reduction will be over NHW, like it should be. That allows to get rid of one modulo operation inside the loop (because n/batchIdx now does not have to be computed through modulo, you are just looping over it), and brings in nice speed-up. ", "created_at": "2017-10-24T17:32:45Z", "updated_at": "2018-11-23T15:35:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/3265#discussion_r146635463", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3265", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146635463"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3265#discussion_r146635463"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3265"}}, "body_html": "<p>In the original kernel, a threadblock was used to sum over NHW. I'm using a warp to sum values over HW dimension, and if batchSize is larger than the number of warps, a warp would loop over remaining batch items (e.g. if there are 8 warps, warp 0 would go over 0-8-16 etc image, warp 1 over 1-9-17 etc). Later in blockReduce all the warps will be reduced anyway, thus the full reduction will be over NHW, like it should be. That allows to get rid of one modulo operation inside the loop (because n/batchIdx now does not have to be computed through modulo, you are just looping over it), and brings in nice speed-up.</p>", "body_text": "In the original kernel, a threadblock was used to sum over NHW. I'm using a warp to sum values over HW dimension, and if batchSize is larger than the number of warps, a warp would loop over remaining batch items (e.g. if there are 8 warps, warp 0 would go over 0-8-16 etc image, warp 1 over 1-9-17 etc). Later in blockReduce all the warps will be reduced anyway, thus the full reduction will be over NHW, like it should be. That allows to get rid of one modulo operation inside the loop (because n/batchIdx now does not have to be computed through modulo, you are just looping over it), and brings in nice speed-up.", "in_reply_to_id": 146633310}