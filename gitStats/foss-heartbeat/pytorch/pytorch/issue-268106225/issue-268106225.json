{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3265", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3265/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3265/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3265/events", "html_url": "https://github.com/pytorch/pytorch/pull/3265", "id": 268106225, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQ4NDUzMDE1", "number": 3265, "title": "perf improvements for depthwise convolutions", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-10-24T16:46:55Z", "updated_at": "2018-11-23T15:35:40Z", "closed_at": "2017-10-24T18:57:48Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3265", "html_url": "https://github.com/pytorch/pytorch/pull/3265", "diff_url": "https://github.com/pytorch/pytorch/pull/3265.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3265.patch"}, "body_html": "<table>\n<thead>\n<tr>\n<th>batch size</th>\n<th>Input channels</th>\n<th>Height, Width</th>\n<th>kH, kW</th>\n<th>stride</th>\n<th>current time</th>\n<th><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"264386693\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3057\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3057/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3057\">#3057</a> time</th>\n<th>Speed-up</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>32</td>\n<td>112</td>\n<td>3</td>\n<td>1</td>\n<td>0.0003635788</td>\n<td>0.000360384</td>\n<td>0.9912128843</td>\n</tr>\n<tr>\n<td>1</td>\n<td>64</td>\n<td>112</td>\n<td>3</td>\n<td>2</td>\n<td>0.000231576</td>\n<td>0.0003566647</td>\n<td>1.5401626686</td>\n</tr>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>56</td>\n<td>3</td>\n<td>1</td>\n<td>0.0002005291</td>\n<td>0.000383029</td>\n<td>1.9100917868</td>\n</tr>\n<tr>\n<td>1</td>\n<td>128</td>\n<td>56</td>\n<td>3</td>\n<td>2</td>\n<td>0.000126195</td>\n<td>0.0002591753</td>\n<td>2.053769129</td>\n</tr>\n<tr>\n<td>1</td>\n<td>256</td>\n<td>28</td>\n<td>3</td>\n<td>1</td>\n<td>0.0001298428</td>\n<td>0.0003213358</td>\n<td>2.4748071979</td>\n</tr>\n<tr>\n<td>1</td>\n<td>256</td>\n<td>28</td>\n<td>3</td>\n<td>2</td>\n<td>0.0001278973</td>\n<td>0.000162158</td>\n<td>1.2678771158</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>14</td>\n<td>3</td>\n<td>1</td>\n<td>0.0001259661</td>\n<td>0.0001772976</td>\n<td>1.4075027444</td>\n</tr>\n<tr>\n<td>1</td>\n<td>512</td>\n<td>14</td>\n<td>3</td>\n<td>2</td>\n<td>0.0001271725</td>\n<td>0.0001322794</td>\n<td>1.0401574803</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1024</td>\n<td>7</td>\n<td>3</td>\n<td>1</td>\n<td>0.0001308775</td>\n<td>0.0001342106</td>\n<td>1.0254672642</td>\n</tr>\n<tr>\n<td>64</td>\n<td>32</td>\n<td>112</td>\n<td>3</td>\n<td>1</td>\n<td>0.0076541996</td>\n<td>0.0177096987</td>\n<td>2.3137231327</td>\n</tr>\n<tr>\n<td>64</td>\n<td>64</td>\n<td>112</td>\n<td>3</td>\n<td>2</td>\n<td>0.0076271296</td>\n<td>0.0168428278</td>\n<td>2.2082787077</td>\n</tr>\n<tr>\n<td>64</td>\n<td>128</td>\n<td>56</td>\n<td>3</td>\n<td>1</td>\n<td>0.0070373774</td>\n<td>0.0168711519</td>\n<td>2.3973635443</td>\n</tr>\n<tr>\n<td>64</td>\n<td>128</td>\n<td>56</td>\n<td>3</td>\n<td>2</td>\n<td>0.003865943</td>\n<td>0.0091258287</td>\n<td>2.3605699435</td>\n</tr>\n<tr>\n<td>64</td>\n<td>256</td>\n<td>28</td>\n<td>3</td>\n<td>1</td>\n<td>0.0035696888</td>\n<td>0.0085421801</td>\n<td>2.392976124</td>\n</tr>\n<tr>\n<td>64</td>\n<td>256</td>\n<td>28</td>\n<td>3</td>\n<td>2</td>\n<td>0.0021048355</td>\n<td>0.005251503</td>\n<td>2.4949707306</td>\n</tr>\n<tr>\n<td>64</td>\n<td>512</td>\n<td>14</td>\n<td>3</td>\n<td>1</td>\n<td>0.0019623423</td>\n<td>0.0044877005</td>\n<td>2.2869101627</td>\n</tr>\n<tr>\n<td>64</td>\n<td>512</td>\n<td>14</td>\n<td>3</td>\n<td>2</td>\n<td>0.0011884212</td>\n<td>0.0028495121</td>\n<td>2.3977290053</td>\n</tr>\n<tr>\n<td>64</td>\n<td>1024</td>\n<td>7</td>\n<td>3</td>\n<td>1</td>\n<td>0.0012278891</td>\n<td>0.002658639</td>\n<td>2.1652110428</td>\n</tr>\n<tr>\n<td>128</td>\n<td>32</td>\n<td>112</td>\n<td>3</td>\n<td>1</td>\n<td>0.0144340229</td>\n<td>0.0354276562</td>\n<td>2.4544547567</td>\n</tr>\n<tr>\n<td>128</td>\n<td>64</td>\n<td>112</td>\n<td>3</td>\n<td>2</td>\n<td>0.0154968691</td>\n<td>0.0339261246</td>\n<td>2.1892244415</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>56</td>\n<td>3</td>\n<td>1</td>\n<td>0.014062891</td>\n<td>0.0335231686</td>\n<td>2.3838034831</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>56</td>\n<td>3</td>\n<td>2</td>\n<td>0.0080575609</td>\n<td>0.018141408</td>\n<td>2.2514763643</td>\n</tr>\n<tr>\n<td>128</td>\n<td>256</td>\n<td>28</td>\n<td>3</td>\n<td>1</td>\n<td>0.0070493364</td>\n<td>0.0169120741</td>\n<td>2.3991015678</td>\n</tr>\n<tr>\n<td>128</td>\n<td>256</td>\n<td>28</td>\n<td>3</td>\n<td>2</td>\n<td>0.0041349077</td>\n<td>0.0103336859</td>\n<td>2.4991333709</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>14</td>\n<td>3</td>\n<td>1</td>\n<td>0.003833847</td>\n<td>0.0086662102</td>\n<td>2.2604475533</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>14</td>\n<td>3</td>\n<td>2</td>\n<td>0.0022731161</td>\n<td>0.0053928566</td>\n<td>2.3724510024</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>7</td>\n<td>3</td>\n<td>1</td>\n<td>0.0023054218</td>\n<td>0.0047499275</td>\n<td>2.0603290298</td>\n</tr>\n</tbody>\n</table>\n<p>The biggest performance improvements are due to templating kernels. The benchmarks comparing to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"264386693\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3057\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3057/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3057\">#3057</a> performance are above, I've taken sizes from <a href=\"https://github.com/marvis/pytorch-mobilenet/blob/master/benchmark.py#L19-L46\">https://github.com/marvis/pytorch-mobilenet/blob/master/benchmark.py#L19-L46</a> and some are slightly different from what was listed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"264386693\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3057\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3057/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3057\">#3057</a>. Benchmarks are for 50 iterations, time is given per iteration.</p>", "body_text": "batch size\nInput channels\nHeight, Width\nkH, kW\nstride\ncurrent time\n#3057 time\nSpeed-up\n\n\n\n\n1\n32\n112\n3\n1\n0.0003635788\n0.000360384\n0.9912128843\n\n\n1\n64\n112\n3\n2\n0.000231576\n0.0003566647\n1.5401626686\n\n\n1\n128\n56\n3\n1\n0.0002005291\n0.000383029\n1.9100917868\n\n\n1\n128\n56\n3\n2\n0.000126195\n0.0002591753\n2.053769129\n\n\n1\n256\n28\n3\n1\n0.0001298428\n0.0003213358\n2.4748071979\n\n\n1\n256\n28\n3\n2\n0.0001278973\n0.000162158\n1.2678771158\n\n\n1\n512\n14\n3\n1\n0.0001259661\n0.0001772976\n1.4075027444\n\n\n1\n512\n14\n3\n2\n0.0001271725\n0.0001322794\n1.0401574803\n\n\n1\n1024\n7\n3\n1\n0.0001308775\n0.0001342106\n1.0254672642\n\n\n64\n32\n112\n3\n1\n0.0076541996\n0.0177096987\n2.3137231327\n\n\n64\n64\n112\n3\n2\n0.0076271296\n0.0168428278\n2.2082787077\n\n\n64\n128\n56\n3\n1\n0.0070373774\n0.0168711519\n2.3973635443\n\n\n64\n128\n56\n3\n2\n0.003865943\n0.0091258287\n2.3605699435\n\n\n64\n256\n28\n3\n1\n0.0035696888\n0.0085421801\n2.392976124\n\n\n64\n256\n28\n3\n2\n0.0021048355\n0.005251503\n2.4949707306\n\n\n64\n512\n14\n3\n1\n0.0019623423\n0.0044877005\n2.2869101627\n\n\n64\n512\n14\n3\n2\n0.0011884212\n0.0028495121\n2.3977290053\n\n\n64\n1024\n7\n3\n1\n0.0012278891\n0.002658639\n2.1652110428\n\n\n128\n32\n112\n3\n1\n0.0144340229\n0.0354276562\n2.4544547567\n\n\n128\n64\n112\n3\n2\n0.0154968691\n0.0339261246\n2.1892244415\n\n\n128\n128\n56\n3\n1\n0.014062891\n0.0335231686\n2.3838034831\n\n\n128\n128\n56\n3\n2\n0.0080575609\n0.018141408\n2.2514763643\n\n\n128\n256\n28\n3\n1\n0.0070493364\n0.0169120741\n2.3991015678\n\n\n128\n256\n28\n3\n2\n0.0041349077\n0.0103336859\n2.4991333709\n\n\n128\n512\n14\n3\n1\n0.003833847\n0.0086662102\n2.2604475533\n\n\n128\n512\n14\n3\n2\n0.0022731161\n0.0053928566\n2.3724510024\n\n\n128\n1024\n7\n3\n1\n0.0023054218\n0.0047499275\n2.0603290298\n\n\n\nThe biggest performance improvements are due to templating kernels. The benchmarks comparing to #3057 performance are above, I've taken sizes from https://github.com/marvis/pytorch-mobilenet/blob/master/benchmark.py#L19-L46 and some are slightly different from what was listed in #3057. Benchmarks are for 50 iterations, time is given per iteration.", "body": "batch size | Input channels | Height, Width | kH, kW | stride | current time | #3057 time | Speed-up\r\n-- | -- | -- | -- | -- | -- | -- | --\r\n1 | 32 | 112 | 3 | 1 | 0.0003635788 | 0.000360384 | 0.9912128843\r\n1 | 64 | 112 | 3 | 2 | 0.000231576 | 0.0003566647 | 1.5401626686\r\n1 | 128 | 56 | 3 | 1 | 0.0002005291 | 0.000383029 | 1.9100917868\r\n1 | 128 | 56 | 3 | 2 | 0.000126195 | 0.0002591753 | 2.053769129\r\n1 | 256 | 28 | 3 | 1 | 0.0001298428 | 0.0003213358 | 2.4748071979\r\n1 | 256 | 28 | 3 | 2 | 0.0001278973 | 0.000162158 | 1.2678771158\r\n1 | 512 | 14 | 3 | 1 | 0.0001259661 | 0.0001772976 | 1.4075027444\r\n1 | 512 | 14 | 3 | 2 | 0.0001271725 | 0.0001322794 | 1.0401574803\r\n1 | 1024 | 7 | 3 | 1 | 0.0001308775 | 0.0001342106 | 1.0254672642\r\n64 | 32 | 112 | 3 | 1 | 0.0076541996 | 0.0177096987 | 2.3137231327\r\n64 | 64 | 112 | 3 | 2 | 0.0076271296 | 0.0168428278 | 2.2082787077\r\n64 | 128 | 56 | 3 | 1 | 0.0070373774 | 0.0168711519 | 2.3973635443\r\n64 | 128 | 56 | 3 | 2 | 0.003865943 | 0.0091258287 | 2.3605699435\r\n64 | 256 | 28 | 3 | 1 | 0.0035696888 | 0.0085421801 | 2.392976124\r\n64 | 256 | 28 | 3 | 2 | 0.0021048355 | 0.005251503 | 2.4949707306\r\n64 | 512 | 14 | 3 | 1 | 0.0019623423 | 0.0044877005 | 2.2869101627\r\n64 | 512 | 14 | 3 | 2 | 0.0011884212 | 0.0028495121 | 2.3977290053\r\n64 | 1024 | 7 | 3 | 1 | 0.0012278891 | 0.002658639 | 2.1652110428\r\n128 | 32 | 112 | 3 | 1 | 0.0144340229 | 0.0354276562 | 2.4544547567\r\n128 | 64 | 112 | 3 | 2 | 0.0154968691 | 0.0339261246 | 2.1892244415\r\n128 | 128 | 56 | 3 | 1 | 0.014062891 | 0.0335231686 | 2.3838034831\r\n128 | 128 | 56 | 3 | 2 | 0.0080575609 | 0.018141408 | 2.2514763643\r\n128 | 256 | 28 | 3 | 1 | 0.0070493364 | 0.0169120741 | 2.3991015678\r\n128 | 256 | 28 | 3 | 2 | 0.0041349077 | 0.0103336859 | 2.4991333709\r\n128 | 512 | 14 | 3 | 1 | 0.003833847 | 0.0086662102 | 2.2604475533\r\n128 | 512 | 14 | 3 | 2 | 0.0022731161 | 0.0053928566 | 2.3724510024\r\n128 | 1024 | 7 | 3 | 1 | 0.0023054218 | 0.0047499275 | 2.0603290298\r\n\r\nThe biggest performance improvements are due to templating kernels. The benchmarks comparing to #3057 performance are above, I've taken sizes from https://github.com/marvis/pytorch-mobilenet/blob/master/benchmark.py#L19-L46 and some are slightly different from what was listed in #3057. Benchmarks are for 50 iterations, time is given per iteration. \r\n"}