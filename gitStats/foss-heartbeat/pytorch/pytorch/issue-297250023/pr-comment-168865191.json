{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168865191", "pull_request_review_id": 97310334, "id": 168865191, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODg2NTE5MQ==", "diff_hunk": "@@ -0,0 +1,104 @@\n+#include <Python.h>\n+#include \"tensor_dtypes.h\"\n+#include \"torch/csrc/Dtype.h\"\n+#include \"torch/csrc/DynamicTypes.h\"\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/autograd/generated/VariableType.h\"\n+#include \"torch/csrc/utils/tensor_types.h\"\n+\n+namespace torch { namespace utils {\n+\n+static std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {\n+  switch(scalarType) {\n+    case at::ScalarType::Byte:\n+      // no \"byte\" because byte is signed in numpy and we overload\n+      // byte to mean bool often\n+      return std::make_pair(\"uint8\", \"\");\n+    case at::ScalarType::Char:\n+      // no \"char\" because it is not consistently signed or unsigned; we want\n+      // to move to int8\n+      return std::make_pair(\"int8\", \"\");\n+    case at::ScalarType::Double:\n+      return std::make_pair(\"float64\", \"double\");\n+    case at::ScalarType::Float:\n+      return std::make_pair(\"float32\", \"float\");\n+    case at::ScalarType::Int:\n+      return std::make_pair(\"int32\", \"int\");\n+    case at::ScalarType::Long:\n+      return std::make_pair(\"int64\", \"long\");\n+    case at::ScalarType::Short:\n+      return std::make_pair(\"int16\", \"short\");\n+    case at::ScalarType::Half:\n+      return std::make_pair(\"float16\", \"half\");\n+    default:\n+      throw std::runtime_error(\"Unimplemented scalar type\");\n+  }\n+}\n+\n+void initializeDtypes() {\n+  auto torch_module = THPObjectPtr(PyImport_ImportModule(\"torch\"));\n+  if (!torch_module) python_error();\n+  auto cuda_module = THPObjectPtr(PyImport_ImportModule(\"torch.cuda\"));\n+  if (!cuda_module) python_error();\n+  auto sparse_module = THPObjectPtr(PyImport_ImportModule(\"torch.sparse\"));\n+  if (!sparse_module) python_error();\n+  auto cuda_sparse_module = THPObjectPtr(PyImport_ImportModule(\"torch.cuda.sparse\"));\n+  if (!cuda_sparse_module) python_error();\n+  auto& context = at::globalContext();\n+  for (auto type_pair : torch::utils::all_declared_types()) {\n+    at::Backend backend;\n+    at::ScalarType scalarType;\n+    std::tie(backend, scalarType) = type_pair;\n+    std::string primary_name, legacy_name;\n+    std::tie(primary_name, legacy_name) = getDtypeNames(scalarType);\n+    PyObject *module = nullptr;\n+    bool is_cuda, is_sparse;\n+    switch (backend) {\n+      case at::kCPU: {\n+        module = torch_module.get();\n+        is_cuda = false;\n+        is_sparse = false;\n+        break;\n+      }\n+      case at::kCUDA: {\n+        module = cuda_module.get();\n+        is_cuda = true;\n+        is_sparse = false;\n+        break;\n+      }\n+      case at::kSparseCPU: {\n+        module = sparse_module.get();\n+        is_cuda = false;\n+        is_sparse = true;\n+        break;\n+      }\n+      case at::kSparseCUDA: {\n+        module = cuda_sparse_module.get();\n+        is_cuda = true;\n+        is_sparse = true;\n+        break;\n+      }\n+      default: throw std::runtime_error(\"Unimplemented backend\");\n+    }\n+    // use this rather than context.getType() because getType throws exceptions.\n+    auto baseType = context.type_registry[static_cast<int>(backend)][static_cast<int>(scalarType)].get();\n+    auto type = baseType ? torch::autograd::VariableType::getType(*baseType) : nullptr;\n+    std::string name = std::string(PyModule_GetName(module)) + '.' + primary_name;\n+    PyObject *dtype = THPDtype_New(type, name, is_cuda, is_sparse);\n+    if (type) {\n+      torch::registerDtypeObject((THPDtype*)dtype, *type);\n+    }\n+    Py_INCREF(dtype);\n+    if (PyModule_AddObject(module, primary_name.c_str(), (PyObject*)dtype) != 0) {", "path": "torch/csrc/utils/tensor_dtypes.cpp", "position": null, "original_position": 92, "commit_id": "5709cb6ebc6aaa48dca67dc0a4352d23a5e648b5", "original_commit_id": "23d1adf79611d2c8574c310cfdc384414f07de57", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "These `(PyObject*)` casts look unnecessary now", "created_at": "2018-02-16T20:41:35Z", "updated_at": "2018-11-23T15:39:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/5245#discussion_r168865191", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5245", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168865191"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5245#discussion_r168865191"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5245"}}, "body_html": "<p>These <code>(PyObject*)</code> casts look unnecessary now</p>", "body_text": "These (PyObject*) casts look unnecessary now"}