{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12216", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12216/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12216/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12216/events", "html_url": "https://github.com/pytorch/pytorch/issues/12216", "id": 365373071, "node_id": "MDU6SXNzdWUzNjUzNzMwNzE=", "number": 12216, "title": "How to apply multiple transforms on a dataset without recreating the dataset every time?", "user": {"login": "msamogh", "id": 1230386, "node_id": "MDQ6VXNlcjEyMzAzODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1230386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msamogh", "html_url": "https://github.com/msamogh", "followers_url": "https://api.github.com/users/msamogh/followers", "following_url": "https://api.github.com/users/msamogh/following{/other_user}", "gists_url": "https://api.github.com/users/msamogh/gists{/gist_id}", "starred_url": "https://api.github.com/users/msamogh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msamogh/subscriptions", "organizations_url": "https://api.github.com/users/msamogh/orgs", "repos_url": "https://api.github.com/users/msamogh/repos", "events_url": "https://api.github.com/users/msamogh/events{/privacy}", "received_events_url": "https://api.github.com/users/msamogh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-01T09:17:11Z", "updated_at": "2018-10-01T16:22:14Z", "closed_at": "2018-10-01T16:22:14Z", "author_association": "NONE", "body_html": "<p>I have a custom dataset that loads data from a bunch of text files. The dataset resembles a standard multi-class supervised classification problem.</p>\n<p>However, instead of directly training it to classify into one of N classes, I am trying to train N binary classifiers (one classifier for each class). For this, I am transforming the original dataset into a one-vs-all format (where my target class has a label of 1 and all other classes have the label 0). I have created a<br>\ntransformation class called <code>OneVsAll</code> for this purpose which takes in a <code>target_category</code> parameter and transforms the dataset into a \"<code>target_category</code> vs all\" style dataset.</p>\n<p>I would like to be able to create the Dataset object just once and apply N such <code>OneVsAll</code> transforms one by one. But the go-to method for applying transforms takes a single transformation (or a composition of transformations) in the constructor of the Dataset, which means if I used it, I would be creating the dataset N times, which I don't want to do.</p>\n<p>Also, I want to avoid applying transformations on every individual sample as I feel it defeats the purpose of the elegant syntax that comes with something like specifying it in the constructor.</p>\n<p>How should I go about this?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> Dataset, DataLoader\n\n<span class=\"pl-k\">from</span> gensim.models <span class=\"pl-k\">import</span> word2vec\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">SEMCATDataset</span>(<span class=\"pl-e\">Dataset</span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Mapping from canonical category names to filenames</span>\n    <span class=\"pl-c1\">WORDS_DIR</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>../Categories<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-c1\">CATEGORY_FILES</span> <span class=\"pl-k\">=</span> {\n        f.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>]: f <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> os.listdir(<span class=\"pl-c1\">WORDS_DIR</span>)\n    }\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">transform</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.transform <span class=\"pl-k\">=</span> transform\n        <span class=\"pl-c1\">self</span>.words <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> category, words_file <span class=\"pl-k\">in</span> SEMCATDataset.<span class=\"pl-c1\">CATEGORY_FILES</span>.iteritems():\n            <span class=\"pl-c1\">self</span>.words.extend([{\n                <span class=\"pl-s\"><span class=\"pl-pds\">'</span>word<span class=\"pl-pds\">'</span></span>: word,\n                <span class=\"pl-s\"><span class=\"pl-pds\">'</span>category<span class=\"pl-pds\">'</span></span>: category\n            } <span class=\"pl-k\">for</span> word <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>._get_words_for_category(words_file)])\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_get_words_for_category</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">words_file</span>):\n        words_file <span class=\"pl-k\">=</span> os.path.join(\n            SEMCATDataset.<span class=\"pl-c1\">WORDS_DIR</span>, words_file)\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(words_file, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>r<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n            words <span class=\"pl-k\">=</span> <span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x.strip(), f.readlines())\n            <span class=\"pl-k\">return</span> words\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.target_words) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.non_target_words)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n        sample <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.words[idx]\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.transform:\n            sample <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.transform(sample)\n        <span class=\"pl-k\">return</span> sample\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">OneVsAll</span>(<span class=\"pl-c1\">object</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">target_category</span>):\n        <span class=\"pl-c1\">self</span>.target_category <span class=\"pl-k\">=</span> target_category\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">sample</span>):\n        <span class=\"pl-k\">return</span> {\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>word<span class=\"pl-pds\">'</span></span>: sample[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>word<span class=\"pl-pds\">'</span></span>],\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>category<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">int</span>(sample[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>category<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">==</span> <span class=\"pl-c1\">self</span>.target_category)\n        }</pre></div>", "body_text": "I have a custom dataset that loads data from a bunch of text files. The dataset resembles a standard multi-class supervised classification problem.\nHowever, instead of directly training it to classify into one of N classes, I am trying to train N binary classifiers (one classifier for each class). For this, I am transforming the original dataset into a one-vs-all format (where my target class has a label of 1 and all other classes have the label 0). I have created a\ntransformation class called OneVsAll for this purpose which takes in a target_category parameter and transforms the dataset into a \"target_category vs all\" style dataset.\nI would like to be able to create the Dataset object just once and apply N such OneVsAll transforms one by one. But the go-to method for applying transforms takes a single transformation (or a composition of transformations) in the constructor of the Dataset, which means if I used it, I would be creating the dataset N times, which I don't want to do.\nAlso, I want to avoid applying transformations on every individual sample as I feel it defeats the purpose of the elegant syntax that comes with something like specifying it in the constructor.\nHow should I go about this?\nimport os\nimport random\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom gensim.models import word2vec\n\n\nclass SEMCATDataset(Dataset):\n\n    # Mapping from canonical category names to filenames\n    WORDS_DIR = '../Categories'\n    CATEGORY_FILES = {\n        f.split('-')[0]: f for f in os.listdir(WORDS_DIR)\n    }\n\n    def __init__(self, transform=None):\n        self.transform = transform\n        self.words = []\n        for category, words_file in SEMCATDataset.CATEGORY_FILES.iteritems():\n            self.words.extend([{\n                'word': word,\n                'category': category\n            } for word in self._get_words_for_category(words_file)])\n\n    def _get_words_for_category(self, words_file):\n        words_file = os.path.join(\n            SEMCATDataset.WORDS_DIR, words_file)\n        with open(words_file, 'r') as f:\n            words = map(lambda x: x.strip(), f.readlines())\n            return words\n\n    def __len__(self):\n        return len(self.target_words) + len(self.non_target_words)\n\n    def __getitem__(self, idx):\n        sample = self.words[idx]\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n\nclass OneVsAll(object):\n\n    def __init__(self, target_category):\n        self.target_category = target_category\n\n    def __call__(self, sample):\n        return {\n            'word': sample['word'],\n            'category': int(sample['category'] == self.target_category)\n        }", "body": "I have a custom dataset that loads data from a bunch of text files. The dataset resembles a standard multi-class supervised classification problem.\r\n\r\nHowever, instead of directly training it to classify into one of N classes, I am trying to train N binary classifiers (one classifier for each class). For this, I am transforming the original dataset into a one-vs-all format (where my target class has a label of 1 and all other classes have the label 0). I have created a \r\ntransformation class called `OneVsAll` for this purpose which takes in a `target_category` parameter and transforms the dataset into a \"`target_category` vs all\" style dataset.\r\n\r\nI would like to be able to create the Dataset object just once and apply N such `OneVsAll` transforms one by one. But the go-to method for applying transforms takes a single transformation (or a composition of transformations) in the constructor of the Dataset, which means if I used it, I would be creating the dataset N times, which I don't want to do.\r\n\r\nAlso, I want to avoid applying transformations on every individual sample as I feel it defeats the purpose of the elegant syntax that comes with something like specifying it in the constructor.\r\n\r\nHow should I go about this?\r\n\r\n```python\r\nimport os\r\nimport random\r\n\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom gensim.models import word2vec\r\n\r\n\r\nclass SEMCATDataset(Dataset):\r\n\r\n    # Mapping from canonical category names to filenames\r\n    WORDS_DIR = '../Categories'\r\n    CATEGORY_FILES = {\r\n        f.split('-')[0]: f for f in os.listdir(WORDS_DIR)\r\n    }\r\n\r\n    def __init__(self, transform=None):\r\n        self.transform = transform\r\n        self.words = []\r\n        for category, words_file in SEMCATDataset.CATEGORY_FILES.iteritems():\r\n            self.words.extend([{\r\n                'word': word,\r\n                'category': category\r\n            } for word in self._get_words_for_category(words_file)])\r\n\r\n    def _get_words_for_category(self, words_file):\r\n        words_file = os.path.join(\r\n            SEMCATDataset.WORDS_DIR, words_file)\r\n        with open(words_file, 'r') as f:\r\n            words = map(lambda x: x.strip(), f.readlines())\r\n            return words\r\n\r\n    def __len__(self):\r\n        return len(self.target_words) + len(self.non_target_words)\r\n\r\n    def __getitem__(self, idx):\r\n        sample = self.words[idx]\r\n        if self.transform:\r\n            sample = self.transform(sample)\r\n        return sample\r\n\r\n\r\nclass OneVsAll(object):\r\n\r\n    def __init__(self, target_category):\r\n        self.target_category = target_category\r\n\r\n    def __call__(self, sample):\r\n        return {\r\n            'word': sample['word'],\r\n            'category': int(sample['category'] == self.target_category)\r\n        }\r\n```"}