{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8853", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8853/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8853/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8853/events", "html_url": "https://github.com/pytorch/pytorch/issues/8853", "id": 335497470, "node_id": "MDU6SXNzdWUzMzU0OTc0NzA=", "number": 8853, "title": "Todo functions and autograd supports for Sparse Tensor", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/sparse", "name": "sparse", "color": "bfd4f2", "default": false}], "state": "open", "locked": false, "assignee": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-06-25T17:23:07Z", "updated_at": "2018-11-18T06:50:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Here summarizes a list of requested Sparse Tensor functions and autograd supports from previous PRs. Please feel free to comment on functions that should be added also.</p>\n<h2>Functions</h2>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>sum()</code> with autograd</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>max()</code> with autograd</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>log1p()</code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336409339\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8969\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8969/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8969\">#8969</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>S.copy_(S) with autograd </code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336798976\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9005\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9005/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9005\">#9005</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> indexing (<code>gather()</code>, <code>index_select()</code>)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>mul_(S, D) -&gt; S</code>, <code>mul(S, D) -&gt; S</code> with autograd</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>cuda()</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>nn.Linear</code> with autograd (SxS, SxD, relies on <code>addmm</code> and <code>matmul</code>)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>softmax()</code> with autograd (same as in <a href=\"https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/sparse-softmax\" rel=\"nofollow\">TF</a>: Applies softmax() to a region of a densified tensor submatrix; (2) Masks out the zero locations; (3) Renormalizes the remaining elements. SparseTensor result has exactly the same non-zero indices and shape)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>to_sparse()</code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"364941082\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12171\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12171/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12171\">#12171</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>narrow_copy()</code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357783602\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11342\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11342/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11342\">#11342</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>sparse_mm(S, D) -&gt; D</code> with autograd</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>cat()</code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"377508084\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13577\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13577/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13577\">#13577</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>unsqueeze()</code>, <code>stack()</code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379001587\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13760\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13760/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13760\">#13760</a></li>\n</ul>\n<h3>Wish list</h3>\n<ul>\n<li><code>bmm(S, D)</code> (add an extra sparse dim at <code>indices</code> of SparseTensor as batch dim?)</li>\n<li>broadcasting <code>mul(S, D) -&gt; S</code></li>\n<li><code>Dataset</code>, <code>Dataloader</code></li>\n<li><code>coalesce()</code> backward (some sparse ops can be more efficient if input tensor is already coalesced, so it makes sense to implement backward for coalesce)</li>\n</ul>\n<h2>Existing</h2>\n<ul>\n<li>autograd supported for <code>values()</code> via <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"373164155\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13001\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13001/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13001\">#13001</a> (Thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a>!), that means all element-wise ops are supported in sparse now</li>\n<li>norm (cannot take <code>dim</code> args)</li>\n<li>pow</li>\n<li>clone</li>\n<li>zero_</li>\n<li>t_ / t</li>\n<li>add_ / add(Sparse, Sparse, Scalar) -&gt; Sparse</li>\n<li>add_ / add(Dense, Sparse, Scalar) -&gt; Dense</li>\n<li>sub_ / sub(Sparse, Sparse, Scalar) -&gt; Sparse</li>\n<li>mul_ / mul(Sparse, Sparse) -&gt; Sparse</li>\n<li>mul_ / mul(Sparse, Scalar) -&gt; Sparse</li>\n<li>div_ / div(Sparse, Scalar) -&gt; Sparse</li>\n<li>addmm(Dense, Sparse, Dense, Scalar, Scalar) -&gt; Dense</li>\n<li>sspaddmm(Sparse, Sparse, Dense, Scalar, Scalar) -&gt; Sparse</li>\n<li>mm(Sparse, Dense) -&gt; Dense</li>\n<li>smm(Sparse, Dense) -&gt; Sparse</li>\n<li>hspmm(Sparse, Dense) -&gt; HybridSparse</li>\n<li>spmm(Sparse, Dense) -&gt; Dense</li>\n</ul>", "body_text": "Here summarizes a list of requested Sparse Tensor functions and autograd supports from previous PRs. Please feel free to comment on functions that should be added also.\nFunctions\n\n sum() with autograd\n max() with autograd\n log1p() #8969\n S.copy_(S) with autograd  #9005\n indexing (gather(), index_select())\n mul_(S, D) -> S, mul(S, D) -> S with autograd\n cuda()\n nn.Linear with autograd (SxS, SxD, relies on addmm and matmul)\n softmax() with autograd (same as in TF: Applies softmax() to a region of a densified tensor submatrix; (2) Masks out the zero locations; (3) Renormalizes the remaining elements. SparseTensor result has exactly the same non-zero indices and shape)\n to_sparse() #12171\n narrow_copy() #11342\n sparse_mm(S, D) -> D with autograd\n cat() #13577\n unsqueeze(), stack() #13760\n\nWish list\n\nbmm(S, D) (add an extra sparse dim at indices of SparseTensor as batch dim?)\nbroadcasting mul(S, D) -> S\nDataset, Dataloader\ncoalesce() backward (some sparse ops can be more efficient if input tensor is already coalesced, so it makes sense to implement backward for coalesce)\n\nExisting\n\nautograd supported for values() via #13001 (Thanks to @SsnL!), that means all element-wise ops are supported in sparse now\nnorm (cannot take dim args)\npow\nclone\nzero_\nt_ / t\nadd_ / add(Sparse, Sparse, Scalar) -> Sparse\nadd_ / add(Dense, Sparse, Scalar) -> Dense\nsub_ / sub(Sparse, Sparse, Scalar) -> Sparse\nmul_ / mul(Sparse, Sparse) -> Sparse\nmul_ / mul(Sparse, Scalar) -> Sparse\ndiv_ / div(Sparse, Scalar) -> Sparse\naddmm(Dense, Sparse, Dense, Scalar, Scalar) -> Dense\nsspaddmm(Sparse, Sparse, Dense, Scalar, Scalar) -> Sparse\nmm(Sparse, Dense) -> Dense\nsmm(Sparse, Dense) -> Sparse\nhspmm(Sparse, Dense) -> HybridSparse\nspmm(Sparse, Dense) -> Dense", "body": "Here summarizes a list of requested Sparse Tensor functions and autograd supports from previous PRs. Please feel free to comment on functions that should be added also.\r\n\r\n## Functions\r\n- [ ] `sum()` with autograd\r\n- [ ] `max()` with autograd\r\n- [x] `log1p()` https://github.com/pytorch/pytorch/pull/8969\r\n- [x] `S.copy_(S) with autograd ` https://github.com/pytorch/pytorch/pull/9005\r\n- [ ] indexing (`gather()`, `index_select()`)\r\n- [ ] `mul_(S, D) -> S`, `mul(S, D) -> S` with autograd\r\n- [x] `cuda()`\r\n- [ ] `nn.Linear` with autograd (SxS, SxD, relies on `addmm` and `matmul`)\r\n- [ ] `softmax()` with autograd (same as in [TF](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/sparse-softmax): Applies softmax() to a region of a densified tensor submatrix; (2) Masks out the zero locations; (3) Renormalizes the remaining elements. SparseTensor result has exactly the same non-zero indices and shape)\r\n- [x] `to_sparse()` #12171\r\n- [x] `narrow_copy()` https://github.com/pytorch/pytorch/pull/11342\r\n- [ ] `sparse_mm(S, D) -> D` with autograd\r\n- [x] `cat()` https://github.com/pytorch/pytorch/pull/13577\r\n- [x] `unsqueeze()`, `stack()` https://github.com/pytorch/pytorch/pull/13760\r\n\r\n### Wish list\r\n- `bmm(S, D)` (add an extra sparse dim at `indices` of SparseTensor as batch dim?)\r\n- broadcasting `mul(S, D) -> S`\r\n- `Dataset`, `Dataloader`\r\n- `coalesce()` backward (some sparse ops can be more efficient if input tensor is already coalesced, so it makes sense to implement backward for coalesce)\r\n\r\n## Existing\r\n- autograd supported for `values()` via https://github.com/pytorch/pytorch/pull/13001 (Thanks to @SsnL!), that means all element-wise ops are supported in sparse now\r\n- norm (cannot take `dim` args)\r\n- pow\r\n- clone\r\n- zero_\r\n- t_ / t\r\n- add_ / add(Sparse, Sparse, Scalar) -> Sparse\r\n- add_ / add(Dense, Sparse, Scalar) -> Dense\r\n- sub_ / sub(Sparse, Sparse, Scalar) -> Sparse\r\n- mul_ / mul(Sparse, Sparse) -> Sparse\r\n- mul_ / mul(Sparse, Scalar) -> Sparse\r\n- div_ / div(Sparse, Scalar) -> Sparse\r\n- addmm(Dense, Sparse, Dense, Scalar, Scalar) -> Dense\r\n- sspaddmm(Sparse, Sparse, Dense, Scalar, Scalar) -> Sparse\r\n- mm(Sparse, Dense) -> Dense\r\n- smm(Sparse, Dense) -> Sparse\r\n- hspmm(Sparse, Dense) -> HybridSparse\r\n- spmm(Sparse, Dense) -> Dense"}