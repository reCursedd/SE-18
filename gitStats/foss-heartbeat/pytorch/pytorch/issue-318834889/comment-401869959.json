{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/401869959", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-401869959", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 401869959, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTg2OTk1OQ==", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-02T17:02:44Z", "updated_at": "2018-07-02T18:56:30Z", "author_association": "NONE", "body_html": "<p>Okay... running through this process, here are some things to watch out for:</p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"322495960\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7520\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7520/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7520\">#7520</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"337573445\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9105\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9105/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9105\">#9105</a></li>\n</ul>\n<p>Got distributed running and the error went away but I would not recommend this approach. This is due to:</p>\n<ul>\n<li>The training was approx. 4x slower (1 vs 4 minutes) (Using a single GRU layer running over sequences of 900 with the same batch size 32 split over 4 GPUs.)</li>\n<li>The codebase needs to be restructured into a master worker script (e.g. the same script runs master and worker nodes)</li>\n<li>Because there are multiple processes in a master and worker relationship, the log files become very cluttered with the same logs from both the master and workers. The relationship also means that saving checkpoints or any files for that matter should be hedged to the master node.</li>\n</ul>\n<p>Other notes:</p>\n<ul>\n<li>Distributed should be faster for an RNN that is unrolled with a loop due to:\n<blockquote>\n<p>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and \u201cGIL-thrashing\u201d that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</p>\n</blockquote>\n</li>\n<li>It's not clear why something similar to <code>torch.nn.parallel.data_parallel</code> cannot be added to PyTorch but rather than launching threads it launches processes.</li>\n</ul>", "body_text": "Okay... running through this process, here are some things to watch out for:\n\n#7520\n#9105\n\nGot distributed running and the error went away but I would not recommend this approach. This is due to:\n\nThe training was approx. 4x slower (1 vs 4 minutes) (Using a single GRU layer running over sequences of 900 with the same batch size 32 split over 4 GPUs.)\nThe codebase needs to be restructured into a master worker script (e.g. the same script runs master and worker nodes)\nBecause there are multiple processes in a master and worker relationship, the log files become very cluttered with the same logs from both the master and workers. The relationship also means that saving checkpoints or any files for that matter should be hedged to the master node.\n\nOther notes:\n\nDistributed should be faster for an RNN that is unrolled with a loop due to:\n\nEach process contains an independent Python interpreter, eliminating the extra interpreter overhead and \u201cGIL-thrashing\u201d that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.\n\n\nIt's not clear why something similar to torch.nn.parallel.data_parallel cannot be added to PyTorch but rather than launching threads it launches processes.", "body": "Okay... running through this process, here are some things to watch out for:\r\n\r\n- https://github.com/pytorch/pytorch/pull/7520\r\n- https://github.com/pytorch/pytorch/issues/9105\r\n\r\nGot distributed running and the error went away but I would not recommend this approach. This is due to:\r\n- The training was approx. 4x slower (1 vs 4 minutes) (Using a single GRU layer running over sequences of 900 with the same batch size 32 split over 4 GPUs.)\r\n- The codebase needs to be restructured into a master worker script (e.g. the same script runs master and worker nodes)\r\n- Because there are multiple processes in a master and worker relationship, the log files become very cluttered with the same logs from both the master and workers. The relationship also means that saving checkpoints or any files for that matter should be hedged to the master node.\r\n\r\nOther notes:\r\n* Distributed should be faster for an RNN that is unrolled with a loop due to:\r\n   > Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and \u201cGIL-thrashing\u201d that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.\r\n* It's not clear why something similar to ``torch.nn.parallel.data_parallel`` cannot be added to PyTorch but rather than launching threads it launches processes."}