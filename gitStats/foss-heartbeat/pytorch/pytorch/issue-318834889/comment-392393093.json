{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392393093", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-392393093", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 392393093, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjM5MzA5Mw==", "user": {"login": "wangkenpu", "id": 15028609, "node_id": "MDQ6VXNlcjE1MDI4NjA5", "avatar_url": "https://avatars1.githubusercontent.com/u/15028609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangkenpu", "html_url": "https://github.com/wangkenpu", "followers_url": "https://api.github.com/users/wangkenpu/followers", "following_url": "https://api.github.com/users/wangkenpu/following{/other_user}", "gists_url": "https://api.github.com/users/wangkenpu/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangkenpu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangkenpu/subscriptions", "organizations_url": "https://api.github.com/users/wangkenpu/orgs", "repos_url": "https://api.github.com/users/wangkenpu/repos", "events_url": "https://api.github.com/users/wangkenpu/events{/privacy}", "received_events_url": "https://api.github.com/users/wangkenpu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-28T00:53:45Z", "updated_at": "2018-05-28T02:11:39Z", "author_association": "NONE", "body_html": "<p>I met the same problem with PyTorch 0.4.0. I was wondering if this bug have been solved?</p>\n<p>I found if I deleted <code>self.gru.flatten_parameters()</code> in <code>forward()</code> function, every thing worked well. But I faced another  warning.</p>\n<pre><code>UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n</code></pre>", "body_text": "I met the same problem with PyTorch 0.4.0. I was wondering if this bug have been solved?\nI found if I deleted self.gru.flatten_parameters() in forward() function, every thing worked well. But I faced another  warning.\nUserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().", "body": "I met the same problem with PyTorch 0.4.0. I was wondering if this bug have been solved?\r\n\r\nI found if I deleted ```self.gru.flatten_parameters()``` in ```forward()``` function, every thing worked well. But I faced another  warning.\r\n\r\n```\r\nUserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\r\n```"}