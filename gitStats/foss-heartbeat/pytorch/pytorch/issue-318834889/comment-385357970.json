{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385357970", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-385357970", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 385357970, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTM1Nzk3MA==", "user": {"login": "erogol", "id": 1402048, "node_id": "MDQ6VXNlcjE0MDIwNDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1402048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erogol", "html_url": "https://github.com/erogol", "followers_url": "https://api.github.com/users/erogol/followers", "following_url": "https://api.github.com/users/erogol/following{/other_user}", "gists_url": "https://api.github.com/users/erogol/gists{/gist_id}", "starred_url": "https://api.github.com/users/erogol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erogol/subscriptions", "organizations_url": "https://api.github.com/users/erogol/orgs", "repos_url": "https://api.github.com/users/erogol/repos", "events_url": "https://api.github.com/users/erogol/events{/privacy}", "received_events_url": "https://api.github.com/users/erogol/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-30T10:07:46Z", "updated_at": "2018-04-30T10:07:46Z", "author_association": "NONE", "body_html": "<p>The problem is flattening the parameters. If I dont use it everything works with a warning suggesting it.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.gru <span class=\"pl-k\">=</span> torch.nn.GRU(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                                \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>):\n        <span class=\"pl-c1\">self</span>.gru.flatten_parameters()\n        out, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru(inp)\n        <span class=\"pl-k\">return</span> out\n                        \n\nnet <span class=\"pl-k\">=</span> Net()\ninp <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nnet <span class=\"pl-k\">=</span> torch.nn.DataParallel(net)\n\ninp <span class=\"pl-k\">=</span> inp.cuda()\nnet <span class=\"pl-k\">=</span> net.cuda()\nout <span class=\"pl-k\">=</span> net.forward(inp)</pre></div>", "body_text": "The problem is flattening the parameters. If I dont use it everything works with a warning suggesting it.\nimport torch\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.gru = torch.nn.GRU(128, 64, 1, batch_first=True, bidirectional=True)\n                                \n    def forward(self, inp):\n        self.gru.flatten_parameters()\n        out, _ = self.gru(inp)\n        return out\n                        \n\nnet = Net()\ninp = torch.rand(32, 8, 128, requires_grad=True)\n\nnet = torch.nn.DataParallel(net)\n\ninp = inp.cuda()\nnet = net.cuda()\nout = net.forward(inp)", "body": "The problem is flattening the parameters. If I dont use it everything works with a warning suggesting it.\r\n\r\n```python\r\nimport torch\r\n\r\nclass Net(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.gru = torch.nn.GRU(128, 64, 1, batch_first=True, bidirectional=True)\r\n                                \r\n    def forward(self, inp):\r\n        self.gru.flatten_parameters()\r\n        out, _ = self.gru(inp)\r\n        return out\r\n                        \r\n\r\nnet = Net()\r\ninp = torch.rand(32, 8, 128, requires_grad=True)\r\n\r\nnet = torch.nn.DataParallel(net)\r\n\r\ninp = inp.cuda()\r\nnet = net.cuda()\r\nout = net.forward(inp)\r\n```"}