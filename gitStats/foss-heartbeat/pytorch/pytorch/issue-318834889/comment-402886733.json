{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/402886733", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-402886733", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 402886733, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjg4NjczMw==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-06T00:08:58Z", "updated_at": "2018-07-06T00:08:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7424737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PetrochukM\">@PetrochukM</a> ,</p>\n<ul>\n<li>May I ask what operation need to be run only once? Since each process of DDP is exactly the same as one GPU running on different batches. Running an operation only on master but not all of them basically means you want to apply something on some batches but not the others. Please let me know if you have any context on this.</li>\n<li>If I understand correctly, you are comparing DataParallel(1 process with 4GPUs) with batchsize 32, to 4 DistributedDataParallel processes(1 process per GPU) where batchsize is 8. This is a fair comparison. If that's already the case, it might be the computation time / communication time is too little that means you can simply increase the batchsize per gpu to saturate the GPU first and then compare. In the later case I could help if needed if you post a simple script here. Let me know!</li>\n</ul>", "body_text": "Hi @PetrochukM ,\n\nMay I ask what operation need to be run only once? Since each process of DDP is exactly the same as one GPU running on different batches. Running an operation only on master but not all of them basically means you want to apply something on some batches but not the others. Please let me know if you have any context on this.\nIf I understand correctly, you are comparing DataParallel(1 process with 4GPUs) with batchsize 32, to 4 DistributedDataParallel processes(1 process per GPU) where batchsize is 8. This is a fair comparison. If that's already the case, it might be the computation time / communication time is too little that means you can simply increase the batchsize per gpu to saturate the GPU first and then compare. In the later case I could help if needed if you post a simple script here. Let me know!", "body": "Hi @PetrochukM , \r\n- May I ask what operation need to be run only once? Since each process of DDP is exactly the same as one GPU running on different batches. Running an operation only on master but not all of them basically means you want to apply something on some batches but not the others. Please let me know if you have any context on this.\r\n- If I understand correctly, you are comparing DataParallel(1 process with 4GPUs) with batchsize 32, to 4 DistributedDataParallel processes(1 process per GPU) where batchsize is 8. This is a fair comparison. If that's already the case, it might be the computation time / communication time is too little that means you can simply increase the batchsize per gpu to saturate the GPU first and then compare. In the later case I could help if needed if you post a simple script here. Let me know!"}