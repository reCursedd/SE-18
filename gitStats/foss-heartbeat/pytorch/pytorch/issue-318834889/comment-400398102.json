{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/400398102", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-400398102", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 400398102, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDM5ODEwMg==", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-26T17:29:11Z", "updated_at": "2018-06-26T17:29:20Z", "author_association": "NONE", "body_html": "<p>Any chance applying distributed to training will be as easy as calling a function like: <code>torch.nn.parallel.data_parallel</code>? Thanks for your help!</p>\n<p>Folks, btw, here is a good example of applying distributed to Pytorch: <a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a></p>\n<p>It's a bit cumbersome but doable.</p>", "body_text": "Any chance applying distributed to training will be as easy as calling a function like: torch.nn.parallel.data_parallel? Thanks for your help!\nFolks, btw, here is a good example of applying distributed to Pytorch: https://github.com/pytorch/examples/blob/master/imagenet/main.py\nIt's a bit cumbersome but doable.", "body": "Any chance applying distributed to training will be as easy as calling a function like: ``torch.nn.parallel.data_parallel``? Thanks for your help!\r\n\r\nFolks, btw, here is a good example of applying distributed to Pytorch: https://github.com/pytorch/examples/blob/master/imagenet/main.py\r\n\r\nIt's a bit cumbersome but doable. "}