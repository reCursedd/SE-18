{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392637625", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-392637625", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 392637625, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjYzNzYyNQ==", "user": {"login": "eriche2016", "id": 11784910, "node_id": "MDQ6VXNlcjExNzg0OTEw", "avatar_url": "https://avatars2.githubusercontent.com/u/11784910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eriche2016", "html_url": "https://github.com/eriche2016", "followers_url": "https://api.github.com/users/eriche2016/followers", "following_url": "https://api.github.com/users/eriche2016/following{/other_user}", "gists_url": "https://api.github.com/users/eriche2016/gists{/gist_id}", "starred_url": "https://api.github.com/users/eriche2016/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eriche2016/subscriptions", "organizations_url": "https://api.github.com/users/eriche2016/orgs", "repos_url": "https://api.github.com/users/eriche2016/repos", "events_url": "https://api.github.com/users/eriche2016/events{/privacy}", "received_events_url": "https://api.github.com/users/eriche2016/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T02:44:07Z", "updated_at": "2018-05-29T02:44:07Z", "author_association": "NONE", "body_html": "<p>Can anyone give tips on how to use DistributedDataParallel , I am trying to use  DistributedDataParallel<br>\nas follows,</p>\n<pre><code>import torch\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.gru = torch.nn.GRU(128, 64, 1, batch_first=True, bidirectional=True)\n                                \n    def forward(self, inp):\n        # self.gru.flatten_parameters()\n        out, _ = self.gru(inp)\n        return out\n                        \n\nnet = Net()\ninp = torch.rand(32, 8, 128, requires_grad=True)\n\ninp = inp.cuda()\nnet = net.cuda()\nif False: \n    net = torch.nn.DataParallel(net, device_ids=range(2))\nelse: \n    net = torch.nn.parallel.DistributedDataParallel(net, device_ids=range(2))\n\n\nout = net.forward(inp)\n\n</code></pre>\n<p>gives the errror</p>\n<pre><code>AssertionError: collective only supported in process-group mode\n</code></pre>", "body_text": "Can anyone give tips on how to use DistributedDataParallel , I am trying to use  DistributedDataParallel\nas follows,\nimport torch\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.gru = torch.nn.GRU(128, 64, 1, batch_first=True, bidirectional=True)\n                                \n    def forward(self, inp):\n        # self.gru.flatten_parameters()\n        out, _ = self.gru(inp)\n        return out\n                        \n\nnet = Net()\ninp = torch.rand(32, 8, 128, requires_grad=True)\n\ninp = inp.cuda()\nnet = net.cuda()\nif False: \n    net = torch.nn.DataParallel(net, device_ids=range(2))\nelse: \n    net = torch.nn.parallel.DistributedDataParallel(net, device_ids=range(2))\n\n\nout = net.forward(inp)\n\n\ngives the errror\nAssertionError: collective only supported in process-group mode", "body": "Can anyone give tips on how to use DistributedDataParallel , I am trying to use  DistributedDataParallel\r\nas follows,  \r\n```\r\nimport torch\r\n\r\nclass Net(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.gru = torch.nn.GRU(128, 64, 1, batch_first=True, bidirectional=True)\r\n                                \r\n    def forward(self, inp):\r\n        # self.gru.flatten_parameters()\r\n        out, _ = self.gru(inp)\r\n        return out\r\n                        \r\n\r\nnet = Net()\r\ninp = torch.rand(32, 8, 128, requires_grad=True)\r\n\r\ninp = inp.cuda()\r\nnet = net.cuda()\r\nif False: \r\n    net = torch.nn.DataParallel(net, device_ids=range(2))\r\nelse: \r\n    net = torch.nn.parallel.DistributedDataParallel(net, device_ids=range(2))\r\n\r\n\r\nout = net.forward(inp)\r\n\r\n```\r\ngives the errror\r\n```\r\nAssertionError: collective only supported in process-group mode\r\n```"}