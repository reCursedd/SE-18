{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/402879869", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-402879869", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 402879869, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjg3OTg2OQ==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-05T23:22:22Z", "updated_at": "2018-07-05T23:22:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7424737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PetrochukM\">@PetrochukM</a> , to answer your questions:</p>\n<ul>\n<li>Since you are using single machine with DisitributedDataParallel, yes 1 process per GPU is recommended. An alternative is using 1 process for 8 GPUs along with DataParallel.</li>\n<li>Again for one node DDP, tcp and file initialization are the same. Since with tcp you will always use 127.0.0.1 as master address and this is for sure accessible from all processes. Same for the file initialization. You will need to be careful if you train DDP on multiple machines. tcp requires the machines pingable among each other and file initilization require a shared file system among all nodes.</li>\n<li>The codebase definitely doesn't have to be restructured as master-worker. An example is <a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a>. If you have to do that, something might be wrong. You are welcome to post a minimal repro of your script here so that we can help.</li>\n<li>Yes the log file can be messy if all processes write to the same file. However this can be relieved by either having rank printed in the beginning of the line, or let processes write to a filename suffixed with rank so that they are separated.</li>\n<li>I'm not sure why you see 4x slower performance when scaling from 1GPU to 4. Could you  first confirm that you are not running 4 times of the dataset in 4GPU case? Or you can post a toy script so that I can take a look.<br>\nHope these answer your questions above. Feel free to let me know if I'm missing something.</li>\n</ul>", "body_text": "Hi @PetrochukM , to answer your questions:\n\nSince you are using single machine with DisitributedDataParallel, yes 1 process per GPU is recommended. An alternative is using 1 process for 8 GPUs along with DataParallel.\nAgain for one node DDP, tcp and file initialization are the same. Since with tcp you will always use 127.0.0.1 as master address and this is for sure accessible from all processes. Same for the file initialization. You will need to be careful if you train DDP on multiple machines. tcp requires the machines pingable among each other and file initilization require a shared file system among all nodes.\nThe codebase definitely doesn't have to be restructured as master-worker. An example is https://github.com/pytorch/examples/blob/master/imagenet/main.py. If you have to do that, something might be wrong. You are welcome to post a minimal repro of your script here so that we can help.\nYes the log file can be messy if all processes write to the same file. However this can be relieved by either having rank printed in the beginning of the line, or let processes write to a filename suffixed with rank so that they are separated.\nI'm not sure why you see 4x slower performance when scaling from 1GPU to 4. Could you  first confirm that you are not running 4 times of the dataset in 4GPU case? Or you can post a toy script so that I can take a look.\nHope these answer your questions above. Feel free to let me know if I'm missing something.", "body": "Hi @PetrochukM , to answer your questions:\r\n- Since you are using single machine with DisitributedDataParallel, yes 1 process per GPU is recommended. An alternative is using 1 process for 8 GPUs along with DataParallel. \r\n- Again for one node DDP, tcp and file initialization are the same. Since with tcp you will always use 127.0.0.1 as master address and this is for sure accessible from all processes. Same for the file initialization. You will need to be careful if you train DDP on multiple machines. tcp requires the machines pingable among each other and file initilization require a shared file system among all nodes. \r\n- The codebase definitely doesn't have to be restructured as master-worker. An example is https://github.com/pytorch/examples/blob/master/imagenet/main.py. If you have to do that, something might be wrong. You are welcome to post a minimal repro of your script here so that we can help.\r\n- Yes the log file can be messy if all processes write to the same file. However this can be relieved by either having rank printed in the beginning of the line, or let processes write to a filename suffixed with rank so that they are separated.\r\n- I'm not sure why you see 4x slower performance when scaling from 1GPU to 4. Could you  first confirm that you are not running 4 times of the dataset in 4GPU case? Or you can post a toy script so that I can take a look.\r\nHope these answer your questions above. Feel free to let me know if I'm missing something."}