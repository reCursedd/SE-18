{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407963616", "html_url": "https://github.com/pytorch/pytorch/issues/7092#issuecomment-407963616", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7092", "id": 407963616, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzk2MzYxNg==", "user": {"login": "Aspire1Inspire2", "id": 12798470, "node_id": "MDQ6VXNlcjEyNzk4NDcw", "avatar_url": "https://avatars2.githubusercontent.com/u/12798470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aspire1Inspire2", "html_url": "https://github.com/Aspire1Inspire2", "followers_url": "https://api.github.com/users/Aspire1Inspire2/followers", "following_url": "https://api.github.com/users/Aspire1Inspire2/following{/other_user}", "gists_url": "https://api.github.com/users/Aspire1Inspire2/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aspire1Inspire2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aspire1Inspire2/subscriptions", "organizations_url": "https://api.github.com/users/Aspire1Inspire2/orgs", "repos_url": "https://api.github.com/users/Aspire1Inspire2/repos", "events_url": "https://api.github.com/users/Aspire1Inspire2/events{/privacy}", "received_events_url": "https://api.github.com/users/Aspire1Inspire2/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-26T03:19:52Z", "updated_at": "2018-07-26T03:19:52Z", "author_association": "NONE", "body_html": "<p>Still have the same bug for multi-GPU with Pytorch 0.4.0<br>\nremoving flatten_parameters() gives another warning of downgrading performance.</p>\n<p>Another problem with DataParallel is that the hidden state of RNN is batched on the second dimension.<br>\nThe (dim=1) argument of DataParallel works only on the input/output slicing, disregarding that the RNN hidden state has this batching on the second dimension.<br>\nThe (batch_first=True) argument of RNN also works only on the input/output.<br>\nEither way, DataParallel cannot recognize the correct hidden state tensor size. Work around for user is tedious. Hopefully someone could fix this.</p>", "body_text": "Still have the same bug for multi-GPU with Pytorch 0.4.0\nremoving flatten_parameters() gives another warning of downgrading performance.\nAnother problem with DataParallel is that the hidden state of RNN is batched on the second dimension.\nThe (dim=1) argument of DataParallel works only on the input/output slicing, disregarding that the RNN hidden state has this batching on the second dimension.\nThe (batch_first=True) argument of RNN also works only on the input/output.\nEither way, DataParallel cannot recognize the correct hidden state tensor size. Work around for user is tedious. Hopefully someone could fix this.", "body": "Still have the same bug for multi-GPU with Pytorch 0.4.0\r\nremoving flatten_parameters() gives another warning of downgrading performance.\r\n\r\nAnother problem with DataParallel is that the hidden state of RNN is batched on the second dimension. \r\nThe (dim=1) argument of DataParallel works only on the input/output slicing, disregarding that the RNN hidden state has this batching on the second dimension.\r\nThe (batch_first=True) argument of RNN also works only on the input/output.\r\nEither way, DataParallel cannot recognize the correct hidden state tensor size. Work around for user is tedious. Hopefully someone could fix this.\r\n\r\n"}