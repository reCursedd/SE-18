{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8106", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8106/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8106/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8106/events", "html_url": "https://github.com/pytorch/pytorch/issues/8106", "id": 328924042, "node_id": "MDU6SXNzdWUzMjg5MjQwNDI=", "number": 8106, "title": "dynamically change tensor with requires_grad=False by \"+=\" cause error but \"+\" doesn't", "user": {"login": "wizardk", "id": 20595052, "node_id": "MDQ6VXNlcjIwNTk1MDUy", "avatar_url": "https://avatars3.githubusercontent.com/u/20595052?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wizardk", "html_url": "https://github.com/wizardk", "followers_url": "https://api.github.com/users/wizardk/followers", "following_url": "https://api.github.com/users/wizardk/following{/other_user}", "gists_url": "https://api.github.com/users/wizardk/gists{/gist_id}", "starred_url": "https://api.github.com/users/wizardk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wizardk/subscriptions", "organizations_url": "https://api.github.com/users/wizardk/orgs", "repos_url": "https://api.github.com/users/wizardk/repos", "events_url": "https://api.github.com/users/wizardk/events{/privacy}", "received_events_url": "https://api.github.com/users/wizardk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-04T06:29:46Z", "updated_at": "2018-06-04T09:21:44Z", "closed_at": "2018-06-04T09:21:44Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I'm writing an attention model with history coverage vector.<br>\nI got a RuntimeError when I accumulated this history vector with a new vector by using \"+=\", but \"+\" is OK.</p>\n<p>Error info:<br>\nFile \"...\\torch\\autograd_<em>init</em>_.py\", line 89, in backward<br>\nallow_unreachable=True)  # allow_unreachable flag<br>\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</p>\n<h2>Code example</h2>\n<pre><code>class Attenion(nn.Module):\n  def __init__(self):\n    self.history = torch.zeros(..., requires_grad=False)\n  def forward(self, hidden, x):\n    w = ...\n    self.history += w.view_as(self.history).detach()  # RuntimeError\n    self.history = self.history + w.view_as(self.history).detach()  # OK\n</code></pre>\n<h2>System Info</h2>\n<ul>\n<li>OS: Windows 10</li>\n<li>PyTorch version: 0.4.0</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: 9.1.85, no cuDNN</li>\n</ul>", "body_text": "Issue description\nI'm writing an attention model with history coverage vector.\nI got a RuntimeError when I accumulated this history vector with a new vector by using \"+=\", but \"+\" is OK.\nError info:\nFile \"...\\torch\\autograd_init_.py\", line 89, in backward\nallow_unreachable=True)  # allow_unreachable flag\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\nCode example\nclass Attenion(nn.Module):\n  def __init__(self):\n    self.history = torch.zeros(..., requires_grad=False)\n  def forward(self, hidden, x):\n    w = ...\n    self.history += w.view_as(self.history).detach()  # RuntimeError\n    self.history = self.history + w.view_as(self.history).detach()  # OK\n\nSystem Info\n\nOS: Windows 10\nPyTorch version: 0.4.0\nPython version: 3.6.4\nCUDA/cuDNN version: 9.1.85, no cuDNN", "body": "## Issue description\r\n\r\nI'm writing an attention model with history coverage vector.\r\nI got a RuntimeError when I accumulated this history vector with a new vector by using \"+=\", but \"+\" is OK.\r\n\r\nError info:\r\nFile \"...\\torch\\autograd\\__init__.py\", line 89, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\r\n\r\n## Code example\r\n\r\n```\r\nclass Attenion(nn.Module):\r\n  def __init__(self):\r\n    self.history = torch.zeros(..., requires_grad=False)\r\n  def forward(self, hidden, x):\r\n    w = ...\r\n    self.history += w.view_as(self.history).detach()  # RuntimeError\r\n    self.history = self.history + w.view_as(self.history).detach()  # OK\r\n```\r\n\r\n## System Info\r\n- OS: Windows 10\r\n- PyTorch version: 0.4.0\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: 9.1.85, no cuDNN"}