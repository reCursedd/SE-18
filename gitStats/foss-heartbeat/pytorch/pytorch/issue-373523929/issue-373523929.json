{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13049", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13049/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13049/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13049/events", "html_url": "https://github.com/pytorch/pytorch/issues/13049", "id": 373523929, "node_id": "MDU6SXNzdWUzNzM1MjM5Mjk=", "number": 13049, "title": "[perf] Reduce tensor & aten overhead", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-24T14:50:59Z", "updated_at": "2018-11-15T19:38:41Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>Motivation</h3>\n<p>Launching a <a href=\"https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/test/expect/TestScript.test_lstm_fusion_cuda-forward.expect#L27\">FusionGroup for a JIT LSTM cell</a> takes ~40us CPU time, as reported by the autograd profiler. This is not good because the kernel itself takes &lt; 10us CUDA time and could probably be faster. After seeing nothing noticeably wrong with the JIT, I am looking into core performance overheads.</p>\n<h3>Methodology</h3>\n<p>Performance is measured with gbenchmark microbenchmarks: <a href=\"https://github.com/pytorch/benchmark/tree/master/timing/cpp2\">https://github.com/pytorch/benchmark/tree/master/timing/cpp2</a>. Here is some <a href=\"https://gist.github.com/zou3519/0ce5543357d998b83e11e58533a0eeda\">sample output</a> that motivates many of the below tasks.</p>\n<h3>Investigations</h3>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"371619041\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12824\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12824/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12824\">#12824</a>] Speed up torch.empty({0}). It is implemented by making a tensor and then calling <code>resize_({0})</code>; that <code>resize_</code> should be a no-op but it takes 300ns.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"371732415\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12841\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12841/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12841\">#12841</a>][<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"377578490\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13590\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13590/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13590\">#13590</a>] <code>get_device</code> is slow because it does two dispatches. It's used for the <code>DeviceGuard(tensor)</code> ctor.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"371732415\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12841\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12841/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12841\">#12841</a>] <code>is_cuda</code> is slow. It's used pretty commonly, most notably in DeviceGuard.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"374549939\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13185\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13185/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13185\">#13185</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] rewrite <code>torch.as_strided</code>.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"375216192\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13267\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13267/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13267\">#13267</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] speed up <code>tensor.storage_offset()</code>. <code>as_strided</code> is 100ns slower with/without a storage_offset argument (the lack of presense of one calls <code>tensor.storage_offset()</code>).</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"376124238\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13411\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13411/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13411\">#13411</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] stop allocating and throwing away StorageImpl during torch.as_strided</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [probably done <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] investigate <code>torch.as_strided</code> perf (it shouldn't be that much slower than just creating an empty tensor because we're setting fields on the new tensor).</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"375679095\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13330\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13330/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13330\">#13330</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> ] tensor.options() takes 50ns.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"377520693\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13580\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13580/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13580\">#13580</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] I <em>think</em> Variable::Impl's ctor constructs an empty StorageImpl, which costs like 200ns...</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"378082087\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13649\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13649/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13649\">#13649</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] Use at::SmallVector for TensorImpl's sizes and strides. This saves some time while allocating a tensor by avoiding dynamic allocations</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379291615\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13785\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13785/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13785\">#13785</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>] Speed up at::empty by avoiding DeviceGuard. at::empty_cuda now only does 1 DeviceGuard.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379291615\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13785\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13785/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13785\">#13785</a>] <del>Speed up torch.resize_(...), which is important in creating tensors of non-empty size.</del> Killed resizing logic in torch.empty. It no longer performs a resize; this results in a small speedup.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [done <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"380795301\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13974\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13974/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13974\">#13974</a>] Use fewer DeviceGuards in common ops <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"375223527\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13269\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13269/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13269\">#13269</a> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"378894353\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13741\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13741/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13741\">#13741</a> <a href=\"https://gist.github.com/zou3519/541c4bd3d965f25d65e90be255b1be4f\">motivation timings</a></p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> [<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"380795301\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13974\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13974/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13974\">#13974</a> DeviceGuard is no longer a problem!] DeviceGuard takes <del>300ns (before <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"371732415\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12841\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12841/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12841\">#12841</a>)</del> <del>100ns</del> 200ns. It should ideally only perform a cudaGetDevice and be 34ns. <del>Also, it would be great if we could cache a thread-local device but that may lead to consistency issues.</del> (bad idea). A typical JIT LSTM does 11100 DeviceGuard at 100ns, which is 1.1ms overhead. cudnn does forward + bwd in a total of 17ms.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [open] Library overhead: it takes 4us from an at::mm call until a gemm kernel begins launching. It shouldn't need to take that long; we need to allocate the output tensor but that should be &lt; 2us.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [in progress <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> ] at::detail::infer_type takes 500-700ns typically (35ns when run in a tight loop). This is pretty bad. Should run experiment to cache type (in a hacky way) on TensorImpl and see if that improves the timing or not</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [open] eliminate <code>tensor.type().scalarType()</code> idiom in favor of tensor.scalarType or tensor.dtype(). Not sure if this is actually perf critical</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [open] Investigate THCCachingAllocator performance.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [open] investigate caching device on TensorImpl <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"372560345\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12934\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12934/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12934\">#12934</a></p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [open] Look into overhead of creating a Variable. We currently create a StorageImpl, then use <code>make_tensor</code>, and finally use <code>as_variable</code>; <code>as_variable</code> seems pretty expensive. LSTM forward + backward creates 1100 tensors at 400ns variable overhead each, which is 0.4ms overhead. cudnn does forward + bwd in a total of 17ms.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> [in progress <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"378028003\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13638\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13638/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13638\">#13638</a>] Tensor-Variable merge will get rid of our Variable overhead, leading to significant wins</p>\n</li>\n</ul>", "body_text": "Motivation\nLaunching a FusionGroup for a JIT LSTM cell takes ~40us CPU time, as reported by the autograd profiler. This is not good because the kernel itself takes < 10us CUDA time and could probably be faster. After seeing nothing noticeably wrong with the JIT, I am looking into core performance overheads.\nMethodology\nPerformance is measured with gbenchmark microbenchmarks: https://github.com/pytorch/benchmark/tree/master/timing/cpp2. Here is some sample output that motivates many of the below tasks.\nInvestigations\n\n\n [#12824] Speed up torch.empty({0}). It is implemented by making a tensor and then calling resize_({0}); that resize_ should be a no-op but it takes 300ns.\n\n\n [#12841][#13590] get_device is slow because it does two dispatches. It's used for the DeviceGuard(tensor) ctor.\n\n\n [#12841] is_cuda is slow. It's used pretty commonly, most notably in DeviceGuard.\n\n\n [#13185 @zou3519] rewrite torch.as_strided.\n\n\n [#13267 @zou3519] speed up tensor.storage_offset(). as_strided is 100ns slower with/without a storage_offset argument (the lack of presense of one calls tensor.storage_offset()).\n\n\n [#13411 @zou3519] stop allocating and throwing away StorageImpl during torch.as_strided\n\n\n [probably done @zou3519] investigate torch.as_strided perf (it shouldn't be that much slower than just creating an empty tensor because we're setting fields on the new tensor).\n\n\n [#13330 @zou3519 ] tensor.options() takes 50ns.\n\n\n [#13580 @zou3519] I think Variable::Impl's ctor constructs an empty StorageImpl, which costs like 200ns...\n\n\n [#13649 @zou3519] Use at::SmallVector for TensorImpl's sizes and strides. This saves some time while allocating a tensor by avoiding dynamic allocations\n\n\n [#13785 @zou3519] Speed up at::empty by avoiding DeviceGuard. at::empty_cuda now only does 1 DeviceGuard.\n\n\n [#13785] Speed up torch.resize_(...), which is important in creating tensors of non-empty size. Killed resizing logic in torch.empty. It no longer performs a resize; this results in a small speedup.\n\n\n [done #13974] Use fewer DeviceGuards in common ops #13269 #13741 motivation timings\n\n\n [#13974 DeviceGuard is no longer a problem!] DeviceGuard takes 300ns (before #12841) 100ns 200ns. It should ideally only perform a cudaGetDevice and be 34ns. Also, it would be great if we could cache a thread-local device but that may lead to consistency issues. (bad idea). A typical JIT LSTM does 11100 DeviceGuard at 100ns, which is 1.1ms overhead. cudnn does forward + bwd in a total of 17ms.\n\n\n [open] Library overhead: it takes 4us from an at::mm call until a gemm kernel begins launching. It shouldn't need to take that long; we need to allocate the output tensor but that should be < 2us.\n\n\n [in progress @zou3519 ] at::detail::infer_type takes 500-700ns typically (35ns when run in a tight loop). This is pretty bad. Should run experiment to cache type (in a hacky way) on TensorImpl and see if that improves the timing or not\n\n\n [open] eliminate tensor.type().scalarType() idiom in favor of tensor.scalarType or tensor.dtype(). Not sure if this is actually perf critical\n\n\n [open] Investigate THCCachingAllocator performance.\n\n\n [open] investigate caching device on TensorImpl #12934\n\n\n [open] Look into overhead of creating a Variable. We currently create a StorageImpl, then use make_tensor, and finally use as_variable; as_variable seems pretty expensive. LSTM forward + backward creates 1100 tensors at 400ns variable overhead each, which is 0.4ms overhead. cudnn does forward + bwd in a total of 17ms.\n\n\n [in progress @yf225 #13638] Tensor-Variable merge will get rid of our Variable overhead, leading to significant wins", "body": "### Motivation\r\nLaunching a [FusionGroup for a JIT LSTM cell](https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/test/expect/TestScript.test_lstm_fusion_cuda-forward.expect#L27) takes ~40us CPU time, as reported by the autograd profiler. This is not good because the kernel itself takes < 10us CUDA time and could probably be faster. After seeing nothing noticeably wrong with the JIT, I am looking into core performance overheads.\r\n\r\n### Methodology\r\nPerformance is measured with gbenchmark microbenchmarks: https://github.com/pytorch/benchmark/tree/master/timing/cpp2. Here is some [sample output](https://gist.github.com/zou3519/0ce5543357d998b83e11e58533a0eeda) that motivates many of the below tasks.\r\n\r\n### Investigations\r\n\r\n- [x] [#12824] Speed up torch.empty({0}). It is implemented by making a tensor and then calling `resize_({0})`; that `resize_` should be a no-op but it takes 300ns.\r\n- [x] [#12841][#13590] `get_device` is slow because it does two dispatches. It's used for the `DeviceGuard(tensor)` ctor.\r\n- [x] [#12841] `is_cuda` is slow. It's used pretty commonly, most notably in DeviceGuard.\r\n- [x] [#13185 @zou3519] rewrite `torch.as_strided`.\r\n- [x] [#13267 @zou3519] speed up `tensor.storage_offset()`. `as_strided` is 100ns slower with/without a storage_offset argument (the lack of presense of one calls `tensor.storage_offset()`).\r\n- [x] [#13411 @zou3519] stop allocating and throwing away StorageImpl during torch.as_strided\r\n- [x] [probably done @zou3519] investigate `torch.as_strided` perf (it shouldn't be that much slower than just creating an empty tensor because we're setting fields on the new tensor).\r\n- [x] [#13330 @zou3519 ] tensor.options() takes 50ns. \r\n- [x] [#13580 @zou3519] I *think* Variable::Impl's ctor constructs an empty StorageImpl, which costs like 200ns...\r\n- [x] [#13649 @zou3519] Use at::SmallVector for TensorImpl's sizes and strides. This saves some time while allocating a tensor by avoiding dynamic allocations\r\n- [x] [#13785 @zou3519] Speed up at::empty by avoiding DeviceGuard. at::empty_cuda now only does 1 DeviceGuard. \r\n- [x] [#13785] ~~Speed up torch.resize_(...), which is important in creating tensors of non-empty size.~~ Killed resizing logic in torch.empty. It no longer performs a resize; this results in a small speedup.\r\n- [x] [done #13974] Use fewer DeviceGuards in common ops #13269 #13741 [motivation timings](https://gist.github.com/zou3519/541c4bd3d965f25d65e90be255b1be4f)\r\n- [x] [#13974 DeviceGuard is no longer a problem!] DeviceGuard takes ~~300ns (before #12841)~~ ~~100ns~~ 200ns. It should ideally only perform a cudaGetDevice and be 34ns. ~~Also, it would be great if we could cache a thread-local device but that may lead to consistency issues.~~ (bad idea). A typical JIT LSTM does 11100 DeviceGuard at 100ns, which is 1.1ms overhead. cudnn does forward + bwd in a total of 17ms.\r\n- [ ] [open] Library overhead: it takes 4us from an at::mm call until a gemm kernel begins launching. It shouldn't need to take that long; we need to allocate the output tensor but that should be < 2us.\r\n- [ ] [in progress @zou3519 ] at::detail::infer_type takes 500-700ns typically (35ns when run in a tight loop). This is pretty bad. Should run experiment to cache type (in a hacky way) on TensorImpl and see if that improves the timing or not\r\n- [ ] [open] eliminate `tensor.type().scalarType()` idiom in favor of tensor.scalarType or tensor.dtype(). Not sure if this is actually perf critical\r\n- [ ] [open] Investigate THCCachingAllocator performance. \r\n- [ ] [open] investigate caching device on TensorImpl https://github.com/pytorch/pytorch/issues/12934\r\n\r\n- [ ] [open] Look into overhead of creating a Variable. We currently create a StorageImpl, then use `make_tensor`, and finally use `as_variable`; `as_variable` seems pretty expensive. LSTM forward + backward creates 1100 tensors at 400ns variable overhead each, which is 0.4ms overhead. cudnn does forward + bwd in a total of 17ms.\r\n- [ ] [in progress @yf225 #13638] Tensor-Variable merge will get rid of our Variable overhead, leading to significant wins\r\n\r\n"}