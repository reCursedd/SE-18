{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3029", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3029/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3029/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3029/events", "html_url": "https://github.com/pytorch/pytorch/issues/3029", "id": 263799941, "node_id": "MDU6SXNzdWUyNjM3OTk5NDE=", "number": 3029, "title": "gradients not updating when I use norm?", "user": {"login": "demiguo", "id": 7954176, "node_id": "MDQ6VXNlcjc5NTQxNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/7954176?v=4", "gravatar_id": "", "url": "https://api.github.com/users/demiguo", "html_url": "https://github.com/demiguo", "followers_url": "https://api.github.com/users/demiguo/followers", "following_url": "https://api.github.com/users/demiguo/following{/other_user}", "gists_url": "https://api.github.com/users/demiguo/gists{/gist_id}", "starred_url": "https://api.github.com/users/demiguo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/demiguo/subscriptions", "organizations_url": "https://api.github.com/users/demiguo/orgs", "repos_url": "https://api.github.com/users/demiguo/repos", "events_url": "https://api.github.com/users/demiguo/events{/privacy}", "received_events_url": "https://api.github.com/users/demiguo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-09T07:02:07Z", "updated_at": "2018-11-05T10:03:54Z", "closed_at": "2018-11-05T10:03:54Z", "author_association": "NONE", "body_html": "<p>When I include this in loss function, I got 50% accuracy and sometimes my model converge. (I'm guessing the gradients are not calculated properly).</p>\n<p>loss2 = (self.l * torch.norm(self.linear_layer.weight, p=1))</p>\n<p>However, If I change it to L1Loss - it works! I got a normal accuracy (89%):<br>\nloss2 = torch.nn.L1Loss()(self.linear_layer.weight, Variable(torch.zeros((self.vocab_size, 1))))</p>\n<p>Why is that?<br>\nThanks!</p>", "body_text": "When I include this in loss function, I got 50% accuracy and sometimes my model converge. (I'm guessing the gradients are not calculated properly).\nloss2 = (self.l * torch.norm(self.linear_layer.weight, p=1))\nHowever, If I change it to L1Loss - it works! I got a normal accuracy (89%):\nloss2 = torch.nn.L1Loss()(self.linear_layer.weight, Variable(torch.zeros((self.vocab_size, 1))))\nWhy is that?\nThanks!", "body": "When I include this in loss function, I got 50% accuracy and sometimes my model converge. (I'm guessing the gradients are not calculated properly).\r\n\r\nloss2 = (self.l * torch.norm(self.linear_layer.weight, p=1))\r\n\r\nHowever, If I change it to L1Loss - it works! I got a normal accuracy (89%):\r\nloss2 = torch.nn.L1Loss()(self.linear_layer.weight, Variable(torch.zeros((self.vocab_size, 1))))\r\n\r\n\r\nWhy is that?\r\nThanks!\r\n\r\n"}