{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109314784", "pull_request_review_id": 30409557, "id": 109314784, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTMxNDc4NA==", "diff_hunk": "@@ -55,9 +55,10 @@ def __init__(self, module, device_ids=None, output_device=None, dim=0):\n \n     def forward(self, *inputs, **kwargs):\n         inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n-        if len(self.device_ids) == 1:\n+        n_gpus = len(self.device_ids)\n+        if n_gpus == 1:\n             return self.module(*inputs[0], **kwargs[0])\n-        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n+        replicas = self.replicate(self.module, self.device_ids[:n_gpus])", "path": "torch/nn/parallel/data_parallel.py", "position": 9, "original_position": 9, "commit_id": "2150be2acf3f2720956eef3f427cf585fa303b09", "original_commit_id": "2150be2acf3f2720956eef3f427cf585fa303b09", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I don't think it's correct. The `self.device_ids[:len(self.device_ids)]` is always equal to just `self.device_ids`. We've been using `len(inputs)` because if you e.g. gave a batch of size 3 to DataParallel using 4 GPUs, then `scatter` will return fewer results, and this slicing is necessary.", "created_at": "2017-04-02T17:38:36Z", "updated_at": "2018-11-23T15:32:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/1169#discussion_r109314784", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1169", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109314784"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1169#discussion_r109314784"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1169"}}, "body_html": "<p>I don't think it's correct. The <code>self.device_ids[:len(self.device_ids)]</code> is always equal to just <code>self.device_ids</code>. We've been using <code>len(inputs)</code> because if you e.g. gave a batch of size 3 to DataParallel using 4 GPUs, then <code>scatter</code> will return fewer results, and this slicing is necessary.</p>", "body_text": "I don't think it's correct. The self.device_ids[:len(self.device_ids)] is always equal to just self.device_ids. We've been using len(inputs) because if you e.g. gave a batch of size 3 to DataParallel using 4 GPUs, then scatter will return fewer results, and this slicing is necessary."}