{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/352278659", "html_url": "https://github.com/pytorch/pytorch/pull/4210#issuecomment-352278659", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4210", "id": 352278659, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjI3ODY1OQ==", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-17T19:21:57Z", "updated_at": "2017-12-17T19:22:28Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1762463\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/neerajprad\">@neerajprad</a> Does this seem to permissive? I think this won't be as much of an issue once we have scalar support.</p>\n</blockquote>\n<p>Interesting. This won't be an issue once we have scalar support. The current status is this:</p>\n<ul>\n<li>support scalars as arguments to univariate distributions, but preserve the return type as tensors.</li>\n<li><code>log_prob</code> should throw an error if the value to be scored could not have been generated by any possible invocation of <code>sample</code> with the same parameters.</li>\n<li>no broadcasting for the batch and event dimensions. Specially for the event dimension, we get into this weird issue where expanding the parameters might result in invalid parameterization.</li>\n</ul>\n<p>What we are basically doing here is relaxing the requirement for batch dims to line up (changing the second and third point above). e.g. even this should be possible now:</p>\n<pre><code>        p = Normal(Variable(torch.Tensor([[0.0], [0.0]])),\n                   Variable(torch.Tensor([[1.0], [1.0]])))\n        x = Variable(torch.arange(-2,2,0.01))\n        pdf = torch.exp(p.log_prob(x))  # returns score of size (2, 401)\n</code></pre>\n<p>For the most part, this should be fine, but will add some complexity in the longer term. Also doing this just for this one use case seems a bit overkill?</p>\n<p>Another option to consider would be to preserve the current batch semantics and instead require the user to use <code>p = Normal(0.0, 1.0)</code> to be able to score batches like this. We will need to upcast to Variable instead of Tensor internally, so that we are able to differentiate. This will require no changes to the client side code once we have scalar support, and preserve our batching semantics. Otherwise, the above scoring operation should throw an error once we have scalar support (as it does in Tensorflow). Relaxing the requirement for batch dims to line up is fine too if this is a common use case, but I would need to think a bit more about other side effects. What do you think?</p>", "body_text": "@neerajprad Does this seem to permissive? I think this won't be as much of an issue once we have scalar support.\n\nInteresting. This won't be an issue once we have scalar support. The current status is this:\n\nsupport scalars as arguments to univariate distributions, but preserve the return type as tensors.\nlog_prob should throw an error if the value to be scored could not have been generated by any possible invocation of sample with the same parameters.\nno broadcasting for the batch and event dimensions. Specially for the event dimension, we get into this weird issue where expanding the parameters might result in invalid parameterization.\n\nWhat we are basically doing here is relaxing the requirement for batch dims to line up (changing the second and third point above). e.g. even this should be possible now:\n        p = Normal(Variable(torch.Tensor([[0.0], [0.0]])),\n                   Variable(torch.Tensor([[1.0], [1.0]])))\n        x = Variable(torch.arange(-2,2,0.01))\n        pdf = torch.exp(p.log_prob(x))  # returns score of size (2, 401)\n\nFor the most part, this should be fine, but will add some complexity in the longer term. Also doing this just for this one use case seems a bit overkill?\nAnother option to consider would be to preserve the current batch semantics and instead require the user to use p = Normal(0.0, 1.0) to be able to score batches like this. We will need to upcast to Variable instead of Tensor internally, so that we are able to differentiate. This will require no changes to the client side code once we have scalar support, and preserve our batching semantics. Otherwise, the above scoring operation should throw an error once we have scalar support (as it does in Tensorflow). Relaxing the requirement for batch dims to line up is fine too if this is a common use case, but I would need to think a bit more about other side effects. What do you think?", "body": "> @neerajprad Does this seem to permissive? I think this won't be as much of an issue once we have scalar support.\r\n\r\nInteresting. This won't be an issue once we have scalar support. The current status is this:\r\n - support scalars as arguments to univariate distributions, but preserve the return type as tensors. \r\n - `log_prob` should throw an error if the value to be scored could not have been generated by any possible invocation of `sample` with the same parameters.\r\n - no broadcasting for the batch and event dimensions. Specially for the event dimension, we get into this weird issue where expanding the parameters might result in invalid parameterization. \r\n\r\nWhat we are basically doing here is relaxing the requirement for batch dims to line up (changing the second and third point above). e.g. even this should be possible now:\r\n```\r\n        p = Normal(Variable(torch.Tensor([[0.0], [0.0]])),\r\n                   Variable(torch.Tensor([[1.0], [1.0]])))\r\n        x = Variable(torch.arange(-2,2,0.01))\r\n        pdf = torch.exp(p.log_prob(x))  # returns score of size (2, 401)\r\n```\r\n\r\nFor the most part, this should be fine, but will add some complexity in the longer term. Also doing this just for this one use case seems a bit overkill? \r\n\r\nAnother option to consider would be to preserve the current batch semantics and instead require the user to use `p = Normal(0.0, 1.0)` to be able to score batches like this. We will need to upcast to Variable instead of Tensor internally, so that we are able to differentiate. This will require no changes to the client side code once we have scalar support, and preserve our batching semantics. Otherwise, the above scoring operation should throw an error once we have scalar support (as it does in Tensorflow). Relaxing the requirement for batch dims to line up is fine too if this is a common use case, but I would need to think a bit more about other side effects. What do you think?"}