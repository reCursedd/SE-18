{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/352283090", "html_url": "https://github.com/pytorch/pytorch/pull/4210#issuecomment-352283090", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4210", "id": 352283090, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjI4MzA5MA==", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-17T20:31:01Z", "updated_at": "2017-12-17T20:31:01Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>What if we allow broadcasting over batch_shape but require event_shape to match exactly? That would be enough to resolve this use case.</p>\n</blockquote>\n<p>That's what the change in this PR does to an extent (we can check for event dims explicitly, and simplify the logic). It will resolve this issue, but relaxes broadcasting requirements more generally to handle this one case:</p>\n<ul>\n<li>we are allowing the distribution instance to score samples that could not have been generated through <code>sample</code>, only to handle the scalar case. Otherwise, in practice, we shouldn't need this broadcasting.</li>\n<li>the output from <code>log_prob</code> will be broadcasted, and the result can be of a very different size than the value to be scored. The dimensions won't simply be <code>value.Size()[:-event_dim]</code>.</li>\n</ul>\n<p>To be fair, none of these seem like that big a deal to me at this point. I would prefer to stay close to TensorFlow semantics, in the absence of strong reasons not to do so. Not having scalar support is a valid reason to deviate from that, but can we handle this with minimal changes to the semantics? Can we instead require clients to use scalar parameters to score such shapes instead? This will probably introduce incompatibilities on the Pyro side, which we will have to handle. :( Or we could be more specific, and allow this expansion of the parameters only for <code>batch_shape = (1,) and event_shape=()</code>. When we have scalar support, we could remove this relaxation (the change won't be backwards compatible).</p>", "body_text": "What if we allow broadcasting over batch_shape but require event_shape to match exactly? That would be enough to resolve this use case.\n\nThat's what the change in this PR does to an extent (we can check for event dims explicitly, and simplify the logic). It will resolve this issue, but relaxes broadcasting requirements more generally to handle this one case:\n\nwe are allowing the distribution instance to score samples that could not have been generated through sample, only to handle the scalar case. Otherwise, in practice, we shouldn't need this broadcasting.\nthe output from log_prob will be broadcasted, and the result can be of a very different size than the value to be scored. The dimensions won't simply be value.Size()[:-event_dim].\n\nTo be fair, none of these seem like that big a deal to me at this point. I would prefer to stay close to TensorFlow semantics, in the absence of strong reasons not to do so. Not having scalar support is a valid reason to deviate from that, but can we handle this with minimal changes to the semantics? Can we instead require clients to use scalar parameters to score such shapes instead? This will probably introduce incompatibilities on the Pyro side, which we will have to handle. :( Or we could be more specific, and allow this expansion of the parameters only for batch_shape = (1,) and event_shape=(). When we have scalar support, we could remove this relaxation (the change won't be backwards compatible).", "body": "> What if we allow broadcasting over batch_shape but require event_shape to match exactly? That would be enough to resolve this use case.\r\n\r\nThat's what the change in this PR does to an extent (we can check for event dims explicitly, and simplify the logic). It will resolve this issue, but relaxes broadcasting requirements more generally to handle this one case:\r\n - we are allowing the distribution instance to score samples that could not have been generated through `sample`, only to handle the scalar case. Otherwise, in practice, we shouldn't need this broadcasting.\r\n - the output from `log_prob` will be broadcasted, and the result can be of a very different size than the value to be scored. The dimensions won't simply be `value.Size()[:-event_dim]`.\r\n\r\nTo be fair, none of these seem like that big a deal to me at this point. I would prefer to stay close to TensorFlow semantics, in the absence of strong reasons not to do so. Not having scalar support is a valid reason to deviate from that, but can we handle this with minimal changes to the semantics? Can we instead require clients to use scalar parameters to score such shapes instead? This will probably introduce incompatibilities on the Pyro side, which we will have to handle. :( Or we could be more specific, and allow this expansion of the parameters only for `batch_shape = (1,) and event_shape=()`. When we have scalar support, we could remove this relaxation (the change won't be backwards compatible). "}