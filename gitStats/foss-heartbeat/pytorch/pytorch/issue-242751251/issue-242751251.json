{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2080", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2080/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2080/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2080/events", "html_url": "https://github.com/pytorch/pytorch/issues/2080", "id": 242751251, "node_id": "MDU6SXNzdWUyNDI3NTEyNTE=", "number": 2080, "title": "cuda runtime error (9) invalid configuration argument on softmax when sending .view() variables", "user": {"login": "bishesh", "id": 5794198, "node_id": "MDQ6VXNlcjU3OTQxOTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5794198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bishesh", "html_url": "https://github.com/bishesh", "followers_url": "https://api.github.com/users/bishesh/followers", "following_url": "https://api.github.com/users/bishesh/following{/other_user}", "gists_url": "https://api.github.com/users/bishesh/gists{/gist_id}", "starred_url": "https://api.github.com/users/bishesh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bishesh/subscriptions", "organizations_url": "https://api.github.com/users/bishesh/orgs", "repos_url": "https://api.github.com/users/bishesh/repos", "events_url": "https://api.github.com/users/bishesh/events{/privacy}", "received_events_url": "https://api.github.com/users/bishesh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-07-13T16:01:10Z", "updated_at": "2017-11-03T21:30:30Z", "closed_at": "2017-11-03T21:30:29Z", "author_association": "NONE", "body_html": "<p>I need to use <code>softmax()</code> function on variables containing 5D tensors (batch_size X nChannels X W X D X H ).<br>\nSince <code>softmax()</code> doesn't support 5D, I was changing the variable view as below:<br>\n<code>var = var.view(batch_size, nChannels, -1)</code><br>\nHowever running <code>softmax(var)</code> throws runtime error when W X D X H is bigger than a certain size. E.g. it works for sizes less than or equal to 40 X 40 X 40 but not greater than this.<br>\nThe error I get is the following:<br>\nRuntimeError: cuda runtime error (9) : invalid configuration argument at /py/conda-bld/pytorch_1493681908901/work/torch/lib/THCUNN/generic/SoftMax.cu:72</p>\n<p>An example to reproduce the error:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\nt1 <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">40</span>,<span class=\"pl-c1\">40</span>,<span class=\"pl-c1\">40</span>)).cuda()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Uncommenting the line below breaks softmax</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>t1 = Variable(torch.FloatTensor(2,1,41,41,41)).cuda()</span>\nt1 <span class=\"pl-k\">=</span> t1.contiguous().view(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\nt1<span class=\"pl-k\">=</span>F.softmax(t1)\n<span class=\"pl-c1\">print</span>(t1.data.size())\nt2 <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">41</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">41</span>,<span class=\"pl-c1\">41</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">41</span>)).cuda()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Uncommenting the line below results in error which means total size is not an issue!</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>t2 = t2.contiguous().view(2,1,-1)</span>\nt2<span class=\"pl-k\">=</span>F.softmax(t2)\n<span class=\"pl-c1\">print</span>(t2.data.size())</pre></div>", "body_text": "I need to use softmax() function on variables containing 5D tensors (batch_size X nChannels X W X D X H ).\nSince softmax() doesn't support 5D, I was changing the variable view as below:\nvar = var.view(batch_size, nChannels, -1)\nHowever running softmax(var) throws runtime error when W X D X H is bigger than a certain size. E.g. it works for sizes less than or equal to 40 X 40 X 40 but not greater than this.\nThe error I get is the following:\nRuntimeError: cuda runtime error (9) : invalid configuration argument at /py/conda-bld/pytorch_1493681908901/work/torch/lib/THCUNN/generic/SoftMax.cu:72\nAn example to reproduce the error:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nt1 = Variable(torch.FloatTensor(2,1,40,40,40)).cuda()\n#Uncommenting the line below breaks softmax\n#t1 = Variable(torch.FloatTensor(2,1,41,41,41)).cuda()\nt1 = t1.contiguous().view(2,1,-1)\nt1=F.softmax(t1)\nprint(t1.data.size())\nt2 = Variable(torch.FloatTensor(2,1,41*41,41*41)).cuda()\n# Uncommenting the line below results in error which means total size is not an issue!\n#t2 = t2.contiguous().view(2,1,-1)\nt2=F.softmax(t2)\nprint(t2.data.size())", "body": "I need to use `softmax()` function on variables containing 5D tensors (batch_size X nChannels X W X D X H ). \r\nSince `softmax()` doesn't support 5D, I was changing the variable view as below:\r\n`var = var.view(batch_size, nChannels, -1)`\r\nHowever running `softmax(var)` throws runtime error when W X D X H is bigger than a certain size. E.g. it works for sizes less than or equal to 40 X 40 X 40 but not greater than this.\r\n The error I get is the following:\r\nRuntimeError: cuda runtime error (9) : invalid configuration argument at /py/conda-bld/pytorch_1493681908901/work/torch/lib/THCUNN/generic/SoftMax.cu:72\r\n\r\nAn example to reproduce the error:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as F\r\nt1 = Variable(torch.FloatTensor(2,1,40,40,40)).cuda()\r\n#Uncommenting the line below breaks softmax\r\n#t1 = Variable(torch.FloatTensor(2,1,41,41,41)).cuda()\r\nt1 = t1.contiguous().view(2,1,-1)\r\nt1=F.softmax(t1)\r\nprint(t1.data.size())\r\nt2 = Variable(torch.FloatTensor(2,1,41*41,41*41)).cuda()\r\n# Uncommenting the line below results in error which means total size is not an issue!\r\n#t2 = t2.contiguous().view(2,1,-1)\r\nt2=F.softmax(t2)\r\nprint(t2.data.size())\r\n```"}