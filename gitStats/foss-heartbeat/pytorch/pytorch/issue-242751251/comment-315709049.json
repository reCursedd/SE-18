{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315709049", "html_url": "https://github.com/pytorch/pytorch/issues/2080#issuecomment-315709049", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2080", "id": 315709049, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTcwOTA0OQ==", "user": {"login": "bishesh", "id": 5794198, "node_id": "MDQ6VXNlcjU3OTQxOTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5794198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bishesh", "html_url": "https://github.com/bishesh", "followers_url": "https://api.github.com/users/bishesh/followers", "following_url": "https://api.github.com/users/bishesh/following{/other_user}", "gists_url": "https://api.github.com/users/bishesh/gists{/gist_id}", "starred_url": "https://api.github.com/users/bishesh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bishesh/subscriptions", "organizations_url": "https://api.github.com/users/bishesh/orgs", "repos_url": "https://api.github.com/users/bishesh/repos", "events_url": "https://api.github.com/users/bishesh/events{/privacy}", "received_events_url": "https://api.github.com/users/bishesh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-17T09:37:53Z", "updated_at": "2017-07-17T09:38:08Z", "author_association": "NONE", "body_html": "<p>That will work ONLY for up to a certain size. In general I've found that viewing the tensor by reducing the dimensions from the last ones creates this issue.<br>\nSo a workaround would be to permute the tensor before doing the viewing so that batch size or nChannels are the last dimensions assuming they won't be as big as the other image dimensions. But then we also need to add some logic to compute softmax in a right way because of the inconsistent (annoying!) way the softmax works for <a href=\"https://github.com/pytorch/pytorch/issues/1020\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1020/hovercard\">different dimensions</a>.<br>\nHere is an example showing these problems:</p>\n<div class=\"highlight highlight-source-python\"><pre>cs, bs, nc, w, h, d <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>\ncs, bs, nc, w, h, d <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">39</span>, <span class=\"pl-c1\">41</span>, <span class=\"pl-c1\">41</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>cs, bs, nc, w, h, d = 3, 2, 1, 400, 450, 450 </span>\nt1 <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(bs,nc,w,h,d)).cuda()\n<span class=\"pl-k\">if</span> cs <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Breaks from bs, nc, w, h, d = 1, 1, 39, 41, 41</span>\n    t1 <span class=\"pl-k\">=</span> t1.contiguous().view(bs,nc,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">if</span> cs <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Breaks from bs, nc, w, h, d = 1, 1, 1, 256, 256</span>\n    t1 <span class=\"pl-k\">=</span> t1.contiguous().view(bs,nc,w,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">if</span> cs <span class=\"pl-k\">==</span> <span class=\"pl-c1\">3</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Seems to works until out of memory</span>\n    t1 <span class=\"pl-k\">=</span> t1.permute(<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> d X h X w X nc X bs</span>\n    t1 <span class=\"pl-k\">=</span> t1.contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, nc, bs)\nt1<span class=\"pl-k\">=</span>F.softmax(t1)\n<span class=\"pl-c1\">print</span>(t1.data.size())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The pattern is similar with 4D tensors also where we do view by</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> reducing the last few dimensions.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>t2 = Variable(torch.FloatTensor(2,1,41*41,41*41)).cuda()</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Uncommenting the line below results in error which means total size is not an issue!</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>t2 = t2.contiguous().view(2,1,-1)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>t2=F.softmax(t2)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>print(t2.data.size())</span></pre></div>", "body_text": "That will work ONLY for up to a certain size. In general I've found that viewing the tensor by reducing the dimensions from the last ones creates this issue.\nSo a workaround would be to permute the tensor before doing the viewing so that batch size or nChannels are the last dimensions assuming they won't be as big as the other image dimensions. But then we also need to add some logic to compute softmax in a right way because of the inconsistent (annoying!) way the softmax works for different dimensions.\nHere is an example showing these problems:\ncs, bs, nc, w, h, d = 1, 1, 1, 1, 256, 256\ncs, bs, nc, w, h, d = 2, 1, 1, 39, 41, 41\n#cs, bs, nc, w, h, d = 3, 2, 1, 400, 450, 450 \nt1 = Variable(torch.FloatTensor(bs,nc,w,h,d)).cuda()\nif cs == 1:\n    #Breaks from bs, nc, w, h, d = 1, 1, 39, 41, 41\n    t1 = t1.contiguous().view(bs,nc,-1)\nif cs == 2:\n    # Breaks from bs, nc, w, h, d = 1, 1, 1, 256, 256\n    t1 = t1.contiguous().view(bs,nc,w,-1)\nif cs == 3:\n    # Seems to works until out of memory\n    t1 = t1.permute(4,3,2,1,0) # d X h X w X nc X bs\n    t1 = t1.contiguous().view(-1, nc, bs)\nt1=F.softmax(t1)\nprint(t1.data.size())\n# The pattern is similar with 4D tensors also where we do view by\n# reducing the last few dimensions.\n#t2 = Variable(torch.FloatTensor(2,1,41*41,41*41)).cuda()\n# Uncommenting the line below results in error which means total size is not an issue!\n#t2 = t2.contiguous().view(2,1,-1)\n#t2=F.softmax(t2)\n#print(t2.data.size())", "body": "That will work ONLY for up to a certain size. In general I've found that viewing the tensor by reducing the dimensions from the last ones creates this issue.\r\nSo a workaround would be to permute the tensor before doing the viewing so that batch size or nChannels are the last dimensions assuming they won't be as big as the other image dimensions. But then we also need to add some logic to compute softmax in a right way because of the inconsistent (annoying!) way the softmax works for [different dimensions](https://github.com/pytorch/pytorch/issues/1020).  \r\nHere is an example showing these problems: \r\n```python\r\ncs, bs, nc, w, h, d = 1, 1, 1, 1, 256, 256\r\ncs, bs, nc, w, h, d = 2, 1, 1, 39, 41, 41\r\n#cs, bs, nc, w, h, d = 3, 2, 1, 400, 450, 450 \r\nt1 = Variable(torch.FloatTensor(bs,nc,w,h,d)).cuda()\r\nif cs == 1:\r\n    #Breaks from bs, nc, w, h, d = 1, 1, 39, 41, 41\r\n    t1 = t1.contiguous().view(bs,nc,-1)\r\nif cs == 2:\r\n    # Breaks from bs, nc, w, h, d = 1, 1, 1, 256, 256\r\n    t1 = t1.contiguous().view(bs,nc,w,-1)\r\nif cs == 3:\r\n    # Seems to works until out of memory\r\n    t1 = t1.permute(4,3,2,1,0) # d X h X w X nc X bs\r\n    t1 = t1.contiguous().view(-1, nc, bs)\r\nt1=F.softmax(t1)\r\nprint(t1.data.size())\r\n# The pattern is similar with 4D tensors also where we do view by\r\n# reducing the last few dimensions.\r\n#t2 = Variable(torch.FloatTensor(2,1,41*41,41*41)).cuda()\r\n# Uncommenting the line below results in error which means total size is not an issue!\r\n#t2 = t2.contiguous().view(2,1,-1)\r\n#t2=F.softmax(t2)\r\n#print(t2.data.size())\r\n``` "}