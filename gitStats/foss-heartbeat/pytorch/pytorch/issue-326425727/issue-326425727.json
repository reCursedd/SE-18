{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7841", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7841/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7841/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7841/events", "html_url": "https://github.com/pytorch/pytorch/issues/7841", "id": 326425727, "node_id": "MDU6SXNzdWUzMjY0MjU3Mjc=", "number": 7841, "title": "Compiling Pytorch 0.4 from source for Tegra (arm processor) fails", "user": {"login": "alexgkendall", "id": 8551989, "node_id": "MDQ6VXNlcjg1NTE5ODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/8551989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexgkendall", "html_url": "https://github.com/alexgkendall", "followers_url": "https://api.github.com/users/alexgkendall/followers", "following_url": "https://api.github.com/users/alexgkendall/following{/other_user}", "gists_url": "https://api.github.com/users/alexgkendall/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexgkendall/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexgkendall/subscriptions", "organizations_url": "https://api.github.com/users/alexgkendall/orgs", "repos_url": "https://api.github.com/users/alexgkendall/repos", "events_url": "https://api.github.com/users/alexgkendall/events{/privacy}", "received_events_url": "https://api.github.com/users/alexgkendall/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-25T08:21:17Z", "updated_at": "2018-05-29T17:52:00Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Following this script<br>\n<a href=\"https://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426\">https://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426</a></p>\n<p>but doing<br>\n<code>git checkout tags/v0.4.0</code></p>\n<p>Fails during compilation on an NVIDIA Tegra (arm processor). The same environment on the Tegra will build successfully using <code>git checkout tags/v0.3.1</code>. Can you please offer any suggestions?</p>\n<p>Using cmake version 3.5.1 and gcc 5.4.0</p>\n<p>The final output of <code>pytorch/aten/build/CMakeFiles/CMakeError.log</code> is:</p>\n<pre><code>pytorch$ sudo python3 setup.py install\nrunning install\nrunning build_deps\n+ WITH_CUDA=0\n+ [[ --with-cuda == \\-\\-\\w\\i\\t\\h\\-\\c\\u\\d\\a ]]\n+ WITH_CUDA=1\n+ shift\n+ WITH_NNPACK=0\n+ [[ --with-nnpack == \\-\\-\\w\\i\\t\\h\\-\\n\\n\\p\\a\\c\\k ]]\n+ WITH_NNPACK=1\n+ shift\n+ WITH_MKLDNN=0\n+ [[ nccl == \\-\\-\\w\\i\\t\\h\\-\\m\\k\\l\\d\\n\\n ]]\n+ WITH_GLOO_IBVERBS=0\n+ [[ nccl == \\-\\-\\w\\i\\t\\h\\-\\g\\l\\o\\o\\-\\i\\b\\v\\e\\r\\b\\s ]]\n+ CMAKE_INSTALL='make install'\n+ USER_CFLAGS=\n+ USER_LDFLAGS=\n+ [[ -n '' ]]\n+ [[ -n '' ]]\n+ [[ -n '' ]]\n++ dirname tools/build_pytorch_libs.sh\n+ cd tools/..\n+++ pwd\n++ printf '%q\\n' /home/nvidia/code/tmp/pytorch\n+ PWD=/home/nvidia/code/tmp/pytorch\n+ BASE_DIR=/home/nvidia/code/tmp/pytorch\n+ TORCH_LIB_DIR=/home/nvidia/code/tmp/pytorch/torch/lib\n+ INSTALL_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install\n+ THIRD_PARTY_DIR=/home/nvidia/code/tmp/pytorch/third_party\n+ CMAKE_VERSION=cmake\n+ C_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\"'\n+ C_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1'\n+ LDFLAGS='-L\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\" '\n+ LD_POSTFIX=.so.1\n+ LD_POSTFIX_UNVERSIONED=.so\n++ uname\n+ [[ Linux == \\D\\a\\r\\w\\i\\n ]]\n+ LDFLAGS='-L\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\"  -Wl,-rpath,$ORIGIN'\n+ CPP_FLAGS=' -std=c++11 '\n+ GLOO_FLAGS=\n+ THD_FLAGS=\n+ NCCL_ROOT_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install\n+ [[ 1 -eq 1 ]]\n+ GLOO_FLAGS='-DUSE_CUDA=1 -DNCCL_ROOT_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install'\n+ [[ 0 -eq 1 ]]\n+ CWRAP_FILES='/home/nvidia/code/tmp/pytorch/torch/lib/ATen/Declarations.cwrap;/home/nvidia/code/tmp/pytorch/torch/lib/THNN/generic/THNN.h;/home/nvidia/code/tmp/pytorch/torch/lib/THCUNN/generic/THCUNN.h;/home/nvidia/code/tmp/pytorch/torch/lib/ATen/nn.yaml'\n+ CUDA_NVCC_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1'\n+ [[ '' -eq 1 ]]\n+ '[' -z 6 ']'\n+ BUILD_TYPE=Release\n+ [[ -n '' ]]\n+ [[ -n '' ]]\n+ echo 'Building in Release mode'\nBuilding in Release mode\n+ mkdir -p torch/lib/tmp_install\n+ for arg in '\"$@\"'\n+ [[ nccl == \\n\\c\\c\\l ]]\n+ pushd /home/nvidia/code/tmp/pytorch/third_party\n~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\n+ build_nccl\n+ mkdir -p build/nccl\n+ pushd build/nccl\n~/code/tmp/pytorch/third_party/build/nccl ~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\n+ cmake ../../nccl -DCMAKE_MODULE_PATH=/home/nvidia/code/tmp/pytorch/cmake/FindCUDA -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install '-DCMAKE_C_FLAGS= -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1 ' '-DCMAKE_CXX_FLAGS= -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1  -std=c++11  ' -DCMAKE_SHARED_LINKER_FLAGS=\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/nvidia/code/tmp/pytorch/third_party/build/nccl\n+ make install\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/reduce.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nCompiling src/reduce_scatter.cu               &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    &gt; /home/nvidia/code/tmp/pytorch/third_party/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/nccl.h\n+ mkdir -p /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\n+ cp lib/libnccl.so.1 /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so.1\n+ '[' '!' -f /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so ']'\n+ ln -s /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so.1 /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so\n+ popd\n~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\n+ popd\n~/code/tmp/pytorch\n+ for arg in '\"$@\"'\n+ [[ ATen == \\n\\c\\c\\l ]]\n+ [[ ATen == \\g\\l\\o\\o ]]\n+ [[ ATen == \\A\\T\\e\\n ]]\n+ pushd /home/nvidia/code/tmp/pytorch/aten\n~/code/tmp/pytorch/aten ~/code/tmp/pytorch\n+ build_aten\n+ mkdir -p build\n+ pushd build\n~/code/tmp/pytorch/aten/build ~/code/tmp/pytorch/aten ~/code/tmp/pytorch\n+ cmake .. -DCMAKE_BUILD_TYPE=Release -DNO_CUDA=0 -DNO_NNPACK=0 -DCUDNN_INCLUDE_DIR=/usr/include/ -DCUDNN_LIB_DIR=/usr/lib/aarch64-linux-gnu/ -DCUDNN_LIBRARY=/usr/lib/aarch64-linux-gnu/libcudnn.so.7 -DNO_MKLDNN=1 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DATEN_NO_CONTRIB=1 -DCMAKE_INSTALL_PREFIX=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS=\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \n-- Autodetected CUDA architecture(s): nvrm_gpu: Bug 200215060 workaround enabled.\n6.1 6.2 \nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name nvrm_gpu: in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name Bug in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name 200215060 in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name workaround in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name enabled.\n\n  6.1 in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\n-- Try OpenMP C flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Try OpenMP CXX flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Found OpenMP: -fopenmp  \n-- Compiling with OpenMP support\n-- MAGMA not found. Compiling without MAGMA support\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- asimd/Neon found with compiler flag : -D__NEON__\n-- Performing Test COMPILER_WORKS\n-- Performing Test COMPILER_WORKS - Success\n-- Looking for cpuid.h\n-- Looking for cpuid.h - not found\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Failed\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Failed\n-- Performing Test C_HAS_SSE1_2\n-- Performing Test C_HAS_SSE1_2 - Failed\n-- Performing Test C_HAS_SSE1_3\n-- Performing Test C_HAS_SSE1_3 - Failed\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Failed\n-- Performing Test C_HAS_SSE2_2\n-- Performing Test C_HAS_SSE2_2 - Failed\n-- Performing Test C_HAS_SSE2_3\n-- Performing Test C_HAS_SSE2_3 - Failed\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Failed\n-- Performing Test C_HAS_SSE3_3\n-- Performing Test C_HAS_SSE3_3 - Failed\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Failed\n-- Performing Test C_HAS_SSE4_1_3\n-- Performing Test C_HAS_SSE4_1_3 - Failed\n-- Performing Test C_HAS_SSE4_1_4\n-- Performing Test C_HAS_SSE4_1_4 - Failed\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Failed\n-- Performing Test C_HAS_SSE4_2_3\n-- Performing Test C_HAS_SSE4_2_3 - Failed\n-- Performing Test C_HAS_SSE4_2_4\n-- Performing Test C_HAS_SSE4_2_4 - Failed\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Failed\n-- Performing Test C_HAS_AVX_3\n-- Performing Test C_HAS_AVX_3 - Failed\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Failed\n-- Performing Test C_HAS_AVX2_3\n-- Performing Test C_HAS_AVX2_3 - Failed\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Failed\n-- Performing Test CXX_HAS_SSE1_2\n-- Performing Test CXX_HAS_SSE1_2 - Failed\n-- Performing Test CXX_HAS_SSE1_3\n-- Performing Test CXX_HAS_SSE1_3 - Failed\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Failed\n-- Performing Test CXX_HAS_SSE2_2\n-- Performing Test CXX_HAS_SSE2_2 - Failed\n-- Performing Test CXX_HAS_SSE2_3\n-- Performing Test CXX_HAS_SSE2_3 - Failed\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Failed\n-- Performing Test CXX_HAS_SSE3_3\n-- Performing Test CXX_HAS_SSE3_3 - Failed\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Failed\n-- Performing Test CXX_HAS_SSE4_1_3\n-- Performing Test CXX_HAS_SSE4_1_3 - Failed\n-- Performing Test CXX_HAS_SSE4_1_4\n-- Performing Test CXX_HAS_SSE4_1_4 - Failed\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Failed\n-- Performing Test CXX_HAS_SSE4_2_3\n-- Performing Test CXX_HAS_SSE4_2_3 - Failed\n-- Performing Test CXX_HAS_SSE4_2_4\n-- Performing Test CXX_HAS_SSE4_2_4 - Failed\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Failed\n-- Performing Test CXX_HAS_AVX_3\n-- Performing Test CXX_HAS_AVX_3 - Failed\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Failed\n-- Performing Test CXX_HAS_AVX2_3\n-- Performing Test CXX_HAS_AVX2_3 - Failed\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl - guide - pthread - m]\n--   Library mkl: not found\n-- MKL library not found\n-- Checking for [openblas]\n--   Library openblas: /usr/lib/libopenblas.so\n-- Looking for sgemm_\n-- Looking for sgemm_ - found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (open).\n-- Looking for cheev_\n-- Looking for cheev_ - found\n-- Found a library with LAPACK API (open).\n-- Found CUDNN: /usr/include  \n-- Found cuDNN: v7.0.4  (include: /usr/include, library: /usr/lib/aarch64-linux-gnu/libcudnn.so.7)\ndisabling MKLDNN because NO_MKLDNN is set\n-- Using python found in /usr/bin/python3\n-- Performing Test SUPPORTS_STDCXX11\n-- Performing Test SUPPORTS_STDCXX11 - Success\n-- Performing Test SUPPORTS_MRTM\n-- Performing Test SUPPORTS_MRTM - Failed\n-- Performing Test SUPPORTS_FLIFETIME\n-- Performing Test SUPPORTS_FLIFETIME - Failed\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\n-- Check if compiler accepts -pthread\n-- Check if compiler accepts -pthread - yes\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring incomplete, errors occurred!\nSee also \"/home/nvidia/code/tmp/pytorch/aten/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/nvidia/code/tmp/pytorch/aten/build/CMakeFiles/CMakeError.log\".\n</code></pre>", "body_text": "Following this script\nhttps://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426\nbut doing\ngit checkout tags/v0.4.0\nFails during compilation on an NVIDIA Tegra (arm processor). The same environment on the Tegra will build successfully using git checkout tags/v0.3.1. Can you please offer any suggestions?\nUsing cmake version 3.5.1 and gcc 5.4.0\nThe final output of pytorch/aten/build/CMakeFiles/CMakeError.log is:\npytorch$ sudo python3 setup.py install\nrunning install\nrunning build_deps\n+ WITH_CUDA=0\n+ [[ --with-cuda == \\-\\-\\w\\i\\t\\h\\-\\c\\u\\d\\a ]]\n+ WITH_CUDA=1\n+ shift\n+ WITH_NNPACK=0\n+ [[ --with-nnpack == \\-\\-\\w\\i\\t\\h\\-\\n\\n\\p\\a\\c\\k ]]\n+ WITH_NNPACK=1\n+ shift\n+ WITH_MKLDNN=0\n+ [[ nccl == \\-\\-\\w\\i\\t\\h\\-\\m\\k\\l\\d\\n\\n ]]\n+ WITH_GLOO_IBVERBS=0\n+ [[ nccl == \\-\\-\\w\\i\\t\\h\\-\\g\\l\\o\\o\\-\\i\\b\\v\\e\\r\\b\\s ]]\n+ CMAKE_INSTALL='make install'\n+ USER_CFLAGS=\n+ USER_LDFLAGS=\n+ [[ -n '' ]]\n+ [[ -n '' ]]\n+ [[ -n '' ]]\n++ dirname tools/build_pytorch_libs.sh\n+ cd tools/..\n+++ pwd\n++ printf '%q\\n' /home/nvidia/code/tmp/pytorch\n+ PWD=/home/nvidia/code/tmp/pytorch\n+ BASE_DIR=/home/nvidia/code/tmp/pytorch\n+ TORCH_LIB_DIR=/home/nvidia/code/tmp/pytorch/torch/lib\n+ INSTALL_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install\n+ THIRD_PARTY_DIR=/home/nvidia/code/tmp/pytorch/third_party\n+ CMAKE_VERSION=cmake\n+ C_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\"'\n+ C_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1'\n+ LDFLAGS='-L\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\" '\n+ LD_POSTFIX=.so.1\n+ LD_POSTFIX_UNVERSIONED=.so\n++ uname\n+ [[ Linux == \\D\\a\\r\\w\\i\\n ]]\n+ LDFLAGS='-L\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\"  -Wl,-rpath,$ORIGIN'\n+ CPP_FLAGS=' -std=c++11 '\n+ GLOO_FLAGS=\n+ THD_FLAGS=\n+ NCCL_ROOT_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install\n+ [[ 1 -eq 1 ]]\n+ GLOO_FLAGS='-DUSE_CUDA=1 -DNCCL_ROOT_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install'\n+ [[ 0 -eq 1 ]]\n+ CWRAP_FILES='/home/nvidia/code/tmp/pytorch/torch/lib/ATen/Declarations.cwrap;/home/nvidia/code/tmp/pytorch/torch/lib/THNN/generic/THNN.h;/home/nvidia/code/tmp/pytorch/torch/lib/THCUNN/generic/THCUNN.h;/home/nvidia/code/tmp/pytorch/torch/lib/ATen/nn.yaml'\n+ CUDA_NVCC_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1'\n+ [[ '' -eq 1 ]]\n+ '[' -z 6 ']'\n+ BUILD_TYPE=Release\n+ [[ -n '' ]]\n+ [[ -n '' ]]\n+ echo 'Building in Release mode'\nBuilding in Release mode\n+ mkdir -p torch/lib/tmp_install\n+ for arg in '\"$@\"'\n+ [[ nccl == \\n\\c\\c\\l ]]\n+ pushd /home/nvidia/code/tmp/pytorch/third_party\n~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\n+ build_nccl\n+ mkdir -p build/nccl\n+ pushd build/nccl\n~/code/tmp/pytorch/third_party/build/nccl ~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\n+ cmake ../../nccl -DCMAKE_MODULE_PATH=/home/nvidia/code/tmp/pytorch/cmake/FindCUDA -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install '-DCMAKE_C_FLAGS= -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1 ' '-DCMAKE_CXX_FLAGS= -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1  -std=c++11  ' -DCMAKE_SHARED_LINKER_FLAGS=\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/nvidia/code/tmp/pytorch/third_party/build/nccl\n+ make install\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/reduce.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nCompiling src/reduce_scatter.cu               > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/nccl.h\n+ mkdir -p /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\n+ cp lib/libnccl.so.1 /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so.1\n+ '[' '!' -f /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so ']'\n+ ln -s /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so.1 /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so\n+ popd\n~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\n+ popd\n~/code/tmp/pytorch\n+ for arg in '\"$@\"'\n+ [[ ATen == \\n\\c\\c\\l ]]\n+ [[ ATen == \\g\\l\\o\\o ]]\n+ [[ ATen == \\A\\T\\e\\n ]]\n+ pushd /home/nvidia/code/tmp/pytorch/aten\n~/code/tmp/pytorch/aten ~/code/tmp/pytorch\n+ build_aten\n+ mkdir -p build\n+ pushd build\n~/code/tmp/pytorch/aten/build ~/code/tmp/pytorch/aten ~/code/tmp/pytorch\n+ cmake .. -DCMAKE_BUILD_TYPE=Release -DNO_CUDA=0 -DNO_NNPACK=0 -DCUDNN_INCLUDE_DIR=/usr/include/ -DCUDNN_LIB_DIR=/usr/lib/aarch64-linux-gnu/ -DCUDNN_LIBRARY=/usr/lib/aarch64-linux-gnu/libcudnn.so.7 -DNO_MKLDNN=1 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DATEN_NO_CONTRIB=1 -DCMAKE_INSTALL_PREFIX=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS=\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \n-- Autodetected CUDA architecture(s): nvrm_gpu: Bug 200215060 workaround enabled.\n6.1 6.2 \nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name nvrm_gpu: in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name Bug in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name 200215060 in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name workaround in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\n  Unknown CUDA Architecture Name enabled.\n\n  6.1 in CUDA_SELECT_NVCC_ARCH_FLAGS\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\n  arch_bin wasn't set for some reason\nCall Stack (most recent call first):\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\n\n\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\n-- Try OpenMP C flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Try OpenMP CXX flag = [-fopenmp]\n-- Performing Test OpenMP_FLAG_DETECTED\n-- Performing Test OpenMP_FLAG_DETECTED - Success\n-- Found OpenMP: -fopenmp  \n-- Compiling with OpenMP support\n-- MAGMA not found. Compiling without MAGMA support\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- asimd/Neon found with compiler flag : -D__NEON__\n-- Performing Test COMPILER_WORKS\n-- Performing Test COMPILER_WORKS - Success\n-- Looking for cpuid.h\n-- Looking for cpuid.h - not found\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Failed\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Failed\n-- Performing Test C_HAS_SSE1_2\n-- Performing Test C_HAS_SSE1_2 - Failed\n-- Performing Test C_HAS_SSE1_3\n-- Performing Test C_HAS_SSE1_3 - Failed\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Failed\n-- Performing Test C_HAS_SSE2_2\n-- Performing Test C_HAS_SSE2_2 - Failed\n-- Performing Test C_HAS_SSE2_3\n-- Performing Test C_HAS_SSE2_3 - Failed\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Failed\n-- Performing Test C_HAS_SSE3_3\n-- Performing Test C_HAS_SSE3_3 - Failed\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Failed\n-- Performing Test C_HAS_SSE4_1_3\n-- Performing Test C_HAS_SSE4_1_3 - Failed\n-- Performing Test C_HAS_SSE4_1_4\n-- Performing Test C_HAS_SSE4_1_4 - Failed\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Failed\n-- Performing Test C_HAS_SSE4_2_3\n-- Performing Test C_HAS_SSE4_2_3 - Failed\n-- Performing Test C_HAS_SSE4_2_4\n-- Performing Test C_HAS_SSE4_2_4 - Failed\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Failed\n-- Performing Test C_HAS_AVX_3\n-- Performing Test C_HAS_AVX_3 - Failed\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Failed\n-- Performing Test C_HAS_AVX2_3\n-- Performing Test C_HAS_AVX2_3 - Failed\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Failed\n-- Performing Test CXX_HAS_SSE1_2\n-- Performing Test CXX_HAS_SSE1_2 - Failed\n-- Performing Test CXX_HAS_SSE1_3\n-- Performing Test CXX_HAS_SSE1_3 - Failed\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Failed\n-- Performing Test CXX_HAS_SSE2_2\n-- Performing Test CXX_HAS_SSE2_2 - Failed\n-- Performing Test CXX_HAS_SSE2_3\n-- Performing Test CXX_HAS_SSE2_3 - Failed\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Failed\n-- Performing Test CXX_HAS_SSE3_3\n-- Performing Test CXX_HAS_SSE3_3 - Failed\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Failed\n-- Performing Test CXX_HAS_SSE4_1_3\n-- Performing Test CXX_HAS_SSE4_1_3 - Failed\n-- Performing Test CXX_HAS_SSE4_1_4\n-- Performing Test CXX_HAS_SSE4_1_4 - Failed\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Failed\n-- Performing Test CXX_HAS_SSE4_2_3\n-- Performing Test CXX_HAS_SSE4_2_3 - Failed\n-- Performing Test CXX_HAS_SSE4_2_4\n-- Performing Test CXX_HAS_SSE4_2_4 - Failed\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Failed\n-- Performing Test CXX_HAS_AVX_3\n-- Performing Test CXX_HAS_AVX_3 - Failed\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Failed\n-- Performing Test CXX_HAS_AVX2_3\n-- Performing Test CXX_HAS_AVX2_3 - Failed\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl - guide - pthread - m]\n--   Library mkl: not found\n-- MKL library not found\n-- Checking for [openblas]\n--   Library openblas: /usr/lib/libopenblas.so\n-- Looking for sgemm_\n-- Looking for sgemm_ - found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (open).\n-- Looking for cheev_\n-- Looking for cheev_ - found\n-- Found a library with LAPACK API (open).\n-- Found CUDNN: /usr/include  \n-- Found cuDNN: v7.0.4  (include: /usr/include, library: /usr/lib/aarch64-linux-gnu/libcudnn.so.7)\ndisabling MKLDNN because NO_MKLDNN is set\n-- Using python found in /usr/bin/python3\n-- Performing Test SUPPORTS_STDCXX11\n-- Performing Test SUPPORTS_STDCXX11 - Success\n-- Performing Test SUPPORTS_MRTM\n-- Performing Test SUPPORTS_MRTM - Failed\n-- Performing Test SUPPORTS_FLIFETIME\n-- Performing Test SUPPORTS_FLIFETIME - Failed\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\n-- Check if compiler accepts -pthread\n-- Check if compiler accepts -pthread - yes\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring incomplete, errors occurred!\nSee also \"/home/nvidia/code/tmp/pytorch/aten/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/nvidia/code/tmp/pytorch/aten/build/CMakeFiles/CMakeError.log\".", "body": "Following this script\r\nhttps://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426\r\n\r\nbut doing \r\n`git checkout tags/v0.4.0`\r\n\r\nFails during compilation on an NVIDIA Tegra (arm processor). The same environment on the Tegra will build successfully using `git checkout tags/v0.3.1`. Can you please offer any suggestions?\r\n\r\nUsing cmake version 3.5.1 and gcc 5.4.0\r\n\r\nThe final output of `pytorch/aten/build/CMakeFiles/CMakeError.log` is:\r\n\r\n```\r\npytorch$ sudo python3 setup.py install\r\nrunning install\r\nrunning build_deps\r\n+ WITH_CUDA=0\r\n+ [[ --with-cuda == \\-\\-\\w\\i\\t\\h\\-\\c\\u\\d\\a ]]\r\n+ WITH_CUDA=1\r\n+ shift\r\n+ WITH_NNPACK=0\r\n+ [[ --with-nnpack == \\-\\-\\w\\i\\t\\h\\-\\n\\n\\p\\a\\c\\k ]]\r\n+ WITH_NNPACK=1\r\n+ shift\r\n+ WITH_MKLDNN=0\r\n+ [[ nccl == \\-\\-\\w\\i\\t\\h\\-\\m\\k\\l\\d\\n\\n ]]\r\n+ WITH_GLOO_IBVERBS=0\r\n+ [[ nccl == \\-\\-\\w\\i\\t\\h\\-\\g\\l\\o\\o\\-\\i\\b\\v\\e\\r\\b\\s ]]\r\n+ CMAKE_INSTALL='make install'\r\n+ USER_CFLAGS=\r\n+ USER_LDFLAGS=\r\n+ [[ -n '' ]]\r\n+ [[ -n '' ]]\r\n+ [[ -n '' ]]\r\n++ dirname tools/build_pytorch_libs.sh\r\n+ cd tools/..\r\n+++ pwd\r\n++ printf '%q\\n' /home/nvidia/code/tmp/pytorch\r\n+ PWD=/home/nvidia/code/tmp/pytorch\r\n+ BASE_DIR=/home/nvidia/code/tmp/pytorch\r\n+ TORCH_LIB_DIR=/home/nvidia/code/tmp/pytorch/torch/lib\r\n+ INSTALL_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install\r\n+ THIRD_PARTY_DIR=/home/nvidia/code/tmp/pytorch/third_party\r\n+ CMAKE_VERSION=cmake\r\n+ C_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\"'\r\n+ C_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1'\r\n+ LDFLAGS='-L\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\" '\r\n+ LD_POSTFIX=.so.1\r\n+ LD_POSTFIX_UNVERSIONED=.so\r\n++ uname\r\n+ [[ Linux == \\D\\a\\r\\w\\i\\n ]]\r\n+ LDFLAGS='-L\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\"  -Wl,-rpath,$ORIGIN'\r\n+ CPP_FLAGS=' -std=c++11 '\r\n+ GLOO_FLAGS=\r\n+ THD_FLAGS=\r\n+ NCCL_ROOT_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install\r\n+ [[ 1 -eq 1 ]]\r\n+ GLOO_FLAGS='-DUSE_CUDA=1 -DNCCL_ROOT_DIR=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install'\r\n+ [[ 0 -eq 1 ]]\r\n+ CWRAP_FILES='/home/nvidia/code/tmp/pytorch/torch/lib/ATen/Declarations.cwrap;/home/nvidia/code/tmp/pytorch/torch/lib/THNN/generic/THNN.h;/home/nvidia/code/tmp/pytorch/torch/lib/THCUNN/generic/THCUNN.h;/home/nvidia/code/tmp/pytorch/torch/lib/ATen/nn.yaml'\r\n+ CUDA_NVCC_FLAGS=' -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1'\r\n+ [[ '' -eq 1 ]]\r\n+ '[' -z 6 ']'\r\n+ BUILD_TYPE=Release\r\n+ [[ -n '' ]]\r\n+ [[ -n '' ]]\r\n+ echo 'Building in Release mode'\r\nBuilding in Release mode\r\n+ mkdir -p torch/lib/tmp_install\r\n+ for arg in '\"$@\"'\r\n+ [[ nccl == \\n\\c\\c\\l ]]\r\n+ pushd /home/nvidia/code/tmp/pytorch/third_party\r\n~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\r\n+ build_nccl\r\n+ mkdir -p build/nccl\r\n+ pushd build/nccl\r\n~/code/tmp/pytorch/third_party/build/nccl ~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\r\n+ cmake ../../nccl -DCMAKE_MODULE_PATH=/home/nvidia/code/tmp/pytorch/cmake/FindCUDA -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install '-DCMAKE_C_FLAGS= -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1 ' '-DCMAKE_CXX_FLAGS= -DTH_INDEX_BASE=0 -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/TH\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THC\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THS\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCS\"   -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THNN\" -I\"/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/THCUNN\" -DOMPI_SKIP_MPICXX=1  -std=c++11  ' -DCMAKE_SHARED_LINKER_FLAGS=\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/nvidia/code/tmp/pytorch/third_party/build/nccl\r\n+ make install\r\nScanning dependencies of target nccl\r\n[100%] Generating lib/libnccl.so\r\nGrabbing  src/nccl.h                          > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/include/nccl.h\r\nCompiling src/libwrap.cu                      > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/libwrap.o\r\nCompiling src/core.cu                         > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/core.o\r\nCompiling src/all_gather.cu                   > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/all_gather.o\r\nCompiling src/all_reduce.cu                   > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/all_reduce.o\r\nCompiling src/broadcast.cu                    > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/broadcast.o\r\nCompiling src/reduce.cu                       > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/reduce.o\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nCompiling src/reduce_scatter.cu               > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/obj/reduce_scatter.o\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nLinking   libnccl.so.1.3.5                    > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/lib/libnccl.so.1.3.5\r\nArchiving libnccl_static.a                    > /home/nvidia/code/tmp/pytorch/third_party/build/nccl/lib/libnccl_static.a\r\n[100%] Built target nccl\r\nInstall the project...\r\n-- Install configuration: \"Release\"\r\n-- Installing: /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/include/nccl.h\r\n+ mkdir -p /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib\r\n+ cp lib/libnccl.so.1 /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so.1\r\n+ '[' '!' -f /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so ']'\r\n+ ln -s /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so.1 /home/nvidia/code/tmp/pytorch/torch/lib/tmp_install/lib/libnccl.so\r\n+ popd\r\n~/code/tmp/pytorch/third_party ~/code/tmp/pytorch\r\n+ popd\r\n~/code/tmp/pytorch\r\n+ for arg in '\"$@\"'\r\n+ [[ ATen == \\n\\c\\c\\l ]]\r\n+ [[ ATen == \\g\\l\\o\\o ]]\r\n+ [[ ATen == \\A\\T\\e\\n ]]\r\n+ pushd /home/nvidia/code/tmp/pytorch/aten\r\n~/code/tmp/pytorch/aten ~/code/tmp/pytorch\r\n+ build_aten\r\n+ mkdir -p build\r\n+ pushd build\r\n~/code/tmp/pytorch/aten/build ~/code/tmp/pytorch/aten ~/code/tmp/pytorch\r\n+ cmake .. -DCMAKE_BUILD_TYPE=Release -DNO_CUDA=0 -DNO_NNPACK=0 -DCUDNN_INCLUDE_DIR=/usr/include/ -DCUDNN_LIB_DIR=/usr/lib/aarch64-linux-gnu/ -DCUDNN_LIBRARY=/usr/lib/aarch64-linux-gnu/libcudnn.so.7 -DNO_MKLDNN=1 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DATEN_NO_CONTRIB=1 -DCMAKE_INSTALL_PREFIX=/home/nvidia/code/tmp/pytorch/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS=\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \r\n-- Autodetected CUDA architecture(s): nvrm_gpu: Bug 200215060 workaround enabled.\r\n6.1 6.2 \r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\r\n  Unknown CUDA Architecture Name nvrm_gpu: in CUDA_SELECT_NVCC_ARCH_FLAGS\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\r\n  arch_bin wasn't set for some reason\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\r\n  Unknown CUDA Architecture Name Bug in CUDA_SELECT_NVCC_ARCH_FLAGS\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\r\n  arch_bin wasn't set for some reason\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\r\n  Unknown CUDA Architecture Name 200215060 in CUDA_SELECT_NVCC_ARCH_FLAGS\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\r\n  arch_bin wasn't set for some reason\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\r\n  Unknown CUDA Architecture Name workaround in CUDA_SELECT_NVCC_ARCH_FLAGS\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\r\n  arch_bin wasn't set for some reason\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:168 (message):\r\n  Unknown CUDA Architecture Name enabled.\r\n\r\n  6.1 in CUDA_SELECT_NVCC_ARCH_FLAGS\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\nCMake Error at /home/nvidia/code/tmp/pytorch/cmake/Modules_CUDA_fix/FindCUDA/select_compute_arch.cmake:172 (message):\r\n  arch_bin wasn't set for some reason\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:130 (CUDA_SELECT_NVCC_ARCH_FLAGS)\r\n\r\n\r\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- Try OpenMP C flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Try OpenMP CXX flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Found OpenMP: -fopenmp  \r\n-- Compiling with OpenMP support\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- asimd/Neon found with compiler flag : -D__NEON__\r\n-- Performing Test COMPILER_WORKS\r\n-- Performing Test COMPILER_WORKS - Success\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - not found\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Failed\r\n-- Performing Test C_HAS_SSE1_1\r\n-- Performing Test C_HAS_SSE1_1 - Failed\r\n-- Performing Test C_HAS_SSE1_2\r\n-- Performing Test C_HAS_SSE1_2 - Failed\r\n-- Performing Test C_HAS_SSE1_3\r\n-- Performing Test C_HAS_SSE1_3 - Failed\r\n-- Performing Test C_HAS_SSE2_1\r\n-- Performing Test C_HAS_SSE2_1 - Failed\r\n-- Performing Test C_HAS_SSE2_2\r\n-- Performing Test C_HAS_SSE2_2 - Failed\r\n-- Performing Test C_HAS_SSE2_3\r\n-- Performing Test C_HAS_SSE2_3 - Failed\r\n-- Performing Test C_HAS_SSE3_1\r\n-- Performing Test C_HAS_SSE3_1 - Failed\r\n-- Performing Test C_HAS_SSE3_2\r\n-- Performing Test C_HAS_SSE3_2 - Failed\r\n-- Performing Test C_HAS_SSE3_3\r\n-- Performing Test C_HAS_SSE3_3 - Failed\r\n-- Performing Test C_HAS_SSE4_1_1\r\n-- Performing Test C_HAS_SSE4_1_1 - Failed\r\n-- Performing Test C_HAS_SSE4_1_2\r\n-- Performing Test C_HAS_SSE4_1_2 - Failed\r\n-- Performing Test C_HAS_SSE4_1_3\r\n-- Performing Test C_HAS_SSE4_1_3 - Failed\r\n-- Performing Test C_HAS_SSE4_1_4\r\n-- Performing Test C_HAS_SSE4_1_4 - Failed\r\n-- Performing Test C_HAS_SSE4_2_1\r\n-- Performing Test C_HAS_SSE4_2_1 - Failed\r\n-- Performing Test C_HAS_SSE4_2_2\r\n-- Performing Test C_HAS_SSE4_2_2 - Failed\r\n-- Performing Test C_HAS_SSE4_2_3\r\n-- Performing Test C_HAS_SSE4_2_3 - Failed\r\n-- Performing Test C_HAS_SSE4_2_4\r\n-- Performing Test C_HAS_SSE4_2_4 - Failed\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Failed\r\n-- Performing Test C_HAS_AVX_3\r\n-- Performing Test C_HAS_AVX_3 - Failed\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Failed\r\n-- Performing Test C_HAS_AVX2_3\r\n-- Performing Test C_HAS_AVX2_3 - Failed\r\n-- Performing Test CXX_HAS_SSE1_1\r\n-- Performing Test CXX_HAS_SSE1_1 - Failed\r\n-- Performing Test CXX_HAS_SSE1_2\r\n-- Performing Test CXX_HAS_SSE1_2 - Failed\r\n-- Performing Test CXX_HAS_SSE1_3\r\n-- Performing Test CXX_HAS_SSE1_3 - Failed\r\n-- Performing Test CXX_HAS_SSE2_1\r\n-- Performing Test CXX_HAS_SSE2_1 - Failed\r\n-- Performing Test CXX_HAS_SSE2_2\r\n-- Performing Test CXX_HAS_SSE2_2 - Failed\r\n-- Performing Test CXX_HAS_SSE2_3\r\n-- Performing Test CXX_HAS_SSE2_3 - Failed\r\n-- Performing Test CXX_HAS_SSE3_1\r\n-- Performing Test CXX_HAS_SSE3_1 - Failed\r\n-- Performing Test CXX_HAS_SSE3_2\r\n-- Performing Test CXX_HAS_SSE3_2 - Failed\r\n-- Performing Test CXX_HAS_SSE3_3\r\n-- Performing Test CXX_HAS_SSE3_3 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_1\r\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_2\r\n-- Performing Test CXX_HAS_SSE4_1_2 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_3\r\n-- Performing Test CXX_HAS_SSE4_1_3 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_4\r\n-- Performing Test CXX_HAS_SSE4_1_4 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_1\r\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_2\r\n-- Performing Test CXX_HAS_SSE4_2_2 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_3\r\n-- Performing Test CXX_HAS_SSE4_2_3 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_4\r\n-- Performing Test CXX_HAS_SSE4_2_4 - Failed\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Failed\r\n-- Performing Test CXX_HAS_AVX_3\r\n-- Performing Test CXX_HAS_AVX_3 - Failed\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Failed\r\n-- Performing Test CXX_HAS_AVX2_3\r\n-- Performing Test CXX_HAS_AVX2_3 - Failed\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Failed\r\n-- Performing Test HAS_MSC_ATOMICS\r\n-- Performing Test HAS_MSC_ATOMICS - Failed\r\n-- Performing Test HAS_GCC_ATOMICS\r\n-- Performing Test HAS_GCC_ATOMICS - Success\r\n-- Atomics: using GCC intrinsics\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl - guide - pthread - m]\r\n--   Library mkl: not found\r\n-- MKL library not found\r\n-- Checking for [openblas]\r\n--   Library openblas: /usr/lib/libopenblas.so\r\n-- Looking for sgemm_\r\n-- Looking for sgemm_ - found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (open).\r\n-- Looking for cheev_\r\n-- Looking for cheev_ - found\r\n-- Found a library with LAPACK API (open).\r\n-- Found CUDNN: /usr/include  \r\n-- Found cuDNN: v7.0.4  (include: /usr/include, library: /usr/lib/aarch64-linux-gnu/libcudnn.so.7)\r\ndisabling MKLDNN because NO_MKLDNN is set\r\n-- Using python found in /usr/bin/python3\r\n-- Performing Test SUPPORTS_STDCXX11\r\n-- Performing Test SUPPORTS_STDCXX11 - Success\r\n-- Performing Test SUPPORTS_MRTM\r\n-- Performing Test SUPPORTS_MRTM - Failed\r\n-- Performing Test SUPPORTS_FLIFETIME\r\n-- Performing Test SUPPORTS_FLIFETIME - Failed\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\ndisable contrib because ATEN_NO_CONTRIB is set\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/nvidia/code/tmp/pytorch/aten/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/home/nvidia/code/tmp/pytorch/aten/build/CMakeFiles/CMakeError.log\".\r\n```"}