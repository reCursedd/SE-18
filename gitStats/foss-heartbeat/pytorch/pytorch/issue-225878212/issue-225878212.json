{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1450", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1450/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1450/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1450/events", "html_url": "https://github.com/pytorch/pytorch/issues/1450", "id": 225878212, "node_id": "MDU6SXNzdWUyMjU4NzgyMTI=", "number": 1450, "title": "Bug in new autograd backward (with LSTM Cell)", "user": {"login": "ChenRocks", "id": 6073275, "node_id": "MDQ6VXNlcjYwNzMyNzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6073275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChenRocks", "html_url": "https://github.com/ChenRocks", "followers_url": "https://api.github.com/users/ChenRocks/followers", "following_url": "https://api.github.com/users/ChenRocks/following{/other_user}", "gists_url": "https://api.github.com/users/ChenRocks/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChenRocks/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChenRocks/subscriptions", "organizations_url": "https://api.github.com/users/ChenRocks/orgs", "repos_url": "https://api.github.com/users/ChenRocks/repos", "events_url": "https://api.github.com/users/ChenRocks/events{/privacy}", "received_events_url": "https://api.github.com/users/ChenRocks/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-05-03T03:47:37Z", "updated_at": "2017-07-28T01:54:27Z", "closed_at": "2017-05-03T12:15:37Z", "author_association": "NONE", "body_html": "<p>This is how I implement the decoder of a sequence to sequence model</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> functional <span class=\"pl-k\">as</span> F\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">decoder</span>(<span class=\"pl-smi\">input_</span>, <span class=\"pl-smi\">embedding</span>, <span class=\"pl-smi\">lstm</span>, <span class=\"pl-smi\">projection</span>, <span class=\"pl-smi\">states</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> unroll the LSTM Cell, returns the flattened logits<span class=\"pl-pds\">\"\"\"</span></span>\n    emb <span class=\"pl-k\">=</span> embedding(input_.t())\n    hs <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(input_.size(<span class=\"pl-c1\">1</span>)):\n        h, c <span class=\"pl-k\">=</span> lstm(emb[i], states)\n        hs.append(h)\n        states <span class=\"pl-k\">=</span> (h, c)\n    lstm_out <span class=\"pl-k\">=</span> torch.stack(hs, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    logit <span class=\"pl-k\">=</span> projection(lstm_out.contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, lstm.hidden_size))\n    <span class=\"pl-k\">return</span> logit\n\nembedding <span class=\"pl-k\">=</span> nn.Embedding(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">padding_idx</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>).cuda()\nlstm <span class=\"pl-k\">=</span> nn.LSTMCell(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>).cuda()\nprojection <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">4</span>).cuda()\n\ninput_ <span class=\"pl-k\">=</span> Variable(torch.LongTensor([[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>]])).cuda()\nstates <span class=\"pl-k\">=</span> (Variable(torch.zeros(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">64</span>)).cuda(), Variable(torch.zeros(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">64</span>)).cuda())\ntarget <span class=\"pl-k\">=</span> Variable(torch.LongTensor([[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>]])).cuda()\n\nlogit <span class=\"pl-k\">=</span> decoder(input_, embedding, lstm, projection, states)\nloss <span class=\"pl-k\">=</span> F.cross_entropy(logit, target.t().contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\nloss.backward()  <span class=\"pl-c\"><span class=\"pl-c\">#</span>  RuntimeError: No grad accumulator for a saved leaf!</span>\n</pre></div>\n<p>I'm not sure about the new autograd mechanics but this worked in the previous version.<br>\nIf I didn't make the unrolling codes a function it will work. It will also work with CPU.<br>\nI compiled from source <a href=\"https://github.com/pytorch/pytorch/commit/699755e04f8bbb4378a70f03dfe0849094fd0255\">(699755e)</a> with Python 2.7, CUDA 8.0 and Cudnn 6</p>", "body_text": "This is how I implement the decoder of a sequence to sequence model\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\ndef decoder(input_, embedding, lstm, projection, states):\n    \"\"\" unroll the LSTM Cell, returns the flattened logits\"\"\"\n    emb = embedding(input_.t())\n    hs = []\n    for i in range(input_.size(1)):\n        h, c = lstm(emb[i], states)\n        hs.append(h)\n        states = (h, c)\n    lstm_out = torch.stack(hs, dim=0)\n    logit = projection(lstm_out.contiguous().view(-1, lstm.hidden_size))\n    return logit\n\nembedding = nn.Embedding(4, 64, padding_idx=0).cuda()\nlstm = nn.LSTMCell(64, 64).cuda()\nprojection = nn.Linear(64, 4).cuda()\n\ninput_ = Variable(torch.LongTensor([[1, 2, 3], [3, 2, 1]])).cuda()\nstates = (Variable(torch.zeros(2, 64)).cuda(), Variable(torch.zeros(2, 64)).cuda())\ntarget = Variable(torch.LongTensor([[3, 2, 1], [2, 3, 1]])).cuda()\n\nlogit = decoder(input_, embedding, lstm, projection, states)\nloss = F.cross_entropy(logit, target.t().contiguous().view(-1))\nloss.backward()  #  RuntimeError: No grad accumulator for a saved leaf!\n\nI'm not sure about the new autograd mechanics but this worked in the previous version.\nIf I didn't make the unrolling codes a function it will work. It will also work with CPU.\nI compiled from source (699755e) with Python 2.7, CUDA 8.0 and Cudnn 6", "body": "This is how I implement the decoder of a sequence to sequence model\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\n\r\ndef decoder(input_, embedding, lstm, projection, states):\r\n    \"\"\" unroll the LSTM Cell, returns the flattened logits\"\"\"\r\n    emb = embedding(input_.t())\r\n    hs = []\r\n    for i in range(input_.size(1)):\r\n        h, c = lstm(emb[i], states)\r\n        hs.append(h)\r\n        states = (h, c)\r\n    lstm_out = torch.stack(hs, dim=0)\r\n    logit = projection(lstm_out.contiguous().view(-1, lstm.hidden_size))\r\n    return logit\r\n\r\nembedding = nn.Embedding(4, 64, padding_idx=0).cuda()\r\nlstm = nn.LSTMCell(64, 64).cuda()\r\nprojection = nn.Linear(64, 4).cuda()\r\n\r\ninput_ = Variable(torch.LongTensor([[1, 2, 3], [3, 2, 1]])).cuda()\r\nstates = (Variable(torch.zeros(2, 64)).cuda(), Variable(torch.zeros(2, 64)).cuda())\r\ntarget = Variable(torch.LongTensor([[3, 2, 1], [2, 3, 1]])).cuda()\r\n\r\nlogit = decoder(input_, embedding, lstm, projection, states)\r\nloss = F.cross_entropy(logit, target.t().contiguous().view(-1))\r\nloss.backward()  #  RuntimeError: No grad accumulator for a saved leaf!\r\n\r\n```\r\nI'm not sure about the new autograd mechanics but this worked in the previous version.\r\nIf I didn't make the unrolling codes a function it will work. It will also work with CPU.\r\nI compiled from source [(699755e)](https://github.com/pytorch/pytorch/commit/699755e04f8bbb4378a70f03dfe0849094fd0255) with Python 2.7, CUDA 8.0 and Cudnn 6"}