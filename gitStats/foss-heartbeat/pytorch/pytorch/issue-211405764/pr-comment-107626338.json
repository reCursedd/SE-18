{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/107626338", "pull_request_review_id": 28608531, "id": 107626338, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNzYyNjMzOA==", "diff_hunk": "@@ -738,6 +738,50 @@ def test_caching_pinned_memory_multi_gpu(self):\n         self.assertEqual(gpu_tensor1[0], 1)\n         self.assertEqual(gpu_tensor0[0], 2)\n \n+    def test_btrifact(self):\n+        a = torch.Tensor((((1.3722, -0.9020),\n+                           (1.8849, 1.9169)),\n+                          ((0.7187, -1.1695),\n+                           (-0.0139, 1.3572)),\n+                          ((-1.6181, 0.7148),\n+                           (1.3728, 0.1319)))).cuda()\n+        LU_data, pivots, info = a.btrifact()\n+        self.assertEqual(info.abs().sum(), 0)\n+        I_U = torch.triu(torch.ones(2, 2)).unsqueeze(0).expand(3, 2, 2).type_as(a).byte()\n+        I_L = 1 - I_U\n+        a_L = torch.zeros(a.size()).type_as(a)\n+        a_U = a_L.clone()\n+        a_L[torch.eye(2).unsqueeze(0).expand(3, 2, 2).type_as(a).byte()] = 1.0\n+        a_L[I_L] = LU_data[I_L]\n+        a_U[I_U] = LU_data[I_U]\n+\n+        P = torch.eye(2).unsqueeze(0).expand(3, 2, 2).type_as(a)\n+        for i in range(3):\n+            for j in range(2):\n+                k = pivots[i, j] - 1", "path": "test/test_cuda.py", "position": null, "original_position": 24, "commit_id": "73dcc642f27c87cfd1e908b9bf2a7309c06bcf1b", "original_commit_id": "8bf70088b3ef04be06ca52a36e4bbda6a9d56cc2", "user": {"login": "bamos", "id": 707462, "node_id": "MDQ6VXNlcjcwNzQ2Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/707462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bamos", "html_url": "https://github.com/bamos", "followers_url": "https://api.github.com/users/bamos/followers", "following_url": "https://api.github.com/users/bamos/following{/other_user}", "gists_url": "https://api.github.com/users/bamos/gists{/gist_id}", "starred_url": "https://api.github.com/users/bamos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bamos/subscriptions", "organizations_url": "https://api.github.com/users/bamos/orgs", "repos_url": "https://api.github.com/users/bamos/repos", "events_url": "https://api.github.com/users/bamos/events{/privacy}", "received_events_url": "https://api.github.com/users/bamos/received_events", "type": "User", "site_admin": false}, "body": "Yes, this is standard in LAPACK's C interface (http://www.netlib.org/clapack/old/double/dgetrf.c) and cublas (http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-getrfbatched)", "created_at": "2017-03-23T09:56:01Z", "updated_at": "2018-11-23T15:32:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/903#discussion_r107626338", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/903", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/107626338"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/903#discussion_r107626338"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/903"}}, "body_html": "<p>Yes, this is standard in LAPACK's C interface (<a href=\"http://www.netlib.org/clapack/old/double/dgetrf.c\" rel=\"nofollow\">http://www.netlib.org/clapack/old/double/dgetrf.c</a>) and cublas (<a href=\"http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-getrfbatched\" rel=\"nofollow\">http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-getrfbatched</a>)</p>", "body_text": "Yes, this is standard in LAPACK's C interface (http://www.netlib.org/clapack/old/double/dgetrf.c) and cublas (http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-getrfbatched)", "in_reply_to_id": 107530691}