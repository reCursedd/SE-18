{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8716", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8716/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8716/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8716/events", "html_url": "https://github.com/pytorch/pytorch/issues/8716", "id": 334260941, "node_id": "MDU6SXNzdWUzMzQyNjA5NDE=", "number": 8716, "title": "[JIT] Traced and Script modules are not properly inlined into script functions", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-20T21:44:34Z", "updated_at": "2018-06-29T17:34:39Z", "closed_at": "2018-06-29T17:34:39Z", "author_association": "CONTRIBUTOR", "body_html": "<pre><code>+    @unittest.skip('TODO: Incorrect behavior')\n+    def test_call_traced_mod_from_script_fn(self):\n+        class TracedModule(torch.nn.Module):\n+            def __init__(self):\n+                super(TracedModule, self).__init__()\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\n+\n+            def forward(self, x):\n+                return torch.mm(x, self.param)\n+\n+        tm = torch.jit.trace(torch.rand(3, 4))(TracedModule())\n+\n+        @torch.jit.script\n+        def script_fn(x):\n+            return tm(x) + 1\n+\n+        # Note: At the time of writing this produces the following graph:\n+        # graph(%x : Dynamic) {\n+        #   %1 : Dynamic = ^&lt;python_value&gt;()(%x)\n+        #   %2 : Long() = prim::Constant[value={1}]()\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\n+        #   return (%3);\n+        # }\n+        # This seems incorrect. Similarly to calling a traced module from a\n+        # traced function, the behavior here should likely be that we inline\n+        # the parameter from the traced module as a Constant node and we inline\n+        # the ops into the graph of the script function. TODO: fix\n+        self.assertExpected(str(script_fn.graph))\n+\n+    def test_call_script_mod_from_script_fn(self):\n+        class ScriptMod(torch.jit.ScriptModule):\n+            def __init__(self):\n+                super(ScriptMod, self).__init__()\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\n+\n+            @torch.jit.script_method\n+            def forward(self, x):\n+                return torch.mm(x, self.param)\n+\n+        sm = ScriptMod()\n+\n+        @torch.jit.script\n+        def script_fn(x):\n+            return sm(x) + 1\n+\n+        # Note: At the time of writing this produces the following graph:\n+        # graph(%x : Dynamic) {\n+        #   %1 : Dynamic = ^&lt;python_value&gt;()(%x)\n+        #   %2 : Long() = prim::Constant[value={1}]()\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\n+        #   return (%3);\n+        # }\n+        # This seems incorrect. Similarly to calling a script module from a\n+        # traced function, the behavior here should likely be that we inline\n+        # the parameter from the traced module as a Constant node and we inline\n+        # the ops into the graph of the script function. TODO: fix\n+        self.assertExpected(str(script_fn.graph))\n</code></pre>", "body_text": "+    @unittest.skip('TODO: Incorrect behavior')\n+    def test_call_traced_mod_from_script_fn(self):\n+        class TracedModule(torch.nn.Module):\n+            def __init__(self):\n+                super(TracedModule, self).__init__()\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\n+\n+            def forward(self, x):\n+                return torch.mm(x, self.param)\n+\n+        tm = torch.jit.trace(torch.rand(3, 4))(TracedModule())\n+\n+        @torch.jit.script\n+        def script_fn(x):\n+            return tm(x) + 1\n+\n+        # Note: At the time of writing this produces the following graph:\n+        # graph(%x : Dynamic) {\n+        #   %1 : Dynamic = ^<python_value>()(%x)\n+        #   %2 : Long() = prim::Constant[value={1}]()\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\n+        #   return (%3);\n+        # }\n+        # This seems incorrect. Similarly to calling a traced module from a\n+        # traced function, the behavior here should likely be that we inline\n+        # the parameter from the traced module as a Constant node and we inline\n+        # the ops into the graph of the script function. TODO: fix\n+        self.assertExpected(str(script_fn.graph))\n+\n+    def test_call_script_mod_from_script_fn(self):\n+        class ScriptMod(torch.jit.ScriptModule):\n+            def __init__(self):\n+                super(ScriptMod, self).__init__()\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\n+\n+            @torch.jit.script_method\n+            def forward(self, x):\n+                return torch.mm(x, self.param)\n+\n+        sm = ScriptMod()\n+\n+        @torch.jit.script\n+        def script_fn(x):\n+            return sm(x) + 1\n+\n+        # Note: At the time of writing this produces the following graph:\n+        # graph(%x : Dynamic) {\n+        #   %1 : Dynamic = ^<python_value>()(%x)\n+        #   %2 : Long() = prim::Constant[value={1}]()\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\n+        #   return (%3);\n+        # }\n+        # This seems incorrect. Similarly to calling a script module from a\n+        # traced function, the behavior here should likely be that we inline\n+        # the parameter from the traced module as a Constant node and we inline\n+        # the ops into the graph of the script function. TODO: fix\n+        self.assertExpected(str(script_fn.graph))", "body": "```\r\n+    @unittest.skip('TODO: Incorrect behavior')\r\n+    def test_call_traced_mod_from_script_fn(self):\r\n+        class TracedModule(torch.nn.Module):\r\n+            def __init__(self):\r\n+                super(TracedModule, self).__init__()\r\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\r\n+\r\n+            def forward(self, x):\r\n+                return torch.mm(x, self.param)\r\n+\r\n+        tm = torch.jit.trace(torch.rand(3, 4))(TracedModule())\r\n+\r\n+        @torch.jit.script\r\n+        def script_fn(x):\r\n+            return tm(x) + 1\r\n+\r\n+        # Note: At the time of writing this produces the following graph:\r\n+        # graph(%x : Dynamic) {\r\n+        #   %1 : Dynamic = ^<python_value>()(%x)\r\n+        #   %2 : Long() = prim::Constant[value={1}]()\r\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\r\n+        #   return (%3);\r\n+        # }\r\n+        # This seems incorrect. Similarly to calling a traced module from a\r\n+        # traced function, the behavior here should likely be that we inline\r\n+        # the parameter from the traced module as a Constant node and we inline\r\n+        # the ops into the graph of the script function. TODO: fix\r\n+        self.assertExpected(str(script_fn.graph))\r\n+\r\n+    def test_call_script_mod_from_script_fn(self):\r\n+        class ScriptMod(torch.jit.ScriptModule):\r\n+            def __init__(self):\r\n+                super(ScriptMod, self).__init__()\r\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\r\n+\r\n+            @torch.jit.script_method\r\n+            def forward(self, x):\r\n+                return torch.mm(x, self.param)\r\n+\r\n+        sm = ScriptMod()\r\n+\r\n+        @torch.jit.script\r\n+        def script_fn(x):\r\n+            return sm(x) + 1\r\n+\r\n+        # Note: At the time of writing this produces the following graph:\r\n+        # graph(%x : Dynamic) {\r\n+        #   %1 : Dynamic = ^<python_value>()(%x)\r\n+        #   %2 : Long() = prim::Constant[value={1}]()\r\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\r\n+        #   return (%3);\r\n+        # }\r\n+        # This seems incorrect. Similarly to calling a script module from a\r\n+        # traced function, the behavior here should likely be that we inline\r\n+        # the parameter from the traced module as a Constant node and we inline\r\n+        # the ops into the graph of the script function. TODO: fix\r\n+        self.assertExpected(str(script_fn.graph))\r\n```"}