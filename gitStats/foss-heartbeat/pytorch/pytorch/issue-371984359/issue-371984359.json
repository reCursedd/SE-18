{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12873", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12873/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12873/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12873/events", "html_url": "https://github.com/pytorch/pytorch/issues/12873", "id": 371984359, "node_id": "MDU6SXNzdWUzNzE5ODQzNTk=", "number": 12873, "title": "Massive initial memory overhead GPU", "user": {"login": "davidmascharka", "id": 5611862, "node_id": "MDQ6VXNlcjU2MTE4NjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5611862?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidmascharka", "html_url": "https://github.com/davidmascharka", "followers_url": "https://api.github.com/users/davidmascharka/followers", "following_url": "https://api.github.com/users/davidmascharka/following{/other_user}", "gists_url": "https://api.github.com/users/davidmascharka/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidmascharka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidmascharka/subscriptions", "organizations_url": "https://api.github.com/users/davidmascharka/orgs", "repos_url": "https://api.github.com/users/davidmascharka/repos", "events_url": "https://api.github.com/users/davidmascharka/events{/privacy}", "received_events_url": "https://api.github.com/users/davidmascharka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-10-19T14:34:09Z", "updated_at": "2018-10-22T17:39:10Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>There is a huge RAM overhead for using the GPU even for processing small tensors.</p>\n<p>Here's a standalone script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test.py</span>\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> argparse\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>size<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)\nargs <span class=\"pl-k\">=</span> parser.parse_args()\n\ntorch.set_grad_enabled(<span class=\"pl-c1\">False</span>)\ndevice <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> torch.cuda.is_available() <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>\nmodel <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).to(device)\nx <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, args.size, args.size).to(device)\ny <span class=\"pl-k\">=</span> model(x)</pre></div>\n<p>Recording using <a href=\"https://www.gnu.org/software/time/\" rel=\"nofollow\">GNU time</a>:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ /usr/bin/time -v python test.py 100\n        Command being timed: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python test.py 100<span class=\"pl-pds\">\"</span></span>\n        User <span class=\"pl-k\">time</span> (seconds): 0.26\n        System <span class=\"pl-k\">time</span> (seconds): 0.03\n        Percent of CPU this job got: 114%\n        Elapsed (wall clock) <span class=\"pl-k\">time</span> (h:mm:ss or m:ss): 0:00.26\n        Average shared text size (kbytes): 0\n        Average unshared data size (kbytes): 0\n        Average stack size (kbytes): 0\n        Average total size (kbytes): 0\n        Maximum resident <span class=\"pl-c1\">set</span> size (kbytes): 1904088\n        Average resident <span class=\"pl-c1\">set</span> size (kbytes): 0\n        Major (requiring I/O) page faults: 0\n        Minor (reclaiming a frame) page faults: 16238\n        Voluntary context switches: 40\n        Involuntary context switches: 19\n        Swaps: 0\n        File system inputs: 0\n        File system outputs: 0\n        Socket messages sent: 0\n        Socket messages received: 0\n        Signals delivered: 0\n        Page size (bytes): 4096\n        Exit status: 0</pre></div>\n<p>The line to pay attention here is: <strong>Maximum resident set size (kbytes): 1904088</strong>. It takes roughly 2 GB of RAM in order to simply <em>use the GPU</em> to process a 100x100 image. In contrast, doing the same for CPU:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ CUDA_VISIBLE_DEVICES=<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span> /usr/bin/time -v python test.py 100\n        Command being timed: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python test.py 100<span class=\"pl-pds\">\"</span></span>\n        User <span class=\"pl-k\">time</span> (seconds): 0.29\n        System <span class=\"pl-k\">time</span> (seconds): 0.04\n        Percent of CPU this job got: 116%\n        Elapsed (wall clock) <span class=\"pl-k\">time</span> (h:mm:ss or m:ss): 0:00.29\n        Average shared text size (kbytes): 0\n        Average unshared data size (kbytes): 0\n        Average stack size (kbytes): 0\n        Average total size (kbytes): 0\n        Maximum resident <span class=\"pl-c1\">set</span> size (kbytes): 149352\n        Average resident <span class=\"pl-c1\">set</span> size (kbytes): 0\n        Major (requiring I/O) page faults: 0\n        Minor (reclaiming a frame) page faults: 16432\n        Voluntary context switches: 39\n        Involuntary context switches: 19\n        Swaps: 0\n        File system inputs: 0\n        File system outputs: 0\n        Socket messages sent: 0\n        Socket messages received: 0\n        Signals delivered: 0\n        Page size (bytes): 4096\n        Exit status: 0</pre></div>\n<p>takes only ~150 MB. Using the following script, I constructed a plot of RAM usage vs image size:</p>\n<div class=\"highlight highlight-source-perl\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test.pl</span>\n<span class=\"pl-k\">foreach</span> <span class=\"pl-k\">my</span> <span class=\"pl-smi\">$device</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, 0) {\n    <span class=\"pl-k\">foreach</span> (1..30) {\n        <span class=\"pl-smi\">$_</span> *= 100;\n        <span class=\"pl-k\">my</span> <span class=\"pl-smi\">@outs</span> = <span class=\"pl-c1\">split</span> /\\n/, <span class=\"pl-s\"><span class=\"pl-pds\">`</span>CUDA_VISIBLE_DEVICES=<span class=\"pl-smi\">$device</span> /usr/bin/time -v python test.py <span class=\"pl-smi\">$_</span> 2&gt;&amp;1<span class=\"pl-pds\">`</span></span>;\n        <span class=\"pl-k\">foreach</span> (<span class=\"pl-smi\">@outs</span>) { <span class=\"pl-c1\">print</span> <span class=\"pl-smi\">$1</span> / 1024 . <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> <span class=\"pl-sr\"><span class=\"pl-pds\"><span class=\"pl-c1\">m</span>/</span>Maximum resident.*?(<span class=\"pl-cce\">\\d</span>+)<span class=\"pl-pds\">/</span></span> }\n    }\n}</pre></div>\n<p>Running <code>perl test.pl</code> produces 60 lines of output; the first 30 are for CPU, the second 30 are GPU. Plotting these yields:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/davidmascharka/davidmascharka.com/master/img.png?token=AFWhVkGfeKu__pRyB5eke9RRz_U1G9gCks5b0yXLwA%3D%3D\"><img src=\"https://raw.githubusercontent.com/davidmascharka/davidmascharka.com/master/img.png?token=AFWhVkGfeKu__pRyB5eke9RRz_U1G9gCks5b0yXLwA%3D%3D\" alt=\"memory use\" style=\"max-width:100%;\"></a></p>\n<p>The numbers produced on my machine are as follows:</p>\n<pre><code># CPU\n145.5234375,\n145.90234375,\n145.609375,\n145.43359375,\n145.56640625,\n145.33984375,\n145.51171875,\n145.3359375,\n146.34375,\n149.0078125,\n150.75,\n153.23046875,\n156.47265625,\n159.55859375,\n162.66796875,\n166.2734375,\n170.31640625,\n173.98046875,\n178.40234375,\n183.2109375,\n187.625,\n192.75390625,\n197.88671875,\n202.8828125,\n209.078125,\n214.2578125,\n220.86328125,\n226.41796875,\n233.5078125,\n239.9375,\n\n# GPU\n1859.98828125,\n1859.20703125,\n1859.90234375,\n1861.25,\n1862.359375,\n1861.1171875,\n1859.54296875,\n1858.77734375,\n1858.9765625,\n1863.28125,\n1862.94921875,\n1859.296875,\n1860.77734375,\n1861.5625,\n1862.75390625,\n1859.83984375,\n1859.99609375,\n1860.80078125,\n1860.09375,\n1862.703125,\n1858.71875,\n1858.75,\n1860.671875,\n1859.6875,\n1859.0234375,\n1858.921875,\n1859.98046875,\n1860.04296875,\n1859.015625,\n1858.77734375,\n</code></pre>\n<h2>Expected behavior</h2>\n<p>Memory usage on the GPU side should not be significantly higher than on the CPU side. Luckily, RAM usage does not grow substantially (as indeed it should not), but the high startup cost is concerning, especially since this is just a 1x1 conv operating on 1-d input.</p>\n<h2>Environment</h2>\n<pre><code>PyTorch version: 1.0.0a0+ff608a9\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\nCMake version: version 3.12.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\n\nNvidia driver version: 390.87\ncuDNN version: 7.0.3\n\nVersions of relevant libraries:\nnumpy (1.15.2)\n</code></pre>\n<h2>Additional Notes</h2>\n<p>I've observed stranger behavior in the curve on the CPU where for small images the memory consumption grows exponentially up to ~2 GB then drops and grows linearly. I'm attempting to reproduce this behavior in a small, standalone script like the above.</p>", "body_text": "\ud83d\udc1b Bug\nThere is a huge RAM overhead for using the GPU even for processing small tensors.\nHere's a standalone script:\n# test.py\nimport torch\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('size', type=int)\nargs = parser.parse_args()\n\ntorch.set_grad_enabled(False)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = torch.nn.Conv2d(1, 1, 1).to(device)\nx = torch.rand(1, 1, args.size, args.size).to(device)\ny = model(x)\nRecording using GNU time:\n$ /usr/bin/time -v python test.py 100\n        Command being timed: \"python test.py 100\"\n        User time (seconds): 0.26\n        System time (seconds): 0.03\n        Percent of CPU this job got: 114%\n        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.26\n        Average shared text size (kbytes): 0\n        Average unshared data size (kbytes): 0\n        Average stack size (kbytes): 0\n        Average total size (kbytes): 0\n        Maximum resident set size (kbytes): 1904088\n        Average resident set size (kbytes): 0\n        Major (requiring I/O) page faults: 0\n        Minor (reclaiming a frame) page faults: 16238\n        Voluntary context switches: 40\n        Involuntary context switches: 19\n        Swaps: 0\n        File system inputs: 0\n        File system outputs: 0\n        Socket messages sent: 0\n        Socket messages received: 0\n        Signals delivered: 0\n        Page size (bytes): 4096\n        Exit status: 0\nThe line to pay attention here is: Maximum resident set size (kbytes): 1904088. It takes roughly 2 GB of RAM in order to simply use the GPU to process a 100x100 image. In contrast, doing the same for CPU:\n$ CUDA_VISIBLE_DEVICES='' /usr/bin/time -v python test.py 100\n        Command being timed: \"python test.py 100\"\n        User time (seconds): 0.29\n        System time (seconds): 0.04\n        Percent of CPU this job got: 116%\n        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.29\n        Average shared text size (kbytes): 0\n        Average unshared data size (kbytes): 0\n        Average stack size (kbytes): 0\n        Average total size (kbytes): 0\n        Maximum resident set size (kbytes): 149352\n        Average resident set size (kbytes): 0\n        Major (requiring I/O) page faults: 0\n        Minor (reclaiming a frame) page faults: 16432\n        Voluntary context switches: 39\n        Involuntary context switches: 19\n        Swaps: 0\n        File system inputs: 0\n        File system outputs: 0\n        Socket messages sent: 0\n        Socket messages received: 0\n        Signals delivered: 0\n        Page size (bytes): 4096\n        Exit status: 0\ntakes only ~150 MB. Using the following script, I constructed a plot of RAM usage vs image size:\n# test.pl\nforeach my $device ('', 0) {\n    foreach (1..30) {\n        $_ *= 100;\n        my @outs = split /\\n/, `CUDA_VISIBLE_DEVICES=$device /usr/bin/time -v python test.py $_ 2>&1`;\n        foreach (@outs) { print $1 / 1024 . \",\\n\" if m/Maximum resident.*?(\\d+)/ }\n    }\n}\nRunning perl test.pl produces 60 lines of output; the first 30 are for CPU, the second 30 are GPU. Plotting these yields:\n\nThe numbers produced on my machine are as follows:\n# CPU\n145.5234375,\n145.90234375,\n145.609375,\n145.43359375,\n145.56640625,\n145.33984375,\n145.51171875,\n145.3359375,\n146.34375,\n149.0078125,\n150.75,\n153.23046875,\n156.47265625,\n159.55859375,\n162.66796875,\n166.2734375,\n170.31640625,\n173.98046875,\n178.40234375,\n183.2109375,\n187.625,\n192.75390625,\n197.88671875,\n202.8828125,\n209.078125,\n214.2578125,\n220.86328125,\n226.41796875,\n233.5078125,\n239.9375,\n\n# GPU\n1859.98828125,\n1859.20703125,\n1859.90234375,\n1861.25,\n1862.359375,\n1861.1171875,\n1859.54296875,\n1858.77734375,\n1858.9765625,\n1863.28125,\n1862.94921875,\n1859.296875,\n1860.77734375,\n1861.5625,\n1862.75390625,\n1859.83984375,\n1859.99609375,\n1860.80078125,\n1860.09375,\n1862.703125,\n1858.71875,\n1858.75,\n1860.671875,\n1859.6875,\n1859.0234375,\n1858.921875,\n1859.98046875,\n1860.04296875,\n1859.015625,\n1858.77734375,\n\nExpected behavior\nMemory usage on the GPU side should not be significantly higher than on the CPU side. Luckily, RAM usage does not grow substantially (as indeed it should not), but the high startup cost is concerning, especially since this is just a 1x1 conv operating on 1-d input.\nEnvironment\nPyTorch version: 1.0.0a0+ff608a9\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\nCMake version: version 3.12.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\n\nNvidia driver version: 390.87\ncuDNN version: 7.0.3\n\nVersions of relevant libraries:\nnumpy (1.15.2)\n\nAdditional Notes\nI've observed stranger behavior in the curve on the CPU where for small images the memory consumption grows exponentially up to ~2 GB then drops and grows linearly. I'm attempting to reproduce this behavior in a small, standalone script like the above.", "body": "## \ud83d\udc1b Bug\r\n\r\nThere is a huge RAM overhead for using the GPU even for processing small tensors.\r\n\r\nHere's a standalone script:\r\n\r\n``` python\r\n# test.py\r\nimport torch\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('size', type=int)\r\nargs = parser.parse_args()\r\n\r\ntorch.set_grad_enabled(False)\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nmodel = torch.nn.Conv2d(1, 1, 1).to(device)\r\nx = torch.rand(1, 1, args.size, args.size).to(device)\r\ny = model(x)\r\n```\r\n\r\nRecording using [GNU time](https://www.gnu.org/software/time/):\r\n\r\n``` shell\r\n$ /usr/bin/time -v python test.py 100\r\n        Command being timed: \"python test.py 100\"\r\n        User time (seconds): 0.26\r\n        System time (seconds): 0.03\r\n        Percent of CPU this job got: 114%\r\n        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.26\r\n        Average shared text size (kbytes): 0\r\n        Average unshared data size (kbytes): 0\r\n        Average stack size (kbytes): 0\r\n        Average total size (kbytes): 0\r\n        Maximum resident set size (kbytes): 1904088\r\n        Average resident set size (kbytes): 0\r\n        Major (requiring I/O) page faults: 0\r\n        Minor (reclaiming a frame) page faults: 16238\r\n        Voluntary context switches: 40\r\n        Involuntary context switches: 19\r\n        Swaps: 0\r\n        File system inputs: 0\r\n        File system outputs: 0\r\n        Socket messages sent: 0\r\n        Socket messages received: 0\r\n        Signals delivered: 0\r\n        Page size (bytes): 4096\r\n        Exit status: 0\r\n```\r\n\r\nThe line to pay attention here is: **Maximum resident set size (kbytes): 1904088**. It takes roughly 2 GB of RAM in order to simply *use the GPU* to process a 100x100 image. In contrast, doing the same for CPU:\r\n\r\n``` shell\r\n$ CUDA_VISIBLE_DEVICES='' /usr/bin/time -v python test.py 100\r\n        Command being timed: \"python test.py 100\"\r\n        User time (seconds): 0.29\r\n        System time (seconds): 0.04\r\n        Percent of CPU this job got: 116%\r\n        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.29\r\n        Average shared text size (kbytes): 0\r\n        Average unshared data size (kbytes): 0\r\n        Average stack size (kbytes): 0\r\n        Average total size (kbytes): 0\r\n        Maximum resident set size (kbytes): 149352\r\n        Average resident set size (kbytes): 0\r\n        Major (requiring I/O) page faults: 0\r\n        Minor (reclaiming a frame) page faults: 16432\r\n        Voluntary context switches: 39\r\n        Involuntary context switches: 19\r\n        Swaps: 0\r\n        File system inputs: 0\r\n        File system outputs: 0\r\n        Socket messages sent: 0\r\n        Socket messages received: 0\r\n        Signals delivered: 0\r\n        Page size (bytes): 4096\r\n        Exit status: 0\r\n```\r\n\r\ntakes only ~150 MB. Using the following script, I constructed a plot of RAM usage vs image size:\r\n\r\n``` perl\r\n# test.pl\r\nforeach my $device ('', 0) {\r\n    foreach (1..30) {\r\n        $_ *= 100;\r\n        my @outs = split /\\n/, `CUDA_VISIBLE_DEVICES=$device /usr/bin/time -v python test.py $_ 2>&1`;\r\n        foreach (@outs) { print $1 / 1024 . \",\\n\" if m/Maximum resident.*?(\\d+)/ }\r\n    }\r\n}\r\n```\r\n\r\nRunning `perl test.pl` produces 60 lines of output; the first 30 are for CPU, the second 30 are GPU. Plotting these yields:\r\n\r\n![memory use](https://raw.githubusercontent.com/davidmascharka/davidmascharka.com/master/img.png?token=AFWhVkGfeKu__pRyB5eke9RRz_U1G9gCks5b0yXLwA%3D%3D)\r\n\r\nThe numbers produced on my machine are as follows:\r\n\r\n```\r\n# CPU\r\n145.5234375,\r\n145.90234375,\r\n145.609375,\r\n145.43359375,\r\n145.56640625,\r\n145.33984375,\r\n145.51171875,\r\n145.3359375,\r\n146.34375,\r\n149.0078125,\r\n150.75,\r\n153.23046875,\r\n156.47265625,\r\n159.55859375,\r\n162.66796875,\r\n166.2734375,\r\n170.31640625,\r\n173.98046875,\r\n178.40234375,\r\n183.2109375,\r\n187.625,\r\n192.75390625,\r\n197.88671875,\r\n202.8828125,\r\n209.078125,\r\n214.2578125,\r\n220.86328125,\r\n226.41796875,\r\n233.5078125,\r\n239.9375,\r\n\r\n# GPU\r\n1859.98828125,\r\n1859.20703125,\r\n1859.90234375,\r\n1861.25,\r\n1862.359375,\r\n1861.1171875,\r\n1859.54296875,\r\n1858.77734375,\r\n1858.9765625,\r\n1863.28125,\r\n1862.94921875,\r\n1859.296875,\r\n1860.77734375,\r\n1861.5625,\r\n1862.75390625,\r\n1859.83984375,\r\n1859.99609375,\r\n1860.80078125,\r\n1860.09375,\r\n1862.703125,\r\n1858.71875,\r\n1858.75,\r\n1860.671875,\r\n1859.6875,\r\n1859.0234375,\r\n1858.921875,\r\n1859.98046875,\r\n1860.04296875,\r\n1859.015625,\r\n1858.77734375,\r\n```\r\n\r\n## Expected behavior\r\n\r\nMemory usage on the GPU side should not be significantly higher than on the CPU side. Luckily, RAM usage does not grow substantially (as indeed it should not), but the high startup cost is concerning, especially since this is just a 1x1 conv operating on 1-d input.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.0a0+ff608a9\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.87\r\ncuDNN version: 7.0.3\r\n\r\nVersions of relevant libraries:\r\nnumpy (1.15.2)\r\n```\r\n\r\n## Additional Notes\r\nI've observed stranger behavior in the curve on the CPU where for small images the memory consumption grows exponentially up to ~2 GB then drops and grows linearly. I'm attempting to reproduce this behavior in a small, standalone script like the above."}