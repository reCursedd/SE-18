{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1760", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1760/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1760/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1760/events", "html_url": "https://github.com/pytorch/pytorch/issues/1760", "id": 234785272, "node_id": "MDU6SXNzdWUyMzQ3ODUyNzI=", "number": 1760, "title": "Variable does not get gradient", "user": {"login": "greatwall1995", "id": 11371139, "node_id": "MDQ6VXNlcjExMzcxMTM5", "avatar_url": "https://avatars0.githubusercontent.com/u/11371139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/greatwall1995", "html_url": "https://github.com/greatwall1995", "followers_url": "https://api.github.com/users/greatwall1995/followers", "following_url": "https://api.github.com/users/greatwall1995/following{/other_user}", "gists_url": "https://api.github.com/users/greatwall1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/greatwall1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/greatwall1995/subscriptions", "organizations_url": "https://api.github.com/users/greatwall1995/orgs", "repos_url": "https://api.github.com/users/greatwall1995/repos", "events_url": "https://api.github.com/users/greatwall1995/events{/privacy}", "received_events_url": "https://api.github.com/users/greatwall1995/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-09T10:23:46Z", "updated_at": "2017-06-09T14:12:26Z", "closed_at": "2017-06-09T14:12:26Z", "author_association": "NONE", "body_html": "<p>Is there any difference between <code>x</code> and <code>Variable(torch.Tensor(x.data.numpy()), requires_grad=x.requires_grad, volatile=x.volatile)</code> ?</p>\n<p>In my code, I set <code>self.y=x</code> in <code>forward()</code> function. After calling <code>backward()</code> function (without parameter in it), <code>y.grad</code> is <code>None</code>. However, if I set  <code>self.y=Variable(torch.Tensor(x.data.numpy()), requires_grad=x.requires_grad, volatile=x.volatile)</code>, <code>y.grad</code> can be calculated.</p>\n<p>It quite strange to me that the results of these two codes are different.</p>\n<p><strong>UPD:</strong>  I finally condense the code like this:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc0 = nn.Linear(3, 5)\n    def forward(self, x):\n        x = torch.cat(x)\n        self.y = x #Variable(x.data, requires_grad=x.requires_grad, volatile=x.volatile)\n        x = self.fc0(self.y)\n        return x\n\nmodel = Net()\nx = [Variable(torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), requires_grad=True) for i in xrange(3)]\noutput = model(x)\noutput[0][0].backward()\nprint model.y.grad\n</code></pre>\n<p><strong>UPD2:</strong> And I also tried another code where the input of <code>forward()</code> function is a Variable. It still has the same problem.</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc = nn.Linear(3, 5)\n        self.internal = [Variable(torch.Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])) for i in xrange(3)]\n    def forward(self, x):\n        x = torch.cat([torch.mm(x, self.internal[i]) for i in xrange(3)])\n        self.y = x #Variable(x.data, requires_grad=x.requires_grad, volatile=x.volatile)\n        x = self.fc(self.y)\n        return x\n\nmodel = Net()\nx = Variable(torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), requires_grad=True)\noutput = model(x)\noutput[0][0].backward()\nprint model.y.grad\n</code></pre>", "body_text": "Is there any difference between x and Variable(torch.Tensor(x.data.numpy()), requires_grad=x.requires_grad, volatile=x.volatile) ?\nIn my code, I set self.y=x in forward() function. After calling backward() function (without parameter in it), y.grad is None. However, if I set  self.y=Variable(torch.Tensor(x.data.numpy()), requires_grad=x.requires_grad, volatile=x.volatile), y.grad can be calculated.\nIt quite strange to me that the results of these two codes are different.\nUPD:  I finally condense the code like this:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc0 = nn.Linear(3, 5)\n    def forward(self, x):\n        x = torch.cat(x)\n        self.y = x #Variable(x.data, requires_grad=x.requires_grad, volatile=x.volatile)\n        x = self.fc0(self.y)\n        return x\n\nmodel = Net()\nx = [Variable(torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), requires_grad=True) for i in xrange(3)]\noutput = model(x)\noutput[0][0].backward()\nprint model.y.grad\n\nUPD2: And I also tried another code where the input of forward() function is a Variable. It still has the same problem.\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc = nn.Linear(3, 5)\n        self.internal = [Variable(torch.Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])) for i in xrange(3)]\n    def forward(self, x):\n        x = torch.cat([torch.mm(x, self.internal[i]) for i in xrange(3)])\n        self.y = x #Variable(x.data, requires_grad=x.requires_grad, volatile=x.volatile)\n        x = self.fc(self.y)\n        return x\n\nmodel = Net()\nx = Variable(torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), requires_grad=True)\noutput = model(x)\noutput[0][0].backward()\nprint model.y.grad", "body": "Is there any difference between `x` and `Variable(torch.Tensor(x.data.numpy()), requires_grad=x.requires_grad, volatile=x.volatile)` ?\r\n\r\nIn my code, I set `self.y=x` in `forward()` function. After calling `backward()` function (without parameter in it), `y.grad` is `None`. However, if I set  `self.y=Variable(torch.Tensor(x.data.numpy()), requires_grad=x.requires_grad, volatile=x.volatile)`, `y.grad` can be calculated.\r\n\r\nIt quite strange to me that the results of these two codes are different.\r\n\r\n**UPD:**  I finally condense the code like this:\r\n~~~~\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc0 = nn.Linear(3, 5)\r\n    def forward(self, x):\r\n        x = torch.cat(x)\r\n        self.y = x #Variable(x.data, requires_grad=x.requires_grad, volatile=x.volatile)\r\n        x = self.fc0(self.y)\r\n        return x\r\n\r\nmodel = Net()\r\nx = [Variable(torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), requires_grad=True) for i in xrange(3)]\r\noutput = model(x)\r\noutput[0][0].backward()\r\nprint model.y.grad\r\n~~~~\r\n\r\n**UPD2:** And I also tried another code where the input of `forward()` function is a Variable. It still has the same problem.\r\n~~~~\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc = nn.Linear(3, 5)\r\n        self.internal = [Variable(torch.Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])) for i in xrange(3)]\r\n    def forward(self, x):\r\n        x = torch.cat([torch.mm(x, self.internal[i]) for i in xrange(3)])\r\n        self.y = x #Variable(x.data, requires_grad=x.requires_grad, volatile=x.volatile)\r\n        x = self.fc(self.y)\r\n        return x\r\n\r\nmodel = Net()\r\nx = Variable(torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), requires_grad=True)\r\noutput = model(x)\r\noutput[0][0].backward()\r\nprint model.y.grad\r\n~~~~"}