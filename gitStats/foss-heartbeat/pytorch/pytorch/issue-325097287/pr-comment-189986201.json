{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189986201", "pull_request_review_id": 122271520, "id": 189986201, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTk4NjIwMQ==", "diff_hunk": "@@ -149,6 +159,32 @@ static std::vector<Value*> gradientForNode(Node* node, ArrayRef<Value*> grad_val\n           return returned_grads;\n         }\n       }\n+      case aten::addmm: {\n+        JIT_ASSERT(inputs.size() == 3);\n+        JIT_ASSERT(outputs.size() == 1);\n+        JIT_ASSERT(grads.size() == 1);\n+\n+        // First take the grads from the matmul\n+        auto returned_grads = mm_grads({inputs[1], inputs[2]}, grads);\n+\n+        // Scale grads if applicable\n+        auto alpha = at::Scalar(node->t(attr::alpha));\n+        if (alpha.to<double>() != 1.0) {\n+          for (size_t i=0; i < returned_grads.size(); ++i) {\n+            returned_grads[i] = returned_grads[i] * alpha;\n+          }\n+        }\n+\n+        // Grad from the addition\n+        auto beta = at::Scalar(node->t(attr::beta));\n+        auto it = returned_grads.begin();\n+        if (beta.to<double>() != 1.0)\n+          returned_grads.insert(it, grads[0] * beta);\n+        else\n+          returned_grads.insert(it, grads[0]);", "path": "torch/csrc/jit/autodiff.cpp", "position": 104, "original_position": 104, "commit_id": "61a34571fbd5134a1fc71c532e43ef19e7e01921", "original_commit_id": "61a34571fbd5134a1fc71c532e43ef19e7e01921", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "body": "Yeah, that's the issue we ran into and is the reason why we're going to do fission pass", "created_at": "2018-05-22T17:24:55Z", "updated_at": "2018-11-23T15:44:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/7748#discussion_r189986201", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7748", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189986201"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7748#discussion_r189986201"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7748"}}, "body_html": "<p>Yeah, that's the issue we ran into and is the reason why we're going to do fission pass</p>", "body_text": "Yeah, that's the issue we ran into and is the reason why we're going to do fission pass", "in_reply_to_id": 189828340}