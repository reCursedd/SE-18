{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5902", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5902/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5902/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5902/events", "html_url": "https://github.com/pytorch/pytorch/issues/5902", "id": 306839767, "node_id": "MDU6SXNzdWUzMDY4Mzk3Njc=", "number": 5902, "title": "DataLoader uses 3x more memory when num_workers > 0", "user": {"login": "jseppanen", "id": 83203, "node_id": "MDQ6VXNlcjgzMjAz", "avatar_url": "https://avatars0.githubusercontent.com/u/83203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseppanen", "html_url": "https://github.com/jseppanen", "followers_url": "https://api.github.com/users/jseppanen/followers", "following_url": "https://api.github.com/users/jseppanen/following{/other_user}", "gists_url": "https://api.github.com/users/jseppanen/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseppanen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseppanen/subscriptions", "organizations_url": "https://api.github.com/users/jseppanen/orgs", "repos_url": "https://api.github.com/users/jseppanen/repos", "events_url": "https://api.github.com/users/jseppanen/events{/privacy}", "received_events_url": "https://api.github.com/users/jseppanen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-03-20T12:12:36Z", "updated_at": "2018-10-28T15:10:29Z", "closed_at": "2018-03-20T13:57:20Z", "author_association": "NONE", "body_html": "<p>DataLoader causes the main memory usage to slowly increase from 5GB to 17GB, over 30 minutes of running, when <code>num_workers</code> is nonzero. The resident memory size of the python process itself does not grow but stays at 5GB flat. The program is not using CUDA.</p>\n<p>When run with <code>num_workers=0</code>, the main memory usage stays at 5GB flat. This is what I would expect also when <code>num_workers &gt; 0</code>. My issue is that when running with real data sets that use 50GB of memory, DataLoader with <code>num_workers &gt; 0</code> will cause OOM after running for a while, even with 60GB of total memory. On the other hand, <code>num_workers=0</code> limits my GPU utilization and slows down training.</p>\n<p>Minimal repro case:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport torch\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nnp.__version__\n#'1.13.3'\n\npd.__version__\n#'0.22.0'\n\ntorch.__version__\n#'0.3.1.post2'\n\n\nclass ParquetData(Dataset):\n    def __init__(self, path):\n        super().__init__()\n        self.rawdata = pd.read_parquet(path)\n        print(f'loaded {len(self)} examples from {path}')\n\n    def __len__(self):\n        return len(self.rawdata)\n\n    def __getitem__(self, i):\n        row = self.rawdata.iloc[i]\n        image = np.zeros((48, 64, 36), dtype=np.float32)\n        image[row['cc'], row['ii'], row['jj']] = row['vv']\n        input = {\n            'image': image\n        }\n        target = {\n            'label': 0\n        }\n        return input, target\n\n\ndataset = ParquetData('dummydata.parquet')\nloader = DataLoader(dataset,\n                    batch_size=32,\n                    shuffle=True,\n                    num_workers=4)  # change to 0 to prevent leak\n\nfor input, target in loader:\n    pass\n</code></pre>\n<p>The <code>dummydata.parquet</code> file (68MB) can be loaded from <a href=\"https://www.dropbox.com/s/1mspwpx16j1aurf/dummydata.parquet?dl=0\" rel=\"nofollow\">https://www.dropbox.com/s/1mspwpx16j1aurf/dummydata.parquet?dl=0</a></p>\n<p>Here's a log of the memory usage growth during 30 minutes, when running the above repro script. I started logging only after the data set had been loaded:</p>\n<pre><code>$ while true; do free; sleep 60; done\n              total        used        free      shared  buff/cache   available\nMem:       62880028     5909424    55600892       95508     1369712    56381080\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028     9803196    51699124      102792     1377708    52479628\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    12532152    48970912      102412     1376964    49751232\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    14393964    47095452      116004     1390612    47875808\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    15684068    45834900       86608     1361060    46615408\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    16577944    44929360       97812     1372724    45709892\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    17185068    44293464      126392     1401496    45074108\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    17605992    43916200       82792     1357836    44696824\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    17886180    43603728      115176     1390120    44384304\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18075168    43441828       87860     1363032    44222480\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18203868    43312948       88000     1363212    44093624\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18291180    43187060      126904     1401788    43967616\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18349496    43174860       81500     1355672    43955264\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18388340    43130920       86184     1360768    43911324\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18411444    43068368      125224     1400216    43848988\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18431264    43079488       94880     1369276    43859812\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18441160    43087336       77100     1351532    43867704\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18446668    43063836       94820     1369524    43843952\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18452324    43061728       91412     1365976    43842168\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18454720    43039724      110916     1385584    43820204\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18457852    43060536       86364     1361640    43841332\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18459276    43048000       98056     1372752    43828520\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18459216    43054892       91644     1365920    43835444\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460244    43053256       91712     1366528    43833848\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460924    43019588      124620     1399516    43800204\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18459568    43034724      105704     1385736    43820352\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460844    43050296       88732     1368888    43836000\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460092    43033144      106824     1386792    43818756\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18462872    43031916      105028     1385240    43817680\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18462068    43035420      102272     1382540    43821196\nSwap:             0           0           0\n</code></pre>\n<p>When submitting a bug report, please include the following information (where relevant):</p>\n<ul>\n<li>OS: Ubuntu 16.04.4 LTS</li>\n<li>PyTorch version: 0.3.1.post2</li>\n<li>How you installed PyTorch (conda, pip, source): conda (<code>py36_cuda9.0.176_cudnn7.0.5_2</code>)</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: cuda 9.0 cudnn 7.0.5</li>\n<li>GPU models and configuration: 1x Tesla V100 (Amazon ec2 p3.2xlarge instance)</li>\n</ul>\n<p>FWIW, the dummy data set was generated in PySpark:</p>\n<pre><code>df = spark.range(10000000).selectExpr(\n    'ARRAY(id % 48) AS cc', 'ARRAY(id % 64) AS ii',\n    'ARRAY(id % 36) AS jj', 'ARRAY(LOG(1 + id)) AS vv')\n\ndf.repartition(1).write.parquet('s3://my-bucket/dummydata')\n</code></pre>\n<p>Jarno</p>", "body_text": "DataLoader causes the main memory usage to slowly increase from 5GB to 17GB, over 30 minutes of running, when num_workers is nonzero. The resident memory size of the python process itself does not grow but stays at 5GB flat. The program is not using CUDA.\nWhen run with num_workers=0, the main memory usage stays at 5GB flat. This is what I would expect also when num_workers > 0. My issue is that when running with real data sets that use 50GB of memory, DataLoader with num_workers > 0 will cause OOM after running for a while, even with 60GB of total memory. On the other hand, num_workers=0 limits my GPU utilization and slows down training.\nMinimal repro case:\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nnp.__version__\n#'1.13.3'\n\npd.__version__\n#'0.22.0'\n\ntorch.__version__\n#'0.3.1.post2'\n\n\nclass ParquetData(Dataset):\n    def __init__(self, path):\n        super().__init__()\n        self.rawdata = pd.read_parquet(path)\n        print(f'loaded {len(self)} examples from {path}')\n\n    def __len__(self):\n        return len(self.rawdata)\n\n    def __getitem__(self, i):\n        row = self.rawdata.iloc[i]\n        image = np.zeros((48, 64, 36), dtype=np.float32)\n        image[row['cc'], row['ii'], row['jj']] = row['vv']\n        input = {\n            'image': image\n        }\n        target = {\n            'label': 0\n        }\n        return input, target\n\n\ndataset = ParquetData('dummydata.parquet')\nloader = DataLoader(dataset,\n                    batch_size=32,\n                    shuffle=True,\n                    num_workers=4)  # change to 0 to prevent leak\n\nfor input, target in loader:\n    pass\n\nThe dummydata.parquet file (68MB) can be loaded from https://www.dropbox.com/s/1mspwpx16j1aurf/dummydata.parquet?dl=0\nHere's a log of the memory usage growth during 30 minutes, when running the above repro script. I started logging only after the data set had been loaded:\n$ while true; do free; sleep 60; done\n              total        used        free      shared  buff/cache   available\nMem:       62880028     5909424    55600892       95508     1369712    56381080\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028     9803196    51699124      102792     1377708    52479628\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    12532152    48970912      102412     1376964    49751232\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    14393964    47095452      116004     1390612    47875808\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    15684068    45834900       86608     1361060    46615408\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    16577944    44929360       97812     1372724    45709892\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    17185068    44293464      126392     1401496    45074108\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    17605992    43916200       82792     1357836    44696824\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    17886180    43603728      115176     1390120    44384304\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18075168    43441828       87860     1363032    44222480\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18203868    43312948       88000     1363212    44093624\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18291180    43187060      126904     1401788    43967616\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18349496    43174860       81500     1355672    43955264\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18388340    43130920       86184     1360768    43911324\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18411444    43068368      125224     1400216    43848988\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18431264    43079488       94880     1369276    43859812\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18441160    43087336       77100     1351532    43867704\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18446668    43063836       94820     1369524    43843952\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18452324    43061728       91412     1365976    43842168\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18454720    43039724      110916     1385584    43820204\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18457852    43060536       86364     1361640    43841332\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18459276    43048000       98056     1372752    43828520\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18459216    43054892       91644     1365920    43835444\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460244    43053256       91712     1366528    43833848\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460924    43019588      124620     1399516    43800204\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18459568    43034724      105704     1385736    43820352\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460844    43050296       88732     1368888    43836000\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18460092    43033144      106824     1386792    43818756\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18462872    43031916      105028     1385240    43817680\nSwap:             0           0           0\n              total        used        free      shared  buff/cache   available\nMem:       62880028    18462068    43035420      102272     1382540    43821196\nSwap:             0           0           0\n\nWhen submitting a bug report, please include the following information (where relevant):\n\nOS: Ubuntu 16.04.4 LTS\nPyTorch version: 0.3.1.post2\nHow you installed PyTorch (conda, pip, source): conda (py36_cuda9.0.176_cudnn7.0.5_2)\nPython version: 3.6.4\nCUDA/cuDNN version: cuda 9.0 cudnn 7.0.5\nGPU models and configuration: 1x Tesla V100 (Amazon ec2 p3.2xlarge instance)\n\nFWIW, the dummy data set was generated in PySpark:\ndf = spark.range(10000000).selectExpr(\n    'ARRAY(id % 48) AS cc', 'ARRAY(id % 64) AS ii',\n    'ARRAY(id % 36) AS jj', 'ARRAY(LOG(1 + id)) AS vv')\n\ndf.repartition(1).write.parquet('s3://my-bucket/dummydata')\n\nJarno", "body": "DataLoader causes the main memory usage to slowly increase from 5GB to 17GB, over 30 minutes of running, when `num_workers` is nonzero. The resident memory size of the python process itself does not grow but stays at 5GB flat. The program is not using CUDA.\r\n\r\nWhen run with `num_workers=0`, the main memory usage stays at 5GB flat. This is what I would expect also when `num_workers > 0`. My issue is that when running with real data sets that use 50GB of memory, DataLoader with `num_workers > 0` will cause OOM after running for a while, even with 60GB of total memory. On the other hand, `num_workers=0` limits my GPU utilization and slows down training.\r\n\r\nMinimal repro case:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\n\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import Dataset\r\n\r\nnp.__version__\r\n#'1.13.3'\r\n\r\npd.__version__\r\n#'0.22.0'\r\n\r\ntorch.__version__\r\n#'0.3.1.post2'\r\n\r\n\r\nclass ParquetData(Dataset):\r\n    def __init__(self, path):\r\n        super().__init__()\r\n        self.rawdata = pd.read_parquet(path)\r\n        print(f'loaded {len(self)} examples from {path}')\r\n\r\n    def __len__(self):\r\n        return len(self.rawdata)\r\n\r\n    def __getitem__(self, i):\r\n        row = self.rawdata.iloc[i]\r\n        image = np.zeros((48, 64, 36), dtype=np.float32)\r\n        image[row['cc'], row['ii'], row['jj']] = row['vv']\r\n        input = {\r\n            'image': image\r\n        }\r\n        target = {\r\n            'label': 0\r\n        }\r\n        return input, target\r\n\r\n\r\ndataset = ParquetData('dummydata.parquet')\r\nloader = DataLoader(dataset,\r\n                    batch_size=32,\r\n                    shuffle=True,\r\n                    num_workers=4)  # change to 0 to prevent leak\r\n\r\nfor input, target in loader:\r\n    pass\r\n```\r\n\r\nThe `dummydata.parquet` file (68MB) can be loaded from https://www.dropbox.com/s/1mspwpx16j1aurf/dummydata.parquet?dl=0\r\n\r\nHere's a log of the memory usage growth during 30 minutes, when running the above repro script. I started logging only after the data set had been loaded:\r\n```\r\n$ while true; do free; sleep 60; done\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028     5909424    55600892       95508     1369712    56381080\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028     9803196    51699124      102792     1377708    52479628\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    12532152    48970912      102412     1376964    49751232\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    14393964    47095452      116004     1390612    47875808\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    15684068    45834900       86608     1361060    46615408\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    16577944    44929360       97812     1372724    45709892\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    17185068    44293464      126392     1401496    45074108\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    17605992    43916200       82792     1357836    44696824\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    17886180    43603728      115176     1390120    44384304\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18075168    43441828       87860     1363032    44222480\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18203868    43312948       88000     1363212    44093624\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18291180    43187060      126904     1401788    43967616\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18349496    43174860       81500     1355672    43955264\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18388340    43130920       86184     1360768    43911324\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18411444    43068368      125224     1400216    43848988\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18431264    43079488       94880     1369276    43859812\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18441160    43087336       77100     1351532    43867704\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18446668    43063836       94820     1369524    43843952\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18452324    43061728       91412     1365976    43842168\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18454720    43039724      110916     1385584    43820204\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18457852    43060536       86364     1361640    43841332\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18459276    43048000       98056     1372752    43828520\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18459216    43054892       91644     1365920    43835444\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18460244    43053256       91712     1366528    43833848\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18460924    43019588      124620     1399516    43800204\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18459568    43034724      105704     1385736    43820352\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18460844    43050296       88732     1368888    43836000\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18460092    43033144      106824     1386792    43818756\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18462872    43031916      105028     1385240    43817680\r\nSwap:             0           0           0\r\n              total        used        free      shared  buff/cache   available\r\nMem:       62880028    18462068    43035420      102272     1382540    43821196\r\nSwap:             0           0           0\r\n```\r\n\r\nWhen submitting a bug report, please include the following information (where relevant):\r\n- OS: Ubuntu 16.04.4 LTS\r\n- PyTorch version: 0.3.1.post2\r\n- How you installed PyTorch (conda, pip, source): conda (`py36_cuda9.0.176_cudnn7.0.5_2`)\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: cuda 9.0 cudnn 7.0.5\r\n- GPU models and configuration: 1x Tesla V100 (Amazon ec2 p3.2xlarge instance)\r\n\r\nFWIW, the dummy data set was generated in PySpark:\r\n\r\n```\r\ndf = spark.range(10000000).selectExpr(\r\n    'ARRAY(id % 48) AS cc', 'ARRAY(id % 64) AS ii',\r\n    'ARRAY(id % 36) AS jj', 'ARRAY(LOG(1 + id)) AS vv')\r\n\r\ndf.repartition(1).write.parquet('s3://my-bucket/dummydata')\r\n```\r\n\r\nJarno\r\n"}