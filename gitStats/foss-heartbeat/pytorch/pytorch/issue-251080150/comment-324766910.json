{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/324766910", "html_url": "https://github.com/pytorch/pytorch/issues/2478#issuecomment-324766910", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2478", "id": 324766910, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDc2NjkxMA==", "user": {"login": "Jiaming-Liu", "id": 16099575, "node_id": "MDQ6VXNlcjE2MDk5NTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/16099575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jiaming-Liu", "html_url": "https://github.com/Jiaming-Liu", "followers_url": "https://api.github.com/users/Jiaming-Liu/followers", "following_url": "https://api.github.com/users/Jiaming-Liu/following{/other_user}", "gists_url": "https://api.github.com/users/Jiaming-Liu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jiaming-Liu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jiaming-Liu/subscriptions", "organizations_url": "https://api.github.com/users/Jiaming-Liu/orgs", "repos_url": "https://api.github.com/users/Jiaming-Liu/repos", "events_url": "https://api.github.com/users/Jiaming-Liu/events{/privacy}", "received_events_url": "https://api.github.com/users/Jiaming-Liu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T21:48:49Z", "updated_at": "2017-08-24T22:20:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=31113375\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Taha-Bahadori\">@Taha-Bahadori</a> I think this snippet would be closer to the real use-case. Note that in-place tensor operation is used in <code>optimizer.step()</code>. That is, <code>m.mem.data = torch.ones(1)</code> in your snippet should be something like <code>m.mem.data.copy_(torch.ones(1))</code> instead.</p>\n<p>As well, your <code>ReduceLROnPlateauBT</code> ignores the state_dict of <code>optimizer</code>, which contains some important history info (like momentum for SGD).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nnet <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>)\noptim <span class=\"pl-k\">=</span> torch.optim.Adam(net.parameters())\nstate_dict <span class=\"pl-k\">=</span> net.state_dict()\n<span class=\"pl-c1\">print</span>(state_dict)\n\nx <span class=\"pl-k\">=</span> torch.FloatTensor([[<span class=\"pl-c1\">1</span>],[<span class=\"pl-c1\">2</span>]])\nx <span class=\"pl-k\">=</span> torch.autograd.Variable(x)\ny <span class=\"pl-k\">=</span> torch.FloatTensor([[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>],[<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>]])\ny <span class=\"pl-k\">=</span> torch.autograd.Variable(y)\nloss <span class=\"pl-k\">=</span> torch.nn.functional.mse_loss(net(x),y)\nloss.backward()\noptim.step() <span class=\"pl-c\"><span class=\"pl-c\">#</span> Changing the value of the parameter</span>\n\nnet.load_state_dict(state_dict)\n<span class=\"pl-c1\">print</span>(net.state_dict())</pre></div>", "body_text": "@Taha-Bahadori I think this snippet would be closer to the real use-case. Note that in-place tensor operation is used in optimizer.step(). That is, m.mem.data = torch.ones(1) in your snippet should be something like m.mem.data.copy_(torch.ones(1)) instead.\nAs well, your ReduceLROnPlateauBT ignores the state_dict of optimizer, which contains some important history info (like momentum for SGD).\nimport torch\nnet = torch.nn.Linear(1,2)\noptim = torch.optim.Adam(net.parameters())\nstate_dict = net.state_dict()\nprint(state_dict)\n\nx = torch.FloatTensor([[1],[2]])\nx = torch.autograd.Variable(x)\ny = torch.FloatTensor([[0,1],[2,3]])\ny = torch.autograd.Variable(y)\nloss = torch.nn.functional.mse_loss(net(x),y)\nloss.backward()\noptim.step() # Changing the value of the parameter\n\nnet.load_state_dict(state_dict)\nprint(net.state_dict())", "body": "@Taha-Bahadori I think this snippet would be closer to the real use-case. Note that in-place tensor operation is used in `optimizer.step()`. That is, `m.mem.data = torch.ones(1)` in your snippet should be something like `m.mem.data.copy_(torch.ones(1))` instead.\r\n\r\nAs well, your `ReduceLROnPlateauBT` ignores the state_dict of `optimizer`, which contains some important history info (like momentum for SGD).\r\n``` Python\r\nimport torch\r\nnet = torch.nn.Linear(1,2)\r\noptim = torch.optim.Adam(net.parameters())\r\nstate_dict = net.state_dict()\r\nprint(state_dict)\r\n\r\nx = torch.FloatTensor([[1],[2]])\r\nx = torch.autograd.Variable(x)\r\ny = torch.FloatTensor([[0,1],[2,3]])\r\ny = torch.autograd.Variable(y)\r\nloss = torch.nn.functional.mse_loss(net(x),y)\r\nloss.backward()\r\noptim.step() # Changing the value of the parameter\r\n\r\nnet.load_state_dict(state_dict)\r\nprint(net.state_dict())\r\n```"}