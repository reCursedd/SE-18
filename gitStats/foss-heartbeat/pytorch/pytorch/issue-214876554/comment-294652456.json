{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/294652456", "html_url": "https://github.com/pytorch/pytorch/issues/1020#issuecomment-294652456", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1020", "id": 294652456, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDY1MjQ1Ng==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-18T02:15:17Z", "updated_at": "2017-04-18T02:15:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If you want to wrap <code>nn.Softmax</code> with additional logic to make it work for different dimensions, you don't have to do this in a <code>Function</code> (in fact it's not a good idea to call <code>Function</code>s inside other <code>Function</code>s). You should just do it in a <code>Module</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MySoftmax</span>(<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_</span>):\n        batch_size <span class=\"pl-k\">=</span> input_.size()[<span class=\"pl-c1\">0</span>]\n        output_ <span class=\"pl-k\">=</span> torch.stack([F.softmax(input_[i]) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_size)], <span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">return</span> output_</pre></div>", "body_text": "If you want to wrap nn.Softmax with additional logic to make it work for different dimensions, you don't have to do this in a Function (in fact it's not a good idea to call Functions inside other Functions). You should just do it in a Module:\nclass MySoftmax(Module):\n\n    def forward(self, input_):\n        batch_size = input_.size()[0]\n        output_ = torch.stack([F.softmax(input_[i]) for i in range(batch_size)], 0)\n        return output_", "body": "If you want to wrap `nn.Softmax` with additional logic to make it work for different dimensions, you don't have to do this in a `Function` (in fact it's not a good idea to call `Function`s inside other `Function`s). You should just do it in a `Module`:\r\n```python\r\nclass MySoftmax(Module):\r\n\r\n    def forward(self, input_):\r\n        batch_size = input_.size()[0]\r\n        output_ = torch.stack([F.softmax(input_[i]) for i in range(batch_size)], 0)\r\n        return output_\r\n```"}