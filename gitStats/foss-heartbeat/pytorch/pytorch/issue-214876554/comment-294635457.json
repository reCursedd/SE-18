{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/294635457", "html_url": "https://github.com/pytorch/pytorch/issues/1020#issuecomment-294635457", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1020", "id": 294635457, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDYzNTQ1Nw==", "user": {"login": "pavanramkumar", "id": 3664715, "node_id": "MDQ6VXNlcjM2NjQ3MTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3664715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pavanramkumar", "html_url": "https://github.com/pavanramkumar", "followers_url": "https://api.github.com/users/pavanramkumar/followers", "following_url": "https://api.github.com/users/pavanramkumar/following{/other_user}", "gists_url": "https://api.github.com/users/pavanramkumar/gists{/gist_id}", "starred_url": "https://api.github.com/users/pavanramkumar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pavanramkumar/subscriptions", "organizations_url": "https://api.github.com/users/pavanramkumar/orgs", "repos_url": "https://api.github.com/users/pavanramkumar/repos", "events_url": "https://api.github.com/users/pavanramkumar/events{/privacy}", "received_events_url": "https://api.github.com/users/pavanramkumar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-18T00:29:01Z", "updated_at": "2017-04-18T00:36:55Z", "author_association": "NONE", "body_html": "<p>I'm writing a <code>Function</code> for this, but am running into errors where neither <code>Variable</code> nor <code>FloatTensor</code> are acceptable inputs to <code>torch.nn.Softmax()</code>. <code>input_</code> is <code>batch_size</code> x <code>n_symbols</code> x <code>sequence_length</code> and I want to apply row-wise softmax on each sample.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MySoftmax</span>(<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_</span>):\n        <span class=\"pl-c1\">self</span>.save_for_backward(input_)\n        batch_size <span class=\"pl-k\">=</span> input_.size()[<span class=\"pl-c1\">0</span>]\n        output_ <span class=\"pl-k\">=</span> Variable(torch.zeros(batch_size))\n        <span class=\"pl-k\">for</span> sample <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_size):\n            output_[sample, :, :] <span class=\"pl-k\">=</span> torch.nn.Softmax()(input_[sample, :, :])\n        <span class=\"pl-k\">return</span> output_\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        input_ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.saved_tensors\n        grad_input_ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        <span class=\"pl-k\">return</span> grad_input</pre></div>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-270-18016cd1d581&gt; in &lt;module&gt;()\n----&gt; 1 output_ = MySoftmax()(input_)\n\n&lt;ipython-input-268-df82429e7b17&gt; in forward(self, input_)\n      6         output_ = Variable(torch.zeros(batch_size))\n      7         for sample in range(batch_size):\n----&gt; 8             output_[sample, :, :] = torch.nn.Softmax()(input_[sample, :, :])\n      9         return output_\n     10 \n\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    204 \n    205     def __call__(self, *input, **kwargs):\n--&gt; 206         result = self.forward(*input, **kwargs)\n    207         for hook in self._forward_hooks.values():\n    208             hook_result = hook(self, input, result)\n\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/modules/activation.pyc in forward(self, input)\n    557     def forward(self, input):\n    558         assert input.dim() == 2, 'Softmax requires a 2D tensor as input'\n--&gt; 559         return F.softmax(input)\n    560 \n    561     def __repr__(self):\n\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc in softmax(input)\n    408 \n    409 def softmax(input):\n--&gt; 410     return _functions.thnn.auto.Softmax()(input)\n    411 \n    412 \n\nRuntimeError: expected a Variable argument, but got FloatTensor\n</code></pre>\n<p>I've looked at looping logic but couldn't find something obvious. What am I missing? Thanks!</p>", "body_text": "I'm writing a Function for this, but am running into errors where neither Variable nor FloatTensor are acceptable inputs to torch.nn.Softmax(). input_ is batch_size x n_symbols x sequence_length and I want to apply row-wise softmax on each sample.\nclass MySoftmax(Function):\n\n    def forward(self, input_):\n        self.save_for_backward(input_)\n        batch_size = input_.size()[0]\n        output_ = Variable(torch.zeros(batch_size))\n        for sample in range(batch_size):\n            output_[sample, :, :] = torch.nn.Softmax()(input_[sample, :, :])\n        return output_\n    \n    def backward(self, grad_output):\n        input_ = self.saved_tensors\n        grad_input_ = None\n        return grad_input\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-270-18016cd1d581> in <module>()\n----> 1 output_ = MySoftmax()(input_)\n\n<ipython-input-268-df82429e7b17> in forward(self, input_)\n      6         output_ = Variable(torch.zeros(batch_size))\n      7         for sample in range(batch_size):\n----> 8             output_[sample, :, :] = torch.nn.Softmax()(input_[sample, :, :])\n      9         return output_\n     10 \n\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    204 \n    205     def __call__(self, *input, **kwargs):\n--> 206         result = self.forward(*input, **kwargs)\n    207         for hook in self._forward_hooks.values():\n    208             hook_result = hook(self, input, result)\n\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/modules/activation.pyc in forward(self, input)\n    557     def forward(self, input):\n    558         assert input.dim() == 2, 'Softmax requires a 2D tensor as input'\n--> 559         return F.softmax(input)\n    560 \n    561     def __repr__(self):\n\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc in softmax(input)\n    408 \n    409 def softmax(input):\n--> 410     return _functions.thnn.auto.Softmax()(input)\n    411 \n    412 \n\nRuntimeError: expected a Variable argument, but got FloatTensor\n\nI've looked at looping logic but couldn't find something obvious. What am I missing? Thanks!", "body": "I'm writing a `Function` for this, but am running into errors where neither `Variable` nor `FloatTensor` are acceptable inputs to `torch.nn.Softmax()`. `input_` is `batch_size` x `n_symbols` x `sequence_length` and I want to apply row-wise softmax on each sample.\r\n\r\n```python\r\nclass MySoftmax(Function):\r\n\r\n    def forward(self, input_):\r\n        self.save_for_backward(input_)\r\n        batch_size = input_.size()[0]\r\n        output_ = Variable(torch.zeros(batch_size))\r\n        for sample in range(batch_size):\r\n            output_[sample, :, :] = torch.nn.Softmax()(input_[sample, :, :])\r\n        return output_\r\n    \r\n    def backward(self, grad_output):\r\n        input_ = self.saved_tensors\r\n        grad_input_ = None\r\n        return grad_input\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-270-18016cd1d581> in <module>()\r\n----> 1 output_ = MySoftmax()(input_)\r\n\r\n<ipython-input-268-df82429e7b17> in forward(self, input_)\r\n      6         output_ = Variable(torch.zeros(batch_size))\r\n      7         for sample in range(batch_size):\r\n----> 8             output_[sample, :, :] = torch.nn.Softmax()(input_[sample, :, :])\r\n      9         return output_\r\n     10 \r\n\r\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    204 \r\n    205     def __call__(self, *input, **kwargs):\r\n--> 206         result = self.forward(*input, **kwargs)\r\n    207         for hook in self._forward_hooks.values():\r\n    208             hook_result = hook(self, input, result)\r\n\r\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/modules/activation.pyc in forward(self, input)\r\n    557     def forward(self, input):\r\n    558         assert input.dim() == 2, 'Softmax requires a 2D tensor as input'\r\n--> 559         return F.softmax(input)\r\n    560 \r\n    561     def __repr__(self):\r\n\r\n/Users/pavanramkumar/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc in softmax(input)\r\n    408 \r\n    409 def softmax(input):\r\n--> 410     return _functions.thnn.auto.Softmax()(input)\r\n    411 \r\n    412 \r\n\r\nRuntimeError: expected a Variable argument, but got FloatTensor\r\n```\r\n\r\nI've looked at looping logic but couldn't find something obvious. What am I missing? Thanks!"}