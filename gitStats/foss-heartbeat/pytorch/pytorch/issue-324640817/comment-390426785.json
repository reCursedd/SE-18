{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390426785", "html_url": "https://github.com/pytorch/pytorch/issues/7705#issuecomment-390426785", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7705", "id": 390426785, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDQyNjc4NQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-19T19:22:26Z", "updated_at": "2018-05-19T19:22:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for reporting. I know the problem. The <code>MultivariateNormal</code> uses a lazily calculated property <code>scale_tril</code> to compute everything, including the samples, log prob, etc. The <code>sample()</code> func in distributions is just <code>rsample()</code> wrapped in <code>torch.no_grad</code> (see <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/distributions/distribution.py#L96-L97\">here</a>). So when you do <code>distr.sample()</code>, it first calculates <code>scale_tril</code> <strong>in a <code>no_grad</code> block</strong>, and subsequent computations, which all use <code>scale_tril</code>, lose history.</p>\n<p>Therefore, if you do</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.distributions <span class=\"pl-k\">import</span> MultivariateNormal\n\ntorch.manual_seed(<span class=\"pl-c1\">124</span>)\ninp <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>)\nW <span class=\"pl-k\">=</span> torch.randn((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nloc <span class=\"pl-k\">=</span> W<span class=\"pl-k\">@</span>inp\ncov <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">0</span>.], [<span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">2</span>.]], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ndistr <span class=\"pl-k\">=</span> MultivariateNormal(loc, cov)\ndistr.scale_tril  <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;------ force it to calculate scale_tril here</span>\n\nx <span class=\"pl-k\">=</span> distr.sample()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> difference here</span>\nloss <span class=\"pl-k\">=</span> distr.log_prob(x)\nloss.backward()\nW.grad, cov.grad</pre></div>\n<p>It will give correct gradients.</p>\n<p>Fix incoming.</p>", "body_text": "Thanks for reporting. I know the problem. The MultivariateNormal uses a lazily calculated property scale_tril to compute everything, including the samples, log prob, etc. The sample() func in distributions is just rsample() wrapped in torch.no_grad (see here). So when you do distr.sample(), it first calculates scale_tril in a no_grad block, and subsequent computations, which all use scale_tril, lose history.\nTherefore, if you do\nimport torch\nfrom torch.distributions import MultivariateNormal\n\ntorch.manual_seed(124)\ninp = torch.randn(3)\nW = torch.randn((2, 3), requires_grad=True)\nloc = W@inp\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\ndistr = MultivariateNormal(loc, cov)\ndistr.scale_tril  # <------ force it to calculate scale_tril here\n\nx = distr.sample()  # difference here\nloss = distr.log_prob(x)\nloss.backward()\nW.grad, cov.grad\nIt will give correct gradients.\nFix incoming.", "body": "Thanks for reporting. I know the problem. The `MultivariateNormal` uses a lazily calculated property `scale_tril` to compute everything, including the samples, log prob, etc. The `sample()` func in distributions is just `rsample()` wrapped in `torch.no_grad` (see [here](https://github.com/pytorch/pytorch/blob/master/torch/distributions/distribution.py#L96-L97)). So when you do `distr.sample()`, it first calculates `scale_tril` **in a `no_grad` block**, and subsequent computations, which all use `scale_tril`, lose history.\r\n\r\nTherefore, if you do \r\n```py\r\nimport torch\r\nfrom torch.distributions import MultivariateNormal\r\n\r\ntorch.manual_seed(124)\r\ninp = torch.randn(3)\r\nW = torch.randn((2, 3), requires_grad=True)\r\nloc = W@inp\r\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\r\ndistr = MultivariateNormal(loc, cov)\r\ndistr.scale_tril  # <------ force it to calculate scale_tril here\r\n\r\nx = distr.sample()  # difference here\r\nloss = distr.log_prob(x)\r\nloss.backward()\r\nW.grad, cov.grad\r\n```\r\n\r\nIt will give correct gradients. \r\n\r\nFix incoming."}