{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7705", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7705/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7705/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7705/events", "html_url": "https://github.com/pytorch/pytorch/issues/7705", "id": 324640817, "node_id": "MDU6SXNzdWUzMjQ2NDA4MTc=", "number": 7705, "title": "[distributions] rsample().detach() and sample() yields different gradients", "user": {"login": "nikishin-evg", "id": 14283069, "node_id": "MDQ6VXNlcjE0MjgzMDY5", "avatar_url": "https://avatars3.githubusercontent.com/u/14283069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikishin-evg", "html_url": "https://github.com/nikishin-evg", "followers_url": "https://api.github.com/users/nikishin-evg/followers", "following_url": "https://api.github.com/users/nikishin-evg/following{/other_user}", "gists_url": "https://api.github.com/users/nikishin-evg/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikishin-evg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikishin-evg/subscriptions", "organizations_url": "https://api.github.com/users/nikishin-evg/orgs", "repos_url": "https://api.github.com/users/nikishin-evg/repos", "events_url": "https://api.github.com/users/nikishin-evg/events{/privacy}", "received_events_url": "https://api.github.com/users/nikishin-evg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-19T16:13:30Z", "updated_at": "2018-05-24T15:22:40Z", "closed_at": "2018-05-24T15:22:40Z", "author_association": "NONE", "body_html": "<p>I have a MultivariateNormal distribution with loc defined as output of neural net (given input) and diagonal covariance matrix with trainable parameters (but does not depending on some input).<br>\nIf I sample via <code>distr.rsample().detach()</code> and optimise sum of log_probs, <code>.backward()</code> provides correct gradients w.r.t. both loc and cov matrix params. But if I sample via <code>distr.sample()</code>, <code>.backward()</code> sets <code>None</code> gradients for cov matrix params.</p>\n<p>Here is a minimal reproducing code:</p>\n<pre><code>import torch\nfrom torch.distributions import MultivariateNormal\n\ntorch.manual_seed(124)\ninp = torch.randn(3)\nW = torch.randn((2, 3), requires_grad=True)\nloc = W@inp\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\ndistr = MultivariateNormal(loc, cov)\n\ny = distr.rsample().detach()  # difference here\nloss = distr.log_prob(y)\nloss.backward()\nW.grad, cov.grad\n</code></pre>\n<p>yields</p>\n<pre><code>(tensor([[-0.1991, -1.0776, -0.6339],\n         [ 0.3455,  1.8700,  1.1000]]), \n tensor([[-0.2678,  0.0000],\n         [-0.8057,  0.4491]]))\n</code></pre>\n<p>while</p>\n<pre><code>import torch\nfrom torch.distributions import MultivariateNormal\n\ntorch.manual_seed(124)\ninp = torch.randn(3)\nW = torch.randn((2, 3), requires_grad=True)\nloc = W@inp\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\ndistr = MultivariateNormal(loc, cov)\n\nx = distr.sample()  # difference here\nloss = distr.log_prob(x)\nloss.backward()\nW.grad, cov.grad\n</code></pre>\n<p>yields</p>\n<pre><code>(tensor([[-0.1991, -1.0776, -0.6339],\n         [ 0.3455,  1.8700,  1.1000]]), \n None)\n</code></pre>\n<p>I am observing this problem both on macOS and Ubuntu systems.</p>", "body_text": "I have a MultivariateNormal distribution with loc defined as output of neural net (given input) and diagonal covariance matrix with trainable parameters (but does not depending on some input).\nIf I sample via distr.rsample().detach() and optimise sum of log_probs, .backward() provides correct gradients w.r.t. both loc and cov matrix params. But if I sample via distr.sample(), .backward() sets None gradients for cov matrix params.\nHere is a minimal reproducing code:\nimport torch\nfrom torch.distributions import MultivariateNormal\n\ntorch.manual_seed(124)\ninp = torch.randn(3)\nW = torch.randn((2, 3), requires_grad=True)\nloc = W@inp\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\ndistr = MultivariateNormal(loc, cov)\n\ny = distr.rsample().detach()  # difference here\nloss = distr.log_prob(y)\nloss.backward()\nW.grad, cov.grad\n\nyields\n(tensor([[-0.1991, -1.0776, -0.6339],\n         [ 0.3455,  1.8700,  1.1000]]), \n tensor([[-0.2678,  0.0000],\n         [-0.8057,  0.4491]]))\n\nwhile\nimport torch\nfrom torch.distributions import MultivariateNormal\n\ntorch.manual_seed(124)\ninp = torch.randn(3)\nW = torch.randn((2, 3), requires_grad=True)\nloc = W@inp\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\ndistr = MultivariateNormal(loc, cov)\n\nx = distr.sample()  # difference here\nloss = distr.log_prob(x)\nloss.backward()\nW.grad, cov.grad\n\nyields\n(tensor([[-0.1991, -1.0776, -0.6339],\n         [ 0.3455,  1.8700,  1.1000]]), \n None)\n\nI am observing this problem both on macOS and Ubuntu systems.", "body": "I have a MultivariateNormal distribution with loc defined as output of neural net (given input) and diagonal covariance matrix with trainable parameters (but does not depending on some input). \r\nIf I sample via `distr.rsample().detach()` and optimise sum of log_probs, `.backward()` provides correct gradients w.r.t. both loc and cov matrix params. But if I sample via `distr.sample()`, `.backward()` sets `None` gradients for cov matrix params.\r\n\r\nHere is a minimal reproducing code:\r\n```\r\nimport torch\r\nfrom torch.distributions import MultivariateNormal\r\n\r\ntorch.manual_seed(124)\r\ninp = torch.randn(3)\r\nW = torch.randn((2, 3), requires_grad=True)\r\nloc = W@inp\r\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\r\ndistr = MultivariateNormal(loc, cov)\r\n\r\ny = distr.rsample().detach()  # difference here\r\nloss = distr.log_prob(y)\r\nloss.backward()\r\nW.grad, cov.grad\r\n```\r\n\r\nyields \r\n\r\n```\r\n(tensor([[-0.1991, -1.0776, -0.6339],\r\n         [ 0.3455,  1.8700,  1.1000]]), \r\n tensor([[-0.2678,  0.0000],\r\n         [-0.8057,  0.4491]]))\r\n```\r\n\r\nwhile \r\n\r\n```\r\nimport torch\r\nfrom torch.distributions import MultivariateNormal\r\n\r\ntorch.manual_seed(124)\r\ninp = torch.randn(3)\r\nW = torch.randn((2, 3), requires_grad=True)\r\nloc = W@inp\r\ncov = torch.tensor([[1., 0.], [0., 2.]], requires_grad=True)\r\ndistr = MultivariateNormal(loc, cov)\r\n\r\nx = distr.sample()  # difference here\r\nloss = distr.log_prob(x)\r\nloss.backward()\r\nW.grad, cov.grad\r\n```\r\nyields\r\n```\r\n(tensor([[-0.1991, -1.0776, -0.6339],\r\n         [ 0.3455,  1.8700,  1.1000]]), \r\n None)\r\n```\r\n\r\nI am observing this problem both on macOS and Ubuntu systems."}