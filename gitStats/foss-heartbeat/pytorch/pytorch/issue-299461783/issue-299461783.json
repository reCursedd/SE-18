{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5353", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5353/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5353/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5353/events", "html_url": "https://github.com/pytorch/pytorch/issues/5353", "id": 299461783, "node_id": "MDU6SXNzdWUyOTk0NjE3ODM=", "number": 5353, "title": "Bugs: Score Function approach in REINFORCE for PONG", "user": {"login": "LUKELIEM", "id": 10457709, "node_id": "MDQ6VXNlcjEwNDU3NzA5", "avatar_url": "https://avatars0.githubusercontent.com/u/10457709?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LUKELIEM", "html_url": "https://github.com/LUKELIEM", "followers_url": "https://api.github.com/users/LUKELIEM/followers", "following_url": "https://api.github.com/users/LUKELIEM/following{/other_user}", "gists_url": "https://api.github.com/users/LUKELIEM/gists{/gist_id}", "starred_url": "https://api.github.com/users/LUKELIEM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LUKELIEM/subscriptions", "organizations_url": "https://api.github.com/users/LUKELIEM/orgs", "repos_url": "https://api.github.com/users/LUKELIEM/repos", "events_url": "https://api.github.com/users/LUKELIEM/events{/privacy}", "received_events_url": "https://api.github.com/users/LUKELIEM/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/crash", "name": "crash", "color": "d93f0b", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-22T18:35:59Z", "updated_at": "2018-05-14T19:32:48Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I adapted the reinforce and actor-critic code for Cartpole to PONG. The original Cartpole code is located in:</p>\n<p><a href=\"https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\">https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py</a></p>\n<p>In the Cartpole example, the input to the policy NN is the state outputted by OpenAI, which is an array of 4 float. In PONG, the input is a downsampled image vector of 1600 (40x40) or 6400 (80x80). When using the reinforce.py as the guideline and using the score function approach described in Pytorch.org documentation (<a href=\"http://pytorch.org/docs/master/distributions.html\" rel=\"nofollow\">http://pytorch.org/docs/master/distributions.html</a>), I encountered 2 bugs:</p>\n<p>(1) GPU Memory Error<br>\nThe exact error is:<br>\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu:58</p>\n<p>The REINFORCE implementation requires the policy NN to run forward pass 1000-1500 times depending on how many game steps it takes for 21 points to be reached. Running \"nvidia-smi l 1\", I discover that GPU memory is not released on the Nvidia card even after the program exit.</p>\n<p>I have to do 2 things to get the code to run:</p>\n<ul>\n<li>Add torch.cuda.empty_cache() at the end of each episode</li>\n<li>Limit the forward passes to a fixed batch size (e.g. 800-1200) instead of letting the code run until a flag (done) is True</li>\n</ul>\n<p>The typical workflow for policy-based RL is 1000-10000 forward pass, and then 1 single backward pass utilizing stacked intermediary data such as hidden units, states, rewards, log_prob and so on. Either Torch or the underlying CUDA has memory management problem with this kind of workflow.</p>\n<p>(2) Underperformance of the trained RL agent</p>\n<p>I have run the REINFORCE agent to 30K episodes and it is not learning as well as the original REINFORCE implemented by Andrej Karpathy using numpy.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10457709/36557214-83f34f6c-17bc-11e8-9e8c-754da4573f90.png\"><img src=\"https://user-images.githubusercontent.com/10457709/36557214-83f34f6c-17bc-11e8-9e8c-754da4573f90.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Details of my experimentation is detailed in:<br>\n<a href=\"https://github.com/LUKELIEM/deep_rl/blob/master/gym/reinforce.ipynb\">https://github.com/LUKELIEM/deep_rl/blob/master/gym/reinforce.ipynb</a></p>\n<p>There is 1 major difference between my code and Karpathy's py-pong.py:</p>\n<ul>\n<li>\n<p>Karpathy implements the backprop directly</p>\n<ul>\n<li>His code accumulates the intermediary states (epx), hidden states (eph), rewards (epr) and log_prob (eplogd) during the forward pass</li>\n<li>At the single backpass after 1000-1500 forward passes, the backprop is implemented by matrix multiplication of these stacked intermediary values to produce a gradient</li>\n<li>The code then uses RMSprop to update the weights directly.</li>\n</ul>\n</li>\n<li>\n<p>Pytorch's code implements backprop using policy_loss.backward() through this code fragment:</p>\n<p>probs = policy(Variable(state))<br>\nm = Categorical(probs)<br>\naction = m.sample()<br>\npolicy.saved_log_probs.append(m.log_prob(action))</p>\n</li>\n</ul>\n<p>I have tried various update rules and they all seem incapable to learning beyond running reward of 0.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10457709/36679195-7a01bada-1ac7-11e8-9122-5c330d3fe9c3.png\"><img src=\"https://user-images.githubusercontent.com/10457709/36679195-7a01bada-1ac7-11e8-9122-5c330d3fe9c3.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version:  0.3.0</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration: ASUS ROG STRIX GeForce GTX 1080 TI 11GB VR Ready 5K HD Gaming Graphics Card</li>\n<li>GCC version (if compiling from source):</li>\n</ul>", "body_text": "I adapted the reinforce and actor-critic code for Cartpole to PONG. The original Cartpole code is located in:\nhttps://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\nIn the Cartpole example, the input to the policy NN is the state outputted by OpenAI, which is an array of 4 float. In PONG, the input is a downsampled image vector of 1600 (40x40) or 6400 (80x80). When using the reinforce.py as the guideline and using the score function approach described in Pytorch.org documentation (http://pytorch.org/docs/master/distributions.html), I encountered 2 bugs:\n(1) GPU Memory Error\nThe exact error is:\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu:58\nThe REINFORCE implementation requires the policy NN to run forward pass 1000-1500 times depending on how many game steps it takes for 21 points to be reached. Running \"nvidia-smi l 1\", I discover that GPU memory is not released on the Nvidia card even after the program exit.\nI have to do 2 things to get the code to run:\n\nAdd torch.cuda.empty_cache() at the end of each episode\nLimit the forward passes to a fixed batch size (e.g. 800-1200) instead of letting the code run until a flag (done) is True\n\nThe typical workflow for policy-based RL is 1000-10000 forward pass, and then 1 single backward pass utilizing stacked intermediary data such as hidden units, states, rewards, log_prob and so on. Either Torch or the underlying CUDA has memory management problem with this kind of workflow.\n(2) Underperformance of the trained RL agent\nI have run the REINFORCE agent to 30K episodes and it is not learning as well as the original REINFORCE implemented by Andrej Karpathy using numpy.\n\nDetails of my experimentation is detailed in:\nhttps://github.com/LUKELIEM/deep_rl/blob/master/gym/reinforce.ipynb\nThere is 1 major difference between my code and Karpathy's py-pong.py:\n\n\nKarpathy implements the backprop directly\n\nHis code accumulates the intermediary states (epx), hidden states (eph), rewards (epr) and log_prob (eplogd) during the forward pass\nAt the single backpass after 1000-1500 forward passes, the backprop is implemented by matrix multiplication of these stacked intermediary values to produce a gradient\nThe code then uses RMSprop to update the weights directly.\n\n\n\nPytorch's code implements backprop using policy_loss.backward() through this code fragment:\nprobs = policy(Variable(state))\nm = Categorical(probs)\naction = m.sample()\npolicy.saved_log_probs.append(m.log_prob(action))\n\n\nI have tried various update rules and they all seem incapable to learning beyond running reward of 0.\n\n\nOS: Ubuntu 16.04\nPyTorch version:  0.3.0\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6.4\nCUDA/cuDNN version:\nGPU models and configuration: ASUS ROG STRIX GeForce GTX 1080 TI 11GB VR Ready 5K HD Gaming Graphics Card\nGCC version (if compiling from source):", "body": "I adapted the reinforce and actor-critic code for Cartpole to PONG. The original Cartpole code is located in:\r\n\r\nhttps://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\r\n\r\nIn the Cartpole example, the input to the policy NN is the state outputted by OpenAI, which is an array of 4 float. In PONG, the input is a downsampled image vector of 1600 (40x40) or 6400 (80x80). When using the reinforce.py as the guideline and using the score function approach described in Pytorch.org documentation (http://pytorch.org/docs/master/distributions.html), I encountered 2 bugs:\r\n\r\n(1) GPU Memory Error\r\nThe exact error is:\r\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu:58\r\n\r\nThe REINFORCE implementation requires the policy NN to run forward pass 1000-1500 times depending on how many game steps it takes for 21 points to be reached. Running \"nvidia-smi l 1\", I discover that GPU memory is not released on the Nvidia card even after the program exit. \r\n\r\nI have to do 2 things to get the code to run:\r\n- Add torch.cuda.empty_cache() at the end of each episode\r\n- Limit the forward passes to a fixed batch size (e.g. 800-1200) instead of letting the code run until a flag (done) is True\r\n\r\nThe typical workflow for policy-based RL is 1000-10000 forward pass, and then 1 single backward pass utilizing stacked intermediary data such as hidden units, states, rewards, log_prob and so on. Either Torch or the underlying CUDA has memory management problem with this kind of workflow. \r\n\r\n(2) Underperformance of the trained RL agent\r\n\r\nI have run the REINFORCE agent to 30K episodes and it is not learning as well as the original REINFORCE implemented by Andrej Karpathy using numpy. \r\n\r\n![image](https://user-images.githubusercontent.com/10457709/36557214-83f34f6c-17bc-11e8-9e8c-754da4573f90.png)\r\n\r\nDetails of my experimentation is detailed in:\r\nhttps://github.com/LUKELIEM/deep_rl/blob/master/gym/reinforce.ipynb\r\n\r\nThere is 1 major difference between my code and Karpathy's py-pong.py:\r\n\r\n- Karpathy implements the backprop directly \r\n   - His code accumulates the intermediary states (epx), hidden states (eph), rewards (epr) and log_prob (eplogd) during the forward pass\r\n   - At the single backpass after 1000-1500 forward passes, the backprop is implemented by matrix multiplication of these stacked intermediary values to produce a gradient \r\n   - The code then uses RMSprop to update the weights directly.\r\n  \r\n- Pytorch's code implements backprop using policy_loss.backward() through this code fragment:\r\n\r\n    probs = policy(Variable(state))\r\n    m = Categorical(probs)\r\n    action = m.sample()\r\n    policy.saved_log_probs.append(m.log_prob(action))\r\n\r\nI have tried various update rules and they all seem incapable to learning beyond running reward of 0.\r\n\r\n![image](https://user-images.githubusercontent.com/10457709/36679195-7a01bada-1ac7-11e8-9122-5c330d3fe9c3.png)\r\n\r\n\r\n- OS: Ubuntu 16.04\r\n- PyTorch version:  0.3.0\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: \r\n- GPU models and configuration: ASUS ROG STRIX GeForce GTX 1080 TI 11GB VR Ready 5K HD Gaming Graphics Card \r\n- GCC version (if compiling from source):\r\n\r\n\r\n"}