{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/432176694", "html_url": "https://github.com/pytorch/pytorch/issues/12659#issuecomment-432176694", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12659", "id": 432176694, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjE3NjY5NA==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-23T09:44:46Z", "updated_at": "2018-10-23T09:44:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't think this fits well with how optimizers operate on parameters currently.</p>\n<p>I wonder if something like <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"320453909\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7313\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7313/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7313\">#7313</a> - changing what Parameters are - could help here. You could collect the gradient updates in a calculated parameter (which would still be a bit hacky, but one generic one for all your params).<br>\nThen you would have to replace optimzier.step by something that special cases those ops and probably also need a \"consolidation\" step that folds the updates into the param properly to free the graph.</p>", "body_text": "I don't think this fits well with how optimizers operate on parameters currently.\nI wonder if something like #7313 - changing what Parameters are - could help here. You could collect the gradient updates in a calculated parameter (which would still be a bit hacky, but one generic one for all your params).\nThen you would have to replace optimzier.step by something that special cases those ops and probably also need a \"consolidation\" step that folds the updates into the param properly to free the graph.", "body": "I don't think this fits well with how optimizers operate on parameters currently.\r\n\r\nI wonder if something like #7313 - changing what Parameters are - could help here. You could collect the gradient updates in a calculated parameter (which would still be a bit hacky, but one generic one for all your params).\r\nThen you would have to replace optimzier.step by something that special cases those ops and probably also need a \"consolidation\" step that folds the updates into the param properly to free the graph.\r\n"}