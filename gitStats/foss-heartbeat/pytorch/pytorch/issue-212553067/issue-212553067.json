{"url": "https://api.github.com/repos/pytorch/pytorch/issues/953", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/953/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/953/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/953/events", "html_url": "https://github.com/pytorch/pytorch/issues/953", "id": 212553067, "node_id": "MDU6SXNzdWUyMTI1NTMwNjc=", "number": 953, "title": "issue when using LSTM with Dropout", "user": {"login": "glample", "id": 8885556, "node_id": "MDQ6VXNlcjg4ODU1NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8885556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glample", "html_url": "https://github.com/glample", "followers_url": "https://api.github.com/users/glample/followers", "following_url": "https://api.github.com/users/glample/following{/other_user}", "gists_url": "https://api.github.com/users/glample/gists{/gist_id}", "starred_url": "https://api.github.com/users/glample/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glample/subscriptions", "organizations_url": "https://api.github.com/users/glample/orgs", "repos_url": "https://api.github.com/users/glample/repos", "events_url": "https://api.github.com/users/glample/events{/privacy}", "received_events_url": "https://api.github.com/users/glample/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-07T20:44:18Z", "updated_at": "2017-03-07T22:40:55Z", "closed_at": "2017-03-07T21:48:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm facing issues when I use CUDNN LSTM with dropout, during the training only. Sorry for not being able to create a simple repro for now.. in the meantime maybe the error message will be enough:</p>\n<pre><code>/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--&gt; 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/guismay/projects/nlpGAN/autoencoder/src/model.pyc in forward(self, sequence, lengths, temperature)\n     94         # forward\n     95         embeddings = self.embeddings(sequence)\n---&gt; 96         enc_lstm_output, (_, _) = self.lstm(embeddings)\n     97         assert enc_lstm_output.size() == (seq_len, batch_size, self.hidden_dim)\n     98 \n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--&gt; 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc in forward(self, input, hx)\n     89             dropout_state=self.dropout_state\n     90         )\n---&gt; 91         output, hidden = func(input, self.all_weights, hx)\n     92         if is_packed:\n     93             output = PackedSequence(output, batch_sizes)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc in forward(input, *fargs, **fkwargs)\n    325         else:\n    326             func = AutogradRNN(*args, **kwargs)\n--&gt; 327         return func(input, *fargs, **fkwargs)\n    328 \n    329     return forward\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/autograd/function.pyc in _do_forward(self, *input)\n    199         self._nested_input = input\n    200         flat_input = tuple(_iter_variables(input))\n--&gt; 201         flat_output = super(NestedIOFunction, self)._do_forward(*flat_input)\n    202         nested_output = self._nested_output\n    203         nested_variables = _unflatten(flat_output, self._nested_output)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/autograd/function.pyc in forward(self, *args)\n    221     def forward(self, *args):\n    222         nested_tensors = _map_variable_tensor(self._nested_input)\n--&gt; 223         result = self.forward_extended(*nested_tensors)\n    224         del self._nested_input\n    225         self._nested_output = result\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc in forward_extended(self, input, weight, hx)\n    267             hy = tuple(h.new() for h in hx)\n    268 \n--&gt; 269         cudnn.rnn.forward(self, input, hx, weight, output, hy)\n    270 \n    271         self.save_for_backward(input, hx, weight, output)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc in forward(fn, input, hx, weight, output, hy)\n    230 \n    231         # init descriptors\n--&gt; 232         fn.rnn_desc = init_rnn_descriptor(fn, handle)\n    233         if is_input_packed:\n    234             fn.x_descs = cudnn.descriptor_sequence(x, fn.batch_sizes)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc in init_rnn_descriptor(fn, handle)\n     43         )\n     44     dropout_desc = fn.dropout_state[dropout_desc_name].get()\n---&gt; 45     dropout_desc.set_dropout(dropout_p, fn.dropout_seed)\n     46     return cudnn.RNNDescriptor(\n     47         handle,\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in set_dropout(self, dropout, seed)\n    254     def set_dropout(self, dropout, seed):\n    255         if dropout != self.dropout:\n--&gt; 256             self._set(dropout, seed)\n    257 \n    258     def _set(self, dropout, seed):\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in _set(self, dropout, seed)\n    275             ctypes.c_void_p(state_ptr),\n    276             ctypes.c_size_t(state_size),\n--&gt; 277             ctypes.c_ulonglong(seed),\n    278         ))\n    279 \n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in check_error(status)\n    342 def check_error(status):\n    343     if status is not 0:\n--&gt; 344         raise CuDNNError(status)\n    345 \n    346 \n\nCuDNNError: 8: CUDNN_STATUS_EXECUTION_FAILED\n</code></pre>", "body_text": "I'm facing issues when I use CUDNN LSTM with dropout, during the training only. Sorry for not being able to create a simple repro for now.. in the meantime maybe the error message will be enough:\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--> 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/guismay/projects/nlpGAN/autoencoder/src/model.pyc in forward(self, sequence, lengths, temperature)\n     94         # forward\n     95         embeddings = self.embeddings(sequence)\n---> 96         enc_lstm_output, (_, _) = self.lstm(embeddings)\n     97         assert enc_lstm_output.size() == (seq_len, batch_size, self.hidden_dim)\n     98 \n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--> 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc in forward(self, input, hx)\n     89             dropout_state=self.dropout_state\n     90         )\n---> 91         output, hidden = func(input, self.all_weights, hx)\n     92         if is_packed:\n     93             output = PackedSequence(output, batch_sizes)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc in forward(input, *fargs, **fkwargs)\n    325         else:\n    326             func = AutogradRNN(*args, **kwargs)\n--> 327         return func(input, *fargs, **fkwargs)\n    328 \n    329     return forward\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/autograd/function.pyc in _do_forward(self, *input)\n    199         self._nested_input = input\n    200         flat_input = tuple(_iter_variables(input))\n--> 201         flat_output = super(NestedIOFunction, self)._do_forward(*flat_input)\n    202         nested_output = self._nested_output\n    203         nested_variables = _unflatten(flat_output, self._nested_output)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/autograd/function.pyc in forward(self, *args)\n    221     def forward(self, *args):\n    222         nested_tensors = _map_variable_tensor(self._nested_input)\n--> 223         result = self.forward_extended(*nested_tensors)\n    224         del self._nested_input\n    225         self._nested_output = result\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc in forward_extended(self, input, weight, hx)\n    267             hy = tuple(h.new() for h in hx)\n    268 \n--> 269         cudnn.rnn.forward(self, input, hx, weight, output, hy)\n    270 \n    271         self.save_for_backward(input, hx, weight, output)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc in forward(fn, input, hx, weight, output, hy)\n    230 \n    231         # init descriptors\n--> 232         fn.rnn_desc = init_rnn_descriptor(fn, handle)\n    233         if is_input_packed:\n    234             fn.x_descs = cudnn.descriptor_sequence(x, fn.batch_sizes)\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc in init_rnn_descriptor(fn, handle)\n     43         )\n     44     dropout_desc = fn.dropout_state[dropout_desc_name].get()\n---> 45     dropout_desc.set_dropout(dropout_p, fn.dropout_seed)\n     46     return cudnn.RNNDescriptor(\n     47         handle,\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in set_dropout(self, dropout, seed)\n    254     def set_dropout(self, dropout, seed):\n    255         if dropout != self.dropout:\n--> 256             self._set(dropout, seed)\n    257 \n    258     def _set(self, dropout, seed):\n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in _set(self, dropout, seed)\n    275             ctypes.c_void_p(state_ptr),\n    276             ctypes.c_size_t(state_size),\n--> 277             ctypes.c_ulonglong(seed),\n    278         ))\n    279 \n\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in check_error(status)\n    342 def check_error(status):\n    343     if status is not 0:\n--> 344         raise CuDNNError(status)\n    345 \n    346 \n\nCuDNNError: 8: CUDNN_STATUS_EXECUTION_FAILED", "body": "I'm facing issues when I use CUDNN LSTM with dropout, during the training only. Sorry for not being able to create a simple repro for now.. in the meantime maybe the error message will be enough:\r\n\r\n```\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    200 \r\n    201     def __call__(self, *input, **kwargs):\r\n--> 202         result = self.forward(*input, **kwargs)\r\n    203         for hook in self._forward_hooks.values():\r\n    204             hook_result = hook(self, input, result)\r\n\r\n/home/guismay/projects/nlpGAN/autoencoder/src/model.pyc in forward(self, sequence, lengths, temperature)\r\n     94         # forward\r\n     95         embeddings = self.embeddings(sequence)\r\n---> 96         enc_lstm_output, (_, _) = self.lstm(embeddings)\r\n     97         assert enc_lstm_output.size() == (seq_len, batch_size, self.hidden_dim)\r\n     98 \r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    200 \r\n    201     def __call__(self, *input, **kwargs):\r\n--> 202         result = self.forward(*input, **kwargs)\r\n    203         for hook in self._forward_hooks.values():\r\n    204             hook_result = hook(self, input, result)\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc in forward(self, input, hx)\r\n     89             dropout_state=self.dropout_state\r\n     90         )\r\n---> 91         output, hidden = func(input, self.all_weights, hx)\r\n     92         if is_packed:\r\n     93             output = PackedSequence(output, batch_sizes)\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc in forward(input, *fargs, **fkwargs)\r\n    325         else:\r\n    326             func = AutogradRNN(*args, **kwargs)\r\n--> 327         return func(input, *fargs, **fkwargs)\r\n    328 \r\n    329     return forward\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/autograd/function.pyc in _do_forward(self, *input)\r\n    199         self._nested_input = input\r\n    200         flat_input = tuple(_iter_variables(input))\r\n--> 201         flat_output = super(NestedIOFunction, self)._do_forward(*flat_input)\r\n    202         nested_output = self._nested_output\r\n    203         nested_variables = _unflatten(flat_output, self._nested_output)\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/autograd/function.pyc in forward(self, *args)\r\n    221     def forward(self, *args):\r\n    222         nested_tensors = _map_variable_tensor(self._nested_input)\r\n--> 223         result = self.forward_extended(*nested_tensors)\r\n    224         del self._nested_input\r\n    225         self._nested_output = result\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/rnn.pyc in forward_extended(self, input, weight, hx)\r\n    267             hy = tuple(h.new() for h in hx)\r\n    268 \r\n--> 269         cudnn.rnn.forward(self, input, hx, weight, output, hy)\r\n    270 \r\n    271         self.save_for_backward(input, hx, weight, output)\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc in forward(fn, input, hx, weight, output, hy)\r\n    230 \r\n    231         # init descriptors\r\n--> 232         fn.rnn_desc = init_rnn_descriptor(fn, handle)\r\n    233         if is_input_packed:\r\n    234             fn.x_descs = cudnn.descriptor_sequence(x, fn.batch_sizes)\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.pyc in init_rnn_descriptor(fn, handle)\r\n     43         )\r\n     44     dropout_desc = fn.dropout_state[dropout_desc_name].get()\r\n---> 45     dropout_desc.set_dropout(dropout_p, fn.dropout_seed)\r\n     46     return cudnn.RNNDescriptor(\r\n     47         handle,\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in set_dropout(self, dropout, seed)\r\n    254     def set_dropout(self, dropout, seed):\r\n    255         if dropout != self.dropout:\r\n--> 256             self._set(dropout, seed)\r\n    257 \r\n    258     def _set(self, dropout, seed):\r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in _set(self, dropout, seed)\r\n    275             ctypes.c_void_p(state_ptr),\r\n    276             ctypes.c_size_t(state_size),\r\n--> 277             ctypes.c_ulonglong(seed),\r\n    278         ))\r\n    279 \r\n\r\n/home/guismay/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.pyc in check_error(status)\r\n    342 def check_error(status):\r\n    343     if status is not 0:\r\n--> 344         raise CuDNNError(status)\r\n    345 \r\n    346 \r\n\r\nCuDNNError: 8: CUDNN_STATUS_EXECUTION_FAILED\r\n```"}