{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/439931580", "html_url": "https://github.com/pytorch/pytorch/issues/8103#issuecomment-439931580", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8103", "id": 439931580, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTkzMTU4MA==", "user": {"login": "Bfzanchetta", "id": 12160655, "node_id": "MDQ6VXNlcjEyMTYwNjU1", "avatar_url": "https://avatars3.githubusercontent.com/u/12160655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bfzanchetta", "html_url": "https://github.com/Bfzanchetta", "followers_url": "https://api.github.com/users/Bfzanchetta/followers", "following_url": "https://api.github.com/users/Bfzanchetta/following{/other_user}", "gists_url": "https://api.github.com/users/Bfzanchetta/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bfzanchetta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bfzanchetta/subscriptions", "organizations_url": "https://api.github.com/users/Bfzanchetta/orgs", "repos_url": "https://api.github.com/users/Bfzanchetta/repos", "events_url": "https://api.github.com/users/Bfzanchetta/events{/privacy}", "received_events_url": "https://api.github.com/users/Bfzanchetta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-19T15:27:29Z", "updated_at": "2018-11-19T15:27:29Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=483114\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewssobral\">@andrewssobral</a> , I have pytorch working on my TX2. I'm at SHA1 ID <a href=\"https://github.com/pytorch/pytorch/commit/cfa05706efa71454bede06c2aa25abc287db1f04\">cfa0570</a> and have only done the following change in \"aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu\":</p>\n<p>Around line 62:<br>\ncomment out THCState_getCurrentDeviceProperties(state)-&gt;maxThraedsPerBlock;<br>\nSet<br>\nconst int num_threads = 512;</p>\n<p>Around line 97<br>\ncomment out THCState_getCurrentDeviceProperties(state)-&gt;maxThraedsPerBlock;<br>\nSet<br>\nconst int num_threads = 512;</p>\n<p>I followed this guide for installation: <a href=\"https://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426\">https://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426</a> using the install mode command \"sudo python setup.py install\"</p>\n<p>Hope this helps</p>\n</blockquote>\n<p>Thanks! Worked perfectly without the need to add launch_bounds(1024)</p>", "body_text": "Hi @andrewssobral , I have pytorch working on my TX2. I'm at SHA1 ID cfa0570 and have only done the following change in \"aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu\":\nAround line 62:\ncomment out THCState_getCurrentDeviceProperties(state)->maxThraedsPerBlock;\nSet\nconst int num_threads = 512;\nAround line 97\ncomment out THCState_getCurrentDeviceProperties(state)->maxThraedsPerBlock;\nSet\nconst int num_threads = 512;\nI followed this guide for installation: https://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426 using the install mode command \"sudo python setup.py install\"\nHope this helps\n\nThanks! Worked perfectly without the need to add launch_bounds(1024)", "body": "> Hi @andrewssobral , I have pytorch working on my TX2. I'm at SHA1 ID [cfa0570](https://github.com/pytorch/pytorch/commit/cfa05706efa71454bede06c2aa25abc287db1f04) and have only done the following change in \"aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu\":\r\n> \r\n> Around line 62:\r\n> comment out THCState_getCurrentDeviceProperties(state)->maxThraedsPerBlock;\r\n> Set\r\n> const int num_threads = 512;\r\n> \r\n> Around line 97\r\n> comment out THCState_getCurrentDeviceProperties(state)->maxThraedsPerBlock;\r\n> Set\r\n> const int num_threads = 512;\r\n> \r\n> I followed this guide for installation: https://gist.github.com/dusty-nv/ef2b372301c00c0a9d3203e42fd83426 using the install mode command \"sudo python setup.py install\"\r\n> \r\n> Hope this helps\r\n\r\nThanks! Worked perfectly without the need to add launch_bounds(1024)"}