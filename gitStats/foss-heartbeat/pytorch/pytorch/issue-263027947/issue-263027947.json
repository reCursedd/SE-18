{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2983", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2983/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2983/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2983/events", "html_url": "https://github.com/pytorch/pytorch/issues/2983", "id": 263027947, "node_id": "MDU6SXNzdWUyNjMwMjc5NDc=", "number": 2983, "title": "Illegal memory access -- bug ", "user": {"login": "johnwlambert", "id": 16724970, "node_id": "MDQ6VXNlcjE2NzI0OTcw", "avatar_url": "https://avatars2.githubusercontent.com/u/16724970?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnwlambert", "html_url": "https://github.com/johnwlambert", "followers_url": "https://api.github.com/users/johnwlambert/followers", "following_url": "https://api.github.com/users/johnwlambert/following{/other_user}", "gists_url": "https://api.github.com/users/johnwlambert/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnwlambert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnwlambert/subscriptions", "organizations_url": "https://api.github.com/users/johnwlambert/orgs", "repos_url": "https://api.github.com/users/johnwlambert/repos", "events_url": "https://api.github.com/users/johnwlambert/events{/privacy}", "received_events_url": "https://api.github.com/users/johnwlambert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-10-05T07:29:33Z", "updated_at": "2017-10-29T09:22:05Z", "closed_at": "2017-10-29T09:22:05Z", "author_association": "NONE", "body_html": "<p>My pytorch training ran successfully for 150 epochs, then crashes as follows (I've never experienced this bug before in 6+ months of pytorch):</p>\n<blockquote>\n<p>THCudaCheck FAIL file=/py/conda-bld/pytorch_1490981920203/work/torch/lib/THC/THCTensorCopy.cu line=85 error=77 : an illegal memory access was encountered<br>\nTraceback (most recent call last):<br>\nFile \"train.py\", line 462, in <br>\nmain(opt)<br>\nFile \"train.py\", line 176, in main<br>\n_, _ = run_epoch(                  opt, epoch, train_loader, model, criterion, optimizer, None,         criterion2,  train=True)<br>\nFile \"train.py\", line 192, in run_epoch<br>\ncriterion, model, epoch, step, data_loader, optimizer, tag, criterion2)<br>\nFile \"train.py\", line 329, in run_iteration<br>\nx_output_v = model(images_v)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 62, in forward<br>\nreturn self.gather(outputs, self.output_device)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 74, in gather<br>\nreturn gather(outputs, output_device, dim=self.dim)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 50, in gather<br>\nreturn gather_map(outputs)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 46, in gather_map<br>\nreturn Gather(target_device, dim=dim)(*outputs)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/<em>functions.py\", line 38, in forward<br>\nreturn comm.gather(inputs, self.dim, self.target_device)<br>\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 206, in gather<br>\nresult.narrow(dim, chunk_start, tensor.size(dim)).copy</em>(tensor, True)<br>\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /py/conda-bld/pytorch_1490981920203/work/torch/lib/THC/THCTensorCopy.cu:85</p>\n</blockquote>", "body_text": "My pytorch training ran successfully for 150 epochs, then crashes as follows (I've never experienced this bug before in 6+ months of pytorch):\n\nTHCudaCheck FAIL file=/py/conda-bld/pytorch_1490981920203/work/torch/lib/THC/THCTensorCopy.cu line=85 error=77 : an illegal memory access was encountered\nTraceback (most recent call last):\nFile \"train.py\", line 462, in \nmain(opt)\nFile \"train.py\", line 176, in main\n_, _ = run_epoch(                  opt, epoch, train_loader, model, criterion, optimizer, None,         criterion2,  train=True)\nFile \"train.py\", line 192, in run_epoch\ncriterion, model, epoch, step, data_loader, optimizer, tag, criterion2)\nFile \"train.py\", line 329, in run_iteration\nx_output_v = model(images_v)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in call\nresult = self.forward(*input, **kwargs)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 62, in forward\nreturn self.gather(outputs, self.output_device)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 74, in gather\nreturn gather(outputs, output_device, dim=self.dim)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 50, in gather\nreturn gather_map(outputs)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 46, in gather_map\nreturn Gather(target_device, dim=dim)(*outputs)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/functions.py\", line 38, in forward\nreturn comm.gather(inputs, self.dim, self.target_device)\nFile \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 206, in gather\nresult.narrow(dim, chunk_start, tensor.size(dim)).copy(tensor, True)\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /py/conda-bld/pytorch_1490981920203/work/torch/lib/THC/THCTensorCopy.cu:85", "body": "My pytorch training ran successfully for 150 epochs, then crashes as follows (I've never experienced this bug before in 6+ months of pytorch):\r\n\r\n\r\n> THCudaCheck FAIL file=/py/conda-bld/pytorch_1490981920203/work/torch/lib/THC/THCTensorCopy.cu line=85 error=77 : an illegal memory access was encountered\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 462, in <module>\r\n    main(opt)\r\n  File \"train.py\", line 176, in main\r\n    _, _ = run_epoch(                  opt, epoch, train_loader, model, criterion, optimizer, None,         criterion2,  train=True)\r\n  File \"train.py\", line 192, in run_epoch\r\n    criterion, model, epoch, step, data_loader, optimizer, tag, criterion2)\r\n  File \"train.py\", line 329, in run_iteration\r\n    x_output_v = model(images_v)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 62, in forward\r\n    return self.gather(outputs, self.output_device)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 74, in gather\r\n    return gather(outputs, output_device, dim=self.dim)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 50, in gather\r\n    return gather_map(outputs)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 46, in gather_map\r\n    return Gather(target_device, dim=dim)(*outputs)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 38, in forward\r\n    return comm.gather(inputs, self.dim, self.target_device)\r\n  File \"/PATH_TO_HERE/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 206, in gather\r\n    result.narrow(dim, chunk_start, tensor.size(dim)).copy_(tensor, True)\r\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /py/conda-bld/pytorch_1490981920203/work/torch/lib/THC/THCTensorCopy.cu:85"}