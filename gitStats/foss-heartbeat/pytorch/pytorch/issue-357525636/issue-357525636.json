{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11327", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11327/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11327/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11327/events", "html_url": "https://github.com/pytorch/pytorch/issues/11327", "id": 357525636, "node_id": "MDU6SXNzdWUzNTc1MjU2MzY=", "number": 11327, "title": "Distributed Training shut down on second epoch", "user": {"login": "yingjianling", "id": 19993503, "node_id": "MDQ6VXNlcjE5OTkzNTAz", "avatar_url": "https://avatars3.githubusercontent.com/u/19993503?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yingjianling", "html_url": "https://github.com/yingjianling", "followers_url": "https://api.github.com/users/yingjianling/followers", "following_url": "https://api.github.com/users/yingjianling/following{/other_user}", "gists_url": "https://api.github.com/users/yingjianling/gists{/gist_id}", "starred_url": "https://api.github.com/users/yingjianling/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yingjianling/subscriptions", "organizations_url": "https://api.github.com/users/yingjianling/orgs", "repos_url": "https://api.github.com/users/yingjianling/repos", "events_url": "https://api.github.com/users/yingjianling/events{/privacy}", "received_events_url": "https://api.github.com/users/yingjianling/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2018-09-06T07:17:58Z", "updated_at": "2018-09-18T01:28:54Z", "closed_at": "2018-09-17T14:57:13Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Hi, guys. I am trying to train a resnet model on single node and 8 GPUs with DistributedDataParallel. Every thing is ok during the first epoch. However, the script shut down without any error report when the second epoch starts. I have tried to track the code and find that the code stops at:<br>\n<code>for batch_idx, (data, label) in enumerate(train_loader, 0):</code><br>\nMeanwhile, I create a small version dataset with 34 classes, and the error is gone.<br>\nHere is my command line:<br>\n<code>python -m torch.distributed.launch --nproc_per_node=8 train.py</code><br>\nAny help will be appreciate.</p>\n<h2>Code example</h2>\n<pre><code>import argparse,os,time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.distributed as dist\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport models\nfrom util import *\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--start_epoch', type=int, default=1, help='start epoch number')\nparser.add_argument('--epoch', type=int, default=25, help='number of epochs to train for')\nparser.add_argument('--lr', type=float, default=0.1, help='learning rate, default=0.1')\nparser.add_argument('--momentum', type=float, default=0.9, help='momentum, default=0.9')\nparser.add_argument('--weight_decay', type=float, default=0.0002, help='weight_decay, default=0.0002')\nparser.add_argument('--batch_s', type=int, default=64, help='input batch size')\nparser.add_argument('--grid_s', type=int, default=8, help='grid size')\nparser.add_argument('--data', type=str, default='../vgg2data', help='data directory')\nparser.add_argument('--workers', type=int, default=1, help='number of data loading workers')\nparser.add_argument('--output_dir', type=str, default='./output/', help='model_saving directory')\nparser.add_argument('--resume', type=str, default='', help='resume')\nparser.add_argument(\"--display_interval\", type=int, default=50)\nparser.add_argument(\"--local_rank\", type=int)\nopt = parser.parse_args()\ntorch.cuda.set_device(opt.local_rank)\n\ndist.init_process_group(backend='nccl', init_method='env://', world_size=8)\n\ntrain_dir = os.path.join(opt.data, 'train')\ntrain_dataset = datasets.ImageFolder(\n    train_dir,\n    transforms.Compose([transforms.ToTensor()])\n)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=opt.batch_s,\n        num_workers=opt.workers,\n        pin_memory=True,\n        shuffle=False,\n        sampler=train_sampler\n    )\n\ninput_size = (opt.batch_s, 3, 128, 128)\nnum_classes = 9092\n#num_classes = 34\nmodel = models.se_resnet34_v3(input_size, opt.grid_s, num_classes)\nif opt.resume:\n    if os.path.isfile(opt.resume):\n        print(\"=&gt; loading checkpoint '{}'\".format(opt.resume))\n        checkpoint = torch.load(opt.resume)\n        model.load_state_dict(checkpoint['state_dict'])\n        print(\"=&gt; loaded checkpoint '{}' (epoch {})\"\n            .format(opt.resume, checkpoint['epoch']))\nmodel.cuda()\nmodel = torch.nn.parallel.DistributedDataParallel(model,\\\n    device_ids=[opt.local_rank], output_device=opt.local_rank)\n\noptimizer = optim.SGD([\n        {'params': get_parameters(model, bias=False)},\n        {'params': get_parameters(model, bias=True), 'lr':opt.lr * 2, 'weight_decay': 0},\n        {'params': get_parameters(model, bn=True), 'lr':opt.lr * 1.00001001358, 'weight_decay':0}\n    ], lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\nif opt.resume:\n    if os.path.isfile(opt.resume):\n        checkpoint = torch.load(opt.resume)\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, \\\n    milestones=[8,15,18], gamma=0.5)\n\ndef train(epoch):\n    for batch_idx, (data, label) in enumerate(train_loader, 0):\n        optimizer.zero_grad()\n        data, label = data.cuda(), label.cuda()\n        output, grid = model(data)\n        nll_loss = F.nll_loss(output, label)\n        de_loss = deformation_constraint_loss(grid, opt.grid_s)\n        loss =  nll_loss + de_loss\n        loss.backward()\n        optimizer.step()\nfor epoch in range(opt.start_epoch, opt.epoch + 1):\n    train_sampler.set_epoch(epoch)\n    scheduler.step() \n    model.train()\n    train(epoch)\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 2.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.44<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080 Ti<br>\nGPU 1: GeForce GTX 1080 Ti<br>\nGPU 2: GeForce GTX 1080 Ti<br>\nGPU 3: GeForce GTX 1080 Ti<br>\nGPU 4: GeForce GTX 1080 Ti<br>\nGPU 5: GeForce GTX 1080 Ti<br>\nGPU 6: GeForce GTX 1080 Ti<br>\nGPU 7: GeForce GTX 1080 Ti</p>\n<p>Nvidia driver version: 390.12<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.1<br>\n/usr/local/cuda-8.0/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.3)<br>\n[pip] numpydoc (0.8.0)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] cuda80                    1.0                  h205658b_0    <a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\" rel=\"nofollow\">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch</a><br>\n[conda] pytorch                   0.4.1           py27_cuda8.0.61_cudnn7.1.2_1  [cuda80]  <a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\" rel=\"nofollow\">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch</a><br>\n[conda] torchvision               0.2.1                    py27_1    <a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\" rel=\"nofollow\">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch</a></p>", "body_text": "Issue description\nHi, guys. I am trying to train a resnet model on single node and 8 GPUs with DistributedDataParallel. Every thing is ok during the first epoch. However, the script shut down without any error report when the second epoch starts. I have tried to track the code and find that the code stops at:\nfor batch_idx, (data, label) in enumerate(train_loader, 0):\nMeanwhile, I create a small version dataset with 34 classes, and the error is gone.\nHere is my command line:\npython -m torch.distributed.launch --nproc_per_node=8 train.py\nAny help will be appreciate.\nCode example\nimport argparse,os,time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.distributed as dist\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport models\nfrom util import *\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--start_epoch', type=int, default=1, help='start epoch number')\nparser.add_argument('--epoch', type=int, default=25, help='number of epochs to train for')\nparser.add_argument('--lr', type=float, default=0.1, help='learning rate, default=0.1')\nparser.add_argument('--momentum', type=float, default=0.9, help='momentum, default=0.9')\nparser.add_argument('--weight_decay', type=float, default=0.0002, help='weight_decay, default=0.0002')\nparser.add_argument('--batch_s', type=int, default=64, help='input batch size')\nparser.add_argument('--grid_s', type=int, default=8, help='grid size')\nparser.add_argument('--data', type=str, default='../vgg2data', help='data directory')\nparser.add_argument('--workers', type=int, default=1, help='number of data loading workers')\nparser.add_argument('--output_dir', type=str, default='./output/', help='model_saving directory')\nparser.add_argument('--resume', type=str, default='', help='resume')\nparser.add_argument(\"--display_interval\", type=int, default=50)\nparser.add_argument(\"--local_rank\", type=int)\nopt = parser.parse_args()\ntorch.cuda.set_device(opt.local_rank)\n\ndist.init_process_group(backend='nccl', init_method='env://', world_size=8)\n\ntrain_dir = os.path.join(opt.data, 'train')\ntrain_dataset = datasets.ImageFolder(\n    train_dir,\n    transforms.Compose([transforms.ToTensor()])\n)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=opt.batch_s,\n        num_workers=opt.workers,\n        pin_memory=True,\n        shuffle=False,\n        sampler=train_sampler\n    )\n\ninput_size = (opt.batch_s, 3, 128, 128)\nnum_classes = 9092\n#num_classes = 34\nmodel = models.se_resnet34_v3(input_size, opt.grid_s, num_classes)\nif opt.resume:\n    if os.path.isfile(opt.resume):\n        print(\"=> loading checkpoint '{}'\".format(opt.resume))\n        checkpoint = torch.load(opt.resume)\n        model.load_state_dict(checkpoint['state_dict'])\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\n            .format(opt.resume, checkpoint['epoch']))\nmodel.cuda()\nmodel = torch.nn.parallel.DistributedDataParallel(model,\\\n    device_ids=[opt.local_rank], output_device=opt.local_rank)\n\noptimizer = optim.SGD([\n        {'params': get_parameters(model, bias=False)},\n        {'params': get_parameters(model, bias=True), 'lr':opt.lr * 2, 'weight_decay': 0},\n        {'params': get_parameters(model, bn=True), 'lr':opt.lr * 1.00001001358, 'weight_decay':0}\n    ], lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\nif opt.resume:\n    if os.path.isfile(opt.resume):\n        checkpoint = torch.load(opt.resume)\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, \\\n    milestones=[8,15,18], gamma=0.5)\n\ndef train(epoch):\n    for batch_idx, (data, label) in enumerate(train_loader, 0):\n        optimizer.zero_grad()\n        data, label = data.cuda(), label.cuda()\n        output, grid = model(data)\n        nll_loss = F.nll_loss(output, label)\n        de_loss = deformation_constraint_loss(grid, opt.grid_s)\n        loss =  nll_loss + de_loss\n        loss.backward()\n        optimizer.step()\nfor epoch in range(opt.start_epoch, opt.epoch + 1):\n    train_sampler.set_epoch(epoch)\n    scheduler.step() \n    model.train()\n    train(epoch)\n\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 8.0.44\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\nGPU 4: GeForce GTX 1080 Ti\nGPU 5: GeForce GTX 1080 Ti\nGPU 6: GeForce GTX 1080 Ti\nGPU 7: GeForce GTX 1080 Ti\nNvidia driver version: 390.12\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.1\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] numpydoc (0.8.0)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] cuda80                    1.0                  h205658b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\n[conda] pytorch                   0.4.1           py27_cuda8.0.61_cudnn7.1.2_1  [cuda80]  https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\n[conda] torchvision               0.2.1                    py27_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch", "body": "## Issue description\r\n\r\nHi, guys. I am trying to train a resnet model on single node and 8 GPUs with DistributedDataParallel. Every thing is ok during the first epoch. However, the script shut down without any error report when the second epoch starts. I have tried to track the code and find that the code stops at:\r\n`for batch_idx, (data, label) in enumerate(train_loader, 0):`\r\nMeanwhile, I create a small version dataset with 34 classes, and the error is gone.\r\nHere is my command line:\r\n`python -m torch.distributed.launch --nproc_per_node=8 train.py`\r\nAny help will be appreciate.\r\n## Code example\r\n```\r\nimport argparse,os,time\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nimport torch.distributed as dist\r\nimport torch.utils.data\r\nimport torch.utils.data.distributed\r\nimport torchvision\r\nfrom torchvision import datasets, transforms\r\nimport numpy as np\r\nimport models\r\nfrom util import *\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--start_epoch', type=int, default=1, help='start epoch number')\r\nparser.add_argument('--epoch', type=int, default=25, help='number of epochs to train for')\r\nparser.add_argument('--lr', type=float, default=0.1, help='learning rate, default=0.1')\r\nparser.add_argument('--momentum', type=float, default=0.9, help='momentum, default=0.9')\r\nparser.add_argument('--weight_decay', type=float, default=0.0002, help='weight_decay, default=0.0002')\r\nparser.add_argument('--batch_s', type=int, default=64, help='input batch size')\r\nparser.add_argument('--grid_s', type=int, default=8, help='grid size')\r\nparser.add_argument('--data', type=str, default='../vgg2data', help='data directory')\r\nparser.add_argument('--workers', type=int, default=1, help='number of data loading workers')\r\nparser.add_argument('--output_dir', type=str, default='./output/', help='model_saving directory')\r\nparser.add_argument('--resume', type=str, default='', help='resume')\r\nparser.add_argument(\"--display_interval\", type=int, default=50)\r\nparser.add_argument(\"--local_rank\", type=int)\r\nopt = parser.parse_args()\r\ntorch.cuda.set_device(opt.local_rank)\r\n\r\ndist.init_process_group(backend='nccl', init_method='env://', world_size=8)\r\n\r\ntrain_dir = os.path.join(opt.data, 'train')\r\ntrain_dataset = datasets.ImageFolder(\r\n    train_dir,\r\n    transforms.Compose([transforms.ToTensor()])\r\n)\r\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\ntrain_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=opt.batch_s,\r\n        num_workers=opt.workers,\r\n        pin_memory=True,\r\n        shuffle=False,\r\n        sampler=train_sampler\r\n    )\r\n\r\ninput_size = (opt.batch_s, 3, 128, 128)\r\nnum_classes = 9092\r\n#num_classes = 34\r\nmodel = models.se_resnet34_v3(input_size, opt.grid_s, num_classes)\r\nif opt.resume:\r\n    if os.path.isfile(opt.resume):\r\n        print(\"=> loading checkpoint '{}'\".format(opt.resume))\r\n        checkpoint = torch.load(opt.resume)\r\n        model.load_state_dict(checkpoint['state_dict'])\r\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\r\n            .format(opt.resume, checkpoint['epoch']))\r\nmodel.cuda()\r\nmodel = torch.nn.parallel.DistributedDataParallel(model,\\\r\n    device_ids=[opt.local_rank], output_device=opt.local_rank)\r\n\r\noptimizer = optim.SGD([\r\n        {'params': get_parameters(model, bias=False)},\r\n        {'params': get_parameters(model, bias=True), 'lr':opt.lr * 2, 'weight_decay': 0},\r\n        {'params': get_parameters(model, bn=True), 'lr':opt.lr * 1.00001001358, 'weight_decay':0}\r\n    ], lr=opt.lr, momentum=opt.momentum, weight_decay=opt.weight_decay)\r\nif opt.resume:\r\n    if os.path.isfile(opt.resume):\r\n        checkpoint = torch.load(opt.resume)\r\n        optimizer.load_state_dict(checkpoint['optimizer'])\r\n\r\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, \\\r\n    milestones=[8,15,18], gamma=0.5)\r\n\r\ndef train(epoch):\r\n    for batch_idx, (data, label) in enumerate(train_loader, 0):\r\n        optimizer.zero_grad()\r\n        data, label = data.cuda(), label.cuda()\r\n        output, grid = model(data)\r\n        nll_loss = F.nll_loss(output, label)\r\n        de_loss = deformation_constraint_loss(grid, opt.grid_s)\r\n        loss =  nll_loss + de_loss\r\n        loss.backward()\r\n        optimizer.step()\r\nfor epoch in range(opt.start_epoch, opt.epoch + 1):\r\n    train_sampler.set_epoch(epoch)\r\n    scheduler.step() \r\n    model.train()\r\n    train(epoch)\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.12\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.1\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda80                    1.0                  h205658b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\r\n[conda] pytorch                   0.4.1           py27_cuda8.0.61_cudnn7.1.2_1  [cuda80]  https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\r\n[conda] torchvision               0.2.1                    py27_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\r\n"}