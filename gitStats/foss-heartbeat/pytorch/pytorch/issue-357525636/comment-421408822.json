{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/421408822", "html_url": "https://github.com/pytorch/pytorch/issues/11327#issuecomment-421408822", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11327", "id": 421408822, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTQwODgyMg==", "user": {"login": "wangdongxuking61", "id": 9778128, "node_id": "MDQ6VXNlcjk3NzgxMjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/9778128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangdongxuking61", "html_url": "https://github.com/wangdongxuking61", "followers_url": "https://api.github.com/users/wangdongxuking61/followers", "following_url": "https://api.github.com/users/wangdongxuking61/following{/other_user}", "gists_url": "https://api.github.com/users/wangdongxuking61/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangdongxuking61/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangdongxuking61/subscriptions", "organizations_url": "https://api.github.com/users/wangdongxuking61/orgs", "repos_url": "https://api.github.com/users/wangdongxuking61/repos", "events_url": "https://api.github.com/users/wangdongxuking61/events{/privacy}", "received_events_url": "https://api.github.com/users/wangdongxuking61/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-14T16:15:12Z", "updated_at": "2018-09-14T16:15:12Z", "author_association": "NONE", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9778128\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wangdongxuking61\">@wangdongxuking61</a> Thank you, I'm running this locally now the Pytorch dockerhub container (version 0.4.1). I do get an error at the end of the first epoch, but it's not silent:</p>\n<pre><code>ConnectionResetError: [Errno 104] Connection reset by peer\n</code></pre>\n<p>25 is an unusually large number of workers. There's no point having more workers than the number of (logical) cores on the machine, right?</p>\n<p>As an initial sanity check, I'm going to try two things: 1. Run with fewer workers per process. 2. Run with top-of-tree pytorch.</p>\n</blockquote>\n<p>Now I tell my story...</p>\n<p>At first, I want to run apex/example/imagenet/main.py with <code>fp16</code> on my server(have 2 V100, 56 logical core).</p>\n<p>But I always got the <code>ConnectionResetError: [Errno 104] Connection reset by peer</code>and <code>EOFError</code></p>\n<p>at the last several iterations in first epoch.</p>\n<p>So I go to see the pytorch's issues about <code>dataloader.py</code> . The <a href=\"https://github.com/pytorch/pytorch/pull/10366\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10366/hovercard\">issue 10366</a> is similar to mine. So i update <code>dataloader.py</code>to the latest version.  Then I got  <code>shut down on second epoch</code>error (maybe some other reason cause the error, i can't remembered clearly, because i do a lot attempt to avoid the <code>ConnectionResetError</code> error...). Then i report my problem at this issue.</p>\n<p>Today, I can't reproduce <code>shut down on second epoch</code> error again now, but i am sure i have met it. Now I only got the first error:<code>ConnectionResetError: [Errno 104] Connection reset by peer</code>and <code>EOFError</code></p>\n<p>So I do a lot tests...</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>#server</th>\n<th>#gpu per server</th>\n<th>torch version(python3.6)</th>\n<th>batch size per gpu</th>\n<th>float precision(not important)</th>\n<th>result</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>test1</td>\n<td>1</td>\n<td>2</td>\n<td>0.4.1</td>\n<td>256</td>\n<td>fp16</td>\n<td>error</td>\n</tr>\n<tr>\n<td>test2</td>\n<td>1</td>\n<td>2</td>\n<td>0.4.0</td>\n<td>256</td>\n<td>fp16</td>\n<td>OK</td>\n</tr>\n<tr>\n<td>test3</td>\n<td>1</td>\n<td>2</td>\n<td>0.4.1+replace dataloader.py by latest one in top-of-tree</td>\n<td>256</td>\n<td>fp16</td>\n<td>error</td>\n</tr>\n<tr>\n<td>test4</td>\n<td>1</td>\n<td>1/4/8</td>\n<td>0.4.1</td>\n<td>8/16/32/64/128</td>\n<td>fp16/fp32</td>\n<td>OK</td>\n</tr>\n<tr>\n<td>test5</td>\n<td>2</td>\n<td>1</td>\n<td>0.4.1</td>\n<td>8/16/32/64/128</td>\n<td>fp16/fp32</td>\n<td>error</td>\n</tr>\n<tr>\n<td>test6</td>\n<td>2</td>\n<td>2/4/8</td>\n<td>0.4.1</td>\n<td>8/16/32/64/128</td>\n<td>fp16/fp32</td>\n<td>OK</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Ps1: test1,2,3 are only tested on apex's DDP. test4,5,6 are tested on both <code>apex' DDP</code>and <code>torch.nn.parallel.DistributedDataParallel</code>. The result is unrelated with these two DDP.</li>\n<li>Ps2: i use mp.set_start_method('forkserver'), it is useless. And i also change python3.6 to 2.7,3.7, it is useless too...</li>\n</ul>\n<p>the command is:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test1</span>\npython -m torch.distributed.launch --nproc_per_node=2 main.py --fp16 --arch resnet18 --epochs 90 --workers 25 --batch-size=256 /imagenet\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> test2 test3</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the same as test1</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> test4</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> should change: nproc_per_node, batch-size, --fp16</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> test5, test6</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> should change: nproc_per_node, batch-size, --fp16</span>\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>192.168.1.1<span class=\"pl-pds\">\"</span></span> --master_port=1234 main.py --fp16 --arch resnet18 --epochs 90 --workers 25 --batch-size=256 /imagenet</pre></div>\n<p><strong>My personal Conclusion, the error will only happen in a certain situation:</strong></p>\n<ul>\n<li>torch version &gt;=0.4.1(related to dataloader.py)</li>\n<li>DDP process number=2(two gpus in 1 server, or 2 servers each have a gpu)</li>\n</ul>\n<p>Another two pieces of information:</p>\n<ul>\n<li>\n<p><code>python torch-0.4.1/test/test_dataloader.py</code> will also cause <code>ConnectionResetError: [Errno 104]</code>(torch0.4.1 installed in python env).</p>\n<p>But torch-0.4.0/test/test_dataloader.py is OK(torch0.4.0 installed in python env).</p>\n</li>\n<li>\n<p>the stopped iteration number is associated with the worker number, like a linear relationship</p>\n</li>\n</ul>", "body_text": "@wangdongxuking61 Thank you, I'm running this locally now the Pytorch dockerhub container (version 0.4.1). I do get an error at the end of the first epoch, but it's not silent:\nConnectionResetError: [Errno 104] Connection reset by peer\n\n25 is an unusually large number of workers. There's no point having more workers than the number of (logical) cores on the machine, right?\nAs an initial sanity check, I'm going to try two things: 1. Run with fewer workers per process. 2. Run with top-of-tree pytorch.\n\nNow I tell my story...\nAt first, I want to run apex/example/imagenet/main.py with fp16 on my server(have 2 V100, 56 logical core).\nBut I always got the ConnectionResetError: [Errno 104] Connection reset by peerand EOFError\nat the last several iterations in first epoch.\nSo I go to see the pytorch's issues about dataloader.py . The issue 10366 is similar to mine. So i update dataloader.pyto the latest version.  Then I got  shut down on second epocherror (maybe some other reason cause the error, i can't remembered clearly, because i do a lot attempt to avoid the ConnectionResetError error...). Then i report my problem at this issue.\nToday, I can't reproduce shut down on second epoch error again now, but i am sure i have met it. Now I only got the first error:ConnectionResetError: [Errno 104] Connection reset by peerand EOFError\nSo I do a lot tests...\n\n\n\n\n#server\n#gpu per server\ntorch version(python3.6)\nbatch size per gpu\nfloat precision(not important)\nresult\n\n\n\n\ntest1\n1\n2\n0.4.1\n256\nfp16\nerror\n\n\ntest2\n1\n2\n0.4.0\n256\nfp16\nOK\n\n\ntest3\n1\n2\n0.4.1+replace dataloader.py by latest one in top-of-tree\n256\nfp16\nerror\n\n\ntest4\n1\n1/4/8\n0.4.1\n8/16/32/64/128\nfp16/fp32\nOK\n\n\ntest5\n2\n1\n0.4.1\n8/16/32/64/128\nfp16/fp32\nerror\n\n\ntest6\n2\n2/4/8\n0.4.1\n8/16/32/64/128\nfp16/fp32\nOK\n\n\n\n\nPs1: test1,2,3 are only tested on apex's DDP. test4,5,6 are tested on both apex' DDPand torch.nn.parallel.DistributedDataParallel. The result is unrelated with these two DDP.\nPs2: i use mp.set_start_method('forkserver'), it is useless. And i also change python3.6 to 2.7,3.7, it is useless too...\n\nthe command is:\n# test1\npython -m torch.distributed.launch --nproc_per_node=2 main.py --fp16 --arch resnet18 --epochs 90 --workers 25 --batch-size=256 /imagenet\n\n# test2 test3\n# the same as test1\n\n# test4\n# should change: nproc_per_node, batch-size, --fp16\n\n# test5, test6\n# should change: nproc_per_node, batch-size, --fp16\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 main.py --fp16 --arch resnet18 --epochs 90 --workers 25 --batch-size=256 /imagenet\nMy personal Conclusion, the error will only happen in a certain situation:\n\ntorch version >=0.4.1(related to dataloader.py)\nDDP process number=2(two gpus in 1 server, or 2 servers each have a gpu)\n\nAnother two pieces of information:\n\n\npython torch-0.4.1/test/test_dataloader.py will also cause ConnectionResetError: [Errno 104](torch0.4.1 installed in python env).\nBut torch-0.4.0/test/test_dataloader.py is OK(torch0.4.0 installed in python env).\n\n\nthe stopped iteration number is associated with the worker number, like a linear relationship", "body": "> @wangdongxuking61 Thank you, I'm running this locally now the Pytorch dockerhub container (version 0.4.1). I do get an error at the end of the first epoch, but it's not silent:\r\n> \r\n> ```\r\n> ConnectionResetError: [Errno 104] Connection reset by peer\r\n> ```\r\n> 25 is an unusually large number of workers. There's no point having more workers than the number of (logical) cores on the machine, right?\r\n> \r\n> As an initial sanity check, I'm going to try two things: 1. Run with fewer workers per process. 2. Run with top-of-tree pytorch.\r\n\r\nNow I tell my story...\r\n\r\nAt first, I want to run apex/example/imagenet/main.py with `fp16` on my server(have 2 V100, 56 logical core).\r\n\r\nBut I always got the `ConnectionResetError: [Errno 104] Connection reset by peer`and `EOFError`\r\n\r\nat the last several iterations in first epoch.\r\n\r\nSo I go to see the pytorch's issues about `dataloader.py` . The [issue 10366](https://github.com/pytorch/pytorch/pull/10366) is similar to mine. So i update `dataloader.py`to the latest version.  Then I got  `shut down on second epoch`error (maybe some other reason cause the error, i can't remembered clearly, because i do a lot attempt to avoid the `ConnectionResetError` error...). Then i report my problem at this issue. \r\n\r\nToday, I can't reproduce `shut down on second epoch` error again now, but i am sure i have met it. Now I only got the first error:`ConnectionResetError: [Errno 104] Connection reset by peer`and `EOFError`\r\n\r\n\r\n\r\nSo I do a lot tests...\r\n\r\n|       | #server | #gpu per server | torch version(python3.6)                                 | batch size per gpu | float precision(not important) | result |\r\n| ----- | ------- | --------------- | -------------------------------------------------------- | ------------------ | ------------------------------ | ------ |\r\n| test1 | 1       | 2               | 0.4.1                                                    | 256                | fp16                           | error  |\r\n| test2 | 1       | 2               | 0.4.0                                                    | 256                | fp16                           | OK     |\r\n| test3 | 1       | 2               | 0.4.1+replace dataloader.py by latest one in top-of-tree | 256                | fp16                           | error  |\r\n| test4 | 1       | 1/4/8           | 0.4.1                                                    | 8/16/32/64/128     | fp16/fp32                      | OK     |\r\n| test5 | 2       | 1               | 0.4.1                                                    | 8/16/32/64/128     | fp16/fp32                      | error  |\r\n| test6 | 2       | 2/4/8           | 0.4.1                                                    | 8/16/32/64/128     | fp16/fp32                      | OK     |\r\n\r\n- Ps1: test1,2,3 are only tested on apex's DDP. test4,5,6 are tested on both `apex' DDP`and `torch.nn.parallel.DistributedDataParallel`. The result is unrelated with these two DDP.\r\n- Ps2: i use mp.set_start_method('forkserver'), it is useless. And i also change python3.6 to 2.7,3.7, it is useless too...\r\n\r\n\r\n\r\nthe command is:\r\n\r\n```bash\r\n# test1\r\npython -m torch.distributed.launch --nproc_per_node=2 main.py --fp16 --arch resnet18 --epochs 90 --workers 25 --batch-size=256 /imagenet\r\n\r\n# test2 test3\r\n# the same as test1\r\n\r\n# test4\r\n# should change: nproc_per_node, batch-size, --fp16\r\n\r\n# test5, test6\r\n# should change: nproc_per_node, batch-size, --fp16\r\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 main.py --fp16 --arch resnet18 --epochs 90 --workers 25 --batch-size=256 /imagenet\r\n```\r\n\r\n**My personal Conclusion, the error will only happen in a certain situation:**\r\n\r\n- torch version >=0.4.1(related to dataloader.py)\r\n- DDP process number=2(two gpus in 1 server, or 2 servers each have a gpu)\r\n\r\nAnother two pieces of information:\r\n\r\n- `python torch-0.4.1/test/test_dataloader.py` will also cause `ConnectionResetError: [Errno 104]`(torch0.4.1 installed in python env). \r\n\r\n  But torch-0.4.0/test/test_dataloader.py is OK(torch0.4.0 installed in python env).\r\n\r\n- the stopped iteration number is associated with the worker number, like a linear relationship"}