{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179643092", "pull_request_review_id": 109927646, "id": 179643092, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTY0MzA5Mg==", "diff_hunk": "@@ -4108,6 +4108,44 @@\n \n \"\"\")\n \n+add_docstr(torch.randint,\n+           r\"\"\"\n+randint(low=0, high, sizes, out=None) -> Tensor\n+\n+Returns a tensor filled with random integers generated uniformly\n+between :attr:`low` (inclusive) and :attr:`high` (exclusive).\n+\n+The shape of the tensor is defined by the variable argument :attr:`sizes`.\n+\n+Args:\n+    low (int, optional): Lowest (positive) integer to be drawn from the distribution. Default: 0.", "path": "torch/_torch_docs.py", "position": null, "original_position": 14, "commit_id": "68aab08a0b3727a1f306e3609c3b7af3781b5b7d", "original_commit_id": "a2cca5de9158e44258ef64727ef599d8d20c5d1d", "user": {"login": "Naman-ntc", "id": 23135406, "node_id": "MDQ6VXNlcjIzMTM1NDA2", "avatar_url": "https://avatars2.githubusercontent.com/u/23135406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Naman-ntc", "html_url": "https://github.com/Naman-ntc", "followers_url": "https://api.github.com/users/Naman-ntc/followers", "following_url": "https://api.github.com/users/Naman-ntc/following{/other_user}", "gists_url": "https://api.github.com/users/Naman-ntc/gists{/gist_id}", "starred_url": "https://api.github.com/users/Naman-ntc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Naman-ntc/subscriptions", "organizations_url": "https://api.github.com/users/Naman-ntc/orgs", "repos_url": "https://api.github.com/users/Naman-ntc/repos", "events_url": "https://api.github.com/users/Naman-ntc/events{/privacy}", "received_events_url": "https://api.github.com/users/Naman-ntc/received_events", "type": "User", "site_admin": false}, "body": "Yeah so in the develop mode \r\n> >>> a = torch.randn(5,6)\r\n>>> a.random_(-10,-1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 84, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 306, in _str\r\n    strt = _matrix_str(self)\r\n  File \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 215, in _matrix_str\r\n    min_sz=5 if not print_full_mat else 0)\r\n  File \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 95, in _number_format\r\n    if value != math.ceil(value.item()):\r\nRuntimeError: Overflow when unpacking long\r\n\r\nSo I thought random_ works only for positive numbers. But  just now checked it's overflow problem and works for torch.LongTensor. \r\nBy the way isn't it random_ implementation issue. In default settings for FloatTensors or even DoubleTensors it would generate this message? \r\nI'll remove positive but I should add warning of overflow then maybe?", "created_at": "2018-04-06T01:46:28Z", "updated_at": "2018-11-23T15:41:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/6136#discussion_r179643092", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6136", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/179643092"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6136#discussion_r179643092"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6136"}}, "body_html": "<p>Yeah so in the develop mode</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>a = torch.randn(5,6)<br>\na.random_(-10,-1)<br>\nTraceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 84, in <strong>repr</strong><br>\nreturn torch._tensor_str._str(self)<br>\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 306, in _str<br>\nstrt = _matrix_str(self)<br>\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 215, in _matrix_str<br>\nmin_sz=5 if not print_full_mat else 0)<br>\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 95, in _number_format<br>\nif value != math.ceil(value.item()):<br>\nRuntimeError: Overflow when unpacking long</p>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>So I thought random_ works only for positive numbers. But  just now checked it's overflow problem and works for torch.LongTensor.<br>\nBy the way isn't it random_ implementation issue. In default settings for FloatTensors or even DoubleTensors it would generate this message?<br>\nI'll remove positive but I should add warning of overflow then maybe?</p>", "body_text": "Yeah so in the develop mode\n\n\n\n\na = torch.randn(5,6)\na.random_(-10,-1)\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 84, in repr\nreturn torch._tensor_str._str(self)\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 306, in _str\nstrt = _matrix_str(self)\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 215, in _matrix_str\nmin_sz=5 if not print_full_mat else 0)\nFile \"/home/naman/PythonPackages/Source-Pytorch/lib/python3.6/site-packages/torch/_tensor_str.py\", line 95, in _number_format\nif value != math.ceil(value.item()):\nRuntimeError: Overflow when unpacking long\n\n\n\n\nSo I thought random_ works only for positive numbers. But  just now checked it's overflow problem and works for torch.LongTensor.\nBy the way isn't it random_ implementation issue. In default settings for FloatTensors or even DoubleTensors it would generate this message?\nI'll remove positive but I should add warning of overflow then maybe?", "in_reply_to_id": 179578927}