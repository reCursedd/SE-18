{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/417834946", "html_url": "https://github.com/pytorch/pytorch/pull/11165#issuecomment-417834946", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11165", "id": 417834946, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzgzNDk0Ng==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-01T05:40:25Z", "updated_at": "2018-09-01T05:40:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Nice work. But I want to comment on this:</p>\n<blockquote>\n<p>When there is some large weight in the network, one optimization people can always do is set parameter.grad to None instead of zero_grad.</p>\n</blockquote>\n<p>Actually you probably don't want to do that, at least not in current code base. Our optimization code assumes that parameters with .grad being <code>None</code> are new parameters, and not only skips gradient steps, but also skips things like weight decay, running stats estimation as well. So you are getting incorrect behavior here.</p>", "body_text": "Nice work. But I want to comment on this:\n\nWhen there is some large weight in the network, one optimization people can always do is set parameter.grad to None instead of zero_grad.\n\nActually you probably don't want to do that, at least not in current code base. Our optimization code assumes that parameters with .grad being None are new parameters, and not only skips gradient steps, but also skips things like weight decay, running stats estimation as well. So you are getting incorrect behavior here.", "body": "Nice work. But I want to comment on this: \r\n> When there is some large weight in the network, one optimization people can always do is set parameter.grad to None instead of zero_grad. \r\n\r\nActually you probably don't want to do that, at least not in current code base. Our optimization code assumes that parameters with .grad being `None` are new parameters, and not only skips gradient steps, but also skips things like weight decay, running stats estimation as well. So you are getting incorrect behavior here. "}