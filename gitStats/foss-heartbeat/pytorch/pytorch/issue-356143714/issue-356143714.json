{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11165", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11165/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11165/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11165/events", "html_url": "https://github.com/pytorch/pytorch/pull/11165", "id": 356143714, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEyNTA5NDkz", "number": 11165, "title": "remove unnecessary clone() when .grad is None", "user": {"login": "FDecaYed", "id": 17164548, "node_id": "MDQ6VXNlcjE3MTY0NTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/17164548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FDecaYed", "html_url": "https://github.com/FDecaYed", "followers_url": "https://api.github.com/users/FDecaYed/followers", "following_url": "https://api.github.com/users/FDecaYed/following{/other_user}", "gists_url": "https://api.github.com/users/FDecaYed/gists{/gist_id}", "starred_url": "https://api.github.com/users/FDecaYed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FDecaYed/subscriptions", "organizations_url": "https://api.github.com/users/FDecaYed/orgs", "repos_url": "https://api.github.com/users/FDecaYed/repos", "events_url": "https://api.github.com/users/FDecaYed/events{/privacy}", "received_events_url": "https://api.github.com/users/FDecaYed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-08-31T22:49:28Z", "updated_at": "2018-11-23T15:50:48Z", "closed_at": "2018-09-08T02:42:52Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11165", "html_url": "https://github.com/pytorch/pytorch/pull/11165", "diff_url": "https://github.com/pytorch/pytorch/pull/11165.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11165.patch"}, "body_html": "<p>Currently gradient is copied into .grad if it is None. This PR aim to remove the copy when it is not absolutely needed.</p>\n<p>It is generally an improvement of speed and memory usage. And here is a case it may help a lot:<br>\nNormally, people do optimizer.zero_grad() every minibatch before backward. It will translate into a memset, and later a point-wise add.<br>\nWhen there is some large weight in the network, one optimization people can always do is set parameter.grad to None instead of zero_grad. This will remove memset and change point-wise add to a memcpy.<br>\nHere is result running following script on V100 GPU. It is 100 iterations of forward/backward/zero_grad on single 1-billion word benchmark size embedding.<br>\n<code>Zero grad: 2.123847723007202</code><br>\n<code>None grad: 1.3342866897583008</code></p>\n<p>With the backend change of this PR, the unnecessary memcpy is removed, thus further speed up is achieved.<br>\n<code>Zero grad: 2.124978542327881</code><br>\n<code>None grad: 0.4396955966949463</code></p>\n<p><a href=\"https://github.com/pytorch/pytorch/files/2341800/benchmark.txt\">benchmark.txt</a></p>\n<p>Some details on the code change:<br>\n.detach() is used because we need to get rid of new_grad being a view without copy data. This should be safe in first-order only mode.<br>\ndata need to be contiguous, otherwise <code>grad_variable.data() += new_grad.data();</code> below will fail.<br>\nOnly the last variable that has reference to the temp gradient will grab its buffer.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7799218\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mcarilli\">@mcarilli</a>  and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> helped on finalizing this PR.</p>", "body_text": "Currently gradient is copied into .grad if it is None. This PR aim to remove the copy when it is not absolutely needed.\nIt is generally an improvement of speed and memory usage. And here is a case it may help a lot:\nNormally, people do optimizer.zero_grad() every minibatch before backward. It will translate into a memset, and later a point-wise add.\nWhen there is some large weight in the network, one optimization people can always do is set parameter.grad to None instead of zero_grad. This will remove memset and change point-wise add to a memcpy.\nHere is result running following script on V100 GPU. It is 100 iterations of forward/backward/zero_grad on single 1-billion word benchmark size embedding.\nZero grad: 2.123847723007202\nNone grad: 1.3342866897583008\nWith the backend change of this PR, the unnecessary memcpy is removed, thus further speed up is achieved.\nZero grad: 2.124978542327881\nNone grad: 0.4396955966949463\nbenchmark.txt\nSome details on the code change:\n.detach() is used because we need to get rid of new_grad being a view without copy data. This should be safe in first-order only mode.\ndata need to be contiguous, otherwise grad_variable.data() += new_grad.data(); below will fail.\nOnly the last variable that has reference to the temp gradient will grab its buffer.\n@ngimel, @mcarilli  and @mruberry helped on finalizing this PR.", "body": "Currently gradient is copied into .grad if it is None. This PR aim to remove the copy when it is not absolutely needed.\r\n\r\nIt is generally an improvement of speed and memory usage. And here is a case it may help a lot:\r\nNormally, people do optimizer.zero_grad() every minibatch before backward. It will translate into a memset, and later a point-wise add.\r\nWhen there is some large weight in the network, one optimization people can always do is set parameter.grad to None instead of zero_grad. This will remove memset and change point-wise add to a memcpy.\r\nHere is result running following script on V100 GPU. It is 100 iterations of forward/backward/zero_grad on single 1-billion word benchmark size embedding.\r\n`Zero grad: 2.123847723007202`\r\n`None grad: 1.3342866897583008`\r\n\r\nWith the backend change of this PR, the unnecessary memcpy is removed, thus further speed up is achieved.\r\n`Zero grad: 2.124978542327881`\r\n`None grad: 0.4396955966949463`\r\n\r\n[benchmark.txt](https://github.com/pytorch/pytorch/files/2341800/benchmark.txt)\r\n\r\nSome details on the code change:\r\n.detach() is used because we need to get rid of new_grad being a view without copy data. This should be safe in first-order only mode.\r\ndata need to be contiguous, otherwise `grad_variable.data() += new_grad.data();` below will fail.\r\nOnly the last variable that has reference to the temp gradient will grab its buffer.\r\n\r\n@ngimel, @mcarilli  and @mruberry helped on finalizing this PR."}