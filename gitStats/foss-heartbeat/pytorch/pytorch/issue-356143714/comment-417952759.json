{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/417952759", "html_url": "https://github.com/pytorch/pytorch/pull/11165#issuecomment-417952759", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11165", "id": 417952759, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzk1Mjc1OQ==", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-02T19:05:05Z", "updated_at": "2018-09-02T19:05:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> yes, imo the easiest way to do it is to register a hook on the variable that does nothing but print the data_ptr() of its argument (and doesnt return anything). Later, after backward is done, compare this printed data_ptr to variable.grad.data_ptr().  If the optimization worked, the data_ptrs should match.</p>\n<p>Of course if the variable also has other hooks registered on it, which may modify the gradient out of place and return something new, this is no longer guaranteed, but for a minimal working test, there\u2019s no reason to register other hooks.</p>", "body_text": "@ezyang yes, imo the easiest way to do it is to register a hook on the variable that does nothing but print the data_ptr() of its argument (and doesnt return anything). Later, after backward is done, compare this printed data_ptr to variable.grad.data_ptr().  If the optimization worked, the data_ptrs should match.\nOf course if the variable also has other hooks registered on it, which may modify the gradient out of place and return something new, this is no longer guaranteed, but for a minimal working test, there\u2019s no reason to register other hooks.", "body": "@ezyang yes, imo the easiest way to do it is to register a hook on the variable that does nothing but print the data_ptr() of its argument (and doesnt return anything). Later, after backward is done, compare this printed data_ptr to variable.grad.data_ptr().  If the optimization worked, the data_ptrs should match.\r\n\r\nOf course if the variable also has other hooks registered on it, which may modify the gradient out of place and return something new, this is no longer guaranteed, but for a minimal working test, there\u2019s no reason to register other hooks."}