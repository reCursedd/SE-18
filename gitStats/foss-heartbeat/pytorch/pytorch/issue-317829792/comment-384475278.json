{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384475278", "html_url": "https://github.com/pytorch/pytorch/issues/6976#issuecomment-384475278", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6976", "id": 384475278, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDQ3NTI3OA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T00:32:44Z", "updated_at": "2018-04-26T00:32:44Z", "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li>Your <code>grad_output</code> is <code>0</code>. So you are calculating <code>dl/df * df/dx</code>, where <code>dl/df = 0</code>. I don't see how the result should be non-zero. If you want to find jacobian, you need to do autograd for each output value.</li>\n<li>you don't need to set <code>requires_grad</code> to <code>True</code> every where. In your case <code>f</code> already has this because both <code>a0</code> and <code>x</code> used to compute it <code>requries_grad</code>. And <code>df_dx</code> isn't involved in any computation, so it doesn't need either.</li>\n</ol>", "body_text": "Your grad_output is 0. So you are calculating dl/df * df/dx, where dl/df = 0. I don't see how the result should be non-zero. If you want to find jacobian, you need to do autograd for each output value.\nyou don't need to set requires_grad to True every where. In your case f already has this because both a0 and x used to compute it requries_grad. And df_dx isn't involved in any computation, so it doesn't need either.", "body": "1. Your `grad_output` is `0`. So you are calculating `dl/df * df/dx`, where `dl/df = 0`. I don't see how the result should be non-zero. If you want to find jacobian, you need to do autograd for each output value.\r\n2. you don't need to set `requires_grad` to `True` every where. In your case `f` already has this because both `a0` and `x` used to compute it `requries_grad`. And `df_dx` isn't involved in any computation, so it doesn't need either."}