{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6976", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6976/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6976/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6976/events", "html_url": "https://github.com/pytorch/pytorch/issues/6976", "id": 317829792, "node_id": "MDU6SXNzdWUzMTc4Mjk3OTI=", "number": 6976, "title": "autograd grad_outputs are zero", "user": {"login": "euhruska", "id": 9983894, "node_id": "MDQ6VXNlcjk5ODM4OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/9983894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/euhruska", "html_url": "https://github.com/euhruska", "followers_url": "https://api.github.com/users/euhruska/followers", "following_url": "https://api.github.com/users/euhruska/following{/other_user}", "gists_url": "https://api.github.com/users/euhruska/gists{/gist_id}", "starred_url": "https://api.github.com/users/euhruska/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/euhruska/subscriptions", "organizations_url": "https://api.github.com/users/euhruska/orgs", "repos_url": "https://api.github.com/users/euhruska/repos", "events_url": "https://api.github.com/users/euhruska/events{/privacy}", "received_events_url": "https://api.github.com/users/euhruska/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-25T23:41:12Z", "updated_at": "2018-04-26T00:32:45Z", "closed_at": "2018-04-26T00:32:44Z", "author_association": "NONE", "body_html": "<p>I want to take the gradient of a function (vector) by x for each of the x values. The purpose is that this gradient is part of the loss function.<br>\nI tried autograd.grad with grad_outputs, but it gives zero gradients (df_dx).  They should be non-zero, so either I made a mistake or this is a bug.</p>\n<pre><code>a0=torch.tensor(1).double().cuda().requires_grad_(True)\nx=torch.randn(10).double().cuda().requires_grad_(True)\nf=torch.tanh(x-a0).requires_grad_(True)\ndf_dx=x=torch.zeros(10).double().cuda().requires_grad_(True)\ngrad_params = torch.autograd.grad(f, [x], create_graph=True,grad_outputs=[df_dx],allow_unused=True)\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Red Hat Enterprise Linux Workstation release 7.4 (Maipo)<br>\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)<br>\nCMake version: version 2.8.12.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: GeForce GTX 1080<br>\nNvidia driver version: 387.26<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.13.3)<br>\n[pip] numpydoc (0.7.0)<br>\n[pip] torch (0.4.0)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "I want to take the gradient of a function (vector) by x for each of the x values. The purpose is that this gradient is part of the loss function.\nI tried autograd.grad with grad_outputs, but it gives zero gradients (df_dx).  They should be non-zero, so either I made a mistake or this is a bug.\na0=torch.tensor(1).double().cuda().requires_grad_(True)\nx=torch.randn(10).double().cuda().requires_grad_(True)\nf=torch.tanh(x-a0).requires_grad_(True)\ndf_dx=x=torch.zeros(10).double().cuda().requires_grad_(True)\ngrad_params = torch.autograd.grad(f, [x], create_graph=True,grad_outputs=[df_dx],allow_unused=True)\n\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Red Hat Enterprise Linux Workstation release 7.4 (Maipo)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: version 2.8.12.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: GeForce GTX 1080\nNvidia driver version: 387.26\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.13.3)\n[pip] numpydoc (0.7.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "I want to take the gradient of a function (vector) by x for each of the x values. The purpose is that this gradient is part of the loss function.\r\nI tried autograd.grad with grad_outputs, but it gives zero gradients (df_dx).  They should be non-zero, so either I made a mistake or this is a bug.\r\n```\r\na0=torch.tensor(1).double().cuda().requires_grad_(True)\r\nx=torch.randn(10).double().cuda().requires_grad_(True)\r\nf=torch.tanh(x-a0).requires_grad_(True)\r\ndf_dx=x=torch.zeros(10).double().cuda().requires_grad_(True)\r\ngrad_params = torch.autograd.grad(f, [x], create_graph=True,grad_outputs=[df_dx],allow_unused=True)\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Red Hat Enterprise Linux Workstation release 7.4 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 387.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.3)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n"}