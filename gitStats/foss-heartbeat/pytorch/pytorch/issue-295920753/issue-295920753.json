{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5157", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5157/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5157/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5157/events", "html_url": "https://github.com/pytorch/pytorch/issues/5157", "id": 295920753, "node_id": "MDU6SXNzdWUyOTU5MjA3NTM=", "number": 5157, "title": "BCELoss - weight parameter shape incorrect", "user": {"login": "ptrblck", "id": 11662379, "node_id": "MDQ6VXNlcjExNjYyMzc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11662379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptrblck", "html_url": "https://github.com/ptrblck", "followers_url": "https://api.github.com/users/ptrblck/followers", "following_url": "https://api.github.com/users/ptrblck/following{/other_user}", "gists_url": "https://api.github.com/users/ptrblck/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptrblck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptrblck/subscriptions", "organizations_url": "https://api.github.com/users/ptrblck/orgs", "repos_url": "https://api.github.com/users/ptrblck/repos", "events_url": "https://api.github.com/users/ptrblck/events{/privacy}", "received_events_url": "https://api.github.com/users/ptrblck/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-02-09T16:03:39Z", "updated_at": "2018-03-15T08:56:46Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>The <code>weight</code> parameter of <code>BCELoss</code> seems to be incorrectly defined when using a multi-dimensional input and target. Related <a href=\"https://discuss.pytorch.org/t/binary-cross-entropy-weights/13299\" rel=\"nofollow\">forum thread</a>.</p>\n<p>The documentation defines <code>weight</code> as:</p>\n<blockquote>\n<p>If given, has to be a Tensor of size \u201cnbatch\u201d.</p>\n</blockquote>\n<p>However, this example throws an error:</p>\n<pre><code>x = Variable(torch.randn(4, 2, 2))\ny = Variable(torch.Tensor(4, 2, 2).random_(2))\noutput = F.sigmoid(x)\n\n# Create weight according to doc in BCELoss\nweight = torch.randn(4)\ncriterion_weighted = nn.BCELoss(weight=weight)\nloss_weighted = criterion_weighted(output, y) # Error!\n</code></pre>\n<p>A workaround is to <code>unsqueeze</code> the <code>weight</code> tensor to match the number of dimensions:</p>\n<pre><code># Unsqueeze weight tensor\nweight = torch.randn(4, 1, 1)\ncriterion_weighted = nn.BCELoss(weight=weight)\nloss_weighted = criterion_weighted(output, y)\n</code></pre>\n<p>Internally, <code>_infer_size</code> is called and fails in the first code snippet.<br>\nThe second code snippet applied a weighting for each batch element, which is fine.</p>\n<p>Should we automatically unsqueeze the weight tensor, if input and target are multi-dimensional?</p>\n<p>Also, how should we handle class weighting?<br>\nIf we just pass 2 weights as the <code>weight</code> tensor, the code successfully runs, but does not apply class weighting, which might mislead some users:</p>\n<pre><code># Create class weights\nweight = torch.FloatTensor([0.1, 0.9]) \n\n# Internally, weight is expanded as\nsize = _infer_size(weight.size(), y.size())\nweight_expanded = weight.expand(size) \nprint(weight_expanded) # This is not, what we wanted as class weights!\n\ncriterion_weighted = nn.BCELoss(weight=weight)\nloss_weighted = criterion_weighted(output, y)\ncriterion_nonreduced = nn.BCELoss(reduce=False)\nloss_unreduced = criterion_nonreduced(output, y)\nloss_weighted_manual = (Variable(weight_expanded) * loss_unreduced).mean()\n\nif loss_weighted == loss_weighted_manual:\n    print('Class weighting failed')\n</code></pre>\n<p>A workaround would be:</p>\n<pre><code>weight_ = weight[y.data.view(-1).long()].view_as(y)\ncriterion = nn.BCELoss(reduce=False)\nloss = criterion(output, y)\nloss_class_weighted = loss * Variable(weight_)\n</code></pre>\n<p>What is wanted behavior of the <code>weight</code> parameter in <code>BCELoss</code>: class or batch weighting?<br>\nBoth cases have some issues at the moment in my opinion.<br>\nI would like to fix this issue, but I would like to hear some opinions on the right behavior.<br>\nClass weighting would be consistent with other loss functions like <code>NLLLoss</code>, but maybe batch weighting is a more common use case for <code>BCELoss</code>.</p>\n<p>PyTorch version: <code>0.4.0a0+492e26f</code> (installed from source)</p>", "body_text": "The weight parameter of BCELoss seems to be incorrectly defined when using a multi-dimensional input and target. Related forum thread.\nThe documentation defines weight as:\n\nIf given, has to be a Tensor of size \u201cnbatch\u201d.\n\nHowever, this example throws an error:\nx = Variable(torch.randn(4, 2, 2))\ny = Variable(torch.Tensor(4, 2, 2).random_(2))\noutput = F.sigmoid(x)\n\n# Create weight according to doc in BCELoss\nweight = torch.randn(4)\ncriterion_weighted = nn.BCELoss(weight=weight)\nloss_weighted = criterion_weighted(output, y) # Error!\n\nA workaround is to unsqueeze the weight tensor to match the number of dimensions:\n# Unsqueeze weight tensor\nweight = torch.randn(4, 1, 1)\ncriterion_weighted = nn.BCELoss(weight=weight)\nloss_weighted = criterion_weighted(output, y)\n\nInternally, _infer_size is called and fails in the first code snippet.\nThe second code snippet applied a weighting for each batch element, which is fine.\nShould we automatically unsqueeze the weight tensor, if input and target are multi-dimensional?\nAlso, how should we handle class weighting?\nIf we just pass 2 weights as the weight tensor, the code successfully runs, but does not apply class weighting, which might mislead some users:\n# Create class weights\nweight = torch.FloatTensor([0.1, 0.9]) \n\n# Internally, weight is expanded as\nsize = _infer_size(weight.size(), y.size())\nweight_expanded = weight.expand(size) \nprint(weight_expanded) # This is not, what we wanted as class weights!\n\ncriterion_weighted = nn.BCELoss(weight=weight)\nloss_weighted = criterion_weighted(output, y)\ncriterion_nonreduced = nn.BCELoss(reduce=False)\nloss_unreduced = criterion_nonreduced(output, y)\nloss_weighted_manual = (Variable(weight_expanded) * loss_unreduced).mean()\n\nif loss_weighted == loss_weighted_manual:\n    print('Class weighting failed')\n\nA workaround would be:\nweight_ = weight[y.data.view(-1).long()].view_as(y)\ncriterion = nn.BCELoss(reduce=False)\nloss = criterion(output, y)\nloss_class_weighted = loss * Variable(weight_)\n\nWhat is wanted behavior of the weight parameter in BCELoss: class or batch weighting?\nBoth cases have some issues at the moment in my opinion.\nI would like to fix this issue, but I would like to hear some opinions on the right behavior.\nClass weighting would be consistent with other loss functions like NLLLoss, but maybe batch weighting is a more common use case for BCELoss.\nPyTorch version: 0.4.0a0+492e26f (installed from source)", "body": "The `weight` parameter of `BCELoss` seems to be incorrectly defined when using a multi-dimensional input and target. Related [forum thread](https://discuss.pytorch.org/t/binary-cross-entropy-weights/13299).\r\n\r\nThe documentation defines `weight` as:\r\n> If given, has to be a Tensor of size \u201cnbatch\u201d.\r\n\r\nHowever, this example throws an error:\r\n```\r\nx = Variable(torch.randn(4, 2, 2))\r\ny = Variable(torch.Tensor(4, 2, 2).random_(2))\r\noutput = F.sigmoid(x)\r\n\r\n# Create weight according to doc in BCELoss\r\nweight = torch.randn(4)\r\ncriterion_weighted = nn.BCELoss(weight=weight)\r\nloss_weighted = criterion_weighted(output, y) # Error!\r\n```\r\n\r\nA workaround is to `unsqueeze` the `weight` tensor to match the number of dimensions:\r\n```\r\n# Unsqueeze weight tensor\r\nweight = torch.randn(4, 1, 1)\r\ncriterion_weighted = nn.BCELoss(weight=weight)\r\nloss_weighted = criterion_weighted(output, y)\r\n```\r\n\r\nInternally, `_infer_size` is called and fails in the first code snippet.\r\nThe second code snippet applied a weighting for each batch element, which is fine.\r\n\r\nShould we automatically unsqueeze the weight tensor, if input and target are multi-dimensional?\r\n\r\nAlso, how should we handle class weighting?\r\nIf we just pass 2 weights as the `weight` tensor, the code successfully runs, but does not apply class weighting, which might mislead some users:\r\n```\r\n# Create class weights\r\nweight = torch.FloatTensor([0.1, 0.9]) \r\n\r\n# Internally, weight is expanded as\r\nsize = _infer_size(weight.size(), y.size())\r\nweight_expanded = weight.expand(size) \r\nprint(weight_expanded) # This is not, what we wanted as class weights!\r\n\r\ncriterion_weighted = nn.BCELoss(weight=weight)\r\nloss_weighted = criterion_weighted(output, y)\r\ncriterion_nonreduced = nn.BCELoss(reduce=False)\r\nloss_unreduced = criterion_nonreduced(output, y)\r\nloss_weighted_manual = (Variable(weight_expanded) * loss_unreduced).mean()\r\n\r\nif loss_weighted == loss_weighted_manual:\r\n    print('Class weighting failed')\r\n```\r\n\r\nA workaround would be:\r\n```\r\nweight_ = weight[y.data.view(-1).long()].view_as(y)\r\ncriterion = nn.BCELoss(reduce=False)\r\nloss = criterion(output, y)\r\nloss_class_weighted = loss * Variable(weight_)\r\n```\r\n\r\nWhat is wanted behavior of the `weight` parameter in `BCELoss`: class or batch weighting?\r\nBoth cases have some issues at the moment in my opinion.\r\nI would like to fix this issue, but I would like to hear some opinions on the right behavior.\r\nClass weighting would be consistent with other loss functions like `NLLLoss`, but maybe batch weighting is a more common use case for `BCELoss`.\r\n\r\nPyTorch version: `0.4.0a0+492e26f` (installed from source)\r\n\r\n"}