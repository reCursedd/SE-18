{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/364510812", "html_url": "https://github.com/pytorch/pytorch/issues/5157#issuecomment-364510812", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5157", "id": 364510812, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDUxMDgxMg==", "user": {"login": "ptrblck", "id": 11662379, "node_id": "MDQ6VXNlcjExNjYyMzc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11662379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptrblck", "html_url": "https://github.com/ptrblck", "followers_url": "https://api.github.com/users/ptrblck/followers", "following_url": "https://api.github.com/users/ptrblck/following{/other_user}", "gists_url": "https://api.github.com/users/ptrblck/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptrblck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptrblck/subscriptions", "organizations_url": "https://api.github.com/users/ptrblck/orgs", "repos_url": "https://api.github.com/users/ptrblck/repos", "events_url": "https://api.github.com/users/ptrblck/events{/privacy}", "received_events_url": "https://api.github.com/users/ptrblck/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T18:03:07Z", "updated_at": "2018-02-09T18:03:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the response <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> !<br>\nYeah, you are right. Class weighting won't work for float targets. I just compared it to other loss functions and was wondering if class weighting would be useful for an imbalanced binary classification problem. <code>NLLLoss</code> is probably more useful in this setting.</p>\n<p>Do you think the method should automatically check, if <code>weight</code> is for batch or element weighting, i.e. <code>[4]</code> or <code>[4, 2, 2]</code>, and broadcast it if necessary?</p>", "body_text": "Thanks for the response @zou3519 !\nYeah, you are right. Class weighting won't work for float targets. I just compared it to other loss functions and was wondering if class weighting would be useful for an imbalanced binary classification problem. NLLLoss is probably more useful in this setting.\nDo you think the method should automatically check, if weight is for batch or element weighting, i.e. [4] or [4, 2, 2], and broadcast it if necessary?", "body": "Thanks for the response @zou3519 !\r\nYeah, you are right. Class weighting won't work for float targets. I just compared it to other loss functions and was wondering if class weighting would be useful for an imbalanced binary classification problem. `NLLLoss` is probably more useful in this setting. \r\n\r\nDo you think the method should automatically check, if `weight` is for batch or element weighting, i.e. `[4]` or `[4, 2, 2]`, and broadcast it if necessary? "}