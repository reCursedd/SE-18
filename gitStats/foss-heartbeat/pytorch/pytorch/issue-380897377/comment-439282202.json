{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/439282202", "html_url": "https://github.com/pytorch/pytorch/issues/13991#issuecomment-439282202", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13991", "id": 439282202, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTI4MjIwMg==", "user": {"login": "wanchaol", "id": 9443650, "node_id": "MDQ6VXNlcjk0NDM2NTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9443650?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wanchaol", "html_url": "https://github.com/wanchaol", "followers_url": "https://api.github.com/users/wanchaol/followers", "following_url": "https://api.github.com/users/wanchaol/following{/other_user}", "gists_url": "https://api.github.com/users/wanchaol/gists{/gist_id}", "starred_url": "https://api.github.com/users/wanchaol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wanchaol/subscriptions", "organizations_url": "https://api.github.com/users/wanchaol/orgs", "repos_url": "https://api.github.com/users/wanchaol/repos", "events_url": "https://api.github.com/users/wanchaol/events{/privacy}", "received_events_url": "https://api.github.com/users/wanchaol/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-16T04:56:44Z", "updated_at": "2018-11-16T04:56:44Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9443650\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wanchaol\">@wanchaol</a> dropout with batch_size = 1 wont make it behave the same as <code>model.eval()</code>. Maybe you are thinking about BatchNorm (which is also not the case that batch_size = 1 will be the same as model.eval())</p>\n</blockquote>\n<p>sorry I was wrong, I actually mean set <code>dropout_prob=0</code> when inference</p>", "body_text": "@wanchaol dropout with batch_size = 1 wont make it behave the same as model.eval(). Maybe you are thinking about BatchNorm (which is also not the case that batch_size = 1 will be the same as model.eval())\n\nsorry I was wrong, I actually mean set dropout_prob=0 when inference", "body": "> @wanchaol dropout with batch_size = 1 wont make it behave the same as `model.eval()`. Maybe you are thinking about BatchNorm (which is also not the case that batch_size = 1 will be the same as model.eval())\r\n\r\nsorry I was wrong, I actually mean set `dropout_prob=0` when inference"}