{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10866", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10866/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10866/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10866/events", "html_url": "https://github.com/pytorch/pytorch/pull/10866", "id": 353892435, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEwODIxNjY4", "number": 10866, "title": "Support Weight Decay to adaptive Optimizers", "user": {"login": "alex1o1o7cloud", "id": 38146646, "node_id": "MDQ6VXNlcjM4MTQ2NjQ2", "avatar_url": "https://avatars0.githubusercontent.com/u/38146646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alex1o1o7cloud", "html_url": "https://github.com/alex1o1o7cloud", "followers_url": "https://api.github.com/users/alex1o1o7cloud/followers", "following_url": "https://api.github.com/users/alex1o1o7cloud/following{/other_user}", "gists_url": "https://api.github.com/users/alex1o1o7cloud/gists{/gist_id}", "starred_url": "https://api.github.com/users/alex1o1o7cloud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alex1o1o7cloud/subscriptions", "organizations_url": "https://api.github.com/users/alex1o1o7cloud/orgs", "repos_url": "https://api.github.com/users/alex1o1o7cloud/repos", "events_url": "https://api.github.com/users/alex1o1o7cloud/events{/privacy}", "received_events_url": "https://api.github.com/users/alex1o1o7cloud/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-24T18:56:10Z", "updated_at": "2018-10-16T23:29:27Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10866", "html_url": "https://github.com/pytorch/pytorch/pull/10866", "diff_url": "https://github.com/pytorch/pytorch/pull/10866.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10866.patch"}, "body_html": "<p>Summary:<br>\nas title.<br>\nAdamW (<a href=\"https://arxiv.org/abs/1711.05101\" rel=\"nofollow\">https://arxiv.org/abs/1711.05101</a>) has shown some good result by applying weight decay to adaptive optimzers.<br>\nThis diff deals with adagrad and adam.</p>\n<p>Currently, only constant weight decay is supported. According to the paper, dynamic weight decay hyperparameters that change according to the # of batches should achieve even better results. We ll implement that in a seperate diff if weight decay techies will be demonstrate to have some early positive results. Also AdamWR is also not implemented.</p>\n<p>Differential Revision: D9496208</p>", "body_text": "Summary:\nas title.\nAdamW (https://arxiv.org/abs/1711.05101) has shown some good result by applying weight decay to adaptive optimzers.\nThis diff deals with adagrad and adam.\nCurrently, only constant weight decay is supported. According to the paper, dynamic weight decay hyperparameters that change according to the # of batches should achieve even better results. We ll implement that in a seperate diff if weight decay techies will be demonstrate to have some early positive results. Also AdamWR is also not implemented.\nDifferential Revision: D9496208", "body": "Summary:\nas title.\nAdamW (https://arxiv.org/abs/1711.05101) has shown some good result by applying weight decay to adaptive optimzers.\nThis diff deals with adagrad and adam.\n\nCurrently, only constant weight decay is supported. According to the paper, dynamic weight decay hyperparameters that change according to the # of batches should achieve even better results. We ll implement that in a seperate diff if weight decay techies will be demonstrate to have some early positive results. Also AdamWR is also not implemented.\n\nDifferential Revision: D9496208\n"}