{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/323117529", "html_url": "https://github.com/pytorch/pytorch/issues/2176#issuecomment-323117529", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2176", "id": 323117529, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzExNzUyOQ==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T16:01:26Z", "updated_at": "2017-08-17T16:01:26Z", "author_association": "MEMBER", "body_html": "<p>Right, so in recurrent nets we're recomputing the <code>weight</code> multiple times, which is unnecessary and leads to extra memory usage.</p>\n<p>The backward hook solution isn't robust: <code>weight</code> needs to be re-computed if <code>weight_v</code> or <code>weight_g</code> changes, not after they're gradients are computed. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre>output <span class=\"pl-k\">=</span> module(<span class=\"pl-c1\">input</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1</span>\noutput.backward()  \noutput2 <span class=\"pl-k\">=</span> module(<span class=\"pl-c1\">input</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 2</span>\noptimizer.step()\noutput3 <span class=\"pl-k\">=</span> module(<span class=\"pl-c1\">input</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 3</span></pre></div>\n<p>With the backward hook, weight would be recomputed at (2) when it should be recomputed at (3).</p>", "body_text": "Right, so in recurrent nets we're recomputing the weight multiple times, which is unnecessary and leads to extra memory usage.\nThe backward hook solution isn't robust: weight needs to be re-computed if weight_v or weight_g changes, not after they're gradients are computed. For example:\noutput = module(input)  # 1\noutput.backward()  \noutput2 = module(input)  # 2\noptimizer.step()\noutput3 = module(input)  # 3\nWith the backward hook, weight would be recomputed at (2) when it should be recomputed at (3).", "body": "Right, so in recurrent nets we're recomputing the `weight` multiple times, which is unnecessary and leads to extra memory usage.\r\n\r\nThe backward hook solution isn't robust: `weight` needs to be re-computed if `weight_v` or `weight_g` changes, not after they're gradients are computed. For example:\r\n\r\n```python\r\noutput = module(input)  # 1\r\noutput.backward()  \r\noutput2 = module(input)  # 2\r\noptimizer.step()\r\noutput3 = module(input)  # 3\r\n```\r\n\r\nWith the backward hook, weight would be recomputed at (2) when it should be recomputed at (3)."}