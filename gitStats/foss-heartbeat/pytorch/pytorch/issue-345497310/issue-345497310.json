{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9982", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9982/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9982/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9982/events", "html_url": "https://github.com/pytorch/pytorch/issues/9982", "id": 345497310, "node_id": "MDU6SXNzdWUzNDU0OTczMTA=", "number": 9982, "title": "Reuse the Variable with the same name with \"For loop\"", "user": {"login": "PkuRainBow", "id": 4639578, "node_id": "MDQ6VXNlcjQ2Mzk1Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/4639578?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PkuRainBow", "html_url": "https://github.com/PkuRainBow", "followers_url": "https://api.github.com/users/PkuRainBow/followers", "following_url": "https://api.github.com/users/PkuRainBow/following{/other_user}", "gists_url": "https://api.github.com/users/PkuRainBow/gists{/gist_id}", "starred_url": "https://api.github.com/users/PkuRainBow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PkuRainBow/subscriptions", "organizations_url": "https://api.github.com/users/PkuRainBow/orgs", "repos_url": "https://api.github.com/users/PkuRainBow/repos", "events_url": "https://api.github.com/users/PkuRainBow/events{/privacy}", "received_events_url": "https://api.github.com/users/PkuRainBow/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-29T03:15:47Z", "updated_at": "2018-07-30T16:13:44Z", "closed_at": "2018-07-30T16:13:44Z", "author_association": "NONE", "body_html": "<p>The problem is about the possible issue with the for loop when I excute the for loop within pytorch,</p>\n<p>Here is a snippet of my implementation within a forward function,</p>\n<pre><code>        local_list = []\n        local_block_cnt = 2*self.scale*self.scale\n        for i in range(0, local_block_cnt, 2):\n            g_x_local = g_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\n            theta_x_local = theta_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\n            phi_x_local = phi_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\n            h_local, w_local = g_x_local.size(2), g_x_local.size(3)\n            g_x_local = g_x_local.contiguous().view(batch_size, self.context_channels, -1)\n            g_x_local = g_x_local.permute(0, 2, 1)\n            theta_x_local = theta_x_local.contiguous().view(batch_size, self.inter_channels, -1)\n            theta_x_local = theta_x_local.permute(0, 2, 1)\n            phi_x_local = phi_x_local.contiguous().view(batch_size, self.inter_channels, -1)\n\n            f_local = torch.matmul(theta_x_local, phi_x_local)\n            f_local = (self.inter_channels**-.5) * f_local\n            softmax_local = F.softmax(f_local, dim=-1)\n            y_local = torch.matmul(softmax_local, g_x_local)\n            y_local = y_local.permute(0, 2, 1).contiguous()\n            y_local = y_local.view(batch_size, self.context_channels, h_local, w_local)\n            local_list.append(y_local)\n</code></pre>\n<p>My concern is about whether such method will influence the computation for the backward as I reuse the \"g_x_local, theta_x_local ...\" for many times.</p>", "body_text": "The problem is about the possible issue with the for loop when I excute the for loop within pytorch,\nHere is a snippet of my implementation within a forward function,\n        local_list = []\n        local_block_cnt = 2*self.scale*self.scale\n        for i in range(0, local_block_cnt, 2):\n            g_x_local = g_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\n            theta_x_local = theta_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\n            phi_x_local = phi_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\n            h_local, w_local = g_x_local.size(2), g_x_local.size(3)\n            g_x_local = g_x_local.contiguous().view(batch_size, self.context_channels, -1)\n            g_x_local = g_x_local.permute(0, 2, 1)\n            theta_x_local = theta_x_local.contiguous().view(batch_size, self.inter_channels, -1)\n            theta_x_local = theta_x_local.permute(0, 2, 1)\n            phi_x_local = phi_x_local.contiguous().view(batch_size, self.inter_channels, -1)\n\n            f_local = torch.matmul(theta_x_local, phi_x_local)\n            f_local = (self.inter_channels**-.5) * f_local\n            softmax_local = F.softmax(f_local, dim=-1)\n            y_local = torch.matmul(softmax_local, g_x_local)\n            y_local = y_local.permute(0, 2, 1).contiguous()\n            y_local = y_local.view(batch_size, self.context_channels, h_local, w_local)\n            local_list.append(y_local)\n\nMy concern is about whether such method will influence the computation for the backward as I reuse the \"g_x_local, theta_x_local ...\" for many times.", "body": "The problem is about the possible issue with the for loop when I excute the for loop within pytorch,\r\n\r\nHere is a snippet of my implementation within a forward function,\r\n\r\n\r\n```\r\n        local_list = []\r\n        local_block_cnt = 2*self.scale*self.scale\r\n        for i in range(0, local_block_cnt, 2):\r\n            g_x_local = g_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\r\n            theta_x_local = theta_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\r\n            phi_x_local = phi_x[:,:,local_x[i]:local_x[i+1],local_y[i]:local_y[i+1]]\r\n            h_local, w_local = g_x_local.size(2), g_x_local.size(3)\r\n            g_x_local = g_x_local.contiguous().view(batch_size, self.context_channels, -1)\r\n            g_x_local = g_x_local.permute(0, 2, 1)\r\n            theta_x_local = theta_x_local.contiguous().view(batch_size, self.inter_channels, -1)\r\n            theta_x_local = theta_x_local.permute(0, 2, 1)\r\n            phi_x_local = phi_x_local.contiguous().view(batch_size, self.inter_channels, -1)\r\n\r\n            f_local = torch.matmul(theta_x_local, phi_x_local)\r\n            f_local = (self.inter_channels**-.5) * f_local\r\n            softmax_local = F.softmax(f_local, dim=-1)\r\n            y_local = torch.matmul(softmax_local, g_x_local)\r\n            y_local = y_local.permute(0, 2, 1).contiguous()\r\n            y_local = y_local.view(batch_size, self.context_channels, h_local, w_local)\r\n            local_list.append(y_local)\r\n```\r\n\r\nMy concern is about whether such method will influence the computation for the backward as I reuse the \"g_x_local, theta_x_local ...\" for many times."}