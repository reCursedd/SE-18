{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1531", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1531/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1531/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1531/events", "html_url": "https://github.com/pytorch/pytorch/pull/1531", "id": 227798043, "node_id": "MDExOlB1bGxSZXF1ZXN0MTE5OTY5Mjk0", "number": 1531, "title": "Twice differentiability of pointwise functions", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-05-10T20:17:46Z", "updated_at": "2018-11-23T15:33:26Z", "closed_at": "2017-05-15T18:01:00Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1531", "html_url": "https://github.com/pytorch/pytorch/pull/1531", "diff_url": "https://github.com/pytorch/pytorch/pull/1531.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1531.patch"}, "body_html": "<p>Hi,</p>\n<p>this attempts to convert pointwise.py to new-style functions.</p>\n<p>I have tried to stay to close to the original and close to the argument order of torch.foo functions (in particular AddCmul and AddCdiv).<br>\nAlso, I am not 100% sure about the backward in _ConstantGrad - the old implementation had quite an elaborate allocation and I just wrote grad_output.mul(cls.grad_value) without fully understanding the motivation for the old code.</p>\n<p>I have converted a few inplace computations to not inplace to deal with gradients being variables.</p>\n<p>It seems to pass the test/test_autograd.py and flake8.</p>\n<p>Thank you for considering this pull request.</p>\n<p>Thomas</p>", "body_text": "Hi,\nthis attempts to convert pointwise.py to new-style functions.\nI have tried to stay to close to the original and close to the argument order of torch.foo functions (in particular AddCmul and AddCdiv).\nAlso, I am not 100% sure about the backward in _ConstantGrad - the old implementation had quite an elaborate allocation and I just wrote grad_output.mul(cls.grad_value) without fully understanding the motivation for the old code.\nI have converted a few inplace computations to not inplace to deal with gradients being variables.\nIt seems to pass the test/test_autograd.py and flake8.\nThank you for considering this pull request.\nThomas", "body": "Hi,\r\n\r\nthis attempts to convert pointwise.py to new-style functions.\r\n\r\nI have tried to stay to close to the original and close to the argument order of torch.foo functions (in particular AddCmul and AddCdiv).\r\nAlso, I am not 100% sure about the backward in _ConstantGrad - the old implementation had quite an elaborate allocation and I just wrote grad_output.mul(cls.grad_value) without fully understanding the motivation for the old code.\r\n\r\nI have converted a few inplace computations to not inplace to deal with gradients being variables.\r\n\r\nIt seems to pass the test/test_autograd.py and flake8.\r\n\r\nThank you for considering this pull request.\r\n\r\nThomas"}