{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115850425", "pull_request_review_id": 37426222, "id": 115850425, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNTg1MDQyNQ==", "diff_hunk": "@@ -331,93 +370,133 @@ class Remainder(_ConstantGrad):\n \n class Lerp(Function):\n \n-    def __init__(self, weight):\n-        super(Lerp, self).__init__()\n-        self.weight = float(weight)\n-\n-    def forward(self, a, b):\n-        return a.lerp(b, self.weight)\n+    @staticmethod\n+    def forward(ctx, a, b, weight):\n+        ctx._weight = float(weight)\n+        return a.lerp(b, ctx._weight)\n \n-    def backward(self, grad_output):\n-        return grad_output.mul(1 - self.weight), grad_output.mul(self.weight)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.mul(1 - ctx._weight), grad_output.mul(ctx._weight), None\n+        # TODO: grad for weight?\n \n \n class Rsqrt(InplaceFunction):\n \n-    def forward(self, input):\n-        if self.inplace:\n-            self.mark_dirty(input)\n-            result = input.rsqrt_()\n+    @staticmethod\n+    def forward(ctx, i, inplace=False):\n+        if inplace:\n+            ctx.mark_dirty(i)\n+            result = i.rsqrt_()\n         else:\n-            result = input.rsqrt()\n-        self.save_for_backward(result)\n+            result = i.rsqrt()\n+        ctx.save_for_backward(result)\n         return result\n \n-    def backward(self, grad_output):\n-        result, = self.saved_tensors\n-        return result.pow(3).div_(-2).mul_(grad_output)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        result, = ctx.saved_variables\n+        return result.pow(3).div_(-2).mul(grad_output), None\n \n \n class Addcmul(InplaceFunction):\n \n-    def __init__(self, scale=1, inplace=False):\n-        super(Addcmul, self).__init__(inplace)\n-        self.scale = scale\n-\n-    def forward(self, add_tensor, mul_tensor1, mul_tensor2):\n-        self.save_for_backward(mul_tensor1, mul_tensor2)\n-        if self.inplace:\n-            self.mark_dirty(add_tensor)\n-            return add_tensor.addcmul_(self.scale, mul_tensor1, mul_tensor2)\n+    @staticmethod\n+    def forward(ctx, add_tensor, *args, **argsd):\n+        inplace = argsd.get(\"inplace\", False)\n+        if len(args) == 3 and type(args[-1]) != bool:\n+            scale, mul_tensor1, mul_tensor2 = args\n+            ctx._have_scale = 1\n+        elif len(args) == 3:\n+            scale = 1.0\n+            mul_tensor1, mul_tensor2, inplace = args\n+            ctx._have_scale = 0\n+        elif len(args) == 4:\n+            scale, mul_tensor1, mul_tensor2, inplace = args\n+            ctx._have_scale = 1\n+        else:\n+            scale = 1.0\n+            mul_tensor1, mul_tensor2 = args\n+            ctx._have_scale = 0\n+        ctx._scale = scale\n+        ctx.save_for_backward(mul_tensor1, mul_tensor2)\n+        if inplace:\n+            ctx.mark_dirty(add_tensor)\n+            return add_tensor.addcmul_(scale, mul_tensor1, mul_tensor2)\n         else:\n-            return add_tensor.addcmul(self.scale, mul_tensor1, mul_tensor2)\n+            return add_tensor.addcmul(scale, mul_tensor1, mul_tensor2)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    def backward(ctx, grad_output):\n         grad_add = grad_mul1 = grad_mul2 = None\n-        mul_tensor1, mul_tensor2 = self.saved_tensors\n+        mul_tensor1, mul_tensor2 = ctx.saved_variables\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_add = grad_output\n \n-        if self.needs_input_grad[1]:\n-            grad_mul1 = grad_output.mul(mul_tensor2).mul(self.scale)\n+        if ctx._have_scale:\n+            assert not ctx.needs_input_grad[1], \"Gradient of scale not implemented\"\n+\n+        if ctx.needs_input_grad[1 + ctx._have_scale]:\n+            grad_mul1 = grad_output.mul(mul_tensor2).mul(ctx._scale)\n \n-        if self.needs_input_grad[2]:\n-            grad_mul2 = grad_output.mul(mul_tensor1).mul(self.scale)\n+        if ctx.needs_input_grad[2 + ctx._have_scale]:\n+            grad_mul2 = grad_output.mul(mul_tensor1).mul(ctx._scale)\n \n-        return grad_add, grad_mul1, grad_mul2\n+        if ctx._have_scale:\n+            return grad_add, None, grad_mul1, grad_mul2, None\n+        else:\n+            return grad_add, grad_mul1, grad_mul2, None\n \n \n class Addcdiv(InplaceFunction):\n \n-    def __init__(self, scale=1, inplace=False):\n-        super(Addcdiv, self).__init__(inplace)\n-        self.scale = scale\n-\n-    def forward(self, add_tensor, div_tensor1, div_tensor2):\n-        self.save_for_backward(div_tensor1, div_tensor2)\n-        if self.inplace:\n-            self.mark_dirty(add_tensor)\n-            return add_tensor.addcdiv_(self.scale, div_tensor1, div_tensor2)\n+    @staticmethod\n+    def forward(ctx, add_tensor, *args, **argsd):", "path": "torch/autograd/_functions/pointwise.py", "position": null, "original_position": 476, "commit_id": "373f65ece18f7421dfff7b191fc9ed4139df5f27", "original_commit_id": "c1dd7586d80ddacb31e668385177bf5f3e9b2830", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Yes, I'd love to. :)\r\n", "created_at": "2017-05-10T20:55:28Z", "updated_at": "2018-11-23T15:33:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/1531#discussion_r115850425", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1531", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115850425"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1531#discussion_r115850425"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1531"}}, "body_html": "<p>Yes, I'd love to. :)</p>", "body_text": "Yes, I'd love to. :)", "in_reply_to_id": 115848111}