{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115843508", "pull_request_review_id": 37418035, "id": 115843508, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNTg0MzUwOA==", "diff_hunk": "@@ -138,103 +156,123 @@ def backward(ctx, grad_output):\n \n class Sqrt(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.sqrt()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n         return grad_output.mul(i.pow(-0.5)).div(2)\n \n \n class Sin(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.sin()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n         return grad_output * i.cos()\n \n \n class Cos(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.cos()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n         return grad_output.mul(i.sin()).neg_()\n \n \n class Tan(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.tan()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n         return grad_output.div(i.cos().pow(2))\n \n \n class Asin(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.asin()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n-        return grad_output * (1 - i.mul(i)).sqrt_().reciprocal_()\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n+        return grad_output * (1 - i.mul(i)).sqrt().reciprocal()\n \n \n class Acos(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.acos()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n-        return grad_output.mul((1 - i.mul(i)).sqrt_().reciprocal_()).neg_()\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n+        return grad_output.mul((1 - i.mul(i)).sqrt().reciprocal()).neg()\n \n \n class Atan(Function):\n \n-    def forward(self, i):\n-        self.save_for_backward(i)\n+    @staticmethod\n+    def forward(ctx, i):\n+        ctx.save_for_backward(i)\n         return i.atan()\n \n-    def backward(self, grad_output):\n-        i, = self.saved_tensors\n-        return grad_output * i.mul(i).add_(1).reciprocal_()\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        i, = ctx.saved_variables\n+        return grad_output * i.mul(i).add(1).reciprocal()\n \n \n class Reciprocal(Function):\n \n-    def forward(self, i):\n+    @staticmethod\n+    def forward(ctx, i):\n         result = i.reciprocal()\n-        self.save_for_backward(result)\n+        ctx.save_for_backward(result)\n         return result\n \n-    def backward(self, grad_output):\n-        result, = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        result, = ctx.saved_variables\n         return grad_output * result.mul(result).neg_()\n \n \n class Cmax(Function):\n \n-    def forward(self, a, b):\n-        self._max_buffer = a.gt(b).type_as(a)\n+    @staticmethod\n+    def forward(ctx, a, b):\n+        ctx.save_for_backward(a, b)\n         return a.max(b)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        a, b = ctx.saved_variables\n+        mask = a.gt(b).type_as(a)", "path": "torch/autograd/_functions/pointwise.py", "position": null, "original_position": 288, "commit_id": "373f65ece18f7421dfff7b191fc9ed4139df5f27", "original_commit_id": "99e24022ce2271b57b073d8df41b48c837edc255", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "It's better to save the mask in forward (just `a.gt(b)` and convert it to `grad_output`'s type here (it's going to save a lot of memory - if a and b are float tensors it's an 8x reduction!). To increase the efficiency even more you can guard the `ctx.mask = a.gt(b)` with an `if ctx.requires_grad`", "created_at": "2017-05-10T20:25:50Z", "updated_at": "2018-11-23T15:33:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/1531#discussion_r115843508", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1531", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115843508"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1531#discussion_r115843508"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1531"}}, "body_html": "<p>It's better to save the mask in forward (just <code>a.gt(b)</code> and convert it to <code>grad_output</code>'s type here (it's going to save a lot of memory - if a and b are float tensors it's an 8x reduction!). To increase the efficiency even more you can guard the <code>ctx.mask = a.gt(b)</code> with an <code>if ctx.requires_grad</code></p>", "body_text": "It's better to save the mask in forward (just a.gt(b) and convert it to grad_output's type here (it's going to save a lot of memory - if a and b are float tensors it's an 8x reduction!). To increase the efficiency even more you can guard the ctx.mask = a.gt(b) with an if ctx.requires_grad"}