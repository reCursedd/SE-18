{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7695", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7695/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7695/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7695/events", "html_url": "https://github.com/pytorch/pytorch/pull/7695", "id": 324587815, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg5MTYyMzA0", "number": 7695, "title": "[WIP] Implemented hardshrink in ATen with cuda", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-19T00:50:44Z", "updated_at": "2018-11-23T15:44:23Z", "closed_at": "2018-06-04T16:20:02Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7695", "html_url": "https://github.com/pytorch/pytorch/pull/7695", "diff_url": "https://github.com/pytorch/pytorch/pull/7695.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7695.patch"}, "body_html": "<p>Summary:</p>\n<ol>\n<li><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #4154.\">fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"281855896\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4154\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4154/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4154\">#4154</a>, implemented hard_shrink in ATen (CPU + CUDA)</li>\n<li>performance test results:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">origfn</span>(<span class=\"pl-smi\">data</span>):\n  f <span class=\"pl-k\">=</span> torch.nn.Hardshrink(<span class=\"pl-c1\">0.3</span>)\n  large_out <span class=\"pl-k\">=</span> f(data)\n<span class=\"pl-k\">%</span>timeit origfn(large_data) </pre></div>\n<p>=&gt; 1 loop, best of 3: 3.99 s per loop</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">newfn</span>(<span class=\"pl-smi\">data</span>):\n    large_out <span class=\"pl-k\">=</span> data.hard_shrink(<span class=\"pl-c1\">0.3</span>)\n<span class=\"pl-k\">%</span>timeit newfn(large_data)</pre></div>\n<p>=&gt; 1 loop, best of 3: 4.04 s per loop</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">origfn_backward</span>(<span class=\"pl-smi\">data</span>):\n    f <span class=\"pl-k\">=</span> torch.nn.Hardshrink(<span class=\"pl-c1\">0.3</span>)\n    large_out <span class=\"pl-k\">=</span> f(data)\n    large_out.sum().backward()\n<span class=\"pl-k\">%</span>timeit origfn_backward(large_data)</pre></div>\n<p>=&gt; 1 loop, best of 3: 10 s per loop</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">newfn_backward</span>(<span class=\"pl-smi\">data</span>):\n    large_out <span class=\"pl-k\">=</span> data.hard_shrink(<span class=\"pl-c1\">0.3</span>)\n    large_out.sum().backward()\n<span class=\"pl-k\">%</span>timeit newfn_backward(large_data)</pre></div>\n<p>=&gt; 1 loop, best of 3: 7.78 s per loop</p>\n<p>TODO:</p>\n<ol>\n<li>fix bug in using default value of Scalar lambda=0.5, currently 0.5 doesn't get picked up as default value. Failing in testcase of test_torch.py</li>\n</ol>", "body_text": "Summary:\n\nfix #4154, implemented hard_shrink in ATen (CPU + CUDA)\nperformance test results:\n\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\ndef origfn(data):\n  f = torch.nn.Hardshrink(0.3)\n  large_out = f(data)\n%timeit origfn(large_data) \n=> 1 loop, best of 3: 3.99 s per loop\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\ndef newfn(data):\n    large_out = data.hard_shrink(0.3)\n%timeit newfn(large_data)\n=> 1 loop, best of 3: 4.04 s per loop\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\ndef origfn_backward(data):\n    f = torch.nn.Hardshrink(0.3)\n    large_out = f(data)\n    large_out.sum().backward()\n%timeit origfn_backward(large_data)\n=> 1 loop, best of 3: 10 s per loop\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\ndef newfn_backward(data):\n    large_out = data.hard_shrink(0.3)\n    large_out.sum().backward()\n%timeit newfn_backward(large_data)\n=> 1 loop, best of 3: 7.78 s per loop\nTODO:\n\nfix bug in using default value of Scalar lambda=0.5, currently 0.5 doesn't get picked up as default value. Failing in testcase of test_torch.py", "body": "Summary:\r\n1. fix #4154, implemented hard_shrink in ATen (CPU + CUDA)\r\n2. performance test results:\r\n\r\n```python\r\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\r\ndef origfn(data):\r\n  f = torch.nn.Hardshrink(0.3)\r\n  large_out = f(data)\r\n%timeit origfn(large_data) \r\n```\r\n=> 1 loop, best of 3: 3.99 s per loop\r\n\r\n```python\r\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\r\ndef newfn(data):\r\n    large_out = data.hard_shrink(0.3)\r\n%timeit newfn(large_data)\r\n```\r\n=> 1 loop, best of 3: 4.04 s per loop\r\n\r\n```python\r\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\r\ndef origfn_backward(data):\r\n    f = torch.nn.Hardshrink(0.3)\r\n    large_out = f(data)\r\n    large_out.sum().backward()\r\n%timeit origfn_backward(large_data)\r\n```\r\n=> 1 loop, best of 3: 10 s per loop\r\n\r\n```python\r\nlarge_data = torch.zeros(1000, 1000, 1000).fill_(0.3).requires_grad_()\r\ndef newfn_backward(data):\r\n    large_out = data.hard_shrink(0.3)\r\n    large_out.sum().backward()\r\n%timeit newfn_backward(large_data)\r\n```\r\n=> 1 loop, best of 3: 7.78 s per loop\r\n\r\nTODO:\r\n1. fix bug in using default value of Scalar lambda=0.5, currently 0.5 doesn't get picked up as default value. Failing in testcase of test_torch.py"}