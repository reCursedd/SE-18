{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3748", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3748/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3748/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3748/events", "html_url": "https://github.com/pytorch/pytorch/issues/3748", "id": 274688225, "node_id": "MDU6SXNzdWUyNzQ2ODgyMjU=", "number": 3748, "title": "FP16 implementation has the same speed as FP32 on Tesla V100", "user": {"login": "michaelhuang74", "id": 23154573, "node_id": "MDQ6VXNlcjIzMTU0NTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/23154573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelhuang74", "html_url": "https://github.com/michaelhuang74", "followers_url": "https://api.github.com/users/michaelhuang74/followers", "following_url": "https://api.github.com/users/michaelhuang74/following{/other_user}", "gists_url": "https://api.github.com/users/michaelhuang74/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelhuang74/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelhuang74/subscriptions", "organizations_url": "https://api.github.com/users/michaelhuang74/orgs", "repos_url": "https://api.github.com/users/michaelhuang74/repos", "events_url": "https://api.github.com/users/michaelhuang74/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelhuang74/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-11-16T22:35:10Z", "updated_at": "2018-02-07T21:43:28Z", "closed_at": "2017-11-17T16:44:13Z", "author_association": "NONE", "body_html": "<p>I am trying to improve the speed of neural style transfer by converting it to FP16 implementation to take the advantage of Tensor Core on the Tesla V100 GPU.</p>\n<p>The original FP32 implementation is at <a href=\"https://github.com/leongatys/PytorchNeuralStyleTransfer\">https://github.com/leongatys/PytorchNeuralStyleTransfer</a></p>\n<p>I put the VGG-19 model to FP16 and leave the loss function at FP32 (to avoid overflow and underflow). The modified code is at <a href=\"https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\">https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90</a></p>\n<p>When I run both FP32 and FP16 implementations using the Tesla V100 on Amazon AWS cloud, the speed is same. When the image size is set 800 pixel, both FP32 and FP16 implementations take around 17 seconds for 100 iterations.</p>\n<p>Following is the environment setting.<br>\nCUDA 9, CUDNN 7, the latest PyTorch.</p>\n<p>I am wondering if the computation is really carried out on the Tensor Cores for my FP16 implementation?</p>", "body_text": "I am trying to improve the speed of neural style transfer by converting it to FP16 implementation to take the advantage of Tensor Core on the Tesla V100 GPU.\nThe original FP32 implementation is at https://github.com/leongatys/PytorchNeuralStyleTransfer\nI put the VGG-19 model to FP16 and leave the loss function at FP32 (to avoid overflow and underflow). The modified code is at https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\nWhen I run both FP32 and FP16 implementations using the Tesla V100 on Amazon AWS cloud, the speed is same. When the image size is set 800 pixel, both FP32 and FP16 implementations take around 17 seconds for 100 iterations.\nFollowing is the environment setting.\nCUDA 9, CUDNN 7, the latest PyTorch.\nI am wondering if the computation is really carried out on the Tensor Cores for my FP16 implementation?", "body": "I am trying to improve the speed of neural style transfer by converting it to FP16 implementation to take the advantage of Tensor Core on the Tesla V100 GPU.\r\n\r\nThe original FP32 implementation is at https://github.com/leongatys/PytorchNeuralStyleTransfer\r\n\r\nI put the VGG-19 model to FP16 and leave the loss function at FP32 (to avoid overflow and underflow). The modified code is at https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\r\n\r\nWhen I run both FP32 and FP16 implementations using the Tesla V100 on Amazon AWS cloud, the speed is same. When the image size is set 800 pixel, both FP32 and FP16 implementations take around 17 seconds for 100 iterations. \r\n\r\nFollowing is the environment setting.\r\nCUDA 9, CUDNN 7, the latest PyTorch. \r\n\r\nI am wondering if the computation is really carried out on the Tensor Cores for my FP16 implementation?\r\n\r\n"}