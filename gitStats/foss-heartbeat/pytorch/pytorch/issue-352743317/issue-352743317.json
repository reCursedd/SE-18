{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10756", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10756/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10756/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10756/events", "html_url": "https://github.com/pytorch/pytorch/issues/10756", "id": 352743317, "node_id": "MDU6SXNzdWUzNTI3NDMzMTc=", "number": 10756, "title": "Unexpected Behavior when Pointwise Operations Write to Expanded Tensors", "user": {"login": "c0nn3r", "id": 6255953, "node_id": "MDQ6VXNlcjYyNTU5NTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6255953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c0nn3r", "html_url": "https://github.com/c0nn3r", "followers_url": "https://api.github.com/users/c0nn3r/followers", "following_url": "https://api.github.com/users/c0nn3r/following{/other_user}", "gists_url": "https://api.github.com/users/c0nn3r/gists{/gist_id}", "starred_url": "https://api.github.com/users/c0nn3r/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c0nn3r/subscriptions", "organizations_url": "https://api.github.com/users/c0nn3r/orgs", "repos_url": "https://api.github.com/users/c0nn3r/repos", "events_url": "https://api.github.com/users/c0nn3r/events{/privacy}", "received_events_url": "https://api.github.com/users/c0nn3r/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-21T23:10:22Z", "updated_at": "2018-08-22T13:07:33Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I discovered this when writing some masking code, it seems that unexpected behavior can arise when using pointwise operations to write to expanded tensors. The code below reproduces and explains the issue:</p>\n<p>We start off by constructing an example array and constructing a mask from it:</p>\n<div class=\"highlight highlight-source-python\"><pre>example_batch <span class=\"pl-k\">=</span> torch.Tensor([\n    [[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n     [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">0</span>],\n     [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">0</span>]]\n])\n\nmask <span class=\"pl-k\">=</span> (example_batch <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> mask\ntensor([[[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n         [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>],\n         [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>]]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.uint8)</pre></div>\n<p>We then expand a range into our example batch:</p>\n<div class=\"highlight highlight-source-python\"><pre>index_filled <span class=\"pl-k\">=</span> torch.arange(<span class=\"pl-c1\">1</span>, example_batch.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>).expand_as(example_batch)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> index_filled\ntensor([[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>]]])</pre></div>\n<p>So far, so good, yet the problem appears when we try to do a <code>masked_fill</code> on <code>index_filled</code> using our masked array:</p>\n<div class=\"highlight highlight-source-python\"><pre>masked <span class=\"pl-k\">=</span> index_filled.masked_fill_(mask, <span class=\"pl-c1\">0</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> masked\ntensor([[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]]])</pre></div>\n<p>This doesn't seem to be what we would expect from our mask operation, what we where expecting was:</p>\n<div class=\"highlight highlight-source-python\"><pre>tensor([[[<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>.],\n         [<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">3</span>., <span class=\"pl-c1\">0</span>.],\n         [<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">3</span>., <span class=\"pl-c1\">0</span>.]]])</pre></div>\n<p>This behavior also happens with <code>.add_</code> (an ATen operation):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> index_filled.add_(mask.long())\ntensor([[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">7</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">7</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">7</span>]]])</pre></div>\n<p>Verse if we try with <code>.contiguous()</code>...</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> index_filled.contiguous().add_(mask.long())\ntensor([[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>],\n         [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>]]])</pre></div>\n<p>The \"quick fix\" to this is to call <code>.contiguous()</code> before <code>add_</code> or <code>masked_fill</code>, however I feel that this solution is non-obvious if the user is not aware of strides (and non-obvious even if the user is). Maybe a check for this type of case should exist? Thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11729078\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jekbradbury\">@jekbradbury</a> for helping me figure out this issue.</p>\n<h2>System Info</h2>\n<ul>\n<li>Python version: 3.6</li>\n<li>CUDA available: No</li>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): Conda</li>\n<li>OS: Mac OSX 10.13.6</li>\n<li>PyTorch version: 0.4.1</li>\n<li>Python version: 3.6</li>\n</ul>", "body_text": "I discovered this when writing some masking code, it seems that unexpected behavior can arise when using pointwise operations to write to expanded tensors. The code below reproduces and explains the issue:\nWe start off by constructing an example array and constructing a mask from it:\nexample_batch = torch.Tensor([\n    [[1, 2, 0, 0],\n     [2, 1, 3, 0],\n     [2, 1, 3, 0]]\n])\n\nmask = (example_batch == 0)\n>>> mask\ntensor([[[0, 0, 1, 1],\n         [0, 0, 0, 1],\n         [0, 0, 0, 1]]], dtype=torch.uint8)\nWe then expand a range into our example batch:\nindex_filled = torch.arange(1, example_batch.size(-1) + 1).expand_as(example_batch)\n>>> index_filled\ntensor([[[1, 2, 3, 4],\n         [1, 2, 3, 4],\n         [1, 2, 3, 4]]])\nSo far, so good, yet the problem appears when we try to do a masked_fill on index_filled using our masked array:\nmasked = index_filled.masked_fill_(mask, 0)\n>>> masked\ntensor([[[1, 2, 0, 0],\n         [1, 2, 0, 0],\n         [1, 2, 0, 0]]])\nThis doesn't seem to be what we would expect from our mask operation, what we where expecting was:\ntensor([[[1., 2., 0., 0.],\n         [1., 2., 3., 0.],\n         [1., 2., 3., 0.]]])\nThis behavior also happens with .add_ (an ATen operation):\n>>> index_filled.add_(mask.long())\ntensor([[[1, 2, 4, 7],\n         [1, 2, 4, 7],\n         [1, 2, 4, 7]]])\nVerse if we try with .contiguous()...\n>>> index_filled.contiguous().add_(mask.long())\ntensor([[[1, 2, 4, 5],\n         [1, 2, 3, 5],\n         [1, 2, 3, 5]]])\nThe \"quick fix\" to this is to call .contiguous() before add_ or masked_fill, however I feel that this solution is non-obvious if the user is not aware of strides (and non-obvious even if the user is). Maybe a check for this type of case should exist? Thanks to @jekbradbury for helping me figure out this issue.\nSystem Info\n\nPython version: 3.6\nCUDA available: No\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): Conda\nOS: Mac OSX 10.13.6\nPyTorch version: 0.4.1\nPython version: 3.6", "body": "I discovered this when writing some masking code, it seems that unexpected behavior can arise when using pointwise operations to write to expanded tensors. The code below reproduces and explains the issue:\r\n\r\nWe start off by constructing an example array and constructing a mask from it:\r\n```python\r\nexample_batch = torch.Tensor([\r\n    [[1, 2, 0, 0],\r\n     [2, 1, 3, 0],\r\n     [2, 1, 3, 0]]\r\n])\r\n\r\nmask = (example_batch == 0)\r\n>>> mask\r\ntensor([[[0, 0, 1, 1],\r\n         [0, 0, 0, 1],\r\n         [0, 0, 0, 1]]], dtype=torch.uint8)\r\n```\r\n\r\nWe then expand a range into our example batch:\r\n\r\n```python\r\nindex_filled = torch.arange(1, example_batch.size(-1) + 1).expand_as(example_batch)\r\n>>> index_filled\r\ntensor([[[1, 2, 3, 4],\r\n         [1, 2, 3, 4],\r\n         [1, 2, 3, 4]]])\r\n```\r\n\r\nSo far, so good, yet the problem appears when we try to do a `masked_fill` on `index_filled` using our masked array:\r\n\r\n```python\r\nmasked = index_filled.masked_fill_(mask, 0)\r\n>>> masked\r\ntensor([[[1, 2, 0, 0],\r\n         [1, 2, 0, 0],\r\n         [1, 2, 0, 0]]])\r\n```\r\n\r\nThis doesn't seem to be what we would expect from our mask operation, what we where expecting was:\r\n\r\n```python\r\ntensor([[[1., 2., 0., 0.],\r\n         [1., 2., 3., 0.],\r\n         [1., 2., 3., 0.]]])\r\n```\r\n\r\nThis behavior also happens with `.add_` (an ATen operation):\r\n```python\r\n>>> index_filled.add_(mask.long())\r\ntensor([[[1, 2, 4, 7],\r\n         [1, 2, 4, 7],\r\n         [1, 2, 4, 7]]])\r\n```\r\nVerse if we try with `.contiguous()`...\r\n```python\r\n>>> index_filled.contiguous().add_(mask.long())\r\ntensor([[[1, 2, 4, 5],\r\n         [1, 2, 3, 5],\r\n         [1, 2, 3, 5]]])\r\n```\r\n\r\nThe \"quick fix\" to this is to call `.contiguous()` before `add_` or `masked_fill`, however I feel that this solution is non-obvious if the user is not aware of strides (and non-obvious even if the user is). Maybe a check for this type of case should exist? Thanks to @jekbradbury for helping me figure out this issue.\r\n\r\n## System Info\r\n- Python version: 3.6\r\n- CUDA available: No\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): Conda\r\n- OS: Mac OSX 10.13.6\r\n- PyTorch version: 0.4.1\r\n- Python version: 3.6"}