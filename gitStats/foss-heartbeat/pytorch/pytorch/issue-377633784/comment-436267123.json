{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436267123", "html_url": "https://github.com/pytorch/pytorch/issues/13598#issuecomment-436267123", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13598", "id": 436267123, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjI2NzEyMw==", "user": {"login": "hartb", "id": 18429659, "node_id": "MDQ6VXNlcjE4NDI5NjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/18429659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hartb", "html_url": "https://github.com/hartb", "followers_url": "https://api.github.com/users/hartb/followers", "following_url": "https://api.github.com/users/hartb/following{/other_user}", "gists_url": "https://api.github.com/users/hartb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hartb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hartb/subscriptions", "organizations_url": "https://api.github.com/users/hartb/orgs", "repos_url": "https://api.github.com/users/hartb/repos", "events_url": "https://api.github.com/users/hartb/events{/privacy}", "received_events_url": "https://api.github.com/users/hartb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-06T14:16:00Z", "updated_at": "2018-11-06T14:16:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For reference, a full test script to recreate:</p>\n<pre><code>from torch.autograd import Variable\nimport torch.onnx\nimport torchvision\n\n\ndummy_input = Variable(torch.randn(10, 3, 227, 227,dtype=torch.float32,device='cuda'))\n\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n\nimport onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)\n\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA\") # or \"CPU\"\n\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\n\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\n# - try the other way.. seems to work ok\n#img = np.random.randn(10, 3, 227, 227).astype(np.float32)\n#outputs = backend.run_model(model, [img])\n\nprint(outputs[0])\n</code></pre>", "body_text": "For reference, a full test script to recreate:\nfrom torch.autograd import Variable\nimport torch.onnx\nimport torchvision\n\n\ndummy_input = Variable(torch.randn(10, 3, 227, 227,dtype=torch.float32,device='cuda'))\n\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n\nimport onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)\n\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA\") # or \"CPU\"\n\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\n\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\n# - try the other way.. seems to work ok\n#img = np.random.randn(10, 3, 227, 227).astype(np.float32)\n#outputs = backend.run_model(model, [img])\n\nprint(outputs[0])", "body": "For reference, a full test script to recreate:\r\n\r\n```\r\nfrom torch.autograd import Variable\r\nimport torch.onnx\r\nimport torchvision\r\n\r\n\r\ndummy_input = Variable(torch.randn(10, 3, 227, 227,dtype=torch.float32,device='cuda'))\r\n\r\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\r\n\r\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\r\noutput_names = [ \"output1\" ]\r\n\r\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\r\n\r\nimport onnx\r\n\r\n# Load the ONNX model\r\nmodel = onnx.load(\"alexnet.onnx\")\r\n\r\n# Check that the IR is well formed\r\nonnx.checker.check_model(model)\r\n\r\n# Print a human readable representation of the graph\r\nonnx.helper.printable_graph(model.graph)\r\n\r\nimport caffe2.python.onnx.backend as backend\r\nimport numpy as np\r\n\r\nrep = backend.prepare(model, device=\"CUDA\") # or \"CPU\"\r\n\r\n# For the Caffe2 backend:\r\n#     rep.predict_net is the Caffe2 protobuf for the network\r\n#     rep.workspace is the Caffe2 workspace for the network\r\n#       (see the class caffe2.python.onnx.backend.Workspace)\r\noutputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\r\n\r\n# To run networks with more than one input, pass a tuple\r\n# rather than a single numpy ndarray.\r\n# - try the other way.. seems to work ok\r\n#img = np.random.randn(10, 3, 227, 227).astype(np.float32)\r\n#outputs = backend.run_model(model, [img])\r\n\r\nprint(outputs[0])\r\n```"}