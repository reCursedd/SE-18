{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436276501", "html_url": "https://github.com/pytorch/pytorch/issues/13598#issuecomment-436276501", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13598", "id": 436276501, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjI3NjUwMQ==", "user": {"login": "hartb", "id": 18429659, "node_id": "MDQ6VXNlcjE4NDI5NjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/18429659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hartb", "html_url": "https://github.com/hartb", "followers_url": "https://api.github.com/users/hartb/followers", "following_url": "https://api.github.com/users/hartb/following{/other_user}", "gists_url": "https://api.github.com/users/hartb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hartb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hartb/subscriptions", "organizations_url": "https://api.github.com/users/hartb/orgs", "repos_url": "https://api.github.com/users/hartb/repos", "events_url": "https://api.github.com/users/hartb/events{/privacy}", "received_events_url": "https://api.github.com/users/hartb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-06T14:43:49Z", "updated_at": "2018-11-06T14:43:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The failure <em>may</em> occur less frequently when the script is run as whole, vs. more frequently when just the Caffe2 ONNX import-and-run side is run against a previously-exported \"alexnet.onnx\" file. (But the frequency difference may be due to other factors.)</p>\n<p>The corruption seems to consistently hit the most-significant half of first shape dimension, turning what should be a \"10\" into a much larger value). Here's another run where <code>0x0000 0a00</code> is the stray value:</p>\n<pre><code>(gdb) print actual_new_shape\n$1 = {...\n    ..._M_start = 0x7ffdd82f5ef0,\n      _M_finish = 0x7ffdd82f5f00,...\n\n(gdb) x/4x 0x7ffdd82f5ef0\n0x7ffdd82f5ef0: 0x0000000a      0x00000a00      0x00002400      0x00000000\n</code></pre>\n<p>Since this is LE, the stray data appears at offset 0x4, between two apparently-wholesome data words at 0x0 and 0x8, so maybe that suggests this isn't a simple overwrite?</p>\n<p>The problem doesn't appear to be sensitive to the input data. If we seed the numpy RNG just before:</p>\n<pre><code>np.random.seed(123)\noutputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\n</code></pre>\n<p>The problem is still intermittent: sometimes not occurring, and with different stray values when it does.</p>\n<p>In some testing we've seen a complaint from <code>_Workspace_feed_blob()</code> in <code>caffe2/python/workspace.py</code>:</p>\n<pre><code>    if device_option and device_option.device_type == caffe2_pb2.CUDA:\n        if arr.dtype == np.dtype('float64'):\n            logger.warning(\n                \"CUDA operators do not support 64-bit doubles, \" +\n                \"please use arr.astype(np.float32) or np.int32 for ints.\" +\n                \" Blob: {}\".format(name) +\n                \" type: {}\".format(str(arr.dtype))\n</code></pre>\n<p>Here the test script is specifying <code>np.float32</code> as the input type, but this shape data is (inherently?) 64-bit and may be passed between host and GPU? Not sure whether this is relevant or a red herring.</p>", "body_text": "The failure may occur less frequently when the script is run as whole, vs. more frequently when just the Caffe2 ONNX import-and-run side is run against a previously-exported \"alexnet.onnx\" file. (But the frequency difference may be due to other factors.)\nThe corruption seems to consistently hit the most-significant half of first shape dimension, turning what should be a \"10\" into a much larger value). Here's another run where 0x0000 0a00 is the stray value:\n(gdb) print actual_new_shape\n$1 = {...\n    ..._M_start = 0x7ffdd82f5ef0,\n      _M_finish = 0x7ffdd82f5f00,...\n\n(gdb) x/4x 0x7ffdd82f5ef0\n0x7ffdd82f5ef0: 0x0000000a      0x00000a00      0x00002400      0x00000000\n\nSince this is LE, the stray data appears at offset 0x4, between two apparently-wholesome data words at 0x0 and 0x8, so maybe that suggests this isn't a simple overwrite?\nThe problem doesn't appear to be sensitive to the input data. If we seed the numpy RNG just before:\nnp.random.seed(123)\noutputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\n\nThe problem is still intermittent: sometimes not occurring, and with different stray values when it does.\nIn some testing we've seen a complaint from _Workspace_feed_blob() in caffe2/python/workspace.py:\n    if device_option and device_option.device_type == caffe2_pb2.CUDA:\n        if arr.dtype == np.dtype('float64'):\n            logger.warning(\n                \"CUDA operators do not support 64-bit doubles, \" +\n                \"please use arr.astype(np.float32) or np.int32 for ints.\" +\n                \" Blob: {}\".format(name) +\n                \" type: {}\".format(str(arr.dtype))\n\nHere the test script is specifying np.float32 as the input type, but this shape data is (inherently?) 64-bit and may be passed between host and GPU? Not sure whether this is relevant or a red herring.", "body": "The failure _may_ occur less frequently when the script is run as whole, vs. more frequently when just the Caffe2 ONNX import-and-run side is run against a previously-exported \"alexnet.onnx\" file. (But the frequency difference may be due to other factors.)\r\n\r\nThe corruption seems to consistently hit the most-significant half of first shape dimension, turning what should be a \"10\" into a much larger value). Here's another run where `0x0000 0a00` is the stray value:\r\n\r\n```\r\n(gdb) print actual_new_shape\r\n$1 = {...\r\n    ..._M_start = 0x7ffdd82f5ef0,\r\n      _M_finish = 0x7ffdd82f5f00,...\r\n\r\n(gdb) x/4x 0x7ffdd82f5ef0\r\n0x7ffdd82f5ef0: 0x0000000a      0x00000a00      0x00002400      0x00000000\r\n```\r\n\r\nSince this is LE, the stray data appears at offset 0x4, between two apparently-wholesome data words at 0x0 and 0x8, so maybe that suggests this isn't a simple overwrite?\r\n\r\nThe problem doesn't appear to be sensitive to the input data. If we seed the numpy RNG just before:\r\n\r\n```\r\nnp.random.seed(123)\r\noutputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\r\n```\r\n\r\nThe problem is still intermittent: sometimes not occurring, and with different stray values when it does.\r\n\r\nIn some testing we've seen a complaint from `_Workspace_feed_blob()` in `caffe2/python/workspace.py`:\r\n\r\n```\r\n    if device_option and device_option.device_type == caffe2_pb2.CUDA:\r\n        if arr.dtype == np.dtype('float64'):\r\n            logger.warning(\r\n                \"CUDA operators do not support 64-bit doubles, \" +\r\n                \"please use arr.astype(np.float32) or np.int32 for ints.\" +\r\n                \" Blob: {}\".format(name) +\r\n                \" type: {}\".format(str(arr.dtype))\r\n```\r\n\r\nHere the test script is specifying `np.float32` as the input type, but this shape data is (inherently?) 64-bit and may be passed between host and GPU? Not sure whether this is relevant or a red herring.\r\n"}