{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13598", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13598/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13598/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13598/events", "html_url": "https://github.com/pytorch/pytorch/issues/13598", "id": 377633784, "node_id": "MDU6SXNzdWUzNzc2MzM3ODQ=", "number": 13598, "title": "caffe2: RuntimeError: [enforce fail at reshape_op.h:110]  with Alexnet onnx test with cuda", "user": {"login": "rjknight", "id": 8375909, "node_id": "MDQ6VXNlcjgzNzU5MDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/8375909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rjknight", "html_url": "https://github.com/rjknight", "followers_url": "https://api.github.com/users/rjknight/followers", "following_url": "https://api.github.com/users/rjknight/following{/other_user}", "gists_url": "https://api.github.com/users/rjknight/gists{/gist_id}", "starred_url": "https://api.github.com/users/rjknight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rjknight/subscriptions", "organizations_url": "https://api.github.com/users/rjknight/orgs", "repos_url": "https://api.github.com/users/rjknight/repos", "events_url": "https://api.github.com/users/rjknight/events{/privacy}", "received_events_url": "https://api.github.com/users/rjknight/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-05T23:55:36Z", "updated_at": "2018-11-16T22:18:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>we are seeing an intermittent failure in the reshape_op when trying to run the EXAMPLE: END-TO-END ALEXNET FROM PYTORCH TO CAFFE2.</p>\n<pre><code>Traceback (most recent call last):\n  File \"/tmp/test.py\", line 35, in &lt;module&gt;\n    outputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/onnx/backend_rep.py\", line 57, in run\n    self.workspace.RunNet(self.predict_net.name)\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/onnx/workspace.py\", line 63, in f\n    return getattr(workspace, attr)(*args, **kwargs)\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 219, in RunNet\n    StringifyNetName(name), num_iter, allow_fail,\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [enforce fail at reshape_op.h:110] total_size == size. 92160 vs -5680358544067434496. Argument `shape` does not agree with the input data. (92160 != -5680358544067434496)Error from operator:\ninput: \"29\" input: \"36\" output: \"37\" output: \"OC2_DUMMY_1\" name: \"\" type: \"Reshape\" device_option { device_type: 1 device_id: 0 }\n</code></pre>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:<br>\nUsing the example code and the exported alexnet.onnx file  run the following sample.</p>\n<pre><code>import onnx\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\nmodel = onnx.load(\"alexnet.onnx\")\n\nrep = backend.prepare(model, device=\"CUDA\") # or \"CPU\"\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\nprint(outputs[0])\n</code></pre>\n<h2>Expected behavior</h2>\n<p>example code runs successfully every time.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 1.0 rc1</li>\n<li>OS (e.g., Linux):  RHEL 7.5 ppc64le</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): Build from source</li>\n<li>Build command you used (if compiling from source): DEBUG=1 USE_OPENCV=1 python setup.py</li>\n<li>Python version: 2.7 or 3.6</li>\n<li>CUDA/cuDNN version:  10 7.31</li>\n<li>GPU models and configuration: v100</li>\n<li>Any other relevant information:</li>\n</ul>\n<p>problem only occurs when use device = cuda, if device = cpu the issue is not see, seen on ppc64le unknown if other platforms are effected.</p>\n<h2>Additional context</h2>\n<p>Checking that very large incorrect value, we see least significant<br>\n32-bits of the total size is correct (i.e. 0x16800, or 92160), but the<br>\nhigh order 32-bits are incorrect (should be 0x0, but contain data):</p>\n<pre><code>$ echo \"obase=16; 2^64 -5680358544067434496\" | bc\nB12B500000016800\n\n$ echo \"ibase=16; 16800\" | bc\n92160\n</code></pre>\n<p>If we build Caffe2 to crash here, we find the cause of the problem seems<br>\nto be that the shape information copied back from the GPU is damaged,<br>\nfor example in one run we see:</p>\n<pre><code>(gdb) print actual_new_shape                                                                                                                                        \n$6 = {&lt;std::_Vector_base&lt;long, std::allocator&lt;long&gt; &gt;&gt; = {                                                                                                          \n    _M_impl = {&lt;std::allocator&lt;long&gt;&gt; = {&lt;__gnu_cxx::new_allocator&lt;long&gt;&gt; = {&lt;No data fields&gt;}, &lt;No data fields&gt;}, _M_start = 0x7ffdf836e930,                       \n      _M_finish = 0x7ffdf836e940, _M_end_of_storage = 0x7ffdf836e940}}, &lt;No data fields&gt;}\n\n(gdb) x/4x 0x7ffdf836e930                                                                                                                                           \n0x7ffdf836e930: 0x0000000a      0x00000005      0x00002400      0x00000000\n</code></pre>\n<p>Here, 2 values (0x0a / 10, and 0x2400 / 9216) should have been copied in<br>\nfrom the GPU, but instead the most-significant 32-bits of the \"10\" value<br>\nhave been overlayed with \"0x05\", resulting in a final apparent value of<br>\n0x0000 0005 0000 000a (21474836490).</p>\n<p>Since the problem is intermittent, it's not clear whether the overwrite<br>\nis always happening (but mostly happens to be 0x0) or only happens<br>\nsometimes.</p>\n", "body_text": "\ud83d\udc1b Bug\nwe are seeing an intermittent failure in the reshape_op when trying to run the EXAMPLE: END-TO-END ALEXNET FROM PYTORCH TO CAFFE2.\nTraceback (most recent call last):\n  File \"/tmp/test.py\", line 35, in <module>\n    outputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/onnx/backend_rep.py\", line 57, in run\n    self.workspace.RunNet(self.predict_net.name)\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/onnx/workspace.py\", line 63, in f\n    return getattr(workspace, attr)(*args, **kwargs)\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 219, in RunNet\n    StringifyNetName(name), num_iter, allow_fail,\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [enforce fail at reshape_op.h:110] total_size == size. 92160 vs -5680358544067434496. Argument `shape` does not agree with the input data. (92160 != -5680358544067434496)Error from operator:\ninput: \"29\" input: \"36\" output: \"37\" output: \"OC2_DUMMY_1\" name: \"\" type: \"Reshape\" device_option { device_type: 1 device_id: 0 }\n\nTo Reproduce\nSteps to reproduce the behavior:\nUsing the example code and the exported alexnet.onnx file  run the following sample.\nimport onnx\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\nmodel = onnx.load(\"alexnet.onnx\")\n\nrep = backend.prepare(model, device=\"CUDA\") # or \"CPU\"\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\nprint(outputs[0])\n\nExpected behavior\nexample code runs successfully every time.\nEnvironment\n\nPyTorch Version (e.g., 1.0): 1.0 rc1\nOS (e.g., Linux):  RHEL 7.5 ppc64le\nHow you installed PyTorch (conda, pip, source): Build from source\nBuild command you used (if compiling from source): DEBUG=1 USE_OPENCV=1 python setup.py\nPython version: 2.7 or 3.6\nCUDA/cuDNN version:  10 7.31\nGPU models and configuration: v100\nAny other relevant information:\n\nproblem only occurs when use device = cuda, if device = cpu the issue is not see, seen on ppc64le unknown if other platforms are effected.\nAdditional context\nChecking that very large incorrect value, we see least significant\n32-bits of the total size is correct (i.e. 0x16800, or 92160), but the\nhigh order 32-bits are incorrect (should be 0x0, but contain data):\n$ echo \"obase=16; 2^64 -5680358544067434496\" | bc\nB12B500000016800\n\n$ echo \"ibase=16; 16800\" | bc\n92160\n\nIf we build Caffe2 to crash here, we find the cause of the problem seems\nto be that the shape information copied back from the GPU is damaged,\nfor example in one run we see:\n(gdb) print actual_new_shape                                                                                                                                        \n$6 = {<std::_Vector_base<long, std::allocator<long> >> = {                                                                                                          \n    _M_impl = {<std::allocator<long>> = {<__gnu_cxx::new_allocator<long>> = {<No data fields>}, <No data fields>}, _M_start = 0x7ffdf836e930,                       \n      _M_finish = 0x7ffdf836e940, _M_end_of_storage = 0x7ffdf836e940}}, <No data fields>}\n\n(gdb) x/4x 0x7ffdf836e930                                                                                                                                           \n0x7ffdf836e930: 0x0000000a      0x00000005      0x00002400      0x00000000\n\nHere, 2 values (0x0a / 10, and 0x2400 / 9216) should have been copied in\nfrom the GPU, but instead the most-significant 32-bits of the \"10\" value\nhave been overlayed with \"0x05\", resulting in a final apparent value of\n0x0000 0005 0000 000a (21474836490).\nSince the problem is intermittent, it's not clear whether the overwrite\nis always happening (but mostly happens to be 0x0) or only happens\nsometimes.", "body": "## \ud83d\udc1b Bug\r\nwe are seeing an intermittent failure in the reshape_op when trying to run the EXAMPLE: END-TO-END ALEXNET FROM PYTORCH TO CAFFE2.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/test.py\", line 35, in <module>\r\n    outputs = rep.run(np.random.randn(10, 3, 227, 227).astype(np.float32))\r\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/onnx/backend_rep.py\", line 57, in run\r\n    self.workspace.RunNet(self.predict_net.name)\r\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/onnx/workspace.py\", line 63, in f\r\n    return getattr(workspace, attr)(*args, **kwargs)\r\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 219, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File \"/opt/DL/pytorch/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [enforce fail at reshape_op.h:110] total_size == size. 92160 vs -5680358544067434496. Argument `shape` does not agree with the input data. (92160 != -5680358544067434496)Error from operator:\r\ninput: \"29\" input: \"36\" output: \"37\" output: \"OC2_DUMMY_1\" name: \"\" type: \"Reshape\" device_option { device_type: 1 device_id: 0 }\r\n```\r\n\r\n## To Reproduce\r\nSteps to reproduce the behavior:\r\nUsing the example code and the exported alexnet.onnx file  run the following sample.\r\n```\r\nimport onnx\r\nimport caffe2.python.onnx.backend as backend\r\nimport numpy as np\r\nmodel = onnx.load(\"alexnet.onnx\")\r\n\r\nrep = backend.prepare(model, device=\"CUDA\") # or \"CPU\"\r\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\r\nprint(outputs[0])\r\n```\r\n## Expected behavior\r\nexample code runs successfully every time.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0 rc1 \r\n - OS (e.g., Linux):  RHEL 7.5 ppc64le\r\n - How you installed PyTorch (`conda`, `pip`, source): Build from source\r\n - Build command you used (if compiling from source): DEBUG=1 USE_OPENCV=1 python setup.py\r\n - Python version: 2.7 or 3.6\r\n - CUDA/cuDNN version:  10 7.31 \r\n - GPU models and configuration: v100\r\n - Any other relevant information:\r\n\r\nproblem only occurs when use device = cuda, if device = cpu the issue is not see, seen on ppc64le unknown if other platforms are effected.\r\n\r\n## Additional context\r\nChecking that very large incorrect value, we see least significant\r\n32-bits of the total size is correct (i.e. 0x16800, or 92160), but the\r\nhigh order 32-bits are incorrect (should be 0x0, but contain data):\r\n\r\n```\r\n$ echo \"obase=16; 2^64 -5680358544067434496\" | bc\r\nB12B500000016800\r\n\r\n$ echo \"ibase=16; 16800\" | bc\r\n92160\r\n```\r\n\r\nIf we build Caffe2 to crash here, we find the cause of the problem seems\r\nto be that the shape information copied back from the GPU is damaged,\r\nfor example in one run we see:\r\n\r\n```\r\n(gdb) print actual_new_shape                                                                                                                                        \r\n$6 = {<std::_Vector_base<long, std::allocator<long> >> = {                                                                                                          \r\n    _M_impl = {<std::allocator<long>> = {<__gnu_cxx::new_allocator<long>> = {<No data fields>}, <No data fields>}, _M_start = 0x7ffdf836e930,                       \r\n      _M_finish = 0x7ffdf836e940, _M_end_of_storage = 0x7ffdf836e940}}, <No data fields>}\r\n\r\n(gdb) x/4x 0x7ffdf836e930                                                                                                                                           \r\n0x7ffdf836e930: 0x0000000a      0x00000005      0x00002400      0x00000000\r\n```\r\n\r\nHere, 2 values (0x0a / 10, and 0x2400 / 9216) should have been copied in\r\nfrom the GPU, but instead the most-significant 32-bits of the \"10\" value\r\nhave been overlayed with \"0x05\", resulting in a final apparent value of\r\n0x0000 0005 0000 000a (21474836490).\r\n\r\nSince the problem is intermittent, it's not clear whether the overwrite\r\nis always happening (but mostly happens to be 0x0) or only happens\r\nsometimes.\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}