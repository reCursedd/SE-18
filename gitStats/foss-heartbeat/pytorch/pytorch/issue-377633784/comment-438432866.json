{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438432866", "html_url": "https://github.com/pytorch/pytorch/issues/13598#issuecomment-438432866", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13598", "id": 438432866, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODQzMjg2Ng==", "user": {"login": "rjknight123", "id": 1077566, "node_id": "MDQ6VXNlcjEwNzc1NjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1077566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rjknight123", "html_url": "https://github.com/rjknight123", "followers_url": "https://api.github.com/users/rjknight123/followers", "following_url": "https://api.github.com/users/rjknight123/following{/other_user}", "gists_url": "https://api.github.com/users/rjknight123/gists{/gist_id}", "starred_url": "https://api.github.com/users/rjknight123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rjknight123/subscriptions", "organizations_url": "https://api.github.com/users/rjknight123/orgs", "repos_url": "https://api.github.com/users/rjknight123/repos", "events_url": "https://api.github.com/users/rjknight123/events{/privacy}", "received_events_url": "https://api.github.com/users/rjknight123/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-13T20:54:23Z", "updated_at": "2018-11-13T20:54:23Z", "author_association": "NONE", "body_html": "<p>Looks like the issue is in the gather operation -<br>\nExamining the net trace below we can figure out the inputs/outputs to each of the operations - For our case, the items of interest are the inputs/outputs for the Reshape operation,<br>\nnamely %29, %36 and the expected output %37</p>\n<pre><code> %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n %30 : Long() = onnx::Constant[value={0}]()\n %31 : Dynamic = onnx::Shape(%29), scope: AlexNet\n %32 : Long() = onnx::Gather[axis=0](%31, %30), scope: AlexNet\n %33 : Long() = onnx::Constant[value={9216}]()\n %34 : Dynamic = onnx::Unsqueeze[axes=[0]](%32), scope: AlexNet\n %35 : Dynamic = onnx::Unsqueeze[axes=[0]](%33), scope: AlexNet\n %36 : int[] = onnx::Concat[axis=0](%34, %35), scope: AlexNet\n %37 : Float(10, 9216) = onnx::Reshape(%29, %36), scope: AlexNet\n</code></pre>\n<p>%29 is the output of the pooling operation. The tensor contains all of the data,<br>\nand consistently seemed to be correctly formed.</p>\n<p>%36 - is the combination of several operations Shape, Gather, Unsqueeze and Concat and contained the data we observed to be corrupted.</p>\n<p>Working backwards through the net trace things seemed to go bad at the output of the Gather operation.</p>\n<p>The Output of Shape (%31) is a tensor with dtype int64 and values [10,256,6,6] which is used as input<br>\nto the Gather function along with a tensor for the desired axis which also has a dtype of int64 and value [0]. The expected output of the Gather function is a tensor of the same dtype as the input tensor and should contain a single value equal to the first value in the %31.</p>\n<p>Gather contains a kernel function to collect pieces of data into a single tensor.</p>\n<pre><code>template &lt;typename T_INDEX&gt;\n__global__ void GatherKernel(\n    const float* X,\n    float* Y,\n    const T_INDEX* indices,\n    const int N,\n    const int block_size) {\n  for (int i = blockIdx.x; i &lt; N; i += gridDim.x) {\n    T_INDEX idx = indices[i];\n    const float* src_offset = X + idx * block_size;\n    float* dst_offset = Y + i * block_size;\n    for (int j = threadIdx.x; j &lt; block_size; j += blockDim.x) {\n      dst_offset[j] = src_offset[j];\n    }   \n  }\n}\n</code></pre>\n<p>when the kernal function is called in the example the inputs are:</p>\n<pre><code>GatherKernel&lt;&lt;&lt; 1, 128, 0, context_.cuda_stream()&gt;&gt;&gt;(src_base, out, idxs, N, block_size);\n\nsrc_base   = float *\nout        = float *\nidxs       = int64_t *\nN          = 1\nblock_size = 1 \n</code></pre>\n<p>since dst_offset and src_offsets are both float pointers derived from the<br>\ndata member of the Output and Input tensors, only 1/2 of the data gets<br>\ncopied.</p>\n<p>I think the fix will be to template the types of dst_offset and src_offset instead of the Index tensor type,  Im working on that now</p>", "body_text": "Looks like the issue is in the gather operation -\nExamining the net trace below we can figure out the inputs/outputs to each of the operations - For our case, the items of interest are the inputs/outputs for the Reshape operation,\nnamely %29, %36 and the expected output %37\n %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n %30 : Long() = onnx::Constant[value={0}]()\n %31 : Dynamic = onnx::Shape(%29), scope: AlexNet\n %32 : Long() = onnx::Gather[axis=0](%31, %30), scope: AlexNet\n %33 : Long() = onnx::Constant[value={9216}]()\n %34 : Dynamic = onnx::Unsqueeze[axes=[0]](%32), scope: AlexNet\n %35 : Dynamic = onnx::Unsqueeze[axes=[0]](%33), scope: AlexNet\n %36 : int[] = onnx::Concat[axis=0](%34, %35), scope: AlexNet\n %37 : Float(10, 9216) = onnx::Reshape(%29, %36), scope: AlexNet\n\n%29 is the output of the pooling operation. The tensor contains all of the data,\nand consistently seemed to be correctly formed.\n%36 - is the combination of several operations Shape, Gather, Unsqueeze and Concat and contained the data we observed to be corrupted.\nWorking backwards through the net trace things seemed to go bad at the output of the Gather operation.\nThe Output of Shape (%31) is a tensor with dtype int64 and values [10,256,6,6] which is used as input\nto the Gather function along with a tensor for the desired axis which also has a dtype of int64 and value [0]. The expected output of the Gather function is a tensor of the same dtype as the input tensor and should contain a single value equal to the first value in the %31.\nGather contains a kernel function to collect pieces of data into a single tensor.\ntemplate <typename T_INDEX>\n__global__ void GatherKernel(\n    const float* X,\n    float* Y,\n    const T_INDEX* indices,\n    const int N,\n    const int block_size) {\n  for (int i = blockIdx.x; i < N; i += gridDim.x) {\n    T_INDEX idx = indices[i];\n    const float* src_offset = X + idx * block_size;\n    float* dst_offset = Y + i * block_size;\n    for (int j = threadIdx.x; j < block_size; j += blockDim.x) {\n      dst_offset[j] = src_offset[j];\n    }   \n  }\n}\n\nwhen the kernal function is called in the example the inputs are:\nGatherKernel<<< 1, 128, 0, context_.cuda_stream()>>>(src_base, out, idxs, N, block_size);\n\nsrc_base   = float *\nout        = float *\nidxs       = int64_t *\nN          = 1\nblock_size = 1 \n\nsince dst_offset and src_offsets are both float pointers derived from the\ndata member of the Output and Input tensors, only 1/2 of the data gets\ncopied.\nI think the fix will be to template the types of dst_offset and src_offset instead of the Index tensor type,  Im working on that now", "body": "Looks like the issue is in the gather operation - \r\nExamining the net trace below we can figure out the inputs/outputs to each of the operations - For our case, the items of interest are the inputs/outputs for the Reshape operation, \r\nnamely %29, %36 and the expected output %37\r\n\r\n```\r\n %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\r\n %30 : Long() = onnx::Constant[value={0}]()\r\n %31 : Dynamic = onnx::Shape(%29), scope: AlexNet\r\n %32 : Long() = onnx::Gather[axis=0](%31, %30), scope: AlexNet\r\n %33 : Long() = onnx::Constant[value={9216}]()\r\n %34 : Dynamic = onnx::Unsqueeze[axes=[0]](%32), scope: AlexNet\r\n %35 : Dynamic = onnx::Unsqueeze[axes=[0]](%33), scope: AlexNet\r\n %36 : int[] = onnx::Concat[axis=0](%34, %35), scope: AlexNet\r\n %37 : Float(10, 9216) = onnx::Reshape(%29, %36), scope: AlexNet\r\n```\r\n\r\n%29 is the output of the pooling operation. The tensor contains all of the data, \r\nand consistently seemed to be correctly formed.\r\n\r\n%36 - is the combination of several operations Shape, Gather, Unsqueeze and Concat and contained the data we observed to be corrupted.\r\n\r\nWorking backwards through the net trace things seemed to go bad at the output of the Gather operation.\r\n\r\nThe Output of Shape (%31) is a tensor with dtype int64 and values [10,256,6,6] which is used as input\r\nto the Gather function along with a tensor for the desired axis which also has a dtype of int64 and value [0]. The expected output of the Gather function is a tensor of the same dtype as the input tensor and should contain a single value equal to the first value in the %31.\r\n\r\nGather contains a kernel function to collect pieces of data into a single tensor.\r\n\r\n```\r\ntemplate <typename T_INDEX>\r\n__global__ void GatherKernel(\r\n    const float* X,\r\n    float* Y,\r\n    const T_INDEX* indices,\r\n    const int N,\r\n    const int block_size) {\r\n  for (int i = blockIdx.x; i < N; i += gridDim.x) {\r\n    T_INDEX idx = indices[i];\r\n    const float* src_offset = X + idx * block_size;\r\n    float* dst_offset = Y + i * block_size;\r\n    for (int j = threadIdx.x; j < block_size; j += blockDim.x) {\r\n      dst_offset[j] = src_offset[j];\r\n    }   \r\n  }\r\n}\r\n```\r\nwhen the kernal function is called in the example the inputs are:\r\n```\r\nGatherKernel<<< 1, 128, 0, context_.cuda_stream()>>>(src_base, out, idxs, N, block_size);\r\n\r\nsrc_base   = float *\r\nout        = float *\r\nidxs       = int64_t *\r\nN          = 1\r\nblock_size = 1 \r\n```\r\nsince dst_offset and src_offsets are both float pointers derived from the\r\ndata member of the Output and Input tensors, only 1/2 of the data gets\r\ncopied.   \r\n\r\nI think the fix will be to template the types of dst_offset and src_offset instead of the Index tensor type,  Im working on that now "}