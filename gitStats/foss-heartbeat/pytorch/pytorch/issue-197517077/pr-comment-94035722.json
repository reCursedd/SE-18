{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/94035722", "pull_request_review_id": 14565752, "id": 94035722, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk0MDM1NzIy", "diff_hunk": "@@ -0,0 +1,249 @@\n+import numbers\n+import torch\n+from . import functions\n+from .modules import utils\n+\n+\n+# Convolution\n+\n+def conv1d(input, weight, bias=None, stride=1):\n+    \"\"\"Applies a 1D convolution over an input signal composed of several input\n+    planes.\n+\n+    ```\n+    The output value of the layer with input (b x iC x W) and filters\n+    (oC x oC x kw) can be precisely described as:\n+    output[b_i][oc_i][w_i] = bias[oc_i]\n+               + sum_iC sum_{ow = 0, oW-1} sum_{kw = 0 to kW-1}\n+                 weight[oc_i][ic_i][kw] * input[b_i][ic_i][stride_w * ow + kw)]\n+    ```\n+\n+    Note that depending of the size of your kernel, several (of the last)\n+    columns of the input might be lost. It is up to the user\n+    to add proper padding.\n+\n+    Args:\n+        input: [ * , in_channels  , * ] : Input is minibatch x in_channels x iW\n+        weight: [out_channels, in_channels, kW] - filters with kernel size kW\n+        bias:   [out_channels] - bias\n+        stride: the stride of the convolving kernel\n+    Output Shape:[ * , out_channels , * ]  : Output shape is precisely\n+                 minibatch x out_channels x floor((iW  + 2*padW - kW) / dW + 1)\n+    Examples:\n+        >>> filters = autograd.Variable(torch.randn(33, 16, 3))\n+        >>> inputs = autograd.Variable(torch.randn(20, 16, 50))\n+        >>> output = m(input)\n+    \"\"\"\n+    f = functions.conv.Conv2d(stride)\n+    input = input.unsqueeze(2)\n+    weight = weight.unsqueeze(2)\n+    return f(input, weight, bias) if bias is not None else f(input, weight)\n+\n+\n+def conv2d(input, weight, bias=None, stride=1, padding=0, groups=1):\n+    \"\"\"Applies a 2D convolution over an input image composed of several input\n+    planes.\n+\n+    ```\n+    The output value of the layer with input (b x iC x H x W) and filters\n+    (oC x iC x kH x kW) can be precisely described as:\n+    output[b_i][oc_i][h_i][w_i] = bias[oc_i]\n+                + sum_iC sum_{oh = 0, oH-1} sum_{ow = 0, oW-1} sum_{kh = 0 to kH-1} sum_{kw = 0 to kW-1}\n+                    weight[oc_i][ic_i][kh][kw] * input[b_i][ic_i][stride_h * oh + kh)][stride_w * ow + kw)]\n+    ```\n+\n+    Note that depending of the size of your kernel, several (of the last)\n+    columns or rows of the input image might be lost. It is up to the user\n+    to add proper padding in images.\n+\n+    Args:\n+        input: [ * , in_channels  , * , * ] : Input is minibatch x in_channels x iH x iW\n+        weight: [out_channels, in_channels, kW] - filters\n+        bias:   [out_channels] - bias\n+        stride: the stride of the convolving kernel.\n+                Can be a single number s or a tuple (sh x sw). Default: 1\n+        padding: implicit zero padding on the input.\n+                 Can be a single number s or a tuple. Default: 0\n+    Output Shape: [ * , out_channels , * , * ]  : Output shape is precisely\n+                        minibatch x \n+                        out_channels x\n+                        floor((iH  + 2*padH - kH) / dH + 1) x\n+                        floor((iW  + 2*padW - kW) / dW + 1)\n+    Examples:\n+        >>> # With square kernels and equal stride\n+        >>> filters = autograd.Variable(torch.randn(8,4,3,3))\n+        >>> inputs = autograd.Variable(torch.randn(1,4,5,5))\n+        >>> output = F.conv2d(input, filters, padding=1)\n+    \"\"\"\n+    f = functions.conv.Conv2d(stride, padding, groups)\n+    return f(input, weight, bias) if bias is not None else f(input, weight)\n+\n+\n+def conv_transpose2d(input, weight, bias=None, stride=1, padding=0, groups=1,\n+        output_padding=0):\n+    \"\"\"Applies a 2D deconvolution operator over an input image composed of\n+    several input planes.\n+    The deconvolution operator multiplies each input value element-wise by a", "path": "torch/nn/functional.py", "position": null, "original_position": 86, "commit_id": "0a50b8cbeb3edfcac15b65f12d922648ced195ba", "original_commit_id": "0afa4ccfe40bbdaadb6abde72724896a832160a8", "user": {"login": "szagoruyko", "id": 4953728, "node_id": "MDQ6VXNlcjQ5NTM3Mjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/4953728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szagoruyko", "html_url": "https://github.com/szagoruyko", "followers_url": "https://api.github.com/users/szagoruyko/followers", "following_url": "https://api.github.com/users/szagoruyko/following{/other_user}", "gists_url": "https://api.github.com/users/szagoruyko/gists{/gist_id}", "starred_url": "https://api.github.com/users/szagoruyko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szagoruyko/subscriptions", "organizations_url": "https://api.github.com/users/szagoruyko/orgs", "repos_url": "https://api.github.com/users/szagoruyko/repos", "events_url": "https://api.github.com/users/szagoruyko/events{/privacy}", "received_events_url": "https://api.github.com/users/szagoruyko/received_events", "type": "User", "site_admin": false}, "body": "I mostly copy-pasted the docs from nn.modules and changed argument documentation. This should probably be fixed in both places.", "created_at": "2016-12-28T14:03:39Z", "updated_at": "2018-11-23T15:32:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/354#discussion_r94035722", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/354", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/94035722"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/354#discussion_r94035722"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/354"}}, "body_html": "<p>I mostly copy-pasted the docs from nn.modules and changed argument documentation. This should probably be fixed in both places.</p>", "body_text": "I mostly copy-pasted the docs from nn.modules and changed argument documentation. This should probably be fixed in both places.", "in_reply_to_id": 93937538}