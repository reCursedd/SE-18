{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217204029", "pull_request_review_id": 154862065, "id": 217204029, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzIwNDAyOQ==", "diff_hunk": "@@ -36,95 +36,53 @@ AnomalyMetadata* Function::metadata() noexcept {\n   return anomaly_metadata_.get();\n }\n \n-/*\n- * Fix for #5534: prevent stack overflow on deletion of deep computation graph\n- *\n- * Sometimes one can end up with a very big computation graph of Functions\n- * and Edges. Each std::shared_ptr<Function> contains a list of Edge, and\n- * each Edge contains a std::shared_ptr<Function>. Deleting a\n- * std::shared_ptr<Function> can trigger the recursive deletion of other\n- * std::shared_ptr<Function>'s: this can stack overflow if the graph\n- * is deep enough. Here is an example of such a graph:\n- *\n- * shared_ptr<Function> -> Edge -> shared_ptr<Function> -> Edge -> ... -> shared_ptr<Function>\n- *\n- * The solution here is to use a custom deleter with each\n- * std::shared_ptr<Function>. The custom deleter keeps track of how many\n- * nested deleters it is in. When this number exceeds the maximum allowed\n- * depth, the Function* to be deleted are accumulated in a per-thread\n- * delete queue and handled by one of the deleters.\n- *\n- * Note that these custom deleters are NOT necessary for deleting PyFunction.\n- * This is because a THPFunction Python object owns a PyFunction that is in a\n- * computation graph. When Python objects get recursively destroyed, they\n- * are also queued into a delete list. This happens very early for them\n- * (at 50 deleters): https://github.com/python/cpython/blob/f320be77ffb73e3b9e7fc98c37b8df3975d84b40/Include/object.h#L1024-L1063\n- * so we don't need to worry about them.\n- */\n-\n-thread_local std::deque<Function*> deleteFunctionQueue;\n-thread_local size_t deleteFunctionRecursionDepth = 0;\n-\n-/*\n- * If this number is set too high, a deep computation graph can still\n- * stack overflow. The procedure for setting this number was to\n- * 1) find the smallest value that would not guard against stack overflows\n- *    on various machines\n- * 2) Take the minimum of all such values and subtract some leeway because\n- *    the memory of these stack frames will probably grow as time passes.\n- * Testing on a few machines machines, the magic numbers were:\n- * - Mac OSX (Macbook Pro 15) : ~60000\n- * - A beefy Ubuntu 16.04 box : ~15000\n- * - Windows AWS instance (g3.4xlarge): variable. My two attempts at different\n- *   times have gotten the following numbers: ~8300, 3669\n- */\n-#ifdef _WIN32\n-size_t deleteFunctionMaxRecursionDepth = 3000;\n-#else\n-size_t deleteFunctionMaxRecursionDepth = 10000;\n-#endif\n-\n-struct RecursionDepthCounter {\n- public:\n-  explicit RecursionDepthCounter() {\n-    ++deleteFunctionRecursionDepth;\n-  }\n-  ~RecursionDepthCounter() {\n-    --deleteFunctionRecursionDepth;\n-  }\n-\n-  size_t value() {\n-    return deleteFunctionRecursionDepth;\n+static void gatherFunctions(Function* func,\n+                            std::vector<std::shared_ptr<Function>>& stack) {\n+  for (auto& edge : func->next_edges()) {\n+    if (edge.function.use_count() == 1) {\n+      stack.emplace_back(std::move(edge.function));\n+    }\n   }\n-};\n+}\n \n /*\n- * Note that the custom deleter deletes in BFS style. Without using\n- * the custom deleter, the computation graph is deleted in a DFS style.\n- * The BFS deletion is valid (and safe) because if a shared_ptr<Function>\n- * 's reference count hits 0, nothing else will access it.\n- */\n+  * Fix for #5534: prevent stack overflow on deletion of deep computation graph\n+  * \n+  * Sometimes one can end up with a very big computation graph of Functions\n+  * and Edges. Each std::shared_ptr<Function> contains a list of Edge, and\n+  * each Edge contains a std::shared_ptr<Function>. Deleting a\n+  * std::shared_ptr<Function> can trigger the recursive deletion of other\n+  * std::shared_ptr<Function>'s: this can stack overflow if the graph\n+  * is deep enough. Here is an example of such a graph:\n+  *\n+  * shared_ptr<Function> -> Edge -> shared_ptr<Function> -> Edge -> ... -> shared_ptr<Function>\n+  *\n+  * The solution here is to detect when we are decrementing away the last\n+  * reference to a Function, and when doing so to buffer up the Function's\n+  * that will be recursively decremented.  We can then decrement (and free)\n+  * the original Function without causing a recursive cascade, before\n+  * draining the buffer applying the same behavior.  This is, in effect,\n+  * converting recursion to a loop, using a heap buffer in place of the\n+  * recursive call stack.\n+  */\n void deleteFunction(Function* function) {\n-  RecursionDepthCounter recursion_depth;\n+  // To avoid stack overflow on large computational graphs,\n+  // we need to track reference decrementing and freeing\n+  // on the heap.\n+  std::vector<std::shared_ptr<Function>> stack;\n+  gatherFunctions(function, stack);\n+  delete function;\n \n-  if (recursion_depth.value() > deleteFunctionMaxRecursionDepth) {\n-    deleteFunctionQueue.push_back(function);\n-    return;\n-  }\n+  while (!stack.empty()) {\n+    auto& curr_func = stack.back();\n \n-  delete function;\n+    if (curr_func.use_count() == 1) {", "path": "torch/csrc/autograd/function.cpp", "position": 115, "original_position": 115, "commit_id": "911581229e8ca9743427ad71865ba2a4493bda60", "original_commit_id": "911581229e8ca9743427ad71865ba2a4493bda60", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "I think `curr_func` is guaranteed to have `use_count = 1` here, right?", "created_at": "2018-09-12T22:01:31Z", "updated_at": "2018-11-23T15:51:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/11534#discussion_r217204029", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11534", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217204029"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11534#discussion_r217204029"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11534"}}, "body_html": "<p>I think <code>curr_func</code> is guaranteed to have <code>use_count = 1</code> here, right?</p>", "body_text": "I think curr_func is guaranteed to have use_count = 1 here, right?"}