{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438893347", "html_url": "https://github.com/pytorch/pytorch/issues/5667#issuecomment-438893347", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5667", "id": 438893347, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODg5MzM0Nw==", "user": {"login": "pencoa", "id": 7123063, "node_id": "MDQ6VXNlcjcxMjMwNjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/7123063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pencoa", "html_url": "https://github.com/pencoa", "followers_url": "https://api.github.com/users/pencoa/followers", "following_url": "https://api.github.com/users/pencoa/following{/other_user}", "gists_url": "https://api.github.com/users/pencoa/gists{/gist_id}", "starred_url": "https://api.github.com/users/pencoa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pencoa/subscriptions", "organizations_url": "https://api.github.com/users/pencoa/orgs", "repos_url": "https://api.github.com/users/pencoa/repos", "events_url": "https://api.github.com/users/pencoa/events{/privacy}", "received_events_url": "https://api.github.com/users/pencoa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-15T02:27:29Z", "updated_at": "2018-11-15T02:27:29Z", "author_association": "NONE", "body_html": "<p>My torch version is 0.4.1, but still face this problem.</p>\n<pre><code>/home/penzm/anaconda3/envs/DataEnv/lib/python3.6/site-packages/ipykernel_launcher.py:1: \nUserWarning: RNN module weights are not part of single contiguous chunk of memory. This means \nthey need to be compacted at every call, possibly greatly increasing memory usage. To compact \nweights again call flatten_parameters().\n  \"\"\"Entry point for launching an IPython kernel.\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-16-0c8427cd9cbc&gt; in &lt;module&gt;()\n----&gt; 1 rnn_outputs, (ht, ct) = rnn(rnn_inputs, (h0, c0))\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/modules/module.py in \n__call__(self, *input, **kwargs)\n475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--&gt; 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/modules/rnn.py in forward(self, \ninput, hx)\n    190             flat_weight=flat_weight\n    191         )\n--&gt; 192         output, hidden = func(input, self.all_weights, hx, batch_sizes)\n    193         if is_packed:\n    194             output = PackedSequence(output, batch_sizes)\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in forward(input, \n*fargs, **fkwargs)\n    322             func = decorator(func)\n    323 \n--&gt; 324         return func(input, *fargs, **fkwargs)\n    325 \n    326     return forward\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in forward(input, \nweight, hx, batch_sizes)\n    286             batch_first, dropout, train, bool(bidirectional),\n    287             list(batch_sizes.data) if variable_length else (),\n--&gt; 288             dropout_ts)\n    289 \n    290         if cx is not None:\n\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch\n</code></pre>", "body_text": "My torch version is 0.4.1, but still face this problem.\n/home/penzm/anaconda3/envs/DataEnv/lib/python3.6/site-packages/ipykernel_launcher.py:1: \nUserWarning: RNN module weights are not part of single contiguous chunk of memory. This means \nthey need to be compacted at every call, possibly greatly increasing memory usage. To compact \nweights again call flatten_parameters().\n  \"\"\"Entry point for launching an IPython kernel.\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-16-0c8427cd9cbc> in <module>()\n----> 1 rnn_outputs, (ht, ct) = rnn(rnn_inputs, (h0, c0))\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/modules/module.py in \n__call__(self, *input, **kwargs)\n475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--> 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/modules/rnn.py in forward(self, \ninput, hx)\n    190             flat_weight=flat_weight\n    191         )\n--> 192         output, hidden = func(input, self.all_weights, hx, batch_sizes)\n    193         if is_packed:\n    194             output = PackedSequence(output, batch_sizes)\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in forward(input, \n*fargs, **fkwargs)\n    322             func = decorator(func)\n    323 \n--> 324         return func(input, *fargs, **fkwargs)\n    325 \n    326     return forward\n\n~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in forward(input, \nweight, hx, batch_sizes)\n    286             batch_first, dropout, train, bool(bidirectional),\n    287             list(batch_sizes.data) if variable_length else (),\n--> 288             dropout_ts)\n    289 \n    290         if cx is not None:\n\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch", "body": "My torch version is 0.4.1, but still face this problem.\r\n\r\n    /home/penzm/anaconda3/envs/DataEnv/lib/python3.6/site-packages/ipykernel_launcher.py:1: \r\n    UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means \r\n    they need to be compacted at every call, possibly greatly increasing memory usage. To compact \r\n    weights again call flatten_parameters().\r\n      \"\"\"Entry point for launching an IPython kernel.\r\n    ---------------------------------------------------------------------------\r\n    RuntimeError                              Traceback (most recent call last)\r\n    <ipython-input-16-0c8427cd9cbc> in <module>()\r\n    ----> 1 rnn_outputs, (ht, ct) = rnn(rnn_inputs, (h0, c0))\r\n    \r\n    ~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/modules/module.py in \r\n    __call__(self, *input, **kwargs)\r\n    475             result = self._slow_forward(*input, **kwargs)\r\n        476         else:\r\n    --> 477             result = self.forward(*input, **kwargs)\r\n        478         for hook in self._forward_hooks.values():\r\n        479             hook_result = hook(self, input, result)\r\n    \r\n    ~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/modules/rnn.py in forward(self, \r\n    input, hx)\r\n        190             flat_weight=flat_weight\r\n        191         )\r\n    --> 192         output, hidden = func(input, self.all_weights, hx, batch_sizes)\r\n        193         if is_packed:\r\n        194             output = PackedSequence(output, batch_sizes)\r\n    \r\n    ~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in forward(input, \r\n    *fargs, **fkwargs)\r\n        322             func = decorator(func)\r\n        323 \r\n    --> 324         return func(input, *fargs, **fkwargs)\r\n        325 \r\n        326     return forward\r\n\r\n    ~/anaconda3/envs/DataEnv/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in forward(input, \r\n    weight, hx, batch_sizes)\r\n        286             batch_first, dropout, train, bool(bidirectional),\r\n        287             list(batch_sizes.data) if variable_length else (),\r\n    --> 288             dropout_ts)\r\n        289 \r\n        290         if cx is not None:\r\n    \r\n    RuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/ATen/native/cudnn/RNN.cpp:491, please report a bug to PyTorch. parameter types mismatch"}