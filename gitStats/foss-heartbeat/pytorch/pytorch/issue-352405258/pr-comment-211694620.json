{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/211694620", "pull_request_review_id": 148171002, "id": 211694620, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMTY5NDYyMA==", "diff_hunk": "@@ -315,5 +315,32 @@ Tensor& matmul_out(Tensor &result, const Tensor & tensor1, const Tensor & tensor\n   return result;\n }\n \n+Tensor frobenius_norm(const Tensor& self) {\n+  IntList dim = {0, 1};\n+  return at::native::frobenius_norm(self, dim);\n }\n+\n+Tensor frobenius_norm(const Tensor& self, IntList dim, bool keepdim) {\n+  AT_CHECK(\n+      dim.size() <= 2,\n+      \"Improper number of chosend dimensions to do frobenius norm.\");\n+  if (dim.size() == 0) {\n+    Tensor result = self.type().tensor();\n+    return at::native::norm_out(result, self, 2, self.dim(), keepdim);\n+  }\n+  if (dim.size() == 1) {\n+    return at::native::norm(self, 2, dim[0], keepdim);\n+  }\n+  Tensor result = self.type().tensor();\n+  return at::sqrt(at::native::sum(self * self, dim, keepdim));\n }\n+\n+Tensor nuclear_norm(const Tensor& self) {\n+  AT_CHECK(", "path": "aten/src/ATen/native/LinearAlgebra.cpp", "position": null, "original_position": 25, "commit_id": "f0003a27f579fdbfe9725225dec4d6796d9f588f", "original_commit_id": "2f48e4bf618ee29f017b989e0f9eeb0ef849df08", "user": {"login": "yya007", "id": 11239571, "node_id": "MDQ6VXNlcjExMjM5NTcx", "avatar_url": "https://avatars3.githubusercontent.com/u/11239571?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yya007", "html_url": "https://github.com/yya007", "followers_url": "https://api.github.com/users/yya007/followers", "following_url": "https://api.github.com/users/yya007/following{/other_user}", "gists_url": "https://api.github.com/users/yya007/gists{/gist_id}", "starred_url": "https://api.github.com/users/yya007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yya007/subscriptions", "organizations_url": "https://api.github.com/users/yya007/orgs", "repos_url": "https://api.github.com/users/yya007/repos", "events_url": "https://api.github.com/users/yya007/events{/privacy}", "received_events_url": "https://api.github.com/users/yya007/received_events", "type": "User", "site_admin": false}, "body": "The svd function doesn't support batched tensors now. This PR intends to create the matrix norms only. Extending the function to batched matrix norm is what we want to do in the future.  We may have another PR to let the nuclear norm to support batched norm.", "created_at": "2018-08-21T17:36:51Z", "updated_at": "2018-11-23T15:49:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/10722#discussion_r211694620", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10722", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/211694620"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10722#discussion_r211694620"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10722"}}, "body_html": "<p>The svd function doesn't support batched tensors now. This PR intends to create the matrix norms only. Extending the function to batched matrix norm is what we want to do in the future.  We may have another PR to let the nuclear norm to support batched norm.</p>", "body_text": "The svd function doesn't support batched tensors now. This PR intends to create the matrix norms only. Extending the function to batched matrix norm is what we want to do in the future.  We may have another PR to let the nuclear norm to support batched norm.", "in_reply_to_id": 211655298}