{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13058", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13058/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13058/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13058/events", "html_url": "https://github.com/pytorch/pytorch/issues/13058", "id": 373597506, "node_id": "MDU6SXNzdWUzNzM1OTc1MDY=", "number": 13058, "title": "Backward pass over torch.nn.functional.pad is extremely slow with half tensors", "user": {"login": "rotabulo", "id": 28045295, "node_id": "MDQ6VXNlcjI4MDQ1Mjk1", "avatar_url": "https://avatars3.githubusercontent.com/u/28045295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rotabulo", "html_url": "https://github.com/rotabulo", "followers_url": "https://api.github.com/users/rotabulo/followers", "following_url": "https://api.github.com/users/rotabulo/following{/other_user}", "gists_url": "https://api.github.com/users/rotabulo/gists{/gist_id}", "starred_url": "https://api.github.com/users/rotabulo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rotabulo/subscriptions", "organizations_url": "https://api.github.com/users/rotabulo/orgs", "repos_url": "https://api.github.com/users/rotabulo/repos", "events_url": "https://api.github.com/users/rotabulo/events{/privacy}", "received_events_url": "https://api.github.com/users/rotabulo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-24T17:38:46Z", "updated_at": "2018-10-29T17:48:48Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>The backward pass over <code>torch.nn.functional.pad</code> is more than 300x slower with half tensors compared to fp32 tensor for the example below.</p>\n<h2>To Reproduce</h2>\n<pre><code>import torch\nimport time\n\ndef exec(x):\n  y=torch.nn.functional.pad(x,pad=(47,48,47,48),mode=\"replicate\")\n  torch.cuda.synchronize()\n  tic=time.time()\n  y.sum().backward()\n  torch.cuda.synchronize()\n  return time.time()-tic\n\nx=torch.rand(4,2048,1,1, requires_grad=True).cuda()\nprint(\"fp32: {}\".format(exec(x)))\nprint(\"fp16: {}\".format(exec(x.half())))\n\n</code></pre>\n<p>My output:</p>\n<pre><code>fp32: 0.14527177810668945\nfp16: 50.3135507106781\n</code></pre>\n<h2>Expected behavior</h2>\n<p>At least as fast as fp32.</p>\n<h2>Environment</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.2.148</p>\n<p>OS: Ubuntu 18.04.1 LTS<br>\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.2.148<br>\nGPU models and configuration:<br>\nGPU 0: Tesla V100-SXM2-32GB<br>\nGPU 1: Tesla V100-SXM2-32GB<br>\nGPU 2: Tesla V100-SXM2-32GB<br>\nGPU 3: Tesla V100-SXM2-32GB<br>\nGPU 4: Tesla V100-SXM2-32GB<br>\nGPU 5: Tesla V100-SXM2-32GB<br>\nGPU 6: Tesla V100-SXM2-32GB<br>\nGPU 7: Tesla V100-SXM2-32GB</p>\n<p>Nvidia driver version: 396.37<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1<br>\n/usr/local/cuda-9.2/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] cuda92                    1.0                           0    pytorch<br>\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch<br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchnet                  0.0.4                     <br>\n[conda] torchvision               0.2.1                    py37_1    pytorch</p>", "body_text": "\ud83d\udc1b Bug\nThe backward pass over torch.nn.functional.pad is more than 300x slower with half tensors compared to fp32 tensor for the example below.\nTo Reproduce\nimport torch\nimport time\n\ndef exec(x):\n  y=torch.nn.functional.pad(x,pad=(47,48,47,48),mode=\"replicate\")\n  torch.cuda.synchronize()\n  tic=time.time()\n  y.sum().backward()\n  torch.cuda.synchronize()\n  return time.time()-tic\n\nx=torch.rand(4,2048,1,1, requires_grad=True).cuda()\nprint(\"fp32: {}\".format(exec(x)))\nprint(\"fp16: {}\".format(exec(x.half())))\n\n\nMy output:\nfp32: 0.14527177810668945\nfp16: 50.3135507106781\n\nExpected behavior\nAt least as fast as fp32.\nEnvironment\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\nCMake version: Could not collect\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: 9.2.148\nGPU models and configuration:\nGPU 0: Tesla V100-SXM2-32GB\nGPU 1: Tesla V100-SXM2-32GB\nGPU 2: Tesla V100-SXM2-32GB\nGPU 3: Tesla V100-SXM2-32GB\nGPU 4: Tesla V100-SXM2-32GB\nGPU 5: Tesla V100-SXM2-32GB\nGPU 6: Tesla V100-SXM2-32GB\nGPU 7: Tesla V100-SXM2-32GB\nNvidia driver version: 396.37\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] cuda92                    1.0                           0    pytorch\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\n[conda] torchfile                 0.1.0                     \n[conda] torchnet                  0.0.4                     \n[conda] torchvision               0.2.1                    py37_1    pytorch", "body": "## \ud83d\udc1b Bug\r\n\r\nThe backward pass over `torch.nn.functional.pad` is more than 300x slower with half tensors compared to fp32 tensor for the example below.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\nimport time\r\n\r\ndef exec(x):\r\n  y=torch.nn.functional.pad(x,pad=(47,48,47,48),mode=\"replicate\")\r\n  torch.cuda.synchronize()\r\n  tic=time.time()\r\n  y.sum().backward()\r\n  torch.cuda.synchronize()\r\n  return time.time()-tic\r\n\r\nx=torch.rand(4,2048,1,1, requires_grad=True).cuda()\r\nprint(\"fp32: {}\".format(exec(x)))\r\nprint(\"fp16: {}\".format(exec(x.half())))\r\n\r\n```\r\n\r\nMy output:\r\n```\r\nfp32: 0.14527177810668945\r\nfp16: 50.3135507106781\r\n```\r\n\r\n## Expected behavior\r\n\r\nAt least as fast as fp32.\r\n\r\n## Environment\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 396.37\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchnet                  0.0.4                     <pip>\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n"}