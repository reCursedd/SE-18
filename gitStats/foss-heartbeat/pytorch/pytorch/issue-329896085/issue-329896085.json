{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8198", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8198/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8198/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8198/events", "html_url": "https://github.com/pytorch/pytorch/issues/8198", "id": 329896085, "node_id": "MDU6SXNzdWUzMjk4OTYwODU=", "number": 8198, "title": "GRUCELL crashes python for more than 2 layers", "user": {"login": "ChristianF88", "id": 9096348, "node_id": "MDQ6VXNlcjkwOTYzNDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/9096348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChristianF88", "html_url": "https://github.com/ChristianF88", "followers_url": "https://api.github.com/users/ChristianF88/followers", "following_url": "https://api.github.com/users/ChristianF88/following{/other_user}", "gists_url": "https://api.github.com/users/ChristianF88/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChristianF88/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChristianF88/subscriptions", "organizations_url": "https://api.github.com/users/ChristianF88/orgs", "repos_url": "https://api.github.com/users/ChristianF88/repos", "events_url": "https://api.github.com/users/ChristianF88/events{/privacy}", "received_events_url": "https://api.github.com/users/ChristianF88/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2018-06-06T14:35:41Z", "updated_at": "2018-06-13T17:00:22Z", "closed_at": "2018-06-11T13:45:41Z", "author_association": "NONE", "body_html": "<p>Hi there,</p>\n<p>my problem is pretty straight forward. I'm using a multilayer GRU network for a regression problem. When I specify the layers to be greater than 2 Python crashes. If I use LSTM cells instead I do not have this problem.</p>\n<p>I do run windows 10 - 64 bit with Python 3.6. That's the code I'm using:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.optim as optim\n\n\nclass GRU(nn.Module):\n    def __init__(self,input_size,hidden_size,num_layers,time_steps):\n        super().__init__()\n        \n        self.input_size=input_size\n        self.hidden_size=hidden_size\n        self.num_layers=num_layers\n        self.time_steps=time_steps\n        \n        self.GRU_lyrs=nn.GRU(input_size = input_size,hidden_size = hidden_size,num_layers = num_layers,batch_first=True)\n        self.outp=nn.Linear(time_steps,time_steps)\n        \n        return\n    \n    def forward(self,x,hstate):\n        x,hstate=self.GRU_lyrs(x,hstate)\n        hstate = Variable(hstate.data)\n        x=self.outp(x[:, :, -1])\n        return x, hstate\n\nclass Sampler():\n    def __init__(self):\n        self.count=0\n        return\n    \n    def sample(self,batchsize):\n        X1=[]\n        X2=[]\n        Y=[]\n        num_ts=400\n        for i in range(batchsize):\n            x=np.linspace(self.count*np.pi,(self.count+4)*np.pi,num_ts)\n            x1=np.sin(x)\n            X1.append(x1)\n            x2=np.cos(x)+2\n            X2.append(x2)\n            Y.append((x1/x2))\n            self.count+=1\n        return np.swapaxes(np.array([X1,X2]).T,0,1),np.array(Y)\n\n\ninput_size=2\nhidden_size=200\nnum_layers=2\ntime_steps=400\n\nlossvals=[]\n\nS=Sampler()\nx,y=S.sample(29)\n\nX=torch.Tensor(x)\nY=torch.Tensor(y)\n\nmynn=GRU(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,time_steps=time_steps)\nh_state=None\n\nlrfac=0.95\n\n\nfor i in range(100):\n    \n    if i%20==0:\n        print(i)\n    Y_pred, h_state = mynn(X,h_state)\n    optimizer=optim.Adam(mynn.parameters(),lr=0.01*lrfac**i)\n    \n    error=nn.MSELoss()\n    loss=error(Y_pred,Y)\n    mynn.zero_grad()\n\n    loss.backward()\n\n    optimizer.step()\n\n    lossvals.append(loss.item())\n\n\n</code></pre>\n<p>Thanks for your help!! :)</p>\n<p>Have a great one!</p>", "body_text": "Hi there,\nmy problem is pretty straight forward. I'm using a multilayer GRU network for a regression problem. When I specify the layers to be greater than 2 Python crashes. If I use LSTM cells instead I do not have this problem.\nI do run windows 10 - 64 bit with Python 3.6. That's the code I'm using:\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.optim as optim\n\n\nclass GRU(nn.Module):\n    def __init__(self,input_size,hidden_size,num_layers,time_steps):\n        super().__init__()\n        \n        self.input_size=input_size\n        self.hidden_size=hidden_size\n        self.num_layers=num_layers\n        self.time_steps=time_steps\n        \n        self.GRU_lyrs=nn.GRU(input_size = input_size,hidden_size = hidden_size,num_layers = num_layers,batch_first=True)\n        self.outp=nn.Linear(time_steps,time_steps)\n        \n        return\n    \n    def forward(self,x,hstate):\n        x,hstate=self.GRU_lyrs(x,hstate)\n        hstate = Variable(hstate.data)\n        x=self.outp(x[:, :, -1])\n        return x, hstate\n\nclass Sampler():\n    def __init__(self):\n        self.count=0\n        return\n    \n    def sample(self,batchsize):\n        X1=[]\n        X2=[]\n        Y=[]\n        num_ts=400\n        for i in range(batchsize):\n            x=np.linspace(self.count*np.pi,(self.count+4)*np.pi,num_ts)\n            x1=np.sin(x)\n            X1.append(x1)\n            x2=np.cos(x)+2\n            X2.append(x2)\n            Y.append((x1/x2))\n            self.count+=1\n        return np.swapaxes(np.array([X1,X2]).T,0,1),np.array(Y)\n\n\ninput_size=2\nhidden_size=200\nnum_layers=2\ntime_steps=400\n\nlossvals=[]\n\nS=Sampler()\nx,y=S.sample(29)\n\nX=torch.Tensor(x)\nY=torch.Tensor(y)\n\nmynn=GRU(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,time_steps=time_steps)\nh_state=None\n\nlrfac=0.95\n\n\nfor i in range(100):\n    \n    if i%20==0:\n        print(i)\n    Y_pred, h_state = mynn(X,h_state)\n    optimizer=optim.Adam(mynn.parameters(),lr=0.01*lrfac**i)\n    \n    error=nn.MSELoss()\n    loss=error(Y_pred,Y)\n    mynn.zero_grad()\n\n    loss.backward()\n\n    optimizer.step()\n\n    lossvals.append(loss.item())\n\n\n\nThanks for your help!! :)\nHave a great one!", "body": "Hi there, \r\n\r\nmy problem is pretty straight forward. I'm using a multilayer GRU network for a regression problem. When I specify the layers to be greater than 2 Python crashes. If I use LSTM cells instead I do not have this problem.\r\n\r\nI do run windows 10 - 64 bit with Python 3.6. That's the code I'm using:\r\n\r\n\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport torch.optim as optim\r\n\r\n\r\nclass GRU(nn.Module):\r\n    def __init__(self,input_size,hidden_size,num_layers,time_steps):\r\n        super().__init__()\r\n        \r\n        self.input_size=input_size\r\n        self.hidden_size=hidden_size\r\n        self.num_layers=num_layers\r\n        self.time_steps=time_steps\r\n        \r\n        self.GRU_lyrs=nn.GRU(input_size = input_size,hidden_size = hidden_size,num_layers = num_layers,batch_first=True)\r\n        self.outp=nn.Linear(time_steps,time_steps)\r\n        \r\n        return\r\n    \r\n    def forward(self,x,hstate):\r\n        x,hstate=self.GRU_lyrs(x,hstate)\r\n        hstate = Variable(hstate.data)\r\n        x=self.outp(x[:, :, -1])\r\n        return x, hstate\r\n\r\nclass Sampler():\r\n    def __init__(self):\r\n        self.count=0\r\n        return\r\n    \r\n    def sample(self,batchsize):\r\n        X1=[]\r\n        X2=[]\r\n        Y=[]\r\n        num_ts=400\r\n        for i in range(batchsize):\r\n            x=np.linspace(self.count*np.pi,(self.count+4)*np.pi,num_ts)\r\n            x1=np.sin(x)\r\n            X1.append(x1)\r\n            x2=np.cos(x)+2\r\n            X2.append(x2)\r\n            Y.append((x1/x2))\r\n            self.count+=1\r\n        return np.swapaxes(np.array([X1,X2]).T,0,1),np.array(Y)\r\n\r\n\r\ninput_size=2\r\nhidden_size=200\r\nnum_layers=2\r\ntime_steps=400\r\n\r\nlossvals=[]\r\n\r\nS=Sampler()\r\nx,y=S.sample(29)\r\n\r\nX=torch.Tensor(x)\r\nY=torch.Tensor(y)\r\n\r\nmynn=GRU(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,time_steps=time_steps)\r\nh_state=None\r\n\r\nlrfac=0.95\r\n\r\n\r\nfor i in range(100):\r\n    \r\n    if i%20==0:\r\n        print(i)\r\n    Y_pred, h_state = mynn(X,h_state)\r\n    optimizer=optim.Adam(mynn.parameters(),lr=0.01*lrfac**i)\r\n    \r\n    error=nn.MSELoss()\r\n    loss=error(Y_pred,Y)\r\n    mynn.zero_grad()\r\n\r\n    loss.backward()\r\n\r\n    optimizer.step()\r\n\r\n    lossvals.append(loss.item())\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\nThanks for your help!! :)\r\n\r\nHave a great one!"}