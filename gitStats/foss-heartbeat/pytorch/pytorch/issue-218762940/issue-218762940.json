{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1171", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1171/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1171/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1171/events", "html_url": "https://github.com/pytorch/pytorch/issues/1171", "id": 218762940, "node_id": "MDU6SXNzdWUyMTg3NjI5NDA=", "number": 1171, "title": "CUDNN_STATUS_NOT_INITIALIZED", "user": {"login": "robintibor", "id": 1178948, "node_id": "MDQ6VXNlcjExNzg5NDg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1178948?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robintibor", "html_url": "https://github.com/robintibor", "followers_url": "https://api.github.com/users/robintibor/followers", "following_url": "https://api.github.com/users/robintibor/following{/other_user}", "gists_url": "https://api.github.com/users/robintibor/gists{/gist_id}", "starred_url": "https://api.github.com/users/robintibor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robintibor/subscriptions", "organizations_url": "https://api.github.com/users/robintibor/orgs", "repos_url": "https://api.github.com/users/robintibor/repos", "events_url": "https://api.github.com/users/robintibor/events{/privacy}", "received_events_url": "https://api.github.com/users/robintibor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-04-02T12:57:18Z", "updated_at": "2017-04-04T08:16:17Z", "closed_at": "2017-04-02T17:44:39Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nI get a CUDNN_STATUS_NOT_INITIALIZED error when running a modified version of <a href=\"http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\" rel=\"nofollow\">Training a classifier \u2014 PyTorch Tutorials documentation</a>:</p>\n<pre><code>$ python3.5 pytorch_cudnn.py\nFiles already downloaded and verified\nFiles already downloaded and verified\nCudnn version: 6020\nTraceback (most recent call last):\n  File \"pytorch_cudnn.py\", line 72, in &lt;module&gt;\n    outputs = net(inputs)\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"pytorch_cudnn.py\", line 39, in forward\n    x = self.pool(F.relu(self.conv1(x)))\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/conv.py\", line 237, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/functional.py\", line 39, in conv2d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_NOT_INITIALIZED\n</code></pre>\n<p>This is the pytorch_cudnn.py file, it runs fine without the calls to .cuda(). My theano programs run fine with cudnn.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torchvision\n<span class=\"pl-k\">import</span> torchvision.transforms <span class=\"pl-k\">as</span> transforms\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">from</span> torch.backends <span class=\"pl-k\">import</span> cudnn\n\n\ntransform <span class=\"pl-k\">=</span> transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>), (<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>))])\n\ntrainset <span class=\"pl-k\">=</span> torchvision.datasets.CIFAR10(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                        <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transform)\ntrainloader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(trainset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,\n                                          <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\ntestset <span class=\"pl-k\">=</span> torchvision.datasets.CIFAR10(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                       <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transform)\ntestloader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(testset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,\n                                         <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\nclasses <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>plane<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>car<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bird<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cat<span class=\"pl-pds\">'</span></span>,\n           <span class=\"pl-s\"><span class=\"pl-pds\">'</span>deer<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dog<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>frog<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>horse<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ship<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>truck<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.pool  <span class=\"pl-k\">=</span> nn.MaxPool2d(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.fc1   <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">16</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">120</span>)\n        <span class=\"pl-c1\">self</span>.fc2   <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">120</span>, <span class=\"pl-c1\">84</span>)\n        <span class=\"pl-c1\">self</span>.fc3   <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">84</span>, <span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.conv1(x)))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.conv2(x)))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span>)\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc2(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc3(x)\n        <span class=\"pl-k\">return</span> x\n\nnet <span class=\"pl-k\">=</span> Net()\n\n\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()\noptimizer <span class=\"pl-k\">=</span> optim.SGD(net.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Cudnn version: <span class=\"pl-c1\">{<span class=\"pl-k\">:d</span>}</span><span class=\"pl-pds\">\"</span></span>.format(cudnn.version()))\nnet.cuda()\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> loop over the dataset multiple times</span>\n\n    running_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    <span class=\"pl-k\">for</span> i, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(trainloader, <span class=\"pl-c1\">0</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> get the inputs</span>\n        inputs, labels <span class=\"pl-k\">=</span> data\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> move to GPU</span>\n        inputs <span class=\"pl-k\">=</span> inputs.cuda()\n        labels <span class=\"pl-k\">=</span> labels.cuda()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> wrap them in Variable</span>\n        inputs, labels <span class=\"pl-k\">=</span> Variable(inputs), Variable(labels)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> zero the parameter gradients</span>\n        optimizer.zero_grad()\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> forward + backward + optimize</span>\n        outputs <span class=\"pl-k\">=</span> net(inputs)\n        loss <span class=\"pl-k\">=</span> criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> print statistics</span>\n        running_loss <span class=\"pl-k\">+=</span> loss.data[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1999</span>:    <span class=\"pl-c\"><span class=\"pl-c\">#</span> print every 2000 mini-batches</span>\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>[<span class=\"pl-c1\">%d</span>, <span class=\"pl-c1\">%5d</span>] loss: <span class=\"pl-c1\">%.3f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, running_loss <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2000</span>))\n            running_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finished Training<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "Hi,\nI get a CUDNN_STATUS_NOT_INITIALIZED error when running a modified version of Training a classifier \u2014 PyTorch Tutorials documentation:\n$ python3.5 pytorch_cudnn.py\nFiles already downloaded and verified\nFiles already downloaded and verified\nCudnn version: 6020\nTraceback (most recent call last):\n  File \"pytorch_cudnn.py\", line 72, in <module>\n    outputs = net(inputs)\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"pytorch_cudnn.py\", line 39, in forward\n    x = self.pool(F.relu(self.conv1(x)))\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/conv.py\", line 237, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/functional.py\", line 39, in conv2d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_NOT_INITIALIZED\n\nThis is the pytorch_cudnn.py file, it runs fine without the calls to .cuda(). My theano programs run fine with cudnn.\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.backends import cudnn\n\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16 * 5 * 5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\nprint(\"Cudnn version: {:d}\".format(cudnn.version()))\nnet.cuda()\n\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n        # move to GPU\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        # wrap them in Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.data[0]\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')", "body": "Hi,\r\nI get a CUDNN_STATUS_NOT_INITIALIZED error when running a modified version of <a href=\"http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\">Training a classifier \u2014 PyTorch Tutorials documentation</a>:\r\n```\r\n$ python3.5 pytorch_cudnn.py\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nCudnn version: 6020\r\nTraceback (most recent call last):\r\n  File \"pytorch_cudnn.py\", line 72, in <module>\r\n    outputs = net(inputs)\r\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"pytorch_cudnn.py\", line 39, in forward\r\n    x = self.pool(F.relu(self.conv1(x)))\r\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/modules/conv.py\", line 237, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"/home/schirrmr/.local/lib/python3.5/site-packages/torch/nn/functional.py\", line 39, in conv2d\r\n    return f(input, weight, bias)\r\nRuntimeError: CUDNN_STATUS_NOT_INITIALIZED\r\n```\r\n\r\nThis is the pytorch_cudnn.py file, it runs fine without the calls to .cuda(). My theano programs run fine with cudnn.\r\n```python\r\nimport torch\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.backends import cudnn\r\n\r\n\r\ntransform = transforms.Compose(\r\n    [transforms.ToTensor(),\r\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n\r\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\r\n                                        download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\r\n                                          shuffle=True, num_workers=2)\r\n\r\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\r\n                                       download=True, transform=transform)\r\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\r\n                                         shuffle=False, num_workers=2)\r\n\r\nclasses = ('plane', 'car', 'bird', 'cat',\r\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 6, 5)\r\n        self.pool  = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(6, 16, 5)\r\n        self.fc1   = nn.Linear(16 * 5 * 5, 120)\r\n        self.fc2   = nn.Linear(120, 84)\r\n        self.fc3   = nn.Linear(84, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.pool(F.relu(self.conv1(x)))\r\n        x = self.pool(F.relu(self.conv2(x)))\r\n        x = x.view(-1, 16 * 5 * 5)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = self.fc3(x)\r\n        return x\r\n\r\nnet = Net()\r\n\r\n\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\r\n\r\nprint(\"Cudnn version: {:d}\".format(cudnn.version()))\r\nnet.cuda()\r\n\r\nfor epoch in range(2):  # loop over the dataset multiple times\r\n\r\n    running_loss = 0.0\r\n    for i, data in enumerate(trainloader, 0):\r\n        # get the inputs\r\n        inputs, labels = data\r\n        # move to GPU\r\n        inputs = inputs.cuda()\r\n        labels = labels.cuda()\r\n        # wrap them in Variable\r\n        inputs, labels = Variable(inputs), Variable(labels)\r\n\r\n        # zero the parameter gradients\r\n        optimizer.zero_grad()\r\n\r\n        # forward + backward + optimize\r\n        outputs = net(inputs)\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # print statistics\r\n        running_loss += loss.data[0]\r\n        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))\r\n            running_loss = 0.0\r\n\r\nprint('Finished Training')\r\n```"}