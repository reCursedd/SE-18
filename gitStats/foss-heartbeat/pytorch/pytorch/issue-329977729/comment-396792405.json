{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396792405", "html_url": "https://github.com/pytorch/pytorch/pull/8205#issuecomment-396792405", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8205", "id": 396792405, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Njc5MjQwNQ==", "user": {"login": "peterjc123", "id": 9998726, "node_id": "MDQ6VXNlcjk5OTg3MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9998726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterjc123", "html_url": "https://github.com/peterjc123", "followers_url": "https://api.github.com/users/peterjc123/followers", "following_url": "https://api.github.com/users/peterjc123/following{/other_user}", "gists_url": "https://api.github.com/users/peterjc123/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterjc123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterjc123/subscriptions", "organizations_url": "https://api.github.com/users/peterjc123/orgs", "repos_url": "https://api.github.com/users/peterjc123/repos", "events_url": "https://api.github.com/users/peterjc123/events{/privacy}", "received_events_url": "https://api.github.com/users/peterjc123/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-13T02:20:28Z", "updated_at": "2018-06-13T02:38:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is my guess though. <code>test_nn.py</code> uses <code>TEST_CUDA</code> and <code>TEST_MULTIGPU</code>. And these variables are causing CUDA OOM too. Maybe some memory will be allocated using <code>torch.cuda.is_available()</code> and since it is not referenced in the spawned processes, it is then released, thus this issue emerges.  But I don't know whether it is the case. But if it is true, then what about making the evaluations of these variables lazy? What do you think?</p>", "body_text": "This is my guess though. test_nn.py uses TEST_CUDA and TEST_MULTIGPU. And these variables are causing CUDA OOM too. Maybe some memory will be allocated using torch.cuda.is_available() and since it is not referenced in the spawned processes, it is then released, thus this issue emerges.  But I don't know whether it is the case. But if it is true, then what about making the evaluations of these variables lazy? What do you think?", "body": "This is my guess though. `test_nn.py` uses `TEST_CUDA` and `TEST_MULTIGPU`. And these variables are causing CUDA OOM too. Maybe some memory will be allocated using `torch.cuda.is_available()` and since it is not referenced in the spawned processes, it is then released, thus this issue emerges.  But I don't know whether it is the case. But if it is true, then what about making the evaluations of these variables lazy? What do you think?"}