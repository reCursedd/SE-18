{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/402129385", "html_url": "https://github.com/pytorch/pytorch/issues/8741#issuecomment-402129385", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8741", "id": 402129385, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjEyOTM4NQ==", "user": {"login": "0phoff", "id": 11853089, "node_id": "MDQ6VXNlcjExODUzMDg5", "avatar_url": "https://avatars3.githubusercontent.com/u/11853089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0phoff", "html_url": "https://github.com/0phoff", "followers_url": "https://api.github.com/users/0phoff/followers", "following_url": "https://api.github.com/users/0phoff/following{/other_user}", "gists_url": "https://api.github.com/users/0phoff/gists{/gist_id}", "starred_url": "https://api.github.com/users/0phoff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0phoff/subscriptions", "organizations_url": "https://api.github.com/users/0phoff/orgs", "repos_url": "https://api.github.com/users/0phoff/repos", "events_url": "https://api.github.com/users/0phoff/events{/privacy}", "received_events_url": "https://api.github.com/users/0phoff/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-03T11:56:13Z", "updated_at": "2018-07-03T11:56:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Something like this seems to work for me.</p>\n<p>I am not sure how the optimizer's <code>state</code> defaultdict is organized.<br>\nIt seems as if it is a dictionary with the Tensors of <code>self.param_groups</code> as key and then the value is a dict containing the actual state tensors as values (with their names as key).<br>\nAre there other ways the <code>state</code> is used? Like global tensors or so?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">optimizer_to</span>(<span class=\"pl-smi\">optim</span>, <span class=\"pl-smi\">device</span>):\n    <span class=\"pl-k\">for</span> param <span class=\"pl-k\">in</span> optim.state.values():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not sure there are any global tensors in the state dict</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(param, torch.Tensor):\n            param.data <span class=\"pl-k\">=</span> param.data.to(device)\n            <span class=\"pl-k\">if</span> param._grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                param._grad.data <span class=\"pl-k\">=</span> param._grad.data.to(device)\n        <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">isinstance</span>(param, <span class=\"pl-c1\">dict</span>):\n            <span class=\"pl-k\">for</span> subparam <span class=\"pl-k\">in</span> param.values():\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(subparam, torch.Tensor):\n                    subparam.data <span class=\"pl-k\">=</span> subparam.data.to(device)\n                    <span class=\"pl-k\">if</span> subparam._grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                        subparam._grad.data <span class=\"pl-k\">=</span> subparam._grad.data.to(device)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">scheduler_to</span>(<span class=\"pl-smi\">sched</span>, <span class=\"pl-smi\">device</span>):\n    <span class=\"pl-k\">for</span> param <span class=\"pl-k\">in</span> sched.<span class=\"pl-c1\">__dict__</span>.values():\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(param, torch.Tensor):\n            param.data <span class=\"pl-k\">=</span> param.data.to(device)\n            <span class=\"pl-k\">if</span> param._grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                param._grad.data <span class=\"pl-k\">=</span> param._grad.data.to(device)</pre></div>\n<hr>\n<p>I really enjoy working with PyTorch, but I have a feeling the optimizers/schedulers do not have a straight forward implementation. (mostly schedulers though).<br>\nIs there a roadmap to create a more unified API for them? Is it something you could use or want help with?</p>", "body_text": "Something like this seems to work for me.\nI am not sure how the optimizer's state defaultdict is organized.\nIt seems as if it is a dictionary with the Tensors of self.param_groups as key and then the value is a dict containing the actual state tensors as values (with their names as key).\nAre there other ways the state is used? Like global tensors or so?\ndef optimizer_to(optim, device):\n    for param in optim.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(device)\n\ndef scheduler_to(sched, device):\n    for param in sched.__dict__.values():\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n\nI really enjoy working with PyTorch, but I have a feeling the optimizers/schedulers do not have a straight forward implementation. (mostly schedulers though).\nIs there a roadmap to create a more unified API for them? Is it something you could use or want help with?", "body": "Something like this seems to work for me.\r\n\r\nI am not sure how the optimizer's ``state`` defaultdict is organized.  \r\nIt seems as if it is a dictionary with the Tensors of ``self.param_groups`` as key and then the value is a dict containing the actual state tensors as values (with their names as key).  \r\nAre there other ways the ``state`` is used? Like global tensors or so?\r\n\r\n```python\r\ndef optimizer_to(optim, device):\r\n    for param in optim.state.values():\r\n        # Not sure there are any global tensors in the state dict\r\n        if isinstance(param, torch.Tensor):\r\n            param.data = param.data.to(device)\r\n            if param._grad is not None:\r\n                param._grad.data = param._grad.data.to(device)\r\n        elif isinstance(param, dict):\r\n            for subparam in param.values():\r\n                if isinstance(subparam, torch.Tensor):\r\n                    subparam.data = subparam.data.to(device)\r\n                    if subparam._grad is not None:\r\n                        subparam._grad.data = subparam._grad.data.to(device)\r\n\r\ndef scheduler_to(sched, device):\r\n    for param in sched.__dict__.values():\r\n        if isinstance(param, torch.Tensor):\r\n            param.data = param.data.to(device)\r\n            if param._grad is not None:\r\n                param._grad.data = param._grad.data.to(device)\r\n```\r\n\r\n---\r\n\r\nI really enjoy working with PyTorch, but I have a feeling the optimizers/schedulers do not have a straight forward implementation. (mostly schedulers though).  \r\nIs there a roadmap to create a more unified API for them? Is it something you could use or want help with? "}