{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/404449697", "html_url": "https://github.com/pytorch/pytorch/issues/8741#issuecomment-404449697", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8741", "id": 404449697, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDQ0OTY5Nw==", "user": {"login": "0phoff", "id": 11853089, "node_id": "MDQ6VXNlcjExODUzMDg5", "avatar_url": "https://avatars3.githubusercontent.com/u/11853089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0phoff", "html_url": "https://github.com/0phoff", "followers_url": "https://api.github.com/users/0phoff/followers", "following_url": "https://api.github.com/users/0phoff/following{/other_user}", "gists_url": "https://api.github.com/users/0phoff/gists{/gist_id}", "starred_url": "https://api.github.com/users/0phoff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0phoff/subscriptions", "organizations_url": "https://api.github.com/users/0phoff/orgs", "repos_url": "https://api.github.com/users/0phoff/repos", "events_url": "https://api.github.com/users/0phoff/events{/privacy}", "received_events_url": "https://api.github.com/users/0phoff/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T09:26:03Z", "updated_at": "2018-07-13T09:39:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Looking back at it, optimizers are quite OK. It would just be nice to have it better documented so people can more easily hack in and create there own<br>\nThe only thing that was quite weird and difficult to grasp, is how serialisation works. The way the <code>state_dict</code> gets constructed with tensors as keys is quite awkward imo, but you guys probably have your reasons for doing so.</p>\n<p>For schedulers I do have a few remarks:</p>\n<ul>\n<li>There is a base <code>_LRScheduler</code>, but not all schedulers use it.<br>\nI know this is because <code>ReduceLROnPlateau</code> should be called after epochs as opposed to before like the others and has different params. However, I think this could be refactored to a unified interface, if you just perform a bit more logic in the schedulers themselves (eg. internally use <code>epoch - 1</code> and then we can call the <code>step()</code> after the epoch on all schedulers)</li>\n<li>Different schedulers have different arguments. If we would use <code>kwargs</code> with default values for all of them, this would allow for a more unified interface where you can switch out one scheduler for another during your training.</li>\n<li>Expand schedulers to work for different parameter groups instead of only for LR. This does not seem to be super hard to implement but would be a nice mechanism to vary momentum, weight decay, etc. I have been wanting to experiment with that for quite some time now, but I should probably look if any papers tried this already. <g-emoji class=\"g-emoji\" alias=\"scroll\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f4dc.png\">\ud83d\udcdc</g-emoji></li>\n<li><code>LambdaLR</code> cannot be serialized when using a lambda function as argument. I think the function itself should not be serialized, as reloading that scheduler already requires you to have that specific scheduler and function available, so what's the point in serializing that function as well.</li>\n<li>Just as the base <code>Optimizer</code> class, you should expose the base <code>Scheduler</code> class for people to be able to create their own.</li>\n</ul>\n<hr>\n<p>If we can get optimizers/schedulers to have a unified <code>step()</code> API with kwargs that have default values, we can construct some kind of <code>OptimizerCompositor</code> and <code>SchedulerCompositor</code>, that allow a user to combine multiple optimizers/schedulers in time during training. One thing that should change in the schedulers for this to work, is how you compute the <code>base_lr</code>. Right now, you get the <code>base_lr</code> upon constructing the scheduler. However, it would be better to get the <code>base_lr</code> upon first calling the <code>step</code> function. If you then chain multiple schedulers, they will work with the last learning_rate that was set by the previous scheduler.</p>\n<p>An example API for the <code>SchedulerCompositor</code>, that I use in my own library:<br>\nNote that I use <code>count</code> instead of <code>epoch</code> as I work with both batches and epochs depending on the network I am training.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">SchedulerCompositor</span>:\n    <span class=\"pl-bu\">```</span>\n    Chain multiple Schedulers together.\n    Call it <span class=\"pl-k\">with</span> <span class=\"pl-bu\">`(count, scheduler)`</span> tuples where count <span class=\"pl-k\">is</span> the batch<span class=\"pl-k\">/</span>epoch at which you want to start to use that scheduler.\n    <span class=\"pl-bu\">```</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(args) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Compositor requires at least one scheduler<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c1\">self</span>.counts, <span class=\"pl-c1\">self</span>.sched <span class=\"pl-k\">=</span> <span class=\"pl-c1\">zip</span>(<span class=\"pl-k\">*</span>args)\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">all</span>(c1 <span class=\"pl-k\">&lt;</span> c2 <span class=\"pl-k\">for</span> c1, c2 <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(<span class=\"pl-c1\">self</span>.counts, <span class=\"pl-c1\">self</span>.counts[<span class=\"pl-c1\">1</span>:])):\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Count values need to be strictly increasing<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">count</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n        <span class=\"pl-k\">for</span> idx, cnt <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">self</span>.counts):\n            <span class=\"pl-k\">if</span> count <span class=\"pl-k\">&lt;=</span> cnt:\n                idx <span class=\"pl-k\">-=</span> <span class=\"pl-c1\">1</span>\n                <span class=\"pl-k\">break</span>\n\n        <span class=\"pl-k\">if</span> idx <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0</span>:\n            log.error(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">No Scheduler defined for count value of </span><span class=\"pl-c1\">{</span>count<span class=\"pl-c1\">}</span><span class=\"pl-pds\">'</span>)\n            <span class=\"pl-k\">return</span>\n\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.sched[i].step(<span class=\"pl-k\">**</span>kwargs)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> [s.state_dict() <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.sched]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">load_state_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">state</span>):\n        [<span class=\"pl-c1\">self</span>.sched[i].load_state_dict(s) <span class=\"pl-k\">for</span> i, s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(state)]</pre></div>\n<hr>\n<p><strong>DISCLAIMER</strong><br>\nThese changes come from my own experience and work, and might not be the best global solution to any usecase for pytorch that is out there.<br>\nIf you decide to go for this kind of solution (or a part of it), I already have some working code in my own <a href=\"https://gitlab.com/EAVISE/lightnet/tree/develop\" rel=\"nofollow\">library</a> and I am willing to help get this into pytorch.</p>", "body_text": "Looking back at it, optimizers are quite OK. It would just be nice to have it better documented so people can more easily hack in and create there own\nThe only thing that was quite weird and difficult to grasp, is how serialisation works. The way the state_dict gets constructed with tensors as keys is quite awkward imo, but you guys probably have your reasons for doing so.\nFor schedulers I do have a few remarks:\n\nThere is a base _LRScheduler, but not all schedulers use it.\nI know this is because ReduceLROnPlateau should be called after epochs as opposed to before like the others and has different params. However, I think this could be refactored to a unified interface, if you just perform a bit more logic in the schedulers themselves (eg. internally use epoch - 1 and then we can call the step() after the epoch on all schedulers)\nDifferent schedulers have different arguments. If we would use kwargs with default values for all of them, this would allow for a more unified interface where you can switch out one scheduler for another during your training.\nExpand schedulers to work for different parameter groups instead of only for LR. This does not seem to be super hard to implement but would be a nice mechanism to vary momentum, weight decay, etc. I have been wanting to experiment with that for quite some time now, but I should probably look if any papers tried this already. \ud83d\udcdc\nLambdaLR cannot be serialized when using a lambda function as argument. I think the function itself should not be serialized, as reloading that scheduler already requires you to have that specific scheduler and function available, so what's the point in serializing that function as well.\nJust as the base Optimizer class, you should expose the base Scheduler class for people to be able to create their own.\n\n\nIf we can get optimizers/schedulers to have a unified step() API with kwargs that have default values, we can construct some kind of OptimizerCompositor and SchedulerCompositor, that allow a user to combine multiple optimizers/schedulers in time during training. One thing that should change in the schedulers for this to work, is how you compute the base_lr. Right now, you get the base_lr upon constructing the scheduler. However, it would be better to get the base_lr upon first calling the step function. If you then chain multiple schedulers, they will work with the last learning_rate that was set by the previous scheduler.\nAn example API for the SchedulerCompositor, that I use in my own library:\nNote that I use count instead of epoch as I work with both batches and epochs depending on the network I am training.\nclass SchedulerCompositor:\n    ```\n    Chain multiple Schedulers together.\n    Call it with `(count, scheduler)` tuples where count is the batch/epoch at which you want to start to use that scheduler.\n    ```\n    def __init__(self, *args):\n        if len(args) == 0:\n            raise ValueError('Compositor requires at least one scheduler')\n\n        self.counts, self.sched = zip(*args)\n        if not all(c1 < c2 for c1, c2 in zip(self.counts, self.counts[1:])):\n            raise ValueError('Count values need to be strictly increasing')\n\n    def step(self, count, **kwargs):\n        for idx, cnt in enumerate(self.counts):\n            if count <= cnt:\n                idx -= 1\n                break\n\n        if idx < 0:\n            log.error(f'No Scheduler defined for count value of {count}')\n            return\n\n        return self.sched[i].step(**kwargs)\n\n    def state_dict(self):\n        return [s.state_dict() for s in self.sched]\n\n    def load_state_dict(self, state):\n        [self.sched[i].load_state_dict(s) for i, s in enumerate(state)]\n\nDISCLAIMER\nThese changes come from my own experience and work, and might not be the best global solution to any usecase for pytorch that is out there.\nIf you decide to go for this kind of solution (or a part of it), I already have some working code in my own library and I am willing to help get this into pytorch.", "body": "Looking back at it, optimizers are quite OK. It would just be nice to have it better documented so people can more easily hack in and create there own  \r\nThe only thing that was quite weird and difficult to grasp, is how serialisation works. The way the ``state_dict`` gets constructed with tensors as keys is quite awkward imo, but you guys probably have your reasons for doing so.\r\n\r\nFor schedulers I do have a few remarks:\r\n  - There is a base ``_LRScheduler``, but not all schedulers use it.  \r\nI know this is because ``ReduceLROnPlateau`` should be called after epochs as opposed to before like the others and has different params. However, I think this could be refactored to a unified interface, if you just perform a bit more logic in the schedulers themselves (eg. internally use ``epoch - 1`` and then we can call the ``step()`` after the epoch on all schedulers)\r\n  - Different schedulers have different arguments. If we would use ``kwargs`` with default values for all of them, this would allow for a more unified interface where you can switch out one scheduler for another during your training.\r\n  - Expand schedulers to work for different parameter groups instead of only for LR. This does not seem to be super hard to implement but would be a nice mechanism to vary momentum, weight decay, etc. I have been wanting to experiment with that for quite some time now, but I should probably look if any papers tried this already. :scroll: \r\n  - ``LambdaLR`` cannot be serialized when using a lambda function as argument. I think the function itself should not be serialized, as reloading that scheduler already requires you to have that specific scheduler and function available, so what's the point in serializing that function as well.\r\n  - Just as the base ``Optimizer`` class, you should expose the base ``Scheduler`` class for people to be able to create their own.\r\n\r\n---\r\n\r\nIf we can get optimizers/schedulers to have a unified ``step()`` API with kwargs that have default values, we can construct some kind of ``OptimizerCompositor`` and ``SchedulerCompositor``, that allow a user to combine multiple optimizers/schedulers in time during training. One thing that should change in the schedulers for this to work, is how you compute the ``base_lr``. Right now, you get the ``base_lr`` upon constructing the scheduler. However, it would be better to get the ``base_lr`` upon first calling the ``step`` function. If you then chain multiple schedulers, they will work with the last learning_rate that was set by the previous scheduler.\r\n\r\nAn example API for the ``SchedulerCompositor``, that I use in my own library:  \r\nNote that I use ``count`` instead of ``epoch`` as I work with both batches and epochs depending on the network I am training.\r\n```python\r\nclass SchedulerCompositor:\r\n    ```\r\n    Chain multiple Schedulers together.\r\n    Call it with `(count, scheduler)` tuples where count is the batch/epoch at which you want to start to use that scheduler.\r\n    ```\r\n    def __init__(self, *args):\r\n        if len(args) == 0:\r\n            raise ValueError('Compositor requires at least one scheduler')\r\n\r\n        self.counts, self.sched = zip(*args)\r\n        if not all(c1 < c2 for c1, c2 in zip(self.counts, self.counts[1:])):\r\n            raise ValueError('Count values need to be strictly increasing')\r\n\r\n    def step(self, count, **kwargs):\r\n        for idx, cnt in enumerate(self.counts):\r\n            if count <= cnt:\r\n                idx -= 1\r\n                break\r\n\r\n        if idx < 0:\r\n            log.error(f'No Scheduler defined for count value of {count}')\r\n            return\r\n\r\n        return self.sched[i].step(**kwargs)\r\n\r\n    def state_dict(self):\r\n        return [s.state_dict() for s in self.sched]\r\n\r\n    def load_state_dict(self, state):\r\n        [self.sched[i].load_state_dict(s) for i, s in enumerate(state)]\r\n```\r\n\r\n---\r\n\r\n__DISCLAIMER__  \r\nThese changes come from my own experience and work, and might not be the best global solution to any usecase for pytorch that is out there.\r\nIf you decide to go for this kind of solution (or a part of it), I already have some working code in my own [library](https://gitlab.com/EAVISE/lightnet/tree/develop) and I am willing to help get this into pytorch."}