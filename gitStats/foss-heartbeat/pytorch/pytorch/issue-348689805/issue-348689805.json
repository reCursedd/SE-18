{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10348", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10348/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10348/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10348/events", "html_url": "https://github.com/pytorch/pytorch/issues/10348", "id": 348689805, "node_id": "MDU6SXNzdWUzNDg2ODk4MDU=", "number": 10348, "title": "Possible memory leak when exceptions are raised", "user": {"login": "benjamin-work", "id": 29862381, "node_id": "MDQ6VXNlcjI5ODYyMzgx", "avatar_url": "https://avatars1.githubusercontent.com/u/29862381?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benjamin-work", "html_url": "https://github.com/benjamin-work", "followers_url": "https://api.github.com/users/benjamin-work/followers", "following_url": "https://api.github.com/users/benjamin-work/following{/other_user}", "gists_url": "https://api.github.com/users/benjamin-work/gists{/gist_id}", "starred_url": "https://api.github.com/users/benjamin-work/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benjamin-work/subscriptions", "organizations_url": "https://api.github.com/users/benjamin-work/orgs", "repos_url": "https://api.github.com/users/benjamin-work/repos", "events_url": "https://api.github.com/users/benjamin-work/events{/privacy}", "received_events_url": "https://api.github.com/users/benjamin-work/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-08T11:43:01Z", "updated_at": "2018-08-08T15:01:23Z", "closed_at": "2018-08-08T15:01:23Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>There seems to be a memory leak when exceptions are raised (and captured) during the training process. This could be caused by references within the exception's traceback that are not properly garbage collected.</p>\n<h2>Code example</h2>\n<p>See: <a href=\"https://gist.github.com/benjamin-work/d27f8e5d61853a0983a7e640ed09f9c8\">https://gist.github.com/benjamin-work/d27f8e5d61853a0983a7e640ed09f9c8</a></p>\n<p>This code generates the following output:</p>\n<pre><code>Running without raising an exception:\nNumber of tensors at the start: 4\nMax allocated    22.550016 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nNumber of tensors at the end: 4\n--------------------------------------------------\nRunning with exception:\nNumber of tensors at the start: 4\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated      25.5872 Mb, cached    33.554432 Mb\nMax allocated      25.5872 Mb, cached    33.554432 Mb\nMax allocated      25.5872 Mb, cached    33.554432 Mb\nNumber of tensors at the end: 12\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: Quadro M6000 24GB<br>\nNvidia driver version: 384.130<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.15.0)<br>\n[pip] numpydoc (0.8.0)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] torch                     0.4.1                     <br>\n[conda] torchvision               0.2.1                     </p>", "body_text": "Issue description\nThere seems to be a memory leak when exceptions are raised (and captured) during the training process. This could be caused by references within the exception's traceback that are not properly garbage collected.\nCode example\nSee: https://gist.github.com/benjamin-work/d27f8e5d61853a0983a7e640ed09f9c8\nThis code generates the following output:\nRunning without raising an exception:\nNumber of tensors at the start: 4\nMax allocated    22.550016 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nNumber of tensors at the end: 4\n--------------------------------------------------\nRunning with exception:\nNumber of tensors at the start: 4\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    22.572032 Mb, cached    23.068672 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated    25.566208 Mb, cached    33.554432 Mb\nMax allocated      25.5872 Mb, cached    33.554432 Mb\nMax allocated      25.5872 Mb, cached    33.554432 Mb\nMax allocated      25.5872 Mb, cached    33.554432 Mb\nNumber of tensors at the end: 12\n\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: Quadro M6000 24GB\nNvidia driver version: 384.130\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] numpydoc (0.8.0)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] torch                     0.4.1                     \n[conda] torchvision               0.2.1", "body": "## Issue description\r\n\r\nThere seems to be a memory leak when exceptions are raised (and captured) during the training process. This could be caused by references within the exception's traceback that are not properly garbage collected.\r\n\r\n## Code example\r\n\r\nSee: https://gist.github.com/benjamin-work/d27f8e5d61853a0983a7e640ed09f9c8\r\n\r\nThis code generates the following output:\r\n\r\n```\r\nRunning without raising an exception:\r\nNumber of tensors at the start: 4\r\nMax allocated    22.550016 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nNumber of tensors at the end: 4\r\n--------------------------------------------------\r\nRunning with exception:\r\nNumber of tensors at the start: 4\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    22.572032 Mb, cached    23.068672 Mb\r\nMax allocated    25.566208 Mb, cached    33.554432 Mb\r\nMax allocated    25.566208 Mb, cached    33.554432 Mb\r\nMax allocated    25.566208 Mb, cached    33.554432 Mb\r\nMax allocated    25.566208 Mb, cached    33.554432 Mb\r\nMax allocated    25.566208 Mb, cached    33.554432 Mb\r\nMax allocated      25.5872 Mb, cached    33.554432 Mb\r\nMax allocated      25.5872 Mb, cached    33.554432 Mb\r\nMax allocated      25.5872 Mb, cached    33.554432 Mb\r\nNumber of tensors at the end: 12\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Quadro M6000 24GB\r\nNvidia driver version: 384.130\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     0.4.1                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>"}