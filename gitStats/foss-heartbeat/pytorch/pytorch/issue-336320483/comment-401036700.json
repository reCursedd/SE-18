{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/401036700", "html_url": "https://github.com/pytorch/pytorch/pull/8945#issuecomment-401036700", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8945", "id": 401036700, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTAzNjcwMA==", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-28T13:35:19Z", "updated_at": "2018-06-28T13:39:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> As the exponent tends to 0, the derivative is 0 for non-zero <code>x</code>, and tends to <code>inf</code> for <code>x</code> = 0. This works correctly in PyTorch:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a\ntensor([<span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">3</span>., <span class=\"pl-c1\">4</span>., <span class=\"pl-c1\">5</span>., <span class=\"pl-c1\">6</span>., <span class=\"pl-c1\">7</span>., <span class=\"pl-c1\">8</span>., <span class=\"pl-c1\">9</span>.], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a.pow(<span class=\"pl-c1\">0.0001</span>).sum().backward()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a.grad\ntensor([   inf, <span class=\"pl-c1\">0.0003</span>, <span class=\"pl-c1\">0.0002</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>,\n        <span class=\"pl-c1\">0.0000</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a.pow(<span class=\"pl-c1\">0.00001</span>).sum().backward()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a.grad\ntensor([   inf, <span class=\"pl-c1\">0.0003</span>, <span class=\"pl-c1\">0.0002</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0000</span>,\n        <span class=\"pl-c1\">0.0000</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a.pow(<span class=\"pl-c1\">0.01</span>).sum().backward()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a.grad\ntensor([   inf, <span class=\"pl-c1\">0.0103</span>, <span class=\"pl-c1\">0.0052</span>, <span class=\"pl-c1\">0.0035</span>, <span class=\"pl-c1\">0.0026</span>, <span class=\"pl-c1\">0.0021</span>, <span class=\"pl-c1\">0.0017</span>, <span class=\"pl-c1\">0.0015</span>, <span class=\"pl-c1\">0.0013</span>,\n        <span class=\"pl-c1\">0.0012</span>])</pre></div>", "body_text": "@ezyang As the exponent tends to 0, the derivative is 0 for non-zero x, and tends to inf for x = 0. This works correctly in PyTorch:\n>>> a\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], requires_grad=True)\n>>> a.pow(0.0001).sum().backward()\n>>> a.grad\ntensor([   inf, 0.0003, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000, 0.0000,\n        0.0000])\n>>> a.pow(0.00001).sum().backward()\n>>> a.grad\ntensor([   inf, 0.0003, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000, 0.0000,\n        0.0000])\n>>> a.pow(0.01).sum().backward()\n>>> a.grad\ntensor([   inf, 0.0103, 0.0052, 0.0035, 0.0026, 0.0021, 0.0017, 0.0015, 0.0013,\n        0.0012])", "body": "@ezyang As the exponent tends to 0, the derivative is 0 for non-zero `x`, and tends to `inf` for `x` = 0. This works correctly in PyTorch:\r\n\r\n```python\r\n>>> a\r\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], requires_grad=True)\r\n>>> a.pow(0.0001).sum().backward()\r\n>>> a.grad\r\ntensor([   inf, 0.0003, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000, 0.0000,\r\n        0.0000])\r\n>>> a.pow(0.00001).sum().backward()\r\n>>> a.grad\r\ntensor([   inf, 0.0003, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0000, 0.0000,\r\n        0.0000])\r\n>>> a.pow(0.01).sum().backward()\r\n>>> a.grad\r\ntensor([   inf, 0.0103, 0.0052, 0.0035, 0.0026, 0.0021, 0.0017, 0.0015, 0.0013,\r\n        0.0012])\r\n```"}