{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413088469", "html_url": "https://github.com/pytorch/pytorch/pull/10429#issuecomment-413088469", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10429", "id": 413088469, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzA4ODQ2OQ==", "user": {"login": "jma127", "id": 2780434, "node_id": "MDQ6VXNlcjI3ODA0MzQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2780434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jma127", "html_url": "https://github.com/jma127", "followers_url": "https://api.github.com/users/jma127/followers", "following_url": "https://api.github.com/users/jma127/following{/other_user}", "gists_url": "https://api.github.com/users/jma127/gists{/gist_id}", "starred_url": "https://api.github.com/users/jma127/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jma127/subscriptions", "organizations_url": "https://api.github.com/users/jma127/orgs", "repos_url": "https://api.github.com/users/jma127/repos", "events_url": "https://api.github.com/users/jma127/events{/privacy}", "received_events_url": "https://api.github.com/users/jma127/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-15T04:06:45Z", "updated_at": "2018-08-15T04:07:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Also, to explain my usecase as clearly as possible:</p>\n<p>Current recommended methods of converting batchnorm models to FP16 in pytorch all involve calling <code>half()</code> on the entire module, then calling <code>float()</code> on individual BatchNorm layers. This is undesirable because the batchnorm parameters undergo an unnecessary conversion from fp32 to fp16 back to fp32. Besides the reduced precision that this induces, this can actually cause NaNs/infs in what would otherwise be a perfectly good model.</p>\n<p>My motivation for this PR is that users should not have to touch private variables (<code>_parameters</code> and <code>_buffers</code>) in order to do either this or any other module-specialized tensor operations.</p>", "body_text": "Also, to explain my usecase as clearly as possible:\nCurrent recommended methods of converting batchnorm models to FP16 in pytorch all involve calling half() on the entire module, then calling float() on individual BatchNorm layers. This is undesirable because the batchnorm parameters undergo an unnecessary conversion from fp32 to fp16 back to fp32. Besides the reduced precision that this induces, this can actually cause NaNs/infs in what would otherwise be a perfectly good model.\nMy motivation for this PR is that users should not have to touch private variables (_parameters and _buffers) in order to do either this or any other module-specialized tensor operations.", "body": "Also, to explain my usecase as clearly as possible:\r\n\r\nCurrent recommended methods of converting batchnorm models to FP16 in pytorch all involve calling ``half()`` on the entire module, then calling ``float()`` on individual BatchNorm layers. This is undesirable because the batchnorm parameters undergo an unnecessary conversion from fp32 to fp16 back to fp32. Besides the reduced precision that this induces, this can actually cause NaNs/infs in what would otherwise be a perfectly good model.\r\n\r\nMy motivation for this PR is that users should not have to touch private variables (``_parameters`` and ``_buffers``) in order to do either this or any other module-specialized tensor operations."}