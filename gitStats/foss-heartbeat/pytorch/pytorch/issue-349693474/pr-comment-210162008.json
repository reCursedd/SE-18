{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210162008", "pull_request_review_id": 146319911, "id": 210162008, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDE2MjAwOA==", "diff_hunk": "@@ -241,6 +223,69 @@ def apply(self, fn):\n         fn(self)\n         return self\n \n+    def apply_tensor(self, fn, to_parameters=True, to_gradients=True,\n+                     to_buffers=True, recursive=True):", "path": "torch/nn/modules/module.py", "position": null, "original_position": 30, "commit_id": "ead08ec98a69f8dceec2d2d09ec3d7c463309c99", "original_commit_id": "66e552d9147f83ad6594b96febe5c49de4b01bce", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Is the generality of the `to_` arguments needed? If you don't need them for your use case, I'd rather drop them for now and add them later if someone shouts.\r\n\r\nI'm also troubled as to whether or not `apply_tensor` should be nonrecursive by default. You can always spell the recursive version as `mod.apply(lambda m: m.apply_tensor(fn))`", "created_at": "2018-08-15T03:17:03Z", "updated_at": "2018-11-23T15:49:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/10429#discussion_r210162008", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10429", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210162008"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10429#discussion_r210162008"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10429"}}, "body_html": "<p>Is the generality of the <code>to_</code> arguments needed? If you don't need them for your use case, I'd rather drop them for now and add them later if someone shouts.</p>\n<p>I'm also troubled as to whether or not <code>apply_tensor</code> should be nonrecursive by default. You can always spell the recursive version as <code>mod.apply(lambda m: m.apply_tensor(fn))</code></p>", "body_text": "Is the generality of the to_ arguments needed? If you don't need them for your use case, I'd rather drop them for now and add them later if someone shouts.\nI'm also troubled as to whether or not apply_tensor should be nonrecursive by default. You can always spell the recursive version as mod.apply(lambda m: m.apply_tensor(fn))"}