{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5587", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5587/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5587/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5587/events", "html_url": "https://github.com/pytorch/pytorch/issues/5587", "id": 302591199, "node_id": "MDU6SXNzdWUzMDI1OTExOTk=", "number": 5587, "title": "[bug]  assert len(modules) == len(inputs) when use torch.distributed to compute last batch", "user": {"login": "piaozhx", "id": 9268675, "node_id": "MDQ6VXNlcjkyNjg2NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9268675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/piaozhx", "html_url": "https://github.com/piaozhx", "followers_url": "https://api.github.com/users/piaozhx/followers", "following_url": "https://api.github.com/users/piaozhx/following{/other_user}", "gists_url": "https://api.github.com/users/piaozhx/gists{/gist_id}", "starred_url": "https://api.github.com/users/piaozhx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/piaozhx/subscriptions", "organizations_url": "https://api.github.com/users/piaozhx/orgs", "repos_url": "https://api.github.com/users/piaozhx/repos", "events_url": "https://api.github.com/users/piaozhx/events{/privacy}", "received_events_url": "https://api.github.com/users/piaozhx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 586698444, "node_id": "MDU6TGFiZWw1ODY2OTg0NDQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/1hr", "name": "1hr", "color": "d4c5f9", "default": false}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-06T07:26:48Z", "updated_at": "2018-09-19T07:16:29Z", "closed_at": "2018-04-04T17:44:26Z", "author_association": "NONE", "body_html": "<h2>PyTorch GitHub Issues Guidelines</h2>\n<ul>\n<li>OS: CentOS 7.2.1511</li>\n<li>PyTorch version: 0.3.1 post2</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version:3.6.3</li>\n<li>CUDA/cuDNN version: cuda8</li>\n<li>GPU models and configuration: Tesla P40</li>\n</ul>\n<p>Recently, I use torch.distributed to run my model on multi node(each node have 4 P40), sometime(e.g. use 3 processes on corresponding nodes, each process use 4 cards) It works well, but most time(e.g. use 4 nodes, 5 nodes ,...) I get a strange error:</p>\n<pre><code>Traceback (most recent call last):\n File \"triplet_dist.py\", line 358, in &lt;module&gt;\n   model.main()\n File \"triplet_dist.py\", line 332, in main\n   self.validate(epoch)\n File \"triplet_dist.py\", line 243, in validate\n   outputs, dist_a, dist_b, embed_x, embed_y, embed_z = self.model(inputs, input0, fgs, bgs)\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n   result = self.forward(*input, **kwargs)\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 167, in forward\n   outputs = self.parallel_apply(self._module_copies, inputs, kwargs)\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 174, in parallel_apply\n   return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 24, in parallel_apply\n   assert len(modules) == len(inputs)\nAssertionError\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n what():  [enforce fail at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/gloo/gloo/cuda_private.h:40] error == cudaSuccess. 29 vs 0. Error at: /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/gloo/gloo/cuda_private.h:40: driver shutting down\n</code></pre>\n<p>I found that error always happened when the model compute last batch of a epoch (for example, total dataset size is 100, batch size is 30, then I have 4 batch but last batch size is 10) ,If I just dropped last batch, everything will be OK. but I think \"just drop it\" is very dirty and is impossible for test dataset. Any good idea for that case?</p>", "body_text": "PyTorch GitHub Issues Guidelines\n\nOS: CentOS 7.2.1511\nPyTorch version: 0.3.1 post2\nHow you installed PyTorch (conda, pip, source): pip\nPython version:3.6.3\nCUDA/cuDNN version: cuda8\nGPU models and configuration: Tesla P40\n\nRecently, I use torch.distributed to run my model on multi node(each node have 4 P40), sometime(e.g. use 3 processes on corresponding nodes, each process use 4 cards) It works well, but most time(e.g. use 4 nodes, 5 nodes ,...) I get a strange error:\nTraceback (most recent call last):\n File \"triplet_dist.py\", line 358, in <module>\n   model.main()\n File \"triplet_dist.py\", line 332, in main\n   self.validate(epoch)\n File \"triplet_dist.py\", line 243, in validate\n   outputs, dist_a, dist_b, embed_x, embed_y, embed_z = self.model(inputs, input0, fgs, bgs)\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n   result = self.forward(*input, **kwargs)\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 167, in forward\n   outputs = self.parallel_apply(self._module_copies, inputs, kwargs)\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 174, in parallel_apply\n   return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 24, in parallel_apply\n   assert len(modules) == len(inputs)\nAssertionError\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n what():  [enforce fail at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/gloo/gloo/cuda_private.h:40] error == cudaSuccess. 29 vs 0. Error at: /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/gloo/gloo/cuda_private.h:40: driver shutting down\n\nI found that error always happened when the model compute last batch of a epoch (for example, total dataset size is 100, batch size is 30, then I have 4 batch but last batch size is 10) ,If I just dropped last batch, everything will be OK. but I think \"just drop it\" is very dirty and is impossible for test dataset. Any good idea for that case?", "body": "PyTorch GitHub Issues Guidelines\r\n--------------------------------\r\n- OS: CentOS 7.2.1511\r\n- PyTorch version: 0.3.1 post2\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version:3.6.3 \r\n- CUDA/cuDNN version: cuda8\r\n- GPU models and configuration: Tesla P40\r\n\r\nRecently, I use torch.distributed to run my model on multi node(each node have 4 P40), sometime(e.g. use 3 processes on corresponding nodes, each process use 4 cards) It works well, but most time(e.g. use 4 nodes, 5 nodes ,...) I get a strange error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n File \"triplet_dist.py\", line 358, in <module>\r\n   model.main()\r\n File \"triplet_dist.py\", line 332, in main\r\n   self.validate(epoch)\r\n File \"triplet_dist.py\", line 243, in validate\r\n   outputs, dist_a, dist_b, embed_x, embed_y, embed_z = self.model(inputs, input0, fgs, bgs)\r\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n   result = self.forward(*input, **kwargs)\r\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 167, in forward\r\n   outputs = self.parallel_apply(self._module_copies, inputs, kwargs)\r\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 174, in parallel_apply\r\n   return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n File \"/public/home/gaoshenghua/zhengjia/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 24, in parallel_apply\r\n   assert len(modules) == len(inputs)\r\nAssertionError\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n what():  [enforce fail at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/gloo/gloo/cuda_private.h:40] error == cudaSuccess. 29 vs 0. Error at: /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/gloo/gloo/cuda_private.h:40: driver shutting down\r\n``` \r\n\r\nI found that error always happened when the model compute last batch of a epoch (for example, total dataset size is 100, batch size is 30, then I have 4 batch but last batch size is 10) ,If I just dropped last batch, everything will be OK. but I think \"just drop it\" is very dirty and is impossible for test dataset. Any good idea for that case? "}