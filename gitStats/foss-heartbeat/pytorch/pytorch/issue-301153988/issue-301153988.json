{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5470", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5470/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5470/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5470/events", "html_url": "https://github.com/pytorch/pytorch/issues/5470", "id": 301153988, "node_id": "MDU6SXNzdWUzMDExNTM5ODg=", "number": 5470, "title": "Error in python: free(): invalid pointer", "user": {"login": "narfanar", "id": 5769148, "node_id": "MDQ6VXNlcjU3NjkxNDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5769148?v=4", "gravatar_id": "", "url": "https://api.github.com/users/narfanar", "html_url": "https://github.com/narfanar", "followers_url": "https://api.github.com/users/narfanar/followers", "following_url": "https://api.github.com/users/narfanar/following{/other_user}", "gists_url": "https://api.github.com/users/narfanar/gists{/gist_id}", "starred_url": "https://api.github.com/users/narfanar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/narfanar/subscriptions", "organizations_url": "https://api.github.com/users/narfanar/orgs", "repos_url": "https://api.github.com/users/narfanar/repos", "events_url": "https://api.github.com/users/narfanar/events{/privacy}", "received_events_url": "https://api.github.com/users/narfanar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-02-28T19:34:39Z", "updated_at": "2018-03-19T14:51:44Z", "closed_at": "2018-03-19T14:51:44Z", "author_association": "NONE", "body_html": "<p>OS: ubuntu 14</p>\n<ul>\n<li>PyTorch version: 0.3.0</li>\n<li>How you installed PyTorch: conda</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version:  9.0.176</li>\n<li>GPU models: Titan X</li>\n</ul>\n<p>the following code crashes after  self.lstm1(embed_atten,  (output1, state1))</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">video_feat</span>, <span class=\"pl-smi\">video_mask</span>, <span class=\"pl-smi\">captions</span>, <span class=\"pl-smi\">caption_mask</span>):\n        state1  <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.dim_hidden).zero_().cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        state2  <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.dim_hidden).zero_().cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n        output1 <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.dim_hidden).zero_().cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        output2 <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.dim_hidden).zero_().cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        vid_emb <span class=\"pl-k\">=</span> video_feat.permute(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b x h</span>\n        h_prev  <span class=\"pl-k\">=</span> vid_emb[<span class=\"pl-c1\">0</span>]\n        total_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>.\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x h x 1</span>\n        brcst_w <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embed_att_w.unsqueeze(<span class=\"pl-c1\">0</span>).expand(<span class=\"pl-c1\">self</span>.n_lstm_steps, <span class=\"pl-c1\">self</span>.embed_att_w.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.embed_att_w.size(<span class=\"pl-c1\">1</span>))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x h x h</span>\n        att_Ua  <span class=\"pl-k\">=</span>  <span class=\"pl-c1\">self</span>.embed_att_Ua.unsqueeze(<span class=\"pl-c1\">0</span>).expand(<span class=\"pl-c1\">self</span>.n_lstm_steps, <span class=\"pl-c1\">self</span>.embed_att_Ua.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.embed_att_Ua.size(<span class=\"pl-c1\">1</span>))\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(\"UA: {} | Ba: {} | Emb: {}\".format(att_Ua.size(), self.embed_att_ba.size(), vid_emb.size()))</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b x h</span>\n        image_part <span class=\"pl-k\">=</span> torch.bmm(vid_emb, att_Ua) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.embed_att_ba\n\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">self</span>.n_lstm_steps):\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>#### soft temporal attention #####</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b x h</span>\n            e_i <span class=\"pl-k\">=</span> torch.tanh(torch.mm(h_prev, <span class=\"pl-c1\">self</span>.embed_att_Wa) <span class=\"pl-k\">+</span> image_part)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b x 1</span>\n            e_i <span class=\"pl-k\">=</span> torch.bmm(e_i, brcst_w)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b</span>\n            e_i <span class=\"pl-k\">=</span> torch.sum(e_i, <span class=\"pl-c1\">2</span>)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b</span>\n            e_hat_exp <span class=\"pl-k\">=</span> torch.mul(torch.t(video_mask), torch.exp(e_i))\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> b</span>\n            denom <span class=\"pl-k\">=</span> torch.sum(e_hat_exp, <span class=\"pl-c1\">0</span>)\n            denom <span class=\"pl-k\">=</span> denom <span class=\"pl-k\">+</span> torch.eq(denom, <span class=\"pl-c1\">0.0</span>).cuda().float()\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b x h</span>\n            e_div <span class=\"pl-k\">=</span> torch.div(e_hat_exp, denom)\n            alphas <span class=\"pl-k\">=</span> e_div.unsqueeze(<span class=\"pl-c1\">2</span>).expand(e_div.size(<span class=\"pl-c1\">0</span>), e_div.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">self</span>.dim_vid)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> n x b x h</span>\n            attention_list <span class=\"pl-k\">=</span> torch.mul(alphas, vid_emb)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> b x h  </span>\n            atten <span class=\"pl-k\">=</span> torch.sum(attention_list,<span class=\"pl-c1\">0</span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>#### lstm #####</span>\n            current_embed   <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.Wemb(captions[:, i])\n\n            embed_atten <span class=\"pl-k\">=</span> torch.cat((current_embed, atten), <span class=\"pl-c1\">1</span>)\n            output1, state1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm1(embed_atten ,  (output1, state1))\n           \n            output2, state2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm2(output1, (output2, state2))\n            h_prev <span class=\"pl-k\">=</span> output2\n           \n            logit_words <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embed_word(output2)\n            loss <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()\n\n            cross_entropy <span class=\"pl-k\">=</span> loss(logit_words, captions[:, i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>])\n            cross_entropy <span class=\"pl-k\">=</span> cross_entropy <span class=\"pl-k\">*</span> caption_mask[:, i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>]\n\n            total_loss <span class=\"pl-k\">=</span> total_loss <span class=\"pl-k\">+</span> (torch.sum(cross_entropy) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">self</span>.batch_size)\n\n        <span class=\"pl-k\">return</span> total_loss</pre></div>", "body_text": "OS: ubuntu 14\n\nPyTorch version: 0.3.0\nHow you installed PyTorch: conda\nPython version: 3.6.4\nCUDA/cuDNN version:  9.0.176\nGPU models: Titan X\n\nthe following code crashes after  self.lstm1(embed_atten,  (output1, state1))\ndef forward(self, video_feat, video_mask, captions, caption_mask):\n        state1  = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\n        state2  = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\n\n        output1 = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\n        output2 = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\n        vid_emb = video_feat.permute(1, 0, 2) # n x b x h\n        h_prev  = vid_emb[0]\n        total_loss = 0.\n\n        # n x h x 1\n        brcst_w = self.embed_att_w.unsqueeze(0).expand(self.n_lstm_steps, self.embed_att_w.size(0), self.embed_att_w.size(1))\n        # n x h x h\n        att_Ua  =  self.embed_att_Ua.unsqueeze(0).expand(self.n_lstm_steps, self.embed_att_Ua.size(0), self.embed_att_Ua.size(1))\n\n        #print(\"UA: {} | Ba: {} | Emb: {}\".format(att_Ua.size(), self.embed_att_ba.size(), vid_emb.size()))\n        # n x b x h\n        image_part = torch.bmm(vid_emb, att_Ua) + self.embed_att_ba\n\n        for i in range(self.n_lstm_steps):\n            ##### soft temporal attention #####\n            # n x b x h\n            e_i = torch.tanh(torch.mm(h_prev, self.embed_att_Wa) + image_part)\n            # n x b x 1\n            e_i = torch.bmm(e_i, brcst_w)\n            # n x b\n            e_i = torch.sum(e_i, 2)\n            # n x b\n            e_hat_exp = torch.mul(torch.t(video_mask), torch.exp(e_i))\n            # b\n            denom = torch.sum(e_hat_exp, 0)\n            denom = denom + torch.eq(denom, 0.0).cuda().float()\n            # n x b x h\n            e_div = torch.div(e_hat_exp, denom)\n            alphas = e_div.unsqueeze(2).expand(e_div.size(0), e_div.size(1), self.dim_vid)\n            # n x b x h\n            attention_list = torch.mul(alphas, vid_emb)\n            # b x h  \n            atten = torch.sum(attention_list,0)\n\n            ##### lstm #####\n            current_embed   = self.Wemb(captions[:, i])\n\n            embed_atten = torch.cat((current_embed, atten), 1)\n            output1, state1 = self.lstm1(embed_atten ,  (output1, state1))\n           \n            output2, state2 = self.lstm2(output1, (output2, state2))\n            h_prev = output2\n           \n            logit_words = self.embed_word(output2)\n            loss = nn.CrossEntropyLoss()\n\n            cross_entropy = loss(logit_words, captions[:, i+1])\n            cross_entropy = cross_entropy * caption_mask[:, i+1]\n\n            total_loss = total_loss + (torch.sum(cross_entropy) / self.batch_size)\n\n        return total_loss", "body": "OS: ubuntu 14\r\n- PyTorch version: 0.3.0\r\n- How you installed PyTorch: conda\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version:  9.0.176\r\n- GPU models: Titan X\r\n\r\nthe following code crashes after  self.lstm1(embed_atten,  (output1, state1))\r\n\r\n```python\r\ndef forward(self, video_feat, video_mask, captions, caption_mask):\r\n        state1  = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\r\n        state2  = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\r\n\r\n        output1 = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\r\n        output2 = Variable(torch.FloatTensor(self.batch_size, self.dim_hidden).zero_().cuda(), requires_grad=False)\r\n        vid_emb = video_feat.permute(1, 0, 2) # n x b x h\r\n        h_prev  = vid_emb[0]\r\n        total_loss = 0.\r\n\r\n        # n x h x 1\r\n        brcst_w = self.embed_att_w.unsqueeze(0).expand(self.n_lstm_steps, self.embed_att_w.size(0), self.embed_att_w.size(1))\r\n        # n x h x h\r\n        att_Ua  =  self.embed_att_Ua.unsqueeze(0).expand(self.n_lstm_steps, self.embed_att_Ua.size(0), self.embed_att_Ua.size(1))\r\n\r\n        #print(\"UA: {} | Ba: {} | Emb: {}\".format(att_Ua.size(), self.embed_att_ba.size(), vid_emb.size()))\r\n        # n x b x h\r\n        image_part = torch.bmm(vid_emb, att_Ua) + self.embed_att_ba\r\n\r\n        for i in range(self.n_lstm_steps):\r\n            ##### soft temporal attention #####\r\n            # n x b x h\r\n            e_i = torch.tanh(torch.mm(h_prev, self.embed_att_Wa) + image_part)\r\n            # n x b x 1\r\n            e_i = torch.bmm(e_i, brcst_w)\r\n            # n x b\r\n            e_i = torch.sum(e_i, 2)\r\n            # n x b\r\n            e_hat_exp = torch.mul(torch.t(video_mask), torch.exp(e_i))\r\n            # b\r\n            denom = torch.sum(e_hat_exp, 0)\r\n            denom = denom + torch.eq(denom, 0.0).cuda().float()\r\n            # n x b x h\r\n            e_div = torch.div(e_hat_exp, denom)\r\n            alphas = e_div.unsqueeze(2).expand(e_div.size(0), e_div.size(1), self.dim_vid)\r\n            # n x b x h\r\n            attention_list = torch.mul(alphas, vid_emb)\r\n            # b x h  \r\n            atten = torch.sum(attention_list,0)\r\n\r\n            ##### lstm #####\r\n            current_embed   = self.Wemb(captions[:, i])\r\n\r\n            embed_atten = torch.cat((current_embed, atten), 1)\r\n            output1, state1 = self.lstm1(embed_atten ,  (output1, state1))\r\n           \r\n            output2, state2 = self.lstm2(output1, (output2, state2))\r\n            h_prev = output2\r\n           \r\n            logit_words = self.embed_word(output2)\r\n            loss = nn.CrossEntropyLoss()\r\n\r\n            cross_entropy = loss(logit_words, captions[:, i+1])\r\n            cross_entropy = cross_entropy * caption_mask[:, i+1]\r\n\r\n            total_loss = total_loss + (torch.sum(cross_entropy) / self.batch_size)\r\n\r\n        return total_loss\r\n```"}