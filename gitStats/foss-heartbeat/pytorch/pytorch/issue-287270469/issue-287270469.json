{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4571", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4571/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4571/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4571/events", "html_url": "https://github.com/pytorch/pytorch/issues/4571", "id": 287270469, "node_id": "MDU6SXNzdWUyODcyNzA0Njk=", "number": 4571, "title": "Allowing MSELoss in torch.nn to be twice differentiable", "user": {"login": "benvcutilli", "id": 23709161, "node_id": "MDQ6VXNlcjIzNzA5MTYx", "avatar_url": "https://avatars1.githubusercontent.com/u/23709161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benvcutilli", "html_url": "https://github.com/benvcutilli", "followers_url": "https://api.github.com/users/benvcutilli/followers", "following_url": "https://api.github.com/users/benvcutilli/following{/other_user}", "gists_url": "https://api.github.com/users/benvcutilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/benvcutilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benvcutilli/subscriptions", "organizations_url": "https://api.github.com/users/benvcutilli/orgs", "repos_url": "https://api.github.com/users/benvcutilli/repos", "events_url": "https://api.github.com/users/benvcutilli/events{/privacy}", "received_events_url": "https://api.github.com/users/benvcutilli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-09T23:52:24Z", "updated_at": "2018-01-10T02:05:52Z", "closed_at": "2018-01-10T01:08:42Z", "author_association": "NONE", "body_html": "<p>I would like to have MSELoss() allow two backward passes (the second of which would need the second derivative of MSELoss()). Apparently this isn't allowed (traceback below). Scouring the codebase shows me no reason why it would be prevented, at least mathematically.</p>\n<pre lang=\"Traceback\" data-meta=\"(most recent call last):\"><code>  File \"&lt;REDACTED&gt;\", line 112, in &lt;module&gt;\n    &lt;REDACTED&gt;\n  File \"&lt;REDACTED&gt;/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"&lt;REDACTED&gt;/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"&lt;REDACTED&gt;/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\", line 91, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"&lt;REDACTED&gt;/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 71, in backward_cls_backward\n    return double_backwards_fn(ctx, *grad_params)\n  File \"&lt;REDACTED&gt;/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 316, in default_double_backwards_fn\n    raise ValueError(class_name + \" can only be differentiated once.\")\nValueError: MSELoss can only be differentiated once.\n  \n</code></pre>", "body_text": "I would like to have MSELoss() allow two backward passes (the second of which would need the second derivative of MSELoss()). Apparently this isn't allowed (traceback below). Scouring the codebase shows me no reason why it would be prevented, at least mathematically.\n  File \"<REDACTED>\", line 112, in <module>\n    <REDACTED>\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\", line 91, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 71, in backward_cls_backward\n    return double_backwards_fn(ctx, *grad_params)\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 316, in default_double_backwards_fn\n    raise ValueError(class_name + \" can only be differentiated once.\")\nValueError: MSELoss can only be differentiated once.", "body": "I would like to have MSELoss() allow two backward passes (the second of which would need the second derivative of MSELoss()). Apparently this isn't allowed (traceback below). Scouring the codebase shows me no reason why it would be prevented, at least mathematically.\r\n\r\n```Traceback (most recent call last):\r\n  File \"<REDACTED>\", line 112, in <module>\r\n    <REDACTED>\r\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 156, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\", line 91, in apply\r\n    return self._forward_cls.backward(self, *args)\r\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 71, in backward_cls_backward\r\n    return double_backwards_fn(ctx, *grad_params)\r\n  File \"<REDACTED>/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 316, in default_double_backwards_fn\r\n    raise ValueError(class_name + \" can only be differentiated once.\")\r\nValueError: MSELoss can only be differentiated once.\r\n  "}