{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218879440", "pull_request_review_id": 156917244, "id": 218879440, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODg3OTQ0MA==", "diff_hunk": "@@ -36,95 +36,59 @@ AnomalyMetadata* Function::metadata() noexcept {\n   return anomaly_metadata_.get();\n }\n \n-/*\n- * Fix for #5534: prevent stack overflow on deletion of deep computation graph\n- *\n- * Sometimes one can end up with a very big computation graph of Functions\n- * and Edges. Each std::shared_ptr<Function> contains a list of Edge, and\n- * each Edge contains a std::shared_ptr<Function>. Deleting a\n- * std::shared_ptr<Function> can trigger the recursive deletion of other\n- * std::shared_ptr<Function>'s: this can stack overflow if the graph\n- * is deep enough. Here is an example of such a graph:\n- *\n- * shared_ptr<Function> -> Edge -> shared_ptr<Function> -> Edge -> ... -> shared_ptr<Function>\n- *\n- * The solution here is to use a custom deleter with each\n- * std::shared_ptr<Function>. The custom deleter keeps track of how many\n- * nested deleters it is in. When this number exceeds the maximum allowed\n- * depth, the Function* to be deleted are accumulated in a per-thread\n- * delete queue and handled by one of the deleters.\n- *\n- * Note that these custom deleters are NOT necessary for deleting PyFunction.\n- * This is because a THPFunction Python object owns a PyFunction that is in a\n- * computation graph. When Python objects get recursively destroyed, they\n- * are also queued into a delete list. This happens very early for them\n- * (at 50 deleters): https://github.com/python/cpython/blob/f320be77ffb73e3b9e7fc98c37b8df3975d84b40/Include/object.h#L1024-L1063\n- * so we don't need to worry about them.\n- */\n-\n-thread_local std::deque<Function*> deleteFunctionQueue;\n-thread_local size_t deleteFunctionRecursionDepth = 0;\n-\n-/*\n- * If this number is set too high, a deep computation graph can still\n- * stack overflow. The procedure for setting this number was to\n- * 1) find the smallest value that would not guard against stack overflows\n- *    on various machines\n- * 2) Take the minimum of all such values and subtract some leeway because\n- *    the memory of these stack frames will probably grow as time passes.\n- * Testing on a few machines machines, the magic numbers were:\n- * - Mac OSX (Macbook Pro 15) : ~60000\n- * - A beefy Ubuntu 16.04 box : ~15000\n- * - Windows AWS instance (g3.4xlarge): variable. My two attempts at different\n- *   times have gotten the following numbers: ~8300, 3669\n- */\n-#ifdef _WIN32\n-size_t deleteFunctionMaxRecursionDepth = 3000;\n-#else\n-size_t deleteFunctionMaxRecursionDepth = 10000;\n-#endif\n-\n-struct RecursionDepthCounter {\n- public:\n-  explicit RecursionDepthCounter() {\n-    ++deleteFunctionRecursionDepth;\n-  }\n-  ~RecursionDepthCounter() {\n-    --deleteFunctionRecursionDepth;\n+void gatherFunctions(Function* func,\n+                     std::vector<std::shared_ptr<Function>>& stack) {\n+  func->release_variables();\n+  auto saved_vars = func->saved_variables();\n+  if (saved_vars) {\n+    for (auto& saved_var : *saved_vars) {\n+      saved_var.reset_grad_function();\n+    }\n   }\n \n-  size_t value() {\n-    return deleteFunctionRecursionDepth;\n+  for (auto& edge : func->next_edges()) {\n+    if (edge.function.use_count() == 1) {\n+      stack.emplace_back(std::move(edge.function));\n+    } else {\n+      edge.function.reset();\n+    }\n   }\n-};\n+}\n \n /*\n- * Note that the custom deleter deletes in BFS style. Without using\n- * the custom deleter, the computation graph is deleted in a DFS style.\n- * The BFS deletion is valid (and safe) because if a shared_ptr<Function>\n- * 's reference count hits 0, nothing else will access it.\n- */\n+  * Fix for #5534: prevent stack overflow on deletion of deep computation graph\n+  *\n+  * Sometimes one can end up with a very big computation graph of Functions\n+  * and Edges. Each std::shared_ptr<Function> contains a list of Edge, and\n+  * each Edge contains a std::shared_ptr<Function>. Deleting a\n+  * std::shared_ptr<Function> can trigger the recursive deletion of other\n+  * std::shared_ptr<Function>'s: this can stack overflow if the graph\n+  * is deep enough. Here is an example of such a graph:\n+  *\n+  * shared_ptr<Function> -> Edge -> shared_ptr<Function> -> Edge -> ... -> shared_ptr<Function>\n+  *\n+  * The solution here is to detect when we are decrementing away the last\n+  * reference to a Function, and when doing so to buffer up the Function's\n+  * that will be recursively decremented.  We can then decrement (and free)\n+  * the original Function without causing a recursive cascade, before\n+  * draining the buffer applying the same behavior.  This is, in effect,\n+  * converting recursion to a loop, using a heap buffer in place of the\n+  * recursive call stack.\n+  */\n void deleteFunction(Function* function) {\n-  RecursionDepthCounter recursion_depth;\n-\n-  if (recursion_depth.value() > deleteFunctionMaxRecursionDepth) {\n-    deleteFunctionQueue.push_back(function);\n-    return;\n-  }\n-\n+  // To avoid stack overflow on large computational graphs,\n+  // we need to track reference decrementing and freeing\n+  // on the heap.\n+  function->release_variables();\n+  std::vector<std::shared_ptr<Function>> stack;\n+  gatherFunctions(function, stack);\n   delete function;\n \n-  if (deleteFunctionQueue.empty()) {\n-    return;\n-  }\n-  if (recursion_depth.value() != deleteFunctionMaxRecursionDepth) {\n-    AT_ERROR(\"Only one deleter per thread should be able to process \"\n-             \"the delete queue. Please open an issue.\");\n-  }\n-  while (!deleteFunctionQueue.empty()) {\n-    auto queued_function = deleteFunctionQueue.front();\n-    deleteFunctionQueue.pop_front();\n-    delete queued_function;\n+  while (!stack.empty()) {\n+    auto func = std::move(stack.back());\n+    stack.pop_back();\n+    gatherFunctions(func.get(), stack);\n+    // Reference count is decremented on the loop backedge.", "path": "torch/csrc/autograd/function.cpp", "position": 133, "original_position": 137, "commit_id": "1779145a2b6ea9f55a4affd41b66f58a64a518aa", "original_commit_id": "f30885c257a9f207d18d02b593dc65f373d9b198", "user": {"login": "resistor", "id": 9796, "node_id": "MDQ6VXNlcjk3OTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/9796?v=4", "gravatar_id": "", "url": "https://api.github.com/users/resistor", "html_url": "https://github.com/resistor", "followers_url": "https://api.github.com/users/resistor/followers", "following_url": "https://api.github.com/users/resistor/following{/other_user}", "gists_url": "https://api.github.com/users/resistor/gists{/gist_id}", "starred_url": "https://api.github.com/users/resistor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/resistor/subscriptions", "organizations_url": "https://api.github.com/users/resistor/orgs", "repos_url": "https://api.github.com/users/resistor/repos", "events_url": "https://api.github.com/users/resistor/events{/privacy}", "received_events_url": "https://api.github.com/users/resistor/received_events", "type": "User", "site_admin": false}, "body": "I don't think you can do it correctly without moving the shared_ptr into a stack temporary (or performing a more expensive mid-vector deletion), because the shared_ptr to be decremented will no longer be at the end of the stack after gatherFunctions() runs.", "created_at": "2018-09-19T16:44:23Z", "updated_at": "2018-11-23T15:51:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/11611#discussion_r218879440", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11611", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218879440"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11611#discussion_r218879440"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11611"}}, "body_html": "<p>I don't think you can do it correctly without moving the shared_ptr into a stack temporary (or performing a more expensive mid-vector deletion), because the shared_ptr to be decremented will no longer be at the end of the stack after gatherFunctions() runs.</p>", "body_text": "I don't think you can do it correctly without moving the shared_ptr into a stack temporary (or performing a more expensive mid-vector deletion), because the shared_ptr to be decremented will no longer be at the end of the stack after gatherFunctions() runs.", "in_reply_to_id": 218877222}