{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/414037384", "html_url": "https://github.com/pytorch/pytorch/issues/9873#issuecomment-414037384", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9873", "id": 414037384, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDAzNzM4NA==", "user": {"login": "simonm3", "id": 1199593, "node_id": "MDQ6VXNlcjExOTk1OTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1199593?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simonm3", "html_url": "https://github.com/simonm3", "followers_url": "https://api.github.com/users/simonm3/followers", "following_url": "https://api.github.com/users/simonm3/following{/other_user}", "gists_url": "https://api.github.com/users/simonm3/gists{/gist_id}", "starred_url": "https://api.github.com/users/simonm3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simonm3/subscriptions", "organizations_url": "https://api.github.com/users/simonm3/orgs", "repos_url": "https://api.github.com/users/simonm3/repos", "events_url": "https://api.github.com/users/simonm3/events{/privacy}", "received_events_url": "https://api.github.com/users/simonm3/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-18T06:58:59Z", "updated_at": "2018-08-18T06:58:59Z", "author_association": "NONE", "body_html": "<p>OK that is much better. However there is an anomaly. After a couple of minutes training the expected times are:</p>\n<h4>8 processors=&gt; 6.5 hours keras, 3.5 hours pytorch</h4>\n<h4>72 processors=&gt; 1 hour keras, 1'20 pytorch</h4>\n<p>So keras is actually slower on 8 processors but gets a 6 times speedup from 9 times the CPUs which sounds as expected. Pytorch is faster on 8 processors but only gets 2 times speedup from 9 times the CPUs. Hence pytorch is about 30% slower on the 72 processor machine. I tried  torch.set_num_threads but this made no difference. Could be because I compiled pytorch on an 8 CPU machine and I noticed some compile warnings - can't recall exact message but were about number of threads/cpus.</p>\n<p>Typically I would like to compile on cheaper machine but use on 72 processor machine. Is there some compile time flag to allow this? Or some way to speed it up at runtime for 72 processors?</p>", "body_text": "OK that is much better. However there is an anomaly. After a couple of minutes training the expected times are:\n8 processors=> 6.5 hours keras, 3.5 hours pytorch\n72 processors=> 1 hour keras, 1'20 pytorch\nSo keras is actually slower on 8 processors but gets a 6 times speedup from 9 times the CPUs which sounds as expected. Pytorch is faster on 8 processors but only gets 2 times speedup from 9 times the CPUs. Hence pytorch is about 30% slower on the 72 processor machine. I tried  torch.set_num_threads but this made no difference. Could be because I compiled pytorch on an 8 CPU machine and I noticed some compile warnings - can't recall exact message but were about number of threads/cpus.\nTypically I would like to compile on cheaper machine but use on 72 processor machine. Is there some compile time flag to allow this? Or some way to speed it up at runtime for 72 processors?", "body": "OK that is much better. However there is an anomaly. After a couple of minutes training the expected times are:\r\n\r\n#### 8 processors=> 6.5 hours keras, 3.5 hours pytorch\r\n#### 72 processors=> 1 hour keras, 1'20 pytorch\r\n\r\nSo keras is actually slower on 8 processors but gets a 6 times speedup from 9 times the CPUs which sounds as expected. Pytorch is faster on 8 processors but only gets 2 times speedup from 9 times the CPUs. Hence pytorch is about 30% slower on the 72 processor machine. I tried  torch.set_num_threads but this made no difference. Could be because I compiled pytorch on an 8 CPU machine and I noticed some compile warnings - can't recall exact message but were about number of threads/cpus.\r\n\r\nTypically I would like to compile on cheaper machine but use on 72 processor machine. Is there some compile time flag to allow this? Or some way to speed it up at runtime for 72 processors?"}