{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9873", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9873/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9873/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9873/events", "html_url": "https://github.com/pytorch/pytorch/issues/9873", "id": 344793696, "node_id": "MDU6SXNzdWUzNDQ3OTM2OTY=", "number": 9873, "title": "Pytorch is slow when only using CPU, and cannot utilize multicore of CPU", "user": {"login": "zhangchenkai", "id": 10665923, "node_id": "MDQ6VXNlcjEwNjY1OTIz", "avatar_url": "https://avatars2.githubusercontent.com/u/10665923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangchenkai", "html_url": "https://github.com/zhangchenkai", "followers_url": "https://api.github.com/users/zhangchenkai/followers", "following_url": "https://api.github.com/users/zhangchenkai/following{/other_user}", "gists_url": "https://api.github.com/users/zhangchenkai/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangchenkai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangchenkai/subscriptions", "organizations_url": "https://api.github.com/users/zhangchenkai/orgs", "repos_url": "https://api.github.com/users/zhangchenkai/repos", "events_url": "https://api.github.com/users/zhangchenkai/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangchenkai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-07-26T10:52:00Z", "updated_at": "2018-10-07T17:55:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>When I testing the acceleration effect on CPU by decomposing convolution layers, I found pytorch is slow  and cannot utilize multicores of CPU.</p>\n<pre><code># To blind GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '  '\n# Generate input data\nt = np.random.randn(50,128,128,64).astype(np.float32)\n</code></pre>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nx = torch.from_numpy(t).permute(0,3,1,2)\n\nmodel1 = nn.Sequential(\n    nn.Conv2d(64, 128, 3, stride=1, padding=1, dilation=1),\n    nn.Conv2d(128, 256, 3, stride=1, padding=1, dilation=1),\n)\n\nmodel2 = nn.Sequential(\n    nn.Sequential(\n        nn.Conv2d(64, 16, kernel_size=(1, 1)),\n        nn.Conv2d(16, 16, kernel_size=(3, 1), padding=(1, 0)),\n        nn.Conv2d(16, 16, kernel_size=(1, 3), padding=(0, 1)),\n        nn.Conv2d(16, 128, kernel_size=(1, 1)),\n    ),\n    nn.Sequential(\n        nn.Conv2d(128, 32, kernel_size=(1, 1)),\n        nn.Conv2d(32, 32, kernel_size=(3, 1), padding=(1, 0)),\n        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n        nn.Conv2d(32, 256, kernel_size=(1, 1)),\n    ),\n)\n\niters = 10\n\nt1 = time.time()\nfor i in range(iters):\n    y=model1(x)\nt2 = time.time()\nm1t = t2-t1\n\nt1 = time.time()\nfor i in range(iters):\n    y=model2(x)\nt2 = time.time()\nm2t = t2-t1\n\nprint(m1t, m2t)\n</code></pre>\n<p>Output is that while cpu usage is 100%-200%</p>\n<blockquote>\n<p>147.48149728775024 16.699654817581177</p>\n</blockquote>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10665923/43258302-eca30248-9104-11e8-8045-49d8d63819a4.PNG\"><img src=\"https://user-images.githubusercontent.com/10665923/43258302-eca30248-9104-11e8-8045-49d8d63819a4.PNG\" alt=\"pytorch\" style=\"max-width:100%;\"></a></p>\n<p>However, Keras (TF  back-end)  is faster and multi-threaded.</p>\n<pre><code>model1 = Sequential([\n    Conv2D(128,3,input_shape=(128,128,64)),\n    Conv2D(256,3),\n])\n\n\nmodel2 = Sequential([\n    Sequential([\n        Conv2D(16,1,input_shape=(128,128,64)),\n        Conv2D(16,(3,1)),\n        Conv2D(16,(1,3)),\n        Conv2D(128,(1,1)),\n    ]),\n    Sequential([\n        Conv2D(32,1,input_shape=(100,100,128)),\n        Conv2D(32,(3,1)),\n        Conv2D(32,(1,3)),\n        Conv2D(256,1)\n    ]),\n])\n</code></pre>\n<p>Output is that while cpu usage is 800%-1600%</p>\n<blockquote>\n<p>26.393059253692627 13.783706188201904</p>\n</blockquote>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/10665923/43258314-f57cf11c-9104-11e8-8cfd-c094a798df48.PNG\"><img src=\"https://user-images.githubusercontent.com/10665923/43258314-f57cf11c-9104-11e8-8cfd-c094a798df48.PNG\" alt=\"keras\" style=\"max-width:100%;\"></a></p>\n<h2>System Info</h2>\n<p>Pytorch 0.4<br>\nUbuntu 16.04</p>", "body_text": "When I testing the acceleration effect on CPU by decomposing convolution layers, I found pytorch is slow  and cannot utilize multicores of CPU.\n# To blind GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '  '\n# Generate input data\nt = np.random.randn(50,128,128,64).astype(np.float32)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nx = torch.from_numpy(t).permute(0,3,1,2)\n\nmodel1 = nn.Sequential(\n    nn.Conv2d(64, 128, 3, stride=1, padding=1, dilation=1),\n    nn.Conv2d(128, 256, 3, stride=1, padding=1, dilation=1),\n)\n\nmodel2 = nn.Sequential(\n    nn.Sequential(\n        nn.Conv2d(64, 16, kernel_size=(1, 1)),\n        nn.Conv2d(16, 16, kernel_size=(3, 1), padding=(1, 0)),\n        nn.Conv2d(16, 16, kernel_size=(1, 3), padding=(0, 1)),\n        nn.Conv2d(16, 128, kernel_size=(1, 1)),\n    ),\n    nn.Sequential(\n        nn.Conv2d(128, 32, kernel_size=(1, 1)),\n        nn.Conv2d(32, 32, kernel_size=(3, 1), padding=(1, 0)),\n        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n        nn.Conv2d(32, 256, kernel_size=(1, 1)),\n    ),\n)\n\niters = 10\n\nt1 = time.time()\nfor i in range(iters):\n    y=model1(x)\nt2 = time.time()\nm1t = t2-t1\n\nt1 = time.time()\nfor i in range(iters):\n    y=model2(x)\nt2 = time.time()\nm2t = t2-t1\n\nprint(m1t, m2t)\n\nOutput is that while cpu usage is 100%-200%\n\n147.48149728775024 16.699654817581177\n\n\nHowever, Keras (TF  back-end)  is faster and multi-threaded.\nmodel1 = Sequential([\n    Conv2D(128,3,input_shape=(128,128,64)),\n    Conv2D(256,3),\n])\n\n\nmodel2 = Sequential([\n    Sequential([\n        Conv2D(16,1,input_shape=(128,128,64)),\n        Conv2D(16,(3,1)),\n        Conv2D(16,(1,3)),\n        Conv2D(128,(1,1)),\n    ]),\n    Sequential([\n        Conv2D(32,1,input_shape=(100,100,128)),\n        Conv2D(32,(3,1)),\n        Conv2D(32,(1,3)),\n        Conv2D(256,1)\n    ]),\n])\n\nOutput is that while cpu usage is 800%-1600%\n\n26.393059253692627 13.783706188201904\n\n\nSystem Info\nPytorch 0.4\nUbuntu 16.04", "body": "When I testing the acceleration effect on CPU by decomposing convolution layers, I found pytorch is slow  and cannot utilize multicores of CPU. \r\n\r\n```\r\n# To blind GPU\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '  '\r\n# Generate input data\r\nt = np.random.randn(50,128,128,64).astype(np.float32)\r\n```\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nx = torch.from_numpy(t).permute(0,3,1,2)\r\n\r\nmodel1 = nn.Sequential(\r\n    nn.Conv2d(64, 128, 3, stride=1, padding=1, dilation=1),\r\n    nn.Conv2d(128, 256, 3, stride=1, padding=1, dilation=1),\r\n)\r\n\r\nmodel2 = nn.Sequential(\r\n    nn.Sequential(\r\n        nn.Conv2d(64, 16, kernel_size=(1, 1)),\r\n        nn.Conv2d(16, 16, kernel_size=(3, 1), padding=(1, 0)),\r\n        nn.Conv2d(16, 16, kernel_size=(1, 3), padding=(0, 1)),\r\n        nn.Conv2d(16, 128, kernel_size=(1, 1)),\r\n    ),\r\n    nn.Sequential(\r\n        nn.Conv2d(128, 32, kernel_size=(1, 1)),\r\n        nn.Conv2d(32, 32, kernel_size=(3, 1), padding=(1, 0)),\r\n        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\r\n        nn.Conv2d(32, 256, kernel_size=(1, 1)),\r\n    ),\r\n)\r\n\r\niters = 10\r\n\r\nt1 = time.time()\r\nfor i in range(iters):\r\n    y=model1(x)\r\nt2 = time.time()\r\nm1t = t2-t1\r\n\r\nt1 = time.time()\r\nfor i in range(iters):\r\n    y=model2(x)\r\nt2 = time.time()\r\nm2t = t2-t1\r\n\r\nprint(m1t, m2t)\r\n```\r\nOutput is that while cpu usage is 100%-200%\r\n> 147.48149728775024 16.699654817581177\r\n\r\n![pytorch](https://user-images.githubusercontent.com/10665923/43258302-eca30248-9104-11e8-8045-49d8d63819a4.PNG)\r\n\r\nHowever, Keras (TF  back-end)  is faster and multi-threaded.\r\n```\r\nmodel1 = Sequential([\r\n    Conv2D(128,3,input_shape=(128,128,64)),\r\n    Conv2D(256,3),\r\n])\r\n\r\n\r\nmodel2 = Sequential([\r\n    Sequential([\r\n        Conv2D(16,1,input_shape=(128,128,64)),\r\n        Conv2D(16,(3,1)),\r\n        Conv2D(16,(1,3)),\r\n        Conv2D(128,(1,1)),\r\n    ]),\r\n    Sequential([\r\n        Conv2D(32,1,input_shape=(100,100,128)),\r\n        Conv2D(32,(3,1)),\r\n        Conv2D(32,(1,3)),\r\n        Conv2D(256,1)\r\n    ]),\r\n])\r\n```\r\nOutput is that while cpu usage is 800%-1600%\r\n> 26.393059253692627 13.783706188201904\r\n\r\n![keras](https://user-images.githubusercontent.com/10665923/43258314-f57cf11c-9104-11e8-8cfd-c094a798df48.PNG)\r\n\r\n## System Info\r\nPytorch 0.4\r\nUbuntu 16.04\r\n\r\n"}