{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5811", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5811/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5811/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5811/events", "html_url": "https://github.com/pytorch/pytorch/issues/5811", "id": 305574882, "node_id": "MDU6SXNzdWUzMDU1NzQ4ODI=", "number": 5811, "title": "Distributed communication never frees target memory", "user": {"login": "TSchmiedlechner", "id": 7840981, "node_id": "MDQ6VXNlcjc4NDA5ODE=", "avatar_url": "https://avatars2.githubusercontent.com/u/7840981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TSchmiedlechner", "html_url": "https://github.com/TSchmiedlechner", "followers_url": "https://api.github.com/users/TSchmiedlechner/followers", "following_url": "https://api.github.com/users/TSchmiedlechner/following{/other_user}", "gists_url": "https://api.github.com/users/TSchmiedlechner/gists{/gist_id}", "starred_url": "https://api.github.com/users/TSchmiedlechner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TSchmiedlechner/subscriptions", "organizations_url": "https://api.github.com/users/TSchmiedlechner/orgs", "repos_url": "https://api.github.com/users/TSchmiedlechner/repos", "events_url": "https://api.github.com/users/TSchmiedlechner/events{/privacy}", "received_events_url": "https://api.github.com/users/TSchmiedlechner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-03-15T14:27:10Z", "updated_at": "2018-04-04T16:49:41Z", "closed_at": "2018-04-04T16:49:41Z", "author_association": "NONE", "body_html": "<p>While working with the distributed package (TCP backend), I noticed that the memory used to store the target tensors in is never released by the garbage collector. Consider the following minimal example that produces a memory leak (and ultimately a crash):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">size</span>):\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        values <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100000</span>)\n        target <span class=\"pl-k\">=</span> [torch.zeros(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100000</span>)] <span class=\"pl-k\">*</span> size\n        dist.all_gather(<span class=\"pl-v\">tensor_list</span><span class=\"pl-k\">=</span>target, <span class=\"pl-v\">tensor</span><span class=\"pl-k\">=</span>values)\n        time.sleep(<span class=\"pl-c1\">1</span>)</pre></div>\n<p>Obviously, this example is pretty extreme, but the issue is also reproducible for smaller data sizes and long-running processes. When <code>all_gather</code> is removed, the memory is freed as expected.</p>\n<p>I generally would suppose that the garbage collector should release the memory after each iteration, or at least some time after each. However, I'm relatively new to Python and PyTorch, so this may be conceptually necessary or intended?</p>\n<p>The easiest workaround is obviously to define the target tensors outside the loop.</p>\n<p>Adding</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">del</span> target\ngc.collect()</pre></div>\n<p>does not solve the problem.</p>\n<p>System information:</p>\n<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version: 3.6</li>\n</ul>\n<p>Thanks for developing PyTorch, keep up the good work!</p>", "body_text": "While working with the distributed package (TCP backend), I noticed that the memory used to store the target tensors in is never released by the garbage collector. Consider the following minimal example that produces a memory leak (and ultimately a crash):\ndef run(size):\n    while True:\n        values = torch.zeros(100, 100000)\n        target = [torch.zeros(100, 100000)] * size\n        dist.all_gather(tensor_list=target, tensor=values)\n        time.sleep(1)\nObviously, this example is pretty extreme, but the issue is also reproducible for smaller data sizes and long-running processes. When all_gather is removed, the memory is freed as expected.\nI generally would suppose that the garbage collector should release the memory after each iteration, or at least some time after each. However, I'm relatively new to Python and PyTorch, so this may be conceptually necessary or intended?\nThe easiest workaround is obviously to define the target tensors outside the loop.\nAdding\ndel target\ngc.collect()\ndoes not solve the problem.\nSystem information:\n\nOS: Ubuntu 16.04\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6\n\nThanks for developing PyTorch, keep up the good work!", "body": "While working with the distributed package (TCP backend), I noticed that the memory used to store the target tensors in is never released by the garbage collector. Consider the following minimal example that produces a memory leak (and ultimately a crash):\r\n\r\n```python\r\ndef run(size):\r\n    while True:\r\n        values = torch.zeros(100, 100000)\r\n        target = [torch.zeros(100, 100000)] * size\r\n        dist.all_gather(tensor_list=target, tensor=values)\r\n        time.sleep(1)\r\n```\r\nObviously, this example is pretty extreme, but the issue is also reproducible for smaller data sizes and long-running processes. When `all_gather` is removed, the memory is freed as expected.\r\n\r\nI generally would suppose that the garbage collector should release the memory after each iteration, or at least some time after each. However, I'm relatively new to Python and PyTorch, so this may be conceptually necessary or intended?\r\n\r\nThe easiest workaround is obviously to define the target tensors outside the loop.\r\n\r\nAdding \r\n```python\r\ndel target\r\ngc.collect()\r\n```\r\ndoes not solve the problem.\r\n\r\nSystem information:\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6\r\n\r\nThanks for developing PyTorch, keep up the good work!"}