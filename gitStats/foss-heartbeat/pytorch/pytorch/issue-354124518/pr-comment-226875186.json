{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226875186", "pull_request_review_id": 166797813, "id": 226875186, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNjg3NTE4Ng==", "diff_hunk": "@@ -1987,6 +1988,13 @@ Tensor log1p_backward(const Tensor& grad, const Tensor& self) {\n   return grad / (self + 1);\n }\n \n+// Because the backward of pad(input, pads) is just pad(grad_output, [-p for p in pads])\n+Tensor constant_pad_nd_backward(const Tensor& grad, IntList pad) {\n+  auto negated_pad = pad.vec();\n+  std::transform(negated_pad.cbegin(), negated_pad.cend(), negated_pad.begin(), std::negate<int64_t>());\n+  return at::native::constant_pad_nd(grad, negated_pad, 0);", "path": "tools/autograd/templates/Functions.cpp", "position": null, "original_position": 16, "commit_id": "f2109c4136c379834d616724b6373791fabc8cee", "original_commit_id": "a03d7d139ff2d63671e821586576bd632cc800ab", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Thanks! One final nit: Could you change to calling `at::constant_pad_nd` instead? Calling the `native` one will not record `constant_pad_nd ` as a single edge in  backward, and instead record each operation in `at::native:: constant_pad_nd ` an edge, and thus causing double backward to be much slower.", "created_at": "2018-10-21T19:34:15Z", "updated_at": "2018-11-23T15:53:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/10885#discussion_r226875186", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10885", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226875186"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10885#discussion_r226875186"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10885"}}, "body_html": "<p>Thanks! One final nit: Could you change to calling <code>at::constant_pad_nd</code> instead? Calling the <code>native</code> one will not record <code>constant_pad_nd </code> as a single edge in  backward, and instead record each operation in <code>at::native:: constant_pad_nd </code> an edge, and thus causing double backward to be much slower.</p>", "body_text": "Thanks! One final nit: Could you change to calling at::constant_pad_nd instead? Calling the native one will not record constant_pad_nd  as a single edge in  backward, and instead record each operation in at::native:: constant_pad_nd  an edge, and thus causing double backward to be much slower."}