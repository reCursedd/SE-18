{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218518264", "pull_request_review_id": 156474898, "id": 218518264, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODUxODI2NA==", "diff_hunk": "@@ -0,0 +1,362 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/CPUApplyUtils.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/ExpandUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include \"ATen/native/LinearAlgebraUtils.h\"\n+\n+#include \"TH.h\"  // for USE_LAPACK\n+\n+#include <vector>\n+\n+// First the required LAPACK implementations are registered here.\n+// A comment above the registered LAPACK routine suggest which batched\n+// linear algebra function uses that routine\n+#ifdef USE_LAPACK\n+\n+// gesv\n+extern \"C\" void dgesv_(int* n, int* nrhs, double* a, int* lda, int *ipiv, double* b, int* ldb, int* info);\n+extern \"C\" void sgesv_(int* n, int* nrhs, float* a, int* lda, int* ipiv, float* b, int* ldb, int* info);\n+\n+// inverse\n+extern \"C\" void dgetrf_(int *m, int *n, double *a, int *lda, int *ipiv, int *info);\n+extern \"C\" void sgetrf_(int *m, int *n, float *a, int *lda, int *ipiv, int *info);\n+extern \"C\" void dgetri_(int *n, double *a, int *lda, int *ipiv, double *work, int *lwork, int *info);\n+extern \"C\" void sgetri_(int *n, float *a, int *lda, int *ipiv, float *work, int *lwork, int *info);\n+\n+// potrf\n+extern \"C\" void dpotrf_(char *uplo, int *n, double *a, int *lda, int *info);\n+extern \"C\" void spotrf_(char *uplo, int *n, float *a, int *lda, int *info);\n+\n+#endif\n+\n+namespace at {\n+namespace native {\n+\n+// Define the per-batch functions to be used in the main implementation of the batched\n+// linear algebra operations\n+template<class scalar_t>\n+void lapackGesv(int n, int nrhs, scalar_t* a, int lda, int* ipiv, scalar_t* b, int ldb, int* info) {\n+  AT_ERROR(\"gesv only takes float or double Tensors\");\n+}\n+\n+template<class scalar_t>\n+void lapackGetrf(int m, int n, scalar_t* a, int lda, int *ipiv, int *info) {\n+  AT_ERROR(\"getrf only takes float or double Tensors\");\n+}\n+\n+template<class scalar_t>\n+void lapackGetri(int n, scalar_t *a, int lda, int *ipiv, scalar_t *work, int lwork, int *info) {\n+  AT_ERROR(\"getri only takes float or double Tensors\");\n+}\n+\n+template<class scalar_t>\n+void lapackPotrf(char *uplo, int *n, scalar_t *a, int *lda, int *info) {\n+  AT_ERROR(\"potrf only takes float or double Tensors\");\n+}\n+\n+#ifdef USE_LAPACK\n+template<> void lapackGesv<double>(int n, int nrhs, double* a, int lda, int* ipiv, double* b, int ldb, int* info) {\n+  dgesv_(&n, &nrhs, a, &lda, ipiv, b, &ldb, info);\n+}\n+\n+template<> void lapackGesv<float>(int n, int nrhs, float* a, int lda, int* ipiv, float* b, int ldb, int* info) {\n+  sgesv_(&n, &nrhs, a, &lda, ipiv, b, &ldb, info);\n+}\n+\n+template<> void lapackGetri<double>(int n, double *a, int lda, int *ipiv, double *work, int lwork, int *info) {\n+  dgetri_(&n, a, &lda, ipiv, work, &lwork, info);\n+}\n+\n+template<> void lapackGetri<float>(int n, float *a, int lda, int *ipiv, float *work, int lwork, int *info) {\n+  sgetri_(&n, a, &lda, ipiv, work, &lwork, info);\n+}\n+\n+template<> void lapackGetrf<double>(int m, int n, double *a, int lda, int *ipiv, int *info) {\n+  dgetrf_(&m, &n, a, &lda, ipiv, info);\n+}\n+\n+template<> void lapackGetrf<float>(int m, int n, float *a, int lda, int *ipiv, int *info) {\n+  sgetrf_(&m, &n, a, &lda, ipiv, info);\n+}\n+\n+template<> void lapackPotrf<double>(char *uplo, int *n, double *a, int *lda, int *info) {\n+  dpotrf_(uplo, n, a, lda, info);\n+}\n+\n+template<> void lapackPotrf<float>(char *uplo, int *n, float *a, int *lda, int *info) {\n+  spotrf_(uplo, n, a, lda, info);\n+}\n+\n+#endif\n+\n+// Below of the definitions of the functions operating on a batch that are going to be dispatched\n+// in the main helper functions for the linear algebra operations\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gesv ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+template<typename scalar_t>\n+static void apply_gesv(Tensor& b, Tensor& A, std::vector<int64_t>& infos) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"gesv: LAPACK library not found in compilation\");\n+#endif\n+  auto A_data = A.data<scalar_t>();\n+  auto b_data = b.data<scalar_t>();\n+  auto A_mat_stride = matrixStride(A);\n+  auto b_mat_stride = matrixStride(b);\n+\n+  auto batch_size = batchCount(A);\n+  auto n = A.size(-2);\n+  auto nrhs = b.size(-1);\n+\n+  auto ipiv = at::empty({n}, b.type().toScalarType(kInt));\n+\n+  for (int64_t i = 0; i < batch_size; i++) {\n+    int info;\n+    scalar_t* A_working_ptr = &A_data[i * A_mat_stride];\n+    scalar_t* b_working_ptr = &b_data[i * b_mat_stride];\n+    lapackGesv<scalar_t>(n, nrhs, A_working_ptr, n, ipiv.data<int>(), b_working_ptr, n, &info);\n+    infos[i] = info;\n+    if (info != 0) {\n+      return;\n+    }\n+  }\n+}\n+\n+// These utilities are specified in LinearAlgebraUtils.h\n+LINALG_HELPER_2_ARGS(gesv, self, A, cpu)\n+\n+// Supports arbitrary batch dimensions for self and A\n+std::tuple<Tensor,Tensor> gesv(const Tensor& self, const Tensor& A) {\n+  if (self.dim() <= 2 && A.dim() <= 2) {\n+    // TODO: #7102: It's not necessary to have gesv (single) bindings for both\n+    // TH and ATen. We should remove the TH gesv bindings, especially\n+    // since the lapackGesv function is already in ATen.\n+    return at::_gesv_single(self, A);\n+  }\n+\n+  gesvCheckInputs(self, A);\n+\n+  // broadcast the batch dimensions of self and A.\n+  IntList self_batch_sizes(self.sizes().data(), self.ndimension() - 2);\n+  IntList A_batch_sizes(A.sizes().data(), A.ndimension() - 2);\n+  std::vector<int64_t> expand_batch_portion = infer_size(self_batch_sizes, A_batch_sizes);\n+\n+  std::vector<int64_t> self_expand_size({expand_batch_portion});\n+  self_expand_size.insert(self_expand_size.end(), { self.size(-2), self.size(-1) });\n+\n+  std::vector<int64_t> A_expand_size({expand_batch_portion});\n+  A_expand_size.insert(A_expand_size.end(), { A.size(-2), A.size(-1) });\n+\n+  Tensor self_broadcasted  = self.expand(self_expand_size);\n+  Tensor A_broadcasted = A.expand(A_expand_size);\n+  return at::_gesv_helper(self_broadcasted, A_broadcasted);\n+}\n+\n+std::tuple<Tensor&,Tensor&> gesv_out(Tensor& solution, Tensor& lu, const Tensor& self, const Tensor& A) {\n+  AT_CHECK(self.dim() == 2 && A.dim() == 2, \n+           \"torch.gesv() with the `out` keyword does not support batching. \"\n+           \"b.dim() (\", self.dim(), \") and A.dim() (\", A.dim(), \") must both be 2.\");\n+  return at::_gesv_single_out(solution, lu, self, A);\n+}\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ inverse ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+template <typename scalar_t>\n+static void apply_inverse(Tensor& self, std::vector<int64_t>& infos) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"inverse: LAPACK library not found in compilation\");\n+#endif\n+  auto self_data = self.data<scalar_t>();\n+  auto self_matrix_stride = matrixStride(self);\n+\n+  auto batch_size = batchCount(self);\n+  auto n = self.size(-2);\n+\n+  auto ipiv = at::empty({n}, self.type().toScalarType(kInt));\n+  int lwork;\n+  scalar_t wkopt;\n+  Tensor work;\n+\n+  for (int64_t i = 0; i < batch_size; i++) {\n+    int info;\n+    scalar_t* self_working_ptr = &self_data[i * self_matrix_stride];\n+    lapackGetrf<scalar_t>(n, n, self_working_ptr, n, ipiv.data<int>(), &info);\n+    infos[i] = info;\n+    if (info != 0) {\n+      return;\n+    }\n+\n+    // Run twice, first to get the optimum work size\n+    lwork = -1;\n+    lapackGetri<scalar_t>(n, self_working_ptr, n, ipiv.data<int>(), &wkopt, lwork, &info);\n+\n+    lwork = static_cast<int>(wkopt);\n+    work = at::empty({lwork}, self.type());\n+\n+    // now to compute the actual inverse\n+    lapackGetri<scalar_t>(n, self_working_ptr, n, ipiv.data<int>(), work.data<scalar_t>(), lwork, &info);\n+    infos[i] = info;\n+    if (info != 0) {\n+      return;\n+    }\n+  }\n+}\n+\n+LINALG_HELPER_1_ARGS(inverse, self, cpu)\n+\n+Tensor inverse(const Tensor &self) {\n+  if (self.size(-1) == 0) {\n+    return at::empty_like(self);\n+  }\n+  if (self.dim() == 2) {\n+    return at::_getri_single(self);\n+  }\n+  inverseCheckInputs(self);\n+  return at::_inverse_helper(self);\n+}\n+\n+Tensor& inverse_out(Tensor &result, const Tensor &self) {\n+  if (self.size(-1) == 0) {\n+    return result.resize_as_(self);\n+  }\n+  result.copy_(native::inverse(self));\n+  return result;\n+}\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ potrf ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+template<typename scalar_t>\n+static void apply_potrf(Tensor& A, bool upper) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"not compiled with LAPACK\");\n+#endif\n+\n+  char uplo = upper ? 'U' : 'L';\n+\n+  auto A_data = A.data<scalar_t>();\n+  auto A_mat_stride = matrixStride(A);\n+\n+  auto batch_size = batchCount(A);\n+  int n = A.size(-2);\n+  AT_CHECK(A.size(-1) == n, \"last two dimensions must be of equal size\");\n+  //THArgCheck(THTensor_nDimensionLegacyAll(a) == 2, 1, \"A should be 2 dimensional\");\n+  int lda = n;\n+\n+  for (int64_t i = 0; i < batch_size; i++) {\n+    int info;\n+    lapackPotrf(&uplo, &n, A_data+i*A_mat_stride, &lda, &info);\n+    AT_CHECK(info == 0, \"The leading minor of order \", info, \" is not positive definite\");\n+  }\n+}\n+\n+Tensor potrf_cpu(const Tensor &self, bool upper) {\n+  if (self.dim() == 0) {\n+    return self.sqrt();\n+  } else if (self.size(-1) == 0) {\n+    return at::empty_like(self);\n+  }\n+  AT_CHECK(self.dim() >= 2, \"tensor must be at least two-dimensional\");\n+  Tensor result = cloneBatchedColumnMajor(self);\n+  AT_DISPATCH_FLOATING_TYPES(result.type(), \"potrf\", [&] {\n+      apply_potrf<scalar_t>(result, upper);\n+    });\n+  if (upper) {\n+    result.triu_();\n+  } else {\n+    result.tril_();\n+  }\n+  return result;\n+}\n+\n+Tensor& potrf_out(Tensor& result, const Tensor &self, bool upper) {\n+  // should check if out is of the right format and copy before...\n+  result.resize_as_(self).copy_(at::potrf(self));\n+  return result;\n+}\n+\n+template <typename scalar_t, bool inplace, bool upper>\n+void apply_triu_tril(Tensor& result, const Tensor& self, int64_t k) {\n+  auto n = self.size(-2);\n+  auto m = self.size(-1);\n+  auto self_batched_ = self.view({-1, n, m});\n+  auto self_batched = self_batched_.accessor<scalar_t, 3>();\n+  auto result_batched_ = result.view({-1, n, m});", "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp", "position": 285, "original_position": 285, "commit_id": "3e2edf7ced20feed117c4398335a697063e84488", "original_commit_id": "6979ebdb2baed4196ebedcdbee0bc9045b3404a9", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Well, I've been thinking about this quite a bit in various contexts, but unless and until we introduce an iterator over multiple axes, I don't think we have good alternatives.\r\nI think it's OK as an assumption, it'll handle any 3d matrices and quite a bit of more general ones. One might ask whether one might want a nicer error message, but I'm not sure that the cost/benefit is good.", "created_at": "2018-09-18T17:11:28Z", "updated_at": "2018-11-23T15:51:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/11796#discussion_r218518264", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11796", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218518264"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11796#discussion_r218518264"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11796"}}, "body_html": "<p>Well, I've been thinking about this quite a bit in various contexts, but unless and until we introduce an iterator over multiple axes, I don't think we have good alternatives.<br>\nI think it's OK as an assumption, it'll handle any 3d matrices and quite a bit of more general ones. One might ask whether one might want a nicer error message, but I'm not sure that the cost/benefit is good.</p>", "body_text": "Well, I've been thinking about this quite a bit in various contexts, but unless and until we introduce an iterator over multiple axes, I don't think we have good alternatives.\nI think it's OK as an assumption, it'll handle any 3d matrices and quite a bit of more general ones. One might ask whether one might want a nicer error message, but I'm not sure that the cost/benefit is good.", "in_reply_to_id": 218509259}