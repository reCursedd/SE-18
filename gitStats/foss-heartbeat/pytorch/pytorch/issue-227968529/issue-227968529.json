{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1538", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1538/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1538/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1538/events", "html_url": "https://github.com/pytorch/pytorch/issues/1538", "id": 227968529, "node_id": "MDU6SXNzdWUyMjc5Njg1Mjk=", "number": 1538, "title": "different result on LSTM whether using batch_first", "user": {"login": "L1aoXingyu", "id": 19245504, "node_id": "MDQ6VXNlcjE5MjQ1NTA0", "avatar_url": "https://avatars0.githubusercontent.com/u/19245504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/L1aoXingyu", "html_url": "https://github.com/L1aoXingyu", "followers_url": "https://api.github.com/users/L1aoXingyu/followers", "following_url": "https://api.github.com/users/L1aoXingyu/following{/other_user}", "gists_url": "https://api.github.com/users/L1aoXingyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/L1aoXingyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/L1aoXingyu/subscriptions", "organizations_url": "https://api.github.com/users/L1aoXingyu/orgs", "repos_url": "https://api.github.com/users/L1aoXingyu/repos", "events_url": "https://api.github.com/users/L1aoXingyu/events{/privacy}", "received_events_url": "https://api.github.com/users/L1aoXingyu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-11T12:04:21Z", "updated_at": "2017-05-11T14:33:12Z", "closed_at": "2017-05-11T14:32:53Z", "author_association": "NONE", "body_html": "<p>I train a simple LSTM to do classifier on MNIST in LSTM, whether I use batch_first, I get the different result, here is my code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn, optim\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> transforms\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-2</span>\nnum_epoches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n\ntrain_dataset <span class=\"pl-k\">=</span> datasets.MNIST(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                               <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.ToTensor(),\n                               <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\ntest_dataset <span class=\"pl-k\">=</span> datasets.MNIST(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                              <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.ToTensor())\n\ntrain_loader <span class=\"pl-k\">=</span> DataLoader(train_dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ntest_loader <span class=\"pl-k\">=</span> DataLoader(test_dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Rnn</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_dim</span>, <span class=\"pl-smi\">hidden_dim</span>, <span class=\"pl-smi\">n_layer</span>, <span class=\"pl-smi\">n_class</span>):\n        <span class=\"pl-c1\">super</span>(Rnn, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.n_layer <span class=\"pl-k\">=</span> n_layer\n        <span class=\"pl-c1\">self</span>.hidden_dim <span class=\"pl-k\">=</span> hidden_dim\n        <span class=\"pl-c1\">self</span>.lstm <span class=\"pl-k\">=</span> nn.LSTM(in_dim, hidden_dim, n_layer)\n                            <span class=\"pl-c\"><span class=\"pl-c\">#</span> batch_first=True)</span>\n        <span class=\"pl-c1\">self</span>.classifier <span class=\"pl-k\">=</span> nn.Linear(hidden_dim, n_class)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> h0 = Variable(torch.zeros(self.n_layer, x.size(1),</span>\n                                <span class=\"pl-c\"><span class=\"pl-c\">#</span>   self.hidden_dim)).cuda()</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> c0 = Variable(torch.zeros(self.n_layer, x.size(1),</span>\n                                <span class=\"pl-c\"><span class=\"pl-c\">#</span>   self.hidden_dim)).cuda()</span>\n        out, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(x)\n        out <span class=\"pl-k\">=</span> out[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :, :]\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.classifier(out)\n        <span class=\"pl-k\">return</span> out\n\n\nmodel <span class=\"pl-k\">=</span> Rnn(<span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>) \nuse_gpu <span class=\"pl-k\">=</span> torch.cuda.is_available()  \n<span class=\"pl-k\">if</span> use_gpu:\n    model <span class=\"pl-k\">=</span> model.cuda()\n\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()\noptimizer <span class=\"pl-k\">=</span> optim.Adam(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>learning_rate)\n\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_epoches):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">*</span><span class=\"pl-c1\">10</span>)\n    running_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    running_acc <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    <span class=\"pl-k\">for</span> i, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader, <span class=\"pl-c1\">1</span>):\n        img, label <span class=\"pl-k\">=</span> data\n        b, c, h, w <span class=\"pl-k\">=</span> img.size()\n        <span class=\"pl-k\">assert</span> c <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>channel must be 1<span class=\"pl-pds\">'</span></span>\n        img <span class=\"pl-k\">=</span> img.view(w, b, h)\n        <span class=\"pl-k\">if</span> use_gpu:\n            img <span class=\"pl-k\">=</span> Variable(img).cuda()\n            label <span class=\"pl-k\">=</span> Variable(label).cuda()\n\n        out <span class=\"pl-k\">=</span> model(img)\n        loss <span class=\"pl-k\">=</span> criterion(out, label)\n        running_loss <span class=\"pl-k\">+=</span> loss.data[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">*</span> label.size(<span class=\"pl-c1\">0</span>)\n        _, pred <span class=\"pl-k\">=</span> torch.max(out, <span class=\"pl-c1\">1</span>)\n        num_correct <span class=\"pl-k\">=</span> (pred <span class=\"pl-k\">==</span> label).sum()\n        running_acc <span class=\"pl-k\">+=</span> num_correct.data[<span class=\"pl-c1\">0</span>]\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">300</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>[<span class=\"pl-c1\">{}</span>/<span class=\"pl-c1\">{}</span>] Loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span>, Acc: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span><span class=\"pl-pds\">'</span></span>.format(\n                epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, num_epoches,\n                running_loss<span class=\"pl-k\">/</span>(batch_size<span class=\"pl-k\">*</span>i),\n                running_acc<span class=\"pl-k\">/</span>(batch_size<span class=\"pl-k\">*</span>i)\n            ))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finish <span class=\"pl-c1\">{}</span> epoch, Loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span>, Acc: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span><span class=\"pl-pds\">'</span></span>.format(\n        epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>,\n        running_loss<span class=\"pl-k\">/</span>(<span class=\"pl-c1\">len</span>(train_dataset)),\n        running_acc<span class=\"pl-k\">/</span>(<span class=\"pl-c1\">len</span>(train_dataset))\n    ))\n</pre></div>\n<p>here is the first epoch result<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/19245504/25948165/f235610a-3684-11e7-94c0-a96d83ededae.png\"><img src=\"https://cloud.githubusercontent.com/assets/19245504/25948165/f235610a-3684-11e7-94c0-a96d83ededae.png\" alt=\"1\" style=\"max-width:100%;\"></a></p>\n<p>but if I use batch_first in LSTM, I get the different result, the different in code is below</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">self</span>.lstm <span class=\"pl-k\">=</span> nn.LSTM(in_dim, hidden_dim, n_layer, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nout <span class=\"pl-k\">=</span> out[:, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :]        \n\nimg <span class=\"pl-k\">=</span> img.view(b, w, h)</pre></div>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/19245504/25948169/f5bb7a6c-3684-11e7-9eed-54d45544b34e.png\"><img src=\"https://cloud.githubusercontent.com/assets/19245504/25948169/f5bb7a6c-3684-11e7-9eed-54d45544b34e.png\" alt=\"2\" style=\"max-width:100%;\"></a></p>\n<p>Is there a bug?</p>", "body_text": "I train a simple LSTM to do classifier on MNIST in LSTM, whether I use batch_first, I get the different result, here is my code\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\n\nbatch_size = 100\nlearning_rate = 1e-2\nnum_epoches = 20\n\ntrain_dataset = datasets.MNIST(root='./data', train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='./data', train=False,\n                              transform=transforms.ToTensor())\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n\nclass Rnn(nn.Module):\n    def __init__(self, in_dim, hidden_dim, n_layer, n_class):\n        super(Rnn, self).__init__()\n        self.n_layer = n_layer\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layer)\n                            # batch_first=True)\n        self.classifier = nn.Linear(hidden_dim, n_class)\n\n    def forward(self, x):\n        # h0 = Variable(torch.zeros(self.n_layer, x.size(1),\n                                #   self.hidden_dim)).cuda()\n        # c0 = Variable(torch.zeros(self.n_layer, x.size(1),\n                                #   self.hidden_dim)).cuda()\n        out, _ = self.lstm(x)\n        out = out[-1, :, :]\n        out = self.classifier(out)\n        return out\n\n\nmodel = Rnn(28, 128, 2, 10) \nuse_gpu = torch.cuda.is_available()  \nif use_gpu:\n    model = model.cuda()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epoches):\n    print('epoch {}'.format(epoch+1))\n    print('*'*10)\n    running_loss = 0.0\n    running_acc = 0.0\n    for i, data in enumerate(train_loader, 1):\n        img, label = data\n        b, c, h, w = img.size()\n        assert c == 1, 'channel must be 1'\n        img = img.view(w, b, h)\n        if use_gpu:\n            img = Variable(img).cuda()\n            label = Variable(label).cuda()\n\n        out = model(img)\n        loss = criterion(out, label)\n        running_loss += loss.data[0] * label.size(0)\n        _, pred = torch.max(out, 1)\n        num_correct = (pred == label).sum()\n        running_acc += num_correct.data[0]\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 300 == 0:\n            print('[{}/{}] Loss: {:.6f}, Acc: {:.6f}'.format(\n                epoch+1, num_epoches,\n                running_loss/(batch_size*i),\n                running_acc/(batch_size*i)\n            ))\n    print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n        epoch+1,\n        running_loss/(len(train_dataset)),\n        running_acc/(len(train_dataset))\n    ))\n\nhere is the first epoch result\n\nbut if I use batch_first in LSTM, I get the different result, the different in code is below\nself.lstm = nn.LSTM(in_dim, hidden_dim, n_layer, batch_first=True)\n\nout = out[:, -1, :]        \n\nimg = img.view(b, w, h)\n\nIs there a bug?", "body": "I train a simple LSTM to do classifier on MNIST in LSTM, whether I use batch_first, I get the different result, here is my code\r\n```python\r\nimport torch\r\nfrom torch import nn, optim\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision import datasets\r\n\r\nbatch_size = 100\r\nlearning_rate = 1e-2\r\nnum_epoches = 20\r\n\r\ntrain_dataset = datasets.MNIST(root='./data', train=True,\r\n                               transform=transforms.ToTensor(),\r\n                               download=True)\r\n\r\ntest_dataset = datasets.MNIST(root='./data', train=False,\r\n                              transform=transforms.ToTensor())\r\n\r\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\r\n\r\n\r\n\r\nclass Rnn(nn.Module):\r\n    def __init__(self, in_dim, hidden_dim, n_layer, n_class):\r\n        super(Rnn, self).__init__()\r\n        self.n_layer = n_layer\r\n        self.hidden_dim = hidden_dim\r\n        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layer)\r\n                            # batch_first=True)\r\n        self.classifier = nn.Linear(hidden_dim, n_class)\r\n\r\n    def forward(self, x):\r\n        # h0 = Variable(torch.zeros(self.n_layer, x.size(1),\r\n                                #   self.hidden_dim)).cuda()\r\n        # c0 = Variable(torch.zeros(self.n_layer, x.size(1),\r\n                                #   self.hidden_dim)).cuda()\r\n        out, _ = self.lstm(x)\r\n        out = out[-1, :, :]\r\n        out = self.classifier(out)\r\n        return out\r\n\r\n\r\nmodel = Rnn(28, 128, 2, 10) \r\nuse_gpu = torch.cuda.is_available()  \r\nif use_gpu:\r\n    model = model.cuda()\r\n\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\n\r\nfor epoch in range(num_epoches):\r\n    print('epoch {}'.format(epoch+1))\r\n    print('*'*10)\r\n    running_loss = 0.0\r\n    running_acc = 0.0\r\n    for i, data in enumerate(train_loader, 1):\r\n        img, label = data\r\n        b, c, h, w = img.size()\r\n        assert c == 1, 'channel must be 1'\r\n        img = img.view(w, b, h)\r\n        if use_gpu:\r\n            img = Variable(img).cuda()\r\n            label = Variable(label).cuda()\r\n\r\n        out = model(img)\r\n        loss = criterion(out, label)\r\n        running_loss += loss.data[0] * label.size(0)\r\n        _, pred = torch.max(out, 1)\r\n        num_correct = (pred == label).sum()\r\n        running_acc += num_correct.data[0]\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        if i % 300 == 0:\r\n            print('[{}/{}] Loss: {:.6f}, Acc: {:.6f}'.format(\r\n                epoch+1, num_epoches,\r\n                running_loss/(batch_size*i),\r\n                running_acc/(batch_size*i)\r\n            ))\r\n    print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\r\n        epoch+1,\r\n        running_loss/(len(train_dataset)),\r\n        running_acc/(len(train_dataset))\r\n    ))\r\n\r\n```\r\nhere is the first epoch result\r\n![1](https://cloud.githubusercontent.com/assets/19245504/25948165/f235610a-3684-11e7-94c0-a96d83ededae.png)\r\n\r\n\r\nbut if I use batch_first in LSTM, I get the different result, the different in code is below\r\n```python\r\nself.lstm = nn.LSTM(in_dim, hidden_dim, n_layer, batch_first=True)\r\n\r\nout = out[:, -1, :]        \r\n\r\nimg = img.view(b, w, h)\r\n```\r\n![2](https://cloud.githubusercontent.com/assets/19245504/25948169/f5bb7a6c-3684-11e7-9eed-54d45544b34e.png)\r\n\r\n\r\nIs there a bug?"}