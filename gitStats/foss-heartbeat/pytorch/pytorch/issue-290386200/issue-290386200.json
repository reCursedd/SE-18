{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4777", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4777/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4777/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4777/events", "html_url": "https://github.com/pytorch/pytorch/issues/4777", "id": 290386200, "node_id": "MDU6SXNzdWUyOTAzODYyMDA=", "number": 4777, "title": "Thread-save random number generator", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-22T08:36:40Z", "updated_at": "2018-01-22T17:06:54Z", "closed_at": "2018-01-22T17:06:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The generator works for one GPU, but when I'm trying <code>DataParallel</code> which creates multiple threads for GPUs, I get the segmentation error. After a few exploration in gdb, I found that this line of code may be the culprit. It's highly likely that the generator is shared among threads and race conditions happen here.</p>\n<div class=\"highlight highlight-source-c\"><pre>  y = *(_generator-&gt;state + (_generator-&gt;next)++);</pre></div>\n<h4>env</h4>\n<ul>\n<li>Pytorch 0.3.0 (pip)</li>\n<li>Ubuntu 16.04</li>\n<li>python 3.6</li>\n</ul>\n<h4>minimal reproducing snippet:</h4>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> threading <span class=\"pl-k\">import</span> Thread\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_worker</span>():\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        probs <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">500</span>, <span class=\"pl-c1\">500</span>).uniform_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n        random <span class=\"pl-k\">=</span> probs\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> random = torch.geometric(probs)</span>\n        <span class=\"pl-c1\">print</span>(random[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>])\n        time.sleep(<span class=\"pl-c1\">1</span>)\n\nthreads <span class=\"pl-k\">=</span> [Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>_worker) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">8</span>)]\n\n<span class=\"pl-k\">for</span> thread <span class=\"pl-k\">in</span> threads:\n    thread.start()\n<span class=\"pl-k\">for</span> thread <span class=\"pl-k\">in</span> threads:\n    thread.join()</pre></div>", "body_text": "The generator works for one GPU, but when I'm trying DataParallel which creates multiple threads for GPUs, I get the segmentation error. After a few exploration in gdb, I found that this line of code may be the culprit. It's highly likely that the generator is shared among threads and race conditions happen here.\n  y = *(_generator->state + (_generator->next)++);\nenv\n\nPytorch 0.3.0 (pip)\nUbuntu 16.04\npython 3.6\n\nminimal reproducing snippet:\nfrom threading import Thread\nimport torch\nimport time\n\ndef _worker():\n    while True:\n        probs = torch.Tensor(500, 500).uniform_(0, 1)\n        random = probs\n        # random = torch.geometric(probs)\n        print(random[0][0])\n        time.sleep(1)\n\nthreads = [Thread(target=_worker) for i in range(8)]\n\nfor thread in threads:\n    thread.start()\nfor thread in threads:\n    thread.join()", "body": "The generator works for one GPU, but when I'm trying `DataParallel` which creates multiple threads for GPUs, I get the segmentation error. After a few exploration in gdb, I found that this line of code may be the culprit. It's highly likely that the generator is shared among threads and race conditions happen here.\r\n\r\n```C\r\n  y = *(_generator->state + (_generator->next)++);\r\n```\r\n\r\n\r\n#### env\r\n- Pytorch 0.3.0 (pip)\r\n- Ubuntu 16.04\r\n- python 3.6\r\n\r\n#### minimal reproducing snippet:\r\n\r\n```python\r\nfrom threading import Thread\r\nimport torch\r\nimport time\r\n\r\ndef _worker():\r\n    while True:\r\n        probs = torch.Tensor(500, 500).uniform_(0, 1)\r\n        random = probs\r\n        # random = torch.geometric(probs)\r\n        print(random[0][0])\r\n        time.sleep(1)\r\n\r\nthreads = [Thread(target=_worker) for i in range(8)]\r\n\r\nfor thread in threads:\r\n    thread.start()\r\nfor thread in threads:\r\n    thread.join()\r\n```"}