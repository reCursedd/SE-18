{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203906240", "pull_request_review_id": 138891608, "id": 203906240, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzkwNjI0MA==", "diff_hunk": "@@ -0,0 +1,371 @@\n+#include \"caffe2/onnx/onnxifi_transformer.h\"\n+\n+#include <iostream>\n+#include <unordered_set>\n+\n+#include <google/protobuf/text_format.h>\n+\n+#include \"caffe2/core/context.h\"\n+#include \"caffe2/core/logging.h\"\n+#include \"caffe2/core/operator.h\"\n+#include \"caffe2/core/tensor.h\"\n+#include \"caffe2/onnx/onnx_exporter.h\"\n+#include \"caffe2/opt/backend_cutting.h\"\n+\n+namespace caffe2 {\n+\n+namespace {\n+\n+// TODO(yinghai): Remove the awkward conversion between unordered_map and map\n+std::unordered_map<std::string, TensorShape> InferShapes(\n+    Workspace* ws,\n+    NetDef* pred_net,\n+    CaffeMap<std::string, TensorShape>* shape_hints_ordered) {\n+  // Populate shapes from workplace\n+  const std::vector<string>& ws_blobs = ws->Blobs();\n+  for (const auto& s : ws_blobs) {\n+    shape_hints_ordered->emplace(s, GetTensorShapeOfBlob(ws->GetBlob(s)));", "path": "caffe2/onnx/onnxifi_transformer.cc", "position": null, "original_position": 27, "commit_id": "bcd44f8d733ba2703a40e9cbd823efcb67115750", "original_commit_id": "60b4564185daf7f45c20956e3ecaaa020bba9207", "user": {"login": "yinghai", "id": 1100089, "node_id": "MDQ6VXNlcjExMDAwODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1100089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinghai", "html_url": "https://github.com/yinghai", "followers_url": "https://api.github.com/users/yinghai/followers", "following_url": "https://api.github.com/users/yinghai/following{/other_user}", "gists_url": "https://api.github.com/users/yinghai/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinghai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinghai/subscriptions", "organizations_url": "https://api.github.com/users/yinghai/orgs", "repos_url": "https://api.github.com/users/yinghai/repos", "events_url": "https://api.github.com/users/yinghai/events{/privacy}", "received_events_url": "https://api.github.com/users/yinghai/received_events", "type": "User", "site_admin": false}, "body": "What's the impact of sparse data on the flow? It will break the shape inference as we don't support sparse tensor shape inference? In this case, we should just run the net once to get the shape info and put it in the hints, which is done in the onnxifi_transform.py. \r\n\r\nIf sparse data results in variable sized shapes downstream, then we are in trouble as I don't know whether there is backend that supports variable sized inputs yet. ", "created_at": "2018-07-19T23:57:45Z", "updated_at": "2018-11-23T15:47:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/9569#discussion_r203906240", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9569", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203906240"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9569#discussion_r203906240"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9569"}}, "body_html": "<p>What's the impact of sparse data on the flow? It will break the shape inference as we don't support sparse tensor shape inference? In this case, we should just run the net once to get the shape info and put it in the hints, which is done in the onnxifi_transform.py.</p>\n<p>If sparse data results in variable sized shapes downstream, then we are in trouble as I don't know whether there is backend that supports variable sized inputs yet.</p>", "body_text": "What's the impact of sparse data on the flow? It will break the shape inference as we don't support sparse tensor shape inference? In this case, we should just run the net once to get the shape info and put it in the hints, which is done in the onnxifi_transform.py.\nIf sparse data results in variable sized shapes downstream, then we are in trouble as I don't know whether there is backend that supports variable sized inputs yet.", "in_reply_to_id": 203893602}