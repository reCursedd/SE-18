{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203827865", "pull_request_review_id": 138775936, "id": 203827865, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzgyNzg2NQ==", "diff_hunk": "@@ -0,0 +1,371 @@\n+#include \"caffe2/onnx/onnxifi_transformer.h\"\n+\n+#include <iostream>\n+#include <unordered_set>\n+\n+#include <google/protobuf/text_format.h>\n+\n+#include \"caffe2/core/context.h\"\n+#include \"caffe2/core/logging.h\"\n+#include \"caffe2/core/operator.h\"\n+#include \"caffe2/core/tensor.h\"\n+#include \"caffe2/onnx/onnx_exporter.h\"\n+#include \"caffe2/opt/backend_cutting.h\"\n+\n+namespace caffe2 {\n+\n+namespace {\n+\n+// TODO(yinghai): Remove the awkward conversion between unordered_map and map\n+std::unordered_map<std::string, TensorShape> InferShapes(\n+    Workspace* ws,\n+    NetDef* pred_net,\n+    CaffeMap<std::string, TensorShape>* shape_hints_ordered) {\n+  // Populate shapes from workplace\n+  const std::vector<string>& ws_blobs = ws->Blobs();\n+  for (const auto& s : ws_blobs) {\n+    shape_hints_ordered->emplace(s, GetTensorShapeOfBlob(ws->GetBlob(s)));\n+  }\n+\n+  std::vector<NetDef*> nets;\n+  nets.emplace_back(pred_net);\n+  InferBlobShapesAndTypes(*shape_hints_ordered, nets);\n+  std::unordered_map<std::string, TensorShape> shape_hints;\n+  for (const auto& kv : *shape_hints_ordered) {\n+    shape_hints.emplace(kv.first, kv.second);\n+  }\n+\n+  return shape_hints;\n+}\n+\n+void DumpModel(const ::ONNX_NAMESPACE::ModelProto& model, const std::string& fname) {\n+  std::ofstream ff(fname);\n+  std::string body;\n+  ::google::protobuf::TextFormat::PrintToString(model.graph(), &body);\n+  ff << body << std::endl;\n+  ff.close();\n+}\n+\n+std::vector<::ONNX_NAMESPACE::ValueInfoProto> ConvertToValueInfo(\n+    const std::vector<std::string>& names,\n+    const std::unordered_map<std::string, TensorShape>& shape_hints) {\n+  std::vector<::ONNX_NAMESPACE::ValueInfoProto> r;\n+  for (const auto& s : names) {\n+    r.emplace_back();\n+    auto& value_info = r.back();\n+    value_info.set_name(s);\n+    const auto it = shape_hints.find(s);\n+    if (it == shape_hints.end()) {\n+      LOG(WARNING) << \"Cannot get shape of \" << s;\n+    } else {\n+      auto* tensor_type = value_info.mutable_type()->mutable_tensor_type();\n+      tensor_type->set_elem_type(\n+          ::ONNX_NAMESPACE::TensorProto_DataType::TensorProto_DataType_FLOAT);\n+      auto* shape = tensor_type->mutable_shape();\n+      for (int i = 0; i < it->second.dims().size(); ++i) {\n+        shape->add_dim()->set_dim_value(it->second.dims(i));\n+      }\n+    }\n+  }\n+  return r;\n+}\n+\n+void FillModelInfo(::ONNX_NAMESPACE::ModelProto* model) {\n+  model->set_ir_version(::ONNX_NAMESPACE::Version::IR_VERSION);\n+  model->set_producer_name(\"caffe2\");\n+  auto* opset_id = model->add_opset_import();\n+  opset_id->set_domain(\"\");\n+  opset_id->set_version(3);\n+}\n+} // namespace\n+\n+OnnxifiTransformer::OnnxifiTransformer(bool debug) : debug_(debug) {\n+  lib_ = onnx::initOnnxifiLibrary();\n+  CAFFE_ENFORCE(lib_, \"Cannot initialize ONNXIFI library\");\n+  CAFFE_ENFORCE_EQ(\n+      lib_->onnxGetBackendIDs(nullptr, &num_backends_), ONNXIFI_STATUS_FALLBACK);\n+  CAFFE_ENFORCE_GT(\n+      num_backends_, 0, \"At least 1 onnxifi backend should be available\");\n+  backend_ids_.resize(num_backends_);\n+  CAFFE_ENFORCE_EQ(\n+      lib_->onnxGetBackendIDs(backend_ids_.data(), &num_backends_),\n+      ONNXIFI_STATUS_SUCCESS);\n+}\n+\n+OperatorDef OnnxifiTransformer::BuildOnnxifiOp(\n+    const std::string& onnx_model_str,\n+    const std::unordered_map<std::string, std::vector<int>>& output_size_hints,\n+    const std::unordered_set<std::string>& initialization_list,\n+    const caffe2::NetDef& net) {\n+  OperatorDef op;\n+  op.set_type(\"Onnxifi\");\n+  auto* onnx_model_arg = op.add_arg();\n+  onnx_model_arg->set_name(\"onnx_model\");\n+  onnx_model_arg->set_s(onnx_model_str);\n+\n+  // Add the names of the initializer blobs that we want to fetch from the\n+  // workspace later\n+  auto* initializers_arg = op.add_arg();\n+  initializers_arg->set_name(\"initializers\");\n+  for (const auto& s : initialization_list) {\n+    initializers_arg->add_strings(s);\n+    initializers_arg->add_strings(input_mapping_.at(s));\n+  }\n+\n+  // Add the input/output\n+  for (const auto& input : net.external_input()) {\n+    if (!initialization_list.count(input)) {\n+      op.add_input(input);\n+    }\n+  }\n+  for (const auto& output : net.external_output()) {\n+    op.add_output(output);\n+  }\n+\n+  // Add output size hints\n+  for (int i = 0; i < op.output_size(); ++i) {\n+    const auto& o = op.output(i);\n+    const auto it = output_size_hints.find(o);\n+    if (it != output_size_hints.end()) {\n+      const auto& dims = it->second;\n+      auto* output_size_hint_arg = op.add_arg();\n+      output_size_hint_arg->set_name(MakeString(\"output_size_hint_\", i));\n+      for (const auto& d : dims) {\n+        output_size_hint_arg->add_ints(d);\n+      }\n+\n+      LOG(INFO) << \"Adding output hint: \" << o;\n+    }\n+  }\n+  return op;\n+}\n+\n+NetDef OnnxifiTransformer::SubnetToOnnxifiOp(\n+    const caffe2::NetDef& net,\n+    Workspace* ws,\n+    onnx::OnnxExporter* exporter,\n+    std::unordered_map<std::string, TensorShape>* shape_hints) {\n+  ::ONNX_NAMESPACE::ModelProto onnx_model;\n+  FillModelInfo(&onnx_model);\n+\n+  // Convert c2 ops to onnx ops, add const weights if there are any\n+  DeviceOption option;\n+  CPUContext context(option);\n+  context.SwitchToDevice();\n+  for (const auto& op : net.op()) {\n+    const auto results = exporter->Caffe2OpToOnnxNodes(op, *shape_hints);\n+    for (const auto& n : results.first) {\n+      onnx_model.mutable_graph()->add_node()->CopyFrom(n);\n+    }\n+    for (const auto& t : results.second) {\n+      VLOG(2) << \"Adding extra init tensor: \" << t.name();\n+      CAFFE_ENFORCE_EQ(\n+          t.data_type(),\n+          ::ONNX_NAMESPACE::TensorProto::FLOAT,\n+          \"Only supports conversion of float type for now\");\n+      TensorShape shape;\n+      shape.mutable_dims()->CopyFrom(t.dims());\n+      shape_hints->emplace(t.name(), std::move(shape));\n+\n+      // Feed into workspace as CPU Tensors\n+      auto* blob = ws->CreateBlob(t.name());\n+      auto* cpu_tensor = blob->GetMutable<TensorCPU>();\n+      std::vector<TIndex> dims;\n+      std::copy(t.dims().begin(), t.dims().end(), dims.begin());\n+      cpu_tensor->Resize(dims);\n+      context.template CopyBytes<CPUContext, CPUContext>(\n+          cpu_tensor->size() * sizeof(float),\n+          static_cast<const void*>(t.raw_data().data()),\n+          cpu_tensor->raw_mutable_data(TypeMeta::Make<float>()));\n+      context.FinishDeviceComputation();\n+\n+      // Add mappings\n+      CAFFE_ENFORCE(\n+          input_mapping_.emplace(t.name(), t.name()).second,\n+          MakeString(\"Tensor \", t.name(), \" already exists in the workspace\"));\n+    }\n+  }\n+\n+  // Convert outputs and compute output shape hints\n+  std::vector<std::string> io_names;\n+  for (const auto& output : net.external_output()) {\n+    io_names.emplace_back(output);\n+  }\n+  auto io_vec = ConvertToValueInfo(io_names, *shape_hints);\n+  std::unordered_map<std::string, std::vector<int>> output_shape_hints;\n+  for (const auto& i : io_vec) {\n+    onnx_model.mutable_graph()->add_output()->CopyFrom(i);\n+    auto ret = output_shape_hints.emplace(i.name(), std::vector<int>());\n+    auto& vec = ret.first->second;\n+    const auto it = shape_hints->find(i.name());\n+    CAFFE_ENFORCE(\n+        it != shape_hints->end(),\n+        \"Cannot find shape info for output \",\n+        i.name());\n+    const auto& shape = it->second;\n+    for (int k = 0; k < shape.dims().size(); ++k) {\n+      vec.push_back(shape.dims(k));\n+    }\n+  }\n+\n+  // Convert inputs and figure out weights\n+  std::unordered_set<std::string> weights;\n+  const std::vector<string>& ws_blobs = ws->Blobs();\n+  for (const auto& s : ws_blobs) {\n+    VLOG(2) << \"Add weights: \" << s;\n+    weights.emplace(s);\n+  }\n+\n+  std::unordered_set<std::string> total_inputs;\n+  std::unordered_set<std::string> initialization_list;\n+  std::vector<std::string> total_inputs_vec;\n+\n+  // Extra intermediate weights created during conversion\n+  for (const auto& extra_weight : onnx_model.graph().initializer()) {\n+    if (total_inputs.emplace(extra_weight.name()).second) {\n+      total_inputs_vec.emplace_back(extra_weight.name());\n+    }\n+  }\n+  // Boundary inputs, should not be weights\n+  std::unordered_set<std::string> boundary_inputs;\n+  for (const auto& i : net.external_input()) {\n+    boundary_inputs.emplace(i);\n+  }\n+\n+  for (const auto& op : net.op()) {\n+    for (const auto& input : op.input()) {\n+      if (total_inputs.emplace(input).second && weights.count(input)) {\n+        // We add weights as inputs too\n+        total_inputs_vec.emplace_back(input);\n+        initialization_list.emplace(input);\n+        VLOG(2) << \"Add input weights: \" << input;\n+      } else if (boundary_inputs.count(input)) {\n+        VLOG(2) << \"Adding boundary input: \" << input;\n+        total_inputs_vec.emplace_back(input);\n+      }\n+    }\n+  }\n+  io_vec = ConvertToValueInfo(total_inputs_vec, *shape_hints);\n+  for (const auto& i : io_vec) {\n+    onnx_model.mutable_graph()->add_input()->CopyFrom(i);\n+  }\n+\n+  // Debugging stuff\n+  if (debug_) {\n+    DumpModel(onnx_model, \"debug.onnxtxt\");\n+  }\n+\n+  // Onnx model is ready. Build ONNXIFI Op\n+  std::string model_str;\n+  onnx_model.SerializeToString(&model_str);\n+  NetDef net_opt;\n+  auto* op = net_opt.add_op();\n+  *op = BuildOnnxifiOp(model_str, output_shape_hints, initialization_list, net);\n+  for (const auto& i : op->input()) {\n+    net_opt.add_external_input(i);\n+  }\n+  for (const auto& i : op->output()) {\n+    net_opt.add_external_output(i);\n+  }\n+\n+  return net_opt;\n+}\n+\n+CaffeMap<std::string, TensorShape> OnnxifiTransformer::SsaRewriteAndMapNames(\n+    Workspace* ws,\n+    NetDef* pred_net,\n+    const std::unordered_map<std::string, TensorShape>& input_shape_hints) {\n+  input_mapping_ = onnx::SsaRewrite(nullptr, pred_net);\n+  std::unordered_map<std::string, std::string> input_reverse_mapping;\n+  std::vector<std::string> external_inputs;\n+  for (const auto kv : input_mapping_) {\n+    input_reverse_mapping.emplace(kv.second, kv.first);\n+    if (!ws->HasBlob(kv.second)) {\n+      external_inputs.emplace_back(kv.first);\n+    }\n+  }\n+  for (const auto& i : external_inputs) {\n+    input_mapping_.erase(i);\n+  }\n+  CaffeMap<std::string, TensorShape> shape_hints_ordered;\n+  for (const auto& kv : input_shape_hints) {\n+    const auto it = input_reverse_mapping.find(kv.first);\n+    if (it != input_reverse_mapping.end()) {\n+      LOG(INFO) << \"Adding input hint: \" << it->second;\n+      shape_hints_ordered.emplace(it->second, kv.second);\n+    } else {\n+      shape_hints_ordered.emplace(kv.first, kv.second);\n+    }\n+  }\n+  return shape_hints_ordered;\n+}\n+\n+// Cutting off the runnable part and replace with ONNXIFI ops. Asssume the nets\n+// were topologically sorted\n+void OnnxifiTransformer::Transform(\n+    Workspace* ws,\n+    NetDef* pred_net,\n+    const std::unordered_map<std::string, TensorShape>& input_shape_hints) {\n+  CAFFE_ENFORCE(ws);\n+  auto shape_hints_ordered =\n+      SsaRewriteAndMapNames(ws, pred_net, input_shape_hints);\n+  Workspace mapped_ws(ws, input_mapping_);\n+  auto shape_hints = InferShapes(&mapped_ws, pred_net, &shape_hints_ordered);\n+\n+  CAFFE_ENFORCE(pred_net, \"Predict net cannot be nullptr\");\n+  onnx::OnnxExporter exporter(nullptr, true);\n+\n+  // function to tell whether the ONNXIFI backend supports a given C2 op or not\n+  // TODO: choose backend id\n+  auto supports = [&exporter,\n+                   &shape_hints,\n+                   backend = lib_,\n+                   backend_id =\n+                       backend_ids_[0]](const caffe2::OperatorDef& op) {\n+    const OpSchema* schema = OpSchemaRegistry::Schema(op.type());\n+    // NB: this might not be a hard constraint as we can just export C2\n+    // domain specific ops to ONNX\n+    if (!schema || schema->onnx_schema().empty()) {\n+      LOG(INFO) << \"Cannot export c2 op \" << op.type() << \" to onnx\";\n+      return false;\n+    }\n+\n+    ::ONNX_NAMESPACE::ModelProto onnx_model;\n+    FillModelInfo(&onnx_model);\n+    auto results = exporter.Caffe2OpToOnnxNodes(op, shape_hints);", "path": "caffe2/onnx/onnxifi_transformer.cc", "position": null, "original_position": 335, "commit_id": "bcd44f8d733ba2703a40e9cbd823efcb67115750", "original_commit_id": "60b4564185daf7f45c20956e3ecaaa020bba9207", "user": {"login": "bddppq", "id": 9300575, "node_id": "MDQ6VXNlcjkzMDA1NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9300575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bddppq", "html_url": "https://github.com/bddppq", "followers_url": "https://api.github.com/users/bddppq/followers", "following_url": "https://api.github.com/users/bddppq/following{/other_user}", "gists_url": "https://api.github.com/users/bddppq/gists{/gist_id}", "starred_url": "https://api.github.com/users/bddppq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bddppq/subscriptions", "organizations_url": "https://api.github.com/users/bddppq/orgs", "repos_url": "https://api.github.com/users/bddppq/repos", "events_url": "https://api.github.com/users/bddppq/events{/privacy}", "received_events_url": "https://api.github.com/users/bddppq/received_events", "type": "User", "site_admin": false}, "body": "eventually we should have the conversion driver in the c++ side instead of python. by driver I mean the part that fills in the meta information and kicks off the for loop for converting each op. once we have the driver in c++ here we should wrap the single c2 op to a c2 net and let the driver handle the full conversion.", "created_at": "2018-07-19T18:28:02Z", "updated_at": "2018-11-23T15:47:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/9569#discussion_r203827865", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9569", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203827865"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9569#discussion_r203827865"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9569"}}, "body_html": "<p>eventually we should have the conversion driver in the c++ side instead of python. by driver I mean the part that fills in the meta information and kicks off the for loop for converting each op. once we have the driver in c++ here we should wrap the single c2 op to a c2 net and let the driver handle the full conversion.</p>", "body_text": "eventually we should have the conversion driver in the c++ side instead of python. by driver I mean the part that fills in the meta information and kicks off the for loop for converting each op. once we have the driver in c++ here we should wrap the single c2 op to a c2 net and let the driver handle the full conversion."}