{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7808", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7808/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7808/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7808/events", "html_url": "https://github.com/pytorch/pytorch/issues/7808", "id": 326126726, "node_id": "MDU6SXNzdWUzMjYxMjY3MjY=", "number": 7808, "title": "dropout masking", "user": {"login": "yiqiaoc11", "id": 30539007, "node_id": "MDQ6VXNlcjMwNTM5MDA3", "avatar_url": "https://avatars2.githubusercontent.com/u/30539007?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yiqiaoc11", "html_url": "https://github.com/yiqiaoc11", "followers_url": "https://api.github.com/users/yiqiaoc11/followers", "following_url": "https://api.github.com/users/yiqiaoc11/following{/other_user}", "gists_url": "https://api.github.com/users/yiqiaoc11/gists{/gist_id}", "starred_url": "https://api.github.com/users/yiqiaoc11/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yiqiaoc11/subscriptions", "organizations_url": "https://api.github.com/users/yiqiaoc11/orgs", "repos_url": "https://api.github.com/users/yiqiaoc11/repos", "events_url": "https://api.github.com/users/yiqiaoc11/events{/privacy}", "received_events_url": "https://api.github.com/users/yiqiaoc11/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-24T13:39:33Z", "updated_at": "2018-05-30T06:40:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>We found memory leaking with Dropout within one epoch. It recovers to be normal after deleting self.dp() ... It seems graph keeps growing. I assume torch/nn/_functions/dropout.py should have mask with requires_grad = False.</p>\n<p>+-------------------------------+----------------------+----------------------+<br>\n| 28%   51C    P2    64W / 250W |   6854MiB / 11178MiB |      3%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n...<br>\n+-------------------------------+----------------------+----------------------+<br>\n| 29%   51C    P2    66W / 250W |   8986MiB / 11178MiB |     10%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n+-------------------------------+----------------------+----------------------+<br>\n| 29%   51C    P0    63W / 250W |  10152MiB / 11178MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<h2>Code example</h2>\n<pre><code>    self.dp = nn.Dropout(dropout)\n</code></pre>\n<p>...<br>\nembed_p = self.x_proj(embed_p)  # embed_p ~ L * B * hidden_size<br>\nembed_h = self.x_proj(embed_h)</p>\n<pre><code>    embed_p = self.dp(embed_p)\n    embed_h = self.dp(embed_h) \n</code></pre>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2:</li>\n<li>How you installed PyTorch (conda, pip, source):</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS:</li>\n<li>PyTorch version:</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nWe found memory leaking with Dropout within one epoch. It recovers to be normal after deleting self.dp() ... It seems graph keeps growing. I assume torch/nn/_functions/dropout.py should have mask with requires_grad = False.\n+-------------------------------+----------------------+----------------------+\n| 28%   51C    P2    64W / 250W |   6854MiB / 11178MiB |      3%      Default |\n+-------------------------------+----------------------+----------------------+\n...\n+-------------------------------+----------------------+----------------------+\n| 29%   51C    P2    66W / 250W |   8986MiB / 11178MiB |     10%      Default |\n+-------------------------------+----------------------+----------------------+\n+-------------------------------+----------------------+----------------------+\n| 29%   51C    P0    63W / 250W |  10152MiB / 11178MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\nCode example\n    self.dp = nn.Dropout(dropout)\n\n...\nembed_p = self.x_proj(embed_p)  # embed_p ~ L * B * hidden_size\nembed_h = self.x_proj(embed_h)\n    embed_p = self.dp(embed_p)\n    embed_h = self.dp(embed_h) \n\nSystem Info\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch or Caffe2:\nHow you installed PyTorch (conda, pip, source):\nBuild command you used (if compiling from source):\nOS:\nPyTorch version:\nPython version:\nCUDA/cuDNN version:\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\nWe found memory leaking with Dropout within one epoch. It recovers to be normal after deleting self.dp() ... It seems graph keeps growing. I assume torch/nn/_functions/dropout.py should have mask with requires_grad = False.\r\n\r\n\r\n+-------------------------------+----------------------+----------------------+\r\n| 28%   51C    P2    64W / 250W |   6854MiB / 11178MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n...\r\n+-------------------------------+----------------------+----------------------+\r\n| 29%   51C    P2    66W / 250W |   8986MiB / 11178MiB |     10%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n+-------------------------------+----------------------+----------------------+\r\n| 29%   51C    P0    63W / 250W |  10152MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n## Code example\r\n\r\n        self.dp = nn.Dropout(dropout)\r\n...\r\n        embed_p = self.x_proj(embed_p)  # embed_p ~ L * B * hidden_size\r\n        embed_h = self.x_proj(embed_h)\r\n\r\n        embed_p = self.dp(embed_p)\r\n        embed_h = self.dp(embed_h) \r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2:\r\n- How you installed PyTorch (conda, pip, source):\r\n- Build command you used (if compiling from source):\r\n- OS:\r\n- PyTorch version:\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}