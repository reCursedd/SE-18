{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8673", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8673/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8673/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8673/events", "html_url": "https://github.com/pytorch/pytorch/issues/8673", "id": 333868042, "node_id": "MDU6SXNzdWUzMzM4NjgwNDI=", "number": 8673, "title": "bernoulli_ is very inefficient on the GPU", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-19T22:58:16Z", "updated_at": "2018-06-20T15:44:52Z", "closed_at": "2018-06-20T15:44:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>bernoulli_ call has become very inefficient recently (may be <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"323259108\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7578\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7578/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7578\">#7578</a> is at fault?)</p>\n<pre><code>a.bernoulli_(p) \n</code></pre>\n<p>if a is gpu float or half tensor this expands to</p>\n<ol>\n<li>fill a size-1 <em>double</em> tensor with p value (should not be neccessary, but ok)</li>\n<li>expand this <em>double</em> tensor to size of a and materialize it (unnecessary expensive copy op)</li>\n<li>generate <em>double</em> tensor filled with bernoulli-distributed vars (unnecessary use of double, perf penalty)</li>\n<li>copy double tensor to a (no copy should be necessary for an inplace op).</li>\n</ol>\n<p>Pytorch, current master<br>\ncc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a></p>", "body_text": "bernoulli_ call has become very inefficient recently (may be #7578 is at fault?)\na.bernoulli_(p) \n\nif a is gpu float or half tensor this expands to\n\nfill a size-1 double tensor with p value (should not be neccessary, but ok)\nexpand this double tensor to size of a and materialize it (unnecessary expensive copy op)\ngenerate double tensor filled with bernoulli-distributed vars (unnecessary use of double, perf penalty)\ncopy double tensor to a (no copy should be necessary for an inplace op).\n\nPytorch, current master\ncc @cpuhrsch, @gchanan", "body": "bernoulli_ call has become very inefficient recently (may be #7578 is at fault?)\r\n\r\n```\r\na.bernoulli_(p) \r\n```\r\nif a is gpu float or half tensor this expands to\r\n1) fill a size-1 *double* tensor with p value (should not be neccessary, but ok)\r\n2) expand this *double* tensor to size of a and materialize it (unnecessary expensive copy op)\r\n3) generate *double* tensor filled with bernoulli-distributed vars (unnecessary use of double, perf penalty)\r\n4) copy double tensor to a (no copy should be necessary for an inplace op). \r\n\r\nPytorch, current master\r\ncc @cpuhrsch, @gchanan"}