{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/381479248", "html_url": "https://github.com/pytorch/pytorch/pull/6616#issuecomment-381479248", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6616", "id": 381479248, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTQ3OTI0OA==", "user": {"login": "atulkum", "id": 372035, "node_id": "MDQ6VXNlcjM3MjAzNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/372035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/atulkum", "html_url": "https://github.com/atulkum", "followers_url": "https://api.github.com/users/atulkum/followers", "following_url": "https://api.github.com/users/atulkum/following{/other_user}", "gists_url": "https://api.github.com/users/atulkum/gists{/gist_id}", "starred_url": "https://api.github.com/users/atulkum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/atulkum/subscriptions", "organizations_url": "https://api.github.com/users/atulkum/orgs", "repos_url": "https://api.github.com/users/atulkum/repos", "events_url": "https://api.github.com/users/atulkum/events{/privacy}", "received_events_url": "https://api.github.com/users/atulkum/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-16T04:51:26Z", "updated_at": "2018-04-16T05:50:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>What I understood is that the effective learning rate \"lr/sqrt(G_ii + eps)\" would be very high in the initial phase of optimization if G_ii starts from 0. So to make the loss fluctuation smooth in the initial phase, its better to initialize G_ii with some small value.  (reference for notation: <a href=\"http://ruder.io/optimizing-gradient-descent/index.html#adagrad\" rel=\"nofollow\">http://ruder.io/optimizing-gradient-descent/index.html#adagrad</a>)</p>\n<p>I was looking for pytorch equivalent implementation for Tensorflow AdagradOptimizer(<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer</a>).</p>", "body_text": "What I understood is that the effective learning rate \"lr/sqrt(G_ii + eps)\" would be very high in the initial phase of optimization if G_ii starts from 0. So to make the loss fluctuation smooth in the initial phase, its better to initialize G_ii with some small value.  (reference for notation: http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\nI was looking for pytorch equivalent implementation for Tensorflow AdagradOptimizer(https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer).", "body": "What I understood is that the effective learning rate \"lr/sqrt(G_ii + eps)\" would be very high in the initial phase of optimization if G_ii starts from 0. So to make the loss fluctuation smooth in the initial phase, its better to initialize G_ii with some small value.  (reference for notation: http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\r\n\r\n I was looking for pytorch equivalent implementation for Tensorflow AdagradOptimizer(https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer).\r\n\r\n"}