{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183676659", "pull_request_review_id": 114717378, "id": 183676659, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MzY3NjY1OQ==", "diff_hunk": "@@ -1515,6 +1515,8 @@ def binary_cross_entropy_with_logits(input, target, weight=None, size_average=Tr\n                 observations for each minibatch depending on :attr:`size_average`. When :attr:`reduce`\n                 is ``False``, returns a loss per input/target element instead and ignores\n                 :attr:`size_average`. Default: ``True``\n+        pos_weight (Tensor, optional): a weight of positive examples.\n+                Must be a vector with length equal to the number of classes.", "path": "torch/nn/functional.py", "position": 14, "original_position": 14, "commit_id": "c1022c31ef025ef405ce48ab31a2a25a87dd1794", "original_commit_id": "c1022c31ef025ef405ce48ab31a2a25a87dd1794", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Also, I'm wondering if we really want to add this. You can easily weight samples according to even more complicated schemes if you only use `reduce=False`. In particular, I think this one is equivalent to the code below, which isn't too complicated.\r\n```python\r\n(F.binary_cross_entropy_with_logits(..., reduce=False) * class_weights).mean()\r\n```\r\nHere `class_weights` allows you to weight both things, if you want to get the `pos_weight` behavior then you do\r\n```python\r\nclass_weights = torch.ones_like(input).mask_fill_(target == 1, pos_weight_value)\r\n```", "created_at": "2018-04-24T10:12:40Z", "updated_at": "2018-11-23T15:43:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/6856#discussion_r183676659", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6856", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183676659"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6856#discussion_r183676659"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6856"}}, "body_html": "<p>Also, I'm wondering if we really want to add this. You can easily weight samples according to even more complicated schemes if you only use <code>reduce=False</code>. In particular, I think this one is equivalent to the code below, which isn't too complicated.</p>\n<div class=\"highlight highlight-source-python\"><pre>(F.binary_cross_entropy_with_logits(<span class=\"pl-c1\">...</span>, <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>) <span class=\"pl-k\">*</span> class_weights).mean()</pre></div>\n<p>Here <code>class_weights</code> allows you to weight both things, if you want to get the <code>pos_weight</code> behavior then you do</p>\n<div class=\"highlight highlight-source-python\"><pre>class_weights <span class=\"pl-k\">=</span> torch.ones_like(<span class=\"pl-c1\">input</span>).mask_fill_(target <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>, pos_weight_value)</pre></div>", "body_text": "Also, I'm wondering if we really want to add this. You can easily weight samples according to even more complicated schemes if you only use reduce=False. In particular, I think this one is equivalent to the code below, which isn't too complicated.\n(F.binary_cross_entropy_with_logits(..., reduce=False) * class_weights).mean()\nHere class_weights allows you to weight both things, if you want to get the pos_weight behavior then you do\nclass_weights = torch.ones_like(input).mask_fill_(target == 1, pos_weight_value)", "in_reply_to_id": 183546011}