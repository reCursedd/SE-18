{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183686582", "pull_request_review_id": 114729459, "id": 183686582, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MzY4NjU4Mg==", "diff_hunk": "@@ -1515,6 +1515,8 @@ def binary_cross_entropy_with_logits(input, target, weight=None, size_average=Tr\n                 observations for each minibatch depending on :attr:`size_average`. When :attr:`reduce`\n                 is ``False``, returns a loss per input/target element instead and ignores\n                 :attr:`size_average`. Default: ``True``\n+        pos_weight (Tensor, optional): a weight of positive examples.\n+                Must be a vector with length equal to the number of classes.", "path": "torch/nn/functional.py", "position": 14, "original_position": 14, "commit_id": "c1022c31ef025ef405ce48ab31a2a25a87dd1794", "original_commit_id": "c1022c31ef025ef405ce48ab31a2a25a87dd1794", "user": {"login": "velikodniy", "id": 1735272, "node_id": "MDQ6VXNlcjE3MzUyNzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1735272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/velikodniy", "html_url": "https://github.com/velikodniy", "followers_url": "https://api.github.com/users/velikodniy/followers", "following_url": "https://api.github.com/users/velikodniy/following{/other_user}", "gists_url": "https://api.github.com/users/velikodniy/gists{/gist_id}", "starred_url": "https://api.github.com/users/velikodniy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/velikodniy/subscriptions", "organizations_url": "https://api.github.com/users/velikodniy/orgs", "repos_url": "https://api.github.com/users/velikodniy/repos", "events_url": "https://api.github.com/users/velikodniy/events{/privacy}", "received_events_url": "https://api.github.com/users/velikodniy/received_events", "type": "User", "site_admin": false}, "body": "`class_weights` are the weights of labels. `pos_weight` acts *inside* BCE formula for every particular label.\r\n\r\n**The usual BCE**:\r\n\r\n`target * -log(sigmoid(input)) + (1 - target) * -log(1 - sigmoid(input))`\r\n\r\nFor multilabel classification we can use vectors for `target` and `input`.\r\n\r\n**The proposed BCE**:\r\n\r\n`pos_weight * target * -log(sigmoid(input)) + (1 - target) * -log(1 - sigmoid(input))`\r\n\r\n`pos_weight` is multiplied by only the \"positive\" part. It allows us to \"increase\" the fraction of positive examples in imbalanced datasets, so we can trade off precision and recall. I don't know any simple way to do it *outside* BCE.\r\n\r\nTF already has [a similar function](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits).\r\n\r\nFor example. If we have 7 labels and the batch size is equal to 4. Then:\r\n- input \u2014 4\u00d77 \u2014 logits;\r\n- target \u2014 4\u00d77 \u2014 ones and zeroes;\r\n- pos_weight \u2014 1\u00d77 (or just 7, thanks to broadcasting) \u2014 a vector of positive part weights for every label.", "created_at": "2018-04-24T10:53:16Z", "updated_at": "2018-11-23T15:43:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/6856#discussion_r183686582", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6856", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183686582"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6856#discussion_r183686582"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6856"}}, "body_html": "<p><code>class_weights</code> are the weights of labels. <code>pos_weight</code> acts <em>inside</em> BCE formula for every particular label.</p>\n<p><strong>The usual BCE</strong>:</p>\n<p><code>target * -log(sigmoid(input)) + (1 - target) * -log(1 - sigmoid(input))</code></p>\n<p>For multilabel classification we can use vectors for <code>target</code> and <code>input</code>.</p>\n<p><strong>The proposed BCE</strong>:</p>\n<p><code>pos_weight * target * -log(sigmoid(input)) + (1 - target) * -log(1 - sigmoid(input))</code></p>\n<p><code>pos_weight</code> is multiplied by only the \"positive\" part. It allows us to \"increase\" the fraction of positive examples in imbalanced datasets, so we can trade off precision and recall. I don't know any simple way to do it <em>outside</em> BCE.</p>\n<p>TF already has <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\" rel=\"nofollow\">a similar function</a>.</p>\n<p>For example. If we have 7 labels and the batch size is equal to 4. Then:</p>\n<ul>\n<li>input \u2014 4\u00d77 \u2014 logits;</li>\n<li>target \u2014 4\u00d77 \u2014 ones and zeroes;</li>\n<li>pos_weight \u2014 1\u00d77 (or just 7, thanks to broadcasting) \u2014 a vector of positive part weights for every label.</li>\n</ul>", "body_text": "class_weights are the weights of labels. pos_weight acts inside BCE formula for every particular label.\nThe usual BCE:\ntarget * -log(sigmoid(input)) + (1 - target) * -log(1 - sigmoid(input))\nFor multilabel classification we can use vectors for target and input.\nThe proposed BCE:\npos_weight * target * -log(sigmoid(input)) + (1 - target) * -log(1 - sigmoid(input))\npos_weight is multiplied by only the \"positive\" part. It allows us to \"increase\" the fraction of positive examples in imbalanced datasets, so we can trade off precision and recall. I don't know any simple way to do it outside BCE.\nTF already has a similar function.\nFor example. If we have 7 labels and the batch size is equal to 4. Then:\n\ninput \u2014 4\u00d77 \u2014 logits;\ntarget \u2014 4\u00d77 \u2014 ones and zeroes;\npos_weight \u2014 1\u00d77 (or just 7, thanks to broadcasting) \u2014 a vector of positive part weights for every label.", "in_reply_to_id": 183546011}