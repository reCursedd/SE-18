{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186883899", "pull_request_review_id": 118553264, "id": 186883899, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Njg4Mzg5OQ==", "diff_hunk": "@@ -44,29 +43,23 @@ Tensor batch_norm(\n   }\n \n   bool use_cudnn = false;\n-#if AT_CUDNN_ENABLED()\n   use_cudnn = (input.type().is_cuda()\n                && (input.type().scalarType() != at::kHalf\n                  || weight.type().scalarType() == at::kFloat)\n                && weight.defined() && bias.defined()\n                && ((running_mean.defined() && running_var.defined())\n                  || (!running_mean.defined() && !running_var.defined() && training))\n                && input.size(0) <= 131070\n-               && cudnn_enabled && CUDNN_VERSION >= 5110L);\n-#else\n-  // avoid unused parameter warning\n-  (void)use_cudnn;\n-  (void)cudnn_enabled;\n-#endif\n-\n-#if AT_CUDNN_ENABLED()\n-  if (use_cudnn && eps >= CUDNN_BN_MIN_EPSILON) {\n+               && detail::getCUDAHooks().compiledWithCuDNN()\n+               && cudnn_enabled && detail::getCUDAHooks().versionCuDNN() >= 5110L);\n+\n+  if (use_cudnn && eps >= detail::getCUDAHooks().batchnormMinEpsilonCuDNN()) {\n     return std::get<0>(at::cudnn_batch_norm(", "path": "aten/src/ATen/native/Normalization.cpp", "position": 39, "original_position": 39, "commit_id": "eb6abd0bc078c77e3ea4f1e8909d2ed494d365b3", "original_commit_id": "241f08d67e60910713b5c77eb16f6dac726cece2", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Not quite.\r\n\r\nFirst, batchnormMinEpsilonCuDNN is a virtual function call defined in libATen_cpu.so, so we never have to worry about symbol resolution; the symbol is always defined, it's simply a matter of what object/vtable we are actually making the virtual function call on.  So how do we get our hands on the object?  We're using the Caffe2 registry, but what this effectively is, is a global variable which gets modified by the static initializer that runs when we dlopen('libATen_cuda.so')\r\n\r\nOK, but as you say, can we just 'magically change the symbol'?  Here things can get a bit complicated.  This patch makes a simplifying assumption: once you make a call to ANY function that would require CUDA initialization, and it turns out that 'libATen_cuda.so' has NOT been loaded at this point, then we (1) give an error and (2) *permanently disable CUDA for the process.*\r\n\r\nThis is not intrinsic to the design, but it's easy to get the locking correct in this scheme.  One elaboration on this scheme, not for this PR, but Soumith/Yangqing/I agreed we should do, is have lazy CUDA initialization attempt to dlopen('libATen_cuda.so') *itself*, so that if you really do have CUDA it won't really be possible to get this wrong.\r\n", "created_at": "2018-05-08T22:16:14Z", "updated_at": "2018-11-23T15:43:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186883899", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7275", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186883899"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186883899"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7275"}}, "body_html": "<p>Not quite.</p>\n<p>First, batchnormMinEpsilonCuDNN is a virtual function call defined in libATen_cpu.so, so we never have to worry about symbol resolution; the symbol is always defined, it's simply a matter of what object/vtable we are actually making the virtual function call on.  So how do we get our hands on the object?  We're using the Caffe2 registry, but what this effectively is, is a global variable which gets modified by the static initializer that runs when we dlopen('libATen_cuda.so')</p>\n<p>OK, but as you say, can we just 'magically change the symbol'?  Here things can get a bit complicated.  This patch makes a simplifying assumption: once you make a call to ANY function that would require CUDA initialization, and it turns out that 'libATen_cuda.so' has NOT been loaded at this point, then we (1) give an error and (2) <em>permanently disable CUDA for the process.</em></p>\n<p>This is not intrinsic to the design, but it's easy to get the locking correct in this scheme.  One elaboration on this scheme, not for this PR, but Soumith/Yangqing/I agreed we should do, is have lazy CUDA initialization attempt to dlopen('libATen_cuda.so') <em>itself</em>, so that if you really do have CUDA it won't really be possible to get this wrong.</p>", "body_text": "Not quite.\nFirst, batchnormMinEpsilonCuDNN is a virtual function call defined in libATen_cpu.so, so we never have to worry about symbol resolution; the symbol is always defined, it's simply a matter of what object/vtable we are actually making the virtual function call on.  So how do we get our hands on the object?  We're using the Caffe2 registry, but what this effectively is, is a global variable which gets modified by the static initializer that runs when we dlopen('libATen_cuda.so')\nOK, but as you say, can we just 'magically change the symbol'?  Here things can get a bit complicated.  This patch makes a simplifying assumption: once you make a call to ANY function that would require CUDA initialization, and it turns out that 'libATen_cuda.so' has NOT been loaded at this point, then we (1) give an error and (2) permanently disable CUDA for the process.\nThis is not intrinsic to the design, but it's easy to get the locking correct in this scheme.  One elaboration on this scheme, not for this PR, but Soumith/Yangqing/I agreed we should do, is have lazy CUDA initialization attempt to dlopen('libATen_cuda.so') itself, so that if you really do have CUDA it won't really be possible to get this wrong.", "in_reply_to_id": 186787643}