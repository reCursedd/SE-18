{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186883174", "pull_request_review_id": 118552373, "id": 186883174, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Njg4MzE3NA==", "diff_hunk": "@@ -0,0 +1,114 @@\n+#pragma once\n+\n+#include <ATen/Registry.h>\n+#include <ATen/Generator.h>\n+#include <ATen/Error.h>\n+#include <ATen/Allocator.h>\n+\n+// Forward declare these CUDA types here to avoid including CUDA headers in\n+// ATen headers, which would make ATen always require CUDA to build.\n+struct THCState;\n+struct CUstream_st;\n+typedef struct CUstream_st *cudaStream_t;\n+struct cudaDeviceProp;\n+\n+namespace at {\n+  class Context;\n+}\n+\n+// NB: Class must live in at due to limitations of Registry.h\n+namespace at {\n+\n+// The CUDAHooksInterface is an omnibus interface for any CUDA functionality\n+// which we may want to call into from CPU code (and thus must be dynamically\n+// dispatched, to allow for separate compilation of CUDA code).  How do I\n+// decide if a function should live in this class?  There are two tests:\n+//\n+//  1. Does the *implementation* of this function require linking against\n+//     CUDA libraries?\n+//\n+//  2. Is this function *called* from non-CUDA ATen code?", "path": "aten/src/ATen/detail/CUDAHooksInterface.h", "position": 30, "original_position": 30, "commit_id": "eb6abd0bc078c77e3ea4f1e8909d2ed494d365b3", "original_commit_id": "241f08d67e60910713b5c77eb16f6dac726cece2", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Right, so I think eventually we want way less functions in this interface. When I put things in here, I didn't bother \"changing\" any of ATen's API; if there was a \"CPU\" part of ATen which needed some CUDA information, I tossed it in. I think that many of the examples you provide can be eliminated.  I absolutely agree with your point about batchnorm min-epsilon.\r\n\r\nI do want to push on the matter of \"should ATen have a getNumGPUs function\" a little more. I think there's a pretty strong case to be made for putting making an \"ATen CUDA interface\", which you can only link against if you actually link against `libATen_cuda.so`. However, I don't think it is a 100% slam dunk. Let's think about an average Python script which supports both CPU and CUDA execution. The same Python script works no matter if you run it with PyTorch compiled with CPU+CUDA, or PyTorch with CPU only. The reason, of course, is that you don't actually exercise the CUDA functionality unless you *actually call it*, so as long as you don't pass `--cuda` to the trainer script it will happily use the CPU functionality. Now let's consider the corresponding situation in C++ land. If you link against a `libATen_cuda.so` only symbol, you MUST have an install of PyTorch which has `libATen_cuda.so` available (in all its 300M glory) (which in turn must have `libcudart.so`), UNLESS we distribute some sort of `stub/libATen_cuda.so` which defines all of the public symbols of libATen_cuda.so but with trivial implementations of them. On the other hand, if the symbols were defined in `libATen.so` and dynamically dispatched to `libATen_cuda.so`, everything would work perfectly fine. Now, to be clear, I am not sure what the correct decision here is, but I don't think it is trivial.\r\n", "created_at": "2018-05-08T22:12:14Z", "updated_at": "2018-11-23T15:43:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186883174", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7275", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186883174"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186883174"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7275"}}, "body_html": "<p>Right, so I think eventually we want way less functions in this interface. When I put things in here, I didn't bother \"changing\" any of ATen's API; if there was a \"CPU\" part of ATen which needed some CUDA information, I tossed it in. I think that many of the examples you provide can be eliminated.  I absolutely agree with your point about batchnorm min-epsilon.</p>\n<p>I do want to push on the matter of \"should ATen have a getNumGPUs function\" a little more. I think there's a pretty strong case to be made for putting making an \"ATen CUDA interface\", which you can only link against if you actually link against <code>libATen_cuda.so</code>. However, I don't think it is a 100% slam dunk. Let's think about an average Python script which supports both CPU and CUDA execution. The same Python script works no matter if you run it with PyTorch compiled with CPU+CUDA, or PyTorch with CPU only. The reason, of course, is that you don't actually exercise the CUDA functionality unless you <em>actually call it</em>, so as long as you don't pass <code>--cuda</code> to the trainer script it will happily use the CPU functionality. Now let's consider the corresponding situation in C++ land. If you link against a <code>libATen_cuda.so</code> only symbol, you MUST have an install of PyTorch which has <code>libATen_cuda.so</code> available (in all its 300M glory) (which in turn must have <code>libcudart.so</code>), UNLESS we distribute some sort of <code>stub/libATen_cuda.so</code> which defines all of the public symbols of libATen_cuda.so but with trivial implementations of them. On the other hand, if the symbols were defined in <code>libATen.so</code> and dynamically dispatched to <code>libATen_cuda.so</code>, everything would work perfectly fine. Now, to be clear, I am not sure what the correct decision here is, but I don't think it is trivial.</p>", "body_text": "Right, so I think eventually we want way less functions in this interface. When I put things in here, I didn't bother \"changing\" any of ATen's API; if there was a \"CPU\" part of ATen which needed some CUDA information, I tossed it in. I think that many of the examples you provide can be eliminated.  I absolutely agree with your point about batchnorm min-epsilon.\nI do want to push on the matter of \"should ATen have a getNumGPUs function\" a little more. I think there's a pretty strong case to be made for putting making an \"ATen CUDA interface\", which you can only link against if you actually link against libATen_cuda.so. However, I don't think it is a 100% slam dunk. Let's think about an average Python script which supports both CPU and CUDA execution. The same Python script works no matter if you run it with PyTorch compiled with CPU+CUDA, or PyTorch with CPU only. The reason, of course, is that you don't actually exercise the CUDA functionality unless you actually call it, so as long as you don't pass --cuda to the trainer script it will happily use the CPU functionality. Now let's consider the corresponding situation in C++ land. If you link against a libATen_cuda.so only symbol, you MUST have an install of PyTorch which has libATen_cuda.so available (in all its 300M glory) (which in turn must have libcudart.so), UNLESS we distribute some sort of stub/libATen_cuda.so which defines all of the public symbols of libATen_cuda.so but with trivial implementations of them. On the other hand, if the symbols were defined in libATen.so and dynamically dispatched to libATen_cuda.so, everything would work perfectly fine. Now, to be clear, I am not sure what the correct decision here is, but I don't think it is trivial.", "in_reply_to_id": 186785301}