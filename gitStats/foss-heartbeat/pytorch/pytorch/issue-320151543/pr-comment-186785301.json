{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186785301", "pull_request_review_id": 118429901, "id": 186785301, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Njc4NTMwMQ==", "diff_hunk": "@@ -0,0 +1,114 @@\n+#pragma once\n+\n+#include <ATen/Registry.h>\n+#include <ATen/Generator.h>\n+#include <ATen/Error.h>\n+#include <ATen/Allocator.h>\n+\n+// Forward declare these CUDA types here to avoid including CUDA headers in\n+// ATen headers, which would make ATen always require CUDA to build.\n+struct THCState;\n+struct CUstream_st;\n+typedef struct CUstream_st *cudaStream_t;\n+struct cudaDeviceProp;\n+\n+namespace at {\n+  class Context;\n+}\n+\n+// NB: Class must live in at due to limitations of Registry.h\n+namespace at {\n+\n+// The CUDAHooksInterface is an omnibus interface for any CUDA functionality\n+// which we may want to call into from CPU code (and thus must be dynamically\n+// dispatched, to allow for separate compilation of CUDA code).  How do I\n+// decide if a function should live in this class?  There are two tests:\n+//\n+//  1. Does the *implementation* of this function require linking against\n+//     CUDA libraries?\n+//\n+//  2. Is this function *called* from non-CUDA ATen code?", "path": "aten/src/ATen/detail/CUDAHooksInterface.h", "position": 30, "original_position": 30, "commit_id": "eb6abd0bc078c77e3ea4f1e8909d2ed494d365b3", "original_commit_id": "241f08d67e60910713b5c77eb16f6dac726cece2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think those two points are not enough to justify having all the functions you put there. Why would CPU-only code care how many CUDA devices there are? Why would it ever want to get a current CUDA stream?\r\n\r\nAlso, having min eps for batch norm in here is very weird too. We should just have a single entry point for CUDA dispatch in the CPU library, and handle choosing a CUDA impl internally in `ATen_cuda`. Ofc it doesn't have to happen in this PR, and I guess C10 might resolve the issue anyway.", "created_at": "2018-05-08T16:17:44Z", "updated_at": "2018-11-23T15:43:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186785301", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7275", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186785301"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186785301"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7275"}}, "body_html": "<p>I think those two points are not enough to justify having all the functions you put there. Why would CPU-only code care how many CUDA devices there are? Why would it ever want to get a current CUDA stream?</p>\n<p>Also, having min eps for batch norm in here is very weird too. We should just have a single entry point for CUDA dispatch in the CPU library, and handle choosing a CUDA impl internally in <code>ATen_cuda</code>. Ofc it doesn't have to happen in this PR, and I guess C10 might resolve the issue anyway.</p>", "body_text": "I think those two points are not enough to justify having all the functions you put there. Why would CPU-only code care how many CUDA devices there are? Why would it ever want to get a current CUDA stream?\nAlso, having min eps for batch norm in here is very weird too. We should just have a single entry point for CUDA dispatch in the CPU library, and handle choosing a CUDA impl internally in ATen_cuda. Ofc it doesn't have to happen in this PR, and I guess C10 might resolve the issue anyway."}