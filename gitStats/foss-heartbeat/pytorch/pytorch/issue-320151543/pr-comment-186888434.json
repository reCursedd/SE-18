{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186888434", "pull_request_review_id": 118558549, "id": 186888434, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Njg4ODQzNA==", "diff_hunk": "@@ -0,0 +1,114 @@\n+#pragma once\n+\n+#include <ATen/Registry.h>\n+#include <ATen/Generator.h>\n+#include <ATen/Error.h>\n+#include <ATen/Allocator.h>\n+\n+// Forward declare these CUDA types here to avoid including CUDA headers in\n+// ATen headers, which would make ATen always require CUDA to build.\n+struct THCState;\n+struct CUstream_st;\n+typedef struct CUstream_st *cudaStream_t;\n+struct cudaDeviceProp;\n+\n+namespace at {\n+  class Context;\n+}\n+\n+// NB: Class must live in at due to limitations of Registry.h\n+namespace at {\n+\n+// The CUDAHooksInterface is an omnibus interface for any CUDA functionality\n+// which we may want to call into from CPU code (and thus must be dynamically\n+// dispatched, to allow for separate compilation of CUDA code).  How do I\n+// decide if a function should live in this class?  There are two tests:\n+//\n+//  1. Does the *implementation* of this function require linking against\n+//     CUDA libraries?\n+//\n+//  2. Is this function *called* from non-CUDA ATen code?", "path": "aten/src/ATen/detail/CUDAHooksInterface.h", "position": 30, "original_position": 30, "commit_id": "eb6abd0bc078c77e3ea4f1e8909d2ed494d365b3", "original_commit_id": "241f08d67e60910713b5c77eb16f6dac726cece2", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "(Blah, actually, batchnorm min-epsilon is not so easy to expunge: the problem is that the current dispatch logic doesn't know how dispatch first to CUDA, and *then* to the autograd-enabled sub-implementation. If I give anything `dispatch: CUDA` then I immediately lose pass through and have to implement the backwards manually. Don't want to do that.)", "created_at": "2018-05-08T22:40:25Z", "updated_at": "2018-11-23T15:43:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186888434", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7275", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186888434"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7275#discussion_r186888434"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7275"}}, "body_html": "<p>(Blah, actually, batchnorm min-epsilon is not so easy to expunge: the problem is that the current dispatch logic doesn't know how dispatch first to CUDA, and <em>then</em> to the autograd-enabled sub-implementation. If I give anything <code>dispatch: CUDA</code> then I immediately lose pass through and have to implement the backwards manually. Don't want to do that.)</p>", "body_text": "(Blah, actually, batchnorm min-epsilon is not so easy to expunge: the problem is that the current dispatch logic doesn't know how dispatch first to CUDA, and then to the autograd-enabled sub-implementation. If I give anything dispatch: CUDA then I immediately lose pass through and have to implement the backwards manually. Don't want to do that.)", "in_reply_to_id": 186785301}