{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7275", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7275/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7275/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7275/events", "html_url": "https://github.com/pytorch/pytorch/pull/7275", "id": 320151543, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg1ODkwOTUy", "number": 7275, "title": "Split libATen.so into libATen_cpu.so and libATen_cuda.so ", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-05-04T04:33:49Z", "updated_at": "2018-11-23T15:43:55Z", "closed_at": "2018-05-10T17:28:34Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7275", "html_url": "https://github.com/pytorch/pytorch/pull/7275", "diff_url": "https://github.com/pytorch/pytorch/pull/7275.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7275.patch"}, "body_html": "<p>Stacked on top of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"319675901\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7197\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7197/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7197\">#7197</a></p>\n<p>Previously, ATen could be built with either CPU-only support, or<br>\nCPU/CUDA support, but only via a compile-time flag, requiring<br>\ntwo separate builds.  This means that if you have a program which<br>\nindirectly uses a CPU-only build of ATen, and a CPU/CUDA-build of<br>\nATen, you're gonna have a bad time.  And you might want a CPU-only<br>\nbuild of ATen, because it is 15M (versus the 300M of a CUDA build).</p>\n<p>This commit splits libATen.so into two libraries, CPU/CUDA, so<br>\nthat it's not necessary to do a full rebuild to get CPU-only<br>\nsupport; instead, if you link against libATen_cpu.so only, you<br>\nare CPU-only; if you additionally link/dlopen libATen_cuda.so,<br>\nthis enables CUDA support.  This brings ATen's dynamic library<br>\nstructure more similar to Caffe2's.</p>\n<p>The general principle for how this works is that we introduce<br>\na <em>hooks</em> interface, which introduces a dynamic dispatch indirection<br>\nbetween a call site and implementation site of CUDA functionality,<br>\nmediated by a static initialization registry.  This means that we can continue<br>\nto, for example, lazily initialize CUDA from Context (a core, CPU class) without<br>\nhaving a direct dependency on the CUDA bits.  Instead, we look up<br>\nin the registry if, e.g., CUDA hooks have been loaded (this loading<br>\nprocess happens at static initialization time), and if they<br>\nhave been we dynamic dispatch to this class.  We similarly use<br>\nthe hooks interface to handle Variable registration.</p>\n<p>We introduce a new invariant: if the backend of a type has not<br>\nbeen initialized (e.g., it's library has not been dlopened; for<br>\nCUDA, this also includes CUDA initialization), then the Type<br>\npointers in the context registry are NULL.  If you access the<br>\nregistry directly you must maintain this invariant.</p>\n<p>We also preserve libATen.so, which is an empty stub library that<br>\ndepends on libATen_cpu.so and libATen_cuda.so, depending on<br>\nthe setting of USE_CUDA.</p>\n<p>There are a few potholes along the way.  I document them here:</p>\n<ul>\n<li>\n<p>Previously, PyTorch maintained a separate registry for variable<br>\ntypes, because no provision for them was made in the Context's<br>\ntype_registry.  Now that we have the hooks mechanism, we can easily<br>\nhave PyTorch register variables in the main registry.  The code<br>\nhas been refactored accordingly.</p>\n</li>\n<li>\n<p>There is a subtle ordering issue between Variable and CUDA.<br>\nWe permit libATen_cuda.so and PyTorch to be loaded in either<br>\norder (in practice, CUDA is always loaded \"after\" PyTorch, because<br>\nit is lazily initialized.)  This means that, when CUDA types are<br>\nloaded, we must subsequently also initialize their Variable equivalents.<br>\nAppropriate hooks were added to VariableHooks to make this possible;<br>\nsimilarly, getVariableHooks() is not referentially transparent, and<br>\nwill change behavior after Variables are loaded.  (This is different<br>\nto CUDAHooks, which is \"burned in\" after you try to initialize CUDA.)</p>\n</li>\n<li>\n<p>The cmake is adjusted to separate dependencies into either CPU<br>\nor CUDA dependencies.  The generator scripts are adjusted to either<br>\ngenerate a file as a CUDA (cuda_file_manager) or CPU file (file_manager).</p>\n</li>\n<li>\n<p>I changed all native functions which were CUDA-only (the cudnn functions)<br>\nto have dispatches for CUDA only (making it permissible to not specify<br>\nall dispatch options.)  This uncovered a bug in how we were handling<br>\nnative functions which dispatch on a Type argument; I introduced a new<br>\nself_ty keyword to handle this case.  I'm not 100% happy about it<br>\nbut it fixed my problem.</p>\n<p>This also exposed the fact that set_history incompletely handles<br>\nheterogenous return tuples combining Tensor and TensorList.  I<br>\nswapped this codegen to use flatten() (at the possible cost of<br>\na slight perf regression, since we're allocating another vector now<br>\nin this code path).</p>\n</li>\n<li>\n<p>thc_state is no longer a public member of Context; use getTHCState() instead</p>\n</li>\n<li>\n<p>This PR comes with Registry from Caffe2, for handling static initialization.</p>\n</li>\n</ul>", "body_text": "Stacked on top of #7197\nPreviously, ATen could be built with either CPU-only support, or\nCPU/CUDA support, but only via a compile-time flag, requiring\ntwo separate builds.  This means that if you have a program which\nindirectly uses a CPU-only build of ATen, and a CPU/CUDA-build of\nATen, you're gonna have a bad time.  And you might want a CPU-only\nbuild of ATen, because it is 15M (versus the 300M of a CUDA build).\nThis commit splits libATen.so into two libraries, CPU/CUDA, so\nthat it's not necessary to do a full rebuild to get CPU-only\nsupport; instead, if you link against libATen_cpu.so only, you\nare CPU-only; if you additionally link/dlopen libATen_cuda.so,\nthis enables CUDA support.  This brings ATen's dynamic library\nstructure more similar to Caffe2's.\nThe general principle for how this works is that we introduce\na hooks interface, which introduces a dynamic dispatch indirection\nbetween a call site and implementation site of CUDA functionality,\nmediated by a static initialization registry.  This means that we can continue\nto, for example, lazily initialize CUDA from Context (a core, CPU class) without\nhaving a direct dependency on the CUDA bits.  Instead, we look up\nin the registry if, e.g., CUDA hooks have been loaded (this loading\nprocess happens at static initialization time), and if they\nhave been we dynamic dispatch to this class.  We similarly use\nthe hooks interface to handle Variable registration.\nWe introduce a new invariant: if the backend of a type has not\nbeen initialized (e.g., it's library has not been dlopened; for\nCUDA, this also includes CUDA initialization), then the Type\npointers in the context registry are NULL.  If you access the\nregistry directly you must maintain this invariant.\nWe also preserve libATen.so, which is an empty stub library that\ndepends on libATen_cpu.so and libATen_cuda.so, depending on\nthe setting of USE_CUDA.\nThere are a few potholes along the way.  I document them here:\n\n\nPreviously, PyTorch maintained a separate registry for variable\ntypes, because no provision for them was made in the Context's\ntype_registry.  Now that we have the hooks mechanism, we can easily\nhave PyTorch register variables in the main registry.  The code\nhas been refactored accordingly.\n\n\nThere is a subtle ordering issue between Variable and CUDA.\nWe permit libATen_cuda.so and PyTorch to be loaded in either\norder (in practice, CUDA is always loaded \"after\" PyTorch, because\nit is lazily initialized.)  This means that, when CUDA types are\nloaded, we must subsequently also initialize their Variable equivalents.\nAppropriate hooks were added to VariableHooks to make this possible;\nsimilarly, getVariableHooks() is not referentially transparent, and\nwill change behavior after Variables are loaded.  (This is different\nto CUDAHooks, which is \"burned in\" after you try to initialize CUDA.)\n\n\nThe cmake is adjusted to separate dependencies into either CPU\nor CUDA dependencies.  The generator scripts are adjusted to either\ngenerate a file as a CUDA (cuda_file_manager) or CPU file (file_manager).\n\n\nI changed all native functions which were CUDA-only (the cudnn functions)\nto have dispatches for CUDA only (making it permissible to not specify\nall dispatch options.)  This uncovered a bug in how we were handling\nnative functions which dispatch on a Type argument; I introduced a new\nself_ty keyword to handle this case.  I'm not 100% happy about it\nbut it fixed my problem.\nThis also exposed the fact that set_history incompletely handles\nheterogenous return tuples combining Tensor and TensorList.  I\nswapped this codegen to use flatten() (at the possible cost of\na slight perf regression, since we're allocating another vector now\nin this code path).\n\n\nthc_state is no longer a public member of Context; use getTHCState() instead\n\n\nThis PR comes with Registry from Caffe2, for handling static initialization.", "body": "Stacked on top of https://github.com/pytorch/pytorch/pull/7197\r\n\r\nPreviously, ATen could be built with either CPU-only support, or\r\nCPU/CUDA support, but only via a compile-time flag, requiring\r\ntwo separate builds.  This means that if you have a program which\r\nindirectly uses a CPU-only build of ATen, and a CPU/CUDA-build of\r\nATen, you're gonna have a bad time.  And you might want a CPU-only\r\nbuild of ATen, because it is 15M (versus the 300M of a CUDA build).\r\n\r\nThis commit splits libATen.so into two libraries, CPU/CUDA, so\r\nthat it's not necessary to do a full rebuild to get CPU-only\r\nsupport; instead, if you link against libATen_cpu.so only, you\r\nare CPU-only; if you additionally link/dlopen libATen_cuda.so,\r\nthis enables CUDA support.  This brings ATen's dynamic library\r\nstructure more similar to Caffe2's.\r\n\r\nThe general principle for how this works is that we introduce\r\na *hooks* interface, which introduces a dynamic dispatch indirection\r\nbetween a call site and implementation site of CUDA functionality,\r\nmediated by a static initialization registry.  This means that we can continue\r\nto, for example, lazily initialize CUDA from Context (a core, CPU class) without\r\nhaving a direct dependency on the CUDA bits.  Instead, we look up\r\nin the registry if, e.g., CUDA hooks have been loaded (this loading\r\nprocess happens at static initialization time), and if they\r\nhave been we dynamic dispatch to this class.  We similarly use\r\nthe hooks interface to handle Variable registration.\r\n\r\nWe introduce a new invariant: if the backend of a type has not\r\nbeen initialized (e.g., it's library has not been dlopened; for\r\nCUDA, this also includes CUDA initialization), then the Type\r\npointers in the context registry are NULL.  If you access the\r\nregistry directly you must maintain this invariant.\r\n\r\nWe also preserve libATen.so, which is an empty stub library that\r\ndepends on libATen_cpu.so and libATen_cuda.so, depending on\r\nthe setting of USE_CUDA.\r\n\r\nThere are a few potholes along the way.  I document them here:\r\n\r\n- Previously, PyTorch maintained a separate registry for variable\r\n  types, because no provision for them was made in the Context's\r\n  type_registry.  Now that we have the hooks mechanism, we can easily\r\n  have PyTorch register variables in the main registry.  The code\r\n  has been refactored accordingly.\r\n\r\n- There is a subtle ordering issue between Variable and CUDA.\r\n  We permit libATen_cuda.so and PyTorch to be loaded in either\r\n  order (in practice, CUDA is always loaded \"after\" PyTorch, because\r\n  it is lazily initialized.)  This means that, when CUDA types are\r\n  loaded, we must subsequently also initialize their Variable equivalents.\r\n  Appropriate hooks were added to VariableHooks to make this possible;\r\n  similarly, getVariableHooks() is not referentially transparent, and\r\n  will change behavior after Variables are loaded.  (This is different\r\n  to CUDAHooks, which is \"burned in\" after you try to initialize CUDA.)\r\n\r\n- The cmake is adjusted to separate dependencies into either CPU\r\n  or CUDA dependencies.  The generator scripts are adjusted to either\r\n  generate a file as a CUDA (cuda_file_manager) or CPU file (file_manager).\r\n\r\n- I changed all native functions which were CUDA-only (the cudnn functions)\r\n  to have dispatches for CUDA only (making it permissible to not specify\r\n  all dispatch options.)  This uncovered a bug in how we were handling\r\n  native functions which dispatch on a Type argument; I introduced a new\r\n  self_ty keyword to handle this case.  I'm not 100% happy about it\r\n  but it fixed my problem.\r\n\r\n  This also exposed the fact that set_history incompletely handles\r\n  heterogenous return tuples combining Tensor and TensorList.  I\r\n  swapped this codegen to use flatten() (at the possible cost of\r\n  a slight perf regression, since we're allocating another vector now\r\n  in this code path).\r\n\r\n- thc_state is no longer a public member of Context; use getTHCState() instead\r\n\r\n- This PR comes with Registry from Caffe2, for handling static initialization.\r\n"}