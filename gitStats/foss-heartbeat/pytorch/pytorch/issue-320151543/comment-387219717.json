{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387219717", "html_url": "https://github.com/pytorch/pytorch/pull/7275#issuecomment-387219717", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7275", "id": 387219717, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzIxOTcxNw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-07T21:57:46Z", "updated_at": "2018-05-08T22:55:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is English updates on all of the bugfixes on this PR:</p>\n<ul>\n<li>We have a general problem which is that on recent Ubuntu distributions, <code>--as-needed</code> is enabled for shared libraries, which is (cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> who was worrying about this in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"319355262\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7160\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7160/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7160\">#7160</a> see also <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"319355262\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7160\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7160/hovercard?comment_id=387216823&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/pull/7160#issuecomment-387216823\">#7160 (comment)</a>). For now, I've hacked this up in the PR to pass <code>-Wl,--no-as-needed</code> to all of the spots necessary to make CI work, but a more sustainable solution is to attempt to <code>dlopen</code> <code>libATen_cuda.so</code> when CUDA functionality is requested.\n<ul>\n<li>(UNDER TESTING) The JIT tests somehow manage to try to touch CUDA without loading <code>libATen_cuda.so</code>. I'm testing a fix which is passing <code>-Wl,--no-as-needed</code> when linking <code>libATen_cuda.so</code> to <code>_C.so</code></li>\n</ul>\n</li>\n<li>I needed to make a bunch of fixes to <code>Registry</code> to make it more portable\n<ul>\n<li>No more <code>##__VA_ARGS__</code> token pasting; instead, it is mandatory to pass at least one argument to the var-args.  CUDAHooks and VariableHooks pass a nullary struct CUDAHooksArgs/VariableHooksArgs to solve the problem. We must get rid of token pasting because it does not work with MSVC.</li>\n<li>It seems MSVC is not willing to generate code for constructors of template classes at use sites which cross DLL boundaries. So we explicitly instantiate the class to get around the problem. This involved tweaks to the boilerplate generating macros, and also required us to shuffle around namespaces a bit, because you can't specialize a template unless you are in the same namespace as the template.</li>\n<li>Insertion of <code>AT_API</code> to appropriate places where the registry must be exported</li>\n</ul>\n</li>\n<li>There is a very subtle linking issue with lapack, which is solved by making sure <code>libATen_cuda.so</code> links against LAPACK. There's a comment in <code>aten/src/ATen/CMakeLists.txt</code> about htis as well as a follow up bug at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"320953850\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7353\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7353/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7353\">#7353</a></li>\n<li>There is no more <code>libATen.so</code> compatibility library. You must use <code>libATen_cpu.so</code> or <code>libATen_cuda.so</code>. A lot of adjustments to CPP extensions, libtorch, etc. were made to make this go through correctly. This will need fbcode fixes as well.</li>\n<li>autogradpp used <code>AT_CUDA_ENABLED</code> directly. We've expunged these uses and added a few more things to <code>CUDAHooks</code> (<code>getNumGPUs)</code></li>\n<li>Added <code>manualSeedAll</code> to <code>Generator</code> so that we can invoke it polymorphically (it only does something different for CUDAGenerator)</li>\n<li>There's a new <code>cuda/CUDAConfig.h</code> header for CUDA-only ifdef macros (<code>AT_CUDNN_ENABLED</code>, most prominently)</li>\n<li>CUDAHooks/VariableHooks structs live in <code>at</code> namespace because Registry's namespace support is not good enough to handle it otherwise (see Registry changes above)</li>\n<li>There's some modest moving around of native functions in ReduceOps and UnaryOps to get the CUDA-only function implementations into separate files, so they are only compiled into <code>libATen_cuda.so</code>. <code>sspaddmm</code> needed a separate CUDA function due to object linkage boundaries.</li>\n<li>Some direct uses of native functions in CUDA code has to go away, since these functions are not exported, so you have to go through the dispatcher (<code>at::native::empty_like</code> to <code>at::empty_like</code>)</li>\n<li>Code in THC/THCS/THCUNN now properly use <code>THC_API</code> macro instead of <code>TH_API</code> (which matters now that TH and THC are not in the same library)\n<ul>\n<li>Some code debt in <code>torch/_thnn/utils.py</code> and other THNN parsing code to handle both <code>TH_API</code> and <code>THC_API</code></li>\n</ul>\n</li>\n<li><code>TensorUtils.h</code> is now properly exported with <code>AT_API</code></li>\n<li>Dead uses of <code>TH_EXPORTS</code> and co expunged; we now use <code>ATen_cpu_exports</code> and <code>ATen_cuda_exports</code> (new, in <code>ATenCUDAGeneral.h</code>) consistently</li>\n<li>Fix some incorrect type annotations on <code>_cudnn_rnn_backward</code>, where we didn't declare a type as possibly undefined when we should have. We didn't catch this previously because optional annotations are not tested on \"pass-through\" native ATen ops (which don't have <code>dispatch</code>). Upstream issue at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"320475543\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7316\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7316/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7316\">#7316</a></li>\n<li>There's a new cmake macro <code>aten_compile_options</code> for applying all of our per-target compile time options. We use this on the cpu and cuda libraries.</li>\n<li><code>test/test_cpp_extensions.py</code> can be run directly by invoking in Python, assuming you've setup your <code>PYTHONPATH</code> setup correctly</li>\n<li><code>type_from_string</code> does some new funny business to only query for all valid CUDA types (which causes CUDA initialization) when we see \"torch.cuda.\" in the requested string</li>\n</ul>", "body_text": "Here is English updates on all of the bugfixes on this PR:\n\nWe have a general problem which is that on recent Ubuntu distributions, --as-needed is enabled for shared libraries, which is (cc @apaszke who was worrying about this in #7160 see also #7160 (comment)). For now, I've hacked this up in the PR to pass -Wl,--no-as-needed to all of the spots necessary to make CI work, but a more sustainable solution is to attempt to dlopen libATen_cuda.so when CUDA functionality is requested.\n\n(UNDER TESTING) The JIT tests somehow manage to try to touch CUDA without loading libATen_cuda.so. I'm testing a fix which is passing -Wl,--no-as-needed when linking libATen_cuda.so to _C.so\n\n\nI needed to make a bunch of fixes to Registry to make it more portable\n\nNo more ##__VA_ARGS__ token pasting; instead, it is mandatory to pass at least one argument to the var-args.  CUDAHooks and VariableHooks pass a nullary struct CUDAHooksArgs/VariableHooksArgs to solve the problem. We must get rid of token pasting because it does not work with MSVC.\nIt seems MSVC is not willing to generate code for constructors of template classes at use sites which cross DLL boundaries. So we explicitly instantiate the class to get around the problem. This involved tweaks to the boilerplate generating macros, and also required us to shuffle around namespaces a bit, because you can't specialize a template unless you are in the same namespace as the template.\nInsertion of AT_API to appropriate places where the registry must be exported\n\n\nThere is a very subtle linking issue with lapack, which is solved by making sure libATen_cuda.so links against LAPACK. There's a comment in aten/src/ATen/CMakeLists.txt about htis as well as a follow up bug at #7353\nThere is no more libATen.so compatibility library. You must use libATen_cpu.so or libATen_cuda.so. A lot of adjustments to CPP extensions, libtorch, etc. were made to make this go through correctly. This will need fbcode fixes as well.\nautogradpp used AT_CUDA_ENABLED directly. We've expunged these uses and added a few more things to CUDAHooks (getNumGPUs)\nAdded manualSeedAll to Generator so that we can invoke it polymorphically (it only does something different for CUDAGenerator)\nThere's a new cuda/CUDAConfig.h header for CUDA-only ifdef macros (AT_CUDNN_ENABLED, most prominently)\nCUDAHooks/VariableHooks structs live in at namespace because Registry's namespace support is not good enough to handle it otherwise (see Registry changes above)\nThere's some modest moving around of native functions in ReduceOps and UnaryOps to get the CUDA-only function implementations into separate files, so they are only compiled into libATen_cuda.so. sspaddmm needed a separate CUDA function due to object linkage boundaries.\nSome direct uses of native functions in CUDA code has to go away, since these functions are not exported, so you have to go through the dispatcher (at::native::empty_like to at::empty_like)\nCode in THC/THCS/THCUNN now properly use THC_API macro instead of TH_API (which matters now that TH and THC are not in the same library)\n\nSome code debt in torch/_thnn/utils.py and other THNN parsing code to handle both TH_API and THC_API\n\n\nTensorUtils.h is now properly exported with AT_API\nDead uses of TH_EXPORTS and co expunged; we now use ATen_cpu_exports and ATen_cuda_exports (new, in ATenCUDAGeneral.h) consistently\nFix some incorrect type annotations on _cudnn_rnn_backward, where we didn't declare a type as possibly undefined when we should have. We didn't catch this previously because optional annotations are not tested on \"pass-through\" native ATen ops (which don't have dispatch). Upstream issue at #7316\nThere's a new cmake macro aten_compile_options for applying all of our per-target compile time options. We use this on the cpu and cuda libraries.\ntest/test_cpp_extensions.py can be run directly by invoking in Python, assuming you've setup your PYTHONPATH setup correctly\ntype_from_string does some new funny business to only query for all valid CUDA types (which causes CUDA initialization) when we see \"torch.cuda.\" in the requested string", "body": "Here is English updates on all of the bugfixes on this PR:\r\n\r\n- We have a general problem which is that on recent Ubuntu distributions, `--as-needed` is enabled for shared libraries, which is (cc @apaszke who was worrying about this in https://github.com/pytorch/pytorch/pull/7160 see also https://github.com/pytorch/pytorch/pull/7160#issuecomment-387216823). For now, I've hacked this up in the PR to pass `-Wl,--no-as-needed` to all of the spots necessary to make CI work, but a more sustainable solution is to attempt to `dlopen` `libATen_cuda.so` when CUDA functionality is requested.\r\n  - (UNDER TESTING) The JIT tests somehow manage to try to touch CUDA without loading `libATen_cuda.so`. I'm testing a fix which is passing `-Wl,--no-as-needed` when linking `libATen_cuda.so` to `_C.so`\r\n- I needed to make a bunch of fixes to `Registry` to make it more portable\r\n  - No more `##__VA_ARGS__` token pasting; instead, it is mandatory to pass at least one argument to the var-args.  CUDAHooks and VariableHooks pass a nullary struct CUDAHooksArgs/VariableHooksArgs to solve the problem. We must get rid of token pasting because it does not work with MSVC.\r\n  - It seems MSVC is not willing to generate code for constructors of template classes at use sites which cross DLL boundaries. So we explicitly instantiate the class to get around the problem. This involved tweaks to the boilerplate generating macros, and also required us to shuffle around namespaces a bit, because you can't specialize a template unless you are in the same namespace as the template.\r\n  - Insertion of `AT_API` to appropriate places where the registry must be exported\r\n- There is a very subtle linking issue with lapack, which is solved by making sure `libATen_cuda.so` links against LAPACK. There's a comment in `aten/src/ATen/CMakeLists.txt` about htis as well as a follow up bug at https://github.com/pytorch/pytorch/issues/7353\r\n- There is no more `libATen.so` compatibility library. You must use `libATen_cpu.so` or `libATen_cuda.so`. A lot of adjustments to CPP extensions, libtorch, etc. were made to make this go through correctly. This will need fbcode fixes as well.\r\n- autogradpp used `AT_CUDA_ENABLED` directly. We've expunged these uses and added a few more things to `CUDAHooks` (`getNumGPUs)`\r\n- Added `manualSeedAll` to `Generator` so that we can invoke it polymorphically (it only does something different for CUDAGenerator)\r\n- There's a new `cuda/CUDAConfig.h` header for CUDA-only ifdef macros (`AT_CUDNN_ENABLED`, most prominently)\r\n- CUDAHooks/VariableHooks structs live in `at` namespace because Registry's namespace support is not good enough to handle it otherwise (see Registry changes above)\r\n- There's some modest moving around of native functions in ReduceOps and UnaryOps to get the CUDA-only function implementations into separate files, so they are only compiled into `libATen_cuda.so`. `sspaddmm` needed a separate CUDA function due to object linkage boundaries.\r\n- Some direct uses of native functions in CUDA code has to go away, since these functions are not exported, so you have to go through the dispatcher (`at::native::empty_like` to `at::empty_like`)\r\n- Code in THC/THCS/THCUNN now properly use `THC_API` macro instead of `TH_API` (which matters now that TH and THC are not in the same library)\r\n  - Some code debt in `torch/_thnn/utils.py` and other THNN parsing code to handle both `TH_API` and `THC_API`\r\n- `TensorUtils.h` is now properly exported with `AT_API`\r\n- Dead uses of `TH_EXPORTS` and co expunged; we now use `ATen_cpu_exports` and `ATen_cuda_exports` (new, in `ATenCUDAGeneral.h`) consistently\r\n- Fix some incorrect type annotations on `_cudnn_rnn_backward`, where we didn't declare a type as possibly undefined when we should have. We didn't catch this previously because optional annotations are not tested on \"pass-through\" native ATen ops (which don't have `dispatch`). Upstream issue at https://github.com/pytorch/pytorch/issues/7316\r\n- There's a new cmake macro `aten_compile_options` for applying all of our per-target compile time options. We use this on the cpu and cuda libraries.\r\n- `test/test_cpp_extensions.py` can be run directly by invoking in Python, assuming you've setup your `PYTHONPATH` setup correctly\r\n- `type_from_string` does some new funny business to only query for all valid CUDA types (which causes CUDA initialization) when we see \"torch.cuda.\" in the requested string"}