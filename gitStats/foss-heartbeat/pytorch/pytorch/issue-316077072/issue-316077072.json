{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6786", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6786/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6786/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6786/events", "html_url": "https://github.com/pytorch/pytorch/pull/6786", "id": 316077072, "node_id": "MDExOlB1bGxSZXF1ZXN0MTgyOTI2MTkx", "number": 6786, "title": "move softmax/logsoftmax to ATen", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2018-04-19T23:40:20Z", "updated_at": "2018-11-23T15:43:27Z", "closed_at": "2018-05-04T18:23:36Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6786", "html_url": "https://github.com/pytorch/pytorch/pull/6786", "diff_url": "https://github.com/pytorch/pytorch/pull/6786.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6786.patch"}, "body_html": "<p>THCUNN kernels are mostly unchanged, with minimum changes to types so that more intermediate values are preserved in AccumT.<br>\nSoftmax/LogSoftmax from THNN are combined into a single templated function.<br>\nRemaining issues</p>\n<ul>\n<li>in THNN accumulation for float used to be in double, ATen does not support this now, would need something like <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh</a>, should we add it? With this PR, accumulation for float is in float.</li>\n<li>in softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward. I don't know if there's a better way of handling it.</li>\n<li>scalar handling is somewhat awkward (I'm checking .dim() == 0, and do .view(1) for scalars), is there a better way of doing it?</li>\n<li>legacy tests are failing, because softmax is no longer legacy. I can rewrite legacy softmax forward to use torch.softmax instead of [backend].SoftMax_UpdateOutput, but what to do with backward? Softmax backward is no longer exposed.</li>\n</ul>", "body_text": "THCUNN kernels are mostly unchanged, with minimum changes to types so that more intermediate values are preserved in AccumT.\nSoftmax/LogSoftmax from THNN are combined into a single templated function.\nRemaining issues\n\nin THNN accumulation for float used to be in double, ATen does not support this now, would need something like https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh, should we add it? With this PR, accumulation for float is in float.\nin softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward. I don't know if there's a better way of handling it.\nscalar handling is somewhat awkward (I'm checking .dim() == 0, and do .view(1) for scalars), is there a better way of doing it?\nlegacy tests are failing, because softmax is no longer legacy. I can rewrite legacy softmax forward to use torch.softmax instead of [backend].SoftMax_UpdateOutput, but what to do with backward? Softmax backward is no longer exposed.", "body": "THCUNN kernels are mostly unchanged, with minimum changes to types so that more intermediate values are preserved in AccumT. \r\nSoftmax/LogSoftmax from THNN are combined into a single templated function.\r\nRemaining issues\r\n - in THNN accumulation for float used to be in double, ATen does not support this now, would need something like https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh, should we add it? With this PR, accumulation for float is in float. \r\n- in softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward. I don't know if there's a better way of handling it. \r\n- scalar handling is somewhat awkward (I'm checking .dim() == 0, and do .view(1) for scalars), is there a better way of doing it?\r\n- legacy tests are failing, because softmax is no longer legacy. I can rewrite legacy softmax forward to use torch.softmax instead of [backend].SoftMax_UpdateOutput, but what to do with backward? Softmax backward is no longer exposed."}