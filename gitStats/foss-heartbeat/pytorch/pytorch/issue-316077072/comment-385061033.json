{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385061033", "html_url": "https://github.com/pytorch/pytorch/pull/6786#issuecomment-385061033", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6786", "id": 385061033, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTA2MTAzMw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-27T18:49:10Z", "updated_at": "2018-04-27T18:49:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Rebased on master, fixed legacy by calling <code>*backward</code>  <code>*backward_data</code> (native functions ending in <code>backward</code> are blacklisted from being exposed in python). Don't particularly like this solution, but can't find any other way.<br>\nMoved AccumulateType to be cpu/gpu accumulate type, with accum types set to what they are now in TH/THC. For cpu float softmax, behavior now replicates THNN, accumulation is in double. Had to change includes in a few  existing files to <code>ATen/AccumulateType.h</code> instead of <code>ATen/cuda/AccumulateType.cuh</code>, and fix the source.</p>", "body_text": "Rebased on master, fixed legacy by calling *backward  *backward_data (native functions ending in backward are blacklisted from being exposed in python). Don't particularly like this solution, but can't find any other way.\nMoved AccumulateType to be cpu/gpu accumulate type, with accum types set to what they are now in TH/THC. For cpu float softmax, behavior now replicates THNN, accumulation is in double. Had to change includes in a few  existing files to ATen/AccumulateType.h instead of ATen/cuda/AccumulateType.cuh, and fix the source.", "body": "Rebased on master, fixed legacy by calling `*backward`  `*backward_data` (native functions ending in `backward` are blacklisted from being exposed in python). Don't particularly like this solution, but can't find any other way. \r\nMoved AccumulateType to be cpu/gpu accumulate type, with accum types set to what they are now in TH/THC. For cpu float softmax, behavior now replicates THNN, accumulation is in double. Had to change includes in a few  existing files to `ATen/AccumulateType.h` instead of `ATen/cuda/AccumulateType.cuh`, and fix the source. "}