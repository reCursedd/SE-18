{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383401089", "html_url": "https://github.com/pytorch/pytorch/pull/6786#issuecomment-383401089", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6786", "id": 383401089, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzQwMTA4OQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-22T18:13:48Z", "updated_at": "2018-04-22T18:13:48Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>in THNN accumulation for float used to be in double, ATen does not support this now, would need something like <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh</a>, should we add it? With this PR, accumulation for float is in float.</p>\n</blockquote>\n<p>Yes, that would be great. I'd probably make another version of this template for \"AccAccumulateType\" or similar (following the TH convention)</p>\n<blockquote>\n<p>in softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward.</p>\n</blockquote>\n<p>I'm a little perplexed by the double-backwards situation; if <code>_backward</code> has no data dependence on <code>self</code>, then there shouldn't be any gradient formula for the double backwards, right?</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> about scalars</p>", "body_text": "in THNN accumulation for float used to be in double, ATen does not support this now, would need something like https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh, should we add it? With this PR, accumulation for float is in float.\n\nYes, that would be great. I'd probably make another version of this template for \"AccAccumulateType\" or similar (following the TH convention)\n\nin softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward.\n\nI'm a little perplexed by the double-backwards situation; if _backward has no data dependence on self, then there shouldn't be any gradient formula for the double backwards, right?\nCC @gchanan about scalars", "body": "> in THNN accumulation for float used to be in double, ATen does not support this now, would need something like https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh, should we add it? With this PR, accumulation for float is in float.\r\n\r\nYes, that would be great. I'd probably make another version of this template for \"AccAccumulateType\" or similar (following the TH convention)\r\n\r\n> in softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward.\r\n\r\nI'm a little perplexed by the double-backwards situation; if `_backward` has no data dependence on `self`, then there shouldn't be any gradient formula for the double backwards, right?\r\n\r\nCC @gchanan about scalars"}