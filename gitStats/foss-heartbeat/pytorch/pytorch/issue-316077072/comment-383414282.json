{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383414282", "html_url": "https://github.com/pytorch/pytorch/pull/6786#issuecomment-383414282", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6786", "id": 383414282, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzQxNDI4Mg==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-22T21:36:40Z", "updated_at": "2018-04-22T21:36:40Z", "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li>For legacy, I'd need to figure out how to expose softmax_backward to python. Any advice?</li>\n<li>I'll add cpu accumulate type template that follows what TH/THNN has now, and use it on the cpu paths (as a side note, I see cpu reductions now being moved from TH to ATen, they should face the same issue).</li>\n<li>It has an implicit dependence on <code>self</code> via <code>output/result</code> that is actually used in the gradient formula (a side note: in derivatives.yaml, for the first derivative softmax_backward has to be declared with <code>result</code> argument, not <code>output</code>, otherwise generator does not properly insert unpacking code. For the second derivative, <code>softmax_backward</code> has to be declared with <code>output</code> argument, otherwise compiler complains about something, don't remember what).<br>\nSo in this piece of code</li>\n</ol>\n<pre><code>name: softmax_backward(Tensor grad_output, Tensor output, int64_t dim, Tensor self)\ngrad_output: softmax_backward(grad, output, dim, self)\nself: softmax_double_backward(grad, grad_output, dim, output)\n</code></pre>\n<p><code>self</code> in the first line is just so that there can be <code>self</code> in the third line. Does it make sense?</p>", "body_text": "For legacy, I'd need to figure out how to expose softmax_backward to python. Any advice?\nI'll add cpu accumulate type template that follows what TH/THNN has now, and use it on the cpu paths (as a side note, I see cpu reductions now being moved from TH to ATen, they should face the same issue).\nIt has an implicit dependence on self via output/result that is actually used in the gradient formula (a side note: in derivatives.yaml, for the first derivative softmax_backward has to be declared with result argument, not output, otherwise generator does not properly insert unpacking code. For the second derivative, softmax_backward has to be declared with output argument, otherwise compiler complains about something, don't remember what).\nSo in this piece of code\n\nname: softmax_backward(Tensor grad_output, Tensor output, int64_t dim, Tensor self)\ngrad_output: softmax_backward(grad, output, dim, self)\nself: softmax_double_backward(grad, grad_output, dim, output)\n\nself in the first line is just so that there can be self in the third line. Does it make sense?", "body": "1) For legacy, I'd need to figure out how to expose softmax_backward to python. Any advice?\r\n2) I'll add cpu accumulate type template that follows what TH/THNN has now, and use it on the cpu paths (as a side note, I see cpu reductions now being moved from TH to ATen, they should face the same issue).\r\n3) It has an implicit dependence on `self` via `output/result` that is actually used in the gradient formula (a side note: in derivatives.yaml, for the first derivative softmax_backward has to be declared with `result` argument, not `output`, otherwise generator does not properly insert unpacking code. For the second derivative, `softmax_backward` has to be declared with `output` argument, otherwise compiler complains about something, don't remember what).\r\nSo in this piece of code\r\n```\r\nname: softmax_backward(Tensor grad_output, Tensor output, int64_t dim, Tensor self)\r\ngrad_output: softmax_backward(grad, output, dim, self)\r\nself: softmax_double_backward(grad, grad_output, dim, output)\r\n```\r\n`self` in the first line is just so that there can be `self` in the third line. Does it make sense?\r\n\r\n"}