{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8837", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8837/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8837/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8837/events", "html_url": "https://github.com/pytorch/pytorch/issues/8837", "id": 335237115, "node_id": "MDU6SXNzdWUzMzUyMzcxMTU=", "number": 8837, "title": "Inconsistency in implementation of _LRScheduler ", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-25T02:11:05Z", "updated_at": "2018-06-25T17:33:15Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I've noticed an odd behavior when attempting to write my own scheduler based on <code>torch.optim.lr_scheduler._LRScheduler</code>.</p>\n<p>If you write a custom get_lr() to work based on self.last_epoch, its impossible to differentiate the<br>\n0th and the 1st epoch.</p>\n<p>Here is a minimal working example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">mwe</span>():\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assuming optimizer has two groups.</span>\n    <span class=\"pl-k\">import</span> torch.optim.lr_scheduler\n    <span class=\"pl-k\">import</span> netharn <span class=\"pl-k\">as</span> nh\n    model <span class=\"pl-k\">=</span> nh.models.ToyNet2d()\n    optimizer <span class=\"pl-k\">=</span> torch.optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">class</span> <span class=\"pl-en\">DummySchedule</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">optim</span>.<span class=\"pl-e\">lr_scheduler</span>.<span class=\"pl-e\">_LRScheduler</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_lr</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Set LR based on self.last_epoch = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(<span class=\"pl-c1\">self</span>.last_epoch))\n            <span class=\"pl-c1\">self</span>._current_lr <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.last_epoch\n            <span class=\"pl-k\">return</span> [<span class=\"pl-c1\">self</span>.last_epoch]\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the optimizer with epoch 0's LR</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> self = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: x)</span>\n    <span class=\"pl-c1\">self</span> <span class=\"pl-k\">=</span> DummySchedule(optimizer)\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>------<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Run epoch = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(epoch))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Pretend we run epoch 0</span>\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Training with self._current_lr = <span class=\"pl-c1\">{<span class=\"pl-k\">!r</span>}</span><span class=\"pl-pds\">'</span></span>.format(<span class=\"pl-c1\">self</span>._current_lr))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Pretend epoch 0 has finished, so step the scheduler.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> self.step(epoch=epoch)</span>\n        <span class=\"pl-c1\">self</span>.step()</pre></div>\n<p>This results in the output</p>\n<pre><code>Set LR based on self.last_epoch = 0\n------\nRun epoch = 0\nTraining with self._current_lr = 0\nSet LR based on self.last_epoch = 0\n------\nRun epoch = 1\nTraining with self._current_lr = 0\nSet LR based on self.last_epoch = 1\n------\nRun epoch = 2\nTraining with self._current_lr = 1\nSet LR based on self.last_epoch = 2\n</code></pre>\n<p>You can see the last epoch is asked to set the learning rate based on the last epoch being 0 twice. This LRScheduler class takes last_epoch as an argument, so it knows how to set the LR for the previous epoch. By default last_epoch=-1, because the first epoch is 0 and no epoch has run yet. On construction it then calls <code>step</code> with <code>last_epoch + 1</code>, which means the step function sets the learning rate for epoch 0. Then last_epoch is reset to -1 immediately after, so the next call to step also sets the learning rate for epoch 0.</p>\n<p>A fix would simply remove the + 1 from <code>self.step(last_epoch + 1)</code>, but this might break existing implementations of <code>get_lr()</code> which wouldn't expect <code>self.last_epoch</code> being set to a negative number.</p>\n<p>I think a more intuitive implementation of this class might track the current epoch rather than the previous one. This would be a backwards incompatible change, but I think it would improve the overall quality of torch. I'm willing to give a re-implementation a shot if it sounds like a good idea to the maintainers.</p>", "body_text": "I've noticed an odd behavior when attempting to write my own scheduler based on torch.optim.lr_scheduler._LRScheduler.\nIf you write a custom get_lr() to work based on self.last_epoch, its impossible to differentiate the\n0th and the 1st epoch.\nHere is a minimal working example:\ndef mwe():\n    # Assuming optimizer has two groups.\n    import torch.optim.lr_scheduler\n    import netharn as nh\n    model = nh.models.ToyNet2d()\n    optimizer = torch.optim.SGD(model.parameters(), lr=10)\n\n    class DummySchedule(torch.optim.lr_scheduler._LRScheduler):\n        def get_lr(self):\n            print('Set LR based on self.last_epoch = {!r}'.format(self.last_epoch))\n            self._current_lr = self.last_epoch\n            return [self.last_epoch]\n\n    # Initialize the optimizer with epoch 0's LR\n    # self = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: x)\n    self = DummySchedule(optimizer)\n    for epoch in range(3):\n        print('------')\n        print('Run epoch = {!r}'.format(epoch))\n        # Pretend we run epoch 0\n        print('Training with self._current_lr = {!r}'.format(self._current_lr))\n        # Pretend epoch 0 has finished, so step the scheduler.\n        # self.step(epoch=epoch)\n        self.step()\nThis results in the output\nSet LR based on self.last_epoch = 0\n------\nRun epoch = 0\nTraining with self._current_lr = 0\nSet LR based on self.last_epoch = 0\n------\nRun epoch = 1\nTraining with self._current_lr = 0\nSet LR based on self.last_epoch = 1\n------\nRun epoch = 2\nTraining with self._current_lr = 1\nSet LR based on self.last_epoch = 2\n\nYou can see the last epoch is asked to set the learning rate based on the last epoch being 0 twice. This LRScheduler class takes last_epoch as an argument, so it knows how to set the LR for the previous epoch. By default last_epoch=-1, because the first epoch is 0 and no epoch has run yet. On construction it then calls step with last_epoch + 1, which means the step function sets the learning rate for epoch 0. Then last_epoch is reset to -1 immediately after, so the next call to step also sets the learning rate for epoch 0.\nA fix would simply remove the + 1 from self.step(last_epoch + 1), but this might break existing implementations of get_lr() which wouldn't expect self.last_epoch being set to a negative number.\nI think a more intuitive implementation of this class might track the current epoch rather than the previous one. This would be a backwards incompatible change, but I think it would improve the overall quality of torch. I'm willing to give a re-implementation a shot if it sounds like a good idea to the maintainers.", "body": "I've noticed an odd behavior when attempting to write my own scheduler based on `torch.optim.lr_scheduler._LRScheduler`. \r\n\r\nIf you write a custom get_lr() to work based on self.last_epoch, its impossible to differentiate the \r\n0th and the 1st epoch. \r\n\r\nHere is a minimal working example:\r\n\r\n```python\r\n\r\ndef mwe():\r\n    # Assuming optimizer has two groups.\r\n    import torch.optim.lr_scheduler\r\n    import netharn as nh\r\n    model = nh.models.ToyNet2d()\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=10)\r\n\r\n    class DummySchedule(torch.optim.lr_scheduler._LRScheduler):\r\n        def get_lr(self):\r\n            print('Set LR based on self.last_epoch = {!r}'.format(self.last_epoch))\r\n            self._current_lr = self.last_epoch\r\n            return [self.last_epoch]\r\n\r\n    # Initialize the optimizer with epoch 0's LR\r\n    # self = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: x)\r\n    self = DummySchedule(optimizer)\r\n    for epoch in range(3):\r\n        print('------')\r\n        print('Run epoch = {!r}'.format(epoch))\r\n        # Pretend we run epoch 0\r\n        print('Training with self._current_lr = {!r}'.format(self._current_lr))\r\n        # Pretend epoch 0 has finished, so step the scheduler.\r\n        # self.step(epoch=epoch)\r\n        self.step()\r\n```\r\n\r\nThis results in the output\r\n\r\n```\r\nSet LR based on self.last_epoch = 0\r\n------\r\nRun epoch = 0\r\nTraining with self._current_lr = 0\r\nSet LR based on self.last_epoch = 0\r\n------\r\nRun epoch = 1\r\nTraining with self._current_lr = 0\r\nSet LR based on self.last_epoch = 1\r\n------\r\nRun epoch = 2\r\nTraining with self._current_lr = 1\r\nSet LR based on self.last_epoch = 2\r\n```\r\n\r\nYou can see the last epoch is asked to set the learning rate based on the last epoch being 0 twice. This LRScheduler class takes last_epoch as an argument, so it knows how to set the LR for the previous epoch. By default last_epoch=-1, because the first epoch is 0 and no epoch has run yet. On construction it then calls `step` with `last_epoch + 1`, which means the step function sets the learning rate for epoch 0. Then last_epoch is reset to -1 immediately after, so the next call to step also sets the learning rate for epoch 0. \r\n\r\nA fix would simply remove the + 1 from `self.step(last_epoch + 1)`, but this might break existing implementations of `get_lr()` which wouldn't expect `self.last_epoch` being set to a negative number. \r\n\r\nI think a more intuitive implementation of this class might track the current epoch rather than the previous one. This would be a backwards incompatible change, but I think it would improve the overall quality of torch. I'm willing to give a re-implementation a shot if it sounds like a good idea to the maintainers. "}