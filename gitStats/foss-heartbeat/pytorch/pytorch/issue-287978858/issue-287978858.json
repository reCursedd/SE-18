{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4621", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4621/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4621/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4621/events", "html_url": "https://github.com/pytorch/pytorch/issues/4621", "id": 287978858, "node_id": "MDU6SXNzdWUyODc5Nzg4NTg=", "number": 4621, "title": "MSEloss with reduce=False returns a tensor of same size as the input and target  (rather than just one loss per batch element)", "user": {"login": "dineshj1", "id": 1779928, "node_id": "MDQ6VXNlcjE3Nzk5Mjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1779928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dineshj1", "html_url": "https://github.com/dineshj1", "followers_url": "https://api.github.com/users/dineshj1/followers", "following_url": "https://api.github.com/users/dineshj1/following{/other_user}", "gists_url": "https://api.github.com/users/dineshj1/gists{/gist_id}", "starred_url": "https://api.github.com/users/dineshj1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dineshj1/subscriptions", "organizations_url": "https://api.github.com/users/dineshj1/orgs", "repos_url": "https://api.github.com/users/dineshj1/repos", "events_url": "https://api.github.com/users/dineshj1/events{/privacy}", "received_events_url": "https://api.github.com/users/dineshj1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-01-12T01:16:47Z", "updated_at": "2018-02-13T18:11:15Z", "closed_at": "2018-02-13T18:11:15Z", "author_association": "NONE", "body_html": "<p>MSELoss with reduce=False doesn't behave as documented.</p>\n<pre><code>loss = nn.MSELoss(reduce=False)\ninput = autograd.Variable(torch.randn(3, 5)) # 3 elements in batch of size 5\ntarget = autograd.Variable(torch.randn(3, 5))\noutput = loss(input, target)\nprint(output)\n</code></pre>\n<p>As documented, this should return one loss for each of the 3 batch elements. Instead, it returns:</p>\n<pre><code>Variable containing:\n  5.4552   2.3369   3.3013   0.0396  13.2650\n  1.4288   0.4156   0.3027   0.3006   0.2276\n  7.6768   1.0778   0.1317   1.9284   3.4640\n[torch.FloatTensor of size 3x5]\n</code></pre>\n<p>A similar setup with with CrossEntropyLoss(reduce=False) instead of MSELoss correctly returns a FloatTensor of size 3.</p>", "body_text": "MSELoss with reduce=False doesn't behave as documented.\nloss = nn.MSELoss(reduce=False)\ninput = autograd.Variable(torch.randn(3, 5)) # 3 elements in batch of size 5\ntarget = autograd.Variable(torch.randn(3, 5))\noutput = loss(input, target)\nprint(output)\n\nAs documented, this should return one loss for each of the 3 batch elements. Instead, it returns:\nVariable containing:\n  5.4552   2.3369   3.3013   0.0396  13.2650\n  1.4288   0.4156   0.3027   0.3006   0.2276\n  7.6768   1.0778   0.1317   1.9284   3.4640\n[torch.FloatTensor of size 3x5]\n\nA similar setup with with CrossEntropyLoss(reduce=False) instead of MSELoss correctly returns a FloatTensor of size 3.", "body": "MSELoss with reduce=False doesn't behave as documented.\r\n```\r\nloss = nn.MSELoss(reduce=False)\r\ninput = autograd.Variable(torch.randn(3, 5)) # 3 elements in batch of size 5\r\ntarget = autograd.Variable(torch.randn(3, 5))\r\noutput = loss(input, target)\r\nprint(output)\r\n```\r\nAs documented, this should return one loss for each of the 3 batch elements. Instead, it returns:\r\n```\r\nVariable containing:\r\n  5.4552   2.3369   3.3013   0.0396  13.2650\r\n  1.4288   0.4156   0.3027   0.3006   0.2276\r\n  7.6768   1.0778   0.1317   1.9284   3.4640\r\n[torch.FloatTensor of size 3x5]\r\n```\r\n\r\nA similar setup with with CrossEntropyLoss(reduce=False) instead of MSELoss correctly returns a FloatTensor of size 3."}