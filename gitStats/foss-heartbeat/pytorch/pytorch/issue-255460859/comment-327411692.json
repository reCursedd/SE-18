{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/327411692", "html_url": "https://github.com/pytorch/pytorch/issues/2639#issuecomment-327411692", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2639", "id": 327411692, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzQxMTY5Mg==", "user": {"login": "cdluminate", "id": 5723047, "node_id": "MDQ6VXNlcjU3MjMwNDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5723047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cdluminate", "html_url": "https://github.com/cdluminate", "followers_url": "https://api.github.com/users/cdluminate/followers", "following_url": "https://api.github.com/users/cdluminate/following{/other_user}", "gists_url": "https://api.github.com/users/cdluminate/gists{/gist_id}", "starred_url": "https://api.github.com/users/cdluminate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cdluminate/subscriptions", "organizations_url": "https://api.github.com/users/cdluminate/orgs", "repos_url": "https://api.github.com/users/cdluminate/repos", "events_url": "https://api.github.com/users/cdluminate/events{/privacy}", "received_events_url": "https://api.github.com/users/cdluminate/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-06T08:21:31Z", "updated_at": "2017-09-06T08:21:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The optimizer will apply weight decay to the biases.</p>\n<p>The optimizer is instantiated with a set of parameters, and the set of parameters contains biases.</p>\n<pre><code>In [1]: import torch as th\n\nIn [2]: th.nn.Linear(10, 10)\nOut[2]: Linear (10 -&gt; 10)\n\nIn [6]: for x in th.nn.Linear(10, 10).parameters(): print(x.size())\ntorch.Size([10, 10])\ntorch.Size([10])\n</code></pre>\n<p>optimizer.py</p>\n<pre><code> 28         self.param_groups = list(params)\n</code></pre>\n<p>sgd.py</p>\n<pre><code> 74         for group in self.param_groups:                                         \n 75             weight_decay = group['weight_decay']                                \n 76             momentum = group['momentum']                                        \n 77             dampening = group['dampening']                                      \n 78             nesterov = group['nesterov']                                        \n 79                                                                                 \n 80             for p in group['params']:                                           \n 81                 if p.grad is None:                                              \n 82                     continue                                                    \n 83                 d_p = p.grad.data                                               \n 84                 if weight_decay != 0:                                           \n 85                     d_p.add_(weight_decay, p.data)   \n</code></pre>\n<p>The biases will be decayed when the optimizer traversed to it.</p>", "body_text": "The optimizer will apply weight decay to the biases.\nThe optimizer is instantiated with a set of parameters, and the set of parameters contains biases.\nIn [1]: import torch as th\n\nIn [2]: th.nn.Linear(10, 10)\nOut[2]: Linear (10 -> 10)\n\nIn [6]: for x in th.nn.Linear(10, 10).parameters(): print(x.size())\ntorch.Size([10, 10])\ntorch.Size([10])\n\noptimizer.py\n 28         self.param_groups = list(params)\n\nsgd.py\n 74         for group in self.param_groups:                                         \n 75             weight_decay = group['weight_decay']                                \n 76             momentum = group['momentum']                                        \n 77             dampening = group['dampening']                                      \n 78             nesterov = group['nesterov']                                        \n 79                                                                                 \n 80             for p in group['params']:                                           \n 81                 if p.grad is None:                                              \n 82                     continue                                                    \n 83                 d_p = p.grad.data                                               \n 84                 if weight_decay != 0:                                           \n 85                     d_p.add_(weight_decay, p.data)   \n\nThe biases will be decayed when the optimizer traversed to it.", "body": "The optimizer will apply weight decay to the biases.\r\n\r\nThe optimizer is instantiated with a set of parameters, and the set of parameters contains biases.\r\n```\r\nIn [1]: import torch as th\r\n\r\nIn [2]: th.nn.Linear(10, 10)\r\nOut[2]: Linear (10 -> 10)\r\n\r\nIn [6]: for x in th.nn.Linear(10, 10).parameters(): print(x.size())\r\ntorch.Size([10, 10])\r\ntorch.Size([10])\r\n```\r\n\r\noptimizer.py\r\n```\r\n 28         self.param_groups = list(params)\r\n```\r\n\r\nsgd.py\r\n```\r\n 74         for group in self.param_groups:                                         \r\n 75             weight_decay = group['weight_decay']                                \r\n 76             momentum = group['momentum']                                        \r\n 77             dampening = group['dampening']                                      \r\n 78             nesterov = group['nesterov']                                        \r\n 79                                                                                 \r\n 80             for p in group['params']:                                           \r\n 81                 if p.grad is None:                                              \r\n 82                     continue                                                    \r\n 83                 d_p = p.grad.data                                               \r\n 84                 if weight_decay != 0:                                           \r\n 85                     d_p.add_(weight_decay, p.data)   \r\n```\r\n\r\nThe biases will be decayed when the optimizer traversed to it."}