{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1529", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1529/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1529/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1529/events", "html_url": "https://github.com/pytorch/pytorch/issues/1529", "id": 227795636, "node_id": "MDU6SXNzdWUyMjc3OTU2MzY=", "number": 1529, "title": "[feature request] Caching allocator diagnostics", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-05-10T20:08:34Z", "updated_at": "2018-10-24T15:06:13Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Would be cool to peek into the state of the caching allocator on things like:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Total cached memory</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Total currently used memory, referenced by Tensors</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Forced free of unused segments</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Tracing of memory allocations (along with some measure of fragmentation) and deallocations. Would be useful for custom anasysis scripts and for understanding a reason of OOM (fragmentation or actual lack of memory)</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Stats about currently existing tensors (if possible, otherwise with a full trace one implement this post-hoc): type, sizes, gpu device. if we had a way to dump timestamp of allocation, would be cool too (would allow to track sort of reliably memory leaks)</p>\n</li>\n</ul>\n<p>With caching allocaotr it's hard to understand sometimes what's happening with memory since after some big allocations / deallocations memory on nvidia-smi always stays high and doesn't reflect actual usage.</p>", "body_text": "Would be cool to peek into the state of the caching allocator on things like:\n\n\n Total cached memory\n\n\n Total currently used memory, referenced by Tensors\n\n\n Forced free of unused segments\n\n\n Tracing of memory allocations (along with some measure of fragmentation) and deallocations. Would be useful for custom anasysis scripts and for understanding a reason of OOM (fragmentation or actual lack of memory)\n\n\n Stats about currently existing tensors (if possible, otherwise with a full trace one implement this post-hoc): type, sizes, gpu device. if we had a way to dump timestamp of allocation, would be cool too (would allow to track sort of reliably memory leaks)\n\n\nWith caching allocaotr it's hard to understand sometimes what's happening with memory since after some big allocations / deallocations memory on nvidia-smi always stays high and doesn't reflect actual usage.", "body": "Would be cool to peek into the state of the caching allocator on things like:\r\n- [x] Total cached memory\r\n- [x] Total currently used memory, referenced by Tensors\r\n- [x] Forced free of unused segments\r\n\r\n- [ ] Tracing of memory allocations (along with some measure of fragmentation) and deallocations. Would be useful for custom anasysis scripts and for understanding a reason of OOM (fragmentation or actual lack of memory)\r\n- [ ] Stats about currently existing tensors (if possible, otherwise with a full trace one implement this post-hoc): type, sizes, gpu device. if we had a way to dump timestamp of allocation, would be cool too (would allow to track sort of reliably memory leaks)\r\n\r\nWith caching allocaotr it's hard to understand sometimes what's happening with memory since after some big allocations / deallocations memory on nvidia-smi always stays high and doesn't reflect actual usage."}