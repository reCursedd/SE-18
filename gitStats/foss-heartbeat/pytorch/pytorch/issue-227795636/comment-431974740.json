{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431974740", "html_url": "https://github.com/pytorch/pytorch/issues/1529#issuecomment-431974740", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1529", "id": 431974740, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTk3NDc0MA==", "user": {"login": "dojoteef", "id": 1501173, "node_id": "MDQ6VXNlcjE1MDExNzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1501173?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dojoteef", "html_url": "https://github.com/dojoteef", "followers_url": "https://api.github.com/users/dojoteef/followers", "following_url": "https://api.github.com/users/dojoteef/following{/other_user}", "gists_url": "https://api.github.com/users/dojoteef/gists{/gist_id}", "starred_url": "https://api.github.com/users/dojoteef/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dojoteef/subscriptions", "organizations_url": "https://api.github.com/users/dojoteef/orgs", "repos_url": "https://api.github.com/users/dojoteef/repos", "events_url": "https://api.github.com/users/dojoteef/events{/privacy}", "received_events_url": "https://api.github.com/users/dojoteef/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-22T20:35:50Z", "updated_at": "2018-10-22T20:35:50Z", "author_association": "NONE", "body_html": "<p>Tracing memory allocations and getting some idea of fragmentation would be super helpful. Adding an option that spits out memory diagnostics when an out of memory (OOM) exception occurs ala Tensorflow could really help users understand what's going on under the hood.</p>\n<p>For example, I spent some time trying to figure out why I was running OOM when using variable sized batches, and I believe it is related to fragmentation. Given batches <code>A</code> &amp; <code>B</code>, running an optimization step on <code>A</code> first, fails with an OOM. If you swap the order and run <code>B</code> first, then <code>A</code> subsequently succeeds.</p>\n<p>In trying to track down the issue, I wrote a <a href=\"https://gist.github.com/dojoteef/26cd46f7cc38b38e6f443c5f62411aa3\">Python-level CUDA memory profiler</a> (I couldn't find any available options that met my needs). I'm sure there's plenty of room for improvement, but it works okay and isn't prohibitively slow, though it is a pretty significant slowdown as expected for Python-level tracing. It prints out memory diagnostics per source line, including across multiple threads running CUDA operations (such as using <code>nn.DataParallel</code>). Unfortunately, the total allocations it reports seems to be less than reported by the <code>torch.cuda.memory_allocated</code> family of methods, so I am likely missing some key types of CUDA memory allocations.</p>\n<p>I hope people find it useful while we wait for a more substantive option built into the Pytorch allocators.</p>", "body_text": "Tracing memory allocations and getting some idea of fragmentation would be super helpful. Adding an option that spits out memory diagnostics when an out of memory (OOM) exception occurs ala Tensorflow could really help users understand what's going on under the hood.\nFor example, I spent some time trying to figure out why I was running OOM when using variable sized batches, and I believe it is related to fragmentation. Given batches A & B, running an optimization step on A first, fails with an OOM. If you swap the order and run B first, then A subsequently succeeds.\nIn trying to track down the issue, I wrote a Python-level CUDA memory profiler (I couldn't find any available options that met my needs). I'm sure there's plenty of room for improvement, but it works okay and isn't prohibitively slow, though it is a pretty significant slowdown as expected for Python-level tracing. It prints out memory diagnostics per source line, including across multiple threads running CUDA operations (such as using nn.DataParallel). Unfortunately, the total allocations it reports seems to be less than reported by the torch.cuda.memory_allocated family of methods, so I am likely missing some key types of CUDA memory allocations.\nI hope people find it useful while we wait for a more substantive option built into the Pytorch allocators.", "body": "Tracing memory allocations and getting some idea of fragmentation would be super helpful. Adding an option that spits out memory diagnostics when an out of memory (OOM) exception occurs ala Tensorflow could really help users understand what's going on under the hood.\r\n\r\nFor example, I spent some time trying to figure out why I was running OOM when using variable sized batches, and I believe it is related to fragmentation. Given batches `A` & `B`, running an optimization step on `A` first, fails with an OOM. If you swap the order and run `B` first, then `A` subsequently succeeds.\r\n\r\nIn trying to track down the issue, I wrote a [Python-level CUDA memory profiler](https://gist.github.com/dojoteef/26cd46f7cc38b38e6f443c5f62411aa3) (I couldn't find any available options that met my needs). I'm sure there's plenty of room for improvement, but it works okay and isn't prohibitively slow, though it is a pretty significant slowdown as expected for Python-level tracing. It prints out memory diagnostics per source line, including across multiple threads running CUDA operations (such as using `nn.DataParallel`). Unfortunately, the total allocations it reports seems to be less than reported by the `torch.cuda.memory_allocated` family of methods, so I am likely missing some key types of CUDA memory allocations.\r\n\r\nI hope people find it useful while we wait for a more substantive option built into the Pytorch allocators."}