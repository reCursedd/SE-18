{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398884541", "html_url": "https://github.com/pytorch/pytorch/issues/8544#issuecomment-398884541", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8544", "id": 398884541, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODg4NDU0MQ==", "user": {"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-20T20:23:54Z", "updated_at": "2018-06-20T20:23:54Z", "author_association": "MEMBER", "body_html": "<p>So, I've investigated this and determined that there is no work to be done. The implementation of pytorch-&gt;onnx and onnx-&gt;caffe2 was already matching the updated semantics (which means it was technically incorrect before, but now it is fine).</p>\n<p>I'm going to record some of my reasoning here, in case this ever comes up again</p>\n<ul>\n<li>\n<p>first, note that Pytorch has the same semantics as ONNX, though it's phrased differently. The ONNX spec says <code>X*(W^T)</code>, while the pytorch spec simply says <code>W*X</code>. However when X is of shape <code>(input_size,)</code>, and W is of shape <code>(hidden_size, input_size)</code>, these are equivalent.</p>\n</li>\n<li>\n<p>second, note that Pytorch spec treats the input and recurrent states identically, as does the new version of the ONNX spec.</p>\n</li>\n<li>\n<p>third, note that the pytorch-&gt;onnx conversion (in symbolic.py) treats the input and recurrent states that come from pytorch identically - it doesn't add any extra transposes (which would have been needed if we had been correctly matching the old version of the ONNX spec)</p>\n</li>\n</ul>\n<p>That means that the pytorch-&gt;onnx conversion is correct.</p>\n<ul>\n<li>fourth, note that we closely test that pytorch-&gt;onnx-&gt;caffe2 produces identical results. This means that the onnx-&gt;caffe2 implementation is matching the pytorch-&gt;onnx implementation, and therefore it's also doing the right thing.</li>\n</ul>", "body_text": "So, I've investigated this and determined that there is no work to be done. The implementation of pytorch->onnx and onnx->caffe2 was already matching the updated semantics (which means it was technically incorrect before, but now it is fine).\nI'm going to record some of my reasoning here, in case this ever comes up again\n\n\nfirst, note that Pytorch has the same semantics as ONNX, though it's phrased differently. The ONNX spec says X*(W^T), while the pytorch spec simply says W*X. However when X is of shape (input_size,), and W is of shape (hidden_size, input_size), these are equivalent.\n\n\nsecond, note that Pytorch spec treats the input and recurrent states identically, as does the new version of the ONNX spec.\n\n\nthird, note that the pytorch->onnx conversion (in symbolic.py) treats the input and recurrent states that come from pytorch identically - it doesn't add any extra transposes (which would have been needed if we had been correctly matching the old version of the ONNX spec)\n\n\nThat means that the pytorch->onnx conversion is correct.\n\nfourth, note that we closely test that pytorch->onnx->caffe2 produces identical results. This means that the onnx->caffe2 implementation is matching the pytorch->onnx implementation, and therefore it's also doing the right thing.", "body": "So, I've investigated this and determined that there is no work to be done. The implementation of pytorch->onnx and onnx->caffe2 was already matching the updated semantics (which means it was technically incorrect before, but now it is fine).\r\n\r\nI'm going to record some of my reasoning here, in case this ever comes up again\r\n\r\n- first, note that Pytorch has the same semantics as ONNX, though it's phrased differently. The ONNX spec says `X*(W^T)`, while the pytorch spec simply says `W*X`. However when X is of shape `(input_size,)`, and W is of shape `(hidden_size, input_size)`, these are equivalent.\r\n\r\n- second, note that Pytorch spec treats the input and recurrent states identically, as does the new version of the ONNX spec.\r\n\r\n- third, note that the pytorch->onnx conversion (in symbolic.py) treats the input and recurrent states that come from pytorch identically - it doesn't add any extra transposes (which would have been needed if we had been correctly matching the old version of the ONNX spec)\r\n\r\nThat means that the pytorch->onnx conversion is correct.\r\n\r\n- fourth, note that we closely test that pytorch->onnx->caffe2 produces identical results. This means that the onnx->caffe2 implementation is matching the pytorch->onnx implementation, and therefore it's also doing the right thing."}