{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6983", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6983/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6983/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6983/events", "html_url": "https://github.com/pytorch/pytorch/issues/6983", "id": 317920252, "node_id": "MDU6SXNzdWUzMTc5MjAyNTI=", "number": 6983, "title": "nn.parallel.gather does not accept scalars (0-dim tensors) of v0.4", "user": {"login": "L0SG", "id": 15963413, "node_id": "MDQ6VXNlcjE1OTYzNDEz", "avatar_url": "https://avatars0.githubusercontent.com/u/15963413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/L0SG", "html_url": "https://github.com/L0SG", "followers_url": "https://api.github.com/users/L0SG/followers", "following_url": "https://api.github.com/users/L0SG/following{/other_user}", "gists_url": "https://api.github.com/users/L0SG/gists{/gist_id}", "starred_url": "https://api.github.com/users/L0SG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/L0SG/subscriptions", "organizations_url": "https://api.github.com/users/L0SG/orgs", "repos_url": "https://api.github.com/users/L0SG/repos", "events_url": "https://api.github.com/users/L0SG/events{/privacy}", "received_events_url": "https://api.github.com/users/L0SG/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-04-26T08:09:01Z", "updated_at": "2018-06-01T20:51:23Z", "closed_at": "2018-06-01T20:51:22Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>Since v0.4 returns scalar (0-dim tensor) loss, gathering the scalar loss manually raises an error like the example.</p>\n<p>Unsqueezing the scalar losses back to 1-dim vector like the previous versions works, but is this an intended behavior of <code>nn.parallel.gather</code>?</p>\n<p>The given parallel GPU code scheme is used in <a href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"nofollow\">Annotated Transformer implementation</a>.</p>\n<h2>Code example</h2>\n<pre><code>import torch.nn as nn\nimport torch\n\n# GPUs to use\ndevices = [0, 1, 2, 3]\n\n# toy feed-forward net\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n# define random data\nrandom_input = torch.randn((4, 10))\nrandom_target = torch.randn((4))\n\nnet = Net().cuda()\n\n# replicate nets, scatter inputs and parallel_apply\nreplicas = nn.parallel.replicate(net, devices)\nrandom_input_scatter = nn.parallel.scatter(random_input, devices)\nreplicas = replicas[:len(random_input_scatter)]\noutputs = nn.parallel.parallel_apply(replicas, random_input_scatter)\n\n# replicate losses, scatter targets, zip output-target pairs\ncriterion = nn.MSELoss()\ncriterion = nn.parallel.replicate(criterion, devices)\nrandom_target_scatter = nn.parallel.scatter(random_target, devices)\noutput_target_pairs = [(output, target) for output, target in zip(outputs, random_target_scatter)]\n\n# this results in scalar (0-dim tensor) losses with v0.4\nloss = nn.parallel.parallel_apply(criterion, output_target_pairs)\n# gathering 0-dim tensors raises error in line 54 of parallel.gather function =&gt; ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\n\n# unsqueezing the scalar loss to vector (like from previous versions) works as intended\n\"\"\"\nfor idx in range(len(loss)):\n    loss[idx] = loss[idx].unsqueeze(0)\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\n\"\"\"\n</code></pre>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"gather_bug.py\", line 43, in <br>\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])<br>\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather<br>\nreturn gather_map(outputs)<br>\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map<br>\nreturn Gather.apply(target_device, dim, *outputs)<br>\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward<br>\nctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))<br>\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in <br>\nctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))<br>\nRuntimeError: dimension specified as 0 but tensor has no dimensions</p>\n</blockquote>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: TITAN Xp<br>\nGPU 1: TITAN Xp<br>\nGPU 2: TITAN Xp<br>\nGPU 3: TITAN Xp</p>\n<p>Nvidia driver version: 390.30<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a<br>\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudnn.so.5.1.5<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] msgpack-numpy (0.4.1)<br>\n[pip] numpy (1.13.3)<br>\n[pip] torch (0.4.0)<br>\n[pip] torchfile (0.1.0)<br>\n[pip] torchnet (0.0.1)<br>\n[pip] torchtext (0.2.3)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] cuda90                    1.0                  h6433d27_0    pytorch<br>\n[conda] pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch<br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchnet                  0.0.1                     <br>\n[conda] torchtext                 0.2.3                     <br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Build command you used (if compiling from source): NA</li>\n<li>OS: ubuntu 16.04</li>\n<li>PyTorch version: 0.4</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9.0.176/7.0.5</li>\n<li>GPU models and configuration: 4x Titan Xp</li>\n<li>GCC version (if compiling from source): N/A</li>\n<li>CMake version: 3.5.1</li>\n<li>Versions of any other relevant libraries: N/A</li>\n</ul>", "body_text": "Issue description\nSince v0.4 returns scalar (0-dim tensor) loss, gathering the scalar loss manually raises an error like the example.\nUnsqueezing the scalar losses back to 1-dim vector like the previous versions works, but is this an intended behavior of nn.parallel.gather?\nThe given parallel GPU code scheme is used in Annotated Transformer implementation.\nCode example\nimport torch.nn as nn\nimport torch\n\n# GPUs to use\ndevices = [0, 1, 2, 3]\n\n# toy feed-forward net\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n# define random data\nrandom_input = torch.randn((4, 10))\nrandom_target = torch.randn((4))\n\nnet = Net().cuda()\n\n# replicate nets, scatter inputs and parallel_apply\nreplicas = nn.parallel.replicate(net, devices)\nrandom_input_scatter = nn.parallel.scatter(random_input, devices)\nreplicas = replicas[:len(random_input_scatter)]\noutputs = nn.parallel.parallel_apply(replicas, random_input_scatter)\n\n# replicate losses, scatter targets, zip output-target pairs\ncriterion = nn.MSELoss()\ncriterion = nn.parallel.replicate(criterion, devices)\nrandom_target_scatter = nn.parallel.scatter(random_target, devices)\noutput_target_pairs = [(output, target) for output, target in zip(outputs, random_target_scatter)]\n\n# this results in scalar (0-dim tensor) losses with v0.4\nloss = nn.parallel.parallel_apply(criterion, output_target_pairs)\n# gathering 0-dim tensors raises error in line 54 of parallel.gather function => ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\n\n# unsqueezing the scalar loss to vector (like from previous versions) works as intended\n\"\"\"\nfor idx in range(len(loss)):\n    loss[idx] = loss[idx].unsqueeze(0)\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\n\"\"\"\n\n\nTraceback (most recent call last):\nFile \"gather_bug.py\", line 43, in \nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\nreturn gather_map(outputs)\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\nreturn Gather.apply(target_device, dim, *outputs)\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\nctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\nFile \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in \nctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\nRuntimeError: dimension specified as 0 but tensor has no dimensions\n\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: TITAN Xp\nGPU 1: TITAN Xp\nGPU 2: TITAN Xp\nGPU 3: TITAN Xp\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudnn.so.5.1.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\nVersions of relevant libraries:\n[pip] msgpack-numpy (0.4.1)\n[pip] numpy (1.13.3)\n[pip] torch (0.4.0)\n[pip] torchfile (0.1.0)\n[pip] torchnet (0.0.1)\n[pip] torchtext (0.2.3)\n[pip] torchvision (0.2.1)\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\n[conda] pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch\n[conda] torchfile                 0.1.0                     \n[conda] torchnet                  0.0.1                     \n[conda] torchtext                 0.2.3                     \n[conda] torchvision               0.2.1                    py36_1    pytorch\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source): NA\nOS: ubuntu 16.04\nPyTorch version: 0.4\nPython version: 3.6\nCUDA/cuDNN version: 9.0.176/7.0.5\nGPU models and configuration: 4x Titan Xp\nGCC version (if compiling from source): N/A\nCMake version: 3.5.1\nVersions of any other relevant libraries: N/A", "body": "## Issue description\r\n\r\nSince v0.4 returns scalar (0-dim tensor) loss, gathering the scalar loss manually raises an error like the example.\r\n\r\nUnsqueezing the scalar losses back to 1-dim vector like the previous versions works, but is this an intended behavior of `nn.parallel.gather`?\r\n\r\nThe given parallel GPU code scheme is used in [Annotated Transformer implementation](http://nlp.seas.harvard.edu/2018/04/03/attention.html).\r\n## Code example\r\n\r\n\r\n```\r\nimport torch.nn as nn\r\nimport torch\r\n\r\n# GPUs to use\r\ndevices = [0, 1, 2, 3]\r\n\r\n# toy feed-forward net\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n\r\n        self.fc1 = nn.Linear(10, 5)\r\n        self.fc2 = nn.Linear(5, 5)\r\n        self.fc3 = nn.Linear(5, 1)\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        x = self.fc2(x)\r\n        x = self.fc3(x)\r\n        return x\r\n\r\n# define random data\r\nrandom_input = torch.randn((4, 10))\r\nrandom_target = torch.randn((4))\r\n\r\nnet = Net().cuda()\r\n\r\n# replicate nets, scatter inputs and parallel_apply\r\nreplicas = nn.parallel.replicate(net, devices)\r\nrandom_input_scatter = nn.parallel.scatter(random_input, devices)\r\nreplicas = replicas[:len(random_input_scatter)]\r\noutputs = nn.parallel.parallel_apply(replicas, random_input_scatter)\r\n\r\n# replicate losses, scatter targets, zip output-target pairs\r\ncriterion = nn.MSELoss()\r\ncriterion = nn.parallel.replicate(criterion, devices)\r\nrandom_target_scatter = nn.parallel.scatter(random_target, devices)\r\noutput_target_pairs = [(output, target) for output, target in zip(outputs, random_target_scatter)]\r\n\r\n# this results in scalar (0-dim tensor) losses with v0.4\r\nloss = nn.parallel.parallel_apply(criterion, output_target_pairs)\r\n# gathering 0-dim tensors raises error in line 54 of parallel.gather function => ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\r\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\r\n\r\n# unsqueezing the scalar loss to vector (like from previous versions) works as intended\r\n\"\"\"\r\nfor idx in range(len(loss)):\r\n    loss[idx] = loss[idx].unsqueeze(0)\r\nloss_gather = nn.parallel.gather(loss, target_device=devices[0])\r\n\"\"\"\r\n```\r\n\r\n> Traceback (most recent call last):\r\n>   File \"gather_bug.py\", line 43, in <module>\r\n>     loss_gather = nn.parallel.gather(loss, target_device=devices[0])\r\n>   File \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\r\n>     return gather_map(outputs)\r\n>   File \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\r\n>     return Gather.apply(target_device, dim, *outputs)\r\n>   File \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\r\n>     ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\r\n>   File \"/home/tkdrlf9202/anaconda3/envs/tkdrlf9202_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in <lambda>\r\n>     ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\r\n> RuntimeError: dimension specified as 0 but tensor has no dimensions\r\n\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy (0.4.1)\r\n[pip] numpy (1.13.3)\r\n[pip] torch (0.4.0)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchnet (0.0.1)\r\n[pip] torchtext (0.2.3)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchnet                  0.0.1                     <pip>\r\n[conda] torchtext                 0.2.3                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Build command you used (if compiling from source): NA\r\n- OS: ubuntu 16.04\r\n- PyTorch version: 0.4\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0.176/7.0.5\r\n- GPU models and configuration: 4x Titan Xp\r\n- GCC version (if compiling from source): N/A\r\n- CMake version: 3.5.1\r\n- Versions of any other relevant libraries: N/A\r\n"}