{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396806191", "html_url": "https://github.com/pytorch/pytorch/issues/7983#issuecomment-396806191", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7983", "id": 396806191, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjgwNjE5MQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-13T03:59:56Z", "updated_at": "2018-06-13T03:59:56Z", "author_association": "MEMBER", "body_html": "<p>I am rejecting this proposal because the proposed optimizations are auto-magical. Such forward-looking optimizations belong to the JIT-compiler, and this should be proposed as a <a href=\"https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/passes\">pass</a> for the jit.</p>\n<p>Let me elaborate a little more.</p>\n<p>In the simple case of</p>\n<pre><code>c = torch.cat_((a,b)) # a is now c[:a_size] and b is c[a_size:]\n</code></pre>\n<p>Initially, <code>a</code> and <code>b</code> are pointing to separate memory regions. But after the optimization, we allocate a region for <code>c</code>, and point <code>a</code> and <code>b</code> to this memory region, then we deallocate the original memory regions that <code>a</code> and <code>b</code> point to.</p>\n<p>It seems to look reasonable if <code>a</code> and <code>b</code> are newly allocated and nothing is pointing to them. However, take the following situation:</p>\n<pre><code>a = A[:, 1]\nc = torch.cat_((a,b))\n</code></pre>\n<p>in this case, the torch.cat is in-place not just mutating the values of <code>a</code> and <code>b</code> but actually changing the sharing situation of <code>a</code>. <code>a</code> is now sharing memory with <code>c</code>, but not with <code>A</code> anymore.</p>\n<p>This situation of magically manipulating storage leads to lots of silent bugs -- in fact we had nightmares with such a situation back in the LuaTorch days when the <code>getParameters()</code> function would manipulate <code>weight</code> storages automagically.</p>\n<p>One can do the same optimization manually (as you pointed out), and that should really be the way forward.</p>\n<p>For example:</p>\n<pre><code>out = torch.empty(....) # create with the size equivalent to size(a)  + size(b)\nout[...] = a\nout[...] = b\ndel a, b\n</code></pre>\n<p>If it's a common pattern that involves boiler-plate code, for example ReflectionPadding+Conv+Tanh+Identity, make a class for such a layer to reduce the boilerplate</p>", "body_text": "I am rejecting this proposal because the proposed optimizations are auto-magical. Such forward-looking optimizations belong to the JIT-compiler, and this should be proposed as a pass for the jit.\nLet me elaborate a little more.\nIn the simple case of\nc = torch.cat_((a,b)) # a is now c[:a_size] and b is c[a_size:]\n\nInitially, a and b are pointing to separate memory regions. But after the optimization, we allocate a region for c, and point a and b to this memory region, then we deallocate the original memory regions that a and b point to.\nIt seems to look reasonable if a and b are newly allocated and nothing is pointing to them. However, take the following situation:\na = A[:, 1]\nc = torch.cat_((a,b))\n\nin this case, the torch.cat is in-place not just mutating the values of a and b but actually changing the sharing situation of a. a is now sharing memory with c, but not with A anymore.\nThis situation of magically manipulating storage leads to lots of silent bugs -- in fact we had nightmares with such a situation back in the LuaTorch days when the getParameters() function would manipulate weight storages automagically.\nOne can do the same optimization manually (as you pointed out), and that should really be the way forward.\nFor example:\nout = torch.empty(....) # create with the size equivalent to size(a)  + size(b)\nout[...] = a\nout[...] = b\ndel a, b\n\nIf it's a common pattern that involves boiler-plate code, for example ReflectionPadding+Conv+Tanh+Identity, make a class for such a layer to reduce the boilerplate", "body": "I am rejecting this proposal because the proposed optimizations are auto-magical. Such forward-looking optimizations belong to the JIT-compiler, and this should be proposed as a [pass](https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/passes) for the jit.\r\n\r\nLet me elaborate a little more.\r\n\r\nIn the simple case of\r\n\r\n```\r\nc = torch.cat_((a,b)) # a is now c[:a_size] and b is c[a_size:]\r\n```\r\n\r\nInitially, `a` and `b` are pointing to separate memory regions. But after the optimization, we allocate a region for `c`, and point `a` and `b` to this memory region, then we deallocate the original memory regions that `a` and `b` point to.\r\n\r\nIt seems to look reasonable if `a` and `b` are newly allocated and nothing is pointing to them. However, take the following situation:\r\n\r\n```\r\na = A[:, 1]\r\nc = torch.cat_((a,b))\r\n```\r\n\r\nin this case, the torch.cat is in-place not just mutating the values of `a` and `b` but actually changing the sharing situation of `a`. `a` is now sharing memory with `c`, but not with `A` anymore.\r\n\r\nThis situation of magically manipulating storage leads to lots of silent bugs -- in fact we had nightmares with such a situation back in the LuaTorch days when the `getParameters()` function would manipulate `weight` storages automagically.\r\n\r\nOne can do the same optimization manually (as you pointed out), and that should really be the way forward.\r\n\r\nFor example:\r\n\r\n```\r\nout = torch.empty(....) # create with the size equivalent to size(a)  + size(b)\r\nout[...] = a\r\nout[...] = b\r\ndel a, b\r\n```\r\n\r\nIf it's a common pattern that involves boiler-plate code, for example ReflectionPadding+Conv+Tanh+Identity, make a class for such a layer to reduce the boilerplate"}