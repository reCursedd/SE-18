{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5057", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5057/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5057/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5057/events", "html_url": "https://github.com/pytorch/pytorch/issues/5057", "id": 294506817, "node_id": "MDU6SXNzdWUyOTQ1MDY4MTc=", "number": 5057, "title": "[BUG?] Strided transposed convolution returning wrong shape", "user": {"login": "eickenberg", "id": 1306635, "node_id": "MDQ6VXNlcjEzMDY2MzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/1306635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eickenberg", "html_url": "https://github.com/eickenberg", "followers_url": "https://api.github.com/users/eickenberg/followers", "following_url": "https://api.github.com/users/eickenberg/following{/other_user}", "gists_url": "https://api.github.com/users/eickenberg/gists{/gist_id}", "starred_url": "https://api.github.com/users/eickenberg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eickenberg/subscriptions", "organizations_url": "https://api.github.com/users/eickenberg/orgs", "repos_url": "https://api.github.com/users/eickenberg/repos", "events_url": "https://api.github.com/users/eickenberg/events{/privacy}", "received_events_url": "https://api.github.com/users/eickenberg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-05T19:05:21Z", "updated_at": "2018-02-15T19:19:34Z", "closed_at": "2018-02-15T19:19:34Z", "author_association": "NONE", "body_html": "<p>Hi everybody,</p>\n<p><code>conv_transpose2d</code> and <code>conv_transpose1d</code> seem to be broken for certain configurations of shapes and strides - IFF by 'conv_transpose' we agree that we mean to compute the transpose/adjoint of the convolution operation with the same parameter specs. In this case, the transpose has to verify <code>dot(Ax, y) = dot(x, A.Ty)</code> for any <code>x, y</code>, where <code>A</code> is the convolution operator and the <code>dot</code>s are inner products.</p>\n<p>Here is a test that can be run for different configurations (code is copy+pasteable):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> sklearn.utils <span class=\"pl-k\">import</span> check_random_state\n\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_convolution_adjointness</span>(<span class=\"pl-smi\">signal_spatial_shape</span>, <span class=\"pl-smi\">kernel_spatial_shape</span>, <span class=\"pl-smi\">stride</span>, \n                                 <span class=\"pl-smi\">conv</span><span class=\"pl-k\">=</span>F.conv2d, <span class=\"pl-smi\">conv_transpose</span><span class=\"pl-k\">=</span>F.conv_transpose2d, <span class=\"pl-smi\">random_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">42</span>,\n                                 <span class=\"pl-smi\">n_samples</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-smi\">n_in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-smi\">n_out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>This test checks whether the operations conv_*d and conv_transpose_*d are actually tranposed/adjoint</span>\n<span class=\"pl-s\">        to each other. This is important for exact gradient computations and to know exactly what operations</span>\n<span class=\"pl-s\">        we are dealing with. The test checks the definition for transpose in spaces with inner product:</span>\n<span class=\"pl-s\">        </span>\n<span class=\"pl-s\">        If A is the linear map in question, and `dot_d` is the inner product in its domain and `dot_c` is the</span>\n<span class=\"pl-s\">        inner product in its codomain (call both of them `dot`, because 1. it is clear which one is meant and</span>\n<span class=\"pl-s\">        2. we use the canonical one always anyway), then its transpose A.T satisifies</span>\n<span class=\"pl-s\">        </span>\n<span class=\"pl-s\">        dot(Ax, y) = dot(x, A.Ty)</span>\n<span class=\"pl-s\">        </span>\n<span class=\"pl-s\">        for any vectors x and y chosen from the domain and codomain of A.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> using numpy rng for local random seed fixing</span>\n    rng <span class=\"pl-k\">=</span> check_random_state(random_state) \n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Prepare random signal and kernel</span>\n    signal_shape <span class=\"pl-k\">=</span> (n_samples, n_in_channels) <span class=\"pl-k\">+</span> signal_spatial_shape\n    kernel_shape <span class=\"pl-k\">=</span> (n_out_channels, n_in_channels) <span class=\"pl-k\">+</span> kernel_spatial_shape\n    \n    signal <span class=\"pl-k\">=</span> Variable(torch.from_numpy(rng.randn(<span class=\"pl-k\">*</span>signal_shape).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)))\n    kernel <span class=\"pl-k\">=</span> Variable(torch.from_numpy(rng.randn(<span class=\"pl-k\">*</span>kernel_shape).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)))\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> convolve to get convolution output and its shape</span>\n    convolved <span class=\"pl-k\">=</span> conv(signal, kernel, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>stride)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> make the random vector y for the dot products</span>\n    random_codomain_vector <span class=\"pl-k\">=</span> Variable(torch.from_numpy(\n            rng.randn(<span class=\"pl-k\">*</span>convolved.size()).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)))\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute the codomain dot product dot(Ax, y)</span>\n    codomain_dot <span class=\"pl-k\">=</span> (convolved <span class=\"pl-k\">*</span> random_codomain_vector).resize(\n        n_samples, <span class=\"pl-c1\">int</span>(np.prod(convolved.size()[<span class=\"pl-c1\">1</span>:]))).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> transposed convolution</span>\n    transposed_output <span class=\"pl-k\">=</span> conv_transpose(random_codomain_vector, kernel, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>stride)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute domain dot product dot(x, A.Ty)</span>\n    domain_dot <span class=\"pl-k\">=</span> (transposed_output <span class=\"pl-k\">*</span> signal).resize(\n        n_samples, <span class=\"pl-c1\">int</span>(np.prod(signal.size()[<span class=\"pl-c1\">1</span>:]))).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    \n    np.testing.assert_array_almost_equal(domain_dot.data.cpu().numpy(), codomain_dot.data.cpu().numpy(), <span class=\"pl-v\">decimal</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)</pre></div>\n<p>The test passes for</p>\n<div class=\"highlight highlight-source-python\"><pre>test_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">1</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">1</span>)\n\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">1</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">1</span>)\n\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">1</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">4</span>)\n\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">1</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">4</span>)\n\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">1</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">4</span>)\n</pre></div>\n<p>It looks like it always passes when the stride is 1. Further, it passes when signal axis and filter axis are both of same parity (either both odd or both even), and the stride for 32 signal size is a power of 2 less than 8.</p>\n<p>And it fails for</p>\n<div class=\"highlight highlight-source-python\"><pre>test_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">31</span>, <span class=\"pl-c1\">31</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">2</span>)\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">4</span>)\n\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">3</span>)\n\ntest_convolution_adjointness((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">8</span>)</pre></div>\n<p>This is when signal and filter size are not of same parity, or the stride is not of the same parity or the stride is too big.</p>\n<p>All the failures are shape-related: The output shape is off by at least 1.</p>\n<p>Similar things hold for <code>conv_transpose1d</code>. I did not check for 3d. Can anybody reproduce this?</p>\n<p>[[Question: Are convolution gradients computed by the same underlying mechanism? (I'm new to pytorch and don't understand the low-level code enough yet to find out. If so, this would have some implications, in case this error persisted there and passed through the meshes due to torch shape management. The recent use of 4x4 filters might make this not be an issue, though)]]</p>\n<p>specs:<br>\nubuntu 16.04<br>\npytorch 0.3.0 (conda version. persists on other versions)<br>\n1080Ti (but using cpu here)</p>", "body_text": "Hi everybody,\nconv_transpose2d and conv_transpose1d seem to be broken for certain configurations of shapes and strides - IFF by 'conv_transpose' we agree that we mean to compute the transpose/adjoint of the convolution operation with the same parameter specs. In this case, the transpose has to verify dot(Ax, y) = dot(x, A.Ty) for any x, y, where A is the convolution operator and the dots are inner products.\nHere is a test that can be run for different configurations (code is copy+pasteable):\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom sklearn.utils import check_random_state\n\n\n\ndef test_convolution_adjointness(signal_spatial_shape, kernel_spatial_shape, stride, \n                                 conv=F.conv2d, conv_transpose=F.conv_transpose2d, random_state=42,\n                                 n_samples=5, n_in_channels=2, n_out_channels=3):\n    \"\"\"This test checks whether the operations conv_*d and conv_transpose_*d are actually tranposed/adjoint\n        to each other. This is important for exact gradient computations and to know exactly what operations\n        we are dealing with. The test checks the definition for transpose in spaces with inner product:\n        \n        If A is the linear map in question, and `dot_d` is the inner product in its domain and `dot_c` is the\n        inner product in its codomain (call both of them `dot`, because 1. it is clear which one is meant and\n        2. we use the canonical one always anyway), then its transpose A.T satisifies\n        \n        dot(Ax, y) = dot(x, A.Ty)\n        \n        for any vectors x and y chosen from the domain and codomain of A.\n    \"\"\"\n\n    # using numpy rng for local random seed fixing\n    rng = check_random_state(random_state) \n\n    # Prepare random signal and kernel\n    signal_shape = (n_samples, n_in_channels) + signal_spatial_shape\n    kernel_shape = (n_out_channels, n_in_channels) + kernel_spatial_shape\n    \n    signal = Variable(torch.from_numpy(rng.randn(*signal_shape).astype('float32')))\n    kernel = Variable(torch.from_numpy(rng.randn(*kernel_shape).astype('float32')))\n    \n    # convolve to get convolution output and its shape\n    convolved = conv(signal, kernel, stride=stride)\n    \n    # make the random vector y for the dot products\n    random_codomain_vector = Variable(torch.from_numpy(\n            rng.randn(*convolved.size()).astype('float32')))\n    \n    # compute the codomain dot product dot(Ax, y)\n    codomain_dot = (convolved * random_codomain_vector).resize(\n        n_samples, int(np.prod(convolved.size()[1:]))).sum(dim=1)\n    \n    # transposed convolution\n    transposed_output = conv_transpose(random_codomain_vector, kernel, stride=stride)\n    \n    # compute domain dot product dot(x, A.Ty)\n    domain_dot = (transposed_output * signal).resize(\n        n_samples, int(np.prod(signal.size()[1:]))).sum(dim=1)\n    \n    np.testing.assert_array_almost_equal(domain_dot.data.cpu().numpy(), codomain_dot.data.cpu().numpy(), decimal=3)\nThe test passes for\ntest_convolution_adjointness((32, 32), (3, 3), 1)\ntest_convolution_adjointness((31, 31), (4, 4), 1)\n\ntest_convolution_adjointness((31, 32), (5, 4), 1)\ntest_convolution_adjointness((30, 31), (5, 4), 1)\n\ntest_convolution_adjointness((32, 32), (4, 4), 1)\ntest_convolution_adjointness((32, 32), (4, 4), 2)\ntest_convolution_adjointness((32, 32), (4, 4), 4)\n\ntest_convolution_adjointness((31, 31), (3, 3), 1)\ntest_convolution_adjointness((31, 31), (3, 3), 2)\ntest_convolution_adjointness((31, 31), (3, 3), 4)\n\ntest_convolution_adjointness((31, 32), (3, 4), 1)\ntest_convolution_adjointness((31, 32), (3, 4), 2)\ntest_convolution_adjointness((31, 32), (3, 4), 4)\n\nIt looks like it always passes when the stride is 1. Further, it passes when signal axis and filter axis are both of same parity (either both odd or both even), and the stride for 32 signal size is a power of 2 less than 8.\nAnd it fails for\ntest_convolution_adjointness((32, 32), (3, 3), 2)\ntest_convolution_adjointness((31, 31), (4, 4), 2)\ntest_convolution_adjointness((32, 32), (4, 3), 2)\ntest_convolution_adjointness((31, 31), (4, 3), 2)\ntest_convolution_adjointness((32, 32), (3, 3), 4)\n\ntest_convolution_adjointness((32, 32), (4, 4), 3)\n\ntest_convolution_adjointness((32, 32), (4, 4), 8)\nThis is when signal and filter size are not of same parity, or the stride is not of the same parity or the stride is too big.\nAll the failures are shape-related: The output shape is off by at least 1.\nSimilar things hold for conv_transpose1d. I did not check for 3d. Can anybody reproduce this?\n[[Question: Are convolution gradients computed by the same underlying mechanism? (I'm new to pytorch and don't understand the low-level code enough yet to find out. If so, this would have some implications, in case this error persisted there and passed through the meshes due to torch shape management. The recent use of 4x4 filters might make this not be an issue, though)]]\nspecs:\nubuntu 16.04\npytorch 0.3.0 (conda version. persists on other versions)\n1080Ti (but using cpu here)", "body": "Hi everybody,\r\n\r\n`conv_transpose2d` and `conv_transpose1d` seem to be broken for certain configurations of shapes and strides - IFF by 'conv_transpose' we agree that we mean to compute the transpose/adjoint of the convolution operation with the same parameter specs. In this case, the transpose has to verify `dot(Ax, y) = dot(x, A.Ty)` for any `x, y`, where `A` is the convolution operator and the `dot`s are inner products.\r\n\r\nHere is a test that can be run for different configurations (code is copy+pasteable):\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\nfrom sklearn.utils import check_random_state\r\n\r\n\r\n\r\ndef test_convolution_adjointness(signal_spatial_shape, kernel_spatial_shape, stride, \r\n                                 conv=F.conv2d, conv_transpose=F.conv_transpose2d, random_state=42,\r\n                                 n_samples=5, n_in_channels=2, n_out_channels=3):\r\n    \"\"\"This test checks whether the operations conv_*d and conv_transpose_*d are actually tranposed/adjoint\r\n        to each other. This is important for exact gradient computations and to know exactly what operations\r\n        we are dealing with. The test checks the definition for transpose in spaces with inner product:\r\n        \r\n        If A is the linear map in question, and `dot_d` is the inner product in its domain and `dot_c` is the\r\n        inner product in its codomain (call both of them `dot`, because 1. it is clear which one is meant and\r\n        2. we use the canonical one always anyway), then its transpose A.T satisifies\r\n        \r\n        dot(Ax, y) = dot(x, A.Ty)\r\n        \r\n        for any vectors x and y chosen from the domain and codomain of A.\r\n    \"\"\"\r\n\r\n    # using numpy rng for local random seed fixing\r\n    rng = check_random_state(random_state) \r\n\r\n    # Prepare random signal and kernel\r\n    signal_shape = (n_samples, n_in_channels) + signal_spatial_shape\r\n    kernel_shape = (n_out_channels, n_in_channels) + kernel_spatial_shape\r\n    \r\n    signal = Variable(torch.from_numpy(rng.randn(*signal_shape).astype('float32')))\r\n    kernel = Variable(torch.from_numpy(rng.randn(*kernel_shape).astype('float32')))\r\n    \r\n    # convolve to get convolution output and its shape\r\n    convolved = conv(signal, kernel, stride=stride)\r\n    \r\n    # make the random vector y for the dot products\r\n    random_codomain_vector = Variable(torch.from_numpy(\r\n            rng.randn(*convolved.size()).astype('float32')))\r\n    \r\n    # compute the codomain dot product dot(Ax, y)\r\n    codomain_dot = (convolved * random_codomain_vector).resize(\r\n        n_samples, int(np.prod(convolved.size()[1:]))).sum(dim=1)\r\n    \r\n    # transposed convolution\r\n    transposed_output = conv_transpose(random_codomain_vector, kernel, stride=stride)\r\n    \r\n    # compute domain dot product dot(x, A.Ty)\r\n    domain_dot = (transposed_output * signal).resize(\r\n        n_samples, int(np.prod(signal.size()[1:]))).sum(dim=1)\r\n    \r\n    np.testing.assert_array_almost_equal(domain_dot.data.cpu().numpy(), codomain_dot.data.cpu().numpy(), decimal=3)\r\n```\r\n\r\nThe test passes for\r\n```python\r\ntest_convolution_adjointness((32, 32), (3, 3), 1)\r\ntest_convolution_adjointness((31, 31), (4, 4), 1)\r\n\r\ntest_convolution_adjointness((31, 32), (5, 4), 1)\r\ntest_convolution_adjointness((30, 31), (5, 4), 1)\r\n\r\ntest_convolution_adjointness((32, 32), (4, 4), 1)\r\ntest_convolution_adjointness((32, 32), (4, 4), 2)\r\ntest_convolution_adjointness((32, 32), (4, 4), 4)\r\n\r\ntest_convolution_adjointness((31, 31), (3, 3), 1)\r\ntest_convolution_adjointness((31, 31), (3, 3), 2)\r\ntest_convolution_adjointness((31, 31), (3, 3), 4)\r\n\r\ntest_convolution_adjointness((31, 32), (3, 4), 1)\r\ntest_convolution_adjointness((31, 32), (3, 4), 2)\r\ntest_convolution_adjointness((31, 32), (3, 4), 4)\r\n\r\n```\r\n\r\nIt looks like it always passes when the stride is 1. Further, it passes when signal axis and filter axis are both of same parity (either both odd or both even), and the stride for 32 signal size is a power of 2 less than 8.\r\n\r\n\r\nAnd it fails for\r\n```python\r\ntest_convolution_adjointness((32, 32), (3, 3), 2)\r\ntest_convolution_adjointness((31, 31), (4, 4), 2)\r\ntest_convolution_adjointness((32, 32), (4, 3), 2)\r\ntest_convolution_adjointness((31, 31), (4, 3), 2)\r\ntest_convolution_adjointness((32, 32), (3, 3), 4)\r\n\r\ntest_convolution_adjointness((32, 32), (4, 4), 3)\r\n\r\ntest_convolution_adjointness((32, 32), (4, 4), 8)\r\n```\r\n\r\nThis is when signal and filter size are not of same parity, or the stride is not of the same parity or the stride is too big.\r\n\r\nAll the failures are shape-related: The output shape is off by at least 1.\r\n\r\nSimilar things hold for `conv_transpose1d`. I did not check for 3d. Can anybody reproduce this?\r\n\r\n[[Question: Are convolution gradients computed by the same underlying mechanism? (I'm new to pytorch and don't understand the low-level code enough yet to find out. If so, this would have some implications, in case this error persisted there and passed through the meshes due to torch shape management. The recent use of 4x4 filters might make this not be an issue, though)]]\r\n\r\n\r\nspecs:\r\nubuntu 16.04\r\npytorch 0.3.0 (conda version. persists on other versions)\r\n1080Ti (but using cpu here)\r\n"}