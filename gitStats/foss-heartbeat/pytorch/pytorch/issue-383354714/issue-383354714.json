{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14305", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14305/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14305/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14305/events", "html_url": "https://github.com/pytorch/pytorch/pull/14305", "id": 383354714, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMyODQwNzk0", "number": 14305, "title": "Add (partial) autodiff support for nll_loss", "user": {"login": "asuhan", "id": 246815, "node_id": "MDQ6VXNlcjI0NjgxNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/246815?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asuhan", "html_url": "https://github.com/asuhan", "followers_url": "https://api.github.com/users/asuhan/followers", "following_url": "https://api.github.com/users/asuhan/following{/other_user}", "gists_url": "https://api.github.com/users/asuhan/gists{/gist_id}", "starred_url": "https://api.github.com/users/asuhan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asuhan/subscriptions", "organizations_url": "https://api.github.com/users/asuhan/orgs", "repos_url": "https://api.github.com/users/asuhan/repos", "events_url": "https://api.github.com/users/asuhan/events{/privacy}", "received_events_url": "https://api.github.com/users/asuhan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-22T02:08:47Z", "updated_at": "2018-11-22T02:11:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14305", "html_url": "https://github.com/pytorch/pytorch/pull/14305", "diff_url": "https://github.com/pytorch/pytorch/pull/14305.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14305.patch"}, "body_html": "<p>Not ready yet, need some comments / help with this. It's good enough for <a href=\"https://github.com/pytorch/xla\">https://github.com/pytorch/xla</a> immediate goals (forward + backward trace fusion), but there are at least two issues with it:</p>\n<ol>\n<li>If we don't allow it, <code>test/test_jit.py</code> fails to cover the change.</li>\n<li>If we allow the weight to be set, running <code>test/test_jit.py TestJitGenerated.test_nn_nll_loss</code> fails with:</li>\n</ol>\n<pre><code>======================================================================\nERROR: test_nn_nll_loss (__main__.TestJitGenerated)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"test/test_jit.py\", line 10001, in do_test\n    fn, f_args_variable, kwargs_variable, no_grad=no_grad)\n  File \"test/test_jit.py\", line 9360, in check_against_reference\n    outputs_test = self.runAndSaveRNG(func, recording_inputs, kwargs)\n  File \"test/test_jit.py\", line 425, in runAndSaveRNG\n    results = func(*inputs, **kwargs)\n  File \"test/test_jit.py\", line 9298, in script_fn\n    self.assertExportImport(CU.the_method.graph, tensors)\n  File \"test/test_jit.py\", line 415, in assertExportImport\n    self.assertExportImportModule(m, inputs)\n  File \"test/test_jit.py\", line 419, in assertExportImportModule\n    self.assertEqual(self.runAndSaveRNG(m.forward, inputs),\n  File \"test/test_jit.py\", line 425, in runAndSaveRNG\n    results = func(*inputs, **kwargs)\nRuntimeError: \narguments for call are not valid:\n  \n  for operator aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor out) -&gt; Tensor:\n  expected a value of type Tensor for argument 'total_weight' but found bool\n  &lt;internally-created-node&gt;\n  ~ &lt;--- HERE\n  \n  for operator aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -&gt; Tensor:\n  expected a value of type Tensor for argument 'total_weight' but found bool\n  &lt;internally-created-node&gt;\n  ~ &lt;--- HERE\nfor call at:\n&lt;internally-created-node&gt;\n~ &lt;--- HERE\n</code></pre>", "body_text": "Not ready yet, need some comments / help with this. It's good enough for https://github.com/pytorch/xla immediate goals (forward + backward trace fusion), but there are at least two issues with it:\n\nIf we don't allow it, test/test_jit.py fails to cover the change.\nIf we allow the weight to be set, running test/test_jit.py TestJitGenerated.test_nn_nll_loss fails with:\n\n======================================================================\nERROR: test_nn_nll_loss (__main__.TestJitGenerated)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"test/test_jit.py\", line 10001, in do_test\n    fn, f_args_variable, kwargs_variable, no_grad=no_grad)\n  File \"test/test_jit.py\", line 9360, in check_against_reference\n    outputs_test = self.runAndSaveRNG(func, recording_inputs, kwargs)\n  File \"test/test_jit.py\", line 425, in runAndSaveRNG\n    results = func(*inputs, **kwargs)\n  File \"test/test_jit.py\", line 9298, in script_fn\n    self.assertExportImport(CU.the_method.graph, tensors)\n  File \"test/test_jit.py\", line 415, in assertExportImport\n    self.assertExportImportModule(m, inputs)\n  File \"test/test_jit.py\", line 419, in assertExportImportModule\n    self.assertEqual(self.runAndSaveRNG(m.forward, inputs),\n  File \"test/test_jit.py\", line 425, in runAndSaveRNG\n    results = func(*inputs, **kwargs)\nRuntimeError: \narguments for call are not valid:\n  \n  for operator aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor out) -> Tensor:\n  expected a value of type Tensor for argument 'total_weight' but found bool\n  <internally-created-node>\n  ~ <--- HERE\n  \n  for operator aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor:\n  expected a value of type Tensor for argument 'total_weight' but found bool\n  <internally-created-node>\n  ~ <--- HERE\nfor call at:\n<internally-created-node>\n~ <--- HERE", "body": "Not ready yet, need some comments / help with this. It's good enough for https://github.com/pytorch/xla immediate goals (forward + backward trace fusion), but there are at least two issues with it:\r\n\r\n1. If we don't allow it, `test/test_jit.py` fails to cover the change.\r\n2. If we allow the weight to be set, running `test/test_jit.py TestJitGenerated.test_nn_nll_loss` fails with:\r\n\r\n```\r\n======================================================================\r\nERROR: test_nn_nll_loss (__main__.TestJitGenerated)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test/test_jit.py\", line 10001, in do_test\r\n    fn, f_args_variable, kwargs_variable, no_grad=no_grad)\r\n  File \"test/test_jit.py\", line 9360, in check_against_reference\r\n    outputs_test = self.runAndSaveRNG(func, recording_inputs, kwargs)\r\n  File \"test/test_jit.py\", line 425, in runAndSaveRNG\r\n    results = func(*inputs, **kwargs)\r\n  File \"test/test_jit.py\", line 9298, in script_fn\r\n    self.assertExportImport(CU.the_method.graph, tensors)\r\n  File \"test/test_jit.py\", line 415, in assertExportImport\r\n    self.assertExportImportModule(m, inputs)\r\n  File \"test/test_jit.py\", line 419, in assertExportImportModule\r\n    self.assertEqual(self.runAndSaveRNG(m.forward, inputs),\r\n  File \"test/test_jit.py\", line 425, in runAndSaveRNG\r\n    results = func(*inputs, **kwargs)\r\nRuntimeError: \r\narguments for call are not valid:\r\n  \r\n  for operator aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor for argument 'total_weight' but found bool\r\n  <internally-created-node>\r\n  ~ <--- HERE\r\n  \r\n  for operator aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor:\r\n  expected a value of type Tensor for argument 'total_weight' but found bool\r\n  <internally-created-node>\r\n  ~ <--- HERE\r\nfor call at:\r\n<internally-created-node>\r\n~ <--- HERE\r\n```\r\n"}