{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7280", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7280/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7280/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7280/events", "html_url": "https://github.com/pytorch/pytorch/issues/7280", "id": 320228466, "node_id": "MDU6SXNzdWUzMjAyMjg0NjY=", "number": 7280, "title": "[BUG] use dropout in multi-LSTM(GRU)  when GPU index is not '0',  will cause \"cublas runtime error\" [pytorch0.4.0]", "user": {"login": "huguyuehuhu", "id": 25881545, "node_id": "MDQ6VXNlcjI1ODgxNTQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/25881545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huguyuehuhu", "html_url": "https://github.com/huguyuehuhu", "followers_url": "https://api.github.com/users/huguyuehuhu/followers", "following_url": "https://api.github.com/users/huguyuehuhu/following{/other_user}", "gists_url": "https://api.github.com/users/huguyuehuhu/gists{/gist_id}", "starred_url": "https://api.github.com/users/huguyuehuhu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huguyuehuhu/subscriptions", "organizations_url": "https://api.github.com/users/huguyuehuhu/orgs", "repos_url": "https://api.github.com/users/huguyuehuhu/repos", "events_url": "https://api.github.com/users/huguyuehuhu/events{/privacy}", "received_events_url": "https://api.github.com/users/huguyuehuhu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-05-04T10:40:41Z", "updated_at": "2018-05-14T20:14:08Z", "closed_at": "2018-05-14T20:14:08Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>In  <code>pytorch0.4.0</code>,<br>\nuse dropout in multi-LSTM(GRU)  when GPU index is not '0', will cause <code> RuntimeError: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:411</code> .<br>\nfor example:</p>\n<ol>\n<li>if set gpu index as <strong>larger than</strong> 0 (<code>device = 2</code>), <strong>with</strong> dropout(<code>dropout = 0.5</code>), <code>num_layers = 2</code>: cause error;</li>\n<li>if set gpu index as <strong>larger than</strong> 0 (<code>device = 2</code>), <strong>without</strong> dropout(<code>dropout = 0</code>), <code>num_layers = 2</code>:  cause no error;</li>\n<li>if set gpu index as  <strong>0</strong> (<code>device = 0</code>), <strong>no matter dropout or not</strong>,  <code>num_layers = 2</code>: cause no error;</li>\n<li><code>num_layers = 1</code>: all cases is okay.</li>\n</ol>\n<p>In pytorch, because <strong>1 layer LSTM has no dropout</strong>, so all cases is okay. Therefore I guess there is a bug at where  dropout is in LSTM(GRU). but  I can't solve it . Can you do me a help?</p>\n<h2>Code example</h2>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nclass test_model(nn.Module):\n    def __init__(self,\n                 in_channel=3,\n                 num_class=60,\n                 seq_length=10,\n                 hidden_size =100,\n                 num_layers=2,\n                 dropout=0.5\n                 ):\n        super(test_model, self).__init__()\n        self.lstm = nn.LSTM(input_size=in_channel,\n                             hidden_size=hidden_size,\n                             num_layers=num_layers,\n                             batch_first=True,\n                             bidirectional=False,\n                             dropout=dropout\n                            )\n        self.fc = nn.Linear(hidden_size*seq_length,num_class)\n\n    def forward(self, x):\n        out,_ = self.lstm(x)\n        out = out.contiguous().view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nif __name__ == '__main__':\n    device = 2\n    num_layers = 2 # 1\n    dropout = 0.5 # 0\n\n    # cronstruct a dataset\n    data = []\n    label = []\n    epoches = 10\n    seq_length = 10\n    cuda = True\n    for i in range(epoches):\n        data.append(torch.randn((2, 10, 3)))\n        label.append(torch.randn((2, 60)))\n\n    # model\n    model = test_model(num_layers=num_layers,dropout=dropout)\n    if cuda:\n        model= model.cuda(device=device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # cre = nn.CrossEntropyLoss()\n    cre = nn.MSELoss()\n\n    for i in range(epoches):\n        if cuda:\n            x = Variable(data[i]).cuda(device=device)\n            y = Variable(label[i]).cuda(device=device)\n        else:\n            x = Variable(data[i])\n            y = Variable(label[i])\n\n        out_put = model(x)\n        loss = cre(out_put, y)\n        # print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print('end')\n\n</code></pre>", "body_text": "Issue description\nIn  pytorch0.4.0,\nuse dropout in multi-LSTM(GRU)  when GPU index is not '0', will cause  RuntimeError: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:411 .\nfor example:\n\nif set gpu index as larger than 0 (device = 2), with dropout(dropout = 0.5), num_layers = 2: cause error;\nif set gpu index as larger than 0 (device = 2), without dropout(dropout = 0), num_layers = 2:  cause no error;\nif set gpu index as  0 (device = 0), no matter dropout or not,  num_layers = 2: cause no error;\nnum_layers = 1: all cases is okay.\n\nIn pytorch, because 1 layer LSTM has no dropout, so all cases is okay. Therefore I guess there is a bug at where  dropout is in LSTM(GRU). but  I can't solve it . Can you do me a help?\nCode example\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nclass test_model(nn.Module):\n    def __init__(self,\n                 in_channel=3,\n                 num_class=60,\n                 seq_length=10,\n                 hidden_size =100,\n                 num_layers=2,\n                 dropout=0.5\n                 ):\n        super(test_model, self).__init__()\n        self.lstm = nn.LSTM(input_size=in_channel,\n                             hidden_size=hidden_size,\n                             num_layers=num_layers,\n                             batch_first=True,\n                             bidirectional=False,\n                             dropout=dropout\n                            )\n        self.fc = nn.Linear(hidden_size*seq_length,num_class)\n\n    def forward(self, x):\n        out,_ = self.lstm(x)\n        out = out.contiguous().view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nif __name__ == '__main__':\n    device = 2\n    num_layers = 2 # 1\n    dropout = 0.5 # 0\n\n    # cronstruct a dataset\n    data = []\n    label = []\n    epoches = 10\n    seq_length = 10\n    cuda = True\n    for i in range(epoches):\n        data.append(torch.randn((2, 10, 3)))\n        label.append(torch.randn((2, 60)))\n\n    # model\n    model = test_model(num_layers=num_layers,dropout=dropout)\n    if cuda:\n        model= model.cuda(device=device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # cre = nn.CrossEntropyLoss()\n    cre = nn.MSELoss()\n\n    for i in range(epoches):\n        if cuda:\n            x = Variable(data[i]).cuda(device=device)\n            y = Variable(label[i]).cuda(device=device)\n        else:\n            x = Variable(data[i])\n            y = Variable(label[i])\n\n        out_put = model(x)\n        loss = cre(out_put, y)\n        # print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print('end')", "body": "## Issue description\r\nIn  `pytorch0.4.0`,\r\nuse dropout in multi-LSTM(GRU)  when GPU index is not '0', will cause ` RuntimeError: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:411` .\r\nfor example:\r\n1. if set gpu index as **larger than** 0 (`device = 2`), **with** dropout(`dropout = 0.5`), `num_layers = 2`: cause error;\r\n2. if set gpu index as **larger than** 0 (`device = 2`), **without** dropout(`dropout = 0`), `num_layers = 2`:  cause no error;\r\n3. if set gpu index as  **0** (`device = 0`), **no matter dropout or not**,  `num_layers = 2`: cause no error;\r\n4. `num_layers = 1`: all cases is okay.\r\n\r\nIn pytorch, because **1 layer LSTM has no dropout**, so all cases is okay. Therefore I guess there is a bug at where  dropout is in LSTM(GRU). but  I can't solve it . Can you do me a help?\r\n\r\n## Code example\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\n\r\nclass test_model(nn.Module):\r\n    def __init__(self,\r\n                 in_channel=3,\r\n                 num_class=60,\r\n                 seq_length=10,\r\n                 hidden_size =100,\r\n                 num_layers=2,\r\n                 dropout=0.5\r\n                 ):\r\n        super(test_model, self).__init__()\r\n        self.lstm = nn.LSTM(input_size=in_channel,\r\n                             hidden_size=hidden_size,\r\n                             num_layers=num_layers,\r\n                             batch_first=True,\r\n                             bidirectional=False,\r\n                             dropout=dropout\r\n                            )\r\n        self.fc = nn.Linear(hidden_size*seq_length,num_class)\r\n\r\n    def forward(self, x):\r\n        out,_ = self.lstm(x)\r\n        out = out.contiguous().view(out.size(0), -1)\r\n        out = self.fc(out)\r\n        return out\r\n\r\nif __name__ == '__main__':\r\n    device = 2\r\n    num_layers = 2 # 1\r\n    dropout = 0.5 # 0\r\n\r\n    # cronstruct a dataset\r\n    data = []\r\n    label = []\r\n    epoches = 10\r\n    seq_length = 10\r\n    cuda = True\r\n    for i in range(epoches):\r\n        data.append(torch.randn((2, 10, 3)))\r\n        label.append(torch.randn((2, 60)))\r\n\r\n    # model\r\n    model = test_model(num_layers=num_layers,dropout=dropout)\r\n    if cuda:\r\n        model= model.cuda(device=device)\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n\r\n    # cre = nn.CrossEntropyLoss()\r\n    cre = nn.MSELoss()\r\n\r\n    for i in range(epoches):\r\n        if cuda:\r\n            x = Variable(data[i]).cuda(device=device)\r\n            y = Variable(label[i]).cuda(device=device)\r\n        else:\r\n            x = Variable(data[i])\r\n            y = Variable(label[i])\r\n\r\n        out_put = model(x)\r\n        loss = cre(out_put, y)\r\n        # print(loss)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n    print('end')\r\n\r\n```"}