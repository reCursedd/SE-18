{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10140", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10140/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10140/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10140/events", "html_url": "https://github.com/pytorch/pytorch/issues/10140", "id": 346759809, "node_id": "MDU6SXNzdWUzNDY3NTk4MDk=", "number": 10140, "title": "[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.", "user": {"login": "hagerrady13", "id": 10363680, "node_id": "MDQ6VXNlcjEwMzYzNjgw", "avatar_url": "https://avatars2.githubusercontent.com/u/10363680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hagerrady13", "html_url": "https://github.com/hagerrady13", "followers_url": "https://api.github.com/users/hagerrady13/followers", "following_url": "https://api.github.com/users/hagerrady13/following{/other_user}", "gists_url": "https://api.github.com/users/hagerrady13/gists{/gist_id}", "starred_url": "https://api.github.com/users/hagerrady13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hagerrady13/subscriptions", "organizations_url": "https://api.github.com/users/hagerrady13/orgs", "repos_url": "https://api.github.com/users/hagerrady13/repos", "events_url": "https://api.github.com/users/hagerrady13/events{/privacy}", "received_events_url": "https://api.github.com/users/hagerrady13/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-08-01T20:39:28Z", "updated_at": "2018-08-07T11:32:24Z", "closed_at": "2018-08-07T11:32:24Z", "author_association": "NONE", "body_html": "<p>I have a <a href=\"https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py\">UNet model</a> that I train on a [2x80x608] image. The model is trained on a TitanX pascal GPU for 100 epochs. When I test the trained model on a an intel i5 machine with no gpu, inference time grows as I train the model for more epochs.<br>\nFor instance, I get 500 ms if I load a checkpoint trained for 10 epochs while i get 5 seconds if I load another checkpoint saved after 100 epochs. This is not exclusive to the intel i5 cpu and the same behavior still holds on a Xeon CPU.</p>\n<p>Note that the checkpoint sizes are the same no matter what epoch it is saved at.</p>\n<p><strong>Can anyone explain this behavior?</strong></p>\n<p>System Info:</p>\n<ul>\n<li>PyTorch is installed with Conda, version 0.4</li>\n<li>Ubuntu 16</li>\n<li>CUDA 9.0 and cuDNN 7.1</li>\n<li>Python version: 3.6</li>\n</ul>", "body_text": "I have a UNet model that I train on a [2x80x608] image. The model is trained on a TitanX pascal GPU for 100 epochs. When I test the trained model on a an intel i5 machine with no gpu, inference time grows as I train the model for more epochs.\nFor instance, I get 500 ms if I load a checkpoint trained for 10 epochs while i get 5 seconds if I load another checkpoint saved after 100 epochs. This is not exclusive to the intel i5 cpu and the same behavior still holds on a Xeon CPU.\nNote that the checkpoint sizes are the same no matter what epoch it is saved at.\nCan anyone explain this behavior?\nSystem Info:\n\nPyTorch is installed with Conda, version 0.4\nUbuntu 16\nCUDA 9.0 and cuDNN 7.1\nPython version: 3.6", "body": "I have a [UNet model](https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py) that I train on a [2x80x608] image. The model is trained on a TitanX pascal GPU for 100 epochs. When I test the trained model on a an intel i5 machine with no gpu, inference time grows as I train the model for more epochs. \r\nFor instance, I get 500 ms if I load a checkpoint trained for 10 epochs while i get 5 seconds if I load another checkpoint saved after 100 epochs. This is not exclusive to the intel i5 cpu and the same behavior still holds on a Xeon CPU. \r\n\r\nNote that the checkpoint sizes are the same no matter what epoch it is saved at.\r\n\r\n**Can anyone explain this behavior?**\r\n\r\nSystem Info:\r\n- PyTorch is installed with Conda, version 0.4\r\n- Ubuntu 16\r\n- CUDA 9.0 and cuDNN 7.1\r\n- Python version: 3.6\r\n"}