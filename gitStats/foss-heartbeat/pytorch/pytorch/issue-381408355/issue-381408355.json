{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14055", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14055/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14055/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14055/events", "html_url": "https://github.com/pytorch/pytorch/pull/14055", "id": 381408355, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMxMzg4MzMw", "number": 14055, "title": "Allow graph fuser to move chunks past multiple nodes.", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-16T01:25:13Z", "updated_at": "2018-11-23T15:55:23Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14055", "html_url": "https://github.com/pytorch/pytorch/pull/14055", "diff_url": "https://github.com/pytorch/pytorch/pull/14055.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14055.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #12290.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"366466433\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12290\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12290/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12290\">#12290</a>. Also speeds up JIT LSTM forward pass from 8.8ms to 7.8ms; previously, each JIT lstm cell used 2 fused kernels. Now, it only uses one fused kernel (which is how many kernels cudnn uses).</p>\n<p>Explanation:</p>\n<p>Let f, g, h be fusible ops.</p>\n<pre><code>x = f(v, w)\nz = g(x, y)\na, b = chunk(z)\nc = h(a, b)\n</code></pre>\n<p>becomes (before this PR):</p>\n<pre><code>x = f(v, w)\nx', y' = broadcast_tensors([x, y])\nax, bx = chunk(x')\nay, by = chunk(y')\na = g(ax, ay)\nb = g(bx, by)\nc = h(a, b)\n</code></pre>\n<p>The graph fuser then puts g, g, and h into one FusionGroup and is unable<br>\nto move <code>x = f(v, w)</code> into the FusionGroup.</p>\n<p>This PR lets the graph fuser move <code>x = f(v, w)</code> into the FusionGroup.<br>\nIt does this by abstracting the broadcast_tensors + multiple chunk nodes<br>\ninto one intermediate <code>prim::BroadcastingChunk[chunks, dim]</code> node.</p>\n<p>A <code>BroadcastingChunk[chunks, dim](*inputs)</code> node is equivalent to:</p>\n<ul>\n<li>broadcasting all of *inputs</li>\n<li>chunk-ing each broadcasted input into <code>chunks</code> chunks along dim <code>dim</code>.</li>\n</ul>\n<p>Abstracting the broadcasting chunk behavior away, it is now a lot easier<br>\nfor the graph fuser to move (broadcast + chunk) past an operation. After<br>\nthis PR, the above graph becomes:</p>\n<pre><code>x = f(v, w)\nax, bx, ay, by = BroadcastingChunk(x, y)\na = g(ax, ay)\nb = g(bx, by)\nc = h(a, b)\n</code></pre>\n<p>Now, to move <code>x = f(v, w)</code> after the BroadcastingChunk, one just needs<br>\nto add f's operands to the BroadcastingChunk:</p>\n<pre><code>ay, by, av, bv, aw, bw = BroadcastingChunk(y, v, w)\nax = f(av, aw)\nby = f(bv, bw)\na = g(ax, ay)\nb = g(bx, by)\nc = h(a, b)\n</code></pre>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a></p>", "body_text": "Fixes #12290. Also speeds up JIT LSTM forward pass from 8.8ms to 7.8ms; previously, each JIT lstm cell used 2 fused kernels. Now, it only uses one fused kernel (which is how many kernels cudnn uses).\nExplanation:\nLet f, g, h be fusible ops.\nx = f(v, w)\nz = g(x, y)\na, b = chunk(z)\nc = h(a, b)\n\nbecomes (before this PR):\nx = f(v, w)\nx', y' = broadcast_tensors([x, y])\nax, bx = chunk(x')\nay, by = chunk(y')\na = g(ax, ay)\nb = g(bx, by)\nc = h(a, b)\n\nThe graph fuser then puts g, g, and h into one FusionGroup and is unable\nto move x = f(v, w) into the FusionGroup.\nThis PR lets the graph fuser move x = f(v, w) into the FusionGroup.\nIt does this by abstracting the broadcast_tensors + multiple chunk nodes\ninto one intermediate prim::BroadcastingChunk[chunks, dim] node.\nA BroadcastingChunk[chunks, dim](*inputs) node is equivalent to:\n\nbroadcasting all of *inputs\nchunk-ing each broadcasted input into chunks chunks along dim dim.\n\nAbstracting the broadcasting chunk behavior away, it is now a lot easier\nfor the graph fuser to move (broadcast + chunk) past an operation. After\nthis PR, the above graph becomes:\nx = f(v, w)\nax, bx, ay, by = BroadcastingChunk(x, y)\na = g(ax, ay)\nb = g(bx, by)\nc = h(a, b)\n\nNow, to move x = f(v, w) after the BroadcastingChunk, one just needs\nto add f's operands to the BroadcastingChunk:\nay, by, av, bv, aw, bw = BroadcastingChunk(y, v, w)\nax = f(av, aw)\nby = f(bv, bw)\na = g(ax, ay)\nb = g(bx, by)\nc = h(a, b)\n\ncc @apaszke @mruberry @zdevito", "body": "Fixes #12290. Also speeds up JIT LSTM forward pass from 8.8ms to 7.8ms; previously, each JIT lstm cell used 2 fused kernels. Now, it only uses one fused kernel (which is how many kernels cudnn uses).\r\n\r\nExplanation:\r\n\r\nLet f, g, h be fusible ops.\r\n```\r\nx = f(v, w)\r\nz = g(x, y)\r\na, b = chunk(z)\r\nc = h(a, b)\r\n```\r\nbecomes (before this PR):\r\n```\r\nx = f(v, w)\r\nx', y' = broadcast_tensors([x, y])\r\nax, bx = chunk(x')\r\nay, by = chunk(y')\r\na = g(ax, ay)\r\nb = g(bx, by)\r\nc = h(a, b)\r\n```\r\nThe graph fuser then puts g, g, and h into one FusionGroup and is unable\r\nto move `x = f(v, w)` into the FusionGroup.\r\n\r\nThis PR lets the graph fuser move `x = f(v, w)` into the FusionGroup.\r\nIt does this by abstracting the broadcast_tensors + multiple chunk nodes\r\ninto one intermediate `prim::BroadcastingChunk[chunks, dim]` node.\r\n\r\nA `BroadcastingChunk[chunks, dim](*inputs)` node is equivalent to:\r\n- broadcasting all of *inputs\r\n- chunk-ing each broadcasted input into `chunks` chunks along dim `dim`.\r\n\r\nAbstracting the broadcasting chunk behavior away, it is now a lot easier\r\nfor the graph fuser to move (broadcast + chunk) past an operation. After\r\nthis PR, the above graph becomes:\r\n```\r\nx = f(v, w)\r\nax, bx, ay, by = BroadcastingChunk(x, y)\r\na = g(ax, ay)\r\nb = g(bx, by)\r\nc = h(a, b)\r\n```\r\nNow, to move `x = f(v, w)` after the BroadcastingChunk, one just needs\r\nto add f's operands to the BroadcastingChunk:\r\n```\r\nay, by, av, bv, aw, bw = BroadcastingChunk(y, v, w)\r\nax = f(av, aw)\r\nby = f(bv, bw)\r\na = g(ax, ay)\r\nb = g(bx, by)\r\nc = h(a, b)\r\n```\r\n\r\ncc @apaszke @mruberry @zdevito \r\n"}