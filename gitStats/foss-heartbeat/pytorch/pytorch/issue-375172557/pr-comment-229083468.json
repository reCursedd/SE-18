{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229083468", "pull_request_review_id": 169499069, "id": 229083468, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTA4MzQ2OA==", "diff_hunk": "@@ -702,4 +702,113 @@ Tensor remainder(const Tensor & self, const Tensor & other) {\n   return at::_th_remainder(self, other);\n }\n \n+Tensor & min_out(Tensor & result, const Tensor & self, const Tensor & other) {\n+  return at::_th_min_out(result, self, other);\n+}\n+\n+Tensor min(const Tensor & self, const Tensor & other) {\n+  return at::_th_min(self, other);\n+}\n+\n+Tensor min(const Tensor & self) {\n+  return at::_th_min(self);\n+}\n+\n+Tensor & max_out(Tensor & result, const Tensor & self, const Tensor & other) {\n+  return at::_th_max_out(result, self, other);\n+}\n+Tensor max(const Tensor & self, const Tensor & other) {\n+  return at::_th_max(self, other);\n+}\n+\n+Tensor max(const Tensor & self) {\n+  return at::_th_max(self);\n+}\n+\n+Tensor median(const Tensor & self) {\n+  return at::_th_median(self);\n+}\n+\n+std::tuple<Tensor &,Tensor &> sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {\n+  return at::_th_sort_out(values, indices, self, dim, descending);\n+}\n+\n+std::tuple<Tensor,Tensor> sort(const Tensor & self, int64_t dim, bool descending) {\n+  return at::_th_sort(self, dim, descending);\n+}\n+std::tuple<Tensor &,Tensor &> topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {\n+  return at::_th_topk_out(values, indices, self, k, dim, largest, sorted);\n+}\n+\n+std::tuple<Tensor,Tensor> topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {\n+  return at::_th_topk(self, k, dim, largest, sorted);\n+}\n+\n+Tensor all(const Tensor & self) {\n+  return at::_th_all(self);\n+}\n+\n+Tensor any(const Tensor & self) {\n+  return at::_th_any(self);\n+}\n+\n+Tensor & renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {\n+  return at::_th_renorm_out(result, self, p, dim, maxnorm);\n+}\n+\n+Tensor renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {\n+  return at::_th_renorm(self, p, dim, maxnorm);\n+}\n+\n+Tensor unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) {\n+  return self._th_unfold(dimension, size, step);\n+}\n+\n+bool equal(const Tensor & self, const Tensor & other) {\n+  return at::_th_equal(self, other);\n+}\n+\n+Tensor & pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) {\n+  return at::_th_pow_out(result, self, exponent);\n+}\n+\n+Tensor pow(const Tensor & self, const Tensor & exponent) {\n+  return at::_th_pow(self, exponent);\n+}\n+Tensor & pow_out(Tensor & result, Scalar self, const Tensor & exponent) {\n+  return at::_th_pow_out(result, self, exponent);\n+}\n+\n+Tensor pow(Scalar self, const Tensor & exponent) {\n+  return at::_th_pow(self, exponent);\n+}\n+\n+Tensor & normal_out(Tensor & output, const Tensor & mean, double std, Generator * generator) {\n+  return at::_th_normal_out(output, mean, std, generator);\n+}\n+\n+Tensor normal(const Tensor & mean, double std, Generator * generator) {\n+  return at::_th_normal(mean, std, generator);\n+}\n+\n+Tensor & normal_out(Tensor & output, double mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal_out(output, mean, std, generator);\n+}\n+\n+Tensor normal(double mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal(mean, std, generator);\n+}\n+\n+Tensor & normal_out(Tensor & output, const Tensor & mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal_out(output, mean, std, generator);\n+}\n+\n+Tensor normal(const Tensor & mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal(mean, std, generator);\n+}\n+\n+Tensor alias(const Tensor & self) {\n+  return at::_th_alias(self);", "path": "aten/src/ATen/native/LegacyDefinitions.cpp", "position": 110, "original_position": 110, "commit_id": "3ed81c0fae01072a23f6591d440318a4bf9e63e5", "original_commit_id": "3ed81c0fae01072a23f6591d440318a4bf9e63e5", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "we had a mechanism to do this at one point ('is_type_dispatched'), which would pass 'this' (from Type) along to native function to avoid having to re-dispatch.  The issue is that adds Type as part of the schema when it's really an implementation detail.\r\n\r\nI think a better approach is to just look through the list in LegacyDefinitions.cpp and figure out which ones actually matter in terms of having low overhead and reimplement those as _real_ native functions.", "created_at": "2018-10-29T20:21:18Z", "updated_at": "2018-11-23T15:53:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/13262#discussion_r229083468", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13262", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229083468"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13262#discussion_r229083468"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13262"}}, "body_html": "<p>we had a mechanism to do this at one point ('is_type_dispatched'), which would pass 'this' (from Type) along to native function to avoid having to re-dispatch.  The issue is that adds Type as part of the schema when it's really an implementation detail.</p>\n<p>I think a better approach is to just look through the list in LegacyDefinitions.cpp and figure out which ones actually matter in terms of having low overhead and reimplement those as <em>real</em> native functions.</p>", "body_text": "we had a mechanism to do this at one point ('is_type_dispatched'), which would pass 'this' (from Type) along to native function to avoid having to re-dispatch.  The issue is that adds Type as part of the schema when it's really an implementation detail.\nI think a better approach is to just look through the list in LegacyDefinitions.cpp and figure out which ones actually matter in terms of having low overhead and reimplement those as real native functions.", "in_reply_to_id": 229063654}