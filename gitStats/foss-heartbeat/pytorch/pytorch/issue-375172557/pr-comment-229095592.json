{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229095592", "pull_request_review_id": 169514021, "id": 229095592, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTA5NTU5Mg==", "diff_hunk": "@@ -702,4 +702,113 @@ Tensor remainder(const Tensor & self, const Tensor & other) {\n   return at::_th_remainder(self, other);\n }\n \n+Tensor & min_out(Tensor & result, const Tensor & self, const Tensor & other) {\n+  return at::_th_min_out(result, self, other);\n+}\n+\n+Tensor min(const Tensor & self, const Tensor & other) {\n+  return at::_th_min(self, other);\n+}\n+\n+Tensor min(const Tensor & self) {\n+  return at::_th_min(self);\n+}\n+\n+Tensor & max_out(Tensor & result, const Tensor & self, const Tensor & other) {\n+  return at::_th_max_out(result, self, other);\n+}\n+Tensor max(const Tensor & self, const Tensor & other) {\n+  return at::_th_max(self, other);\n+}\n+\n+Tensor max(const Tensor & self) {\n+  return at::_th_max(self);\n+}\n+\n+Tensor median(const Tensor & self) {\n+  return at::_th_median(self);\n+}\n+\n+std::tuple<Tensor &,Tensor &> sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {\n+  return at::_th_sort_out(values, indices, self, dim, descending);\n+}\n+\n+std::tuple<Tensor,Tensor> sort(const Tensor & self, int64_t dim, bool descending) {\n+  return at::_th_sort(self, dim, descending);\n+}\n+std::tuple<Tensor &,Tensor &> topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {\n+  return at::_th_topk_out(values, indices, self, k, dim, largest, sorted);\n+}\n+\n+std::tuple<Tensor,Tensor> topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {\n+  return at::_th_topk(self, k, dim, largest, sorted);\n+}\n+\n+Tensor all(const Tensor & self) {\n+  return at::_th_all(self);\n+}\n+\n+Tensor any(const Tensor & self) {\n+  return at::_th_any(self);\n+}\n+\n+Tensor & renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {\n+  return at::_th_renorm_out(result, self, p, dim, maxnorm);\n+}\n+\n+Tensor renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {\n+  return at::_th_renorm(self, p, dim, maxnorm);\n+}\n+\n+Tensor unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) {\n+  return self._th_unfold(dimension, size, step);\n+}\n+\n+bool equal(const Tensor & self, const Tensor & other) {\n+  return at::_th_equal(self, other);\n+}\n+\n+Tensor & pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) {\n+  return at::_th_pow_out(result, self, exponent);\n+}\n+\n+Tensor pow(const Tensor & self, const Tensor & exponent) {\n+  return at::_th_pow(self, exponent);\n+}\n+Tensor & pow_out(Tensor & result, Scalar self, const Tensor & exponent) {\n+  return at::_th_pow_out(result, self, exponent);\n+}\n+\n+Tensor pow(Scalar self, const Tensor & exponent) {\n+  return at::_th_pow(self, exponent);\n+}\n+\n+Tensor & normal_out(Tensor & output, const Tensor & mean, double std, Generator * generator) {\n+  return at::_th_normal_out(output, mean, std, generator);\n+}\n+\n+Tensor normal(const Tensor & mean, double std, Generator * generator) {\n+  return at::_th_normal(mean, std, generator);\n+}\n+\n+Tensor & normal_out(Tensor & output, double mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal_out(output, mean, std, generator);\n+}\n+\n+Tensor normal(double mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal(mean, std, generator);\n+}\n+\n+Tensor & normal_out(Tensor & output, const Tensor & mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal_out(output, mean, std, generator);\n+}\n+\n+Tensor normal(const Tensor & mean, const Tensor & std, Generator * generator) {\n+  return at::_th_normal(mean, std, generator);\n+}\n+\n+Tensor alias(const Tensor & self) {\n+  return at::_th_alias(self);", "path": "aten/src/ATen/native/LegacyDefinitions.cpp", "position": 110, "original_position": 110, "commit_id": "3ed81c0fae01072a23f6591d440318a4bf9e63e5", "original_commit_id": "3ed81c0fae01072a23f6591d440318a4bf9e63e5", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "I've actually been implementing the functions that matter by special casing them instead of writing native functions (see get_device, is_cuda, and is_sparse) but yeah that makes sense.", "created_at": "2018-10-29T20:56:30Z", "updated_at": "2018-11-23T15:53:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/13262#discussion_r229095592", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13262", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229095592"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13262#discussion_r229095592"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13262"}}, "body_html": "<p>I've actually been implementing the functions that matter by special casing them instead of writing native functions (see get_device, is_cuda, and is_sparse) but yeah that makes sense.</p>", "body_text": "I've actually been implementing the functions that matter by special casing them instead of writing native functions (see get_device, is_cuda, and is_sparse) but yeah that makes sense.", "in_reply_to_id": 229063654}