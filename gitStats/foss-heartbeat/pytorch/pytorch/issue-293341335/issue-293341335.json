{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4974", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4974/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4974/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4974/events", "html_url": "https://github.com/pytorch/pytorch/issues/4974", "id": 293341335, "node_id": "MDU6SXNzdWUyOTMzNDEzMzU=", "number": 4974, "title": "torch.sum() ignores stride in out tensor", "user": {"login": "lvdmaaten", "id": 1168046, "node_id": "MDQ6VXNlcjExNjgwNDY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1168046?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lvdmaaten", "html_url": "https://github.com/lvdmaaten", "followers_url": "https://api.github.com/users/lvdmaaten/followers", "following_url": "https://api.github.com/users/lvdmaaten/following{/other_user}", "gists_url": "https://api.github.com/users/lvdmaaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/lvdmaaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lvdmaaten/subscriptions", "organizations_url": "https://api.github.com/users/lvdmaaten/orgs", "repos_url": "https://api.github.com/users/lvdmaaten/repos", "events_url": "https://api.github.com/users/lvdmaaten/events{/privacy}", "received_events_url": "https://api.github.com/users/lvdmaaten/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-01-31T22:51:57Z", "updated_at": "2018-02-06T15:50:28Z", "closed_at": "2018-02-06T15:50:28Z", "author_association": "NONE", "body_html": "<p>The <code>torch.sum()</code> function writes its outputs in erroneous locations when the out <code>kwarg</code> is used. Repro:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\ni <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\na <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>)\nb <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>)\n\nac <span class=\"pl-k\">=</span> a.clone()\nac[:, i].copy_(b.sum(<span class=\"pl-c1\">0</span>))\n<span class=\"pl-c1\">print</span>(ac)\n\nac <span class=\"pl-k\">=</span> a.clone()\nb.sum(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">out</span><span class=\"pl-k\">=</span>ac[:, i])\n<span class=\"pl-c1\">print</span>(ac)</pre></div>\n<p>Output:</p>\n<div class=\"highlight highlight-source-shell\"><pre>lvdmaaten-mbp:Desktop lvdmaaten$ python bug.py\n0.0000 -0.1515 0.0000\n0.0000 1.8761 0.0000\n0.0000 -1.7563 0.0000\n0.0000 0.5194 0.0000\n0.0000 1.5322 0.0000\n[torch.FloatTensor of size 5x3]\n0.0000 -0.1515 1.8761\n-1.7563 0.5194 1.5322\n0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000\n[torch.FloatTensor of size 5x3]</pre></div>\n<p>I presume what happens is that <code>torch.sum()</code> does not respect the stride of <code>a[:, i]</code> when putting outputs in place.</p>\n<p>Tested on PyTorch version 0.3.0.post4.</p>", "body_text": "The torch.sum() function writes its outputs in erroneous locations when the out kwarg is used. Repro:\nimport torch\n\ni = 1\na = torch.zeros(5, 3)\nb = torch.randn(3, 5)\n\nac = a.clone()\nac[:, i].copy_(b.sum(0))\nprint(ac)\n\nac = a.clone()\nb.sum(0, out=ac[:, i])\nprint(ac)\nOutput:\nlvdmaaten-mbp:Desktop lvdmaaten$ python bug.py\n0.0000 -0.1515 0.0000\n0.0000 1.8761 0.0000\n0.0000 -1.7563 0.0000\n0.0000 0.5194 0.0000\n0.0000 1.5322 0.0000\n[torch.FloatTensor of size 5x3]\n0.0000 -0.1515 1.8761\n-1.7563 0.5194 1.5322\n0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000\n[torch.FloatTensor of size 5x3]\nI presume what happens is that torch.sum() does not respect the stride of a[:, i] when putting outputs in place.\nTested on PyTorch version 0.3.0.post4.", "body": "The `torch.sum()` function writes its outputs in erroneous locations when the out `kwarg` is used. Repro:\r\n\r\n```python\r\nimport torch\r\n\r\ni = 1\r\na = torch.zeros(5, 3)\r\nb = torch.randn(3, 5)\r\n\r\nac = a.clone()\r\nac[:, i].copy_(b.sum(0))\r\nprint(ac)\r\n\r\nac = a.clone()\r\nb.sum(0, out=ac[:, i])\r\nprint(ac)\r\n```\r\n\r\nOutput:\r\n```bash\r\nlvdmaaten-mbp:Desktop lvdmaaten$ python bug.py\r\n0.0000 -0.1515 0.0000\r\n0.0000 1.8761 0.0000\r\n0.0000 -1.7563 0.0000\r\n0.0000 0.5194 0.0000\r\n0.0000 1.5322 0.0000\r\n[torch.FloatTensor of size 5x3]\r\n0.0000 -0.1515 1.8761\r\n-1.7563 0.5194 1.5322\r\n0.0000 0.0000 0.0000\r\n0.0000 0.0000 0.0000\r\n0.0000 0.0000 0.0000\r\n[torch.FloatTensor of size 5x3]\r\n```\r\n\r\nI presume what happens is that `torch.sum()` does not respect the stride of `a[:, i]` when putting outputs in place.\r\n\r\nTested on PyTorch version 0.3.0.post4."}