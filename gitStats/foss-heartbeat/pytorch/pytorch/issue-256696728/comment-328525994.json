{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328525994", "html_url": "https://github.com/pytorch/pytorch/issues/2688#issuecomment-328525994", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2688", "id": 328525994, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODUyNTk5NA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T13:18:03Z", "updated_at": "2017-09-11T13:18:03Z", "author_association": "MEMBER", "body_html": "<p>This is expected, and happens because <code>1-1e-8</code> can not be represented exactly in float precision (which is the default precision in pytorch).<br>\nFor example, see</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">print</span>(torch.FloatTensor([<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1e-8</span>])[<span class=\"pl-c1\">0</span>])\n<span class=\"pl-c1\">print</span>(torch.DoubleTensor([<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1e-8</span>])[<span class=\"pl-c1\">0</span>])</pre></div>\n<p>yields</p>\n<pre><code>1.0\n0.99999999\n</code></pre>\n<p>The same thing happens in numpy:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">1</span>.], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\nb <span class=\"pl-k\">=</span> np.clip(a, <span class=\"pl-v\">a_min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">a_max</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1e-8</span>)\n<span class=\"pl-c1\">print</span>(b)\n<span class=\"pl-k\">&gt;</span> array([ <span class=\"pl-c1\">1</span>.], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>float32)</pre></div>", "body_text": "This is expected, and happens because 1-1e-8 can not be represented exactly in float precision (which is the default precision in pytorch).\nFor example, see\nprint(torch.FloatTensor([1-1e-8])[0])\nprint(torch.DoubleTensor([1-1e-8])[0])\nyields\n1.0\n0.99999999\n\nThe same thing happens in numpy:\na = np.array([1.], dtype=np.float32)\nb = np.clip(a, a_min=0, a_max=1-1e-8)\nprint(b)\n> array([ 1.], dtype=float32)", "body": "This is expected, and happens because `1-1e-8` can not be represented exactly in float precision (which is the default precision in pytorch).\r\nFor example, see\r\n```python\r\nprint(torch.FloatTensor([1-1e-8])[0])\r\nprint(torch.DoubleTensor([1-1e-8])[0])\r\n```\r\nyields\r\n```\r\n1.0\r\n0.99999999\r\n```\r\n\r\nThe same thing happens in numpy:\r\n```python\r\na = np.array([1.], dtype=np.float32)\r\nb = np.clip(a, a_min=0, a_max=1-1e-8)\r\nprint(b)\r\n> array([ 1.], dtype=float32)\r\n```"}