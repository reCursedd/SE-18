{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/395857565", "html_url": "https://github.com/pytorch/pytorch/issues/8287#issuecomment-395857565", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8287", "id": 395857565, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTg1NzU2NQ==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-08T19:00:20Z", "updated_at": "2018-06-08T19:00:20Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>x = torch.randn(2, 2, requires_grad=True).cuda(0)</p>\n</blockquote>\n<p>If you want gradients to accumulate in <code>x</code>, you should change the above line to</p>\n<pre><code>x = torch.randn(2, 2, requires_grad=True, device='cuda:0')\n</code></pre>\n<p>The reason why your code doesn't work is because of</p>\n<blockquote>\n<p>x = torch.randn(2, 2, requires_grad=True).cuda(0)</p>\n</blockquote>\n<p>What this is doing is creating a \"leaf\" tensor <code>(torch.randn(2, 2, requires_grad=True)</code> and then assigning a copy of it (that is on CUDA device 0) to <code>x</code>. Gradients can only accumulate in leaf tensors.</p>", "body_text": "x = torch.randn(2, 2, requires_grad=True).cuda(0)\n\nIf you want gradients to accumulate in x, you should change the above line to\nx = torch.randn(2, 2, requires_grad=True, device='cuda:0')\n\nThe reason why your code doesn't work is because of\n\nx = torch.randn(2, 2, requires_grad=True).cuda(0)\n\nWhat this is doing is creating a \"leaf\" tensor (torch.randn(2, 2, requires_grad=True) and then assigning a copy of it (that is on CUDA device 0) to x. Gradients can only accumulate in leaf tensors.", "body": "> x = torch.randn(2, 2, requires_grad=True).cuda(0)\r\n\r\nIf you want gradients to accumulate in `x`, you should change the above line to \r\n``` \r\nx = torch.randn(2, 2, requires_grad=True, device='cuda:0')\r\n```\r\n\r\nThe reason why your code doesn't work is because of \r\n> x = torch.randn(2, 2, requires_grad=True).cuda(0)\r\n\r\nWhat this is doing is creating a \"leaf\" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors."}