{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8287", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8287/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8287/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8287/events", "html_url": "https://github.com/pytorch/pytorch/issues/8287", "id": 330762609, "node_id": "MDU6SXNzdWUzMzA3NjI2MDk=", "number": 8287, "title": "[Bug] Failure to acquire gradient with requires_grad and backward(gradient) in cuda (with 0.4.0)", "user": {"login": "ryonakamura", "id": 9457467, "node_id": "MDQ6VXNlcjk0NTc0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/9457467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ryonakamura", "html_url": "https://github.com/ryonakamura", "followers_url": "https://api.github.com/users/ryonakamura/followers", "following_url": "https://api.github.com/users/ryonakamura/following{/other_user}", "gists_url": "https://api.github.com/users/ryonakamura/gists{/gist_id}", "starred_url": "https://api.github.com/users/ryonakamura/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ryonakamura/subscriptions", "organizations_url": "https://api.github.com/users/ryonakamura/orgs", "repos_url": "https://api.github.com/users/ryonakamura/repos", "events_url": "https://api.github.com/users/ryonakamura/events{/privacy}", "received_events_url": "https://api.github.com/users/ryonakamura/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-08T18:39:52Z", "updated_at": "2018-06-08T19:00:20Z", "closed_at": "2018-06-08T19:00:20Z", "author_association": "NONE", "body_html": "<p>The following snippet is a reproduction of bug.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\nx <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).cuda(<span class=\"pl-c1\">0</span>)\nt <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>).cuda(<span class=\"pl-c1\">0</span>)\nloss_func <span class=\"pl-k\">=</span> nn.MSELoss().cuda(<span class=\"pl-c1\">0</span>)\nl1 <span class=\"pl-k\">=</span> nn.Sequential(<span class=\"pl-k\">*</span>[nn.Linear(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>)]).cuda(<span class=\"pl-c1\">0</span>)\nl2 <span class=\"pl-k\">=</span> nn.Sequential(<span class=\"pl-k\">*</span>[nn.Linear(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>)]).cuda(<span class=\"pl-c1\">0</span>)\nh1 <span class=\"pl-k\">=</span> l1(x)\nh2 <span class=\"pl-k\">=</span> h1.detach().requires_grad_()\nh3 <span class=\"pl-k\">=</span> l2(h2)\nl <span class=\"pl-k\">=</span> loss_func(h3, t)\nl.backward()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>h2:<span class=\"pl-pds\">'</span></span>, h2.grad)\nh1.backward(<span class=\"pl-v\">gradient</span><span class=\"pl-k\">=</span>h2.grad)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x:<span class=\"pl-pds\">'</span></span>, x.grad)</pre></div>\n<p>It is possible to acquire gradient of <code>x</code> in cpu, but it fails in cuda.</p>", "body_text": "The following snippet is a reproduction of bug.\nimport torch\nimport torch.nn as nn\n\nx = torch.randn(2, 2, requires_grad=True).cuda(0)\nt = torch.randn(2, 2).cuda(0)\nloss_func = nn.MSELoss().cuda(0)\nl1 = nn.Sequential(*[nn.Linear(2, 2) for _ in range(3)]).cuda(0)\nl2 = nn.Sequential(*[nn.Linear(2, 2) for _ in range(3)]).cuda(0)\nh1 = l1(x)\nh2 = h1.detach().requires_grad_()\nh3 = l2(h2)\nl = loss_func(h3, t)\nl.backward()\nprint('h2:', h2.grad)\nh1.backward(gradient=h2.grad)\nprint('x:', x.grad)\nIt is possible to acquire gradient of x in cpu, but it fails in cuda.", "body": "The following snippet is a reproduction of bug.\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nx = torch.randn(2, 2, requires_grad=True).cuda(0)\r\nt = torch.randn(2, 2).cuda(0)\r\nloss_func = nn.MSELoss().cuda(0)\r\nl1 = nn.Sequential(*[nn.Linear(2, 2) for _ in range(3)]).cuda(0)\r\nl2 = nn.Sequential(*[nn.Linear(2, 2) for _ in range(3)]).cuda(0)\r\nh1 = l1(x)\r\nh2 = h1.detach().requires_grad_()\r\nh3 = l2(h2)\r\nl = loss_func(h3, t)\r\nl.backward()\r\nprint('h2:', h2.grad)\r\nh1.backward(gradient=h2.grad)\r\nprint('x:', x.grad)\r\n```\r\nIt is possible to acquire gradient of `x` in cpu, but it fails in cuda."}