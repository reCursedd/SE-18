{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2367", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2367/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2367/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2367/events", "html_url": "https://github.com/pytorch/pytorch/issues/2367", "id": 249279162, "node_id": "MDU6SXNzdWUyNDkyNzkxNjI=", "number": 2367, "title": "torch.cat accepts both Variable and tensor", "user": {"login": "ruotianluo", "id": 16023153, "node_id": "MDQ6VXNlcjE2MDIzMTUz", "avatar_url": "https://avatars2.githubusercontent.com/u/16023153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ruotianluo", "html_url": "https://github.com/ruotianluo", "followers_url": "https://api.github.com/users/ruotianluo/followers", "following_url": "https://api.github.com/users/ruotianluo/following{/other_user}", "gists_url": "https://api.github.com/users/ruotianluo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ruotianluo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ruotianluo/subscriptions", "organizations_url": "https://api.github.com/users/ruotianluo/orgs", "repos_url": "https://api.github.com/users/ruotianluo/repos", "events_url": "https://api.github.com/users/ruotianluo/events{/privacy}", "received_events_url": "https://api.github.com/users/ruotianluo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-10T09:03:02Z", "updated_at": "2018-04-24T20:30:49Z", "closed_at": "2018-04-24T20:30:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems that torch.cat can concatenate variables and tensors and output a variable. However, this will cause backward crash.</p>\n<pre><code>import torch\nx = torch.autograd.Variable(torch.randn(3,4), requires_grad=True)\ny = torch.randn(3,2)\nz = torch.cat([x,y],1)\nz.sum().backward()\n</code></pre>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"/Users/rluo/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br>\nFile \"/Users/rluo/anaconda2/lib/python2.7/site-packages/torch/autograd/<strong>init</strong>.py\", line 98, in backward<br>\nvariables, grad_variables, retain_graph)<br>\nRuntimeError: function ConcatBackward returned a gradient different than None at position 3, but the corresponding forward input was not a Variable</p>\n</blockquote>\n<ol>\n<li>Is it a good feature?</li>\n<li>If you want to keep it, should the framework be able to infer the tensor to be a leaf node?</li>\n</ol>", "body_text": "It seems that torch.cat can concatenate variables and tensors and output a variable. However, this will cause backward crash.\nimport torch\nx = torch.autograd.Variable(torch.randn(3,4), requires_grad=True)\ny = torch.randn(3,2)\nz = torch.cat([x,y],1)\nz.sum().backward()\n\n\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/Users/rluo/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/Users/rluo/anaconda2/lib/python2.7/site-packages/torch/autograd/init.py\", line 98, in backward\nvariables, grad_variables, retain_graph)\nRuntimeError: function ConcatBackward returned a gradient different than None at position 3, but the corresponding forward input was not a Variable\n\n\nIs it a good feature?\nIf you want to keep it, should the framework be able to infer the tensor to be a leaf node?", "body": "It seems that torch.cat can concatenate variables and tensors and output a variable. However, this will cause backward crash.\r\n\r\n```\r\nimport torch\r\nx = torch.autograd.Variable(torch.randn(3,4), requires_grad=True)\r\ny = torch.randn(3,2)\r\nz = torch.cat([x,y],1)\r\nz.sum().backward()\r\n```\r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/Users/rluo/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\r\n>     torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n>   File \"/Users/rluo/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n>     variables, grad_variables, retain_graph)\r\n> RuntimeError: function ConcatBackward returned a gradient different than None at position 3, but the corresponding forward input was not a Variable\r\n\r\n1. Is it a good feature?\r\n2. If you want to keep it, should the framework be able to infer the tensor to be a leaf node?\r\n"}