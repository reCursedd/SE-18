{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140538337", "pull_request_review_id": 64631021, "id": 140538337, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDUzODMzNw==", "diff_hunk": "@@ -30,9 +30,10 @@ def backward(self, reward):\n         output_probs = probs.gather(1, samples)\n         output_probs.add_(1e-6).reciprocal_()\n         output_probs.neg_().mul_(reward)\n-        # TODO: add batched index_add\n-        for i in range(probs.size(0)):\n-            grad_probs[i].index_add_(0, samples[i], output_probs[i])\n+        # Fill in gradients\n+        idx = grad_probs.new().resize_(grad_probs.size(0)).fill_(1)", "path": "torch/autograd/_functions/stochastic.py", "position": null, "original_position": 8, "commit_id": "e350cd383c1223e9f915b4995d415adc39fea020", "original_commit_id": "695c36fc83aba6500bd350ad55ee7ac135d77ae6", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "body": "a more efficient way to write this code is:\r\n\r\n```python\r\nidx = grad_probs.new().long().resize_(grad_probs.size(0))\r\ntorch.arange(0, grad_probs.size(0), out=idx)\r\ngrad_probs[idx, samples.t()] += output_probs.t()\r\n```\r\n\r\nHowever, your change is not correct. I think advanced indexing doesn't handle collissions well (two adds on same element): cc @killeent do you know if we handle collissions?\r\n\r\nHere's a simple test I wrote to verify that the old and new results differ:\r\n\r\n```python\r\nimport torch\r\n\r\nprobs = torch.randn(2, 5).add_(10)\r\nsamples = probs.multinomial(10, True)\r\n\r\ngrad_probs = probs.new().resize_as_(probs).zero_()\r\ngrad_probs.normal_() # some random gradients\r\n\r\noutput_probs = probs.gather(1, samples)\r\noutput_probs.add_(1e-6).reciprocal_()\r\n\r\n# now save original grad_probs\r\ngrad_probs_old = grad_probs.clone()\r\n\r\n\r\n# do it in the method of existing code\r\nfor i in range(probs.size(0)):\r\n    grad_probs[i].index_add_(0, samples[i], output_probs[i])\r\n\r\ngrad_probs_unbatched = grad_probs.clone()\r\nprint(grad_probs_unbatched)\r\n\r\n# restore grad_probs again, for batched method\r\ngrad_probs = grad_probs_old.clone()\r\nidx = grad_probs.new().resize_(grad_probs.size(0)).fill_(1)\r\nidx = idx.cumsum(0) - 1\r\ngrad_probs[idx.long(), samples.t()] += output_probs.t()\r\n\r\n\r\nprint(grad_probs)\r\nprint(grad_probs_manual)\r\n```", "created_at": "2017-09-22T16:28:12Z", "updated_at": "2018-11-23T15:34:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/2814#discussion_r140538337", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2814", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140538337"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2814#discussion_r140538337"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2814"}}, "body_html": "<p>a more efficient way to write this code is:</p>\n<div class=\"highlight highlight-source-python\"><pre>idx <span class=\"pl-k\">=</span> grad_probs.new().long().resize_(grad_probs.size(<span class=\"pl-c1\">0</span>))\ntorch.arange(<span class=\"pl-c1\">0</span>, grad_probs.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-v\">out</span><span class=\"pl-k\">=</span>idx)\ngrad_probs[idx, samples.t()] <span class=\"pl-k\">+=</span> output_probs.t()</pre></div>\n<p>However, your change is not correct. I think advanced indexing doesn't handle collissions well (two adds on same element): cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4529377\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/killeent\">@killeent</a> do you know if we handle collissions?</p>\n<p>Here's a simple test I wrote to verify that the old and new results differ:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\nprobs <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>).add_(<span class=\"pl-c1\">10</span>)\nsamples <span class=\"pl-k\">=</span> probs.multinomial(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">True</span>)\n\ngrad_probs <span class=\"pl-k\">=</span> probs.new().resize_as_(probs).zero_()\ngrad_probs.normal_() <span class=\"pl-c\"><span class=\"pl-c\">#</span> some random gradients</span>\n\noutput_probs <span class=\"pl-k\">=</span> probs.gather(<span class=\"pl-c1\">1</span>, samples)\noutput_probs.add_(<span class=\"pl-c1\">1e-6</span>).reciprocal_()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> now save original grad_probs</span>\ngrad_probs_old <span class=\"pl-k\">=</span> grad_probs.clone()\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> do it in the method of existing code</span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(probs.size(<span class=\"pl-c1\">0</span>)):\n    grad_probs[i].index_add_(<span class=\"pl-c1\">0</span>, samples[i], output_probs[i])\n\ngrad_probs_unbatched <span class=\"pl-k\">=</span> grad_probs.clone()\n<span class=\"pl-c1\">print</span>(grad_probs_unbatched)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> restore grad_probs again, for batched method</span>\ngrad_probs <span class=\"pl-k\">=</span> grad_probs_old.clone()\nidx <span class=\"pl-k\">=</span> grad_probs.new().resize_(grad_probs.size(<span class=\"pl-c1\">0</span>)).fill_(<span class=\"pl-c1\">1</span>)\nidx <span class=\"pl-k\">=</span> idx.cumsum(<span class=\"pl-c1\">0</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\ngrad_probs[idx.long(), samples.t()] <span class=\"pl-k\">+=</span> output_probs.t()\n\n\n<span class=\"pl-c1\">print</span>(grad_probs)\n<span class=\"pl-c1\">print</span>(grad_probs_manual)</pre></div>", "body_text": "a more efficient way to write this code is:\nidx = grad_probs.new().long().resize_(grad_probs.size(0))\ntorch.arange(0, grad_probs.size(0), out=idx)\ngrad_probs[idx, samples.t()] += output_probs.t()\nHowever, your change is not correct. I think advanced indexing doesn't handle collissions well (two adds on same element): cc @killeent do you know if we handle collissions?\nHere's a simple test I wrote to verify that the old and new results differ:\nimport torch\n\nprobs = torch.randn(2, 5).add_(10)\nsamples = probs.multinomial(10, True)\n\ngrad_probs = probs.new().resize_as_(probs).zero_()\ngrad_probs.normal_() # some random gradients\n\noutput_probs = probs.gather(1, samples)\noutput_probs.add_(1e-6).reciprocal_()\n\n# now save original grad_probs\ngrad_probs_old = grad_probs.clone()\n\n\n# do it in the method of existing code\nfor i in range(probs.size(0)):\n    grad_probs[i].index_add_(0, samples[i], output_probs[i])\n\ngrad_probs_unbatched = grad_probs.clone()\nprint(grad_probs_unbatched)\n\n# restore grad_probs again, for batched method\ngrad_probs = grad_probs_old.clone()\nidx = grad_probs.new().resize_(grad_probs.size(0)).fill_(1)\nidx = idx.cumsum(0) - 1\ngrad_probs[idx.long(), samples.t()] += output_probs.t()\n\n\nprint(grad_probs)\nprint(grad_probs_manual)"}