{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140620200", "pull_request_review_id": 64716587, "id": 140620200, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDYyMDIwMA==", "diff_hunk": "@@ -30,9 +30,10 @@ def backward(self, reward):\n         output_probs = probs.gather(1, samples)\n         output_probs.add_(1e-6).reciprocal_()\n         output_probs.neg_().mul_(reward)\n-        # TODO: add batched index_add\n-        for i in range(probs.size(0)):\n-            grad_probs[i].index_add_(0, samples[i], output_probs[i])\n+        # Fill in gradients\n+        idx = grad_probs.new().resize_(grad_probs.size(0)).fill_(1)", "path": "torch/autograd/_functions/stochastic.py", "position": null, "original_position": 8, "commit_id": "e350cd383c1223e9f915b4995d415adc39fea020", "original_commit_id": "695c36fc83aba6500bd350ad55ee7ac135d77ae6", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "body": "@soumith \r\n\r\nI have made the following change which computes the correct answer. \r\n\r\nThe only problem is, when testing with 3000 samples, the code with `mask_idx` is roughly 43 times slower than for loop. The one uses `torch.arange` instead, is `40%` faster than for loop. The only problem is in `_functions/stochastic.py`, is it allowed to `import torch` ?\r\n\r\n```python\r\nimport torch\r\nimport time\r\n\r\nprobs = torch.randn(3000, 5).add_(10)\r\nsamples = probs.multinomial(10, True)\r\n\r\ngrad_probs = probs.new().resize_as_(probs).zero_()\r\ngrad_probs.normal_() # some random gradients\r\n\r\noutput_probs = probs.gather(1, samples)\r\noutput_probs.add_(1e-6).reciprocal_()\r\n\r\n# now save original grad_probs\r\ngrad_probs_old = grad_probs.clone()\r\n\r\n\r\n#print('Original: ', grad_probs)\r\n\r\nt = time.time()\r\n# do it in the method of existing code\r\nfor i in range(probs.size(0)):\r\n    grad_probs[i].index_add_(0, samples[i], output_probs[i])\r\n\r\ngrad_probs_unbatched = grad_probs.clone()\r\n\r\nduration_1 = time.time() - t\r\nprint('After changing ({} s): '.format(duration_1))\r\n\r\n# restore grad_probs again, for batched method\r\ngrad_probs = grad_probs_old.clone()\r\n\r\nt = time.time()\r\n\r\n#print('Original:', grad_probs)\r\n\r\nmask = grad_probs.new().resize_as_(grad_probs).fill_(0).repeat(samples.size(1), 1)\r\n#mask_idx = mask.new().resize_(mask.size(0)).fill_(1)\r\n#mask_idx = mask_idx.cumsum(0) - 1\r\nmask[torch.arange(0, mask.size(0)).long(), samples.t().contiguous().view(-1)] = 1\r\nmask = mask.view(-1, grad_probs.size(0), grad_probs.size(1)).sum(0)\r\n\r\ngrad_probs += mask*probs.add_(1e-6).reciprocal_()\r\n\r\nduration_2 = time.time() - t\r\nprint('After changing: ({} s)'.format(duration_2))\r\n\r\nprint('Time ratio: ', duration_2/duration_1)\r\n\r\n(grad_probs_unbatched - grad_probs).sum()\r\n```", "created_at": "2017-09-23T00:34:57Z", "updated_at": "2018-11-23T15:34:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/2814#discussion_r140620200", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2814", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140620200"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2814#discussion_r140620200"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2814"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>\n<p>I have made the following change which computes the correct answer.</p>\n<p>The only problem is, when testing with 3000 samples, the code with <code>mask_idx</code> is roughly 43 times slower than for loop. The one uses <code>torch.arange</code> instead, is <code>40%</code> faster than for loop. The only problem is in <code>_functions/stochastic.py</code>, is it allowed to <code>import torch</code> ?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> time\n\nprobs <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3000</span>, <span class=\"pl-c1\">5</span>).add_(<span class=\"pl-c1\">10</span>)\nsamples <span class=\"pl-k\">=</span> probs.multinomial(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">True</span>)\n\ngrad_probs <span class=\"pl-k\">=</span> probs.new().resize_as_(probs).zero_()\ngrad_probs.normal_() <span class=\"pl-c\"><span class=\"pl-c\">#</span> some random gradients</span>\n\noutput_probs <span class=\"pl-k\">=</span> probs.gather(<span class=\"pl-c1\">1</span>, samples)\noutput_probs.add_(<span class=\"pl-c1\">1e-6</span>).reciprocal_()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> now save original grad_probs</span>\ngrad_probs_old <span class=\"pl-k\">=</span> grad_probs.clone()\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>print('Original: ', grad_probs)</span>\n\nt <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> do it in the method of existing code</span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(probs.size(<span class=\"pl-c1\">0</span>)):\n    grad_probs[i].index_add_(<span class=\"pl-c1\">0</span>, samples[i], output_probs[i])\n\ngrad_probs_unbatched <span class=\"pl-k\">=</span> grad_probs.clone()\n\nduration_1 <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> t\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>After changing (<span class=\"pl-c1\">{}</span> s): <span class=\"pl-pds\">'</span></span>.format(duration_1))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> restore grad_probs again, for batched method</span>\ngrad_probs <span class=\"pl-k\">=</span> grad_probs_old.clone()\n\nt <span class=\"pl-k\">=</span> time.time()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>print('Original:', grad_probs)</span>\n\nmask <span class=\"pl-k\">=</span> grad_probs.new().resize_as_(grad_probs).fill_(<span class=\"pl-c1\">0</span>).repeat(samples.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>mask_idx = mask.new().resize_(mask.size(0)).fill_(1)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>mask_idx = mask_idx.cumsum(0) - 1</span>\nmask[torch.arange(<span class=\"pl-c1\">0</span>, mask.size(<span class=\"pl-c1\">0</span>)).long(), samples.t().contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nmask <span class=\"pl-k\">=</span> mask.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, grad_probs.size(<span class=\"pl-c1\">0</span>), grad_probs.size(<span class=\"pl-c1\">1</span>)).sum(<span class=\"pl-c1\">0</span>)\n\ngrad_probs <span class=\"pl-k\">+=</span> mask<span class=\"pl-k\">*</span>probs.add_(<span class=\"pl-c1\">1e-6</span>).reciprocal_()\n\nduration_2 <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> t\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>After changing: (<span class=\"pl-c1\">{}</span> s)<span class=\"pl-pds\">'</span></span>.format(duration_2))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Time ratio: <span class=\"pl-pds\">'</span></span>, duration_2<span class=\"pl-k\">/</span>duration_1)\n\n(grad_probs_unbatched <span class=\"pl-k\">-</span> grad_probs).sum()</pre></div>", "body_text": "@soumith\nI have made the following change which computes the correct answer.\nThe only problem is, when testing with 3000 samples, the code with mask_idx is roughly 43 times slower than for loop. The one uses torch.arange instead, is 40% faster than for loop. The only problem is in _functions/stochastic.py, is it allowed to import torch ?\nimport torch\nimport time\n\nprobs = torch.randn(3000, 5).add_(10)\nsamples = probs.multinomial(10, True)\n\ngrad_probs = probs.new().resize_as_(probs).zero_()\ngrad_probs.normal_() # some random gradients\n\noutput_probs = probs.gather(1, samples)\noutput_probs.add_(1e-6).reciprocal_()\n\n# now save original grad_probs\ngrad_probs_old = grad_probs.clone()\n\n\n#print('Original: ', grad_probs)\n\nt = time.time()\n# do it in the method of existing code\nfor i in range(probs.size(0)):\n    grad_probs[i].index_add_(0, samples[i], output_probs[i])\n\ngrad_probs_unbatched = grad_probs.clone()\n\nduration_1 = time.time() - t\nprint('After changing ({} s): '.format(duration_1))\n\n# restore grad_probs again, for batched method\ngrad_probs = grad_probs_old.clone()\n\nt = time.time()\n\n#print('Original:', grad_probs)\n\nmask = grad_probs.new().resize_as_(grad_probs).fill_(0).repeat(samples.size(1), 1)\n#mask_idx = mask.new().resize_(mask.size(0)).fill_(1)\n#mask_idx = mask_idx.cumsum(0) - 1\nmask[torch.arange(0, mask.size(0)).long(), samples.t().contiguous().view(-1)] = 1\nmask = mask.view(-1, grad_probs.size(0), grad_probs.size(1)).sum(0)\n\ngrad_probs += mask*probs.add_(1e-6).reciprocal_()\n\nduration_2 = time.time() - t\nprint('After changing: ({} s)'.format(duration_2))\n\nprint('Time ratio: ', duration_2/duration_1)\n\n(grad_probs_unbatched - grad_probs).sum()", "in_reply_to_id": 140538337}