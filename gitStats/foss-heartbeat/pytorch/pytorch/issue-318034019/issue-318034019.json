{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6988", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6988/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6988/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6988/events", "html_url": "https://github.com/pytorch/pytorch/issues/6988", "id": 318034019, "node_id": "MDU6SXNzdWUzMTgwMzQwMTk=", "number": 6988, "title": "The loss computation with `size_average` should average over batch example or batch element ?", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 897287230, "node_id": "MDU6TGFiZWw4OTcyODcyMzA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/documentation", "name": "documentation", "color": "f9d0c4", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-04-26T13:38:48Z", "updated_at": "2018-05-29T19:37:23Z", "closed_at": "2018-05-26T15:18:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Traditionally, when we have a batched data with shape <code>(N, D)</code> where <code>N</code> is batch size and <code>D</code> is data dimension. The losses are often calculated for each training example say</p>\n<pre><code>L_i = loss(X_i), i = 1, ..., N\n</code></pre>\n<p>And then total loss is averaged over the batch size</p>\n<pre><code>L = (1/N)*sum(L_i)\n</code></pre>\n<p>However, it seems it is not what <code>nn.*Loss</code> is doing for the flag <code>size_average=True/False</code></p>\n<p>e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>)\ntarget <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>)\n<span class=\"pl-c1\">print</span>(target)\n\nfull_loss <span class=\"pl-k\">=</span> F.mse_loss(<span class=\"pl-c1\">input</span>, target, <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n<span class=\"pl-c1\">print</span>(full_loss)\nloss_sum <span class=\"pl-k\">=</span> F.mse_loss(<span class=\"pl-c1\">input</span>, target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nloss_mean <span class=\"pl-k\">=</span> F.mse_loss(<span class=\"pl-c1\">input</span>, target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">print</span>(loss_sum)\n<span class=\"pl-c1\">print</span>(loss_mean)\n\nbatch_loss <span class=\"pl-k\">=</span> full_loss.sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ncorrect_loss_mean <span class=\"pl-k\">=</span> batch_loss.mean()\ncorrect_loss_sum <span class=\"pl-k\">=</span> batch_loss.sum()\n<span class=\"pl-c1\">print</span>(correct_loss_mean)\n<span class=\"pl-c1\">print</span>(correct_loss_sum)\n\n<span class=\"pl-k\">assert</span> correct_loss_sum <span class=\"pl-k\">==</span> loss_sum\n<span class=\"pl-k\">assert</span> correct_loss_mean <span class=\"pl-k\">==</span> loss_mean</pre></div>\n<p>it seems the <code>size_average</code> does not properly average the loss over the batch of examples, but averaged over all dimensions.</p>", "body_text": "Traditionally, when we have a batched data with shape (N, D) where N is batch size and D is data dimension. The losses are often calculated for each training example say\nL_i = loss(X_i), i = 1, ..., N\n\nAnd then total loss is averaged over the batch size\nL = (1/N)*sum(L_i)\n\nHowever, it seems it is not what nn.*Loss is doing for the flag size_average=True/False\ne.g.\nimport torch\nimport torch.nn.functional as F\n\ninput = torch.randn(3, 2)\ntarget = torch.rand(3, 2)\n\nprint(input)\nprint(target)\n\nfull_loss = F.mse_loss(input, target, reduce=False)\nprint(full_loss)\nloss_sum = F.mse_loss(input, target, size_average=False)\nloss_mean = F.mse_loss(input, target, size_average=True)\nprint(loss_sum)\nprint(loss_mean)\n\nbatch_loss = full_loss.sum(dim=-1)\ncorrect_loss_mean = batch_loss.mean()\ncorrect_loss_sum = batch_loss.sum()\nprint(correct_loss_mean)\nprint(correct_loss_sum)\n\nassert correct_loss_sum == loss_sum\nassert correct_loss_mean == loss_mean\nit seems the size_average does not properly average the loss over the batch of examples, but averaged over all dimensions.", "body": "Traditionally, when we have a batched data with shape `(N, D)` where `N` is batch size and `D` is data dimension. The losses are often calculated for each training example say\r\n```\r\nL_i = loss(X_i), i = 1, ..., N\r\n```\r\nAnd then total loss is averaged over the batch size\r\n```\r\nL = (1/N)*sum(L_i)\r\n```\r\n\r\nHowever, it seems it is not what `nn.*Loss` is doing for the flag `size_average=True/False`\r\n\r\ne.g. \r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ninput = torch.randn(3, 2)\r\ntarget = torch.rand(3, 2)\r\n\r\nprint(input)\r\nprint(target)\r\n\r\nfull_loss = F.mse_loss(input, target, reduce=False)\r\nprint(full_loss)\r\nloss_sum = F.mse_loss(input, target, size_average=False)\r\nloss_mean = F.mse_loss(input, target, size_average=True)\r\nprint(loss_sum)\r\nprint(loss_mean)\r\n\r\nbatch_loss = full_loss.sum(dim=-1)\r\ncorrect_loss_mean = batch_loss.mean()\r\ncorrect_loss_sum = batch_loss.sum()\r\nprint(correct_loss_mean)\r\nprint(correct_loss_sum)\r\n\r\nassert correct_loss_sum == loss_sum\r\nassert correct_loss_mean == loss_mean\r\n```\r\nit seems the `size_average` does not properly average the loss over the batch of examples, but averaged over all dimensions. "}