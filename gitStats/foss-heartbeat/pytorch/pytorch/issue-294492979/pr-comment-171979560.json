{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171979560", "pull_request_review_id": 100933664, "id": 171979560, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTk3OTU2MA==", "diff_hunk": "@@ -0,0 +1,89 @@\n+#ifndef THC_OFFSET_INFO_INC\n+#define THC_OFFSET_INFO_INC\n+\n+#include \"THCIntegerDivider.cuh\"\n+#include \"THCTensorInfo.cuh\"\n+\n+// A faster implementation of IndexToOffset that uses faster integer division:\n+// we transform each division into integer multiplication by a pre-computed\n+// constant.  (See IntDivider for details.)\n+\n+template <typename T, typename IndexType, int Dims>\n+struct OffsetInfo {\n+  explicit OffsetInfo(const TensorInfo<T, IndexType>& tinfo) {\n+    assert(tinfo.dims == Dims);\n+    data = tinfo.data;\n+\n+    for (int i = 0; i < Dims; ++i) {\n+      sizes[i] = IntDivider<IndexType>(tinfo.sizes[i]);\n+      strides[i] = tinfo.strides[i];\n+    }\n+  }\n+\n+  __host__ __device__ T* get(IndexType linearIndex) const {\n+    IndexType offset = 0;\n+\n+    for (int i = Dims - 1; i > 0; --i) {\n+      DivMod<IndexType> divmod = sizes[i].divmod(linearIndex);\n+      linearIndex = divmod.div;\n+      offset += divmod.mod * strides[i];\n+    }\n+\n+    offset += linearIndex * strides[0];\n+    return &data[offset];\n+  }\n+\n+  T* data;\n+  IntDivider<IndexType> sizes[Dims];\n+  IndexType strides[Dims];\n+};\n+\n+// For contiguous tensors (Dims=-2), offset equals linear index.\n+template <typename T, typename IndexType>\n+struct OffsetInfo<T, IndexType, -2> {\n+  explicit OffsetInfo(const TensorInfo<T, IndexType>& tinfo)\n+    : data(tinfo.data) {\n+    assert(tinfo.isContiguous());\n+  }\n+\n+  __host__ __device__ T* get(IndexType linearIndex) const {\n+    return &data[linearIndex];\n+  }\n+\n+  T* data;\n+};\n+\n+// Dims=-1 is used when the dimension is unknown at compile time.\n+//\n+// Unfortunately, pre-computation does not work here, because of a bug in nvcc\n+// (tested on CUDA 8.0): if a kernel argument contains an array that is\n+// dynamically accessed, the whole array is first copied into the local memory.\n+// (That is, every kernel thread makes its own copy of the argument, even if it\n+// is never updated.)  Pre-computation makes it worse because now we have more\n+// data to copy.\n+//\n+// So let's fall back to vanilla division approach.\n+\n+template <typename T, typename IndexType>\n+struct OffsetInfo<T, IndexType, -1> {\n+  explicit OffsetInfo(const TensorInfo<T, IndexType>& tinfo)\n+    : tinfo(tinfo) { }\n+\n+  __host__ __device__ T* get(IndexType linearIndex) const {\n+    IndexType offset = 0;\n+\n+    for (int i = tinfo.dims - 1; i > 0; --i) {", "path": "aten/src/THC/THCOffsetInfo.cuh", "position": null, "original_position": 75, "commit_id": "68723f3fd0b2a826fa4cd71878db24f40e99b2cf", "original_commit_id": "dccab1a119b2520b46a9a2f92f2a7c1524a45c1b", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "body": "Done: also changed IndexToOffset so that it skips the remainder operation for dim 0 (matches the code when Dims is statically known).", "created_at": "2018-03-02T22:27:24Z", "updated_at": "2018-11-23T15:40:18Z", "html_url": "https://github.com/pytorch/pytorch/pull/5054#discussion_r171979560", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5054", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171979560"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5054#discussion_r171979560"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5054"}}, "body_html": "<p>Done: also changed IndexToOffset so that it skips the remainder operation for dim 0 (matches the code when Dims is statically known).</p>", "body_text": "Done: also changed IndexToOffset so that it skips the remainder operation for dim 0 (matches the code when Dims is statically known).", "in_reply_to_id": 171657280}