{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/363541466", "html_url": "https://github.com/pytorch/pytorch/pull/5054#issuecomment-363541466", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5054", "id": 363541466, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MzU0MTQ2Ng==", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-06T19:42:39Z", "updated_at": "2018-02-06T19:42:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1911637\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wickedfoo\">@wickedfoo</a>, thanks for the detailed review, and I understand your point that the code is too complicated for the (rather unimpressive) speedup.  I'll try just using the constant division algorithm and get back to you.  Might take a few days.</p>\n<p>On the other hand, I do think there's a measurable speedup for some cases.  One case I found:</p>\n<pre><code>A = torch.cuda.FloatTensor(4096, 4096)\nB = torch.cuda.FloatTensor(2048)\nA = A[:, :2048]\nA.pow_(B)  # Changes from ~389 us to ~315 us (speedup ~23%)\n</code></pre>\n<p>Ironically, using even larger tensor doesn't show larger speedup, because then (I suppose) memory bandwidth dominates everything.</p>", "body_text": "Hi @wickedfoo, thanks for the detailed review, and I understand your point that the code is too complicated for the (rather unimpressive) speedup.  I'll try just using the constant division algorithm and get back to you.  Might take a few days.\nOn the other hand, I do think there's a measurable speedup for some cases.  One case I found:\nA = torch.cuda.FloatTensor(4096, 4096)\nB = torch.cuda.FloatTensor(2048)\nA = A[:, :2048]\nA.pow_(B)  # Changes from ~389 us to ~315 us (speedup ~23%)\n\nIronically, using even larger tensor doesn't show larger speedup, because then (I suppose) memory bandwidth dominates everything.", "body": "Hi @wickedfoo, thanks for the detailed review, and I understand your point that the code is too complicated for the (rather unimpressive) speedup.  I'll try just using the constant division algorithm and get back to you.  Might take a few days.\r\n\r\nOn the other hand, I do think there's a measurable speedup for some cases.  One case I found:\r\n\r\n    A = torch.cuda.FloatTensor(4096, 4096)\r\n    B = torch.cuda.FloatTensor(2048)\r\n    A = A[:, :2048]\r\n    A.pow_(B)  # Changes from ~389 us to ~315 us (speedup ~23%)\r\n\r\nIronically, using even larger tensor doesn't show larger speedup, because then (I suppose) memory bandwidth dominates everything."}