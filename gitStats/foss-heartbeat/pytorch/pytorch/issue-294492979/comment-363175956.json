{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/363175956", "html_url": "https://github.com/pytorch/pytorch/pull/5054#issuecomment-363175956", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5054", "id": 363175956, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MzE3NTk1Ng==", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-05T18:28:59Z", "updated_at": "2018-02-05T18:28:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This PR improves some float operations by ~20% (and some operations on ByteTensor by up to ~45%), but in general the performance impact seems small, unless one uses a lot of non-contiguous tensors and/or broadcasting with large dimensions.</p>\n<p>Here's an example where I could get ~20% improvement on GTX 1080:</p>\n<pre><code>A = torch.cuda.FloatTensor(1000, 256)\nB = torch.cuda.FloatTensor(128)\nA = A[:, :128]\nA.pow_(B)  # Improves from ~7.6 to ~6.0 usec\n</code></pre>\n<p>I found at least one case where it becomes slower by ~5%, but such cases seem to be rare, so I still think it's a net performance win on average, although small.</p>\n<pre><code>A = torch.cuda.IntTensor(2048, 2048)\nA = A[:, :2000]\nB = torch.cuda.IntTensor(2000).fill_(10)\nA.remainder_(B)  # Changes from ~180 to 189 usec.\n</code></pre>\n<p>Raw benchmark results are <a href=\"https://github.com/yongjik/pt_test/tree/master/results/offset\">https://github.com/yongjik/pt_test/tree/master/results/offset</a> in case anybody's interested.</p>\n<ul>\n<li>This PR also fixes overflow errors with large tensors (2G elements or more), which makes me suspect that nobody has actually been using 64-bit index math so far...</li>\n</ul>", "body_text": "This PR improves some float operations by ~20% (and some operations on ByteTensor by up to ~45%), but in general the performance impact seems small, unless one uses a lot of non-contiguous tensors and/or broadcasting with large dimensions.\nHere's an example where I could get ~20% improvement on GTX 1080:\nA = torch.cuda.FloatTensor(1000, 256)\nB = torch.cuda.FloatTensor(128)\nA = A[:, :128]\nA.pow_(B)  # Improves from ~7.6 to ~6.0 usec\n\nI found at least one case where it becomes slower by ~5%, but such cases seem to be rare, so I still think it's a net performance win on average, although small.\nA = torch.cuda.IntTensor(2048, 2048)\nA = A[:, :2000]\nB = torch.cuda.IntTensor(2000).fill_(10)\nA.remainder_(B)  # Changes from ~180 to 189 usec.\n\nRaw benchmark results are https://github.com/yongjik/pt_test/tree/master/results/offset in case anybody's interested.\n\nThis PR also fixes overflow errors with large tensors (2G elements or more), which makes me suspect that nobody has actually been using 64-bit index math so far...", "body": "This PR improves some float operations by ~20% (and some operations on ByteTensor by up to ~45%), but in general the performance impact seems small, unless one uses a lot of non-contiguous tensors and/or broadcasting with large dimensions.\r\n\r\nHere's an example where I could get ~20% improvement on GTX 1080:\r\n\r\n    A = torch.cuda.FloatTensor(1000, 256)\r\n    B = torch.cuda.FloatTensor(128)\r\n    A = A[:, :128]\r\n    A.pow_(B)  # Improves from ~7.6 to ~6.0 usec\r\n\r\nI found at least one case where it becomes slower by ~5%, but such cases seem to be rare, so I still think it's a net performance win on average, although small.\r\n\r\n    A = torch.cuda.IntTensor(2048, 2048)\r\n    A = A[:, :2000]\r\n    B = torch.cuda.IntTensor(2000).fill_(10)\r\n    A.remainder_(B)  # Changes from ~180 to 189 usec.\r\n\r\nRaw benchmark results are https://github.com/yongjik/pt_test/tree/master/results/offset in case anybody's interested.\r\n\r\n* This PR also fixes overflow errors with large tensors (2G elements or more), which makes me suspect that nobody has actually been using 64-bit index math so far..."}