{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166432445", "pull_request_review_id": 94493607, "id": 166432445, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjQzMjQ0NQ==", "diff_hunk": "@@ -0,0 +1,120 @@\n+#ifndef THC_INTEGER_DIVIDER_INC\n+#define THC_INTEGER_DIVIDER_INC\n+\n+#include <assert.h>\n+\n+// A utility class to implement integer division by muliplication, given a fixed\n+// divisor.\n+//\n+// (NOTE: Below, \"2^k\" denotes exponentiation, i.e., 1<<k.)\n+//\n+// For any N-bit unsigned integer d (> 0), we can find a \"magic number\" m (2^N\n+// <= m < 2^(N+1)) and shift s such that:\n+//\n+//    \\floor(n / d) = \\floor((m * n) / 2^(N+s)).\n+//\n+// Given such m and s, the integer division can be then implemented as:\n+//\n+//    let m' = m - 2^N  // 0 <= m' < 2^N\n+//        s1 = (s == 0) ? 0 : 1\n+//        s2 = s - s1\n+//\n+//    fast_integer_division(n):\n+//      // Multiply two N-bit unsigned integers: the result is a 2N-bit unsigned\n+//      // integer.  Then take the higher N bits.\n+//      t = (m' * n) >> N\n+//\n+//      // The return value is simply (t + n) >> s, but computing this directly\n+//      // may overflow, so we shift twice.\n+//      t2 = t + ((n - t) >> s1)\n+//      return t2 >> s2\n+//\n+// We have to be careful with (t + n) because it may overflow.  Finding such a\n+// magic number is surprisingly easy:\n+//\n+//    s  = \\ceil(\\log_2 d)\n+//    m' = \\floor(2^N * (2^s - d) / d) + 1  // Need 2N-bit integer arithmetic.\n+//\n+// See also:\n+//    - Division by Invariant Integers Using Multiplication,\n+//      Torbj\u00f6rn Granlund and Peter L. Montgomery, 1994.\n+//\n+//    - http://www.hackersdelight.org/magic.htm\n+//\n+//    - http://ridiculousfish.com/blog/posts/labor-of-division-episode-i.html\n+\n+// Result of div/mod operation stored together.\n+template <typename Value>\n+struct DivMod {\n+  Value div, mod;\n+\n+  __host__ __device__ DivMod(Value div, Value mod) : div(div), mod(mod) { }\n+};\n+\n+// Base case: we only have an implementation for uint32_t for now.  For\n+// everything else, we use plain division.\n+template <typename Value>\n+struct IntDivider {\n+  IntDivider() { }  // Dummy constructor for arrays.\n+  IntDivider(Value d) : divisor(d) { }\n+\n+  __host__ __device__ inline Value div(Value n) const { return n / divisor; }\n+  __host__ __device__ inline Value mod(Value n) const { return n % divisor; }\n+  __host__ __device__ inline DivMod<Value> divmod(Value n) const {\n+    return DivMod<Value>(n / divisor, n % divisor);\n+  }\n+\n+  Value divisor;\n+};\n+\n+// Implement fast integer division.\n+template <>\n+struct IntDivider<unsigned int> {\n+  static_assert(sizeof(unsigned int) == 4, \"Assumes 32-bit unsigned int.\");\n+\n+  IntDivider() { }  // Dummy constructor for arrays.\n+\n+  IntDivider(unsigned int d) : divisor(d) {\n+    assert(divisor != 0);\n+\n+    // TODO: gcc/clang has __builtin_clz() but it's not portable.\n+    unsigned int shift;\n+    for (shift = 0; shift < 32; shift++) if ((1U << shift) >= divisor) break;\n+    s1 = (shift == 0) ? 0 : 1;\n+    s2 = shift - s1;\n+\n+    uint64_t one = 1;\n+    uint64_t magic = ((one << 32) * ((one << shift) - divisor)) / divisor + 1;\n+    m1 = magic;\n+    assert(m1 > 0 && m1 == magic);  // m1 must fit in 32 bits.\n+  }\n+\n+  __host__ __device__ inline unsigned int div(unsigned int n) const {\n+#ifdef __CUDA_ARCH__\n+    // 't' is the higher 32-bits of unsigned 32-bit multiplication of 'n' and\n+    // 'm1'.\n+    unsigned int t = __umulhi(n, m1);", "path": "aten/src/THC/THCIntegerDivider.cuh", "position": 97, "original_position": 96, "commit_id": "68723f3fd0b2a826fa4cd71878db24f40e99b2cf", "original_commit_id": "d8961cb8cf9f8cecc749b7c0d9cb833dbce96a56", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "body": "I just built two toy kernels to compare, and the relevant SASS code for sm_61 (GTX 1080) looks like this:\r\n\r\n    // *data = *data * a;\r\n\r\n        ...\r\n        /*0028*/ LDG.E R0, [R2];\r\n        /*0030*/ XMAD R5, R0.reuse, c[0x0] [0x148], RZ;\r\n        /*0038*/ XMAD.MRG R6, R0.reuse, c[0x0] [0x148].H1, RZ;\r\n\r\n        /*0048*/ XMAD.PSL.CBCC R0, R0.H1, R6.H1, R5;\r\n        /*0050*/ STG.E [R2], R0;\r\n        ...\r\n\r\n    // *data = __umulhi(*data, a);\r\n\r\n        ...\r\n        /*0028*/ LDG.E R0, [R2];\r\n        /*0030*/ XMAD R5, R0.reuse, c[0x0] [0x148], RZ;\r\n        /*0038*/ XMAD R4, R0.reuse, c[0x0] [0x148].H1, RZ;\r\n\r\n        /*0048*/ XMAD R6, R0.H1, c[0x0] [0x148].H1, RZ;\r\n        /*0050*/ XMAD.CHI R5, R0.H1, c[0x0] [0x148], R5;\r\n        /*0058*/ IADD3.RS R0, R5, R4, R6;\r\n\r\n        /*0068*/ STG.E [R2], R0;\r\n        ...\r\n\r\nXMAD is listed as \"Integer Short Multiply Add\", so it looks like the difference is between (3 FMA operations) vs. (4 FMA operations + 1 addition): it looks reasonable to me, unless .CHI is some secret codeword for \"super heavy slow computation\".\r\n\r\n(While we're at it, is there any public documentation for SASS instructions?  I could only find http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html which is not much help.)", "created_at": "2018-02-06T20:26:51Z", "updated_at": "2018-11-23T15:39:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/5054#discussion_r166432445", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5054", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166432445"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5054#discussion_r166432445"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5054"}}, "body_html": "<p>I just built two toy kernels to compare, and the relevant SASS code for sm_61 (GTX 1080) looks like this:</p>\n<pre><code>// *data = *data * a;\n\n    ...\n    /*0028*/ LDG.E R0, [R2];\n    /*0030*/ XMAD R5, R0.reuse, c[0x0] [0x148], RZ;\n    /*0038*/ XMAD.MRG R6, R0.reuse, c[0x0] [0x148].H1, RZ;\n\n    /*0048*/ XMAD.PSL.CBCC R0, R0.H1, R6.H1, R5;\n    /*0050*/ STG.E [R2], R0;\n    ...\n\n// *data = __umulhi(*data, a);\n\n    ...\n    /*0028*/ LDG.E R0, [R2];\n    /*0030*/ XMAD R5, R0.reuse, c[0x0] [0x148], RZ;\n    /*0038*/ XMAD R4, R0.reuse, c[0x0] [0x148].H1, RZ;\n\n    /*0048*/ XMAD R6, R0.H1, c[0x0] [0x148].H1, RZ;\n    /*0050*/ XMAD.CHI R5, R0.H1, c[0x0] [0x148], R5;\n    /*0058*/ IADD3.RS R0, R5, R4, R6;\n\n    /*0068*/ STG.E [R2], R0;\n    ...\n</code></pre>\n<p>XMAD is listed as \"Integer Short Multiply Add\", so it looks like the difference is between (3 FMA operations) vs. (4 FMA operations + 1 addition): it looks reasonable to me, unless .CHI is some secret codeword for \"super heavy slow computation\".</p>\n<p>(While we're at it, is there any public documentation for SASS instructions?  I could only find <a href=\"http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html\" rel=\"nofollow\">http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html</a> which is not much help.)</p>", "body_text": "I just built two toy kernels to compare, and the relevant SASS code for sm_61 (GTX 1080) looks like this:\n// *data = *data * a;\n\n    ...\n    /*0028*/ LDG.E R0, [R2];\n    /*0030*/ XMAD R5, R0.reuse, c[0x0] [0x148], RZ;\n    /*0038*/ XMAD.MRG R6, R0.reuse, c[0x0] [0x148].H1, RZ;\n\n    /*0048*/ XMAD.PSL.CBCC R0, R0.H1, R6.H1, R5;\n    /*0050*/ STG.E [R2], R0;\n    ...\n\n// *data = __umulhi(*data, a);\n\n    ...\n    /*0028*/ LDG.E R0, [R2];\n    /*0030*/ XMAD R5, R0.reuse, c[0x0] [0x148], RZ;\n    /*0038*/ XMAD R4, R0.reuse, c[0x0] [0x148].H1, RZ;\n\n    /*0048*/ XMAD R6, R0.H1, c[0x0] [0x148].H1, RZ;\n    /*0050*/ XMAD.CHI R5, R0.H1, c[0x0] [0x148], R5;\n    /*0058*/ IADD3.RS R0, R5, R4, R6;\n\n    /*0068*/ STG.E [R2], R0;\n    ...\n\nXMAD is listed as \"Integer Short Multiply Add\", so it looks like the difference is between (3 FMA operations) vs. (4 FMA operations + 1 addition): it looks reasonable to me, unless .CHI is some secret codeword for \"super heavy slow computation\".\n(While we're at it, is there any public documentation for SASS instructions?  I could only find http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html which is not much help.)", "in_reply_to_id": 166386897}