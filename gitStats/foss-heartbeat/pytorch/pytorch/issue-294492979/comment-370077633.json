{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/370077633", "html_url": "https://github.com/pytorch/pytorch/pull/5054#issuecomment-370077633", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5054", "id": 370077633, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDA3NzYzMw==", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-02T22:54:46Z", "updated_at": "2018-03-02T22:54:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1911637\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wickedfoo\">@wickedfoo</a>, thanks for the review.</p>\n<p>I ran several hundred configurations of tensor operations (on GTX 1080 / CUDA 9.1), including <code>add</code>, <code>mul</code>, <code>tanh</code>, <code>pow</code>, and <code>remainder</code>, on tensors with sizes ranging between 1000x64 and 4096x4096.  About 1/3 of them showed speedup of 5% or more.  There are some cases when performance improves by up to ~50%.  For the rest, performance didn't change meaningfully (within 5%).  I saw several cases of slowdown of ~5%, but these operations were too small (&lt; 20 us) and I couldn't reproduce them reliably: might be just random noise.</p>\n<p>The biggest win I could find was:</p>\n<pre><code>A = torch.cuda.HalfTensor(4096, 4096)\nB = torch.cuda.HalfTensor(2048)\nC = torch.cuda.HalfTensor(4096, 4096)\n\n# ~266 us to ~172 us (speedup 55%)\ntorch.mul(A[:, :2048], B, out=C[:, :2048])\n\nA = torch.cuda.HalfTensor(8192, 8192)\nB = torch.cuda.HalfTensor(4096)\nC = torch.cuda.HalfTensor(8192, 8192)\n\n# ~1.06 to ~0.69 ms (speedup 54%)\ntorch.mul(A[:, :4096], B, out=C[:, :4096])\n</code></pre>\n<p>We also have speedup for float operations, though not as dramatic:</p>\n<pre><code># ~26 to ~21 us (speedup 24%, but it's too noisy so might not be accurate)\nnrows, ncols, ncols2 = 1024, 1024, 512\n\n# ~365 to 312 us (speedup 17%)\nnrows, ncols, ncols2 = 4096, 4096, 2048\n\n# ~1.38 to 1.17 ms (speedup 18%)\nnrows, ncols, ncols2 = 8000, 8000, 4000\n\nA = torch.cuda.FloatTensor(nrows, ncols)\nB = torch.cuda.FloatTensor(ncols2)\nA = A[:, :ncols2]\n\nA.pow_(B)\n</code></pre>\n<p>For some other float operations, I observed speedup of ~25% for mid-size tensors (around 1000x128), but it becomes smaller as tensors get bigger (~9% for 1024x1024, ~3% for 8000x3000), probably because memory latency dominates everything for these tensors.</p>", "body_text": "Hi @wickedfoo, thanks for the review.\nI ran several hundred configurations of tensor operations (on GTX 1080 / CUDA 9.1), including add, mul, tanh, pow, and remainder, on tensors with sizes ranging between 1000x64 and 4096x4096.  About 1/3 of them showed speedup of 5% or more.  There are some cases when performance improves by up to ~50%.  For the rest, performance didn't change meaningfully (within 5%).  I saw several cases of slowdown of ~5%, but these operations were too small (< 20 us) and I couldn't reproduce them reliably: might be just random noise.\nThe biggest win I could find was:\nA = torch.cuda.HalfTensor(4096, 4096)\nB = torch.cuda.HalfTensor(2048)\nC = torch.cuda.HalfTensor(4096, 4096)\n\n# ~266 us to ~172 us (speedup 55%)\ntorch.mul(A[:, :2048], B, out=C[:, :2048])\n\nA = torch.cuda.HalfTensor(8192, 8192)\nB = torch.cuda.HalfTensor(4096)\nC = torch.cuda.HalfTensor(8192, 8192)\n\n# ~1.06 to ~0.69 ms (speedup 54%)\ntorch.mul(A[:, :4096], B, out=C[:, :4096])\n\nWe also have speedup for float operations, though not as dramatic:\n# ~26 to ~21 us (speedup 24%, but it's too noisy so might not be accurate)\nnrows, ncols, ncols2 = 1024, 1024, 512\n\n# ~365 to 312 us (speedup 17%)\nnrows, ncols, ncols2 = 4096, 4096, 2048\n\n# ~1.38 to 1.17 ms (speedup 18%)\nnrows, ncols, ncols2 = 8000, 8000, 4000\n\nA = torch.cuda.FloatTensor(nrows, ncols)\nB = torch.cuda.FloatTensor(ncols2)\nA = A[:, :ncols2]\n\nA.pow_(B)\n\nFor some other float operations, I observed speedup of ~25% for mid-size tensors (around 1000x128), but it becomes smaller as tensors get bigger (~9% for 1024x1024, ~3% for 8000x3000), probably because memory latency dominates everything for these tensors.", "body": "Hi @wickedfoo, thanks for the review.\r\n\r\nI ran several hundred configurations of tensor operations (on GTX 1080 / CUDA 9.1), including `add`, `mul`, `tanh`, `pow`, and `remainder`, on tensors with sizes ranging between 1000x64 and 4096x4096.  About 1/3 of them showed speedup of 5% or more.  There are some cases when performance improves by up to ~50%.  For the rest, performance didn't change meaningfully (within 5%).  I saw several cases of slowdown of ~5%, but these operations were too small (< 20 us) and I couldn't reproduce them reliably: might be just random noise.\r\n\r\nThe biggest win I could find was:\r\n\r\n    A = torch.cuda.HalfTensor(4096, 4096)\r\n    B = torch.cuda.HalfTensor(2048)\r\n    C = torch.cuda.HalfTensor(4096, 4096)\r\n\r\n    # ~266 us to ~172 us (speedup 55%)\r\n    torch.mul(A[:, :2048], B, out=C[:, :2048])\r\n\r\n    A = torch.cuda.HalfTensor(8192, 8192)\r\n    B = torch.cuda.HalfTensor(4096)\r\n    C = torch.cuda.HalfTensor(8192, 8192)\r\n\r\n    # ~1.06 to ~0.69 ms (speedup 54%)\r\n    torch.mul(A[:, :4096], B, out=C[:, :4096])\r\n\r\nWe also have speedup for float operations, though not as dramatic:\r\n\r\n    # ~26 to ~21 us (speedup 24%, but it's too noisy so might not be accurate)\r\n    nrows, ncols, ncols2 = 1024, 1024, 512\r\n\r\n    # ~365 to 312 us (speedup 17%)\r\n    nrows, ncols, ncols2 = 4096, 4096, 2048\r\n\r\n    # ~1.38 to 1.17 ms (speedup 18%)\r\n    nrows, ncols, ncols2 = 8000, 8000, 4000\r\n\r\n    A = torch.cuda.FloatTensor(nrows, ncols)\r\n    B = torch.cuda.FloatTensor(ncols2)\r\n    A = A[:, :ncols2]\r\n\r\n    A.pow_(B)\r\n\r\nFor some other float operations, I observed speedup of ~25% for mid-size tensors (around 1000x128), but it becomes smaller as tensors get bigger (~9% for 1024x1024, ~3% for 8000x3000), probably because memory latency dominates everything for these tensors."}