{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171657280", "pull_request_review_id": 100549799, "id": 171657280, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTY1NzI4MA==", "diff_hunk": "@@ -0,0 +1,89 @@\n+#ifndef THC_OFFSET_INFO_INC\n+#define THC_OFFSET_INFO_INC\n+\n+#include \"THCIntegerDivider.cuh\"\n+#include \"THCTensorInfo.cuh\"\n+\n+// A faster implementation of IndexToOffset that uses faster integer division:\n+// we transform each division into integer multiplication by a pre-computed\n+// constant.  (See IntDivider for details.)\n+\n+template <typename T, typename IndexType, int Dims>\n+struct OffsetInfo {\n+  explicit OffsetInfo(const TensorInfo<T, IndexType>& tinfo) {\n+    assert(tinfo.dims == Dims);\n+    data = tinfo.data;\n+\n+    for (int i = 0; i < Dims; ++i) {\n+      sizes[i] = IntDivider<IndexType>(tinfo.sizes[i]);\n+      strides[i] = tinfo.strides[i];\n+    }\n+  }\n+\n+  __host__ __device__ T* get(IndexType linearIndex) const {\n+    IndexType offset = 0;\n+\n+    for (int i = Dims - 1; i > 0; --i) {\n+      DivMod<IndexType> divmod = sizes[i].divmod(linearIndex);\n+      linearIndex = divmod.div;\n+      offset += divmod.mod * strides[i];\n+    }\n+\n+    offset += linearIndex * strides[0];\n+    return &data[offset];\n+  }\n+\n+  T* data;\n+  IntDivider<IndexType> sizes[Dims];\n+  IndexType strides[Dims];\n+};\n+\n+// For contiguous tensors (Dims=-2), offset equals linear index.\n+template <typename T, typename IndexType>\n+struct OffsetInfo<T, IndexType, -2> {\n+  explicit OffsetInfo(const TensorInfo<T, IndexType>& tinfo)\n+    : data(tinfo.data) {\n+    assert(tinfo.isContiguous());\n+  }\n+\n+  __host__ __device__ T* get(IndexType linearIndex) const {\n+    return &data[linearIndex];\n+  }\n+\n+  T* data;\n+};\n+\n+// Dims=-1 is used when the dimension is unknown at compile time.\n+//\n+// Unfortunately, pre-computation does not work here, because of a bug in nvcc\n+// (tested on CUDA 8.0): if a kernel argument contains an array that is\n+// dynamically accessed, the whole array is first copied into the local memory.\n+// (That is, every kernel thread makes its own copy of the argument, even if it\n+// is never updated.)  Pre-computation makes it worse because now we have more\n+// data to copy.\n+//\n+// So let's fall back to vanilla division approach.\n+\n+template <typename T, typename IndexType>\n+struct OffsetInfo<T, IndexType, -1> {\n+  explicit OffsetInfo(const TensorInfo<T, IndexType>& tinfo)\n+    : tinfo(tinfo) { }\n+\n+  __host__ __device__ T* get(IndexType linearIndex) const {\n+    IndexType offset = 0;\n+\n+    for (int i = tinfo.dims - 1; i > 0; --i) {", "path": "aten/src/THC/THCOffsetInfo.cuh", "position": null, "original_position": 75, "commit_id": "68723f3fd0b2a826fa4cd71878db24f40e99b2cf", "original_commit_id": "dccab1a119b2520b46a9a2f92f2a7c1524a45c1b", "user": {"login": "wickedfoo", "id": 1911637, "node_id": "MDQ6VXNlcjE5MTE2Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1911637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wickedfoo", "html_url": "https://github.com/wickedfoo", "followers_url": "https://api.github.com/users/wickedfoo/followers", "following_url": "https://api.github.com/users/wickedfoo/following{/other_user}", "gists_url": "https://api.github.com/users/wickedfoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/wickedfoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wickedfoo/subscriptions", "organizations_url": "https://api.github.com/users/wickedfoo/orgs", "repos_url": "https://api.github.com/users/wickedfoo/repos", "events_url": "https://api.github.com/users/wickedfoo/events{/privacy}", "received_events_url": "https://api.github.com/users/wickedfoo/received_events", "type": "User", "site_admin": false}, "body": "since IndexToOffset still exists, can you forward to IndexToOffset<T, IndexType, -1>, since this is effectively duplicated code?", "created_at": "2018-03-01T18:53:41Z", "updated_at": "2018-11-23T15:40:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/5054#discussion_r171657280", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5054", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171657280"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5054#discussion_r171657280"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5054"}}, "body_html": "<p>since IndexToOffset still exists, can you forward to IndexToOffset&lt;T, IndexType, -1&gt;, since this is effectively duplicated code?</p>", "body_text": "since IndexToOffset still exists, can you forward to IndexToOffset<T, IndexType, -1>, since this is effectively duplicated code?"}