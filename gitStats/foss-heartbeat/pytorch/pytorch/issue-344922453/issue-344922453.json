{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9882", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9882/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9882/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9882/events", "html_url": "https://github.com/pytorch/pytorch/issues/9882", "id": 344922453, "node_id": "MDU6SXNzdWUzNDQ5MjI0NTM=", "number": 9882, "title": "Unintuitive reduction of mini-batch loss for NLLLoss", "user": {"login": "timonbimon", "id": 14793026, "node_id": "MDQ6VXNlcjE0NzkzMDI2", "avatar_url": "https://avatars3.githubusercontent.com/u/14793026?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timonbimon", "html_url": "https://github.com/timonbimon", "followers_url": "https://api.github.com/users/timonbimon/followers", "following_url": "https://api.github.com/users/timonbimon/following{/other_user}", "gists_url": "https://api.github.com/users/timonbimon/gists{/gist_id}", "starred_url": "https://api.github.com/users/timonbimon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timonbimon/subscriptions", "organizations_url": "https://api.github.com/users/timonbimon/orgs", "repos_url": "https://api.github.com/users/timonbimon/repos", "events_url": "https://api.github.com/users/timonbimon/events{/privacy}", "received_events_url": "https://api.github.com/users/timonbimon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-07-26T16:47:03Z", "updated_at": "2018-08-08T04:57:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I find the reduction method that was chosen for the NLLLoss quite unintutive.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/14793026/43275573-774953a0-9102-11e8-8f84-8d8f7955cd9a.png\"><img width=\"712\" alt=\"screen shot 2018-07-26 at 18 33 10\" src=\"https://user-images.githubusercontent.com/14793026/43275573-774953a0-9102-11e8-8f84-8d8f7955cd9a.png\" style=\"max-width:100%;\"></a></p>\n<p>This introduces a weird interdependence of the chosen class weights with the chose batch size (and more: the influence of the class weights depend on which ground-truth classes are present in the mini-batch)<br>\nExtreme case with the current implementation: with batch size one, it does not matter which class weights I choose, my net will always see the same gradients.</p>\n<p>In other words: I would expect <code>F.nll_loss(..., reduce=True) == torch.mean(F.nll_los(..., reduce=False))</code> but this does not hold true when using different class weights.</p>\n<p>In the documentation of the CrossEntropyLoss it also says the following</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/14793026/43276191-2a0489c8-9104-11e8-9348-23b16ad5543b.png\"><img width=\"647\" alt=\"screen shot 2018-07-26 at 18 45 47\" src=\"https://user-images.githubusercontent.com/14793026/43276191-2a0489c8-9104-11e8-9348-23b16ad5543b.png\" style=\"max-width:100%;\"></a></p>\n<p>Especially the sentence \"The losses are averaged across observations for each minibatch.\" is very misleading with the current implementation if you are using class weights.</p>\n<p>I can only guess that the reason this implementation was chosen is s.t. your loss value doesn't change when you change the class weights (which makes multiple runs with different class weights more comparable when you're just looking at the loss), but it seems to come at a cost of a very unintuitive treatment of class weights, that in my opinion is not worth it.</p>", "body_text": "I find the reduction method that was chosen for the NLLLoss quite unintutive.\n\nThis introduces a weird interdependence of the chosen class weights with the chose batch size (and more: the influence of the class weights depend on which ground-truth classes are present in the mini-batch)\nExtreme case with the current implementation: with batch size one, it does not matter which class weights I choose, my net will always see the same gradients.\nIn other words: I would expect F.nll_loss(..., reduce=True) == torch.mean(F.nll_los(..., reduce=False)) but this does not hold true when using different class weights.\nIn the documentation of the CrossEntropyLoss it also says the following\n\nEspecially the sentence \"The losses are averaged across observations for each minibatch.\" is very misleading with the current implementation if you are using class weights.\nI can only guess that the reason this implementation was chosen is s.t. your loss value doesn't change when you change the class weights (which makes multiple runs with different class weights more comparable when you're just looking at the loss), but it seems to come at a cost of a very unintuitive treatment of class weights, that in my opinion is not worth it.", "body": "I find the reduction method that was chosen for the NLLLoss quite unintutive.\r\n\r\n<img width=\"712\" alt=\"screen shot 2018-07-26 at 18 33 10\" src=\"https://user-images.githubusercontent.com/14793026/43275573-774953a0-9102-11e8-8f84-8d8f7955cd9a.png\">\r\n\r\nThis introduces a weird interdependence of the chosen class weights with the chose batch size (and more: the influence of the class weights depend on which ground-truth classes are present in the mini-batch)\r\nExtreme case with the current implementation: with batch size one, it does not matter which class weights I choose, my net will always see the same gradients.\r\n\r\nIn other words: I would expect `F.nll_loss(..., reduce=True) == torch.mean(F.nll_los(..., reduce=False))` but this does not hold true when using different class weights.\r\n\r\nIn the documentation of the CrossEntropyLoss it also says the following\r\n\r\n<img width=\"647\" alt=\"screen shot 2018-07-26 at 18 45 47\" src=\"https://user-images.githubusercontent.com/14793026/43276191-2a0489c8-9104-11e8-9348-23b16ad5543b.png\">\r\n\r\nEspecially the sentence \"The losses are averaged across observations for each minibatch.\" is very misleading with the current implementation if you are using class weights.\r\n\r\nI can only guess that the reason this implementation was chosen is s.t. your loss value doesn't change when you change the class weights (which makes multiple runs with different class weights more comparable when you're just looking at the loss), but it seems to come at a cost of a very unintuitive treatment of class weights, that in my opinion is not worth it."}