{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163382541", "pull_request_review_id": 90985723, "id": 163382541, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzM4MjU0MQ==", "diff_hunk": "@@ -0,0 +1,109 @@\n+ATen native functions are a mechanism to write ATen methods which only\n+make use of other ATen operations (e.g., it is not necessary to bind into\n+TH/THC code).  These functions are declared in this file and then folded\n+into the ATen code generation process.\n+\n+## Registering it in `native_functions.yaml`\n+\n+The first step is to write an entry for your function in\n+`native_functions.yaml`.  The format is as follows:\n+\n+```\n+- func: func_name(ArgType arg0[=default], ArgType arg1[=default], ...) -> ReturnType\n+```\n+\n+ArgType(s) are allowed to be simple types understood by ATen\n+(e.g. `Tensor`, `TensorList`, `IntList`, `int64_t`, `double`, ...).\n+`Tensor?` means that the tensor is optional; if it is not passed by\n+the user it defaults to an undefined tensor (if you declare an argument\n+as optional, you must check if it is the case).\n+\n+ReturnType is allowed to be any ArgType or tuple combination of ArgTypes(s),\n+e.g. `(Tensor, Tensor)` defaults are optional and are only allowed to be numbers\n+(e.g. '0' for `int64_t`, '5.0' for `double`)\n+\n+The ATen code generation process will generate a header corresponding\n+to the C++ implementation you will have to write.\n+The C++ function declarations won't match the declaration here because\n+they will undergo the standard ATen C++ transformations, e.g. use of const-ref\n+for non-inplace Tensor arguments (instead of `Tensor` you will\n+take `const Tensor&`).  So after you finish writing your header, we recommend\n+running a build and getting the declaration from `NativeFunctions.h`\n+(find it with `find -name NativeFunctions.h`) to copy paste into your\n+C++ file.\n+\n+The declarations also support the following attributes:\n+\n+```\n+variants: function, method\n+```\n+\n+Controls whether Tensor method (`t.foo()`) or namespace Function (`at::foo()`) is\n+generated as a result of this declaration.  If the declaration is a method,\n+you must have an argument `Tensor self` as the first argument.  In general, you\n+should default to defining a new ATen function as `variants: function`, unless\n+you know you want something to be usable as a method.\n+\n+```\n+dispatch:\n+    CPU: func_cpu\n+    CUDA: func_cuda\n+```\n+\n+This specifies the actual name of the function you want to dispatch to, so you\n+can dispatch to different functions depending on whether or not you have CPU or\n+CUDA tensors.  Technically, it is also possible to write `dispatch: func_name`\n+to unconditionally dispatch to a native function whose name is different than\n+the name in the public ATen API, but this is generally frowned upon (just name\n+them the same thing!)\n+\n+```\n+python_default_init:\n+  argument_name: initializing_expression\n+```\n+\n+A map from argument names to default initialize expressions in C++. Such default\n+expressions will only be used in Python API. This allows us to write argument\n+with a default value that can either cause ambiguity in C++ (e.g., `Scalar p`\n+argument in `norm`) or have a type that doesn't allow default value\n+None/NULL/nullptr (e.g., `int64_t fft_size` argument in stft, which we want to\n+default to value of another argument if not provided).\n+\n+## Writing the implementation\n+\n+Implementations of native functions go in an appropriate C++ file in the\n+`native/` directory (they are organized roughly by topic, but there is no\n+semantic meaning to their organization, except for CUDA files, which must\n+go in `cuda`.)  To write a native function, you only need to write a C++\n+implementation (no header necessary) with a matching signature to\n+the generated header from the ATen metadata.  There are many\n+simple native functions; take a look at some of them to see what to do.\n+\n+There are some important gotchas which are important to keep in mind when\n+writing a native function:\n+\n+* NEVER EVER EVER use `at::CPU` or `at::CUDA` directly; instead, create tensors\n+  from the `type()` of one of the input tensors, e.g., `input.type().tensor()`\n+  or `input.type().toScalarType(kByte)` if you need a tensor type directly.\n+  See https://github.com/pytorch/pytorch/issues/4477 for more details.\n+\n+* Keep in mind whether or not your function must have an explicit derivative", "path": "aten/src/ATen/native/README.md", "position": null, "original_position": 90, "commit_id": "373dda6a697556b7b5dcfb6e0f6898b826e9877b", "original_commit_id": "f8b4ab1f6bd7f9527718773ef49690a2023cab05", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "If I was just starting out, I wouldn't know how to analyze this.  How do I know if my function is automatically differentiable or not?  (Maybe suggest they write an autograd test for it?)", "created_at": "2018-01-23T21:29:18Z", "updated_at": "2018-11-23T15:38:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/4816#discussion_r163382541", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4816", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163382541"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4816#discussion_r163382541"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4816"}}, "body_html": "<p>If I was just starting out, I wouldn't know how to analyze this.  How do I know if my function is automatically differentiable or not?  (Maybe suggest they write an autograd test for it?)</p>", "body_text": "If I was just starting out, I wouldn't know how to analyze this.  How do I know if my function is automatically differentiable or not?  (Maybe suggest they write an autograd test for it?)"}