{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10747", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10747/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10747/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10747/events", "html_url": "https://github.com/pytorch/pytorch/issues/10747", "id": 352682511, "node_id": "MDU6SXNzdWUzNTI2ODI1MTE=", "number": 10747, "title": "[JIT] Traced implicit promotion of types makes no sense", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-21T19:44:03Z", "updated_at": "2018-08-21T19:44:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Example:</p>\n<pre><code>import torch\n\n@torch.jit.trace(torch.rand(3))\ndef ne_test_explicit_byte(x):\n    mask = x != 0\n    return torch.ByteTensor([1]) - mask\n\nprint(ne_test_explicit_byte.graph)\n\n@torch.jit.trace(torch.rand(3))\ndef ne_test_implicit(x):\n    mask = x != 0\n    return 1 - mask\n\nprint(ne_test_implicit.graph)\n\n\n=====\n\ngraph(%0 : Float(3)) {\n  %1 : int = prim::Constant[value=0]()\n  %2 : Byte(3) = aten::ne(%0, %1)\n  %3 : Byte(1) = prim::Constant[value={1}]()\n  %4 : int = prim::Constant[value=0]()\n  %5 : Byte(1) = aten::_cast_Byte(%3, %4)\n  %6 : int = prim::Constant[value=1]()\n  %7 : Byte(3) = aten::sub(%5, %2, %6)\n  return (%7);\n}\n\ngraph(%0 : Float(3)) {\n  %1 : int = prim::Constant[value=0]()\n  %2 : Byte(3) = aten::ne(%0, %1)\n  %3 : Long() = prim::Constant[value={1}]()\n  %4 : int = prim::Constant[value=1]()\n  %5 : Byte(3) = aten::sub(%3, %2, %4)\n  return (%5);\n}\n</code></pre>\n<p>We're adding the _cast_Byte call when we don't need it (i.e. when the operand is already of type Byte) but we don't add it when we need it. This screws up some ONNX conversions into caffe2</p>", "body_text": "Example:\nimport torch\n\n@torch.jit.trace(torch.rand(3))\ndef ne_test_explicit_byte(x):\n    mask = x != 0\n    return torch.ByteTensor([1]) - mask\n\nprint(ne_test_explicit_byte.graph)\n\n@torch.jit.trace(torch.rand(3))\ndef ne_test_implicit(x):\n    mask = x != 0\n    return 1 - mask\n\nprint(ne_test_implicit.graph)\n\n\n=====\n\ngraph(%0 : Float(3)) {\n  %1 : int = prim::Constant[value=0]()\n  %2 : Byte(3) = aten::ne(%0, %1)\n  %3 : Byte(1) = prim::Constant[value={1}]()\n  %4 : int = prim::Constant[value=0]()\n  %5 : Byte(1) = aten::_cast_Byte(%3, %4)\n  %6 : int = prim::Constant[value=1]()\n  %7 : Byte(3) = aten::sub(%5, %2, %6)\n  return (%7);\n}\n\ngraph(%0 : Float(3)) {\n  %1 : int = prim::Constant[value=0]()\n  %2 : Byte(3) = aten::ne(%0, %1)\n  %3 : Long() = prim::Constant[value={1}]()\n  %4 : int = prim::Constant[value=1]()\n  %5 : Byte(3) = aten::sub(%3, %2, %4)\n  return (%5);\n}\n\nWe're adding the _cast_Byte call when we don't need it (i.e. when the operand is already of type Byte) but we don't add it when we need it. This screws up some ONNX conversions into caffe2", "body": "Example:\r\n\r\n```\r\nimport torch\r\n\r\n@torch.jit.trace(torch.rand(3))\r\ndef ne_test_explicit_byte(x):\r\n    mask = x != 0\r\n    return torch.ByteTensor([1]) - mask\r\n\r\nprint(ne_test_explicit_byte.graph)\r\n\r\n@torch.jit.trace(torch.rand(3))\r\ndef ne_test_implicit(x):\r\n    mask = x != 0\r\n    return 1 - mask\r\n\r\nprint(ne_test_implicit.graph)\r\n\r\n\r\n=====\r\n\r\ngraph(%0 : Float(3)) {\r\n  %1 : int = prim::Constant[value=0]()\r\n  %2 : Byte(3) = aten::ne(%0, %1)\r\n  %3 : Byte(1) = prim::Constant[value={1}]()\r\n  %4 : int = prim::Constant[value=0]()\r\n  %5 : Byte(1) = aten::_cast_Byte(%3, %4)\r\n  %6 : int = prim::Constant[value=1]()\r\n  %7 : Byte(3) = aten::sub(%5, %2, %6)\r\n  return (%7);\r\n}\r\n\r\ngraph(%0 : Float(3)) {\r\n  %1 : int = prim::Constant[value=0]()\r\n  %2 : Byte(3) = aten::ne(%0, %1)\r\n  %3 : Long() = prim::Constant[value={1}]()\r\n  %4 : int = prim::Constant[value=1]()\r\n  %5 : Byte(3) = aten::sub(%3, %2, %4)\r\n  return (%5);\r\n}\r\n```\r\n\r\nWe're adding the _cast_Byte call when we don't need it (i.e. when the operand is already of type Byte) but we don't add it when we need it. This screws up some ONNX conversions into caffe2"}