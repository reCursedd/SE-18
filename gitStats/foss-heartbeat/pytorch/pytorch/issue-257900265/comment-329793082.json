{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/329793082", "html_url": "https://github.com/pytorch/pytorch/issues/2747#issuecomment-329793082", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2747", "id": 329793082, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTc5MzA4Mg==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-15T14:07:50Z", "updated_at": "2017-09-15T14:07:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank you for the report. We definitely want to fix this. Here are a few things we'd like it if you tried:</p>\n<ol>\n<li>Is there any output to stderr when exporting fails? It should look like <code>Error occurred while handling: ... </code> and give you a pretty big clue. Is your IPython notebook suppressing stderr output?</li>\n<li>Can you apply this patch, and give us the graph printout from it?</li>\n</ol>\n<pre><code>diff --git a/torch/onnx.py b/torch/onnx.py\nindex b71802f..d9bdc86 100644\n--- a/torch/onnx.py\n+++ b/torch/onnx.py\n@@ -44,6 +44,7 @@ def _export(model, args, f, export_params=True, kwargs=None, verbose=False):\n     if not kwargs:\n         kwargs = {}\n     trace, torch_out = torch.jit.record_trace(model, *args, **kwargs)\n+    print(str(trace))\n     # TODO: Don't allocate a in-memory string for the protobuf\n     if export_params:\n         # NB: OrderedDict values is not actually a list, but trace.export is\n</code></pre>\n<p>It will look something like:</p>\n<pre><code>graph(%1 : Double(13, 16, 3, 3)\n      %2 : Double(20, 16, 50, 40)) {\n  %4 : UNKNOWN_TYPE = Undefined(), uses = [%3.i2];\n  %5 : Double(20, 13, 48, 38), %6 : Handle = CppOp[ConvForward](%2, %1, %4), uses = [[%0.i0], []];\n  return (%5);\n}\n</code></pre>\n<ol start=\"3\">\n<li>Can you post your PyTorch network? It doesn't have to be minimal, just enough for us to repro.</li>\n</ol>", "body_text": "Thank you for the report. We definitely want to fix this. Here are a few things we'd like it if you tried:\n\nIs there any output to stderr when exporting fails? It should look like Error occurred while handling: ...  and give you a pretty big clue. Is your IPython notebook suppressing stderr output?\nCan you apply this patch, and give us the graph printout from it?\n\ndiff --git a/torch/onnx.py b/torch/onnx.py\nindex b71802f..d9bdc86 100644\n--- a/torch/onnx.py\n+++ b/torch/onnx.py\n@@ -44,6 +44,7 @@ def _export(model, args, f, export_params=True, kwargs=None, verbose=False):\n     if not kwargs:\n         kwargs = {}\n     trace, torch_out = torch.jit.record_trace(model, *args, **kwargs)\n+    print(str(trace))\n     # TODO: Don't allocate a in-memory string for the protobuf\n     if export_params:\n         # NB: OrderedDict values is not actually a list, but trace.export is\n\nIt will look something like:\ngraph(%1 : Double(13, 16, 3, 3)\n      %2 : Double(20, 16, 50, 40)) {\n  %4 : UNKNOWN_TYPE = Undefined(), uses = [%3.i2];\n  %5 : Double(20, 13, 48, 38), %6 : Handle = CppOp[ConvForward](%2, %1, %4), uses = [[%0.i0], []];\n  return (%5);\n}\n\n\nCan you post your PyTorch network? It doesn't have to be minimal, just enough for us to repro.", "body": "Thank you for the report. We definitely want to fix this. Here are a few things we'd like it if you tried:\r\n\r\n1. Is there any output to stderr when exporting fails? It should look like `Error occurred while handling: ... ` and give you a pretty big clue. Is your IPython notebook suppressing stderr output?\r\n2. Can you apply this patch, and give us the graph printout from it?\r\n\r\n```\r\ndiff --git a/torch/onnx.py b/torch/onnx.py\r\nindex b71802f..d9bdc86 100644\r\n--- a/torch/onnx.py\r\n+++ b/torch/onnx.py\r\n@@ -44,6 +44,7 @@ def _export(model, args, f, export_params=True, kwargs=None, verbose=False):\r\n     if not kwargs:\r\n         kwargs = {}\r\n     trace, torch_out = torch.jit.record_trace(model, *args, **kwargs)\r\n+    print(str(trace))\r\n     # TODO: Don't allocate a in-memory string for the protobuf\r\n     if export_params:\r\n         # NB: OrderedDict values is not actually a list, but trace.export is\r\n```\r\n\r\nIt will look something like:\r\n\r\n```\r\ngraph(%1 : Double(13, 16, 3, 3)\r\n      %2 : Double(20, 16, 50, 40)) {\r\n  %4 : UNKNOWN_TYPE = Undefined(), uses = [%3.i2];\r\n  %5 : Double(20, 13, 48, 38), %6 : Handle = CppOp[ConvForward](%2, %1, %4), uses = [[%0.i0], []];\r\n  return (%5);\r\n}\r\n```\r\n\r\n3. Can you post your PyTorch network? It doesn't have to be minimal, just enough for us to repro."}