{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193563797", "pull_request_review_id": 126551952, "id": 193563797, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MzU2Mzc5Nw==", "diff_hunk": "@@ -0,0 +1,236 @@\n+#include \"ProcessGroupNCCL.hpp\"\n+#include \"CUDAUtils.hpp\"\n+#include \"FileStore.hpp\"\n+#include \"private/CUDAUtils.hpp\"\n+\n+#include \"test/CUDATest.hpp\"\n+#include \"test/TestUtils.hpp\"\n+\n+#include <iostream>\n+\n+using namespace c10d::test;\n+\n+using c10d::CUDADevice;\n+using c10d::CUDAStream;\n+using c10d::ProcessGroup;\n+using c10d::THCStreamGuard;\n+\n+class NCCLTestBase {\n+ public:\n+  NCCLTestBase(const std::string& path) : path_(path) {}\n+\n+  NCCLTestBase(NCCLTestBase&& other) {\n+    path_ = std::move(other.path_);\n+    pg_ = std::move(other.pg_);\n+  }\n+\n+  ::c10d::ProcessGroupNCCL& getProcessGroup() {\n+    return *pg_;\n+  }\n+\n+  void initialize(int rank, int size) {\n+    auto store = std::make_shared<::c10d::FileStore>(path_);\n+\n+    pg_ = std::unique_ptr<::c10d::ProcessGroupNCCL>(\n+        new ::c10d::ProcessGroupNCCL(store, rank, size));\n+  }\n+\n+ protected:\n+  std::string path_;\n+  std::unique_ptr<::c10d::ProcessGroupNCCL> pg_;\n+};\n+\n+class NCCLTest : public NCCLTestBase {\n+ public:\n+  NCCLTest(const std::string& path)\n+      : NCCLTestBase(path),\n+        numDevices_(cudaNumDevices()),\n+        state_(::at::globalContext().lazyInitCUDA()) {\n+    const auto& type = at::getType(at::kCUDA, at::kFloat);\n+\n+    // Each device has a single tensor to perf the NCCL op\n+    inputs_.resize(numDevices_);\n+    for (auto i = 0; i < numDevices_; i++) {\n+      CUDADevice device(i);\n+      inputs_[i] = type.tensor({3, 3});\n+    }\n+\n+    // Allocate a stream per device.\n+    //\n+    // The \"current stream\" is set globally per device in THC, so we\n+    // can't make two tensors on the same device use different streams\n+    // and pass this along to the collective (since it uses the THC\n+    // getters to retrieve the current stream).\n+    //\n+    streams_.resize(numDevices_);\n+    for (auto i = 0; i < numDevices_; i++) {\n+      CUDADevice device(i);\n+      streams_[i] = CUDAStream::create();\n+    }\n+  }\n+\n+  std::vector<THCStreamGuard> createStreamGuard() {\n+    std::vector<THCStreamGuard> guards;\n+    for (auto& stream : streams_) {\n+      guards.push_back(std::move(THCStreamGuard(state_, stream)));\n+    }\n+    return guards;\n+  }\n+\n+  void wait(std::shared_ptr<ProcessGroup::Work>& work) {\n+    auto guards = createStreamGuard();\n+    work->wait();\n+  }\n+\n+  std::vector<at::Tensor> getTensors() {\n+    std::vector<at::Tensor> outputs(numDevices_);\n+\n+    // For the duration of this function, make THC use our streams\n+    auto guards = createStreamGuard();\n+\n+    // Copy inputs to outputs\n+    for (auto i = 0; i < numDevices_; i++) {\n+      cudaStreamSynchronize(streams_[i].getStream());\n+      outputs[i] = inputs_[i].toBackend(at::kCPU);\n+    }\n+\n+    return outputs;\n+  }\n+\n+  int numDevices() const {\n+    return numDevices_;\n+  }\n+\n+ protected:\n+  const int numDevices_;\n+  THCState* state_;\n+  std::vector<at::Tensor> inputs_;\n+  std::vector<CUDAStream> streams_;\n+};\n+\n+class AllreduceNCCLTest : public NCCLTest {\n+ public:\n+  AllreduceNCCLTest(const std::string& path) : NCCLTest(path) {}\n+\n+  std::shared_ptr<c10d::ProcessGroup::Work> run() {\n+    // For the duration of this function, make THC use our streams\n+    auto guards = createStreamGuard();\n+\n+    // Launch sleep on every device\n+    for (auto i = 0; i < numDevices_; i++) {\n+      CUDADevice device(i);\n+      cudaSleep(streams_[i], 2000 * 1000 * 1000);\n+    }\n+\n+    // Launch value initialization for every tensor\n+    for (auto i = 0; i < numDevices_; i++) {\n+      CUDADevice device(i);\n+      inputs_[i].fill_(pg_->getRank() * numDevices_ + i);\n+    }\n+\n+    return pg_->allreduce(inputs_);\n+  }\n+};\n+\n+class BroadcastNCCLTest : public NCCLTest {\n+ public:\n+  BroadcastNCCLTest(const std::string& path) : NCCLTest(path) {}\n+\n+  std::shared_ptr<c10d::ProcessGroup::Work> run(int rootRank, int rootTensor) {\n+    // For the duration of this function, make THC use our streams\n+    auto guards = createStreamGuard();\n+\n+    // Launch sleep on every device\n+    for (auto i = 0; i < numDevices_; i++) {\n+      CUDADevice device(i);\n+      cudaSleep(streams_[i], 2000 * 1000 * 1000);\n+    }\n+\n+    // Launch value initialization for every tensor\n+    for (auto i = 0; i < numDevices_; i++) {\n+      CUDADevice device(i);\n+      inputs_[i].fill_(pg_->getRank() * numDevices_ + i);\n+    }\n+\n+    ::c10d::BroadcastOptions options;\n+    options.rootRank = rootRank;\n+    options.rootTensor = rootTensor;\n+    return pg_->broadcast(inputs_, options);\n+  }\n+};\n+\n+void testAllreduce(const std::string& path, int rank, int size) {\n+  auto test = AllreduceNCCLTest(path);\n+  test.initialize(rank, size);\n+  auto work = test.run();\n+  // Wait for work to finish\n+  test.wait(work);\n+\n+  // Validation\n+  const int totalNumGPUs = test.numDevices() * size;\n+  const auto expected = (totalNumGPUs * (totalNumGPUs - 1)) / 2;\n+  auto tensors = test.getTensors();\n+  for (auto j = 0; j < tensors.size(); j++) {\n+    auto& tensor = tensors[j];\n+    auto data = tensor.data<float>();\n+    for (auto k = 0; k < tensor.numel(); k++) {\n+      if (data[k] != expected) {\n+        throw std::runtime_error(\"BOOM!\");\n+      }\n+    }\n+  }\n+  std::cout << \"Allreduce test successful\" << std::endl;\n+}\n+\n+void testBroadcast(const std::string& path, int rank, int size) {\n+  auto test = BroadcastNCCLTest(path);\n+  test.initialize(rank, size);\n+\n+  const int numDevices = test.numDevices();\n+  // Try every permutation of root rank and root tensor\n+  for (auto rootRank = 0; rootRank < size; rootRank++) {\n+    for (auto rootTensor = 0; rootTensor < numDevices; rootTensor++) {\n+      auto work = test.run(rootRank, rootTensor);\n+\n+      // Wait for work to complete\n+      test.wait(work);\n+\n+      // Check results\n+      const auto expected = (rootRank * numDevices + rootTensor);\n+      auto tensors = test.getTensors();\n+      for (auto j = 0; j < tensors.size(); j++) {\n+        auto& tensor = tensors[j];\n+        auto data = tensor.data<float>();\n+        for (auto k = 0; k < tensor.numel(); k++) {\n+          if (data[k] != expected) {\n+            throw std::runtime_error(\"BOOM!\");\n+          }\n+        }\n+      }\n+    }\n+  }\n+  std::cout << \"Broadcast test successful\" << std::endl;\n+}\n+\n+int main(int argc, char** argv) {\n+  // Use WORLD_SIZE and RANK environmental variables to do multi-node\n+  // distributed testing\n+  auto sizeEnv = std::getenv(\"WORLD_SIZE\");\n+  auto rankEnv = std::getenv(\"RANK\");\n+\n+  int size = 1;\n+  int rank = 0;\n+\n+  if (sizeEnv && rankEnv) {\n+    size = std::stoi(std::string(sizeEnv));\n+    rank = std::stoi(std::string(rankEnv));\n+    std::cout << \"Multi-node world size: \" << size << \" rank: \" << rank\n+              << std::endl;\n+  }\n+\n+  TemporaryFile file;", "path": "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp", "position": null, "original_position": 231, "commit_id": "d50e66c0853fa5d79687e01eb1406ab5f2beb832", "original_commit_id": "1dae2c00d30fefed868ecf0890a6df0588265a33", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "The different tests are not scoped to the store, so they will end up writing to the same keys in this file. To fix this you can either use a different file per test, or add some kind of scoped store (there is one in caffe2/distributed for the caffe2 store API).", "created_at": "2018-06-06T21:23:36Z", "updated_at": "2018-11-23T15:45:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/8182#discussion_r193563797", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8182", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193563797"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8182#discussion_r193563797"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8182"}}, "body_html": "<p>The different tests are not scoped to the store, so they will end up writing to the same keys in this file. To fix this you can either use a different file per test, or add some kind of scoped store (there is one in caffe2/distributed for the caffe2 store API).</p>", "body_text": "The different tests are not scoped to the store, so they will end up writing to the same keys in this file. To fix this you can either use a different file per test, or add some kind of scoped store (there is one in caffe2/distributed for the caffe2 store API)."}