{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1483", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1483/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1483/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1483/events", "html_url": "https://github.com/pytorch/pytorch/issues/1483", "id": 226512882, "node_id": "MDU6SXNzdWUyMjY1MTI4ODI=", "number": 1483, "title": "How to calculate high order gradient of nn.Module with Non-linear Activations?", "user": {"login": "caogang", "id": 2537027, "node_id": "MDQ6VXNlcjI1MzcwMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2537027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caogang", "html_url": "https://github.com/caogang", "followers_url": "https://api.github.com/users/caogang/followers", "following_url": "https://api.github.com/users/caogang/following{/other_user}", "gists_url": "https://api.github.com/users/caogang/gists{/gist_id}", "starred_url": "https://api.github.com/users/caogang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caogang/subscriptions", "organizations_url": "https://api.github.com/users/caogang/orgs", "repos_url": "https://api.github.com/users/caogang/repos", "events_url": "https://api.github.com/users/caogang/events{/privacy}", "received_events_url": "https://api.github.com/users/caogang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-05T09:02:19Z", "updated_at": "2017-05-05T13:14:00Z", "closed_at": "2017-05-05T13:13:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I fail to calculate high order gradient of nn.Module with Non-linear Activations</p>\n<pre><code>def calc_gradient_penalty(netD, real_data, fake_data):\n    alpha = torch.rand(BATCH_SIZE, 1)\n    alpha = alpha.expand(real_data.size())\n    alpha = alpha.cuda() if use_cuda else alpha\n    \n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n    if use_cuda:\n        interpolates = interpolates.cuda()\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\n\n    disc_interpolates = netD(interpolates)\n\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(disc_interpolates.size()),\n                              create_graph=True, only_inputs=True, retain_graph=True)[0]\n\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n    return gradient_penalty\n\nuse_cuda = False\nBATCH_SIZE=256\nLAMBDA = 0.1\nDIM = 512\nnoise = torch.randn(BATCH_SIZE, 2)\nif use_cuda:\n    noise = noise.cuda()\nnoisev = autograd.Variable(noise)\n\nnoise1 = torch.randn(BATCH_SIZE, 2)\nif use_cuda:\n    noise1 = noise1.cuda()\nnoise1v = autograd.Variable(noise1)\n\nnetD = nn.Sequential(\n            nn.Linear(2, DIM),\n            nn.ReLU(True),\n            nn.Linear(DIM, DIM),\n            nn.ReLU(True),\n            nn.Linear(DIM, DIM),\n            nn.ReLU(True),\n            nn.Linear(DIM, 1),\n        )\nnetD.zero_grad()\nprint netD\ngp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\ngp.backward()\n</code></pre>\n<p>Then I got Runtime Error</p>\n<pre><code>RuntimeErrorTraceback (most recent call last)\n&lt;ipython-input-108-e8fda420b53c&gt; in &lt;module&gt;()\n     27 #     print p.grad\n     28 gp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\n---&gt; 29 gp.backward()\n     30 # for p in netD.parameters():\n     31 #     print p.grad\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_variables)\n    142                 raise TypeError(\"gradient has to be a Tensor, Variable or None\")\n    143             gradient = Variable(gradient, volatile=True)\n--&gt; 144         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n    145 \n    146     def register_hook(self, hook):\n\nRuntimeError: Threshold is not differentiable twice\n</code></pre>\n<p>So how can I solve this?</p>", "body_text": "I fail to calculate high order gradient of nn.Module with Non-linear Activations\ndef calc_gradient_penalty(netD, real_data, fake_data):\n    alpha = torch.rand(BATCH_SIZE, 1)\n    alpha = alpha.expand(real_data.size())\n    alpha = alpha.cuda() if use_cuda else alpha\n    \n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n    if use_cuda:\n        interpolates = interpolates.cuda()\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\n\n    disc_interpolates = netD(interpolates)\n\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(disc_interpolates.size()),\n                              create_graph=True, only_inputs=True, retain_graph=True)[0]\n\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n    return gradient_penalty\n\nuse_cuda = False\nBATCH_SIZE=256\nLAMBDA = 0.1\nDIM = 512\nnoise = torch.randn(BATCH_SIZE, 2)\nif use_cuda:\n    noise = noise.cuda()\nnoisev = autograd.Variable(noise)\n\nnoise1 = torch.randn(BATCH_SIZE, 2)\nif use_cuda:\n    noise1 = noise1.cuda()\nnoise1v = autograd.Variable(noise1)\n\nnetD = nn.Sequential(\n            nn.Linear(2, DIM),\n            nn.ReLU(True),\n            nn.Linear(DIM, DIM),\n            nn.ReLU(True),\n            nn.Linear(DIM, DIM),\n            nn.ReLU(True),\n            nn.Linear(DIM, 1),\n        )\nnetD.zero_grad()\nprint netD\ngp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\ngp.backward()\n\nThen I got Runtime Error\nRuntimeErrorTraceback (most recent call last)\n<ipython-input-108-e8fda420b53c> in <module>()\n     27 #     print p.grad\n     28 gp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\n---> 29 gp.backward()\n     30 # for p in netD.parameters():\n     31 #     print p.grad\n\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_variables)\n    142                 raise TypeError(\"gradient has to be a Tensor, Variable or None\")\n    143             gradient = Variable(gradient, volatile=True)\n--> 144         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n    145 \n    146     def register_hook(self, hook):\n\nRuntimeError: Threshold is not differentiable twice\n\nSo how can I solve this?", "body": "I fail to calculate high order gradient of nn.Module with Non-linear Activations\r\n```\r\ndef calc_gradient_penalty(netD, real_data, fake_data):\r\n    alpha = torch.rand(BATCH_SIZE, 1)\r\n    alpha = alpha.expand(real_data.size())\r\n    alpha = alpha.cuda() if use_cuda else alpha\r\n    \r\n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\r\n    if use_cuda:\r\n        interpolates = interpolates.cuda()\r\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\r\n\r\n    disc_interpolates = netD(interpolates)\r\n\r\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\r\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(disc_interpolates.size()),\r\n                              create_graph=True, only_inputs=True, retain_graph=True)[0]\r\n\r\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\r\n    return gradient_penalty\r\n\r\nuse_cuda = False\r\nBATCH_SIZE=256\r\nLAMBDA = 0.1\r\nDIM = 512\r\nnoise = torch.randn(BATCH_SIZE, 2)\r\nif use_cuda:\r\n    noise = noise.cuda()\r\nnoisev = autograd.Variable(noise)\r\n\r\nnoise1 = torch.randn(BATCH_SIZE, 2)\r\nif use_cuda:\r\n    noise1 = noise1.cuda()\r\nnoise1v = autograd.Variable(noise1)\r\n\r\nnetD = nn.Sequential(\r\n            nn.Linear(2, DIM),\r\n            nn.ReLU(True),\r\n            nn.Linear(DIM, DIM),\r\n            nn.ReLU(True),\r\n            nn.Linear(DIM, DIM),\r\n            nn.ReLU(True),\r\n            nn.Linear(DIM, 1),\r\n        )\r\nnetD.zero_grad()\r\nprint netD\r\ngp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\r\ngp.backward()\r\n```\r\nThen I got Runtime Error\r\n```\r\nRuntimeErrorTraceback (most recent call last)\r\n<ipython-input-108-e8fda420b53c> in <module>()\r\n     27 #     print p.grad\r\n     28 gp = calc_gradient_penalty(netD, noisev.data, noise1v.data)\r\n---> 29 gp.backward()\r\n     30 # for p in netD.parameters():\r\n     31 #     print p.grad\r\n\r\n/home/users/gang.cao/env/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_variables)\r\n    142                 raise TypeError(\"gradient has to be a Tensor, Variable or None\")\r\n    143             gradient = Variable(gradient, volatile=True)\r\n--> 144         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n    145 \r\n    146     def register_hook(self, hook):\r\n\r\nRuntimeError: Threshold is not differentiable twice\r\n```\r\n\r\nSo how can I solve this?"}