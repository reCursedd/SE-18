{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9183", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9183/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9183/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9183/events", "html_url": "https://github.com/pytorch/pytorch/pull/9183", "id": 338565418, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk5NDYzMDk5", "number": 9183, "title": "Updated WeightedRandomSampler to fit usage of other samplers", "user": {"login": "rsnk96", "id": 10851575, "node_id": "MDQ6VXNlcjEwODUxNTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/10851575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsnk96", "html_url": "https://github.com/rsnk96", "followers_url": "https://api.github.com/users/rsnk96/followers", "following_url": "https://api.github.com/users/rsnk96/following{/other_user}", "gists_url": "https://api.github.com/users/rsnk96/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsnk96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsnk96/subscriptions", "organizations_url": "https://api.github.com/users/rsnk96/orgs", "repos_url": "https://api.github.com/users/rsnk96/repos", "events_url": "https://api.github.com/users/rsnk96/events{/privacy}", "received_events_url": "https://api.github.com/users/rsnk96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 559719279, "node_id": "MDU6TGFiZWw1NTk3MTkyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ready%20for%20review", "name": "ready for review", "color": "b60205", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-05T13:02:48Z", "updated_at": "2018-09-12T18:59:02Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9183", "html_url": "https://github.com/pytorch/pytorch/pull/9183", "diff_url": "https://github.com/pytorch/pytorch/pull/9183.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9183.patch"}, "body_html": "<p>Commit to address <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"338373840\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9171\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9171/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9171\">#9171</a></p>\n<p>It does not break functioning of <code>BatchSampler()</code> or any of the other samplers, tested with the following code. It should also make it easy to dynamically change sampling weights in every call of the iterator of <code>WeightedRandomSampler</code></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets, transforms\n<span class=\"pl-k\">from</span> torch._six <span class=\"pl-k\">import</span> int_classes <span class=\"pl-k\">as</span> _int_classes\n\n\nmnist_dataset <span class=\"pl-k\">=</span> datasets.MNIST(\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>../data<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n    <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose([transforms.ToTensor()]),\n)\ntest_loader_no_sampler <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>\n)\ntest_loader_random <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_dataset, <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.RandomSampler(mnist_dataset)\n)\ntest_loader_subsetrandom <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_dataset, <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.SubsetRandomSampler(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>))\n)\ntest_loader_subsetrandom_2 <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_dataset,\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n    <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.SubsetRandomSampler(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>)),\n)\ntest_loader_weightedrandom <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_dataset,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.WeightedRandomSampler(<span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">10000</span>),\n)\ntest_loader_weightedrandom_2 <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_dataset,\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n    <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.WeightedRandomSampler(\n        <span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">10000</span>, <span class=\"pl-v\">num_samples</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>\n    ),\n)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader_no_sampler):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>No sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader_random):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader_subsetrandom):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Subset Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader_subsetrandom_2):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Subset Random sampler with batch size 100: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader_weightedrandom):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Weighted Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader_weightedrandom_2):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Weighted Random sampler 100 batch size and 2 samples every draw: <span class=\"pl-pds\">\"</span></span>, iter_no)\n</pre></div>\n<p>Output:</p>\n<pre><code>No sampler:  9999\nRandom sampler:  9999\nSubset Random sampler:  9999\nSubset Random sampler with batch size 100:  99\nWeighted Random sampler:  9999\nWeighted Random sampler 100 batch size and 2 samples every draw:  199\n</code></pre>", "body_text": "Commit to address #9171\nIt does not break functioning of BatchSampler() or any of the other samplers, tested with the following code. It should also make it easy to dynamically change sampling weights in every call of the iterator of WeightedRandomSampler\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch._six import int_classes as _int_classes\n\n\nmnist_dataset = datasets.MNIST(\n    \"../data\",\n    train=False,\n    download=True,\n    transform=transforms.Compose([transforms.ToTensor()]),\n)\ntest_loader_no_sampler = torch.utils.data.DataLoader(\n    mnist_dataset, batch_size=1, shuffle=True\n)\ntest_loader_random = torch.utils.data.DataLoader(\n    mnist_dataset, sampler=torch.utils.data.sampler.RandomSampler(mnist_dataset)\n)\ntest_loader_subsetrandom = torch.utils.data.DataLoader(\n    mnist_dataset, sampler=torch.utils.data.sampler.SubsetRandomSampler(range(10000))\n)\ntest_loader_subsetrandom_2 = torch.utils.data.DataLoader(\n    mnist_dataset,\n    batch_size=100,\n    num_workers=4,\n    sampler=torch.utils.data.sampler.SubsetRandomSampler(range(10000)),\n)\ntest_loader_weightedrandom = torch.utils.data.DataLoader(\n    mnist_dataset,\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(weights=[1] * 10000),\n)\ntest_loader_weightedrandom_2 = torch.utils.data.DataLoader(\n    mnist_dataset,\n    batch_size=100,\n    num_workers=4,\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(\n        weights=[1] * 10000, num_samples=2\n    ),\n)\n\nfor iter_no, (data, label) in enumerate(test_loader_no_sampler):\n    continue\nprint(\"No sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(test_loader_random):\n    continue\nprint(\"Random sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(test_loader_subsetrandom):\n    continue\nprint(\"Subset Random sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(test_loader_subsetrandom_2):\n    continue\nprint(\"Subset Random sampler with batch size 100: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(test_loader_weightedrandom):\n    continue\nprint(\"Weighted Random sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(test_loader_weightedrandom_2):\n    continue\nprint(\"Weighted Random sampler 100 batch size and 2 samples every draw: \", iter_no)\n\nOutput:\nNo sampler:  9999\nRandom sampler:  9999\nSubset Random sampler:  9999\nSubset Random sampler with batch size 100:  99\nWeighted Random sampler:  9999\nWeighted Random sampler 100 batch size and 2 samples every draw:  199", "body": "Commit to address #9171 \r\n\r\nIt does not break functioning of `BatchSampler()` or any of the other samplers, tested with the following code. It should also make it easy to dynamically change sampling weights in every call of the iterator of `WeightedRandomSampler`\r\n\r\n```python\r\nimport torch\r\nfrom torchvision import datasets, transforms\r\nfrom torch._six import int_classes as _int_classes\r\n\r\n\r\nmnist_dataset = datasets.MNIST(\r\n    \"../data\",\r\n    train=False,\r\n    download=True,\r\n    transform=transforms.Compose([transforms.ToTensor()]),\r\n)\r\ntest_loader_no_sampler = torch.utils.data.DataLoader(\r\n    mnist_dataset, batch_size=1, shuffle=True\r\n)\r\ntest_loader_random = torch.utils.data.DataLoader(\r\n    mnist_dataset, sampler=torch.utils.data.sampler.RandomSampler(mnist_dataset)\r\n)\r\ntest_loader_subsetrandom = torch.utils.data.DataLoader(\r\n    mnist_dataset, sampler=torch.utils.data.sampler.SubsetRandomSampler(range(10000))\r\n)\r\ntest_loader_subsetrandom_2 = torch.utils.data.DataLoader(\r\n    mnist_dataset,\r\n    batch_size=100,\r\n    num_workers=4,\r\n    sampler=torch.utils.data.sampler.SubsetRandomSampler(range(10000)),\r\n)\r\ntest_loader_weightedrandom = torch.utils.data.DataLoader(\r\n    mnist_dataset,\r\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(weights=[1] * 10000),\r\n)\r\ntest_loader_weightedrandom_2 = torch.utils.data.DataLoader(\r\n    mnist_dataset,\r\n    batch_size=100,\r\n    num_workers=4,\r\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(\r\n        weights=[1] * 10000, num_samples=2\r\n    ),\r\n)\r\n\r\nfor iter_no, (data, label) in enumerate(test_loader_no_sampler):\r\n    continue\r\nprint(\"No sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(test_loader_random):\r\n    continue\r\nprint(\"Random sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(test_loader_subsetrandom):\r\n    continue\r\nprint(\"Subset Random sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(test_loader_subsetrandom_2):\r\n    continue\r\nprint(\"Subset Random sampler with batch size 100: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(test_loader_weightedrandom):\r\n    continue\r\nprint(\"Weighted Random sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(test_loader_weightedrandom_2):\r\n    continue\r\nprint(\"Weighted Random sampler 100 batch size and 2 samples every draw: \", iter_no)\r\n\r\n```\r\n\r\nOutput:\r\n```\r\nNo sampler:  9999\r\nRandom sampler:  9999\r\nSubset Random sampler:  9999\r\nSubset Random sampler with batch size 100:  99\r\nWeighted Random sampler:  9999\r\nWeighted Random sampler 100 batch size and 2 samples every draw:  199\r\n```"}