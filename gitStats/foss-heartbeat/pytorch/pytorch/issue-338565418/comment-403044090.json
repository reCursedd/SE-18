{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/403044090", "html_url": "https://github.com/pytorch/pytorch/pull/9183#issuecomment-403044090", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9183", "id": 403044090, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzA0NDA5MA==", "user": {"login": "rsnk96", "id": 10851575, "node_id": "MDQ6VXNlcjEwODUxNTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/10851575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsnk96", "html_url": "https://github.com/rsnk96", "followers_url": "https://api.github.com/users/rsnk96/followers", "following_url": "https://api.github.com/users/rsnk96/following{/other_user}", "gists_url": "https://api.github.com/users/rsnk96/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsnk96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsnk96/subscriptions", "organizations_url": "https://api.github.com/users/rsnk96/orgs", "repos_url": "https://api.github.com/users/rsnk96/repos", "events_url": "https://api.github.com/users/rsnk96/events{/privacy}", "received_events_url": "https://api.github.com/users/rsnk96/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-06T14:04:24Z", "updated_at": "2018-07-06T14:15:11Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Thank you for your comments</p>\n<ol>\n<li>I agree that all samplers should have the same interface, which is why this pull request. Please refer to issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"338373840\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9171\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9171/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9171\">#9171</a> . This pull request makes it possible to use <code>WeightedRandomSampler</code> the same way as the other samplers (see code below for an example)\n<ol>\n<li>Other samplers don't require the <code>num_samples</code> argument as a mandatory argument. So a user who has not gone through the source code might assume it is the number of samples drawn per iteration of the weight distribution, which they would leave as 1 (hence, a for loop would iterate over the <code>DataLoader</code> only once). Take <code>SubsetRandomSampler</code>, for instance. It's usage should ideally be the same as a <code>WeightedRandomSampler</code> code with all the weights being the same (as in the test example in the PR). This can be solved by either adding a <code>num_samples</code> argument to all the other samplers or by making <code>WeightedRandomSampler</code> similar to the other samplers right now (see code below for an example).</li>\n<li>Another problem that is solved with this PR is that a user can now dynamically change the weights of the distribution after every sample if the code is written in a manner similar to the <a href=\"https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#training-the-model\" rel=\"nofollow\">Pytorch Tutorials</a> where we iterate over the epochs and then an inner for loop for iterating over samples within that epoch. Doing this right now (without this PR) using <code>WeightedRandomSampler</code> would require us to set <code>num_samples = 1</code>, and iterate over just one outer loop similar to the following code</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> iter_no <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epochs<span class=\"pl-k\">*</span>dataset_size):  <span class=\"pl-c\"><span class=\"pl-c\">#</span>Note that the epochs and within-epoch loops have been condensed here</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># Do some stuff, calculate new weights</span>\n    train_loader_weightedrandom.sampler.weights <span class=\"pl-k\">=</span> new_weights</pre></div>\n</li>\n<li>Thank you for pointing out the iterability issue. I've pushed another change, it can be iterated over multiple times now. You can use the following code to check the same</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets, transforms\n\nmnist_dataset <span class=\"pl-k\">=</span> datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, \n                               <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose(\n                                   [transforms.ToTensor()]))\ntrain_loader_weightedrandom <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(mnist_dataset,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.WeightedRandomSampler(<span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">10000</span>))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> With this PR, this is the equivalent of</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train_loader_subsetrandom = torch.utils.data.DataLoader(mnist_dataset,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>         sampler = torch.utils.data.sampler.SubsetRandomSampler(range(10000)))</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> W/o the updates of this PR, we'd have to also give num_samples=10000</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> But in cases like dynamically changing the weights (pointed out above), you'd still </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> have to condense the epoch and within-epoch loops</span>\n\n<span class=\"pl-k\">for</span> epoch_no <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assuming 3 epochs for simplicity</span>\n    iter_no <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader_weightedrandom):\n        <span class=\"pl-k\">continue</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Weighted Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)<span class=\"pl-bu\">```</span></pre></div>\n<p>Output:</p>\n<pre><code>Weighted Random sampler:  9999\nWeighted Random sampler:  9999\nWeighted Random sampler:  9999\n</code></pre>", "body_text": "@apaszke Thank you for your comments\n\nI agree that all samplers should have the same interface, which is why this pull request. Please refer to issue #9171 . This pull request makes it possible to use WeightedRandomSampler the same way as the other samplers (see code below for an example)\n\nOther samplers don't require the num_samples argument as a mandatory argument. So a user who has not gone through the source code might assume it is the number of samples drawn per iteration of the weight distribution, which they would leave as 1 (hence, a for loop would iterate over the DataLoader only once). Take SubsetRandomSampler, for instance. It's usage should ideally be the same as a WeightedRandomSampler code with all the weights being the same (as in the test example in the PR). This can be solved by either adding a num_samples argument to all the other samplers or by making WeightedRandomSampler similar to the other samplers right now (see code below for an example).\nAnother problem that is solved with this PR is that a user can now dynamically change the weights of the distribution after every sample if the code is written in a manner similar to the Pytorch Tutorials where we iterate over the epochs and then an inner for loop for iterating over samples within that epoch. Doing this right now (without this PR) using WeightedRandomSampler would require us to set num_samples = 1, and iterate over just one outer loop similar to the following code\n\nfor iter_no in range(epochs*dataset_size):  #Note that the epochs and within-epoch loops have been condensed here\n    ## Do some stuff, calculate new weights\n    train_loader_weightedrandom.sampler.weights = new_weights\n\nThank you for pointing out the iterability issue. I've pushed another change, it can be iterated over multiple times now. You can use the following code to check the same\n\nimport torch\nfrom torchvision import datasets, transforms\n\nmnist_dataset = datasets.MNIST('data', train=False, download=True, \n                               transform=transforms.Compose(\n                                   [transforms.ToTensor()]))\ntrain_loader_weightedrandom = torch.utils.data.DataLoader(mnist_dataset,\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(weights=[1] * 10000))\n# With this PR, this is the equivalent of\n# train_loader_subsetrandom = torch.utils.data.DataLoader(mnist_dataset,\n#         sampler = torch.utils.data.sampler.SubsetRandomSampler(range(10000)))\n# W/o the updates of this PR, we'd have to also give num_samples=10000\n# But in cases like dynamically changing the weights (pointed out above), you'd still \n# have to condense the epoch and within-epoch loops\n\nfor epoch_no in range(3):  # Assuming 3 epochs for simplicity\n    iter_no = 0\n    for iter_no, (data, label) in enumerate(train_loader_weightedrandom):\n        continue\n    print(\"Weighted Random sampler: \", iter_no)```\nOutput:\nWeighted Random sampler:  9999\nWeighted Random sampler:  9999\nWeighted Random sampler:  9999", "body": "@apaszke Thank you for your comments\r\n\r\n1. I agree that all samplers should have the same interface, which is why this pull request. Please refer to issue #9171 . This pull request makes it possible to use `WeightedRandomSampler` the same way as the other samplers (see code below for an example)\r\n    1. Other samplers don't require the `num_samples` argument as a mandatory argument. So a user who has not gone through the source code might assume it is the number of samples drawn per iteration of the weight distribution, which they would leave as 1 (hence, a for loop would iterate over the `DataLoader` only once). Take `SubsetRandomSampler`, for instance. It's usage should ideally be the same as a `WeightedRandomSampler` code with all the weights being the same (as in the test example in the PR). This can be solved by either adding a `num_samples` argument to all the other samplers or by making `WeightedRandomSampler` similar to the other samplers right now (see code below for an example).\r\n    2. Another problem that is solved with this PR is that a user can now dynamically change the weights of the distribution after every sample if the code is written in a manner similar to the [Pytorch Tutorials](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#training-the-model) where we iterate over the epochs and then an inner for loop for iterating over samples within that epoch. Doing this right now (without this PR) using `WeightedRandomSampler` would require us to set `num_samples = 1`, and iterate over just one outer loop similar to the following code\r\n    ```python\r\n    for iter_no in range(epochs*dataset_size):  #Note that the epochs and within-epoch loops have been condensed here\r\n        ## Do some stuff, calculate new weights\r\n        train_loader_weightedrandom.sampler.weights = new_weights\r\n    ```\r\n2. Thank you for pointing out the iterability issue. I've pushed another change, it can be iterated over multiple times now. You can use the following code to check the same\r\n\r\n```python\r\nimport torch\r\nfrom torchvision import datasets, transforms\r\n\r\nmnist_dataset = datasets.MNIST('data', train=False, download=True, \r\n                               transform=transforms.Compose(\r\n                                   [transforms.ToTensor()]))\r\ntrain_loader_weightedrandom = torch.utils.data.DataLoader(mnist_dataset,\r\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(weights=[1] * 10000))\r\n# With this PR, this is the equivalent of\r\n# train_loader_subsetrandom = torch.utils.data.DataLoader(mnist_dataset,\r\n#         sampler = torch.utils.data.sampler.SubsetRandomSampler(range(10000)))\r\n# W/o the updates of this PR, we'd have to also give num_samples=10000\r\n# But in cases like dynamically changing the weights (pointed out above), you'd still \r\n# have to condense the epoch and within-epoch loops\r\n\r\nfor epoch_no in range(3):  # Assuming 3 epochs for simplicity\r\n    iter_no = 0\r\n    for iter_no, (data, label) in enumerate(train_loader_weightedrandom):\r\n        continue\r\n    print(\"Weighted Random sampler: \", iter_no)```\r\n```\r\n\r\nOutput:\r\n```\r\nWeighted Random sampler:  9999\r\nWeighted Random sampler:  9999\r\nWeighted Random sampler:  9999\r\n```"}