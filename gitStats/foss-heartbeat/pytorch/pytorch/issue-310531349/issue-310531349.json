{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6190", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6190/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6190/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6190/events", "html_url": "https://github.com/pytorch/pytorch/issues/6190", "id": 310531349, "node_id": "MDU6SXNzdWUzMTA1MzEzNDk=", "number": 6190, "title": "[PyTorch]: bad digamma float32 accuracy", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-02T16:25:51Z", "updated_at": "2018-04-13T15:49:10Z", "closed_at": "2018-04-13T15:49:10Z", "author_association": "MEMBER", "body_html": "<p>PyTorch master@92a0f78</p>\n<p>Our digamma single-precision floating point accuracy is bad near the poles. This is not caught by our tests because we only test on a few values:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/92a0f7835e7e2f6342fe2fd32a25299ea86aed31/test/test_torch.py#L272-L274\">pytorch/test/test_torch.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 272 to 274\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/92a0f7835e7e2f6342fe2fd32a25299ea86aed31\">92a0f78</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L272\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"272\"></td>\n          <td id=\"LC272\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_digamma</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L273\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"273\"></td>\n          <td id=\"LC273\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">from</span> scipy.special <span class=\"pl-k\">import</span> digamma </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L274\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"274\"></td>\n          <td id=\"LC274\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c1\">self</span>._testMath(torch.digamma, digamma, <span class=\"pl-v\">large</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">precs</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2e-8</span>, <span class=\"pl-c1\">3e-4</span>)) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.99999994</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float32)\nfp32 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.digamma()\nfp64 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.double().digamma()\n<span class=\"pl-c1\">print</span>(((fp32.double() <span class=\"pl-k\">-</span> fp64) <span class=\"pl-k\">/</span> fp64).item())  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 0.24012388522631392</span></pre></div>\n<p>This is a relative error of <strong>24%</strong></p>\n<p>We also are returning real numbers at the poles (for both float32 and float64) when we should be returning inf.</p>", "body_text": "PyTorch master@92a0f78\nOur digamma single-precision floating point accuracy is bad near the poles. This is not caught by our tests because we only test on a few values:\n\n  \n    \n      pytorch/test/test_torch.py\n    \n    \n        Lines 272 to 274\n      in\n      92a0f78\n    \n    \n    \n    \n\n        \n          \n           def test_digamma(self): \n        \n\n        \n          \n               from scipy.special import digamma \n        \n\n        \n          \n               self._testMath(torch.digamma, digamma, large=False, precs=(2e-8, 3e-4)) \n        \n    \n  \n\n\ninput = torch.tensor([-1.99999994], dtype=torch.float32)\nfp32 = input.digamma()\nfp64 = input.double().digamma()\nprint(((fp32.double() - fp64) / fp64).item())  # 0.24012388522631392\nThis is a relative error of 24%\nWe also are returning real numbers at the poles (for both float32 and float64) when we should be returning inf.", "body": "PyTorch master@92a0f78\r\n\r\nOur digamma single-precision floating point accuracy is bad near the poles. This is not caught by our tests because we only test on a few values:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/92a0f7835e7e2f6342fe2fd32a25299ea86aed31/test/test_torch.py#L272-L274\r\n\r\n```python\r\ninput = torch.tensor([-1.99999994], dtype=torch.float32)\r\nfp32 = input.digamma()\r\nfp64 = input.double().digamma()\r\nprint(((fp32.double() - fp64) / fp64).item())  # 0.24012388522631392\r\n```\r\n\r\nThis is a relative error of **24%**\r\n\r\nWe also are returning real numbers at the poles (for both float32 and float64) when we should be returning inf."}