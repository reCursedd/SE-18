{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/429546280", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-429546280", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 429546280, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU0NjI4MA==", "user": {"login": "AntoinePrv", "id": 11088808, "node_id": "MDQ6VXNlcjExMDg4ODA4", "avatar_url": "https://avatars0.githubusercontent.com/u/11088808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AntoinePrv", "html_url": "https://github.com/AntoinePrv", "followers_url": "https://api.github.com/users/AntoinePrv/followers", "following_url": "https://api.github.com/users/AntoinePrv/following{/other_user}", "gists_url": "https://api.github.com/users/AntoinePrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/AntoinePrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AntoinePrv/subscriptions", "organizations_url": "https://api.github.com/users/AntoinePrv/orgs", "repos_url": "https://api.github.com/users/AntoinePrv/repos", "events_url": "https://api.github.com/users/AntoinePrv/events{/privacy}", "received_events_url": "https://api.github.com/users/AntoinePrv/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-13T14:24:18Z", "updated_at": "2018-10-13T14:26:52Z", "author_association": "NONE", "body_html": "<p>Another use case comes to mind. It would be very convenient if one could (limited) slice/ concat/ stack, across a batch dimension. This would be to represent sparse data batched together, and work seamlessly with <code>Dataset</code>/<code>Dataloader</code> (<code>Dataparallel</code> ?).</p>\n<p><strong>Caveat</strong>: if all tensors in the batch don't have the same size, we can make the batch tensor have the max size across feature dimensions (doesn't store anything more), but the user would need to remember the original sizes by themselves.<br>\nStill seems a bit hacky... I don't know if there is something better. Perhaps, something similar to <code>PackedSequences</code>?</p>", "body_text": "Another use case comes to mind. It would be very convenient if one could (limited) slice/ concat/ stack, across a batch dimension. This would be to represent sparse data batched together, and work seamlessly with Dataset/Dataloader (Dataparallel ?).\nCaveat: if all tensors in the batch don't have the same size, we can make the batch tensor have the max size across feature dimensions (doesn't store anything more), but the user would need to remember the original sizes by themselves.\nStill seems a bit hacky... I don't know if there is something better. Perhaps, something similar to PackedSequences?", "body": "Another use case comes to mind. It would be very convenient if one could (limited) slice/ concat/ stack, across a batch dimension. This would be to represent sparse data batched together, and work seamlessly with `Dataset`/`Dataloader` (`Dataparallel` ?).\r\n\r\n**Caveat**: if all tensors in the batch don't have the same size, we can make the batch tensor have the max size across feature dimensions (doesn't store anything more), but the user would need to remember the original sizes by themselves.\r\nStill seems a bit hacky... I don't know if there is something better. Perhaps, something similar to `PackedSequences`?"}