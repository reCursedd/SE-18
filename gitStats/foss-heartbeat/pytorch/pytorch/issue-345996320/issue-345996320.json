{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043/events", "html_url": "https://github.com/pytorch/pytorch/issues/10043", "id": 345996320, "node_id": "MDU6SXNzdWUzNDU5OTYzMjA=", "number": 10043, "title": "Sparse tensor use cases", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/sparse", "name": "sparse", "color": "bfd4f2", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 49, "created_at": "2018-07-31T01:06:25Z", "updated_at": "2018-11-20T22:21:24Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>We are working on to increase supports for sparse tensor. Currently we have <a href=\"https://github.com/pytorch/pytorch/issues/9674\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9674/hovercard\">summarized current state of sparse tensor</a> and listed out <a href=\"https://github.com/pytorch/pytorch/issues/8853\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8853/hovercard\">sparse ops to support</a>. We would like to collect sparse tensor use cases to facilitate the design decisions and prioritize TODO list according. It will be very helpful if you can post your use cases and desired sparse ops here or at the <a href=\"https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047\" rel=\"nofollow\">PyTorch Forum</a>. Thanks!</p>\n<p>I find these questions useful when writing use cases:</p>\n<ul>\n<li>Where do I need sparse tensor? During training deep learning model?</li>\n<li>Do I need autograd support for the sparse ops?</li>\n</ul>\n<p>A possible example will be:</p>\n<p>I am training model that has <code>mul(Sparse, Dense)</code> ops. I would like to have its forward and backward. I know there will be a dense gradient at the backward of <code>mul</code>, so here I am asking for a special kind of <code>mul</code> ops (called <code>sparse_mul</code>) that returns a sparse grad tensor and only keep the nnz's gradients.</p>", "body_text": "We are working on to increase supports for sparse tensor. Currently we have summarized current state of sparse tensor and listed out sparse ops to support. We would like to collect sparse tensor use cases to facilitate the design decisions and prioritize TODO list according. It will be very helpful if you can post your use cases and desired sparse ops here or at the PyTorch Forum. Thanks!\nI find these questions useful when writing use cases:\n\nWhere do I need sparse tensor? During training deep learning model?\nDo I need autograd support for the sparse ops?\n\nA possible example will be:\nI am training model that has mul(Sparse, Dense) ops. I would like to have its forward and backward. I know there will be a dense gradient at the backward of mul, so here I am asking for a special kind of mul ops (called sparse_mul) that returns a sparse grad tensor and only keep the nnz's gradients.", "body": "We are working on to increase supports for sparse tensor. Currently we have [summarized current state of sparse tensor](https://github.com/pytorch/pytorch/issues/9674) and listed out [sparse ops to support](https://github.com/pytorch/pytorch/issues/8853). We would like to collect sparse tensor use cases to facilitate the design decisions and prioritize TODO list according. It will be very helpful if you can post your use cases and desired sparse ops here or at the [PyTorch Forum](https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047). Thanks! \r\n\r\nI find these questions useful when writing use cases:\r\n- Where do I need sparse tensor? During training deep learning model?\r\n- Do I need autograd support for the sparse ops?\r\n\r\nA possible example will be:\r\n\r\nI am training model that has `mul(Sparse, Dense)` ops. I would like to have its forward and backward. I know there will be a dense gradient at the backward of `mul`, so here I am asking for a special kind of `mul` ops (called `sparse_mul`) that returns a sparse grad tensor and only keep the nnz's gradients.\r\n"}