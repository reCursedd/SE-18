{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427128585", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-427128585", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 427128585, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzEyODU4NQ==", "user": {"login": "gasse", "id": 1726818, "node_id": "MDQ6VXNlcjE3MjY4MTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1726818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gasse", "html_url": "https://github.com/gasse", "followers_url": "https://api.github.com/users/gasse/followers", "following_url": "https://api.github.com/users/gasse/following{/other_user}", "gists_url": "https://api.github.com/users/gasse/gists{/gist_id}", "starred_url": "https://api.github.com/users/gasse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gasse/subscriptions", "organizations_url": "https://api.github.com/users/gasse/orgs", "repos_url": "https://api.github.com/users/gasse/repos", "events_url": "https://api.github.com/users/gasse/events{/privacy}", "received_events_url": "https://api.github.com/users/gasse/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-04T18:46:50Z", "updated_at": "2018-10-04T18:46:50Z", "author_association": "NONE", "body_html": "<p>I'm also using <code>SparseTensor</code>'s operations to train Graph Convolutional Networks on big sparse graphs, same as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11088808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AntoinePrv\">@AntoinePrv</a> . I really would love having a <code>max()</code> operation on one (or some) of the sparse dimensions.</p>\n<p>In order to properly define sparse matrix multiplication with sparse gradients, one may introduce an implicit mask, e.g.: <code>(A*M) @ B</code> instead of <code>A @ B</code>, where <code>A</code> is a sparse matrix and <code>M</code> a binary matrix with same sparsity pattern as <code>A</code>. Then the gradients of the whole operation wrt. <code>A</code> are also sparse with the same pattern. The resulting operation could be named <code>masked_mm</code> or something, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11088808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AntoinePrv\">@AntoinePrv</a> suggests.</p>\n<p>Same goes with a masked softmax: <code>(A.exp()*M) / (A.exp()*M).sum(dim=1)</code> or something similar. I wasn't able to implement it efficiently though, as I am missing a <code>max(dim)</code> for the softmax stability trick !</p>", "body_text": "I'm also using SparseTensor's operations to train Graph Convolutional Networks on big sparse graphs, same as @AntoinePrv . I really would love having a max() operation on one (or some) of the sparse dimensions.\nIn order to properly define sparse matrix multiplication with sparse gradients, one may introduce an implicit mask, e.g.: (A*M) @ B instead of A @ B, where A is a sparse matrix and M a binary matrix with same sparsity pattern as A. Then the gradients of the whole operation wrt. A are also sparse with the same pattern. The resulting operation could be named masked_mm or something, as @AntoinePrv suggests.\nSame goes with a masked softmax: (A.exp()*M) / (A.exp()*M).sum(dim=1) or something similar. I wasn't able to implement it efficiently though, as I am missing a max(dim) for the softmax stability trick !", "body": "I'm also using `SparseTensor`'s operations to train Graph Convolutional Networks on big sparse graphs, same as @AntoinePrv . I really would love having a `max()` operation on one (or some) of the sparse dimensions.\r\n\r\nIn order to properly define sparse matrix multiplication with sparse gradients, one may introduce an implicit mask, e.g.: `(A*M) @ B` instead of `A @ B`, where `A` is a sparse matrix and `M` a binary matrix with same sparsity pattern as `A`. Then the gradients of the whole operation wrt. `A` are also sparse with the same pattern. The resulting operation could be named `masked_mm` or something, as @AntoinePrv suggests.\r\n\r\nSame goes with a masked softmax: `(A.exp()*M) / (A.exp()*M).sum(dim=1)` or something similar. I wasn't able to implement it efficiently though, as I am missing a `max(dim)` for the softmax stability trick !"}