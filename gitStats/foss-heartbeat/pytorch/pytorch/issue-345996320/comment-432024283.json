{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/432024283", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-432024283", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 432024283, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjAyNDI4Mw==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-22T23:14:58Z", "updated_at": "2018-10-22T23:14:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11088808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AntoinePrv\">@AntoinePrv</a> Yes, indeed <code>cat</code>, <code>stack</code> and <code>chunk</code>, <code>split</code> will be very useful to work with batched sparse tensors. We have <code>cat</code> in the TODO <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335497470\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8853\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8853/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8853\">#8853</a>, and will add the others later. To make batched sparse tensor work, we need to represent it in a convenient way, one possibility is to cat a list of sparse tensors of the same shape and add an extra dim at indices as batch_dim, here is an example:</p>\n<pre><code>dims = [3, 4]\nmax_nnz = reduce(lambda x, y: x * y, dims)\n\nlist_of_sparse = []\nfor _ in range(10):\n    nnz = torch.randint(2, max_nnz, (1,)).data\n    I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n    V = torch.randn(nnz)\n    S = torch.sparse_coo_tensor(I, V, dims)\n    list_of_sparse.append(S.coalesce())\n\ndef create_sparse_batch(list_of_sparse):\n    batch_size = len(list_of_sparse)\n    I_list = []\n    V_list = []\n    S_size = list_of_sparse[0].size()\n    new_size = [batch_size] + list(S_size)\n    for i in range(batch_size):\n        I = list_of_sparse[i]._indices()\n        I_size = list(I.size())\n        I_size[0] = 1\n        batch_idx = torch.empty(I_size, dtype=torch.long).fill_(i)\n        I_list.append(torch.cat([batch_idx, I]))\n        V_list.append(list_of_sparse[i]._values())\n    new_I = torch.cat(I_list, 1)\n    new_V = torch.cat(V_list, 0)\n    return torch.sparse_coo_tensor(new_I, new_V, new_size)\n\nsparse_batch = create_sparse_batch(list_of_sparse)\n</code></pre>\n<p>As for <code>Dataset</code> / <code>Dataloader</code>, we will need to work out the details once batch sparse tensor format is finalized.</p>", "body_text": "@AntoinePrv Yes, indeed cat, stack and chunk, split will be very useful to work with batched sparse tensors. We have cat in the TODO #8853, and will add the others later. To make batched sparse tensor work, we need to represent it in a convenient way, one possibility is to cat a list of sparse tensors of the same shape and add an extra dim at indices as batch_dim, here is an example:\ndims = [3, 4]\nmax_nnz = reduce(lambda x, y: x * y, dims)\n\nlist_of_sparse = []\nfor _ in range(10):\n    nnz = torch.randint(2, max_nnz, (1,)).data\n    I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n    V = torch.randn(nnz)\n    S = torch.sparse_coo_tensor(I, V, dims)\n    list_of_sparse.append(S.coalesce())\n\ndef create_sparse_batch(list_of_sparse):\n    batch_size = len(list_of_sparse)\n    I_list = []\n    V_list = []\n    S_size = list_of_sparse[0].size()\n    new_size = [batch_size] + list(S_size)\n    for i in range(batch_size):\n        I = list_of_sparse[i]._indices()\n        I_size = list(I.size())\n        I_size[0] = 1\n        batch_idx = torch.empty(I_size, dtype=torch.long).fill_(i)\n        I_list.append(torch.cat([batch_idx, I]))\n        V_list.append(list_of_sparse[i]._values())\n    new_I = torch.cat(I_list, 1)\n    new_V = torch.cat(V_list, 0)\n    return torch.sparse_coo_tensor(new_I, new_V, new_size)\n\nsparse_batch = create_sparse_batch(list_of_sparse)\n\nAs for Dataset / Dataloader, we will need to work out the details once batch sparse tensor format is finalized.", "body": "@AntoinePrv Yes, indeed `cat`, `stack` and `chunk`, `split` will be very useful to work with batched sparse tensors. We have `cat` in the TODO #8853, and will add the others later. To make batched sparse tensor work, we need to represent it in a convenient way, one possibility is to cat a list of sparse tensors of the same shape and add an extra dim at indices as batch_dim, here is an example:\r\n```\r\ndims = [3, 4]\r\nmax_nnz = reduce(lambda x, y: x * y, dims)\r\n\r\nlist_of_sparse = []\r\nfor _ in range(10):\r\n    nnz = torch.randint(2, max_nnz, (1,)).data\r\n    I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \r\n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\r\n    V = torch.randn(nnz)\r\n    S = torch.sparse_coo_tensor(I, V, dims)\r\n    list_of_sparse.append(S.coalesce())\r\n\r\ndef create_sparse_batch(list_of_sparse):\r\n    batch_size = len(list_of_sparse)\r\n    I_list = []\r\n    V_list = []\r\n    S_size = list_of_sparse[0].size()\r\n    new_size = [batch_size] + list(S_size)\r\n    for i in range(batch_size):\r\n        I = list_of_sparse[i]._indices()\r\n        I_size = list(I.size())\r\n        I_size[0] = 1\r\n        batch_idx = torch.empty(I_size, dtype=torch.long).fill_(i)\r\n        I_list.append(torch.cat([batch_idx, I]))\r\n        V_list.append(list_of_sparse[i]._values())\r\n    new_I = torch.cat(I_list, 1)\r\n    new_V = torch.cat(V_list, 0)\r\n    return torch.sparse_coo_tensor(new_I, new_V, new_size)\r\n\r\nsparse_batch = create_sparse_batch(list_of_sparse)\r\n```\r\n\r\nAs for `Dataset` / `Dataloader`, we will need to work out the details once batch sparse tensor format is finalized."}