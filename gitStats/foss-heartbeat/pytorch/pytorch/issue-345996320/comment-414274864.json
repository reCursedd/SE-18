{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/414274864", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-414274864", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 414274864, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDI3NDg2NA==", "user": {"login": "snakers4", "id": 12515440, "node_id": "MDQ6VXNlcjEyNTE1NDQw", "avatar_url": "https://avatars0.githubusercontent.com/u/12515440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snakers4", "html_url": "https://github.com/snakers4", "followers_url": "https://api.github.com/users/snakers4/followers", "following_url": "https://api.github.com/users/snakers4/following{/other_user}", "gists_url": "https://api.github.com/users/snakers4/gists{/gist_id}", "starred_url": "https://api.github.com/users/snakers4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snakers4/subscriptions", "organizations_url": "https://api.github.com/users/snakers4/orgs", "repos_url": "https://api.github.com/users/snakers4/repos", "events_url": "https://api.github.com/users/snakers4/events{/privacy}", "received_events_url": "https://api.github.com/users/snakers4/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-20T10:46:36Z", "updated_at": "2018-08-20T10:46:36Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Where do I need sparse tensor? During training deep learning model?</p>\n</blockquote>\n<p><strong>Use case 1</strong></p>\n<p>I am interested in large-matrix text embeddings techniques.<br>\nCurrently I have been using UMAP (PCA =&gt; UMAP =&gt; HDBSCAN), but I start facing bottle-necks starting from 3m+ data points.</p>\n<p>Combining KNN graph using GPU calculations with some sort of LargeVis implementation seems intriguing. Wrote some details <a href=\"https://github.com/lmcinnes/umap/issues/125#issuecomment-414107150\" data-hovercard-type=\"issue\" data-hovercard-url=\"/lmcinnes/umap/issues/125/hovercard\">here</a>, but the papers are:</p>\n<ul>\n<li><a href=\"http://arxiv.org/abs/1602.00370%0A\" rel=\"nofollow\">LargeVis</a></li>\n<li>Brute <a href=\"https://www.researchgate.net/publication/318019998_Multi-GPU_k-Nearest_Neighbor_Search_in_the_Context_of_Data_Embedding\" rel=\"nofollow\">forcing</a> KNN graphs on the GPU</li>\n</ul>\n<p><strong>Use case 2</strong></p>\n<p>Word2vec-like models. I have not seen any real-sized w2v implemented on PyTorch.<br>\nOfc, you can use supervised methods (CNN with an embedding layer), but you may not have annotation for your domain.</p>\n<blockquote>\n<p>Do I need autograd support for the sparse ops?</p>\n</blockquote>\n<p>Yes</p>", "body_text": "Where do I need sparse tensor? During training deep learning model?\n\nUse case 1\nI am interested in large-matrix text embeddings techniques.\nCurrently I have been using UMAP (PCA => UMAP => HDBSCAN), but I start facing bottle-necks starting from 3m+ data points.\nCombining KNN graph using GPU calculations with some sort of LargeVis implementation seems intriguing. Wrote some details here, but the papers are:\n\nLargeVis\nBrute forcing KNN graphs on the GPU\n\nUse case 2\nWord2vec-like models. I have not seen any real-sized w2v implemented on PyTorch.\nOfc, you can use supervised methods (CNN with an embedding layer), but you may not have annotation for your domain.\n\nDo I need autograd support for the sparse ops?\n\nYes", "body": "> Where do I need sparse tensor? During training deep learning model?\r\n\r\n**Use case 1**\r\n\r\nI am interested in large-matrix text embeddings techniques.\r\nCurrently I have been using UMAP (PCA => UMAP => HDBSCAN), but I start facing bottle-necks starting from 3m+ data points.\r\n\r\nCombining KNN graph using GPU calculations with some sort of LargeVis implementation seems intriguing. Wrote some details [here](https://github.com/lmcinnes/umap/issues/125#issuecomment-414107150), but the papers are:\r\n- [LargeVis](http://arxiv.org/abs/1602.00370%0A)\r\n- Brute [forcing](https://www.researchgate.net/publication/318019998_Multi-GPU_k-Nearest_Neighbor_Search_in_the_Context_of_Data_Embedding) KNN graphs on the GPU\r\n\r\n**Use case 2**\r\n\r\nWord2vec-like models. I have not seen any real-sized w2v implemented on PyTorch.\r\nOfc, you can use supervised methods (CNN with an embedding layer), but you may not have annotation for your domain.\r\n\r\n> Do I need autograd support for the sparse ops?\r\n\r\nYes"}