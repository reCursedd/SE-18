{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/439510569", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-439510569", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 439510569, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTUxMDU2OQ==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-16T20:02:21Z", "updated_at": "2018-11-16T20:02:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6945922\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rusty1s\">@rusty1s</a>, thanks a lot for your comments! You have a great summary on what we can do to improve sparse support for users :) For your applications at Graph Neural Networks, can I get some pointers as well? I am excited to see how real world applications can benefit from sparse. Also thanks a lot for working on the extension libraries for sparse!</p>\n<blockquote>\n<p>sum/min/max/mean/abs/norm/softmax/clamp: I guess this also requires adding scatter_max/scatter_min/scatter_mean functionalities to PyTorch</p>\n</blockquote>\n<p>Yes, we definitely need reductions. After I land the PR for sum, I will prioritize on other reduction ops based on the application needs. Please point me to the places of your model that need reductions.</p>\n<blockquote>\n<p>improved coalesce, so that the user can choose how values for equal indices are aggregated (sum/max/min/mean)</p>\n</blockquote>\n<p>We can do this as well, but it largely depends on use cases. Currently we use coalesce for elementwise ops / prevent sparse tensor getting too large after <code>cat</code></p>\n<blockquote>\n<p>transpose (only in the first two dimensions?)</p>\n</blockquote>\n<p>This one is supported already for any two dims, we really need to improve the docs</p>\n<blockquote>\n<p>universal matmul: I guess sparse-sparse matrix multiplication is definitely needed. However, the most common operation in this scenario is adjacency normalization (e.g., D^{-1} @ A), where D is a diagonal matrix. This case should be handled on its own to reduce computation, as it can be reduced to a single element-wise multiplication.</p>\n</blockquote>\n<p>Yes, this one is on my list, not sure if I can finish universal matmul in a single PR, but can definitely start with the simple cases that will be used heavily</p>\n<blockquote>\n<p>Indexing/Slicing: It would be really awesome to do something like A[:, 0] = ....</p>\n</blockquote>\n<p>This one can be tricky since we don't have support for view at sparse yet</p>\n<blockquote>\n<p>Autograd support for _values()</p>\n</blockquote>\n<p>This one is done by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a>! <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"373164155\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13001\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13001/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13001\">#13001</a>, and I think the docs is coming soon</p>\n<blockquote>\n<p>Sparse Batch support. This is a tricky one. I can see it implemented as a sparse tensor with a first dense dimension, but this kind of contradicts my use cases (as it requires equal sparse dimensions for all examples in a batch). In addition, it would require all operations to work on either batched sparse matrices or single sparse matrices.</p>\n</blockquote>\n<p>I thought a batch sparse can be simply created by <code>stack</code>: <code>BS = torch.stack([S, S, S, ..., S], 0)</code>, and yes the 1st sparse_dim will be \"dense\" (yet it is a sparse_dim), I don't see an issue there yet. Would you like to give an example?</p>\n<p>Will a batch sparse tensor requires all downstream ops to support batch? I don't have a answer for this yet. But ops like <code>bmm</code> will output a dense tensor and so it should be fine.</p>", "body_text": "Hi @rusty1s, thanks a lot for your comments! You have a great summary on what we can do to improve sparse support for users :) For your applications at Graph Neural Networks, can I get some pointers as well? I am excited to see how real world applications can benefit from sparse. Also thanks a lot for working on the extension libraries for sparse!\n\nsum/min/max/mean/abs/norm/softmax/clamp: I guess this also requires adding scatter_max/scatter_min/scatter_mean functionalities to PyTorch\n\nYes, we definitely need reductions. After I land the PR for sum, I will prioritize on other reduction ops based on the application needs. Please point me to the places of your model that need reductions.\n\nimproved coalesce, so that the user can choose how values for equal indices are aggregated (sum/max/min/mean)\n\nWe can do this as well, but it largely depends on use cases. Currently we use coalesce for elementwise ops / prevent sparse tensor getting too large after cat\n\ntranspose (only in the first two dimensions?)\n\nThis one is supported already for any two dims, we really need to improve the docs\n\nuniversal matmul: I guess sparse-sparse matrix multiplication is definitely needed. However, the most common operation in this scenario is adjacency normalization (e.g., D^{-1} @ A), where D is a diagonal matrix. This case should be handled on its own to reduce computation, as it can be reduced to a single element-wise multiplication.\n\nYes, this one is on my list, not sure if I can finish universal matmul in a single PR, but can definitely start with the simple cases that will be used heavily\n\nIndexing/Slicing: It would be really awesome to do something like A[:, 0] = ....\n\nThis one can be tricky since we don't have support for view at sparse yet\n\nAutograd support for _values()\n\nThis one is done by @SsnL! #13001, and I think the docs is coming soon\n\nSparse Batch support. This is a tricky one. I can see it implemented as a sparse tensor with a first dense dimension, but this kind of contradicts my use cases (as it requires equal sparse dimensions for all examples in a batch). In addition, it would require all operations to work on either batched sparse matrices or single sparse matrices.\n\nI thought a batch sparse can be simply created by stack: BS = torch.stack([S, S, S, ..., S], 0), and yes the 1st sparse_dim will be \"dense\" (yet it is a sparse_dim), I don't see an issue there yet. Would you like to give an example?\nWill a batch sparse tensor requires all downstream ops to support batch? I don't have a answer for this yet. But ops like bmm will output a dense tensor and so it should be fine.", "body": "Hi @rusty1s, thanks a lot for your comments! You have a great summary on what we can do to improve sparse support for users :) For your applications at Graph Neural Networks, can I get some pointers as well? I am excited to see how real world applications can benefit from sparse. Also thanks a lot for working on the extension libraries for sparse!\r\n\r\n> sum/min/max/mean/abs/norm/softmax/clamp: I guess this also requires adding scatter_max/scatter_min/scatter_mean functionalities to PyTorch\r\n\r\nYes, we definitely need reductions. After I land the PR for sum, I will prioritize on other reduction ops based on the application needs. Please point me to the places of your model that need reductions.\r\n\r\n> improved coalesce, so that the user can choose how values for equal indices are aggregated (sum/max/min/mean)\r\n\r\nWe can do this as well, but it largely depends on use cases. Currently we use coalesce for elementwise ops / prevent sparse tensor getting too large after `cat`\r\n\r\n> transpose (only in the first two dimensions?)\r\n\r\nThis one is supported already for any two dims, we really need to improve the docs\r\n\r\n> universal matmul: I guess sparse-sparse matrix multiplication is definitely needed. However, the most common operation in this scenario is adjacency normalization (e.g., D^{-1} @ A), where D is a diagonal matrix. This case should be handled on its own to reduce computation, as it can be reduced to a single element-wise multiplication.\r\n\r\nYes, this one is on my list, not sure if I can finish universal matmul in a single PR, but can definitely start with the simple cases that will be used heavily\r\n\r\n> Indexing/Slicing: It would be really awesome to do something like A[:, 0] = ....\r\n\r\nThis one can be tricky since we don't have support for view at sparse yet\r\n\r\n> Autograd support for _values()\r\n\r\nThis one is done by @SsnL! https://github.com/pytorch/pytorch/pull/13001, and I think the docs is coming soon\r\n\r\n> Sparse Batch support. This is a tricky one. I can see it implemented as a sparse tensor with a first dense dimension, but this kind of contradicts my use cases (as it requires equal sparse dimensions for all examples in a batch). In addition, it would require all operations to work on either batched sparse matrices or single sparse matrices.\r\n\r\nI thought a batch sparse can be simply created by `stack`: `BS = torch.stack([S, S, S, ..., S], 0)`, and yes the 1st sparse_dim will be \"dense\" (yet it is a sparse_dim), I don't see an issue there yet. Would you like to give an example? \r\n\r\nWill a batch sparse tensor requires all downstream ops to support batch? I don't have a answer for this yet. But ops like `bmm` will output a dense tensor and so it should be fine."}