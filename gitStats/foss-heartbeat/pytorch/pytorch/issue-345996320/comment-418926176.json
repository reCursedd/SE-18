{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/418926176", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-418926176", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 418926176, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODkyNjE3Ng==", "user": {"login": "benvcutilli", "id": 23709161, "node_id": "MDQ6VXNlcjIzNzA5MTYx", "avatar_url": "https://avatars1.githubusercontent.com/u/23709161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benvcutilli", "html_url": "https://github.com/benvcutilli", "followers_url": "https://api.github.com/users/benvcutilli/followers", "following_url": "https://api.github.com/users/benvcutilli/following{/other_user}", "gists_url": "https://api.github.com/users/benvcutilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/benvcutilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benvcutilli/subscriptions", "organizations_url": "https://api.github.com/users/benvcutilli/orgs", "repos_url": "https://api.github.com/users/benvcutilli/repos", "events_url": "https://api.github.com/users/benvcutilli/events{/privacy}", "received_events_url": "https://api.github.com/users/benvcutilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-06T00:43:20Z", "updated_at": "2018-09-06T01:00:30Z", "author_association": "NONE", "body_html": "<p>I agree with having autograd for sparse; as I said (in some other issue), there are layers that one may want to implement where sparse tensors are the only tractable option.</p>\n<p>I would just say that <em>every</em> tensor function needs (dense, dense) -&gt; dense, (dense, sparse) -&gt; dense/sparse, (sparse, dense) -&gt; dense/sparse, and (sparse, sparse) -&gt; sparse. I know these are a lot of combinations. Maybe it would be a good idea to have an abstract class implement a general value access mechanism on the C/C++ side of things so that you don't have to write 6 different versions of the same function? You probably already do this, but just in case. Just based off my knowledge from my Scientific Computing class, at the very least, you probably need a separate (dense, dense) -&gt; dense function, but I would guess in the sparse case a lot of code is replicated if you were to use an abstract class, so you could fold that into one function with minor differences.</p>\n<p>For the latter sparse case replication, I'm sure that for a lot of functions, you just need to know which tensor is sparse, take its COO indices, and operate just off that set of indices. That way you don't need to write flipped code if, say, matrix multiply has its dense and sparse matrices reversed in the argument passing. If you have two sparse matrices, maybe it would be better to take the set intersection of indices (if one is dense, then the set intersection is just the sparse tensor's indices).</p>", "body_text": "I agree with having autograd for sparse; as I said (in some other issue), there are layers that one may want to implement where sparse tensors are the only tractable option.\nI would just say that every tensor function needs (dense, dense) -> dense, (dense, sparse) -> dense/sparse, (sparse, dense) -> dense/sparse, and (sparse, sparse) -> sparse. I know these are a lot of combinations. Maybe it would be a good idea to have an abstract class implement a general value access mechanism on the C/C++ side of things so that you don't have to write 6 different versions of the same function? You probably already do this, but just in case. Just based off my knowledge from my Scientific Computing class, at the very least, you probably need a separate (dense, dense) -> dense function, but I would guess in the sparse case a lot of code is replicated if you were to use an abstract class, so you could fold that into one function with minor differences.\nFor the latter sparse case replication, I'm sure that for a lot of functions, you just need to know which tensor is sparse, take its COO indices, and operate just off that set of indices. That way you don't need to write flipped code if, say, matrix multiply has its dense and sparse matrices reversed in the argument passing. If you have two sparse matrices, maybe it would be better to take the set intersection of indices (if one is dense, then the set intersection is just the sparse tensor's indices).", "body": "I agree with having autograd for sparse; as I said (in some other issue), there are layers that one may want to implement where sparse tensors are the only tractable option.\r\n\r\nI would just say that _every_ tensor function needs (dense, dense) -> dense, (dense, sparse) -> dense/sparse, (sparse, dense) -> dense/sparse, and (sparse, sparse) -> sparse. I know these are a lot of combinations. Maybe it would be a good idea to have an abstract class implement a general value access mechanism on the C/C++ side of things so that you don't have to write 6 different versions of the same function? You probably already do this, but just in case. Just based off my knowledge from my Scientific Computing class, at the very least, you probably need a separate (dense, dense) -> dense function, but I would guess in the sparse case a lot of code is replicated if you were to use an abstract class, so you could fold that into one function with minor differences.\r\n\r\nFor the latter sparse case replication, I'm sure that for a lot of functions, you just need to know which tensor is sparse, take its COO indices, and operate just off that set of indices. That way you don't need to write flipped code if, say, matrix multiply has its dense and sparse matrices reversed in the argument passing. If you have two sparse matrices, maybe it would be better to take the set intersection of indices (if one is dense, then the set intersection is just the sparse tensor's indices)."}