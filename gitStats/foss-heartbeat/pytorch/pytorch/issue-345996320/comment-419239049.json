{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419239049", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-419239049", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 419239049, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTIzOTA0OQ==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-06T20:58:05Z", "updated_at": "2018-09-06T21:21:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23709161\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benvcutilli\">@benvcutilli</a>, thanks for the comments.</p>\n<blockquote>\n<p>I would just say that every tensor function needs (dense, dense) -&gt; dense, (dense, sparse) -&gt; dense/sparse, (sparse, dense) -&gt; dense/sparse, and (sparse, sparse) -&gt; sparse. I know these are a lot of combinations. Maybe it would be a good idea to have an abstract class implement a general value access mechanism on the C/C++ side of things so that you don't have to write 6 different versions of the same function? You probably already do this, but just in case. Just based off my knowledge from my Scientific Computing class, at the very least, you probably need a separate (dense, dense) -&gt; dense function, but I would guess in the sparse case a lot of code is replicated if you were to use an abstract class, so you could fold that into one function with minor differences.</p>\n</blockquote>\n<p>Currently we have all (dense, dense) -&gt; dense implemented by default, also (dense, sparse) -&gt; dense/sparse and (sparse, dense) -&gt; dense/sparse are counterparts to each other. So for sparse ops, most likely we need (sparse, dense) -&gt; dense/sparse and (sparse, sparse) -&gt; dense/sparse depending on user requests. Notice that for autograd, for those ops with dense local gradients, we also need special <code>sparse_ops</code> to maintain the sparse gradients by zeroing out non-nnz gradients. In terms of code replication, there are some but not so much (mostly on set* nnz, _indices, _values, is_coalesced). The reason is for the same op with different combination of input tensors, it requires different kernels to optimize for efficiency. For instance, (sparse, dense) can have CPU kernel parallelize over sparse elements, where (sparse, sparse) may requires a two-pointer search for indices matching.</p>\n<blockquote>\n<p>For the latter sparse case replication, I'm sure that for a lot of functions, you just need to know which tensor is sparse, take its COO indices, and operate just off that set of indices. That way you don't need to write flipped code if, say, matrix multiply has its dense and sparse matrices reversed in the argument passing. If you have two sparse matrices, maybe it would be better to take the set intersection of indices (if one is dense, then the set intersection is just the sparse tensor's indices).</p>\n</blockquote>\n<p>Right, and this is also how the current kernels look like in sparse ops.</p>", "body_text": "Hi @benvcutilli, thanks for the comments.\n\nI would just say that every tensor function needs (dense, dense) -> dense, (dense, sparse) -> dense/sparse, (sparse, dense) -> dense/sparse, and (sparse, sparse) -> sparse. I know these are a lot of combinations. Maybe it would be a good idea to have an abstract class implement a general value access mechanism on the C/C++ side of things so that you don't have to write 6 different versions of the same function? You probably already do this, but just in case. Just based off my knowledge from my Scientific Computing class, at the very least, you probably need a separate (dense, dense) -> dense function, but I would guess in the sparse case a lot of code is replicated if you were to use an abstract class, so you could fold that into one function with minor differences.\n\nCurrently we have all (dense, dense) -> dense implemented by default, also (dense, sparse) -> dense/sparse and (sparse, dense) -> dense/sparse are counterparts to each other. So for sparse ops, most likely we need (sparse, dense) -> dense/sparse and (sparse, sparse) -> dense/sparse depending on user requests. Notice that for autograd, for those ops with dense local gradients, we also need special sparse_ops to maintain the sparse gradients by zeroing out non-nnz gradients. In terms of code replication, there are some but not so much (mostly on set* nnz, _indices, _values, is_coalesced). The reason is for the same op with different combination of input tensors, it requires different kernels to optimize for efficiency. For instance, (sparse, dense) can have CPU kernel parallelize over sparse elements, where (sparse, sparse) may requires a two-pointer search for indices matching.\n\nFor the latter sparse case replication, I'm sure that for a lot of functions, you just need to know which tensor is sparse, take its COO indices, and operate just off that set of indices. That way you don't need to write flipped code if, say, matrix multiply has its dense and sparse matrices reversed in the argument passing. If you have two sparse matrices, maybe it would be better to take the set intersection of indices (if one is dense, then the set intersection is just the sparse tensor's indices).\n\nRight, and this is also how the current kernels look like in sparse ops.", "body": "Hi @benvcutilli, thanks for the comments.\r\n\r\n> I would just say that every tensor function needs (dense, dense) -> dense, (dense, sparse) -> dense/sparse, (sparse, dense) -> dense/sparse, and (sparse, sparse) -> sparse. I know these are a lot of combinations. Maybe it would be a good idea to have an abstract class implement a general value access mechanism on the C/C++ side of things so that you don't have to write 6 different versions of the same function? You probably already do this, but just in case. Just based off my knowledge from my Scientific Computing class, at the very least, you probably need a separate (dense, dense) -> dense function, but I would guess in the sparse case a lot of code is replicated if you were to use an abstract class, so you could fold that into one function with minor differences.\r\n\r\nCurrently we have all (dense, dense) -> dense implemented by default, also (dense, sparse) -> dense/sparse and (sparse, dense) -> dense/sparse are counterparts to each other. So for sparse ops, most likely we need (sparse, dense) -> dense/sparse and (sparse, sparse) -> dense/sparse depending on user requests. Notice that for autograd, for those ops with dense local gradients, we also need special `sparse_ops` to maintain the sparse gradients by zeroing out non-nnz gradients. In terms of code replication, there are some but not so much (mostly on set* nnz, _indices, _values, is_coalesced). The reason is for the same op with different combination of input tensors, it requires different kernels to optimize for efficiency. For instance, (sparse, dense) can have CPU kernel parallelize over sparse elements, where (sparse, sparse) may requires a two-pointer search for indices matching.\r\n\r\n> For the latter sparse case replication, I'm sure that for a lot of functions, you just need to know which tensor is sparse, take its COO indices, and operate just off that set of indices. That way you don't need to write flipped code if, say, matrix multiply has its dense and sparse matrices reversed in the argument passing. If you have two sparse matrices, maybe it would be better to take the set intersection of indices (if one is dense, then the set intersection is just the sparse tensor's indices).\r\n\r\nRight, and this is also how the current kernels look like in sparse ops."}