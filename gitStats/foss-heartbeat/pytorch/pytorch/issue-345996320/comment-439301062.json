{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/439301062", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-439301062", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 439301062, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTMwMTA2Mg==", "user": {"login": "rusty1s", "id": 6945922, "node_id": "MDQ6VXNlcjY5NDU5MjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/6945922?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rusty1s", "html_url": "https://github.com/rusty1s", "followers_url": "https://api.github.com/users/rusty1s/followers", "following_url": "https://api.github.com/users/rusty1s/following{/other_user}", "gists_url": "https://api.github.com/users/rusty1s/gists{/gist_id}", "starred_url": "https://api.github.com/users/rusty1s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rusty1s/subscriptions", "organizations_url": "https://api.github.com/users/rusty1s/orgs", "repos_url": "https://api.github.com/users/rusty1s/repos", "events_url": "https://api.github.com/users/rusty1s/events{/privacy}", "received_events_url": "https://api.github.com/users/rusty1s/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-16T07:01:30Z", "updated_at": "2018-11-16T07:02:08Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nReally looking forward to you making PyTorch a first class representative for differentiable sparse matrix support. I myself am mostly interested in sparse tensor support for Graph Neural Networks. A lot of needed functionality has been already mentioned:</p>\n<ul>\n<li><code>sum</code>/<code>min</code>/<code>max</code>/<code>mean</code>/<code>abs</code>/<code>norm</code>/<code>softmax</code>/<code>clamp</code>: I guess this also requires adding <code>scatter_max</code>/<code>scatter_min</code>/<code>scatter_mean</code> functionalities to PyTorch</li>\n<li>improved <code>coalesce</code>, so that the user can choose how values for equal indices are aggregated (<code>sum</code>/<code>max</code>/<code>min</code>/<code>mean</code>)</li>\n<li><code>transpose</code> (only in the first two dimensions?)</li>\n<li>universal <code>matmul</code>: I guess sparse-sparse matrix multiplication is definitely needed. However, the most common operation in this scenario is adjacency normalization (e.g., <code>D^{-1} @ A</code>), where <code>D</code> is a diagonal matrix. This case should be handled on its own to reduce computation, as it can be reduced to a single element-wise multiplication.</li>\n<li>Indexing/Slicing: It would be really awesome to do something like <code>A[:, 0] = ...</code>.</li>\n<li>Autograd support for <code>_values()</code></li>\n<li>Sparse Batch support. This is a tricky one. I can see it implemented as a sparse tensor with a first dense dimension, but this kind of contradicts my use cases (as it requires equal sparse dimensions for all examples in a batch). In addition, it would require all operations to work on either batched sparse matrices or single sparse matrices.</li>\n</ul>", "body_text": "Hi,\nReally looking forward to you making PyTorch a first class representative for differentiable sparse matrix support. I myself am mostly interested in sparse tensor support for Graph Neural Networks. A lot of needed functionality has been already mentioned:\n\nsum/min/max/mean/abs/norm/softmax/clamp: I guess this also requires adding scatter_max/scatter_min/scatter_mean functionalities to PyTorch\nimproved coalesce, so that the user can choose how values for equal indices are aggregated (sum/max/min/mean)\ntranspose (only in the first two dimensions?)\nuniversal matmul: I guess sparse-sparse matrix multiplication is definitely needed. However, the most common operation in this scenario is adjacency normalization (e.g., D^{-1} @ A), where D is a diagonal matrix. This case should be handled on its own to reduce computation, as it can be reduced to a single element-wise multiplication.\nIndexing/Slicing: It would be really awesome to do something like A[:, 0] = ....\nAutograd support for _values()\nSparse Batch support. This is a tricky one. I can see it implemented as a sparse tensor with a first dense dimension, but this kind of contradicts my use cases (as it requires equal sparse dimensions for all examples in a batch). In addition, it would require all operations to work on either batched sparse matrices or single sparse matrices.", "body": "Hi,\r\nReally looking forward to you making PyTorch a first class representative for differentiable sparse matrix support. I myself am mostly interested in sparse tensor support for Graph Neural Networks. A lot of needed functionality has been already mentioned:\r\n* `sum`/`min`/`max`/`mean`/`abs`/`norm`/`softmax`/`clamp`: I guess this also requires adding `scatter_max`/`scatter_min`/`scatter_mean` functionalities to PyTorch\r\n* improved `coalesce`, so that the user can choose how values for equal indices are aggregated (`sum`/`max`/`min`/`mean`)\r\n* `transpose` (only in the first two dimensions?)\r\n* universal `matmul`: I guess sparse-sparse matrix multiplication is definitely needed. However, the most common operation in this scenario is adjacency normalization (e.g., `D^{-1} @ A`), where `D` is a diagonal matrix. This case should be handled on its own to reduce computation, as it can be reduced to a single element-wise multiplication.\r\n* Indexing/Slicing: It would be really awesome to do something like `A[:, 0] = ...`.\r\n* Autograd support for `_values()`\r\n* Sparse Batch support. This is a tricky one. I can see it implemented as a sparse tensor with a first dense dimension, but this kind of contradicts my use cases (as it requires equal sparse dimensions for all examples in a batch). In addition, it would require all operations to work on either batched sparse matrices or single sparse matrices."}