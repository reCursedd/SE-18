{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/421129275", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-421129275", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 421129275, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTEyOTI3NQ==", "user": {"login": "AntoinePrv", "id": 11088808, "node_id": "MDQ6VXNlcjExMDg4ODA4", "avatar_url": "https://avatars0.githubusercontent.com/u/11088808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AntoinePrv", "html_url": "https://github.com/AntoinePrv", "followers_url": "https://api.github.com/users/AntoinePrv/followers", "following_url": "https://api.github.com/users/AntoinePrv/following{/other_user}", "gists_url": "https://api.github.com/users/AntoinePrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/AntoinePrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AntoinePrv/subscriptions", "organizations_url": "https://api.github.com/users/AntoinePrv/orgs", "repos_url": "https://api.github.com/users/AntoinePrv/repos", "events_url": "https://api.github.com/users/AntoinePrv/events{/privacy}", "received_events_url": "https://api.github.com/users/AntoinePrv/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-13T19:44:32Z", "updated_at": "2018-10-13T14:17:09Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I use sparse tensors to build Graph neural network on big sparse input graphs. The common approach of masking non existing edges in the adjacency matrix just doesn't scale for my inputs.</p>\n<p>I don't have sparse parameters (although it could be useful).</p>\n<ul>\n<li>First, a sparse matrix represent the adjacency matrix as the input of the network.</li>\n<li>Later on, information is propagated from one node to another according to the topology of the graph in a way similar to attention. A sparse matrix (eventually hybrid) holds attention weights outputted from another (dense) network taking as input the features of two nodes. Sparse matrix multiplication is used to sum over neighboring nodes.</li>\n</ul>\n<p>In my case I differentiate (via my own functions). The kind of usage is a bit different than expected: the gradient wrt to some of my sparse tensors would usually not be sparse, but <strong>still keep the same sparsity pattern</strong>.  This is because in the case of graphs, the sparsity pattern represent <strong>allowed computation</strong>. The rest should is theoretically non existant, so it the gradient wrt to the zero values in the tensor doesn't make sense.</p>\n<p>In my case it should be called a <code>SparseMaskedTensor</code> or something similar. The values not represented by the indices are non-opts.</p>\n<p>I does seem particular, but I think it's general to many graph based models.</p>\n<p>In this masked tensors, other operations would also have different behaviors: taking the element-wise <code>exp</code> would keep the same sparsity pattern (because zeros are non-opt). This is easy to implement using <code>_values()</code> (and <code>coalesce</code>), but something like the soft-max across every row is a bit tricky...</p>\n<hr>\n<p><strong>Edit Oct 13 2018</strong>:<br>\nBecause my use case is for <em>masked</em> tensors (i.e. defining which computation should be done), I also have an operation that takes two dense tensors <code>A</code> and <code>B</code>, and compute the product <em>only where specified in a (sparse) mask</em> <code>m</code>. So same results as <code>(A @ B) * m</code> except all unnecessary computations are not done.</p>", "body_text": "Hello,\nI use sparse tensors to build Graph neural network on big sparse input graphs. The common approach of masking non existing edges in the adjacency matrix just doesn't scale for my inputs.\nI don't have sparse parameters (although it could be useful).\n\nFirst, a sparse matrix represent the adjacency matrix as the input of the network.\nLater on, information is propagated from one node to another according to the topology of the graph in a way similar to attention. A sparse matrix (eventually hybrid) holds attention weights outputted from another (dense) network taking as input the features of two nodes. Sparse matrix multiplication is used to sum over neighboring nodes.\n\nIn my case I differentiate (via my own functions). The kind of usage is a bit different than expected: the gradient wrt to some of my sparse tensors would usually not be sparse, but still keep the same sparsity pattern.  This is because in the case of graphs, the sparsity pattern represent allowed computation. The rest should is theoretically non existant, so it the gradient wrt to the zero values in the tensor doesn't make sense.\nIn my case it should be called a SparseMaskedTensor or something similar. The values not represented by the indices are non-opts.\nI does seem particular, but I think it's general to many graph based models.\nIn this masked tensors, other operations would also have different behaviors: taking the element-wise exp would keep the same sparsity pattern (because zeros are non-opt). This is easy to implement using _values() (and coalesce), but something like the soft-max across every row is a bit tricky...\n\nEdit Oct 13 2018:\nBecause my use case is for masked tensors (i.e. defining which computation should be done), I also have an operation that takes two dense tensors A and B, and compute the product only where specified in a (sparse) mask m. So same results as (A @ B) * m except all unnecessary computations are not done.", "body": "Hello,\r\n\r\nI use sparse tensors to build Graph neural network on big sparse input graphs. The common approach of masking non existing edges in the adjacency matrix just doesn't scale for my inputs.\r\n\r\nI don't have sparse parameters (although it could be useful). \r\n - First, a sparse matrix represent the adjacency matrix as the input of the network. \r\n - Later on, information is propagated from one node to another according to the topology of the graph in a way similar to attention. A sparse matrix (eventually hybrid) holds attention weights outputted from another (dense) network taking as input the features of two nodes. Sparse matrix multiplication is used to sum over neighboring nodes.\r\n\r\nIn my case I differentiate (via my own functions). The kind of usage is a bit different than expected: the gradient wrt to some of my sparse tensors would usually not be sparse, but __still keep the same sparsity pattern__.  This is because in the case of graphs, the sparsity pattern represent __allowed computation__. The rest should is theoretically non existant, so it the gradient wrt to the zero values in the tensor doesn't make sense.\r\n\r\nIn my case it should be called a `SparseMaskedTensor` or something similar. The values not represented by the indices are non-opts.\r\n\r\nI does seem particular, but I think it's general to many graph based models.\r\n\r\nIn this masked tensors, other operations would also have different behaviors: taking the element-wise `exp` would keep the same sparsity pattern (because zeros are non-opt). This is easy to implement using `_values()` (and `coalesce`), but something like the soft-max across every row is a bit tricky...\r\n\r\n--------------\r\n**Edit Oct 13 2018**:\r\nBecause my use case is for _masked_ tensors (i.e. defining which computation should be done), I also have an operation that takes two dense tensors `A` and `B`, and compute the product _only where specified in a (sparse) mask_ `m`. So same results as `(A @ B) * m` except all unnecessary computations are not done."}