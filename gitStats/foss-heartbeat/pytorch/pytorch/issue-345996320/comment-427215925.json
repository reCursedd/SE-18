{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427215925", "html_url": "https://github.com/pytorch/pytorch/issues/10043#issuecomment-427215925", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10043", "id": 427215925, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzIxNTkyNQ==", "user": {"login": "djsaunde", "id": 1245942, "node_id": "MDQ6VXNlcjEyNDU5NDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1245942?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djsaunde", "html_url": "https://github.com/djsaunde", "followers_url": "https://api.github.com/users/djsaunde/followers", "following_url": "https://api.github.com/users/djsaunde/following{/other_user}", "gists_url": "https://api.github.com/users/djsaunde/gists{/gist_id}", "starred_url": "https://api.github.com/users/djsaunde/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djsaunde/subscriptions", "organizations_url": "https://api.github.com/users/djsaunde/orgs", "repos_url": "https://api.github.com/users/djsaunde/repos", "events_url": "https://api.github.com/users/djsaunde/events{/privacy}", "received_events_url": "https://api.github.com/users/djsaunde/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-05T01:06:26Z", "updated_at": "2018-10-05T01:06:26Z", "author_association": "NONE", "body_html": "<p>I'd like to be able to do outer products of sparse vectors; i.e., for <code>x</code> and <code>y</code> sparse, <code>x.ger(y) -&gt; sparse</code>. This would dramatically speed up all learning rules for spiking neural networks (SNNs) in the <a href=\"https://github.com/Hananel-Hazan/bindsnet\">BindsNET</a> project, where neuron activations are 0/1 spikes.</p>\n<p>For example, using <code>scipy.sparse.coo_matrix</code>:</p>\n<pre><code># For comparison\nIn [32]: x, y = np.random.rand(10000, 1), np.random.rand(1, 10000)\n\nIn [33]: %timeit x.dot(y)\n278 ms \u00b1 1.45 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [34]: x, y = np.random.binomial(2, 0.01, [10000, 1]), np.random.binomial(2, 0.01, [1, 10000])\n\nIn [35]: x, y = coo_matrix(x), coo_matrix(y)\n\nIn [36]: %timeit x.dot(y)\n509 \u00b5s \u00b1 2.88 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nIn [37]: x, y = np.random.binomial(2, 0.1, [10000, 1]), np.random.binomial(2, 0.1, [1, 10000])\n\nIn [38]: x, y = coo_matrix(x), coo_matrix(y)\n\nIn [39]: %timeit x.dot(y)\n21.3 ms \u00b1 236 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre>", "body_text": "I'd like to be able to do outer products of sparse vectors; i.e., for x and y sparse, x.ger(y) -> sparse. This would dramatically speed up all learning rules for spiking neural networks (SNNs) in the BindsNET project, where neuron activations are 0/1 spikes.\nFor example, using scipy.sparse.coo_matrix:\n# For comparison\nIn [32]: x, y = np.random.rand(10000, 1), np.random.rand(1, 10000)\n\nIn [33]: %timeit x.dot(y)\n278 ms \u00b1 1.45 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [34]: x, y = np.random.binomial(2, 0.01, [10000, 1]), np.random.binomial(2, 0.01, [1, 10000])\n\nIn [35]: x, y = coo_matrix(x), coo_matrix(y)\n\nIn [36]: %timeit x.dot(y)\n509 \u00b5s \u00b1 2.88 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nIn [37]: x, y = np.random.binomial(2, 0.1, [10000, 1]), np.random.binomial(2, 0.1, [1, 10000])\n\nIn [38]: x, y = coo_matrix(x), coo_matrix(y)\n\nIn [39]: %timeit x.dot(y)\n21.3 ms \u00b1 236 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)", "body": "I'd like to be able to do outer products of sparse vectors; i.e., for `x` and `y` sparse, `x.ger(y) -> sparse`. This would dramatically speed up all learning rules for spiking neural networks (SNNs) in the [BindsNET](https://github.com/Hananel-Hazan/bindsnet) project, where neuron activations are 0/1 spikes.\r\n\r\nFor example, using `scipy.sparse.coo_matrix`:\r\n\r\n```\r\n# For comparison\r\nIn [32]: x, y = np.random.rand(10000, 1), np.random.rand(1, 10000)\r\n\r\nIn [33]: %timeit x.dot(y)\r\n278 ms \u00b1 1.45 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [34]: x, y = np.random.binomial(2, 0.01, [10000, 1]), np.random.binomial(2, 0.01, [1, 10000])\r\n\r\nIn [35]: x, y = coo_matrix(x), coo_matrix(y)\r\n\r\nIn [36]: %timeit x.dot(y)\r\n509 \u00b5s \u00b1 2.88 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\nIn [37]: x, y = np.random.binomial(2, 0.1, [10000, 1]), np.random.binomial(2, 0.1, [1, 10000])\r\n\r\nIn [38]: x, y = coo_matrix(x), coo_matrix(y)\r\n\r\nIn [39]: %timeit x.dot(y)\r\n21.3 ms \u00b1 236 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```"}