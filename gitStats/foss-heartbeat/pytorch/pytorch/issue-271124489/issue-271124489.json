{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3474", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3474/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3474/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3474/events", "html_url": "https://github.com/pytorch/pytorch/pull/3474", "id": 271124489, "node_id": "MDExOlB1bGxSZXF1ZXN0MTUwNjMzMDY4", "number": 3474, "title": "Signal handling in DataLoader workers; Timeout option", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 825466279, "node_id": "MDU6TGFiZWw4MjU0NjYyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/0.3.1", "name": "0.3.1", "color": "aefcae", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 26, "created_at": "2017-11-03T21:32:27Z", "updated_at": "2018-11-23T15:36:58Z", "closed_at": "2017-11-29T22:52:15Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3474", "html_url": "https://github.com/pytorch/pytorch/pull/3474", "diff_url": "https://github.com/pytorch/pytorch/pull/3474.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3474.patch"}, "body_html": "<ol>\n<li>Make DataLoader workers a bit more verbose on bus error and segfault, implemented with C++ signal handlers. Partially addresses <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"224245073\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1355\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1355/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1355\">#1355</a></li>\n<li>Adding timeout option to DataLoader, implemented with SIGALRM. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"250953181\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2474\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2474/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2474\">#2474</a></li>\n</ol>\n<p>Test plan (modified from script by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"230028085\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1595\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1595/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1595\">#1595</a> ):</p>\n<ol>\n<li>Segfault case:<br>\nscript:</li>\n</ol>\n<pre><code>class DS(object):\n    def __getitem__(self, idx):\n        import ctypes;ctypes.string_at(0)\n        return torch.rand(1000000)\n    def __len__(self):\n        return 200\n\nds = DS()\nit = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10)\n\nfor i, data in enumerate(it):\n    print(i)\n</code></pre>\n<p>output:</p>\n<pre><code>[~] python ds.py\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\n[~] # Doesn't hang anymore\n</code></pre>\n<ol start=\"2\">\n<li>Timeout:<br>\nscript:</li>\n</ol>\n<pre><code>import time\n\nclass DS(object):\n    def __getitem__(self, idx):\n        time.sleep(10)\n        return torch.rand(1000000)\n    def __len__(self):\n        return 200\n\nds = DS()\nit = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10, timeout=2)\n\nfor i, data in enumerate(it):\n    print(i)\n</code></pre>\n<p>output:</p>\n<pre><code>[~] python ds.py\nERROR: Time out when fetching data in DataLoader.\n[~]\n</code></pre>\n<p>Since this is deep into multiprocessing and low level signal handling, there might be cases/things I haven't considered. I'll also leave some comment below.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "Make DataLoader workers a bit more verbose on bus error and segfault, implemented with C++ signal handlers. Partially addresses #1355\nAdding timeout option to DataLoader, implemented with SIGALRM. #2474\n\nTest plan (modified from script by @fmassa in #1595 ):\n\nSegfault case:\nscript:\n\nclass DS(object):\n    def __getitem__(self, idx):\n        import ctypes;ctypes.string_at(0)\n        return torch.rand(1000000)\n    def __len__(self):\n        return 200\n\nds = DS()\nit = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10)\n\nfor i, data in enumerate(it):\n    print(i)\n\noutput:\n[~] python ds.py\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\nERROR: Unexpected segmentation fault encountered in worker.\n[~] # Doesn't hang anymore\n\n\nTimeout:\nscript:\n\nimport time\n\nclass DS(object):\n    def __getitem__(self, idx):\n        time.sleep(10)\n        return torch.rand(1000000)\n    def __len__(self):\n        return 200\n\nds = DS()\nit = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10, timeout=2)\n\nfor i, data in enumerate(it):\n    print(i)\n\noutput:\n[~] python ds.py\nERROR: Time out when fetching data in DataLoader.\n[~]\n\nSince this is deep into multiprocessing and low level signal handling, there might be cases/things I haven't considered. I'll also leave some comment below.\ncc @apaszke @colesbury", "body": "1. Make DataLoader workers a bit more verbose on bus error and segfault, implemented with C++ signal handlers. Partially addresses https://github.com/pytorch/pytorch/issues/1355\r\n2. Adding timeout option to DataLoader, implemented with SIGALRM. https://github.com/pytorch/pytorch/issues/2474 \r\n\r\nTest plan (modified from script by @fmassa in #1595 ):\r\n1. Segfault case:\r\nscript: \r\n```\r\nclass DS(object):\r\n    def __getitem__(self, idx):\r\n        import ctypes;ctypes.string_at(0)\r\n        return torch.rand(1000000)\r\n    def __len__(self):\r\n        return 200\r\n\r\nds = DS()\r\nit = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10)\r\n\r\nfor i, data in enumerate(it):\r\n    print(i)\r\n```\r\noutput: \r\n```\r\n[~] python ds.py\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nERROR: Unexpected segmentation fault encountered in worker.\r\n[~] # Doesn't hang anymore\r\n```\r\n\r\n2. Timeout:\r\nscript:\r\n```\r\nimport time\r\n\r\nclass DS(object):\r\n    def __getitem__(self, idx):\r\n        time.sleep(10)\r\n        return torch.rand(1000000)\r\n    def __len__(self):\r\n        return 200\r\n\r\nds = DS()\r\nit = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10, timeout=2)\r\n\r\nfor i, data in enumerate(it):\r\n    print(i)\r\n```\r\noutput:\r\n```\r\n[~] python ds.py\r\nERROR: Time out when fetching data in DataLoader.\r\n[~]\r\n```\r\n\r\nSince this is deep into multiprocessing and low level signal handling, there might be cases/things I haven't considered. I'll also leave some comment below.\r\n\r\ncc @apaszke @colesbury "}