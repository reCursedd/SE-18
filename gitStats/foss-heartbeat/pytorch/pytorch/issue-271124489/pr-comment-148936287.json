{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148936287", "pull_request_review_id": 74260375, "id": 148936287, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODkzNjI4Nw==", "diff_hunk": "@@ -570,6 +573,118 @@ PyObject *THPModule_fromDLPack(PyObject *_unused, PyObject *data)\n   return torch::createPyObject(atensor);\n }\n \n+// In cases like data loader, if a worker process die due to bus error/segfault\n+// or just hang, the main process, if implemented with multiprocessing.queue.,\n+// will hang waiting for data. This is difficult to avoid on PyTorch side as it\n+// can be caused by limited shm, other libraries users call in the workers. The\n+// following methods is an effort to do our best provide some error message to\n+// users when such unfortunate events happen.\n+\n+// Critical signal handlers should be registered on worker processes before\n+// doing work.\n+// Python handle is _set_worker_signal_handlers().\n+#define SIGNAL_HANDLER(SIGNAL, HANDLER_NAME, ERROR_MSG)                       \\\n+static void HANDLER_NAME(int sig)                                             \\\n+{                                                                             \\\n+    write(fileno(stderr), ERROR_MSG, strlen(ERROR_MSG));                      \\\n+    _exit(EXIT_FAILURE);                                                      \\\n+}\n+\n+SIGNAL_HANDLER(SIGBUS, handler_SIGBUS, \"ERROR: Unexpected bus error encountered in worker. \"\n+  \"This might be caused by insufficient shared memory (shm).\\n\");\n+SIGNAL_HANDLER(SIGSEGV, handler_SIGSEGV, \"ERROR: Unexpected segmentation fault encountered in worker.\\n\");\n+\n+static std::mutex worker_pid_mutex;\n+static std::vector<pid_t> worker_pid_vec = {};\n+// The following are needed since std::vector is not asynchronous safe.\n+static pid_t *worker_pids;\n+static size_t num_worker_pids = 0;\n+\n+PyObject *THPModule_setWorkerSignalHandlers(PyObject *module, PyObject *arg) {\n+    signal(SIGBUS, handler_SIGBUS);\n+    signal(SIGSEGV, handler_SIGSEGV);\n+    Py_RETURN_NONE;\n+}\n+\n+// SIGCHLD hander should be registered on main loader process to catch any\n+// worker failing. SIGALRM handler is needed for implementing timeout.\n+// Python handles are _set_main_signal_handers() and\n+// _remove_main_signal_handers().\n+static void handler_SIGCHLD_main(int sig) {\n+  int status;\n+  pid_t p;\n+  pid_t *pid_ptr;\n+\n+  // The flags and status checks ensure that we are really observing a child\n+  // exiting, rather than other cases such as SIGSTOP and SIGCONT.\n+  // https://stackoverflow.com/a/40707100\n+  while ((p = waitpid(-1, &status, WNOHANG|WUNTRACED|WCONTINUED)) > 0) {\n+    if (WIFCONTINUED(status) || WIFSTOPPED(status))\n+      continue;\n+    if (WIFEXITED(status) != 0 && WEXITSTATUS(status) == 0)\n+      continue;\n+    // child must have exited with signal/error, check if it is one of the pid\n+    // we care about.\n+    pid_ptr = worker_pids;\n+    for (size_t i = 0; i < num_worker_pids; i++) {\n+      if (*pid_ptr == p)\n+        _exit(EXIT_FAILURE);\n+      pid_ptr++;\n+    }\n+  }\n+}\n+\n+SIGNAL_HANDLER(SIGALRM, handler_SIGALRM, \"ERROR: Time out when fetching data in DataLoader.\\n\")\n+\n+// We don't want to exit on any SIGCHLD from any child. child_pids is a sequence\n+// of pids we are interested in.\n+PyObject *THPModule_setMainSignalHandlers(PyObject *module, PyObject *child_pids) {\n+  auto tuple = PyTuple_Check(child_pids);\n+  THPUtils_assert(tuple || PyList_Check(child_pids), \"_set_main_signal_handler \"\n+        \"expects a tuple or list, but got %s\", THPUtils_typename(child_pids));\n+\n+  auto size = tuple ? PyTuple_GET_SIZE(child_pids) : PyList_GET_SIZE(child_pids);\n+  {\n+    std::unique_lock<std::mutex> lock(worker_pid_mutex);\n+    for (int idx = 0; idx < size; idx++) {\n+      PyObject* obj = tuple ? PyTuple_GET_ITEM(child_pids, idx) : PyList_GET_ITEM(child_pids, idx);\n+      worker_pid_vec.push_back((pid_t) THPUtils_unpackLong(obj));\n+    }\n+    worker_pids = &worker_pid_vec[0];\n+    num_worker_pids = worker_pid_vec.size();\n+  }\n+\n+  signal(SIGCHLD, handler_SIGCHLD_main);\n+  signal(SIGALRM, handler_SIGALRM);\n+  Py_RETURN_NONE;\n+}\n+\n+PyObject *THPModule_removeMainSignalHandlers(PyObject *module, PyObject *child_pids) {\n+  auto tuple = PyTuple_Check(child_pids);\n+  THPUtils_assert(tuple || PyList_Check(child_pids), \"_remove_main_signal_handler \"\n+        \"expects a tuple or list, but got %s\", THPUtils_typename(child_pids));\n+\n+  auto size = tuple ? PyTuple_GET_SIZE(child_pids) : PyList_GET_SIZE(child_pids);\n+  std::set<pid_t> pid_set;\n+  for (int idx = 0; idx < size; idx++) {\n+    PyObject* obj = tuple ? PyTuple_GET_ITEM(child_pids, idx) : PyList_GET_ITEM(child_pids, idx);\n+    pid_set.insert((pid_t) THPUtils_unpackLong(obj));\n+  }\n+  {\n+    std::unique_lock<std::mutex> lock(worker_pid_mutex);\n+    worker_pid_vec.erase(std::remove_if(worker_pid_vec.begin(), worker_pid_vec.end(),\n+      [&](pid_t p){return pid_set.find(p) != pid_set.end();}), worker_pid_vec.end());\n+    worker_pids = &worker_pid_vec[0];", "path": "torch/csrc/Module.cpp", "position": null, "original_position": 116, "commit_id": "5733b553bcf269fb3782f7a0dbd4a12918998a5e", "original_commit_id": "b900f6041a6e0c55a17dc65a3ee01260ae72efb5", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I'm not sure how to fix this, but all this manipulation seems really unsafe. You're fiddling with the vector (making it smaller), even though `num_worker_pids` still contains the large value, so these elements at the end might still be read by the signal handler.", "created_at": "2017-11-04T17:20:21Z", "updated_at": "2018-11-23T15:36:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/3474#discussion_r148936287", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3474", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148936287"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3474#discussion_r148936287"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3474"}}, "body_html": "<p>I'm not sure how to fix this, but all this manipulation seems really unsafe. You're fiddling with the vector (making it smaller), even though <code>num_worker_pids</code> still contains the large value, so these elements at the end might still be read by the signal handler.</p>", "body_text": "I'm not sure how to fix this, but all this manipulation seems really unsafe. You're fiddling with the vector (making it smaller), even though num_worker_pids still contains the large value, so these elements at the end might still be read by the signal handler."}