{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148897977", "pull_request_review_id": 74219048, "id": 148897977, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODg5Nzk3Nw==", "diff_hunk": "@@ -570,6 +573,118 @@ PyObject *THPModule_fromDLPack(PyObject *_unused, PyObject *data)\n   return torch::createPyObject(atensor);\n }\n \n+// In cases like data loader, if a worker process die due to bus error/segfault\n+// or just hang, the main process, if implemented with multiprocessing.queue.,\n+// will hang waiting for data. This is difficult to avoid on PyTorch side as it\n+// can be caused by limited shm, other libraries users call in the workers. The\n+// following methods is an effort to do our best provide some error message to\n+// users when such unfortunate events happen.\n+\n+// Critical signal handlers should be registered on worker processes before\n+// doing work.\n+// Python handle is _set_worker_signal_handlers().\n+#define SIGNAL_HANDLER(SIGNAL, HANDLER_NAME, ERROR_MSG)                       \\\n+static void HANDLER_NAME(int sig)                                             \\\n+{                                                                             \\\n+    write(fileno(stderr), ERROR_MSG, strlen(ERROR_MSG));                      \\\n+    _exit(EXIT_FAILURE);                                                      \\\n+}\n+\n+SIGNAL_HANDLER(SIGBUS, handler_SIGBUS, \"ERROR: Unexpected bus error encountered in worker. \"\n+  \"This might be caused by insufficient shared memory (shm).\\n\");\n+SIGNAL_HANDLER(SIGSEGV, handler_SIGSEGV, \"ERROR: Unexpected segmentation fault encountered in worker.\\n\");\n+\n+static std::mutex worker_pid_mutex;\n+static std::vector<pid_t> worker_pid_vec = {};\n+// The following are needed since std::vector is not asynchronous safe.\n+static pid_t *worker_pids;\n+static size_t num_worker_pids = 0;\n+\n+PyObject *THPModule_setWorkerSignalHandlers(PyObject *module, PyObject *arg) {\n+    signal(SIGBUS, handler_SIGBUS);\n+    signal(SIGSEGV, handler_SIGSEGV);\n+    Py_RETURN_NONE;\n+}\n+\n+// SIGCHLD hander should be registered on main loader process to catch any\n+// worker failing. SIGALRM handler is needed for implementing timeout.\n+// Python handles are _set_main_signal_handers() and\n+// _remove_main_signal_handers().\n+static void handler_SIGCHLD_main(int sig) {\n+  int status;\n+  pid_t p;\n+  pid_t *pid_ptr;\n+\n+  // The flags and status checks ensure that we are really observing a child\n+  // exiting, rather than other cases such as SIGSTOP and SIGCONT.\n+  // https://stackoverflow.com/a/40707100\n+  while ((p = waitpid(-1, &status, WNOHANG|WUNTRACED|WCONTINUED)) > 0) {\n+    if (WIFCONTINUED(status) || WIFSTOPPED(status))\n+      continue;\n+    if (WIFEXITED(status) != 0 && WEXITSTATUS(status) == 0)\n+      continue;\n+    // child must have exited with signal/error, check if it is one of the pid\n+    // we care about.\n+    pid_ptr = worker_pids;\n+    for (size_t i = 0; i < num_worker_pids; i++) {\n+      if (*pid_ptr == p)\n+        _exit(EXIT_FAILURE);\n+      pid_ptr++;\n+    }\n+  }\n+}\n+\n+SIGNAL_HANDLER(SIGALRM, handler_SIGALRM, \"ERROR: Time out when fetching data in DataLoader.\\n\")\n+\n+// We don't want to exit on any SIGCHLD from any child. child_pids is a sequence\n+// of pids we are interested in.\n+PyObject *THPModule_setMainSignalHandlers(PyObject *module, PyObject *child_pids) {\n+  auto tuple = PyTuple_Check(child_pids);\n+  THPUtils_assert(tuple || PyList_Check(child_pids), \"_set_main_signal_handler \"\n+        \"expects a tuple or list, but got %s\", THPUtils_typename(child_pids));\n+\n+  auto size = tuple ? PyTuple_GET_SIZE(child_pids) : PyList_GET_SIZE(child_pids);\n+  {\n+    std::unique_lock<std::mutex> lock(worker_pid_mutex);", "path": "torch/csrc/Module.cpp", "position": null, "original_position": 87, "commit_id": "5733b553bcf269fb3782f7a0dbd4a12918998a5e", "original_commit_id": "b900f6041a6e0c55a17dc65a3ee01260ae72efb5", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Is guarding this with a lock necessary? Is GIL acquired here?", "created_at": "2017-11-03T21:33:10Z", "updated_at": "2018-11-23T15:35:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/3474#discussion_r148897977", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3474", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148897977"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3474#discussion_r148897977"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3474"}}, "body_html": "<p>Is guarding this with a lock necessary? Is GIL acquired here?</p>", "body_text": "Is guarding this with a lock necessary? Is GIL acquired here?"}