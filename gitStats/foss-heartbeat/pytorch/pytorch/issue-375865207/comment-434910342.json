{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434910342", "html_url": "https://github.com/pytorch/pytorch/issues/13381#issuecomment-434910342", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13381", "id": 434910342, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDkxMDM0Mg==", "user": {"login": "YunYang1994", "id": 30433053, "node_id": "MDQ6VXNlcjMwNDMzMDUz", "avatar_url": "https://avatars3.githubusercontent.com/u/30433053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YunYang1994", "html_url": "https://github.com/YunYang1994", "followers_url": "https://api.github.com/users/YunYang1994/followers", "following_url": "https://api.github.com/users/YunYang1994/following{/other_user}", "gists_url": "https://api.github.com/users/YunYang1994/gists{/gist_id}", "starred_url": "https://api.github.com/users/YunYang1994/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YunYang1994/subscriptions", "organizations_url": "https://api.github.com/users/YunYang1994/orgs", "repos_url": "https://api.github.com/users/YunYang1994/repos", "events_url": "https://api.github.com/users/YunYang1994/events{/privacy}", "received_events_url": "https://api.github.com/users/YunYang1994/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T02:29:13Z", "updated_at": "2018-11-01T02:40:40Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=629706\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/f0k\">@f0k</a> , Thanks for your answer, I got what you meant!  I realized that Running code on the GPU was<br>\nasynchronous. As you wished, I put a line <code>torch.cuda.synchronize()</code> before <code>end = time.time()</code>. but it seemed that it did not work very much?</p>\n<h3>for pytorch (+sync)</h3>\n<div class=\"highlight highlight-source-python\"><pre>start <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    tmp_tensor <span class=\"pl-k\">=</span> functional.conv2d(Input, weight, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">groups</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    tmp_tensor <span class=\"pl-k\">=</span> functional.batch_norm(tmp_tensor, weights_1, weights_1, weights_1, weights_1, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">9.99999974738e-06</span>)\n    tmp_tensor <span class=\"pl-k\">=</span> functional.relu(tmp_tensor)\n\ntorch.cuda.synchronize()\nend <span class=\"pl-k\">=</span> time.time()</pre></div>\n<table>\n<thead>\n<tr>\n<th>iterations</th>\n<th>1</th>\n<th>10</th>\n<th>100</th>\n<th>1000</th>\n<th>5000</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>pytorch(GPU)</code></td>\n<td>0.0026 s</td>\n<td>0.0031 s</td>\n<td>0.0082 s</td>\n<td>0.6314 s</td>\n<td>3.5506 s</td>\n<td>7.162 s</td>\n</tr>\n<tr>\n<td><code>pytorch(GPU+sync)</code></td>\n<td>0.0499 s</td>\n<td>0.0587 s</td>\n<td>0.1411 s</td>\n<td>0.8223 s</td>\n<td>3.8552 s</td>\n<td>7.5512 s</td>\n</tr>\n<tr>\n<td><code>ATen(GPU)</code></td>\n<td>0.54001 s</td>\n<td>0.54106 s</td>\n<td>0.54503 s</td>\n<td>1.1248 s</td>\n<td>4.0507 s</td>\n<td>7.6824 s</td>\n</tr>\n</tbody>\n</table>", "body_text": "@f0k , Thanks for your answer, I got what you meant!  I realized that Running code on the GPU was\nasynchronous. As you wished, I put a line torch.cuda.synchronize() before end = time.time(). but it seemed that it did not work very much?\nfor pytorch (+sync)\nstart = time.time()\nfor i in range(N):\n    tmp_tensor = functional.conv2d(Input, weight, bias=None, stride=2, padding=3, dilation=1, groups=1)\n    tmp_tensor = functional.batch_norm(tmp_tensor, weights_1, weights_1, weights_1, weights_1, training=False, momentum=1.0, eps=9.99999974738e-06)\n    tmp_tensor = functional.relu(tmp_tensor)\n\ntorch.cuda.synchronize()\nend = time.time()\n\n\n\niterations\n1\n10\n100\n1000\n5000\n10000\n\n\n\n\npytorch(GPU)\n0.0026 s\n0.0031 s\n0.0082 s\n0.6314 s\n3.5506 s\n7.162 s\n\n\npytorch(GPU+sync)\n0.0499 s\n0.0587 s\n0.1411 s\n0.8223 s\n3.8552 s\n7.5512 s\n\n\nATen(GPU)\n0.54001 s\n0.54106 s\n0.54503 s\n1.1248 s\n4.0507 s\n7.6824 s", "body": "@f0k , Thanks for your answer, I got what you meant!  I realized that Running code on the GPU was \r\n asynchronous. As you wished, I put a line `torch.cuda.synchronize()` before `end = time.time()`. but it seemed that it did not work very much?\r\n\r\n### for pytorch (+sync)\r\n```py\r\nstart = time.time()\r\nfor i in range(N):\r\n    tmp_tensor = functional.conv2d(Input, weight, bias=None, stride=2, padding=3, dilation=1, groups=1)\r\n    tmp_tensor = functional.batch_norm(tmp_tensor, weights_1, weights_1, weights_1, weights_1, training=False, momentum=1.0, eps=9.99999974738e-06)\r\n    tmp_tensor = functional.relu(tmp_tensor)\r\n\r\ntorch.cuda.synchronize()\r\nend = time.time()\r\n```\r\n\r\n\r\n| iterations | 1 | 10 | 100 | 1000 | 5000 | 10000 |\r\n| ---------- | -----------| ---------- | -----------| ---------- | -----------| ---------- |\r\n| `pytorch(GPU)`   | 0.0026 s  | 0.0031 s | 0.0082 s | 0.6314 s | 3.5506 s | 7.162 s |\r\n| `pytorch(GPU+sync)`   | 0.0499 s  | 0.0587 s | 0.1411 s | 0.8223 s | 3.8552 s | 7.5512 s |\r\n| `ATen(GPU)`   | 0.54001 s  | 0.54106 s | 0.54503 s | 1.1248 s | 4.0507 s | 7.6824 s |"}