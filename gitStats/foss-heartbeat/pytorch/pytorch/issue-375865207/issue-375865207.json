{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13381", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13381/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13381/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13381/events", "html_url": "https://github.com/pytorch/pytorch/issues/13381", "id": 375865207, "node_id": "MDU6SXNzdWUzNzU4NjUyMDc=", "number": 13381, "title": "the running time difference between the ATen library and pytorch ?", "user": {"login": "YunYang1994", "id": 30433053, "node_id": "MDQ6VXNlcjMwNDMzMDUz", "avatar_url": "https://avatars3.githubusercontent.com/u/30433053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YunYang1994", "html_url": "https://github.com/YunYang1994", "followers_url": "https://api.github.com/users/YunYang1994/followers", "following_url": "https://api.github.com/users/YunYang1994/following{/other_user}", "gists_url": "https://api.github.com/users/YunYang1994/gists{/gist_id}", "starred_url": "https://api.github.com/users/YunYang1994/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YunYang1994/subscriptions", "organizations_url": "https://api.github.com/users/YunYang1994/orgs", "repos_url": "https://api.github.com/users/YunYang1994/repos", "events_url": "https://api.github.com/users/YunYang1994/events{/privacy}", "received_events_url": "https://api.github.com/users/YunYang1994/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-31T09:15:40Z", "updated_at": "2018-11-01T02:40:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"question\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/2753.png\">\u2753</g-emoji> the running time difference between the ATen library and pytorch</h2>\n<p><a href=\"https://github.com/zdevito/ATen/tree/master/aten\">ATen</a> is a simple tensor library thats exposes the Tensor operations in <a href=\"https://pytorch.org/\" rel=\"nofollow\">Pytorch</a> directly in C++11. I wrote some codes to compare their performance and encountered some problems .<a href=\"https://github.com/YunYang1994/ATenTest\">here</a> are my codes.</p>\n<h2>On pytorch platform</h2>\n<p>source file <a href=\"https://github.com/YunYang1994/ATenTest/blob/master/test.py\"><code>test.py</code></a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> functional\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser(<span class=\"pl-v\">description</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>compare running time on between cpu and gpu<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>the number of iterations<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>device<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1: use cuda, 0: not use<span class=\"pl-pds\">\"</span></span>)\n\nN <span class=\"pl-k\">=</span> parser.parse_args().N\ndevice <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gpu<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> parser.parse_args().device <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu<span class=\"pl-pds\">\"</span></span>\n\ninput_data <span class=\"pl-k\">=</span> np.ones([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">224</span>,<span class=\"pl-c1\">224</span>])\ninput_data <span class=\"pl-k\">=</span> input_data.astype(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\n\nweight_data <span class=\"pl-k\">=</span> np.ones([<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">7</span>,<span class=\"pl-c1\">7</span>])\nweight_data <span class=\"pl-k\">=</span> weight_data.astype(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\nbn_weight <span class=\"pl-k\">=</span> np.ones([<span class=\"pl-c1\">64</span>,])\nbn_weight <span class=\"pl-k\">=</span> bn_weight.astype(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\n\nInput <span class=\"pl-k\">=</span> torch.from_numpy(input_data)\nweight <span class=\"pl-k\">=</span> torch.from_numpy(weight_data)\nweights_1 <span class=\"pl-k\">=</span> torch.from_numpy(bn_weight)\n\n<span class=\"pl-k\">if</span> device <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gpu<span class=\"pl-pds\">\"</span></span>:\n    Input <span class=\"pl-k\">=</span> Input.cuda()\n    weight <span class=\"pl-k\">=</span> weight.cuda()\n    weights_1 <span class=\"pl-k\">=</span> weights_1.cuda()\n\nstart <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    tmp_tensor <span class=\"pl-k\">=</span> functional.conv2d(Input, weight, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">groups</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    tmp_tensor <span class=\"pl-k\">=</span> functional.batch_norm(tmp_tensor, weights_1, weights_1, weights_1, weights_1, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">9.99999974738e-06</span>)\n    tmp_tensor <span class=\"pl-k\">=</span> functional.relu(tmp_tensor)\nend <span class=\"pl-k\">=</span> time.time()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>running on <span class=\"pl-c1\">%s</span> <span class=\"pl-c1\">%.4f</span> s<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>(device, end <span class=\"pl-k\">-</span> start))</pre></div>\n<pre lang=\"bashrc\"><code>yang@yang:~/ATenTest$ python test.py 1 0 # CPU\nrunning on cpu 0.0155 s\nyang@yang:~/ATenTest$ python test.py 1 1 # GPU\nrunning on gpu 0.0027 s\n</code></pre>\n<h2>On ATen library</h2>\n<p>source file <a href=\"https://github.com/YunYang1994/ATenTest/blob/master/test.cpp\"><code>test.cpp</code></a></p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>vector<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>stdlib.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>time.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>unistd.h<span class=\"pl-pds\">&gt;</span></span>\n\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>ATen/ATen.h<span class=\"pl-pds\">&gt;</span></span>  <span class=\"pl-c\"><span class=\"pl-c\">//</span> containing cuda header files (.h) !!!</span>\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">at</span><span class=\"pl-k\">;</span>\n\n<span class=\"pl-k\">int</span> <span class=\"pl-en\">main</span>(<span class=\"pl-k\">int</span> argc, <span class=\"pl-k\">char</span>** argv) {\n\n    <span class=\"pl-c1\">clock_t</span> t_start, t_end; <span class=\"pl-c\"><span class=\"pl-c\">//</span> recording time</span>\n    <span class=\"pl-k\">double</span> totaltime;\n    <span class=\"pl-k\">int</span> N = <span class=\"pl-c1\">atof</span>(argv[<span class=\"pl-c1\">1</span>]); <span class=\"pl-c\"><span class=\"pl-c\">//</span> the number of iterations</span>\n    <span class=\"pl-k\">bool</span> gpu = <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">atof</span>(argv[<span class=\"pl-c1\">2</span>]));\n    std::vector&lt;Tensor&gt; <span class=\"pl-c1\">Input</span>(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">weights</span>(<span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">tmp_tensor</span>(<span class=\"pl-c1\">1</span>); <span class=\"pl-c\"><span class=\"pl-c\">//</span> prepare vector for data</span>\n\n    <span class=\"pl-k\">if</span>(gpu){\n        std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>running on gpu <span class=\"pl-pds\">\"</span></span>;\n        {Tensor buff=<span class=\"pl-c1\">CUDA</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">ones</span>({<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">224</span>,<span class=\"pl-c1\">224</span>}); Input[<span class=\"pl-c1\">0</span>]=buff.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">copy</span>(buff);}\n        {Tensor buff=<span class=\"pl-c1\">CUDA</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">ones</span>({<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">7</span>,<span class=\"pl-c1\">7</span>}); weights[<span class=\"pl-c1\">0</span>]=buff.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">copy</span>(buff);}\n        {Tensor buff=<span class=\"pl-c1\">CUDA</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">ones</span>({<span class=\"pl-c1\">64</span>,}); weights[<span class=\"pl-c1\">1</span>]=buff.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">copy</span>(buff);}\n    }\n    <span class=\"pl-k\">else</span>{\n        std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>running on cpu <span class=\"pl-pds\">\"</span></span>;\n        {Tensor buff=<span class=\"pl-c1\">CPU</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">ones</span>({<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">224</span>,<span class=\"pl-c1\">224</span>}); Input[<span class=\"pl-c1\">0</span>]=buff.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">copy</span>(buff);}\n        {Tensor buff=<span class=\"pl-c1\">CPU</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">ones</span>({<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">7</span>,<span class=\"pl-c1\">7</span>}); weights[<span class=\"pl-c1\">0</span>]=buff.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">copy</span>(buff);}\n        {Tensor buff=<span class=\"pl-c1\">CPU</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">ones</span>({<span class=\"pl-c1\">64</span>,}); weights[<span class=\"pl-c1\">1</span>]=buff.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">copy</span>(buff);}\n    }\n\n    t_start = <span class=\"pl-c1\">clock</span>();\n    <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> i=<span class=\"pl-c1\">0</span>; i&lt;N; i++){\n        tmp_tensor[<span class=\"pl-c1\">0</span>] = <span class=\"pl-c1\">convolution</span>(Input[<span class=\"pl-c1\">0</span>], weights[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">Tensor</span>(), {{<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>}}, {{<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>}}, {{<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>}}, <span class=\"pl-c1\">false</span>, {{<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>}}, <span class=\"pl-c1\">1</span>);\n        tmp_tensor[<span class=\"pl-c1\">0</span>] = <span class=\"pl-c1\">batch_norm</span>(tmp_tensor[<span class=\"pl-c1\">0</span>], weights[<span class=\"pl-c1\">1</span>], weights[<span class=\"pl-c1\">1</span>], weights[<span class=\"pl-c1\">1</span>], weights[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">false</span>, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">9.99999974738e-06</span>, <span class=\"pl-c1\">false</span>);\n        tmp_tensor[<span class=\"pl-c1\">0</span>] = <span class=\"pl-c1\">relu</span>(tmp_tensor[<span class=\"pl-c1\">0</span>]);\n    }\n    t_end = <span class=\"pl-c1\">clock</span>();\n\n    totaltime = (<span class=\"pl-k\">double</span>)(t_end - t_start)/CLOCKS_PER_SEC;\n    std::cout &lt;&lt; totaltime &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> s<span class=\"pl-pds\">\"</span></span> &lt;&lt;std::endl;\n}</pre></div>\n<p>I compiled it with following command on Linux:</p>\n<pre lang=\"bashrc\"><code>g++ -Wl,--no-as-needed -w test.cpp -o main -I ./include -L . -lATen_cuda -lATen_cpu -std=c++11\n</code></pre>\n<p>In the following step, I got <a href=\"https://github.com/YunYang1994/ATenTest/blob/master/main\"><code>main</code></a> which was an executable file, and can then run it on CPU or GPU like this.</p>\n<pre lang=\"bashrc\"><code>yang@yang:~/ATenTest$ ./main 1 1 # on GPU (argv[1] is the iterations, argv[2] means use cuda or not\nrunning on gpu 0.542298 s\nyang@yang:~/ATenTest$ ./main 1 0 # on CPU\nrunning on cpu 0.072884 s\n</code></pre>\n<p>Now let's consider trying a range of iterations on CPU or GPU plateform, the code running time difference shows in the following table.</p>\n<table>\n<thead>\n<tr>\n<th>iterations</th>\n<th>1</th>\n<th>10</th>\n<th>100</th>\n<th>1000</th>\n<th>5000</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>pytorch(CPU)</code></td>\n<td>0.0156 s</td>\n<td>0.1062 s</td>\n<td>0.9916 s</td>\n<td>9.8697 s</td>\n<td>50.2868 s</td>\n<td>98.1801 s</td>\n</tr>\n<tr>\n<td><code>pytorch(GPU)</code></td>\n<td>0.0026 s</td>\n<td>0.0031 s</td>\n<td>0.0082 s</td>\n<td>0.6314 s</td>\n<td>3.5506 s</td>\n<td>7.162 s</td>\n</tr>\n<tr>\n<td><code>ATen(CPU)</code></td>\n<td>0.071542 s</td>\n<td>0.424533 s</td>\n<td>3.85708 s</td>\n<td>38.1909 s</td>\n<td>191.34 s</td>\n<td>383.295 s</td>\n</tr>\n<tr>\n<td><code>ATen(GPU)</code></td>\n<td>0.540007 s</td>\n<td>0.541056 s</td>\n<td>0.545027 s</td>\n<td>1.12482 s</td>\n<td>4.0507 s</td>\n<td>7.68236 s</td>\n</tr>\n</tbody>\n</table>\n<p>As a result, the code on GPU was much more faster than on the CPU for both pytorch and ATen. And it was expectd that there was less running time difference between pytorch plateform and ATen library, but the result showed that code on ATen libaray was always much slower than on the pytorch plateform (This makes no sense, for the reason that C++ should be faster and lighter than Python)! The confusing point in my mind is</p>\n<h5>Did pytorch do any optimization on the basis of ATen library ?</h5>", "body_text": "\u2753 the running time difference between the ATen library and pytorch\nATen is a simple tensor library thats exposes the Tensor operations in Pytorch directly in C++11. I wrote some codes to compare their performance and encountered some problems .here are my codes.\nOn pytorch platform\nsource file test.py\nimport numpy as np\nimport torch\nimport time\nimport argparse\nfrom torch.nn import functional\n\nparser = argparse.ArgumentParser(description=\"compare running time on between cpu and gpu\")\nparser.add_argument('N', type=int, help=\"the number of iterations\")\nparser.add_argument('device', type=int, help=\"1: use cuda, 0: not use\")\n\nN = parser.parse_args().N\ndevice = \"gpu\" if parser.parse_args().device else \"cpu\"\n\ninput_data = np.ones([1,3,224,224])\ninput_data = input_data.astype(\"float32\")\n\nweight_data = np.ones([64,3,7,7])\nweight_data = weight_data.astype(\"float32\")\nbn_weight = np.ones([64,])\nbn_weight = bn_weight.astype(\"float32\")\n\nInput = torch.from_numpy(input_data)\nweight = torch.from_numpy(weight_data)\nweights_1 = torch.from_numpy(bn_weight)\n\nif device == \"gpu\":\n    Input = Input.cuda()\n    weight = weight.cuda()\n    weights_1 = weights_1.cuda()\n\nstart = time.time()\nfor i in range(N):\n    tmp_tensor = functional.conv2d(Input, weight, bias=None, stride=2, padding=3, dilation=1, groups=1)\n    tmp_tensor = functional.batch_norm(tmp_tensor, weights_1, weights_1, weights_1, weights_1, training=False, momentum=1.0, eps=9.99999974738e-06)\n    tmp_tensor = functional.relu(tmp_tensor)\nend = time.time()\n\nprint(\"running on %s %.4f s\" %(device, end - start))\nyang@yang:~/ATenTest$ python test.py 1 0 # CPU\nrunning on cpu 0.0155 s\nyang@yang:~/ATenTest$ python test.py 1 1 # GPU\nrunning on gpu 0.0027 s\n\nOn ATen library\nsource file test.cpp\n#include <iostream>\n#include <vector>\n#include <stdlib.h>\n#include <time.h>\n#include <unistd.h>\n\n#include <ATen/ATen.h>  // containing cuda header files (.h) !!!\nusing namespace at;\n\nint main(int argc, char** argv) {\n\n    clock_t t_start, t_end; // recording time\n    double totaltime;\n    int N = atof(argv[1]); // the number of iterations\n    bool gpu = int(atof(argv[2]));\n    std::vector<Tensor> Input(1), weights(3), tmp_tensor(1); // prepare vector for data\n\n    if(gpu){\n        std::cout << \"running on gpu \";\n        {Tensor buff=CUDA(kFloat).ones({1,3,224,224}); Input[0]=buff.type().copy(buff);}\n        {Tensor buff=CUDA(kFloat).ones({64,3,7,7}); weights[0]=buff.type().copy(buff);}\n        {Tensor buff=CUDA(kFloat).ones({64,}); weights[1]=buff.type().copy(buff);}\n    }\n    else{\n        std::cout << \"running on cpu \";\n        {Tensor buff=CPU(kFloat).ones({1,3,224,224}); Input[0]=buff.type().copy(buff);}\n        {Tensor buff=CPU(kFloat).ones({64,3,7,7}); weights[0]=buff.type().copy(buff);}\n        {Tensor buff=CPU(kFloat).ones({64,}); weights[1]=buff.type().copy(buff);}\n    }\n\n    t_start = clock();\n    for(int i=0; i<N; i++){\n        tmp_tensor[0] = convolution(Input[0], weights[0], Tensor(), {{2,2}}, {{3,3}}, {{1,1}}, false, {{0,0}}, 1);\n        tmp_tensor[0] = batch_norm(tmp_tensor[0], weights[1], weights[1], weights[1], weights[1], false, 1.0, 9.99999974738e-06, false);\n        tmp_tensor[0] = relu(tmp_tensor[0]);\n    }\n    t_end = clock();\n\n    totaltime = (double)(t_end - t_start)/CLOCKS_PER_SEC;\n    std::cout << totaltime << \" s\" <<std::endl;\n}\nI compiled it with following command on Linux:\ng++ -Wl,--no-as-needed -w test.cpp -o main -I ./include -L . -lATen_cuda -lATen_cpu -std=c++11\n\nIn the following step, I got main which was an executable file, and can then run it on CPU or GPU like this.\nyang@yang:~/ATenTest$ ./main 1 1 # on GPU (argv[1] is the iterations, argv[2] means use cuda or not\nrunning on gpu 0.542298 s\nyang@yang:~/ATenTest$ ./main 1 0 # on CPU\nrunning on cpu 0.072884 s\n\nNow let's consider trying a range of iterations on CPU or GPU plateform, the code running time difference shows in the following table.\n\n\n\niterations\n1\n10\n100\n1000\n5000\n10000\n\n\n\n\npytorch(CPU)\n0.0156 s\n0.1062 s\n0.9916 s\n9.8697 s\n50.2868 s\n98.1801 s\n\n\npytorch(GPU)\n0.0026 s\n0.0031 s\n0.0082 s\n0.6314 s\n3.5506 s\n7.162 s\n\n\nATen(CPU)\n0.071542 s\n0.424533 s\n3.85708 s\n38.1909 s\n191.34 s\n383.295 s\n\n\nATen(GPU)\n0.540007 s\n0.541056 s\n0.545027 s\n1.12482 s\n4.0507 s\n7.68236 s\n\n\n\nAs a result, the code on GPU was much more faster than on the CPU for both pytorch and ATen. And it was expectd that there was less running time difference between pytorch plateform and ATen library, but the result showed that code on ATen libaray was always much slower than on the pytorch plateform (This makes no sense, for the reason that C++ should be faster and lighter than Python)! The confusing point in my mind is\nDid pytorch do any optimization on the basis of ATen library ?", "body": "## \u2753 the running time difference between the ATen library and pytorch\r\n\r\n\r\n\r\n[ATen](https://github.com/zdevito/ATen/tree/master/aten) is a simple tensor library thats exposes the Tensor operations in [Pytorch](https://pytorch.org/) directly in C++11. I wrote some codes to compare their performance and encountered some problems .[here](https://github.com/YunYang1994/ATenTest) are my codes.\r\n\r\n## On pytorch platform\r\nsource file [`test.py`](https://github.com/YunYang1994/ATenTest/blob/master/test.py)\r\n```py\r\nimport numpy as np\r\nimport torch\r\nimport time\r\nimport argparse\r\nfrom torch.nn import functional\r\n\r\nparser = argparse.ArgumentParser(description=\"compare running time on between cpu and gpu\")\r\nparser.add_argument('N', type=int, help=\"the number of iterations\")\r\nparser.add_argument('device', type=int, help=\"1: use cuda, 0: not use\")\r\n\r\nN = parser.parse_args().N\r\ndevice = \"gpu\" if parser.parse_args().device else \"cpu\"\r\n\r\ninput_data = np.ones([1,3,224,224])\r\ninput_data = input_data.astype(\"float32\")\r\n\r\nweight_data = np.ones([64,3,7,7])\r\nweight_data = weight_data.astype(\"float32\")\r\nbn_weight = np.ones([64,])\r\nbn_weight = bn_weight.astype(\"float32\")\r\n\r\nInput = torch.from_numpy(input_data)\r\nweight = torch.from_numpy(weight_data)\r\nweights_1 = torch.from_numpy(bn_weight)\r\n\r\nif device == \"gpu\":\r\n    Input = Input.cuda()\r\n    weight = weight.cuda()\r\n    weights_1 = weights_1.cuda()\r\n\r\nstart = time.time()\r\nfor i in range(N):\r\n    tmp_tensor = functional.conv2d(Input, weight, bias=None, stride=2, padding=3, dilation=1, groups=1)\r\n    tmp_tensor = functional.batch_norm(tmp_tensor, weights_1, weights_1, weights_1, weights_1, training=False, momentum=1.0, eps=9.99999974738e-06)\r\n    tmp_tensor = functional.relu(tmp_tensor)\r\nend = time.time()\r\n\r\nprint(\"running on %s %.4f s\" %(device, end - start))\r\n```\r\n\r\n```bashrc\r\nyang@yang:~/ATenTest$ python test.py 1 0 # CPU\r\nrunning on cpu 0.0155 s\r\nyang@yang:~/ATenTest$ python test.py 1 1 # GPU\r\nrunning on gpu 0.0027 s\r\n```\r\n\r\n## On ATen library\r\nsource file [`test.cpp`](https://github.com/YunYang1994/ATenTest/blob/master/test.cpp)\r\n```cpp\r\n#include <iostream>\r\n#include <vector>\r\n#include <stdlib.h>\r\n#include <time.h>\r\n#include <unistd.h>\r\n\r\n#include <ATen/ATen.h>  // containing cuda header files (.h) !!!\r\nusing namespace at;\r\n\r\nint main(int argc, char** argv) {\r\n\r\n    clock_t t_start, t_end; // recording time\r\n    double totaltime;\r\n    int N = atof(argv[1]); // the number of iterations\r\n    bool gpu = int(atof(argv[2]));\r\n    std::vector<Tensor> Input(1), weights(3), tmp_tensor(1); // prepare vector for data\r\n\r\n    if(gpu){\r\n        std::cout << \"running on gpu \";\r\n        {Tensor buff=CUDA(kFloat).ones({1,3,224,224}); Input[0]=buff.type().copy(buff);}\r\n        {Tensor buff=CUDA(kFloat).ones({64,3,7,7}); weights[0]=buff.type().copy(buff);}\r\n        {Tensor buff=CUDA(kFloat).ones({64,}); weights[1]=buff.type().copy(buff);}\r\n    }\r\n    else{\r\n        std::cout << \"running on cpu \";\r\n        {Tensor buff=CPU(kFloat).ones({1,3,224,224}); Input[0]=buff.type().copy(buff);}\r\n        {Tensor buff=CPU(kFloat).ones({64,3,7,7}); weights[0]=buff.type().copy(buff);}\r\n        {Tensor buff=CPU(kFloat).ones({64,}); weights[1]=buff.type().copy(buff);}\r\n    }\r\n\r\n    t_start = clock();\r\n    for(int i=0; i<N; i++){\r\n        tmp_tensor[0] = convolution(Input[0], weights[0], Tensor(), {{2,2}}, {{3,3}}, {{1,1}}, false, {{0,0}}, 1);\r\n        tmp_tensor[0] = batch_norm(tmp_tensor[0], weights[1], weights[1], weights[1], weights[1], false, 1.0, 9.99999974738e-06, false);\r\n        tmp_tensor[0] = relu(tmp_tensor[0]);\r\n    }\r\n    t_end = clock();\r\n\r\n    totaltime = (double)(t_end - t_start)/CLOCKS_PER_SEC;\r\n    std::cout << totaltime << \" s\" <<std::endl;\r\n}\r\n```\r\n\r\nI compiled it with following command on Linux:\r\n\r\n```bashrc\r\ng++ -Wl,--no-as-needed -w test.cpp -o main -I ./include -L . -lATen_cuda -lATen_cpu -std=c++11\r\n```\r\n\r\nIn the following step, I got [`main`](https://github.com/YunYang1994/ATenTest/blob/master/main) which was an executable file, and can then run it on CPU or GPU like this. \r\n\r\n```bashrc\r\nyang@yang:~/ATenTest$ ./main 1 1 # on GPU (argv[1] is the iterations, argv[2] means use cuda or not\r\nrunning on gpu 0.542298 s\r\nyang@yang:~/ATenTest$ ./main 1 0 # on CPU\r\nrunning on cpu 0.072884 s\r\n```\r\nNow let's consider trying a range of iterations on CPU or GPU plateform, the code running time difference shows in the following table.\r\n\r\n| iterations | 1 | 10 | 100 | 1000 | 5000 | 10000 |\r\n| ---------- | -----------| ---------- | -----------| ---------- | -----------| ---------- |\r\n| `pytorch(CPU)`   | 0.0156 s  | 0.1062 s | 0.9916 s | 9.8697 s | 50.2868 s | 98.1801 s |\r\n| `pytorch(GPU)`   | 0.0026 s  | 0.0031 s | 0.0082 s | 0.6314 s | 3.5506 s | 7.162 s |\r\n| `ATen(CPU)`   | 0.071542 s  | 0.424533 s | 3.85708 s |  38.1909 s | 191.34 s | 383.295 s |\r\n| `ATen(GPU)`   | 0.540007 s  | 0.541056 s | 0.545027 s | 1.12482 s | 4.0507 s | 7.68236 s |\r\n\r\nAs a result, the code on GPU was much more faster than on the CPU for both pytorch and ATen. And it was expectd that there was less running time difference between pytorch plateform and ATen library, but the result showed that code on ATen libaray was always much slower than on the pytorch plateform (This makes no sense, for the reason that C++ should be faster and lighter than Python)! The confusing point in my mind is\r\n##### Did pytorch do any optimization on the basis of ATen library ? #####\r\n\r\n"}