{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5687", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5687/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5687/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5687/events", "html_url": "https://github.com/pytorch/pytorch/issues/5687", "id": 304064113, "node_id": "MDU6SXNzdWUzMDQwNjQxMTM=", "number": 5687, "title": "ReLU(inplace=True) seems something wrong internal", "user": {"login": "sonack", "id": 12935189, "node_id": "MDQ6VXNlcjEyOTM1MTg5", "avatar_url": "https://avatars1.githubusercontent.com/u/12935189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sonack", "html_url": "https://github.com/sonack", "followers_url": "https://api.github.com/users/sonack/followers", "following_url": "https://api.github.com/users/sonack/following{/other_user}", "gists_url": "https://api.github.com/users/sonack/gists{/gist_id}", "starred_url": "https://api.github.com/users/sonack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sonack/subscriptions", "organizations_url": "https://api.github.com/users/sonack/orgs", "repos_url": "https://api.github.com/users/sonack/repos", "events_url": "https://api.github.com/users/sonack/events{/privacy}", "received_events_url": "https://api.github.com/users/sonack/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2018-03-10T11:40:07Z", "updated_at": "2018-08-13T22:07:03Z", "closed_at": "2018-08-13T22:07:02Z", "author_association": "NONE", "body_html": "<p>With ReLU(inplace=True), my model can not be trained, and its loss goes to hundreds of thousands after a few iterations. However, when I replace with ReLU(inplace=False), all trouble disappear and my loss can be converge gradually.<br>\nPytorch didn't complain any about my inplace ReLU usage before, everything seemed peace, except the larger and large training loss. I have heard someone said \"When pytorch doesnt give you error or warning about your inplace operation usage, it almost be correctly working\".  But it seems some wrong internal , how did it happen?</p>\n<p>Below is my model code:</p>\n<pre><code>class ResidualBlock(nn.Module):\n    def __init__(self, ch_in, ch_out, shortcut = None):\n        super(ResidualBlock, self).__init__()\n        self.left = nn.Sequential(\n            nn.Conv2d(ch_in, 128, 3, 1, 1),  # ch_in, ch_out, kernel_size, stride, pad\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, ch_out, 3, 1, 1),\n            nn.ReLU(inplace=True),\n        )\n        self.right = shortcut\n    \n    def forward(self, x):\n        out = self.left(x)\n        residual = x if self.right is None else self.right(x)\n        out += residual\n        return F.relu(out, inplace=False)\n\nclass ContentWeightedCNN(BasicModule):\n    def __init__(self):\n        super(ContentWeightedCNN, self).__init__()\n        self.model_name = 'ContentWeightedCNN'\n        self.encoder = self.make_encoder()\n        self.decoder = self.make_decoder()\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        self.apply(weights_initialization)\n    \n    def forward(self, x):\n        mgdata = self.encoder(x)\n        dec_data = self.decoder(mgdata)\n        return dec_data\n\n    def make_encoder(self):\n        layers = [\n            nn.Conv2d(3, 128, 8, 4, 2),\n            nn.ReLU(inplace=True), # 54\n\n            ResidualBlock(128, 128),\n\n            nn.Conv2d(128, 256, 4, 2, 1), # 115\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(256, 256),\n\n            nn.Conv2d(256, 256, 3, 1, 1), #192\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(256, 256),\n\n            nn.Conv2d(256, 64, 1, 1, 0),    # conv 4  64 is n\n            nn.Sigmoid(),                    \n        ]\n        return nn.Sequential(*layers)\n\n    def make_decoder(self):\n        layers = [\n            nn.Conv2d(64, 512, 3, 1, 1),\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(512, 512),\n\n            nn.Conv2d(512, 512, 3, 1, 1),\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(512, 512),\n\n            nn.PixelShuffle(2),\n\n            nn.Conv2d(128, 256, 3, 1, 1),\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(256, 256),\n\n            nn.PixelShuffle(4),\n\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(32, 3, 1, 1, 0)   \n        ]\n        return nn.Sequential(*layers)\n \n            \n</code></pre>", "body_text": "With ReLU(inplace=True), my model can not be trained, and its loss goes to hundreds of thousands after a few iterations. However, when I replace with ReLU(inplace=False), all trouble disappear and my loss can be converge gradually.\nPytorch didn't complain any about my inplace ReLU usage before, everything seemed peace, except the larger and large training loss. I have heard someone said \"When pytorch doesnt give you error or warning about your inplace operation usage, it almost be correctly working\".  But it seems some wrong internal , how did it happen?\nBelow is my model code:\nclass ResidualBlock(nn.Module):\n    def __init__(self, ch_in, ch_out, shortcut = None):\n        super(ResidualBlock, self).__init__()\n        self.left = nn.Sequential(\n            nn.Conv2d(ch_in, 128, 3, 1, 1),  # ch_in, ch_out, kernel_size, stride, pad\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, ch_out, 3, 1, 1),\n            nn.ReLU(inplace=True),\n        )\n        self.right = shortcut\n    \n    def forward(self, x):\n        out = self.left(x)\n        residual = x if self.right is None else self.right(x)\n        out += residual\n        return F.relu(out, inplace=False)\n\nclass ContentWeightedCNN(BasicModule):\n    def __init__(self):\n        super(ContentWeightedCNN, self).__init__()\n        self.model_name = 'ContentWeightedCNN'\n        self.encoder = self.make_encoder()\n        self.decoder = self.make_decoder()\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        self.apply(weights_initialization)\n    \n    def forward(self, x):\n        mgdata = self.encoder(x)\n        dec_data = self.decoder(mgdata)\n        return dec_data\n\n    def make_encoder(self):\n        layers = [\n            nn.Conv2d(3, 128, 8, 4, 2),\n            nn.ReLU(inplace=True), # 54\n\n            ResidualBlock(128, 128),\n\n            nn.Conv2d(128, 256, 4, 2, 1), # 115\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(256, 256),\n\n            nn.Conv2d(256, 256, 3, 1, 1), #192\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(256, 256),\n\n            nn.Conv2d(256, 64, 1, 1, 0),    # conv 4  64 is n\n            nn.Sigmoid(),                    \n        ]\n        return nn.Sequential(*layers)\n\n    def make_decoder(self):\n        layers = [\n            nn.Conv2d(64, 512, 3, 1, 1),\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(512, 512),\n\n            nn.Conv2d(512, 512, 3, 1, 1),\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(512, 512),\n\n            nn.PixelShuffle(2),\n\n            nn.Conv2d(128, 256, 3, 1, 1),\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(256, 256),\n\n            nn.PixelShuffle(4),\n\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(32, 3, 1, 1, 0)   \n        ]\n        return nn.Sequential(*layers)", "body": "With ReLU(inplace=True), my model can not be trained, and its loss goes to hundreds of thousands after a few iterations. However, when I replace with ReLU(inplace=False), all trouble disappear and my loss can be converge gradually. \r\nPytorch didn't complain any about my inplace ReLU usage before, everything seemed peace, except the larger and large training loss. I have heard someone said \"When pytorch doesnt give you error or warning about your inplace operation usage, it almost be correctly working\".  But it seems some wrong internal , how did it happen?\r\n\r\nBelow is my model code:\r\n\r\n```\r\nclass ResidualBlock(nn.Module):\r\n    def __init__(self, ch_in, ch_out, shortcut = None):\r\n        super(ResidualBlock, self).__init__()\r\n        self.left = nn.Sequential(\r\n            nn.Conv2d(ch_in, 128, 3, 1, 1),  # ch_in, ch_out, kernel_size, stride, pad\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(128, ch_out, 3, 1, 1),\r\n            nn.ReLU(inplace=True),\r\n        )\r\n        self.right = shortcut\r\n    \r\n    def forward(self, x):\r\n        out = self.left(x)\r\n        residual = x if self.right is None else self.right(x)\r\n        out += residual\r\n        return F.relu(out, inplace=False)\r\n\r\nclass ContentWeightedCNN(BasicModule):\r\n    def __init__(self):\r\n        super(ContentWeightedCNN, self).__init__()\r\n        self.model_name = 'ContentWeightedCNN'\r\n        self.encoder = self.make_encoder()\r\n        self.decoder = self.make_decoder()\r\n        self.reset_parameters()\r\n    \r\n    def reset_parameters(self):\r\n        self.apply(weights_initialization)\r\n    \r\n    def forward(self, x):\r\n        mgdata = self.encoder(x)\r\n        dec_data = self.decoder(mgdata)\r\n        return dec_data\r\n\r\n    def make_encoder(self):\r\n        layers = [\r\n            nn.Conv2d(3, 128, 8, 4, 2),\r\n            nn.ReLU(inplace=True), # 54\r\n\r\n            ResidualBlock(128, 128),\r\n\r\n            nn.Conv2d(128, 256, 4, 2, 1), # 115\r\n            nn.ReLU(inplace=True),\r\n\r\n            ResidualBlock(256, 256),\r\n\r\n            nn.Conv2d(256, 256, 3, 1, 1), #192\r\n            nn.ReLU(inplace=True),\r\n\r\n            ResidualBlock(256, 256),\r\n\r\n            nn.Conv2d(256, 64, 1, 1, 0),    # conv 4  64 is n\r\n            nn.Sigmoid(),                    \r\n        ]\r\n        return nn.Sequential(*layers)\r\n\r\n    def make_decoder(self):\r\n        layers = [\r\n            nn.Conv2d(64, 512, 3, 1, 1),\r\n            nn.ReLU(inplace=True),\r\n\r\n            ResidualBlock(512, 512),\r\n\r\n            nn.Conv2d(512, 512, 3, 1, 1),\r\n            nn.ReLU(inplace=True),\r\n\r\n            ResidualBlock(512, 512),\r\n\r\n            nn.PixelShuffle(2),\r\n\r\n            nn.Conv2d(128, 256, 3, 1, 1),\r\n            nn.ReLU(inplace=True),\r\n\r\n            ResidualBlock(256, 256),\r\n\r\n            nn.PixelShuffle(4),\r\n\r\n            nn.Conv2d(16, 32, 3, 1, 1),\r\n            nn.ReLU(inplace=True),\r\n            \r\n            nn.Conv2d(32, 3, 1, 1, 0)   \r\n        ]\r\n        return nn.Sequential(*layers)\r\n \r\n            \r\n```"}