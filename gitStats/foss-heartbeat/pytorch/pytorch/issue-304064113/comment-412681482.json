{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/412681482", "html_url": "https://github.com/pytorch/pytorch/issues/5687#issuecomment-412681482", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5687", "id": 412681482, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjY4MTQ4Mg==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-13T22:07:02Z", "updated_at": "2018-08-13T22:07:02Z", "author_association": "MEMBER", "body_html": "<p>Yes, it's fixed. You can't do an inplace-addition after an inplace relu:</p>\n<pre><code>import torch\nrelu = torch.nn.ReLU(inplace=True)\nx = torch.randn(1024, requires_grad=True)\nout = x.clone()\nout = relu(out)\nout += x\nout.sum().backward()\n</code></pre>\n<pre><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n</code></pre>\n<p>The code in ResNet is OK because the order is swapped (addition first). Addition doesn't require saving inputs or outputs to compute the gradient. ReLU requires saving the input.</p>", "body_text": "Yes, it's fixed. You can't do an inplace-addition after an inplace relu:\nimport torch\nrelu = torch.nn.ReLU(inplace=True)\nx = torch.randn(1024, requires_grad=True)\nout = x.clone()\nout = relu(out)\nout += x\nout.sum().backward()\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n\nThe code in ResNet is OK because the order is swapped (addition first). Addition doesn't require saving inputs or outputs to compute the gradient. ReLU requires saving the input.", "body": "Yes, it's fixed. You can't do an inplace-addition after an inplace relu:\r\n\r\n```\r\nimport torch\r\nrelu = torch.nn.ReLU(inplace=True)\r\nx = torch.randn(1024, requires_grad=True)\r\nout = x.clone()\r\nout = relu(out)\r\nout += x\r\nout.sum().backward()\r\n```\r\n```\r\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\r\n```\r\n\r\nThe code in ResNet is OK because the order is swapped (addition first). Addition doesn't require saving inputs or outputs to compute the gradient. ReLU requires saving the input."}