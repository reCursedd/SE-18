{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3320", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3320/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3320/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3320/events", "html_url": "https://github.com/pytorch/pytorch/issues/3320", "id": 269062665, "node_id": "MDU6SXNzdWUyNjkwNjI2NjU=", "number": 3320, "title": "A possible bug in `binary_cross_entropy`.", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-27T10:37:01Z", "updated_at": "2017-10-27T10:57:13Z", "closed_at": "2017-10-27T10:43:21Z", "author_association": "NONE", "body_html": "<p>After upgrading to 0.2 I am facing an issue with using <code>binary_cross_entropy</code>.</p>\n<p>I tried to run the example code for using <code>binary_cross_entropy</code> in the documentation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> autograd.Variable(torch.randn(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ntarget <span class=\"pl-k\">=</span> autograd.Variable(torch.LongTensor(<span class=\"pl-c1\">3</span>).random_(<span class=\"pl-c1\">2</span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Error happens here</span>\nloss <span class=\"pl-k\">=</span> F.binary_cross_entropy(F.sigmoid(<span class=\"pl-c1\">input</span>), target)\nloss.backward()</pre></div>\n<p>but I get the following run-time error:</p>\n<pre><code>*** TypeError: is_same_size received an invalid combination of arguments - got (torch.FloatTensor), but expected (torch.LongTensor other)\n</code></pre>\n<p>It seems that <code>binary_cross_entropy</code> checks whether the predicted sigmoid values are <code>long</code> - should that be the case ?</p>", "body_text": "After upgrading to 0.2 I am facing an issue with using binary_cross_entropy.\nI tried to run the example code for using binary_cross_entropy in the documentation:\ninput = autograd.Variable(torch.randn(3), requires_grad=True)\ntarget = autograd.Variable(torch.LongTensor(3).random_(2))\n\n# Error happens here\nloss = F.binary_cross_entropy(F.sigmoid(input), target)\nloss.backward()\nbut I get the following run-time error:\n*** TypeError: is_same_size received an invalid combination of arguments - got (torch.FloatTensor), but expected (torch.LongTensor other)\n\nIt seems that binary_cross_entropy checks whether the predicted sigmoid values are long - should that be the case ?", "body": "After upgrading to 0.2 I am facing an issue with using `binary_cross_entropy`.\r\n\r\nI tried to run the example code for using `binary_cross_entropy` in the documentation:\r\n```python\r\ninput = autograd.Variable(torch.randn(3), requires_grad=True)\r\ntarget = autograd.Variable(torch.LongTensor(3).random_(2))\r\n\r\n# Error happens here\r\nloss = F.binary_cross_entropy(F.sigmoid(input), target)\r\nloss.backward()\r\n```\r\nbut I get the following run-time error:\r\n```\r\n*** TypeError: is_same_size received an invalid combination of arguments - got (torch.FloatTensor), but expected (torch.LongTensor other)\r\n```\r\nIt seems that `binary_cross_entropy` checks whether the predicted sigmoid values are `long` - should that be the case ?\r\n"}