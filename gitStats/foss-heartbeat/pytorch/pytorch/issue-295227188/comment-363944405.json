{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/363944405", "html_url": "https://github.com/pytorch/pytorch/issues/5112#issuecomment-363944405", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5112", "id": 363944405, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mzk0NDQwNQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-07T23:16:44Z", "updated_at": "2018-02-08T00:44:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Bisected any failure of test script to <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/a88a8ec8278e19f52cfd6e75a685ce0e9200b96b/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/a88a8ec8278e19f52cfd6e75a685ce0e9200b96b\"><tt>a88a8ec</tt></a></p>\n<pre><code>commit a88a8ec8278e19f52cfd6e75a685ce0e9200b96b\nAuthor: Edward Z. Yang &lt;ezyang@mit.edu&gt;\nDate:   Wed Dec 20 14:19:27 2017 -0500\n\n    Convolution derivatives in ATen (#4116)\n    \n    * Convolution derivatives in ATen\n    \n    This PR introduces ATen implementation of convolution, which dispatches to\n    THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose\n    this function out of the various forward-backward pairs of specific\n    implementations, rather than write a monolithic function with backwards (which\n    is what we did before because the boilerplate of doing it otherwise would have\n    been very high.) The new API provides the following functions:\n    \n      - _convolution, which is a fully generic, native convolution implementation\n        that dispatches to various other convolution implementations depending on\n        input characteristics. This is prefixed with an underscore because it\n        explicitly takes benchmark, deterministic and cudnn_enabled which are\n        implementation details for CuDNN. The intent is to eventually provide a\n        convolution that reads these parameters out of the context using #4104.\n      - _convolution_nogroup is a convolution implementation for non-CuDNN\n        algorithms which don't support group convolution natively.\n      - _convolution_double_backward is the generic double-backwards implementation\n        for convolution.\n</code></pre>\n<p>Bisect command: <code>git bisect run sh -c \"env DEBUG=1 NO_CUDA=1 NO_DISTRIBUTED=1 python setup.py build_deps develop &amp;&amp; python corrupt.py\"</code></p>\n<p>The failure is different, however:</p>\n<pre><code>Traceback (most recent call last):\n  File \"corrupt.py\", line 51, in &lt;module&gt;\n    Rop(ds, xs, vs)\n  File \"corrupt.py\", line 20, in Rop\n    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True)\n  File \"/data/users/ezyang/pytorch/torch/autograd/__init__.py\", line 142, in grad\n    inputs, only_inputs, allow_unused)\nRuntimeError: Given groups=1, weight[2, 1, 6, 6], so expected input[1, 2, 3, 3] to have 1 channels, but got 2 channels instead\n</code></pre>", "body_text": "Bisected any failure of test script to a88a8ec\ncommit a88a8ec8278e19f52cfd6e75a685ce0e9200b96b\nAuthor: Edward Z. Yang <ezyang@mit.edu>\nDate:   Wed Dec 20 14:19:27 2017 -0500\n\n    Convolution derivatives in ATen (#4116)\n    \n    * Convolution derivatives in ATen\n    \n    This PR introduces ATen implementation of convolution, which dispatches to\n    THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose\n    this function out of the various forward-backward pairs of specific\n    implementations, rather than write a monolithic function with backwards (which\n    is what we did before because the boilerplate of doing it otherwise would have\n    been very high.) The new API provides the following functions:\n    \n      - _convolution, which is a fully generic, native convolution implementation\n        that dispatches to various other convolution implementations depending on\n        input characteristics. This is prefixed with an underscore because it\n        explicitly takes benchmark, deterministic and cudnn_enabled which are\n        implementation details for CuDNN. The intent is to eventually provide a\n        convolution that reads these parameters out of the context using #4104.\n      - _convolution_nogroup is a convolution implementation for non-CuDNN\n        algorithms which don't support group convolution natively.\n      - _convolution_double_backward is the generic double-backwards implementation\n        for convolution.\n\nBisect command: git bisect run sh -c \"env DEBUG=1 NO_CUDA=1 NO_DISTRIBUTED=1 python setup.py build_deps develop && python corrupt.py\"\nThe failure is different, however:\nTraceback (most recent call last):\n  File \"corrupt.py\", line 51, in <module>\n    Rop(ds, xs, vs)\n  File \"corrupt.py\", line 20, in Rop\n    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True)\n  File \"/data/users/ezyang/pytorch/torch/autograd/__init__.py\", line 142, in grad\n    inputs, only_inputs, allow_unused)\nRuntimeError: Given groups=1, weight[2, 1, 6, 6], so expected input[1, 2, 3, 3] to have 1 channels, but got 2 channels instead", "body": "Bisected any failure of test script to a88a8ec8278e19f52cfd6e75a685ce0e9200b96b\r\n\r\n```\r\ncommit a88a8ec8278e19f52cfd6e75a685ce0e9200b96b\r\nAuthor: Edward Z. Yang <ezyang@mit.edu>\r\nDate:   Wed Dec 20 14:19:27 2017 -0500\r\n\r\n    Convolution derivatives in ATen (#4116)\r\n    \r\n    * Convolution derivatives in ATen\r\n    \r\n    This PR introduces ATen implementation of convolution, which dispatches to\r\n    THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose\r\n    this function out of the various forward-backward pairs of specific\r\n    implementations, rather than write a monolithic function with backwards (which\r\n    is what we did before because the boilerplate of doing it otherwise would have\r\n    been very high.) The new API provides the following functions:\r\n    \r\n      - _convolution, which is a fully generic, native convolution implementation\r\n        that dispatches to various other convolution implementations depending on\r\n        input characteristics. This is prefixed with an underscore because it\r\n        explicitly takes benchmark, deterministic and cudnn_enabled which are\r\n        implementation details for CuDNN. The intent is to eventually provide a\r\n        convolution that reads these parameters out of the context using #4104.\r\n      - _convolution_nogroup is a convolution implementation for non-CuDNN\r\n        algorithms which don't support group convolution natively.\r\n      - _convolution_double_backward is the generic double-backwards implementation\r\n        for convolution.\r\n```\r\n\r\nBisect command: `git bisect run sh -c \"env DEBUG=1 NO_CUDA=1 NO_DISTRIBUTED=1 python setup.py build_deps develop && python corrupt.py\"`\r\n\r\nThe failure is different, however:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"corrupt.py\", line 51, in <module>\r\n    Rop(ds, xs, vs)\r\n  File \"corrupt.py\", line 20, in Rop\r\n    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True)\r\n  File \"/data/users/ezyang/pytorch/torch/autograd/__init__.py\", line 142, in grad\r\n    inputs, only_inputs, allow_unused)\r\nRuntimeError: Given groups=1, weight[2, 1, 6, 6], so expected input[1, 2, 3, 3] to have 1 channels, but got 2 channels instead\r\n```"}