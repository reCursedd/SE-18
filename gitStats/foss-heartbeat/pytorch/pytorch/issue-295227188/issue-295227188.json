{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5112", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5112/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5112/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5112/events", "html_url": "https://github.com/pytorch/pytorch/issues/5112", "id": 295227188, "node_id": "MDU6SXNzdWUyOTUyMjcxODg=", "number": 5112, "title": "ConvTranspose backwards with dilation and output padding segfaults", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/crash", "name": "crash", "color": "d93f0b", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-02-07T17:33:16Z", "updated_at": "2018-02-14T17:39:02Z", "closed_at": "2018-02-14T17:39:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>UPDATE: ASAN reports a problem with the following test case:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nmodel = nn.ConvTranspose2d(4, 1, 3, output_padding=1, dilation=2, bias=False)\nmodel.weight.data.fill_(1)\ni = Variable(torch.ones(1, 4, 1, 1))\no = model(i)\no.sum().backward()\n</code></pre>\n<p>(So, it looks like we're not checking <code>output_padding &lt; stride</code>.)</p>\n<hr>\n<p>The following program reliably segfaults for me:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nbatch_size = 1\nx = Variable(torch.randn(batch_size, 3, 32, 32))\n\ndef Lop(ys, xs, ws):\n    return torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\n\ndef Rop(ys, xs, vs):\n    if isinstance(ys, tuple):\n        ws = [Variable(torch.zeros(y.size()), requires_grad=True) for y in ys]\n    else:\n        ws = Variable(torch.zeros(ys.size()), requires_grad=True)\n\n    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\n    re = torch.autograd.grad(gs, ws, grad_outputs=vs, create_graph=True, retain_graph=True, allow_unused=True)\n    return re\n\nclass Test(nn.Module):\n    def __init__(self):\n        super(Test, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n        self.conv2 = nn.Conv2d(16, 160, kernel_size=3)\n        self.conv3 = nn.Conv2d(160, 160, kernel_size=1)\n        self.conv4 = nn.Conv2d(16, 160, kernel_size=3)\n        self.conv5 = nn.Conv2d(160, 160, kernel_size=1)\n        self.conv6 = nn.Conv2d(160, 160, kernel_size=1)\n        self.conv7 = nn.Conv2d(160, 320, kernel_size=1, stride=2)\n        self.conv8 = nn.Conv2d(320, 320, kernel_size=1)\n        self.conv9 = nn.Conv2d(160, 320, kernel_size=1, stride=2)\n        self.linear = nn.Linear(62720, 10, bias=False)\n    def forward(self, t0):\n        t53 = self.conv1(t0)\n        t63 = self.conv2(t53)\n        t73 = self.conv3(t63)\n        t77 = self.conv4(t53)\n        t78 = t77 + t73\n\n        t88 = self.conv5(t78)\n        t98 = self.conv6(t88)\n        t99 = t78 + t98\n\n        t109 = self.conv7(t99)\n        t119 = self.conv8(t109)\n        t123 = self.conv9(t99)\n        t124 = t123 + t119\n\n        x = t124\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n\nm = Test()\ny = m(x)\nl = y.sum()\n\nxs = list(m.parameters())\nds = torch.autograd.grad(l, xs, create_graph=True, retain_graph=True)\n\nvs = [Variable(torch.randn(g.size())) for g in ds]\nRop(ds, xs, vs)\n\"\"\"\n# Correctness check\nfor r1, r2 in zip(Lop(ds, xs, vs), Rop(ds, xs, vs)):\n    print((r2-r1).abs().sum())\n\"\"\"\n</code></pre>\n<p>Looks like an engine problem.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>\n<p>EDIT: updated with smaller repro<br>\nEDIT 2: restored original repro, it seems to trigger problems more reliably</p>", "body_text": "UPDATE: ASAN reports a problem with the following test case:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nmodel = nn.ConvTranspose2d(4, 1, 3, output_padding=1, dilation=2, bias=False)\nmodel.weight.data.fill_(1)\ni = Variable(torch.ones(1, 4, 1, 1))\no = model(i)\no.sum().backward()\n\n(So, it looks like we're not checking output_padding < stride.)\n\nThe following program reliably segfaults for me:\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nbatch_size = 1\nx = Variable(torch.randn(batch_size, 3, 32, 32))\n\ndef Lop(ys, xs, ws):\n    return torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\n\ndef Rop(ys, xs, vs):\n    if isinstance(ys, tuple):\n        ws = [Variable(torch.zeros(y.size()), requires_grad=True) for y in ys]\n    else:\n        ws = Variable(torch.zeros(ys.size()), requires_grad=True)\n\n    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\n    re = torch.autograd.grad(gs, ws, grad_outputs=vs, create_graph=True, retain_graph=True, allow_unused=True)\n    return re\n\nclass Test(nn.Module):\n    def __init__(self):\n        super(Test, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n        self.conv2 = nn.Conv2d(16, 160, kernel_size=3)\n        self.conv3 = nn.Conv2d(160, 160, kernel_size=1)\n        self.conv4 = nn.Conv2d(16, 160, kernel_size=3)\n        self.conv5 = nn.Conv2d(160, 160, kernel_size=1)\n        self.conv6 = nn.Conv2d(160, 160, kernel_size=1)\n        self.conv7 = nn.Conv2d(160, 320, kernel_size=1, stride=2)\n        self.conv8 = nn.Conv2d(320, 320, kernel_size=1)\n        self.conv9 = nn.Conv2d(160, 320, kernel_size=1, stride=2)\n        self.linear = nn.Linear(62720, 10, bias=False)\n    def forward(self, t0):\n        t53 = self.conv1(t0)\n        t63 = self.conv2(t53)\n        t73 = self.conv3(t63)\n        t77 = self.conv4(t53)\n        t78 = t77 + t73\n\n        t88 = self.conv5(t78)\n        t98 = self.conv6(t88)\n        t99 = t78 + t98\n\n        t109 = self.conv7(t99)\n        t119 = self.conv8(t109)\n        t123 = self.conv9(t99)\n        t124 = t123 + t119\n\n        x = t124\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n\nm = Test()\ny = m(x)\nl = y.sum()\n\nxs = list(m.parameters())\nds = torch.autograd.grad(l, xs, create_graph=True, retain_graph=True)\n\nvs = [Variable(torch.randn(g.size())) for g in ds]\nRop(ds, xs, vs)\n\"\"\"\n# Correctness check\nfor r1, r2 in zip(Lop(ds, xs, vs), Rop(ds, xs, vs)):\n    print((r2-r1).abs().sum())\n\"\"\"\n\nLooks like an engine problem.\nCC @apaszke @colesbury\nEDIT: updated with smaller repro\nEDIT 2: restored original repro, it seems to trigger problems more reliably", "body": "UPDATE: ASAN reports a problem with the following test case:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nmodel = nn.ConvTranspose2d(4, 1, 3, output_padding=1, dilation=2, bias=False)\r\nmodel.weight.data.fill_(1)\r\ni = Variable(torch.ones(1, 4, 1, 1))\r\no = model(i)\r\no.sum().backward()\r\n```\r\n\r\n(So, it looks like we're not checking `output_padding < stride`.)\r\n\r\n----\r\n\r\nThe following program reliably segfaults for me:\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nbatch_size = 1\r\nx = Variable(torch.randn(batch_size, 3, 32, 32))\r\n\r\ndef Lop(ys, xs, ws):\r\n    return torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\r\n\r\ndef Rop(ys, xs, vs):\r\n    if isinstance(ys, tuple):\r\n        ws = [Variable(torch.zeros(y.size()), requires_grad=True) for y in ys]\r\n    else:\r\n        ws = Variable(torch.zeros(ys.size()), requires_grad=True)\r\n\r\n    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\r\n    re = torch.autograd.grad(gs, ws, grad_outputs=vs, create_graph=True, retain_graph=True, allow_unused=True)\r\n    return re\r\n\r\nclass Test(nn.Module):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\r\n        self.conv2 = nn.Conv2d(16, 160, kernel_size=3)\r\n        self.conv3 = nn.Conv2d(160, 160, kernel_size=1)\r\n        self.conv4 = nn.Conv2d(16, 160, kernel_size=3)\r\n        self.conv5 = nn.Conv2d(160, 160, kernel_size=1)\r\n        self.conv6 = nn.Conv2d(160, 160, kernel_size=1)\r\n        self.conv7 = nn.Conv2d(160, 320, kernel_size=1, stride=2)\r\n        self.conv8 = nn.Conv2d(320, 320, kernel_size=1)\r\n        self.conv9 = nn.Conv2d(160, 320, kernel_size=1, stride=2)\r\n        self.linear = nn.Linear(62720, 10, bias=False)\r\n    def forward(self, t0):\r\n        t53 = self.conv1(t0)\r\n        t63 = self.conv2(t53)\r\n        t73 = self.conv3(t63)\r\n        t77 = self.conv4(t53)\r\n        t78 = t77 + t73\r\n\r\n        t88 = self.conv5(t78)\r\n        t98 = self.conv6(t88)\r\n        t99 = t78 + t98\r\n\r\n        t109 = self.conv7(t99)\r\n        t119 = self.conv8(t109)\r\n        t123 = self.conv9(t99)\r\n        t124 = t123 + t119\r\n\r\n        x = t124\r\n        x = x.view(x.size(0), -1)\r\n        x = self.linear(x)\r\n        return x\r\n\r\nm = Test()\r\ny = m(x)\r\nl = y.sum()\r\n\r\nxs = list(m.parameters())\r\nds = torch.autograd.grad(l, xs, create_graph=True, retain_graph=True)\r\n\r\nvs = [Variable(torch.randn(g.size())) for g in ds]\r\nRop(ds, xs, vs)\r\n\"\"\"\r\n# Correctness check\r\nfor r1, r2 in zip(Lop(ds, xs, vs), Rop(ds, xs, vs)):\r\n    print((r2-r1).abs().sum())\r\n\"\"\"\r\n```\r\n\r\nLooks like an engine problem.\r\n\r\nCC @apaszke @colesbury \r\n\r\nEDIT: updated with smaller repro\r\nEDIT 2: restored original repro, it seems to trigger problems more reliably"}