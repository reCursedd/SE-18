{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391095179", "html_url": "https://github.com/pytorch/pytorch/issues/7716#issuecomment-391095179", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7716", "id": 391095179, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTA5NTE3OQ==", "user": {"login": "jeasinema", "id": 10633528, "node_id": "MDQ6VXNlcjEwNjMzNTI4", "avatar_url": "https://avatars3.githubusercontent.com/u/10633528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeasinema", "html_url": "https://github.com/jeasinema", "followers_url": "https://api.github.com/users/jeasinema/followers", "following_url": "https://api.github.com/users/jeasinema/following{/other_user}", "gists_url": "https://api.github.com/users/jeasinema/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeasinema/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeasinema/subscriptions", "organizations_url": "https://api.github.com/users/jeasinema/orgs", "repos_url": "https://api.github.com/users/jeasinema/repos", "events_url": "https://api.github.com/users/jeasinema/events{/privacy}", "received_events_url": "https://api.github.com/users/jeasinema/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-22T18:32:18Z", "updated_at": "2018-05-22T18:34:40Z", "author_association": "NONE", "body_html": "<p>Notice: before the content below, I need to clarify that the reason why in *norm we have to make sure that there must be &gt;1 value per channel is we use an unbiased estimation of the variance(to be precise, it is to compute a <code>running_variance</code> on the training set for inference) <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" rel=\"nofollow\">ref</a>(just look at alg.2).  I believe that the batchnorm implementations in all the backends of PyTorch are using this unbiased estimation. So it will be unavoidable to produce such error. However, in some other frameworks(tensorflow and mxnet), they use an biased estimation of variance, and the <code>std</code> for only one sample will be <code>0</code> instead of <code>nan</code>. Thus their batchnorm can accept tensor with only 1 value per channel.</p>\n<p>When I try to find some correct words for the doc, I do a quick check on the implementation for batchnorm  of 4 mainstreams DL software stacks including theano, tensorflow, mxnet and pytorch itself.  Here are what I found:</p>\n<ul>\n<li>\n<p>pytorch: the implementation on cpu(THNN) and gpu(THCUNN) can be found <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L55\">here</a> and <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/BatchNormalization.cu#L197\">here</a>. I also find an <code>cuDNN</code> backend <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cudnn/BatchNorm.cpp\">here</a>. For the first 2 implementations, it is obvious that they use an unbiased estimation. I didn't check the <code>cuDNN</code> backend since its source code is unavailable.</p>\n</li>\n<li>\n<p>theano: cpu and gpu implementations can be found <a href=\"https://github.com/Theano/theano/blob/d395439aec5a6ddde8ef5c266fd976412a5c5695/theano/tensor/nnet/bn.py#L499\">here</a> and <a href=\"https://github.com/Theano/theano/blob/d395439aec5a6ddde8ef5c266fd976412a5c5695/theano/gpuarray/dnn.py#L1774\">here</a> (only pseudo code for the later one). It's also obvious that both the implementations use an unbiased estimation.</p>\n</li>\n<li>\n<p>tensorflow and mxnet: Their source code can be hard to read. So I just do some experiments. The test code for tensorflow and mxnet can be found <a href=\"https://paste.ubuntu.com/p/S8hyTkjVgh/\" rel=\"nofollow\">here</a> and <a href=\"https://paste.ubuntu.com/p/7kVHSKbWC2/\" rel=\"nofollow\">here</a>. You can just replace the input with a tensor only have 1 value per channel(channel is the last dimension). And run the code under the latest version(tensorflow 1.8.0 and mxnet 1.1.0). Batchnorm in both tensorflow and mxnet can accept tensor with only 1 val per channel.</p>\n</li>\n</ul>\n<p>I also provide a test code for pytorch <a href=\"https://paste.ubuntu.com/p/dPTD33ShZx/\" rel=\"nofollow\">here</a>. The init val of running_mean and running_var are 0 and 1 respectively in all the 3 framework, and the momentum is all fixed at 0.9(or 0.1=1-0.9 for pytorch).  It can be seen that tensorflow and mxnet will produce almost the same results, and they both use the biased estimation of variance(1*0.9 + (((1-1.5)**2 + (2-1.5)**2)/2)*0.1 = 0.925), while pytorch uses the unbiased estimation and produce different results(1*0.9 + (((1-1.5)**2 + (2-1.5)**2)/(2-1))*0.1 = 0.95).</p>\n<p>Indeed, I'm quite surprised on this. Since different framework have different implementations even with batchnorm.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> Do you have any idea on it? I wonder if it is needed to add an extra note on the doc to clarify the possible difference in the behaviours of batchnorm.</p>", "body_text": "Notice: before the content below, I need to clarify that the reason why in *norm we have to make sure that there must be >1 value per channel is we use an unbiased estimation of the variance(to be precise, it is to compute a running_variance on the training set for inference) ref(just look at alg.2).  I believe that the batchnorm implementations in all the backends of PyTorch are using this unbiased estimation. So it will be unavoidable to produce such error. However, in some other frameworks(tensorflow and mxnet), they use an biased estimation of variance, and the std for only one sample will be 0 instead of nan. Thus their batchnorm can accept tensor with only 1 value per channel.\nWhen I try to find some correct words for the doc, I do a quick check on the implementation for batchnorm  of 4 mainstreams DL software stacks including theano, tensorflow, mxnet and pytorch itself.  Here are what I found:\n\n\npytorch: the implementation on cpu(THNN) and gpu(THCUNN) can be found here and here. I also find an cuDNN backend here. For the first 2 implementations, it is obvious that they use an unbiased estimation. I didn't check the cuDNN backend since its source code is unavailable.\n\n\ntheano: cpu and gpu implementations can be found here and here (only pseudo code for the later one). It's also obvious that both the implementations use an unbiased estimation.\n\n\ntensorflow and mxnet: Their source code can be hard to read. So I just do some experiments. The test code for tensorflow and mxnet can be found here and here. You can just replace the input with a tensor only have 1 value per channel(channel is the last dimension). And run the code under the latest version(tensorflow 1.8.0 and mxnet 1.1.0). Batchnorm in both tensorflow and mxnet can accept tensor with only 1 val per channel.\n\n\nI also provide a test code for pytorch here. The init val of running_mean and running_var are 0 and 1 respectively in all the 3 framework, and the momentum is all fixed at 0.9(or 0.1=1-0.9 for pytorch).  It can be seen that tensorflow and mxnet will produce almost the same results, and they both use the biased estimation of variance(1*0.9 + (((1-1.5)**2 + (2-1.5)**2)/2)*0.1 = 0.925), while pytorch uses the unbiased estimation and produce different results(1*0.9 + (((1-1.5)**2 + (2-1.5)**2)/(2-1))*0.1 = 0.95).\nIndeed, I'm quite surprised on this. Since different framework have different implementations even with batchnorm.\n@t-vi Do you have any idea on it? I wonder if it is needed to add an extra note on the doc to clarify the possible difference in the behaviours of batchnorm.", "body": "Notice: before the content below, I need to clarify that the reason why in *norm we have to make sure that there must be >1 value per channel is we use an unbiased estimation of the variance(to be precise, it is to compute a `running_variance` on the training set for inference) [ref](https://arxiv.org/pdf/1502.03167.pdf)(just look at alg.2).  I believe that the batchnorm implementations in all the backends of PyTorch are using this unbiased estimation. So it will be unavoidable to produce such error. However, in some other frameworks(tensorflow and mxnet), they use an biased estimation of variance, and the `std` for only one sample will be `0` instead of `nan`. Thus their batchnorm can accept tensor with only 1 value per channel.\r\n\r\nWhen I try to find some correct words for the doc, I do a quick check on the implementation for batchnorm  of 4 mainstreams DL software stacks including theano, tensorflow, mxnet and pytorch itself.  Here are what I found:\r\n\r\n- pytorch: the implementation on cpu(THNN) and gpu(THCUNN) can be found [here](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L55) and [here](https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/BatchNormalization.cu#L197). I also find an `cuDNN` backend [here](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cudnn/BatchNorm.cpp). For the first 2 implementations, it is obvious that they use an unbiased estimation. I didn't check the `cuDNN` backend since its source code is unavailable. \r\n\r\n- theano: cpu and gpu implementations can be found [here](https://github.com/Theano/theano/blob/d395439aec5a6ddde8ef5c266fd976412a5c5695/theano/tensor/nnet/bn.py#L499) and [here](https://github.com/Theano/theano/blob/d395439aec5a6ddde8ef5c266fd976412a5c5695/theano/gpuarray/dnn.py#L1774) (only pseudo code for the later one). It's also obvious that both the implementations use an unbiased estimation.\r\n\r\n- tensorflow and mxnet: Their source code can be hard to read. So I just do some experiments. The test code for tensorflow and mxnet can be found [here](https://paste.ubuntu.com/p/S8hyTkjVgh/) and [here](https://paste.ubuntu.com/p/7kVHSKbWC2/). You can just replace the input with a tensor only have 1 value per channel(channel is the last dimension). And run the code under the latest version(tensorflow 1.8.0 and mxnet 1.1.0). Batchnorm in both tensorflow and mxnet can accept tensor with only 1 val per channel. \r\n\r\nI also provide a test code for pytorch [here](https://paste.ubuntu.com/p/dPTD33ShZx/). The init val of running_mean and running_var are 0 and 1 respectively in all the 3 framework, and the momentum is all fixed at 0.9(or 0.1=1-0.9 for pytorch).  It can be seen that tensorflow and mxnet will produce almost the same results, and they both use the biased estimation of variance(1\\*0.9 + (((1-1.5)\\*\\*2 + (2-1.5)\\*\\*2)/2)\\*0.1 = 0.925), while pytorch uses the unbiased estimation and produce different results(1\\*0.9 + (((1-1.5)\\*\\*2 + (2-1.5)\\*\\*2)/(2-1))\\*0.1 = 0.95).\r\n\r\nIndeed, I'm quite surprised on this. Since different framework have different implementations even with batchnorm. \r\n\r\n@t-vi Do you have any idea on it? I wonder if it is needed to add an extra note on the doc to clarify the possible difference in the behaviours of batchnorm. "}