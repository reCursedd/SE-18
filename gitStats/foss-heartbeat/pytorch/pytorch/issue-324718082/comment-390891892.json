{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390891892", "html_url": "https://github.com/pytorch/pytorch/issues/7716#issuecomment-390891892", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7716", "id": 390891892, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDg5MTg5Mg==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-22T07:34:03Z", "updated_at": "2018-05-22T07:34:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10633528\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jeasinema\">@jeasinema</a> The doc changes will be reviewed once you open a PR, so no worries there (my impression documentation you should not get too \"tutorial-style\", but other than that it seems easy to blend in the usual style of the docs).<br>\nIf you wanted to keep nn.DataParallel from getting batch 1, you would probably need to add an option \"minimal batch size per GPU\" and dig through the functions doing the distribution to ensure that that. That might be a change where it is prudent to first open an issue and get the core dev's input, as it may or may not be the right solution.<br>\nI must admit I'm do not know how *norm is currently works or would ideally work in the context of parallelism, it might be that you want to aggregate statistics similar to how gradients are collected.</p>", "body_text": "@jeasinema The doc changes will be reviewed once you open a PR, so no worries there (my impression documentation you should not get too \"tutorial-style\", but other than that it seems easy to blend in the usual style of the docs).\nIf you wanted to keep nn.DataParallel from getting batch 1, you would probably need to add an option \"minimal batch size per GPU\" and dig through the functions doing the distribution to ensure that that. That might be a change where it is prudent to first open an issue and get the core dev's input, as it may or may not be the right solution.\nI must admit I'm do not know how *norm is currently works or would ideally work in the context of parallelism, it might be that you want to aggregate statistics similar to how gradients are collected.", "body": "@jeasinema The doc changes will be reviewed once you open a PR, so no worries there (my impression documentation you should not get too \"tutorial-style\", but other than that it seems easy to blend in the usual style of the docs).\r\nIf you wanted to keep nn.DataParallel from getting batch 1, you would probably need to add an option \"minimal batch size per GPU\" and dig through the functions doing the distribution to ensure that that. That might be a change where it is prudent to first open an issue and get the core dev's input, as it may or may not be the right solution.\r\nI must admit I'm do not know how *norm is currently works or would ideally work in the context of parallelism, it might be that you want to aggregate statistics similar to how gradients are collected."}