{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391259623", "html_url": "https://github.com/pytorch/pytorch/issues/7716#issuecomment-391259623", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7716", "id": 391259623, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTI1OTYyMw==", "user": {"login": "jeasinema", "id": 10633528, "node_id": "MDQ6VXNlcjEwNjMzNTI4", "avatar_url": "https://avatars3.githubusercontent.com/u/10633528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeasinema", "html_url": "https://github.com/jeasinema", "followers_url": "https://api.github.com/users/jeasinema/followers", "following_url": "https://api.github.com/users/jeasinema/following{/other_user}", "gists_url": "https://api.github.com/users/jeasinema/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeasinema/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeasinema/subscriptions", "organizations_url": "https://api.github.com/users/jeasinema/orgs", "repos_url": "https://api.github.com/users/jeasinema/repos", "events_url": "https://api.github.com/users/jeasinema/events{/privacy}", "received_events_url": "https://api.github.com/users/jeasinema/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T08:10:39Z", "updated_at": "2018-05-23T08:10:39Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I have some different opinions:</p>\n<ol>\n<li>Since we have \\epsilon in the denominator, those functions will produce 0 instead of infinity.</li>\n<li>I argree that we shouldn't waste our time on whether batchnorm should work for single input in PyTorch. But as currently there is no support for distributed batchnorm in PyTorch, I think it is necessary to cope with this issue under parallelism. Imagine that we have a model which contains a global pooling operarions, and it produce a <code>N*C*1</code> tensor. And we use <code>nn.DataParallel</code> at the same time with unbalanced GPUs. Under such circumstance something weird may happen becasue PyTorch may assign a minibatch with size 1 to a single GPU.</li>\n</ol>", "body_text": "@apaszke I have some different opinions:\n\nSince we have \\epsilon in the denominator, those functions will produce 0 instead of infinity.\nI argree that we shouldn't waste our time on whether batchnorm should work for single input in PyTorch. But as currently there is no support for distributed batchnorm in PyTorch, I think it is necessary to cope with this issue under parallelism. Imagine that we have a model which contains a global pooling operarions, and it produce a N*C*1 tensor. And we use nn.DataParallel at the same time with unbalanced GPUs. Under such circumstance something weird may happen becasue PyTorch may assign a minibatch with size 1 to a single GPU.", "body": "@apaszke I have some different opinions:\r\n1. Since we have \\epsilon in the denominator, those functions will produce 0 instead of infinity.\r\n2. I argree that we shouldn't waste our time on whether batchnorm should work for single input in PyTorch. But as currently there is no support for distributed batchnorm in PyTorch, I think it is necessary to cope with this issue under parallelism. Imagine that we have a model which contains a global pooling operarions, and it produce a `N*C*1` tensor. And we use `nn.DataParallel` at the same time with unbalanced GPUs. Under such circumstance something weird may happen becasue PyTorch may assign a minibatch with size 1 to a single GPU."}