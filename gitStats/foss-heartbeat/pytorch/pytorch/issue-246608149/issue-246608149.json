{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2257", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2257/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2257/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2257/events", "html_url": "https://github.com/pytorch/pytorch/issues/2257", "id": 246608149, "node_id": "MDU6SXNzdWUyNDY2MDgxNDk=", "number": 2257, "title": "THCudaCheck FAIL happens inconsistently. ", "user": {"login": "blukee", "id": 816311, "node_id": "MDQ6VXNlcjgxNjMxMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/816311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blukee", "html_url": "https://github.com/blukee", "followers_url": "https://api.github.com/users/blukee/followers", "following_url": "https://api.github.com/users/blukee/following{/other_user}", "gists_url": "https://api.github.com/users/blukee/gists{/gist_id}", "starred_url": "https://api.github.com/users/blukee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blukee/subscriptions", "organizations_url": "https://api.github.com/users/blukee/orgs", "repos_url": "https://api.github.com/users/blukee/repos", "events_url": "https://api.github.com/users/blukee/events{/privacy}", "received_events_url": "https://api.github.com/users/blukee/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-30T20:51:32Z", "updated_at": "2017-08-09T14:53:50Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Basic network block:<br>\n(0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))<br>\n(1): LeakyReLU (0.333, inplace)<br>\n(2): FractionalMaxPool2d (<br>\n)</p>\n<p>The following stack traces occur very 1-2 epochs. I randomly pools inputs set to fit memory.</p>\n<p>Some time it throws the following stack trace:<br>\n<code>THCudaCheck FAIL file=/home/ubuntu/src/pytorch/torch/lib/THCUNN/im2col.h line=60 error=59 : device-side assert triggered Traceback (most recent call last): File \"train.py\", line 502, in &lt;module&gt; best_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200) File \"train.py\", line 184, in train_model outputs = model(inputs) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/home/ubuntu/projects/tutorial-pytorch/model_zoo.py\", line 30, in forward x = self.features(x) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.py\", line 67, in forward input = module(input) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.py\", line 237, in forward self.padding, self.dilation, self.groups) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/functional.py\", line 43, in conv2d return f(input, weight, bias) RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/ubuntu/src/pytorch/torch/lib/THCUNN/im2col.h:60</code></p>\n<p>Some time it throws another:</p>\n<p>Traceback (most recent call last):<br>\nFile \"train.py\", line 502, in <br>\nbest_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200)<br>\nFile \"train.py\", line 204, in train_model<br>\nloss.backward()<br>\nFile \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 145, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br>\nFile \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/autograd/<strong>init</strong>.py\", line 98, in backward<br>\nvariables, grad_variables, retain_graph)<br>\nRuntimeError: cublas runtime error : the GPU program failed to execute at /home/ubuntu/src/pytorch/torch/lib/THC/THCBlas.cu:105</p>", "body_text": "Basic network block:\n(0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n(1): LeakyReLU (0.333, inplace)\n(2): FractionalMaxPool2d (\n)\nThe following stack traces occur very 1-2 epochs. I randomly pools inputs set to fit memory.\nSome time it throws the following stack trace:\nTHCudaCheck FAIL file=/home/ubuntu/src/pytorch/torch/lib/THCUNN/im2col.h line=60 error=59 : device-side assert triggered Traceback (most recent call last): File \"train.py\", line 502, in <module> best_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200) File \"train.py\", line 184, in train_model outputs = model(inputs) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/home/ubuntu/projects/tutorial-pytorch/model_zoo.py\", line 30, in forward x = self.features(x) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.py\", line 67, in forward input = module(input) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.py\", line 237, in forward self.padding, self.dilation, self.groups) File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/functional.py\", line 43, in conv2d return f(input, weight, bias) RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/ubuntu/src/pytorch/torch/lib/THCUNN/im2col.h:60\nSome time it throws another:\nTraceback (most recent call last):\nFile \"train.py\", line 502, in \nbest_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200)\nFile \"train.py\", line 204, in train_model\nloss.backward()\nFile \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 145, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/autograd/init.py\", line 98, in backward\nvariables, grad_variables, retain_graph)\nRuntimeError: cublas runtime error : the GPU program failed to execute at /home/ubuntu/src/pytorch/torch/lib/THC/THCBlas.cu:105", "body": "Basic network block: \r\n  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\r\n    (1): LeakyReLU (0.333, inplace)\r\n    (2): FractionalMaxPool2d (\r\n    )\r\n\r\nThe following stack traces occur very 1-2 epochs. I randomly pools inputs set to fit memory. \r\n\r\nSome time it throws the following stack trace:\r\n`THCudaCheck FAIL file=/home/ubuntu/src/pytorch/torch/lib/THCUNN/im2col.h line=60 error=59 : device-side assert triggered\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 502, in <module>\r\n    best_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200)\r\n  File \"train.py\", line 184, in train_model\r\n    outputs = model(inputs)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ubuntu/projects/tutorial-pytorch/model_zoo.py\", line 30, in forward\r\n    x = self.features(x)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.py\", line 67, in forward\r\n    input = module(input)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.py\", line 237, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/nn/functional.py\", line 43, in conv2d\r\n    return f(input, weight, bias)\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /home/ubuntu/src/pytorch/torch/lib/THCUNN/im2col.h:60`\r\n\r\nSome time it throws another: \r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 502, in <module>\r\n    best_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200)\r\n  File \"train.py\", line 204, in train_model\r\n    loss.backward()\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 145, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/home/ubuntu/src/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\nRuntimeError: cublas runtime error : the GPU program failed to execute at /home/ubuntu/src/pytorch/torch/lib/THC/THCBlas.cu:105"}