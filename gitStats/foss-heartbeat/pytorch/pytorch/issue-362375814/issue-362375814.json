{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11908", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11908/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11908/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11908/events", "html_url": "https://github.com/pytorch/pytorch/pull/11908", "id": 362375814, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE3MTEzMTAx", "number": 11908, "title": "Add support for reductions to TensorIterator", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-20T21:31:51Z", "updated_at": "2018-11-23T15:53:28Z", "closed_at": "2018-10-25T16:44:23Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11908", "html_url": "https://github.com/pytorch/pytorch/pull/11908", "diff_url": "https://github.com/pytorch/pytorch/pull/11908.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11908.patch"}, "body_html": "<p>This adds support for reductions like sum() and mul() to TensorIterator.<br>\nPerformance is similar to existing optimized code for CPU, and generally<br>\nbetter than existing code for CUDA kernels.</p>\n<p>The templatized CUDA kernel requires fewer instantiations than the<br>\nexisting THCReduce/THCReduceAll code. For example, sum() previously<br>\ngenerated 43 CUDA kernels, while it now requires only one (larger)<br>\nCUDA kernel. I suspect this should reduce code-size and<br>\ncompilation time, but I haven't measured it.</p>\n<p>Below are timings for sum() on <a href=\"https://ark.intel.com/products/81908/Intel-Xeon-Processor-E5-2680-v3-30M-Cache-2_50-GHz\" rel=\"nofollow\">CPU</a> (12 threads and 1 thread) and CUDA with various tensor sizes.</p>\n<p>CPU</p>\n<table>\n<thead>\n<tr>\n<th>Reduction (dim)</th>\n<th>Master</th>\n<th>PR</th>\n<th>Master (1 thread)</th>\n<th>PR (1 thread)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1024x1024 (all)</td>\n<td>22 us</td>\n<td>34 us</td>\n<td>136 us</td>\n<td>147 us</td>\n</tr>\n<tr>\n<td>1024x1024 (0)</td>\n<td>30 us</td>\n<td>28 us</td>\n<td>160 us</td>\n<td>160 us</td>\n</tr>\n<tr>\n<td>1024x1024 (1)</td>\n<td>25 us</td>\n<td>25 us</td>\n<td>171 us</td>\n<td>146 us</td>\n</tr>\n<tr>\n<td>1024x10x1024 (all)</td>\n<td>542 us</td>\n<td>550 us</td>\n<td>4.14 ms</td>\n<td>3.11 ms</td>\n</tr>\n<tr>\n<td>1024x10x1024 (0)</td>\n<td>658 us</td>\n<td>690 us</td>\n<td>6.80 ms</td>\n<td>5.93 ms</td>\n</tr>\n<tr>\n<td>1024x10x1024 (1)</td>\n<td>761 us</td>\n<td>757 us</td>\n<td>3.34 ms</td>\n<td>3.52 ms</td>\n</tr>\n<tr>\n<td>1024x10x1024 (2)</td>\n<td>538 us</td>\n<td>545 us</td>\n<td>3.73 ms</td>\n<td>3.04 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (all)</td>\n<td>72 ms</td>\n<td>71 ms</td>\n<td>364 ms</td>\n<td>357 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (0)</td>\n<td>94 ms</td>\n<td>90 ms</td>\n<td>935 ms</td>\n<td>927 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (1)</td>\n<td>80 ms</td>\n<td>86 ms</td>\n<td>881 ms</td>\n<td>688 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (2)</td>\n<td>71 ms</td>\n<td>71 ms</td>\n<td>456 ms</td>\n<td>354 ms</td>\n</tr>\n</tbody>\n</table>\n<p>CUDA</p>\n<table>\n<thead>\n<tr>\n<th>Reduction (dim)</th>\n<th>M40 base</th>\n<th>M40 PR</th>\n<th>P100 base</th>\n<th>P100 PR</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1024x10x1024 (all)</td>\n<td>238 us</td>\n<td>182 us</td>\n<td>136 us</td>\n<td>97 us</td>\n</tr>\n<tr>\n<td>1024x10x1024 (0)</td>\n<td>166 us</td>\n<td>179 us</td>\n<td>105 us</td>\n<td>84 us</td>\n</tr>\n<tr>\n<td>1024x10x1024 (1)</td>\n<td>181 us</td>\n<td>182 us</td>\n<td>89 us</td>\n<td>91 us</td>\n</tr>\n<tr>\n<td>1024x10x1024 (2)</td>\n<td>180 us</td>\n<td>168 us</td>\n<td>88 us</td>\n<td>79 us</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (all)</td>\n<td>17.5 ms</td>\n<td>16.4 ms</td>\n<td>8.23 ms</td>\n<td>7.48 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (0)</td>\n<td>27.2 ms</td>\n<td>28.6 ms</td>\n<td>7.63 ms</td>\n<td>7.38 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (1)</td>\n<td>16.5 ms</td>\n<td>16.3 ms</td>\n<td>7.66 ms</td>\n<td>7.40 ms</td>\n</tr>\n<tr>\n<td>1024x1024x1024 (2)</td>\n<td>17.8 ms</td>\n<td>16.4 ms</td>\n<td>8.37 ms</td>\n<td>7.31 ms</td>\n</tr>\n</tbody>\n</table>\n<p>Timings were generated with this script:<br>\n<a href=\"https://gist.github.com/colesbury/d3238b266d8a9872fe6f68f77619b379\">https://gist.github.com/colesbury/d3238b266d8a9872fe6f68f77619b379</a></p>", "body_text": "This adds support for reductions like sum() and mul() to TensorIterator.\nPerformance is similar to existing optimized code for CPU, and generally\nbetter than existing code for CUDA kernels.\nThe templatized CUDA kernel requires fewer instantiations than the\nexisting THCReduce/THCReduceAll code. For example, sum() previously\ngenerated 43 CUDA kernels, while it now requires only one (larger)\nCUDA kernel. I suspect this should reduce code-size and\ncompilation time, but I haven't measured it.\nBelow are timings for sum() on CPU (12 threads and 1 thread) and CUDA with various tensor sizes.\nCPU\n\n\n\nReduction (dim)\nMaster\nPR\nMaster (1 thread)\nPR (1 thread)\n\n\n\n\n1024x1024 (all)\n22 us\n34 us\n136 us\n147 us\n\n\n1024x1024 (0)\n30 us\n28 us\n160 us\n160 us\n\n\n1024x1024 (1)\n25 us\n25 us\n171 us\n146 us\n\n\n1024x10x1024 (all)\n542 us\n550 us\n4.14 ms\n3.11 ms\n\n\n1024x10x1024 (0)\n658 us\n690 us\n6.80 ms\n5.93 ms\n\n\n1024x10x1024 (1)\n761 us\n757 us\n3.34 ms\n3.52 ms\n\n\n1024x10x1024 (2)\n538 us\n545 us\n3.73 ms\n3.04 ms\n\n\n1024x1024x1024 (all)\n72 ms\n71 ms\n364 ms\n357 ms\n\n\n1024x1024x1024 (0)\n94 ms\n90 ms\n935 ms\n927 ms\n\n\n1024x1024x1024 (1)\n80 ms\n86 ms\n881 ms\n688 ms\n\n\n1024x1024x1024 (2)\n71 ms\n71 ms\n456 ms\n354 ms\n\n\n\nCUDA\n\n\n\nReduction (dim)\nM40 base\nM40 PR\nP100 base\nP100 PR\n\n\n\n\n1024x10x1024 (all)\n238 us\n182 us\n136 us\n97 us\n\n\n1024x10x1024 (0)\n166 us\n179 us\n105 us\n84 us\n\n\n1024x10x1024 (1)\n181 us\n182 us\n89 us\n91 us\n\n\n1024x10x1024 (2)\n180 us\n168 us\n88 us\n79 us\n\n\n1024x1024x1024 (all)\n17.5 ms\n16.4 ms\n8.23 ms\n7.48 ms\n\n\n1024x1024x1024 (0)\n27.2 ms\n28.6 ms\n7.63 ms\n7.38 ms\n\n\n1024x1024x1024 (1)\n16.5 ms\n16.3 ms\n7.66 ms\n7.40 ms\n\n\n1024x1024x1024 (2)\n17.8 ms\n16.4 ms\n8.37 ms\n7.31 ms\n\n\n\nTimings were generated with this script:\nhttps://gist.github.com/colesbury/d3238b266d8a9872fe6f68f77619b379", "body": "This adds support for reductions like sum() and mul() to TensorIterator.\r\nPerformance is similar to existing optimized code for CPU, and generally\r\nbetter than existing code for CUDA kernels.\r\n\r\nThe templatized CUDA kernel requires fewer instantiations than the\r\nexisting THCReduce/THCReduceAll code. For example, sum() previously\r\ngenerated 43 CUDA kernels, while it now requires only one (larger)\r\nCUDA kernel. I suspect this should reduce code-size and\r\ncompilation time, but I haven't measured it.\r\n\r\nBelow are timings for sum() on [CPU](https://ark.intel.com/products/81908/Intel-Xeon-Processor-E5-2680-v3-30M-Cache-2_50-GHz) (12 threads and 1 thread) and CUDA with various tensor sizes.\r\n\r\nCPU\r\n\r\n| Reduction (dim)      | Master  | PR      | Master (1 thread) | PR (1 thread) |\r\n|----------------------|---------|---------|-------------------|---------------|\r\n| 1024x1024 (all)      | 22 us   | 34 us   | 136 us            | 147 us        |\r\n| 1024x1024 (0)        | 30 us   | 28 us   | 160 us            | 160 us        |\r\n| 1024x1024 (1)        | 25 us   | 25 us   | 171 us            | 146 us        |\r\n| 1024x10x1024 (all)   | 542 us  | 550 us  | 4.14 ms           | 3.11 ms       |\r\n| 1024x10x1024 (0)     | 658 us  | 690 us  | 6.80 ms           | 5.93 ms       |\r\n| 1024x10x1024 (1)     | 761 us  | 757 us  | 3.34 ms           | 3.52 ms       |\r\n| 1024x10x1024 (2)     | 538 us  | 545 us  | 3.73 ms           | 3.04 ms       |\r\n| 1024x1024x1024 (all) | 72 ms   | 71 ms   | 364 ms            | 357 ms        |\r\n| 1024x1024x1024 (0)   | 94 ms   | 90 ms   | 935 ms            | 927 ms        |\r\n| 1024x1024x1024 (1)   | 80 ms   | 86 ms   | 881 ms            | 688 ms        |\r\n| 1024x1024x1024 (2)   | 71 ms   | 71 ms   | 456 ms            | 354 ms        |\r\n\r\nCUDA\r\n\r\n| Reduction (dim)      | M40 base | M40 PR  | P100 base | P100 PR   |\r\n|----------------------|----------|---------|-----------|-----------|\r\n| 1024x10x1024 (all)   | 238 us   | 182 us  | 136 us    | 97 us     |\r\n| 1024x10x1024 (0)     | 166 us   | 179 us  | 105 us    | 84 us     |\r\n| 1024x10x1024 (1)     | 181 us   | 182 us  | 89 us     | 91 us     |\r\n| 1024x10x1024 (2)     | 180 us   | 168 us  | 88 us     | 79 us     |\r\n| 1024x1024x1024 (all) | 17.5 ms  | 16.4 ms | 8.23 ms   | 7.48 ms   |\r\n| 1024x1024x1024 (0)   | 27.2 ms  | 28.6 ms | 7.63 ms   | 7.38 ms   |\r\n| 1024x1024x1024 (1)   | 16.5 ms  | 16.3 ms | 7.66 ms   | 7.40 ms   |\r\n| 1024x1024x1024 (2)   | 17.8 ms  | 16.4 ms | 8.37 ms   | 7.31 ms   |\r\n\r\nTimings were generated with this script:\r\nhttps://gist.github.com/colesbury/d3238b266d8a9872fe6f68f77619b379\r\n\r\n"}