{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220651143", "pull_request_review_id": 159108250, "id": 220651143, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMDY1MTE0Mw==", "diff_hunk": "@@ -1,53 +1,81 @@\n-#include \"TensorIterator.h\"\n+#include <ATen/native/TensorIterator.h>\n \n+#include <array>\n #include <ATen/ExpandUtils.h>\n #include <ATen/Parallel.h>\n \n namespace at {\n \n-void TensorIterator::reorder_dimensions() {\n-  // Sort the dimensions based on the sum-of-strides in ascending order. NOTE:\n-  // that this inverts the order of C-contiguous tensors. strides[0] is the\n-  // fastest moving dimension instead of strides[ndim - 1].\n+struct DimCounter {\n+  DimCounter(IntList shape, Range range);\n \n-  auto sum_of_strides = SmallVector<double, 6>(ndim(), 0.0);\n-  for (int dim = 0; dim < ndim(); dim++) {\n-    double sum = 0.0;\n-    for (const auto& op : operands_) {\n-      if (op.stride_bytes.size() == 0) continue;\n-      sum += op.stride_bytes[dim];\n-    }\n+  void increment(const std::array<int64_t, 2>& step);\n+  bool is_done() const;\n+  std::array<int64_t, 2> max_step() const;\n \n-    // Weight each dimension by its index. Given two dimensions with equal\n-    // some of strides, this preserves the given relative ordering.\n-    sum += (ndim() - dim - 1) / (double)ndim();\n+  IntList shape;\n+  Range range;\n+  DimVector values;\n+  int64_t offset;\n+};\n \n-    sum_of_strides[dim] = sum;\n-  }\n+using DimMask = TensorIterator::DimMask;\n+using PtrVector = TensorIterator::PtrVector;\n+using loop_t = TensorIterator::loop_t;\n+using loop2d_t = TensorIterator::loop2d_t;\n \n-  // initialize perm with 0, 1, 2, ...\n-  perm_.resize(ndim());\n-  std::iota(std::begin(perm_), std::end(perm_), 0);\n-\n-  std::sort(std::begin(perm_), std::end(perm_), [&](size_t i1, size_t i2) {\n-    return sum_of_strides[i1] < sum_of_strides[i2];\n-  });\n+void TensorIterator::reorder_dimensions() {\n+  // Sort the dimensions based on strides in ascending order with reduced dims", "path": "aten/src/ATen/native/TensorIterator.cpp", "position": 52, "original_position": 52, "commit_id": "85757031de20ad93e21ea7bf68d7c39e9698899d", "original_commit_id": "aeb0b9dd17ddef7b46d0ace6bcd42be2cbedbc0d", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Previously we sorted based on the sum-of-strides. I changed it to only swap dimensions that make the ordering no-worse across all arguments (and better for at least one argument), with stride 0 not counting and reduced dimensions moved to the front. This more closely matches the stride sorting behavior of THC and NumPy, with the exception of the reduced dimensions behavior.\r\n\r\nI don't expect this to affect the runtime for pointwise operations. Most orderings remain the same, and contiguous dims get moved to the front, which is the most important aspect.\r\n\r\nI'm not sure this is the final strategy for ordering strides, but this logic makes it easier to support reductions.", "created_at": "2018-09-26T17:18:25Z", "updated_at": "2018-11-23T15:52:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/11908#discussion_r220651143", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11908", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220651143"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11908#discussion_r220651143"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11908"}}, "body_html": "<p>Previously we sorted based on the sum-of-strides. I changed it to only swap dimensions that make the ordering no-worse across all arguments (and better for at least one argument), with stride 0 not counting and reduced dimensions moved to the front. This more closely matches the stride sorting behavior of THC and NumPy, with the exception of the reduced dimensions behavior.</p>\n<p>I don't expect this to affect the runtime for pointwise operations. Most orderings remain the same, and contiguous dims get moved to the front, which is the most important aspect.</p>\n<p>I'm not sure this is the final strategy for ordering strides, but this logic makes it easier to support reductions.</p>", "body_text": "Previously we sorted based on the sum-of-strides. I changed it to only swap dimensions that make the ordering no-worse across all arguments (and better for at least one argument), with stride 0 not counting and reduced dimensions moved to the front. This more closely matches the stride sorting behavior of THC and NumPy, with the exception of the reduced dimensions behavior.\nI don't expect this to affect the runtime for pointwise operations. Most orderings remain the same, and contiguous dims get moved to the front, which is the most important aspect.\nI'm not sure this is the final strategy for ordering strides, but this logic makes it easier to support reductions.", "in_reply_to_id": 219666795}