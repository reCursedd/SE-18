{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225314671", "pull_request_review_id": 164888504, "id": 225314671, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTMxNDY3MQ==", "diff_hunk": "@@ -0,0 +1,478 @@\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+#include <ATen/cuda/Array.h>\n+#include <ATen/cuda/CUDAContext.h>\n+#include <ATen/cuda/detail/OffsetCalculator.cuh>\n+#include <ATen/detail/FunctionTraits.h>\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCGeneral.hpp>\n+#include <ATen/native/TensorIterator.h>\n+#include <ATen/native/cuda/Loops.cuh>\n+#include <iosfwd>\n+\n+namespace at { namespace native {\n+\n+using at::cuda::Array;\n+\n+static inline int64_t div_up(int64_t a, int64_t b) {\n+  return (a + b - 1) / b;\n+}\n+\n+struct ReduceConfig {\n+  static constexpr int LANE = 0;\n+  static constexpr int WARP = 1;\n+  static constexpr int CTA = 2;\n+  static constexpr int NUM_THREADS = 512;\n+\n+  ReduceConfig(int element_size_bytes, int num_outputs, int num_inputs)\n+    : element_size_bytes(element_size_bytes)\n+    , num_inputs(num_inputs)\n+    , num_outputs(num_outputs) {}\n+\n+  int element_size_bytes;\n+  int num_inputs;\n+  int num_outputs;\n+  int step_input = 1;\n+  int step_output = 1;\n+  int ctas_per_output = 1;\n+  int input_mult[3] = {0, 0, 0};\n+  int output_mult[2] = {0, 0};\n+\n+  int split_input(int parallelism) {\n+    int step = step_input;\n+    step_input *= parallelism;\n+    return step;\n+  }\n+\n+  int split_output(int parallelism) {\n+    int step = step_output;\n+    step_output *= parallelism;\n+    return step;\n+  }\n+\n+  dim3 block() const {\n+    int warp_size = at::cuda::warp_size();\n+    return dim3(warp_size, NUM_THREADS / warp_size);\n+  }\n+\n+  dim3 grid() const {\n+    return dim3(div_up(num_outputs, step_output), ctas_per_output);\n+  }\n+\n+  AT_HOST_DEVICE bool should_warp_reduce() const {\n+    return input_mult[LANE] != 0;\n+  }\n+\n+  AT_HOST_DEVICE bool should_block_reduce() const {\n+    return input_mult[WARP] != 0;\n+  }\n+\n+  AT_HOST_DEVICE bool should_global_reduce() const {\n+    return input_mult[CTA] != 0;\n+  }\n+\n+  AT_DEVICE bool should_store(int output_idx) const {\n+    return output_idx < num_outputs &&\n+      (!should_warp_reduce() || threadIdx.x == 0) &&\n+      (!should_block_reduce() || threadIdx.y == 0);\n+  }\n+\n+  AT_HOST_DEVICE int input_idx() const {\n+    int lane = threadIdx.x;\n+    int warp = threadIdx.y;\n+    int cta2 = blockIdx.y;\n+    return (lane * input_mult[LANE] +\n+            warp * input_mult[WARP] +\n+            cta2 * input_mult[CTA]);\n+  }\n+\n+  AT_HOST_DEVICE int output_idx() const {\n+    int lane = threadIdx.x;\n+    int warp = threadIdx.y;\n+    int cta1 = blockIdx.x;\n+    return (lane * output_mult[LANE] +\n+            warp * output_mult[WARP] +\n+            cta1 * step_output);\n+  }\n+\n+  AT_DEVICE int shared_memory_offset(int offset) const {\n+    return threadIdx.x + (threadIdx.y + offset) * blockDim.x;\n+  }\n+\n+  AT_DEVICE int staging_memory_offset(int cta2) const {\n+    int offset = cta2 + blockIdx.x * gridDim.y;\n+    if (!should_warp_reduce()) {\n+      offset = threadIdx.x + offset * blockDim.x;\n+    }\n+    return offset;\n+  }\n+\n+  int shared_memory_size() const {\n+    if (!should_block_reduce()) {\n+      return 0;\n+    }\n+    return element_size_bytes * NUM_THREADS;\n+  }\n+\n+  int global_memory_size() const {\n+    if (!should_global_reduce()) {\n+      return 0;\n+    }\n+    return element_size_bytes * num_outputs * ctas_per_output;\n+  }\n+\n+  int semaphore_size() const {\n+    if (!should_global_reduce()) {\n+      return 0;\n+    }\n+    return sizeof(int) * grid().x;\n+  }\n+\n+  int values_per_thread() const {\n+    return div_up(num_inputs, step_input);\n+  }\n+};\n+\n+std::ostream& operator<<(std::ostream& out, const ReduceConfig& config);\n+\n+template<int nt, typename R>\n+__launch_bounds__(nt, 4)\n+__global__ void reduce_kernel(R reduction) {\n+  reduction.run();\n+}\n+\n+static OffsetCalculator<2> make_output_calculator(const TensorIterator& iter) {\n+  int num_reduce_dims = iter.num_reduce_dims();\n+  int num_output_dims = iter.ndim() - num_reduce_dims;\n+  std::array<const int64_t*, 2> strides = {\n+    iter.strides(0).data() + num_reduce_dims,\n+    iter.strides(1).data() + num_reduce_dims,\n+  };\n+  auto shape = iter.shape().data() + num_reduce_dims;\n+  return OffsetCalculator<2>(num_output_dims, shape, strides.data());\n+}\n+\n+static OffsetCalculator<1> make_input_calculator(const TensorIterator& iter) {\n+  int num_reduce_dims = iter.num_reduce_dims();\n+  std::array<const int64_t*, 1> strides = {\n+    iter.strides(1).data(),\n+  };\n+  return OffsetCalculator<1>(num_reduce_dims, iter.shape().data(), strides.data());\n+}\n+\n+template <int vt, typename func_t>\n+__device__ void strided_iterate(func_t f, int begin, int end, int stride) {\n+  if (begin + (vt - 1) * stride < end) {\n+    #pragma unroll\n+    for (int i = 0; i < vt; i++) {\n+      f(i, begin + i * stride);\n+    }\n+  } else {\n+    #pragma unroll\n+    for (int i = 0; i < vt; i++) {\n+      int idx = begin + i * stride;\n+      if (idx < end) {\n+        f(i, idx);\n+      }\n+    }\n+  }\n+}\n+\n+template <int vt, typename type_t, typename foo_t>\n+__device__ Array<type_t, vt> load_memory(const type_t* in, int begin, int end, int stride, foo_t foo) {\n+  Array<type_t, vt> res;\n+  strided_iterate<vt>([&](int i, int idx) {\n+    res[i] = in[foo(idx)];\n+  }, begin, end, stride);\n+  return res;\n+}\n+\n+template <int vt, typename type_t>\n+__device__ Array<type_t, vt> load_memory(const type_t* in, int begin, int end, int stride) {\n+  return load_memory<vt>(in, begin, end, stride, [](int idx) { return idx; });\n+}\n+\n+template <typename scalar_t, typename func_t>\n+struct Reduction {", "path": "aten/src/ATen/native/cuda/Reduce.cuh", "position": null, "original_position": 197, "commit_id": "85757031de20ad93e21ea7bf68d7c39e9698899d", "original_commit_id": "0d5e19bb2370df0dab2edc5a3a51c68fb3838838", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "this is super nitty, but why is it called \"ReduceConfig\" and \"Reduction\" instead of the Reduce/Reduction matching?", "created_at": "2018-10-15T20:58:58Z", "updated_at": "2018-11-23T15:53:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/11908#discussion_r225314671", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11908", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225314671"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11908#discussion_r225314671"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11908"}}, "body_html": "<p>this is super nitty, but why is it called \"ReduceConfig\" and \"Reduction\" instead of the Reduce/Reduction matching?</p>", "body_text": "this is super nitty, but why is it called \"ReduceConfig\" and \"Reduction\" instead of the Reduce/Reduction matching?"}