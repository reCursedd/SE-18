{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8054", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8054/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8054/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8054/events", "html_url": "https://github.com/pytorch/pytorch/issues/8054", "id": 328685141, "node_id": "MDU6SXNzdWUzMjg2ODUxNDE=", "number": 8054, "title": "[feature request] Can we have an option to export static graph?", "user": {"login": "qigtang", "id": 7813095, "node_id": "MDQ6VXNlcjc4MTMwOTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7813095?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qigtang", "html_url": "https://github.com/qigtang", "followers_url": "https://api.github.com/users/qigtang/followers", "following_url": "https://api.github.com/users/qigtang/following{/other_user}", "gists_url": "https://api.github.com/users/qigtang/gists{/gist_id}", "starred_url": "https://api.github.com/users/qigtang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qigtang/subscriptions", "organizations_url": "https://api.github.com/users/qigtang/orgs", "repos_url": "https://api.github.com/users/qigtang/repos", "events_url": "https://api.github.com/users/qigtang/events{/privacy}", "received_events_url": "https://api.github.com/users/qigtang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-01T23:04:45Z", "updated_at": "2018-08-30T18:34:37Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry if it is double posted to onnx/onnx git, since I am not sure whether the feature is part of pytorch or onnx.</p>\n<hr>\n<p>When using pytorch 0.4, export resnet to onnx, the FC layer will be something like</p>\n<p>%169 : Float(1, 512, 1, 1) = onnx::AveragePoolkernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[1, 1], scope: ResNet/AvgPool2d[avgpool]<br>\n%170 : Float(1, 512) = onnx::Flattenaxis=1, scope: ResNet<br>\n%171 : Float(1, 6) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%170, %101, %102), scope: ResNet/Linear[fc]<br>\nreturn (%171);</p>\n<p>Latest version of pytorch save it as<br>\n%190 : Float(1, 512, 1, 1) = onnx::AveragePoolkernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[1, 1], scope: ResNet/AvgPool2d[avgpool]<br>\n%191 : Dynamic = onnx::Shape(%190), scope: ResNet<br>\n%192 : Dynamic = onnx::Sliceaxes=[0], ends=[1], starts=[0], scope: ResNet<br>\n%193 : Long() = onnx::Squeezeaxes=[0], scope: ResNet<br>\n%194 : Long() = onnx::Constantvalue={-1}, scope: ResNet<br>\n%195 : Dynamic = onnx::Unsqueezeaxes=[0], scope: ResNet<br>\n%196 : Dynamic = onnx::Unsqueezeaxes=[0], scope: ResNet<br>\n%197 : Dynamic = onnx::Concat[axis=0](%195, %196), scope: ResNet<br>\n%198 : Float(1, 512) = onnx::Reshape(%190, %197), scope: ResNet<br>\n%199 : Float(1, 6) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%198, %121, %122), scope: ResNet/Linear[fc]<br>\nreturn (%199);</p>\n<p>The new one actually caused trouble for us. For embedded device, we want fixed size, and let TensorRT optimize the graph for deployment. I don't want the \"nice\" dynamic feature in onnx.</p>\n<p>It is possible to put an option to latest pytorch, so that we can choose to export a static and simpler graph?</p>", "body_text": "Sorry if it is double posted to onnx/onnx git, since I am not sure whether the feature is part of pytorch or onnx.\n\nWhen using pytorch 0.4, export resnet to onnx, the FC layer will be something like\n%169 : Float(1, 512, 1, 1) = onnx::AveragePoolkernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[1, 1], scope: ResNet/AvgPool2d[avgpool]\n%170 : Float(1, 512) = onnx::Flattenaxis=1, scope: ResNet\n%171 : Float(1, 6) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%170, %101, %102), scope: ResNet/Linear[fc]\nreturn (%171);\nLatest version of pytorch save it as\n%190 : Float(1, 512, 1, 1) = onnx::AveragePoolkernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[1, 1], scope: ResNet/AvgPool2d[avgpool]\n%191 : Dynamic = onnx::Shape(%190), scope: ResNet\n%192 : Dynamic = onnx::Sliceaxes=[0], ends=[1], starts=[0], scope: ResNet\n%193 : Long() = onnx::Squeezeaxes=[0], scope: ResNet\n%194 : Long() = onnx::Constantvalue={-1}, scope: ResNet\n%195 : Dynamic = onnx::Unsqueezeaxes=[0], scope: ResNet\n%196 : Dynamic = onnx::Unsqueezeaxes=[0], scope: ResNet\n%197 : Dynamic = onnx::Concat[axis=0](%195, %196), scope: ResNet\n%198 : Float(1, 512) = onnx::Reshape(%190, %197), scope: ResNet\n%199 : Float(1, 6) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%198, %121, %122), scope: ResNet/Linear[fc]\nreturn (%199);\nThe new one actually caused trouble for us. For embedded device, we want fixed size, and let TensorRT optimize the graph for deployment. I don't want the \"nice\" dynamic feature in onnx.\nIt is possible to put an option to latest pytorch, so that we can choose to export a static and simpler graph?", "body": "Sorry if it is double posted to onnx/onnx git, since I am not sure whether the feature is part of pytorch or onnx. \r\n\r\n------------------------------------------------------------------------------------------\r\n\r\nWhen using pytorch 0.4, export resnet to onnx, the FC layer will be something like\r\n\r\n%169 : Float(1, 512, 1, 1) = onnx::AveragePoolkernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[1, 1], scope: ResNet/AvgPool2d[avgpool]\r\n%170 : Float(1, 512) = onnx::Flattenaxis=1, scope: ResNet\r\n%171 : Float(1, 6) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%170, %101, %102), scope: ResNet/Linear[fc]\r\nreturn (%171);\r\n\r\nLatest version of pytorch save it as\r\n%190 : Float(1, 512, 1, 1) = onnx::AveragePoolkernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[1, 1], scope: ResNet/AvgPool2d[avgpool]\r\n%191 : Dynamic = onnx::Shape(%190), scope: ResNet\r\n%192 : Dynamic = onnx::Sliceaxes=[0], ends=[1], starts=[0], scope: ResNet\r\n%193 : Long() = onnx::Squeezeaxes=[0], scope: ResNet\r\n%194 : Long() = onnx::Constantvalue={-1}, scope: ResNet\r\n%195 : Dynamic = onnx::Unsqueezeaxes=[0], scope: ResNet\r\n%196 : Dynamic = onnx::Unsqueezeaxes=[0], scope: ResNet\r\n%197 : Dynamic = onnx::Concat[axis=0](%195, %196), scope: ResNet\r\n%198 : Float(1, 512) = onnx::Reshape(%190, %197), scope: ResNet\r\n%199 : Float(1, 6) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%198, %121, %122), scope: ResNet/Linear[fc]\r\nreturn (%199);\r\n\r\nThe new one actually caused trouble for us. For embedded device, we want fixed size, and let TensorRT optimize the graph for deployment. I don't want the \"nice\" dynamic feature in onnx.\r\n\r\nIt is possible to put an option to latest pytorch, so that we can choose to export a static and simpler graph?\r\n"}