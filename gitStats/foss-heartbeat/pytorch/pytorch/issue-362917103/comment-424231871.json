{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424231871", "html_url": "https://github.com/pytorch/pytorch/issues/11988#issuecomment-424231871", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11988", "id": 424231871, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDIzMTg3MQ==", "user": {"login": "akamaus", "id": 58955, "node_id": "MDQ6VXNlcjU4OTU1", "avatar_url": "https://avatars0.githubusercontent.com/u/58955?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akamaus", "html_url": "https://github.com/akamaus", "followers_url": "https://api.github.com/users/akamaus/followers", "following_url": "https://api.github.com/users/akamaus/following{/other_user}", "gists_url": "https://api.github.com/users/akamaus/gists{/gist_id}", "starred_url": "https://api.github.com/users/akamaus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akamaus/subscriptions", "organizations_url": "https://api.github.com/users/akamaus/orgs", "repos_url": "https://api.github.com/users/akamaus/repos", "events_url": "https://api.github.com/users/akamaus/events{/privacy}", "received_events_url": "https://api.github.com/users/akamaus/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-25T07:11:40Z", "updated_at": "2018-09-25T07:11:40Z", "author_association": "NONE", "body_html": "<p>Well, I have a beefy desktop where I train models and do most of work and a worn laptop I use to prototyping and coding during travel. So it's very useful for me to be able to at least run and check my code locally. Previous pytorch versions worked fine, as well as tensorflow, for example. pytorch-0.4 brings great usability improvements IMO with all that merged Tensors and Variables from one hand and it breaks compatibility from another. So I can't use older version to work with fresh codebase.</p>\n<p>Maybe it's ok to have binary builds require newer CPU (although TF guys seem to think overwise and require you to compile to take full advantage of your hardware), but source is supposed to be flexible, IMO. We're not in nineties when everybody upgraded every two years. Of course it's impractical to train on older systems, but they're absolutely capable to do inference even for state-of-the art models.</p>", "body_text": "Well, I have a beefy desktop where I train models and do most of work and a worn laptop I use to prototyping and coding during travel. So it's very useful for me to be able to at least run and check my code locally. Previous pytorch versions worked fine, as well as tensorflow, for example. pytorch-0.4 brings great usability improvements IMO with all that merged Tensors and Variables from one hand and it breaks compatibility from another. So I can't use older version to work with fresh codebase.\nMaybe it's ok to have binary builds require newer CPU (although TF guys seem to think overwise and require you to compile to take full advantage of your hardware), but source is supposed to be flexible, IMO. We're not in nineties when everybody upgraded every two years. Of course it's impractical to train on older systems, but they're absolutely capable to do inference even for state-of-the art models.", "body": "Well, I have a beefy desktop where I train models and do most of work and a worn laptop I use to prototyping and coding during travel. So it's very useful for me to be able to at least run and check my code locally. Previous pytorch versions worked fine, as well as tensorflow, for example. pytorch-0.4 brings great usability improvements IMO with all that merged Tensors and Variables from one hand and it breaks compatibility from another. So I can't use older version to work with fresh codebase.\r\n\r\nMaybe it's ok to have binary builds require newer CPU (although TF guys seem to think overwise and require you to compile to take full advantage of your hardware), but source is supposed to be flexible, IMO. We're not in nineties when everybody upgraded every two years. Of course it's impractical to train on older systems, but they're absolutely capable to do inference even for state-of-the art models."}