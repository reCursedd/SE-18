{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1462", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1462/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1462/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1462/events", "html_url": "https://github.com/pytorch/pytorch/issues/1462", "id": 226098263, "node_id": "MDU6SXNzdWUyMjYwOTgyNjM=", "number": 1462, "title": "Change sparse_mask to take indexing mask, rather than entire sparse tensor", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/sparse", "name": "sparse", "color": "bfd4f2", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2017-05-03T20:01:36Z", "updated_at": "2018-09-12T21:03:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Currently, sparse_mask takes a sparse tensor, throws out the values, and uses just the indices to mask a dense tensor. This is not a very good API; for one, it causes trouble when the mask tensor is of different type of the dense tensor you're masking (this shouldn't cause any problems, but right now the dispatcher chokes, because it requires them to be the same.)</p>\n<p>So let's introduce a new function: <code>sparse_select</code>, which takes JUST the indexing mask, and produces a sparse vector of just those indices selected.</p>\n<p>There is one minor downside to doing things this way, which is that we lose information about whether or not the indices are coalesced or not.</p>\n<hr>\n<p><strong>Old description.</strong> Recently I got this error:</p>\n<pre><code>  File \"/Users/ezyang/Dev/pytorch/torch/optim/sgd.py\", line 93, in step\n    d_s = (state['step'] - state['last_update']._sparse_mask(d_p.long())).type_as(p.data)\nTypeError: _sparse_mask received an invalid combination of arguments - got (torch.sparse.LongTensor), but expected (torch.sparse.IntTensor mask)\n</code></pre>\n<p>That's a bit silly: in the end you're only ever going to use the indices of the sparse mask tensor, so you don't really care about what the values in the mask tensor.</p>\n<p>Unfortunately, making sparse_mask polymorphic over the type of the mask tensor seems a bit fiddly to do (one existing function which is similarly polymorphic is <code>copy</code>, but the way it is polymorphic is by generating a copy of the function for each type.</p>\n<p>Another strategy would be to introduce a variant of sparse_mask which takes only the index tensor, rather than the entire sparse tensor. This would be similar to index_select, but index_select indexes along a dimension, whereas with sparse_mask we have the coordinates for all dimensions.</p>\n<p>Additionally, to a certain degree you can work around this problem:</p>\n<ol>\n<li>\n<p>If you don't care about efficiency, you can cast the sparse mask to the required type to make the dispatcher stop complaining. If you do care about efficiency, you could probably make a fake value tensor to get it the right type.</p>\n</li>\n<li>\n<p>If your update is purely additive, you can use <code>spadd</code> to apply your sparse vector of additions to the new tensor. (For example, this is true for adagrad.) If you don't care about efficiency, you can manually difference out the original value so that you have an additive update.</p>\n</li>\n<li>\n<p>If your sparse tensors are 1D, index_select and related functions can serve your needs.</p>\n</li>\n</ol>", "body_text": "Currently, sparse_mask takes a sparse tensor, throws out the values, and uses just the indices to mask a dense tensor. This is not a very good API; for one, it causes trouble when the mask tensor is of different type of the dense tensor you're masking (this shouldn't cause any problems, but right now the dispatcher chokes, because it requires them to be the same.)\nSo let's introduce a new function: sparse_select, which takes JUST the indexing mask, and produces a sparse vector of just those indices selected.\nThere is one minor downside to doing things this way, which is that we lose information about whether or not the indices are coalesced or not.\n\nOld description. Recently I got this error:\n  File \"/Users/ezyang/Dev/pytorch/torch/optim/sgd.py\", line 93, in step\n    d_s = (state['step'] - state['last_update']._sparse_mask(d_p.long())).type_as(p.data)\nTypeError: _sparse_mask received an invalid combination of arguments - got (torch.sparse.LongTensor), but expected (torch.sparse.IntTensor mask)\n\nThat's a bit silly: in the end you're only ever going to use the indices of the sparse mask tensor, so you don't really care about what the values in the mask tensor.\nUnfortunately, making sparse_mask polymorphic over the type of the mask tensor seems a bit fiddly to do (one existing function which is similarly polymorphic is copy, but the way it is polymorphic is by generating a copy of the function for each type.\nAnother strategy would be to introduce a variant of sparse_mask which takes only the index tensor, rather than the entire sparse tensor. This would be similar to index_select, but index_select indexes along a dimension, whereas with sparse_mask we have the coordinates for all dimensions.\nAdditionally, to a certain degree you can work around this problem:\n\n\nIf you don't care about efficiency, you can cast the sparse mask to the required type to make the dispatcher stop complaining. If you do care about efficiency, you could probably make a fake value tensor to get it the right type.\n\n\nIf your update is purely additive, you can use spadd to apply your sparse vector of additions to the new tensor. (For example, this is true for adagrad.) If you don't care about efficiency, you can manually difference out the original value so that you have an additive update.\n\n\nIf your sparse tensors are 1D, index_select and related functions can serve your needs.", "body": "Currently, sparse_mask takes a sparse tensor, throws out the values, and uses just the indices to mask a dense tensor. This is not a very good API; for one, it causes trouble when the mask tensor is of different type of the dense tensor you're masking (this shouldn't cause any problems, but right now the dispatcher chokes, because it requires them to be the same.)\r\n\r\nSo let's introduce a new function: `sparse_select`, which takes JUST the indexing mask, and produces a sparse vector of just those indices selected.\r\n\r\nThere is one minor downside to doing things this way, which is that we lose information about whether or not the indices are coalesced or not.\r\n\r\n----\r\n\r\n**Old description.** Recently I got this error:\r\n\r\n```\r\n  File \"/Users/ezyang/Dev/pytorch/torch/optim/sgd.py\", line 93, in step\r\n    d_s = (state['step'] - state['last_update']._sparse_mask(d_p.long())).type_as(p.data)\r\nTypeError: _sparse_mask received an invalid combination of arguments - got (torch.sparse.LongTensor), but expected (torch.sparse.IntTensor mask)\r\n```\r\n\r\nThat's a bit silly: in the end you're only ever going to use the indices of the sparse mask tensor, so you don't really care about what the values in the mask tensor.\r\n\r\nUnfortunately, making sparse_mask polymorphic over the type of the mask tensor seems a bit fiddly to do (one existing function which is similarly polymorphic is `copy`, but the way it is polymorphic is by generating a copy of the function for each type.\r\n\r\nAnother strategy would be to introduce a variant of sparse_mask which takes only the index tensor, rather than the entire sparse tensor. This would be similar to index_select, but index_select indexes along a dimension, whereas with sparse_mask we have the coordinates for all dimensions.\r\n\r\nAdditionally, to a certain degree you can work around this problem:\r\n\r\n1. If you don't care about efficiency, you can cast the sparse mask to the required type to make the dispatcher stop complaining. If you do care about efficiency, you could probably make a fake value tensor to get it the right type.\r\n\r\n2. If your update is purely additive, you can use `spadd` to apply your sparse vector of additions to the new tensor. (For example, this is true for adagrad.) If you don't care about efficiency, you can manually difference out the original value so that you have an additive update.\r\n\r\n3. If your sparse tensors are 1D, index_select and related functions can serve your needs."}