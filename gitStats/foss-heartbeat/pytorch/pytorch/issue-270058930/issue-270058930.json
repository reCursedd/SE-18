{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3398", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3398/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3398/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3398/events", "html_url": "https://github.com/pytorch/pytorch/issues/3398", "id": 270058930, "node_id": "MDU6SXNzdWUyNzAwNTg5MzA=", "number": 3398, "title": "Cuda runtime error(59) again and weird", "user": {"login": "YazhiGao", "id": 16754389, "node_id": "MDQ6VXNlcjE2NzU0Mzg5", "avatar_url": "https://avatars1.githubusercontent.com/u/16754389?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YazhiGao", "html_url": "https://github.com/YazhiGao", "followers_url": "https://api.github.com/users/YazhiGao/followers", "following_url": "https://api.github.com/users/YazhiGao/following{/other_user}", "gists_url": "https://api.github.com/users/YazhiGao/gists{/gist_id}", "starred_url": "https://api.github.com/users/YazhiGao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YazhiGao/subscriptions", "organizations_url": "https://api.github.com/users/YazhiGao/orgs", "repos_url": "https://api.github.com/users/YazhiGao/repos", "events_url": "https://api.github.com/users/YazhiGao/events{/privacy}", "received_events_url": "https://api.github.com/users/YazhiGao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-31T17:59:46Z", "updated_at": "2017-11-03T02:35:42Z", "closed_at": "2017-11-03T02:35:42Z", "author_association": "NONE", "body_html": "<p>I am encountering the notorious cuda 59 error.<br>\nThe error occurs after some normal iterations.<br>\nI think I do not have label mismatch problem.<br>\nI am using h5py for reading data to dataloader which was accused of multiprocessing problem previously.<br>\nSetting the workers to 1 and add<br>\nimport torch import torch.multiprocessing torch.multiprocessing.set_start_method('spawn')<br>\ndoes not solve my problem.<br>\nAny replacement I can use other than h5py but preserve the speed or did i use numpy the wrong way?<br>\nThe data loader is for resnet image features and text bow representations.<br>\nI have checked my training code, the batch on cpu at the error batch No. can be seen,<br>\nthe variable just got run after i invoke .cuda()</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> model <span class=\"pl-k\">import</span> baseline\n<span class=\"pl-k\">from</span> util <span class=\"pl-k\">import</span> data\n<span class=\"pl-k\">from</span> util <span class=\"pl-k\">import</span> utils\n<span class=\"pl-k\">from</span> util <span class=\"pl-k\">import</span> config\n<span class=\"pl-k\">from</span> tqdm <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> pdb\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span><span class=\"pl-k\">==</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    train <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    cudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    training,train_dict_size <span class=\"pl-k\">=</span>  data.get_loader(<span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,<span class=\"pl-v\">full_batch</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\n    val,val_dict_size <span class=\"pl-k\">=</span> data.get_loader(<span class=\"pl-v\">val</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,<span class=\"pl-v\">full_batch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    model <span class=\"pl-k\">=</span> baseline.BOWIMG(config.max_answers,train_dict_size,config.word_embed_dim,config.image_embed_dim,config.batch_size)\n    best_perf <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    model.cuda()\n    var_params <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>requires_grad<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">False</span>,\n\t<span class=\"pl-s\"><span class=\"pl-pds\">'</span>volatile<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">False</span>\n    }\n    val_params <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>requires_grad<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">False</span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>volatile<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">True</span>\n    }\n    optimizer <span class=\"pl-k\">=</span> optim.Adam([p <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> model.parameters() <span class=\"pl-k\">if</span> p.requires_grad])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>data is fully loaded<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> tqdm(<span class=\"pl-c1\">range</span>(config.epochs)):\n        iteration <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">for</span> v,q,a,item,_ <span class=\"pl-k\">in</span> training:\n            q <span class=\"pl-k\">=</span> Variable(q.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),<span class=\"pl-k\">**</span>var_params)\n            a <span class=\"pl-k\">=</span> Variable(a.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),<span class=\"pl-k\">**</span>var_params)\n            v <span class=\"pl-k\">=</span> Variable(v.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),<span class=\"pl-k\">**</span>var_params)\n            o <span class=\"pl-k\">=</span> model(<span class=\"pl-v\">q</span><span class=\"pl-k\">=</span>q,<span class=\"pl-v\">v</span><span class=\"pl-k\">=</span>v)\n            optimizer.zero_grad()\n            loss <span class=\"pl-k\">=</span>(<span class=\"pl-k\">-</span>o<span class=\"pl-k\">*</span>(a<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span>)).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).mean() <span class=\"pl-c\"><span class=\"pl-c\">#</span> F.nll_loss(o,a)</span>\n            loss.backward()\n            optimizer.step()\n            acc <span class=\"pl-k\">=</span> utils.batch_accuracy(o.data,a.data).cpu()\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epoch <span class=\"pl-c1\">%s</span>, iteration <span class=\"pl-c1\">%s</span>, loss <span class=\"pl-c1\">%s</span>, accuracy <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>(<span class=\"pl-c1\">str</span>(i),<span class=\"pl-c1\">str</span>(iteration),<span class=\"pl-c1\">str</span>(loss.data[<span class=\"pl-c1\">0</span>]),<span class=\"pl-c1\">str</span>(acc.mean())))\n            iteration<span class=\"pl-k\">+=</span><span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">if</span> (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">%</span>config.val_interval <span class=\"pl-k\">==</span><span class=\"pl-c1\">0</span>:\n            <span class=\"pl-k\">for</span> v,q,a,_ <span class=\"pl-k\">in</span> val:\n                q <span class=\"pl-k\">=</span> Variable(q.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),<span class=\"pl-k\">**</span>val_params)\n                a <span class=\"pl-k\">=</span> Variable(a.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),<span class=\"pl-k\">**</span>val_params)\n                v <span class=\"pl-k\">=</span> Variable(v.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),<span class=\"pl-k\">**</span>val_params)\n                o <span class=\"pl-k\">=</span> model(<span class=\"pl-v\">q</span><span class=\"pl-k\">=</span>q,<span class=\"pl-v\">v</span><span class=\"pl-k\">=</span>v)\n                optimizer.zero_grad()\n                loss <span class=\"pl-k\">=</span>(<span class=\"pl-k\">-</span>o<span class=\"pl-k\">*</span>(a<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span>)).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).mean()\n                loss.backward()\n                optimizer.step()\n                acc <span class=\"pl-k\">=</span> utils.batch_accuracy(o.data,a.data).cpu()\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epoch <span class=\"pl-c1\">%s</span>, validation accuracy <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>(<span class=\"pl-c1\">str</span>(i),<span class=\"pl-c1\">str</span>(acc.mean())))\n            <span class=\"pl-k\">if</span> acc <span class=\"pl-k\">&gt;</span> best_perf:\n                best_perf <span class=\"pl-k\">=</span> acc\n                torch.save(model,<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./best_model.model<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>I try to replace h5py with numpy.load but the system got very slow using numpy load.<br>\nIs there any advice on data loading ?<br>\n<a href=\"https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py\">https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py</a><br>\nI used the code above and get (this is the trace i get using CUDA_LAUNCH_BLOCKING =1 trick):</p>\n<p><code>/pytorch/torch/lib/THC/THCTensorIndex.cu:321: void indexSelectLargeIndex(TensorInfo&lt;T, IndexType&gt;, TensorInfo&lt;T,  IndexType&gt;, TensorInfo&lt;long, IndexType&gt;, int, int, IndexType, IndexType, long) [with T = float, IndexT ype = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2]: block: [57,0,0], thread: [95,0,0] Assertion </code>srcIndex &lt;<br>\nsrcSelectDimSize<code> failed. torch.Size([100, 23, 1024]) THCudaCheck FAIL file=/pytorch/torch/lib/THC/generated/../generic/THCTensorMathReduce.cu line=17 error=59 :  device-side assert triggered Traceback (most recent call last): File \"training.py\", line 39, in &lt;module&gt; o = model(q=q,v=v) File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__ result = self.forward(*input, **kwargs) File \"/home/leon_gao19/10707/10707_Project/model/baseline.py\", line 16, in forward embedding = torch.sum(embeds,dim=1) File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/variable.py\", line 476, in sum return Sum.apply(self, dim, keepdim) File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/_functions/reduce.py\", line 21, in forward return input.sum(dim) RuntimeError: cuda runtime error (59) : device-side assert triggered at /  pytorch/torch/lib/THC/generated/../generic/THCTensorMathReduce.cu:17</code></p>", "body_text": "I am encountering the notorious cuda 59 error.\nThe error occurs after some normal iterations.\nI think I do not have label mismatch problem.\nI am using h5py for reading data to dataloader which was accused of multiprocessing problem previously.\nSetting the workers to 1 and add\nimport torch import torch.multiprocessing torch.multiprocessing.set_start_method('spawn')\ndoes not solve my problem.\nAny replacement I can use other than h5py but preserve the speed or did i use numpy the wrong way?\nThe data loader is for resnet image features and text bow representations.\nI have checked my training code, the batch on cpu at the error batch No. can be seen,\nthe variable just got run after i invoke .cuda()\nfrom model import baseline\nfrom util import data\nfrom util import utils\nfrom util import config\nfrom tqdm import *\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pdb\nimport torch\n\nif __name__==\"__main__\":\n    train = True\n    cudnn.benchmark = True\n    training,train_dict_size =  data.get_loader(train=True,full_batch = False)\n    val,val_dict_size = data.get_loader(val=True,full_batch=True)\n    model = baseline.BOWIMG(config.max_answers,train_dict_size,config.word_embed_dim,config.image_embed_dim,config.batch_size)\n    best_perf = 0.0\n    model.cuda()\n    var_params = {\n        'requires_grad': False,\n\t'volatile': False\n    }\n    val_params = {\n        'requires_grad': False,\n        'volatile': True\n    }\n    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad])\n    print(\"data is fully loaded\")\n    for i in tqdm(range(config.epochs)):\n        iteration = 0\n        for v,q,a,item,_ in training:\n            q = Variable(q.cuda(async=True),**var_params)\n            a = Variable(a.cuda(async=True),**var_params)\n            v = Variable(v.cuda(async=True),**var_params)\n            o = model(q=q,v=v)\n            optimizer.zero_grad()\n            loss =(-o*(a/10)).sum(dim=1).mean() # F.nll_loss(o,a)\n            loss.backward()\n            optimizer.step()\n            acc = utils.batch_accuracy(o.data,a.data).cpu()\n            print(\"epoch %s, iteration %s, loss %s, accuracy %s\" %(str(i),str(iteration),str(loss.data[0]),str(acc.mean())))\n            iteration+=1\n        if (i+1)%config.val_interval ==0:\n            for v,q,a,_ in val:\n                q = Variable(q.cuda(async=True),**val_params)\n                a = Variable(a.cuda(async=True),**val_params)\n                v = Variable(v.cuda(async=True),**val_params)\n                o = model(q=q,v=v)\n                optimizer.zero_grad()\n                loss =(-o*(a/10)).sum(dim=1).mean()\n                loss.backward()\n                optimizer.step()\n                acc = utils.batch_accuracy(o.data,a.data).cpu()\n            print(\"epoch %s, validation accuracy %s\" %(str(i),str(acc.mean())))\n            if acc > best_perf:\n                best_perf = acc\n                torch.save(model,\"./best_model.model\")\nI try to replace h5py with numpy.load but the system got very slow using numpy load.\nIs there any advice on data loading ?\nhttps://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py\nI used the code above and get (this is the trace i get using CUDA_LAUNCH_BLOCKING =1 trick):\n/pytorch/torch/lib/THC/THCTensorIndex.cu:321: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T,  IndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexT ype = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2]: block: [57,0,0], thread: [95,0,0] Assertion srcIndex <\nsrcSelectDimSize failed. torch.Size([100, 23, 1024]) THCudaCheck FAIL file=/pytorch/torch/lib/THC/generated/../generic/THCTensorMathReduce.cu line=17 error=59 :  device-side assert triggered Traceback (most recent call last): File \"training.py\", line 39, in <module> o = model(q=q,v=v) File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__ result = self.forward(*input, **kwargs) File \"/home/leon_gao19/10707/10707_Project/model/baseline.py\", line 16, in forward embedding = torch.sum(embeds,dim=1) File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/variable.py\", line 476, in sum return Sum.apply(self, dim, keepdim) File \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/_functions/reduce.py\", line 21, in forward return input.sum(dim) RuntimeError: cuda runtime error (59) : device-side assert triggered at /  pytorch/torch/lib/THC/generated/../generic/THCTensorMathReduce.cu:17", "body": "I am encountering the notorious cuda 59 error.\r\nThe error occurs after some normal iterations.\r\nI think I do not have label mismatch problem.\r\nI am using h5py for reading data to dataloader which was accused of multiprocessing problem previously.\r\nSetting the workers to 1 and add\r\nimport torch import torch.multiprocessing torch.multiprocessing.set_start_method('spawn')\r\ndoes not solve my problem.\r\nAny replacement I can use other than h5py but preserve the speed or did i use numpy the wrong way?\r\nThe data loader is for resnet image features and text bow representations.\r\nI have checked my training code, the batch on cpu at the error batch No. can be seen, \r\nthe variable just got run after i invoke .cuda()\r\n\r\n```python\r\nfrom model import baseline\r\nfrom util import data\r\nfrom util import utils\r\nfrom util import config\r\nfrom tqdm import *\r\nimport torch.backends.cudnn as cudnn\r\nfrom torch.autograd import Variable\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nimport pdb\r\nimport torch\r\n\r\nif __name__==\"__main__\":\r\n    train = True\r\n    cudnn.benchmark = True\r\n    training,train_dict_size =  data.get_loader(train=True,full_batch = False)\r\n    val,val_dict_size = data.get_loader(val=True,full_batch=True)\r\n    model = baseline.BOWIMG(config.max_answers,train_dict_size,config.word_embed_dim,config.image_embed_dim,config.batch_size)\r\n    best_perf = 0.0\r\n    model.cuda()\r\n    var_params = {\r\n        'requires_grad': False,\r\n\t'volatile': False\r\n    }\r\n    val_params = {\r\n        'requires_grad': False,\r\n        'volatile': True\r\n    }\r\n    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad])\r\n    print(\"data is fully loaded\")\r\n    for i in tqdm(range(config.epochs)):\r\n        iteration = 0\r\n        for v,q,a,item,_ in training:\r\n            q = Variable(q.cuda(async=True),**var_params)\r\n            a = Variable(a.cuda(async=True),**var_params)\r\n            v = Variable(v.cuda(async=True),**var_params)\r\n            o = model(q=q,v=v)\r\n            optimizer.zero_grad()\r\n            loss =(-o*(a/10)).sum(dim=1).mean() # F.nll_loss(o,a)\r\n            loss.backward()\r\n            optimizer.step()\r\n            acc = utils.batch_accuracy(o.data,a.data).cpu()\r\n            print(\"epoch %s, iteration %s, loss %s, accuracy %s\" %(str(i),str(iteration),str(loss.data[0]),str(acc.mean())))\r\n            iteration+=1\r\n        if (i+1)%config.val_interval ==0:\r\n            for v,q,a,_ in val:\r\n                q = Variable(q.cuda(async=True),**val_params)\r\n                a = Variable(a.cuda(async=True),**val_params)\r\n                v = Variable(v.cuda(async=True),**val_params)\r\n                o = model(q=q,v=v)\r\n                optimizer.zero_grad()\r\n                loss =(-o*(a/10)).sum(dim=1).mean()\r\n                loss.backward()\r\n                optimizer.step()\r\n                acc = utils.batch_accuracy(o.data,a.data).cpu()\r\n            print(\"epoch %s, validation accuracy %s\" %(str(i),str(acc.mean())))\r\n            if acc > best_perf:\r\n                best_perf = acc\r\n                torch.save(model,\"./best_model.model\")\r\n```\r\n\r\nI try to replace h5py with numpy.load but the system got very slow using numpy load.\r\nIs there any advice on data loading ?\r\nhttps://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py\r\nI used the code above and get (this is the trace i get using CUDA_LAUNCH_BLOCKING =1 trick):\r\n\r\n`/pytorch/torch/lib/THC/THCTensorIndex.cu:321: void indexSelectLargeIndex(TensorInfo<T, IndexType>, TensorInfo<T, \r\nIndexType>, TensorInfo<long, IndexType>, int, int, IndexType, IndexType, long) [with T = float, IndexT\r\nype = unsigned int, DstDim = 2, SrcDim = 2, IdxDim = -2]: block: [57,0,0], thread: [95,0,0] Assertion `srcIndex < \r\nsrcSelectDimSize` failed.\r\ntorch.Size([100, 23, 1024])\r\nTHCudaCheck FAIL file=/pytorch/torch/lib/THC/generated/../generic/THCTensorMathReduce.cu line=17 error=59 : \r\ndevice-side assert triggered\r\nTraceback (most recent call last):\r\nFile \"training.py\", line 39, in <module>\r\n  o = model(q=q,v=v)\r\nFile \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n  result = self.forward(*input, **kwargs)\r\nFile \"/home/leon_gao19/10707/10707_Project/model/baseline.py\", line 16, in forward\r\n  embedding = torch.sum(embeds,dim=1)\r\nFile \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/variable.py\", line 476, in sum\r\n  return Sum.apply(self, dim, keepdim)\r\nFile \"/home/cs231n/myVE35/lib/python3.5/site-packages/torch/autograd/_functions/reduce.py\", line 21, in forward\r\n  return input.sum(dim)\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at / \r\npytorch/torch/lib/THC/generated/../generic/THCTensorMathReduce.cu:17`"}