{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10581", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10581/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10581/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10581/events", "html_url": "https://github.com/pytorch/pytorch/pull/10581", "id": 351229286, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA4ODc3OTM0", "number": 10581, "title": "Remove all cuDNN specific inputs to RNN functions", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-16T14:25:56Z", "updated_at": "2018-11-23T15:49:36Z", "closed_at": "2018-08-17T18:10:57Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10581", "html_url": "https://github.com/pytorch/pytorch/pull/10581", "diff_url": "https://github.com/pytorch/pytorch/pull/10581.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10581.patch"}, "body_html": "<p>This is still not the final PR, but it removes all blockers for actually using the RNN functions directly in the JIT. Next patch should be final, and will actually remove the symbolic_override code, and change it to proper symbolics for those ATen functions. Turns out the symbolic code can be also cleaned up a bit, and I'll do that too.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a><br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> (for minor DispatchStub.h) changes</p>\n<h4>Commit message:</h4>\n<p>There was no way to handle those in the JIT for now, and they turned<br>\nout to be completely unnecessary. It should make the Python and C++<br>\nmodule code much simpler too, since all the logic is now centralized<br>\nin the native functions.</p>\n<p>The downside is that RNN modules no longer own their dropout buffers,<br>\nwhich are shared per-device instead (with appropriate locking and<br>\nsynchronization). This might appear as a perf regression at first, but<br>\nin reality it's highly unlikely that anyone will want to run cuDNN RNNs<br>\non the same GPU in parallel.</p>", "body_text": "This is still not the final PR, but it removes all blockers for actually using the RNN functions directly in the JIT. Next patch should be final, and will actually remove the symbolic_override code, and change it to proper symbolics for those ATen functions. Turns out the symbolic code can be also cleaned up a bit, and I'll do that too.\n@zdevito @ezyang\n@colesbury (for minor DispatchStub.h) changes\nCommit message:\nThere was no way to handle those in the JIT for now, and they turned\nout to be completely unnecessary. It should make the Python and C++\nmodule code much simpler too, since all the logic is now centralized\nin the native functions.\nThe downside is that RNN modules no longer own their dropout buffers,\nwhich are shared per-device instead (with appropriate locking and\nsynchronization). This might appear as a perf regression at first, but\nin reality it's highly unlikely that anyone will want to run cuDNN RNNs\non the same GPU in parallel.", "body": "This is still not the final PR, but it removes all blockers for actually using the RNN functions directly in the JIT. Next patch should be final, and will actually remove the symbolic_override code, and change it to proper symbolics for those ATen functions. Turns out the symbolic code can be also cleaned up a bit, and I'll do that too.\r\n\r\n@zdevito @ezyang \r\n@colesbury (for minor DispatchStub.h) changes\r\n\r\n#### Commit message:\r\n\r\nThere was no way to handle those in the JIT for now, and they turned\r\nout to be completely unnecessary. It should make the Python and C++\r\nmodule code much simpler too, since all the logic is now centralized\r\nin the native functions.\r\n\r\nThe downside is that RNN modules no longer own their dropout buffers,\r\nwhich are shared per-device instead (with appropriate locking and\r\nsynchronization). This might appear as a perf regression at first, but\r\nin reality it's highly unlikely that anyone will want to run cuDNN RNNs\r\non the same GPU in parallel."}