{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4651", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4651/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4651/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4651/events", "html_url": "https://github.com/pytorch/pytorch/issues/4651", "id": 288304354, "node_id": "MDU6SXNzdWUyODgzMDQzNTQ=", "number": 4651, "title": "distributed pytorch in cluster", "user": {"login": "zzy123abc", "id": 22336471, "node_id": "MDQ6VXNlcjIyMzM2NDcx", "avatar_url": "https://avatars0.githubusercontent.com/u/22336471?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zzy123abc", "html_url": "https://github.com/zzy123abc", "followers_url": "https://api.github.com/users/zzy123abc/followers", "following_url": "https://api.github.com/users/zzy123abc/following{/other_user}", "gists_url": "https://api.github.com/users/zzy123abc/gists{/gist_id}", "starred_url": "https://api.github.com/users/zzy123abc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zzy123abc/subscriptions", "organizations_url": "https://api.github.com/users/zzy123abc/orgs", "repos_url": "https://api.github.com/users/zzy123abc/repos", "events_url": "https://api.github.com/users/zzy123abc/events{/privacy}", "received_events_url": "https://api.github.com/users/zzy123abc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-01-13T05:26:39Z", "updated_at": "2018-01-13T07:09:41Z", "closed_at": "2018-01-13T07:09:41Z", "author_association": "NONE", "body_html": "<p>How can I launch a  distributed pytorch code in the cluster which has two nodes?</p>\n<p>Suppose each node has only one gpu.</p>\n<p>def init_processes(rank, size, fn, backend='gloo'):<br>\ndist.init_process_group(backend,init_method='tcp://172.16.1.186:2222', rank=rank,world_size=size)<br>\nfn(rank, size)</p>\n<p>if <strong>name</strong> == \"<strong>main</strong>\":<br>\nsize = 2<br>\nprocesses = []<br>\nfor rank in range(size):<br>\np = Process(target=init_processes, args=(rank, size, run))<br>\np.start()<br>\nprocesses.append(p)</p>\n<p>for p in processes:<br>\np.join()</p>\n<p>This is part of my code.I run it.However,the result is only run in the one node when I qsub the run command in the two nodes.</p>\n<p>The part of pbs command is nodes=gpu06:ppn=24+gpu07:ppn=24.</p>\n<p>The run command is python train_dist.py.</p>\n<p>Is there something wrong?Thanks for your help.</p>", "body_text": "How can I launch a  distributed pytorch code in the cluster which has two nodes?\nSuppose each node has only one gpu.\ndef init_processes(rank, size, fn, backend='gloo'):\ndist.init_process_group(backend,init_method='tcp://172.16.1.186:2222', rank=rank,world_size=size)\nfn(rank, size)\nif name == \"main\":\nsize = 2\nprocesses = []\nfor rank in range(size):\np = Process(target=init_processes, args=(rank, size, run))\np.start()\nprocesses.append(p)\nfor p in processes:\np.join()\nThis is part of my code.I run it.However,the result is only run in the one node when I qsub the run command in the two nodes.\nThe part of pbs command is nodes=gpu06:ppn=24+gpu07:ppn=24.\nThe run command is python train_dist.py.\nIs there something wrong?Thanks for your help.", "body": "How can I launch a  distributed pytorch code in the cluster which has two nodes?\r\n\r\nSuppose each node has only one gpu.\r\n\r\ndef init_processes(rank, size, fn, backend='gloo'):\r\n    dist.init_process_group(backend,init_method='tcp://172.16.1.186:2222', rank=rank,world_size=size)\r\n    fn(rank, size)\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_processes, args=(rank, size, run))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n   for p in processes:\r\n        p.join()\r\n\r\nThis is part of my code.I run it.However,the result is only run in the one node when I qsub the run command in the two nodes.\r\n\r\nThe part of pbs command is nodes=gpu06:ppn=24+gpu07:ppn=24.\r\n\r\nThe run command is python train_dist.py.\r\n\r\nIs there something wrong?Thanks for your help."}