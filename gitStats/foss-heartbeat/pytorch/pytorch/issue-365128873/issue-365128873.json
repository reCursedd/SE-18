{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12198", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12198/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12198/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12198/events", "html_url": "https://github.com/pytorch/pytorch/issues/12198", "id": 365128873, "node_id": "MDU6SXNzdWUzNjUxMjg4NzM=", "number": 12198, "title": "[Feature Request]Synchronized batch norm", "user": {"login": "labor00", "id": 36332518, "node_id": "MDQ6VXNlcjM2MzMyNTE4", "avatar_url": "https://avatars3.githubusercontent.com/u/36332518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/labor00", "html_url": "https://github.com/labor00", "followers_url": "https://api.github.com/users/labor00/followers", "following_url": "https://api.github.com/users/labor00/following{/other_user}", "gists_url": "https://api.github.com/users/labor00/gists{/gist_id}", "starred_url": "https://api.github.com/users/labor00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/labor00/subscriptions", "organizations_url": "https://api.github.com/users/labor00/orgs", "repos_url": "https://api.github.com/users/labor00/repos", "events_url": "https://api.github.com/users/labor00/events{/privacy}", "received_events_url": "https://api.github.com/users/labor00/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-29T12:39:27Z", "updated_at": "2018-10-01T17:24:16Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>Synchronized batch norm. Batch norm operation that synchronizes batch statistics across devices.</p>\n<h2>Motivation</h2>\n<p>Many tasks specially ones using FCN's for  instance/semantic segmentation requires a significant amount of GPU memory. Distributing the batch elements across devices is a common way to train these models. However due to the lack of synchronization of batch statistics this procedure can run into problems.<br>\nThere is at least one issue previously opened (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"254261067\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2584\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2584/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2584\">#2584</a>) about this feature.</p>\n<h2>Alternatives</h2>\n<p>There are also some (and maybe others) community options for instance:</p>\n<p><a href=\"https://github.com/mapillary/inplace_abn\">inplace_abn</a>:  This repository offers synchronized  inplace ABN to reduce memory footprint (combines both batch norm and activation in a single node). However the operator fusion procedure restrict the type of activation used, for instance is not possible to use ReLU activation. (Currently broken with pytorch master).</p>\n<p><a href=\"https://hangzhang.org/PyTorch-Encoding/nn.html#batchnorm2d\" rel=\"nofollow\">encoding</a>: Python only synchronized batch norm operation.</p>\n<p>There are also other options for instance using <a href=\"https://arxiv.org/abs/1803.08494\" rel=\"nofollow\">group norm</a> but this is not always possible or desirable. For instance most of the pretrained models used as encoder on these problems only uses batch norm.</p>", "body_text": "\ud83d\ude80 Feature\nSynchronized batch norm. Batch norm operation that synchronizes batch statistics across devices.\nMotivation\nMany tasks specially ones using FCN's for  instance/semantic segmentation requires a significant amount of GPU memory. Distributing the batch elements across devices is a common way to train these models. However due to the lack of synchronization of batch statistics this procedure can run into problems.\nThere is at least one issue previously opened (#2584) about this feature.\nAlternatives\nThere are also some (and maybe others) community options for instance:\ninplace_abn:  This repository offers synchronized  inplace ABN to reduce memory footprint (combines both batch norm and activation in a single node). However the operator fusion procedure restrict the type of activation used, for instance is not possible to use ReLU activation. (Currently broken with pytorch master).\nencoding: Python only synchronized batch norm operation.\nThere are also other options for instance using group norm but this is not always possible or desirable. For instance most of the pretrained models used as encoder on these problems only uses batch norm.", "body": "## \ud83d\ude80 Feature\r\nSynchronized batch norm. Batch norm operation that synchronizes batch statistics across devices.\r\n\r\n## Motivation\r\nMany tasks specially ones using FCN's for  instance/semantic segmentation requires a significant amount of GPU memory. Distributing the batch elements across devices is a common way to train these models. However due to the lack of synchronization of batch statistics this procedure can run into problems.\r\nThere is at least one issue previously opened (#2584) about this feature.\r\n\r\n## Alternatives\r\nThere are also some (and maybe others) community options for instance:\r\n\r\n[inplace_abn](https://github.com/mapillary/inplace_abn):  This repository offers synchronized  inplace ABN to reduce memory footprint (combines both batch norm and activation in a single node). However the operator fusion procedure restrict the type of activation used, for instance is not possible to use ReLU activation. (Currently broken with pytorch master).\r\n\r\n[encoding](https://hangzhang.org/PyTorch-Encoding/nn.html#batchnorm2d): Python only synchronized batch norm operation.\r\n\r\nThere are also other options for instance using [group norm](https://arxiv.org/abs/1803.08494) but this is not always possible or desirable. For instance most of the pretrained models used as encoder on these problems only uses batch norm.\r\n"}