{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164199151", "pull_request_review_id": 91941507, "id": 164199151, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDE5OTE1MQ==", "diff_hunk": "@@ -80,3 +80,12 @@ def entropy(self):\n         return (torch.lgamma(self.concentration).sum(-1) - torch.lgamma(a0) -\n                 (k - a0) * torch.digamma(a0) -\n                 ((self.concentration - 1.0) * torch.digamma(self.concentration)).sum(-1))\n+\n+    @lazy_property\n+    def natural_params(self):\n+        V1 = Variable(self.concentration.data, requires_grad=True)\n+        return (V1, )", "path": "torch/distributions/dirichlet.py", "position": null, "original_position": 28, "commit_id": "fb1d0870e50aa868b9e94112ed29163780015770", "original_commit_id": "3344066bc4b627358f6bbd8401d652aaaf37aba7", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "Regardless of whether the user wants to take grads, `.entropy()` and `.kl_divergence()` need to use `grad()` under the hood; that is why we are wrapping this with `requires_grad=True`. That is, even if the user does not want to compute gradients of `Dirichlet(concentration).entropy()`, the Bregman divergence implementation uses `grad()` under the hood and must use a differentiable form of `concentration`, i.e. even if the result of `.entropy()` is not differentiable wrt the user's input. See implementation of `ExponentialFamily.entropy()` for clarification.", "created_at": "2018-01-26T19:18:32Z", "updated_at": "2018-11-23T15:38:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/4876#discussion_r164199151", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4876", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164199151"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4876#discussion_r164199151"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4876"}}, "body_html": "<p>Regardless of whether the user wants to take grads, <code>.entropy()</code> and <code>.kl_divergence()</code> need to use <code>grad()</code> under the hood; that is why we are wrapping this with <code>requires_grad=True</code>. That is, even if the user does not want to compute gradients of <code>Dirichlet(concentration).entropy()</code>, the Bregman divergence implementation uses <code>grad()</code> under the hood and must use a differentiable form of <code>concentration</code>, i.e. even if the result of <code>.entropy()</code> is not differentiable wrt the user's input. See implementation of <code>ExponentialFamily.entropy()</code> for clarification.</p>", "body_text": "Regardless of whether the user wants to take grads, .entropy() and .kl_divergence() need to use grad() under the hood; that is why we are wrapping this with requires_grad=True. That is, even if the user does not want to compute gradients of Dirichlet(concentration).entropy(), the Bregman divergence implementation uses grad() under the hood and must use a differentiable form of concentration, i.e. even if the result of .entropy() is not differentiable wrt the user's input. See implementation of ExponentialFamily.entropy() for clarification.", "in_reply_to_id": 164192173}