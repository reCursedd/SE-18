{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164194860", "pull_request_review_id": 91936425, "id": 164194860, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDE5NDg2MA==", "diff_hunk": "@@ -80,3 +80,12 @@ def entropy(self):\n         return (torch.lgamma(self.concentration).sum(-1) - torch.lgamma(a0) -\n                 (k - a0) * torch.digamma(a0) -\n                 ((self.concentration - 1.0) * torch.digamma(self.concentration)).sum(-1))\n+\n+    @lazy_property\n+    def natural_params(self):\n+        V1 = Variable(self.concentration.data, requires_grad=True)\n+        return (V1, )", "path": "torch/distributions/dirichlet.py", "position": null, "original_position": 28, "commit_id": "fb1d0870e50aa868b9e94112ed29163780015770", "original_commit_id": "3344066bc4b627358f6bbd8401d652aaaf37aba7", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "I suppose these could be private, since I don't know of an application outside of `.entropy()` and `.kl_divergence()`; however there may be future applications.\r\n\r\nThe `.natural_params` need to set `requires_grad=True` internally because `.entropy()` and `.kl_divergence()` both use `grad()` under the hood, even if the original variables do not require grad. Would you recommend wrapping in `Variable(-, requires_grad=True)` only in the case that the original variables do not require grad? Or is there a better practice we can follow?", "created_at": "2018-01-26T19:01:54Z", "updated_at": "2018-11-23T15:38:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/4876#discussion_r164194860", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4876", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164194860"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4876#discussion_r164194860"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4876"}}, "body_html": "<p>I suppose these could be private, since I don't know of an application outside of <code>.entropy()</code> and <code>.kl_divergence()</code>; however there may be future applications.</p>\n<p>The <code>.natural_params</code> need to set <code>requires_grad=True</code> internally because <code>.entropy()</code> and <code>.kl_divergence()</code> both use <code>grad()</code> under the hood, even if the original variables do not require grad. Would you recommend wrapping in <code>Variable(-, requires_grad=True)</code> only in the case that the original variables do not require grad? Or is there a better practice we can follow?</p>", "body_text": "I suppose these could be private, since I don't know of an application outside of .entropy() and .kl_divergence(); however there may be future applications.\nThe .natural_params need to set requires_grad=True internally because .entropy() and .kl_divergence() both use grad() under the hood, even if the original variables do not require grad. Would you recommend wrapping in Variable(-, requires_grad=True) only in the case that the original variables do not require grad? Or is there a better practice we can follow?", "in_reply_to_id": 164192173}