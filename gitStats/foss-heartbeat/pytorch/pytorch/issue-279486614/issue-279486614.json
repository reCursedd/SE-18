{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4037", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4037/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4037/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4037/events", "html_url": "https://github.com/pytorch/pytorch/issues/4037", "id": 279486614, "node_id": "MDU6SXNzdWUyNzk0ODY2MTQ=", "number": 4037, "title": "Feature request: allow to distribute load unevenly in DataParallel", "user": {"login": "kmike", "id": 107893, "node_id": "MDQ6VXNlcjEwNzg5Mw==", "avatar_url": "https://avatars3.githubusercontent.com/u/107893?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kmike", "html_url": "https://github.com/kmike", "followers_url": "https://api.github.com/users/kmike/followers", "following_url": "https://api.github.com/users/kmike/following{/other_user}", "gists_url": "https://api.github.com/users/kmike/gists{/gist_id}", "starred_url": "https://api.github.com/users/kmike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kmike/subscriptions", "organizations_url": "https://api.github.com/users/kmike/orgs", "repos_url": "https://api.github.com/users/kmike/repos", "events_url": "https://api.github.com/users/kmike/events{/privacy}", "received_events_url": "https://api.github.com/users/kmike/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-05T18:44:36Z", "updated_at": "2017-12-05T19:44:41Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>It is not uncommon to have 2 GPUs: one, beefy, for training, and another, cheaper one, for validation / experiments. In this setup DataParallel is not very helpful if you want to train a single model faster: a less performant GPU with less RAM can make things slower instead of speeding them up.</p>\n<p>What do you think about adding an option to DataParallel to distribute load unevenly? It can be simply an option to change ratio of data sent to each GPU.</p>\n<p>See also (thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7967030\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/geffy\">@geffy</a> for the links):</p>\n<ul>\n<li><a href=\"https://discuss.pytorch.org/t/single-node-multi-gpu-with-different-cards/1575\" rel=\"nofollow\">https://discuss.pytorch.org/t/single-node-multi-gpu-with-different-cards/1575</a></li>\n<li><a href=\"http://mxnet-tqchen.readthedocs.io/en/latest/how_to/multi_devices.html#advanced-usage\" rel=\"nofollow\">http://mxnet-tqchen.readthedocs.io/en/latest/how_to/multi_devices.html#advanced-usage</a></li>\n</ul>", "body_text": "It is not uncommon to have 2 GPUs: one, beefy, for training, and another, cheaper one, for validation / experiments. In this setup DataParallel is not very helpful if you want to train a single model faster: a less performant GPU with less RAM can make things slower instead of speeding them up.\nWhat do you think about adding an option to DataParallel to distribute load unevenly? It can be simply an option to change ratio of data sent to each GPU.\nSee also (thanks @geffy for the links):\n\nhttps://discuss.pytorch.org/t/single-node-multi-gpu-with-different-cards/1575\nhttp://mxnet-tqchen.readthedocs.io/en/latest/how_to/multi_devices.html#advanced-usage", "body": "It is not uncommon to have 2 GPUs: one, beefy, for training, and another, cheaper one, for validation / experiments. In this setup DataParallel is not very helpful if you want to train a single model faster: a less performant GPU with less RAM can make things slower instead of speeding them up.\r\n\r\nWhat do you think about adding an option to DataParallel to distribute load unevenly? It can be simply an option to change ratio of data sent to each GPU. \r\n\r\nSee also (thanks @geffy for the links): \r\n\r\n* https://discuss.pytorch.org/t/single-node-multi-gpu-with-different-cards/1575 \r\n* http://mxnet-tqchen.readthedocs.io/en/latest/how_to/multi_devices.html#advanced-usage\r\n"}