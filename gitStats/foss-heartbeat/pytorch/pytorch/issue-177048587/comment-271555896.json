{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/271555896", "html_url": "https://github.com/pytorch/pytorch/issues/28#issuecomment-271555896", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/28", "id": 271555896, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTU1NTg5Ng==", "user": {"login": "andreaskoepf", "id": 9976399, "node_id": "MDQ6VXNlcjk5NzYzOTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/9976399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andreaskoepf", "html_url": "https://github.com/andreaskoepf", "followers_url": "https://api.github.com/users/andreaskoepf/followers", "following_url": "https://api.github.com/users/andreaskoepf/following{/other_user}", "gists_url": "https://api.github.com/users/andreaskoepf/gists{/gist_id}", "starred_url": "https://api.github.com/users/andreaskoepf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andreaskoepf/subscriptions", "organizations_url": "https://api.github.com/users/andreaskoepf/orgs", "repos_url": "https://api.github.com/users/andreaskoepf/repos", "events_url": "https://api.github.com/users/andreaskoepf/events{/privacy}", "received_events_url": "https://api.github.com/users/andreaskoepf/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-10T11:48:43Z", "updated_at": "2017-01-10T11:48:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It would be great to support basic cross-type auto-converting arithmetic operations for floating point types as well.</p>\n<p>Copy/Assignment operations are already supported (VERY permissive):</p>\n<pre><code>f = torch.FloatTensor(5)\nd = torch.DoubleTensor(5).fill_(float('nan')) # fill with nan to challenge double-&gt;int...\ni = torch.IntTensor(5)\nd.copy_(f)\nf.copy_(d)    # ok, maybe I accept this as explicit cast...\nf[:] = d[:]   # implicit conversion to smaller type? data-scientists world...\nd[:] = f[:]\ni[:] = d[:]   # wow, even double to int without complaints...\n</code></pre>\n<p>Things become strange/normal (depending on whether you are data or computer scientist) when you try to assign a SINGLE float to an int:</p>\n<pre><code>&gt;&gt;&gt; i[1] = d[1]\nRuntimeError: can't assign a float to a scalar value of type int\n</code></pre>\n<p>A nice feature I would like to see in the future are relatively save cross-type arithmetic ops for +,-,*,/ ... e.g.</p>\n<pre><code>r = d[:] + f[:]\n# currently only possible with explicit conversion (-&gt; additional copy), e.g.\nr = d[:] + f[:].double()\n</code></pre>\n<p>Currently yields:</p>\n<pre><code>    return self.add(other)\nTypeError: add received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor)\n * (torch.DoubleTensor other)\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor)\n * (float value, torch.DoubleTensor other)\n * (torch.DoubleTensor mat1, torch.SparseDoubleTensor mat2)\n * (torch.DoubleTensor mat1, float value, torch.SparseDoubleTensor mat2)\n</code></pre>", "body_text": "It would be great to support basic cross-type auto-converting arithmetic operations for floating point types as well.\nCopy/Assignment operations are already supported (VERY permissive):\nf = torch.FloatTensor(5)\nd = torch.DoubleTensor(5).fill_(float('nan')) # fill with nan to challenge double->int...\ni = torch.IntTensor(5)\nd.copy_(f)\nf.copy_(d)    # ok, maybe I accept this as explicit cast...\nf[:] = d[:]   # implicit conversion to smaller type? data-scientists world...\nd[:] = f[:]\ni[:] = d[:]   # wow, even double to int without complaints...\n\nThings become strange/normal (depending on whether you are data or computer scientist) when you try to assign a SINGLE float to an int:\n>>> i[1] = d[1]\nRuntimeError: can't assign a float to a scalar value of type int\n\nA nice feature I would like to see in the future are relatively save cross-type arithmetic ops for +,-,*,/ ... e.g.\nr = d[:] + f[:]\n# currently only possible with explicit conversion (-> additional copy), e.g.\nr = d[:] + f[:].double()\n\nCurrently yields:\n    return self.add(other)\nTypeError: add received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor)\n * (torch.DoubleTensor other)\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor)\n * (float value, torch.DoubleTensor other)\n * (torch.DoubleTensor mat1, torch.SparseDoubleTensor mat2)\n * (torch.DoubleTensor mat1, float value, torch.SparseDoubleTensor mat2)", "body": "It would be great to support basic cross-type auto-converting arithmetic operations for floating point types as well.\r\n\r\nCopy/Assignment operations are already supported (VERY permissive):\r\n\r\n```\r\nf = torch.FloatTensor(5)\r\nd = torch.DoubleTensor(5).fill_(float('nan')) # fill with nan to challenge double->int...\r\ni = torch.IntTensor(5)\r\nd.copy_(f)\r\nf.copy_(d)    # ok, maybe I accept this as explicit cast...\r\nf[:] = d[:]   # implicit conversion to smaller type? data-scientists world...\r\nd[:] = f[:]\r\ni[:] = d[:]   # wow, even double to int without complaints...\r\n```\r\n\r\nThings become strange/normal (depending on whether you are data or computer scientist) when you try to assign a SINGLE float to an int:\r\n\r\n```\r\n>>> i[1] = d[1]\r\nRuntimeError: can't assign a float to a scalar value of type int\r\n```\r\n\r\nA nice feature I would like to see in the future are relatively save cross-type arithmetic ops for +,-,*,/ ... e.g.\r\n\r\n```\r\nr = d[:] + f[:]\r\n# currently only possible with explicit conversion (-> additional copy), e.g.\r\nr = d[:] + f[:].double()\r\n```\r\n\r\nCurrently yields:\r\n```\r\n    return self.add(other)\r\nTypeError: add received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\r\n * (float value)\r\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor)\r\n * (torch.DoubleTensor other)\r\n      didn't match because some of the arguments have invalid types: (torch.FloatTensor)\r\n * (float value, torch.DoubleTensor other)\r\n * (torch.DoubleTensor mat1, torch.SparseDoubleTensor mat2)\r\n * (torch.DoubleTensor mat1, float value, torch.SparseDoubleTensor mat2)\r\n```"}