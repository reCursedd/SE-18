{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407148013", "html_url": "https://github.com/pytorch/pytorch/issues/9681#issuecomment-407148013", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9681", "id": 407148013, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzE0ODAxMw==", "user": {"login": "AntoinePrv", "id": 11088808, "node_id": "MDQ6VXNlcjExMDg4ODA4", "avatar_url": "https://avatars0.githubusercontent.com/u/11088808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AntoinePrv", "html_url": "https://github.com/AntoinePrv", "followers_url": "https://api.github.com/users/AntoinePrv/followers", "following_url": "https://api.github.com/users/AntoinePrv/following{/other_user}", "gists_url": "https://api.github.com/users/AntoinePrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/AntoinePrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AntoinePrv/subscriptions", "organizations_url": "https://api.github.com/users/AntoinePrv/orgs", "repos_url": "https://api.github.com/users/AntoinePrv/repos", "events_url": "https://api.github.com/users/AntoinePrv/events{/privacy}", "received_events_url": "https://api.github.com/users/AntoinePrv/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-23T18:02:21Z", "updated_at": "2018-07-23T18:02:21Z", "author_association": "NONE", "body_html": "<p>That's nice.<br>\nFor people looking for a quick fix in this particular case, I found the following to be the simpler to implement. Simply set the zero-length to one and process one time step of padded zeros.<br>\nIt's a small extra computation and does not change the training if you apply a mask before the loss.</p>", "body_text": "That's nice.\nFor people looking for a quick fix in this particular case, I found the following to be the simpler to implement. Simply set the zero-length to one and process one time step of padded zeros.\nIt's a small extra computation and does not change the training if you apply a mask before the loss.", "body": "That's nice.\r\nFor people looking for a quick fix in this particular case, I found the following to be the simpler to implement. Simply set the zero-length to one and process one time step of padded zeros.\r\nIt's a small extra computation and does not change the training if you apply a mask before the loss."}