{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434839274", "html_url": "https://github.com/pytorch/pytorch/pull/13399#issuecomment-434839274", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13399", "id": 434839274, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDgzOTI3NA==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T20:38:22Z", "updated_at": "2018-10-31T20:38:22Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9796\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/resistor\">@resistor</a> yeah, <code>minimum</code> and <code>maximum</code> seem like reasonable names as they propagate NaNs in IEEE draft and also <a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html\" rel=\"nofollow\">NumPy</a>.</p>\n<p>Mostly, I'm thinking of operations that need to be written for both scalar types (e.g. float) and the vector types:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/f9c0a08eed723ac14378422f066e6ee1be3854a2/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L20-L21\">pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 20 to 21\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/f9c0a08eed723ac14378422f066e6ee1be3854a2\">f9c0a08</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L20\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"20\"></td>\n          <td id=\"LC20\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> [=](<span class=\"pl-c1\">scalar_t</span> a, <span class=\"pl-c1\">scalar_t</span> b) -&gt; <span class=\"pl-c1\">scalar_t</span> { <span class=\"pl-k\">return</span> a + b; }, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L21\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"21\"></td>\n          <td id=\"LC21\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> [=](Vec256&lt;<span class=\"pl-c1\">scalar_t</span>&gt; a, Vec256&lt;<span class=\"pl-c1\">scalar_t</span>&gt; b) { <span class=\"pl-k\">return</span> a + b; }); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Along these lines, it may also be worth adding a implementation for scalar types. Something like:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">class</span> <span class=\"pl-en\">T</span>&gt; \n<span class=\"pl-k\">inline</span> T <span class=\"pl-en\">minimum</span>(<span class=\"pl-k\">const</span> T&amp; a, <span class=\"pl-k\">const</span> T&amp; b) { ... }</pre></div>\n<p>With the idea being that one could call vec256::minimum(float, float) and vec256::minimum(Vec256, Vec256) and it would perform the same operation.</p>\n<p>An example of this is <a href=\"https://github.com/pytorch/pytorch/blob/f9c0a08eed723ac14378422f066e6ee1be3854a2/aten/src/ATen/cpu/vec256/vec256_base.h#L310-L313\"><code>fmadd</code></a></p>", "body_text": "@resistor yeah, minimum and maximum seem like reasonable names as they propagate NaNs in IEEE draft and also NumPy.\nMostly, I'm thinking of operations that need to be written for both scalar types (e.g. float) and the vector types:\n\n  \n    \n      pytorch/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp\n    \n    \n        Lines 20 to 21\n      in\n      f9c0a08\n    \n    \n    \n    \n\n        \n          \n           [=](scalar_t a, scalar_t b) -> scalar_t { return a + b; }, \n        \n\n        \n          \n           [=](Vec256<scalar_t> a, Vec256<scalar_t> b) { return a + b; }); \n        \n    \n  \n\n\nAlong these lines, it may also be worth adding a implementation for scalar types. Something like:\ntemplate <class T> \ninline T minimum(const T& a, const T& b) { ... }\nWith the idea being that one could call vec256::minimum(float, float) and vec256::minimum(Vec256, Vec256) and it would perform the same operation.\nAn example of this is fmadd", "body": "@resistor yeah, `minimum` and `maximum` seem like reasonable names as they propagate NaNs in IEEE draft and also [NumPy](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html).\r\n\r\nMostly, I'm thinking of operations that need to be written for both scalar types (e.g. float) and the vector types:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f9c0a08eed723ac14378422f066e6ee1be3854a2/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L20-L21\r\n\r\nAlong these lines, it may also be worth adding a implementation for scalar types. Something like:\r\n\r\n```c++\r\ntemplate <class T> \r\ninline T minimum(const T& a, const T& b) { ... }\r\n```\r\n\r\nWith the idea being that one could call vec256::minimum(float, float) and vec256::minimum(Vec256<float>, Vec256<float>) and it would perform the same operation.\r\n\r\nAn example of this is [`fmadd`](https://github.com/pytorch/pytorch/blob/f9c0a08eed723ac14378422f066e6ee1be3854a2/aten/src/ATen/cpu/vec256/vec256_base.h#L310-L313)"}