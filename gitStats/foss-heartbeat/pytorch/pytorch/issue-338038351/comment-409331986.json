{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409331986", "html_url": "https://github.com/pytorch/pytorch/issues/9146#issuecomment-409331986", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9146", "id": 409331986, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTMzMTk4Ng==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T19:01:33Z", "updated_at": "2018-07-31T19:01:33Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13595236\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jfolt\">@jfolt</a> the two results are absolutely not equal, in any scientific computing sense.</p>\n<p>What you are seeing is floating point precision playing with your intuition. You have an <code>inp</code> that contains a range between <code>0 to 1399</code>, which covers a bunch of exponent range. Then, you are doing a <code>W * X + b</code> operation here, where W is quite small (mean = 0.0002). You are doing a lot of multiplying large number with small number and adding another small number.<br>\nIn such cases, the ordering of the operations affects the effective precision quite a lot, and you see floating point errors blow up.</p>\n<p>Instead, if you make <code>inp</code> to be small, i.e. <code>inp = torch.randn(batchSize, inputSize)</code>, then you will see that even in float precision, the <code>diff</code> is in the range of <code>1e-5</code>, more in line with well-behaved floating precision expectations.</p>\n<p>This is not a bug.</p>", "body_text": "@jfolt the two results are absolutely not equal, in any scientific computing sense.\nWhat you are seeing is floating point precision playing with your intuition. You have an inp that contains a range between 0 to 1399, which covers a bunch of exponent range. Then, you are doing a W * X + b operation here, where W is quite small (mean = 0.0002). You are doing a lot of multiplying large number with small number and adding another small number.\nIn such cases, the ordering of the operations affects the effective precision quite a lot, and you see floating point errors blow up.\nInstead, if you make inp to be small, i.e. inp = torch.randn(batchSize, inputSize), then you will see that even in float precision, the diff is in the range of 1e-5, more in line with well-behaved floating precision expectations.\nThis is not a bug.", "body": "@jfolt the two results are absolutely not equal, in any scientific computing sense.\r\n\r\nWhat you are seeing is floating point precision playing with your intuition. You have an `inp` that contains a range between `0 to 1399`, which covers a bunch of exponent range. Then, you are doing a `W * X + b` operation here, where W is quite small (mean = 0.0002). You are doing a lot of multiplying large number with small number and adding another small number.\r\nIn such cases, the ordering of the operations affects the effective precision quite a lot, and you see floating point errors blow up.\r\n\r\nInstead, if you make `inp` to be small, i.e. `inp = torch.randn(batchSize, inputSize)`, then you will see that even in float precision, the `diff` is in the range of `1e-5`, more in line with well-behaved floating precision expectations.\r\n\r\nThis is not a bug."}