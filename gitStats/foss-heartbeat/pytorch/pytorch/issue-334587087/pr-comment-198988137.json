{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/198988137", "pull_request_review_id": 133022302, "id": 198988137, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5ODk4ODEzNw==", "diff_hunk": "@@ -0,0 +1,153 @@\n+#include \"caffe2/operators/onnxifi_op.h\"\n+\n+namespace caffe2 {\n+\n+namespace {\n+\n+void BlobToTensorDescriptor(\n+    const std::string& name,\n+    Workspace* ws,\n+    onnxTensorDescriptor* desc,\n+    std::vector<std::vector<uint64_t>>* shapes) {\n+  const Blob* blob = ws->GetBlob(name);\n+  CAFFE_ENFORCE(blob, \"Blob \", name, \" doesn't exist\");\n+\n+  // Memory type\n+  // We only allow weights to be CPU tensor for now\n+  CAFFE_ENFORCE(\n+      blob->template IsType<TensorCPU>(),\n+      \"Initialization blob \",\n+      name,\n+      \" needs to be TensorCPU\");\n+  desc->memoryType = ONNXIFI_MEMORY_TYPE_CPU;\n+\n+  // Data type\n+  const auto& cpu_tensor = blob->template Get<TensorCPU>();\n+  if (cpu_tensor.template IsType<float>()) {\n+    desc->dataType = ONNXIFI_DATATYPE_FLOAT32;\n+    desc->buffer = reinterpret_cast<onnxPointer>(cpu_tensor.data<float>());\n+  } else if (cpu_tensor.template IsType<int64_t>()) {\n+    desc->dataType = ONNXIFI_DATATYPE_INT64;\n+    desc->buffer = reinterpret_cast<onnxPointer>(cpu_tensor.data<int64_t>());\n+  } else if (cpu_tensor.template IsType<int32_t>()) {\n+    desc->dataType = ONNXIFI_DATATYPE_INT32;\n+    desc->buffer = reinterpret_cast<onnxPointer>(cpu_tensor.data<int32_t>());\n+  }\n+\n+  // Set dims\n+  const auto& shape = cpu_tensor.dims();\n+  desc->dimensions = shape.size();\n+  shapes->emplace_back(shape.cbegin(), shape.cend());\n+  desc->shape = shapes->back().data();\n+}\n+} // namespace\n+\n+template <>\n+std::vector<onnxTensorDescriptor>\n+OnnxifiOp<float, CPUContext>::BuildInitializationList(\n+    Workspace* ws,\n+    std::unordered_set<std::string>* initialization_list,\n+    std::vector<std::string>* weight_names,\n+    std::vector<std::vector<uint64_t>>* weight_shapes) {\n+  const std::vector<string>& ws_blobs = ws->Blobs();\n+  std::vector<onnxTensorDescriptor> descs;\n+  for (const auto& s : ws_blobs) {\n+    auto it = initialization_list->find(s);\n+    if (it != initialization_list->end()) {\n+      weight_names->emplace_back(s);\n+      onnxTensorDescriptor tensor_desc;\n+      tensor_desc.name = weight_names->back().c_str();\n+      BlobToTensorDescriptor(s, ws, &tensor_desc, weight_shapes);\n+      descs.push_back(tensor_desc);\n+      initialization_list->erase(it);\n+    }\n+  }\n+  CAFFE_ENFORCE(\n+      initialization_list->empty(), \"Unfulfilled initialization list\");\n+  return descs;\n+}\n+\n+template <>\n+bool OnnxifiOp<float, CPUContext>::RunOnDevice() {\n+  for (unsigned i = 0U; i < InputSize(); ++i) {\n+    const auto& input_tensor = Input(i);\n+    const auto& tensor_dims = input_tensor.dims();\n+    auto& tensor_descriptor = input_desc_.at(i);\n+    tensor_descriptor.dataType = ONNXIFI_DATATYPE_FLOAT32;\n+    tensor_descriptor.memoryType = ONNXIFI_MEMORY_TYPE_CPU;\n+    tensor_descriptor.dimensions = tensor_dims.size();\n+    input_shapes_.emplace_back(tensor_dims.cbegin(), tensor_dims.cend());\n+    tensor_descriptor.shape = input_shapes_.back().data();\n+    tensor_descriptor.buffer =\n+        reinterpret_cast<onnxPointer>(input_tensor.data<float>());\n+  }\n+\n+  for (unsigned i = 0U; i < OutputSize(); ++i) {\n+    auto* output_tensor = Output(i);\n+    std::vector<TIndex> tensor_dims;\n+    SetOutputShape(i, &tensor_dims);\n+    output_tensor->Resize(tensor_dims);\n+    auto& tensor_descriptor = output_desc_.at(i);\n+    tensor_descriptor.dataType = ONNXIFI_DATATYPE_FLOAT32;\n+    tensor_descriptor.memoryType = ONNXIFI_MEMORY_TYPE_CPU;\n+    tensor_descriptor.dimensions = tensor_dims.size();\n+    output_shapes_.emplace_back(tensor_dims.cbegin(), tensor_dims.cend());\n+    tensor_descriptor.shape = output_shapes_.back().data();\n+    tensor_descriptor.buffer =\n+        reinterpret_cast<onnxPointer>(output_tensor->mutable_data<float>());\n+  }\n+\n+  CAFFE_ENFORCE_EQ(\n+      lib_->onnxSetGraphIO(\n+          graph_,\n+          input_desc_.size(),\n+          input_desc_.data(),\n+          output_desc_.size(),\n+          output_desc_.data()),\n+      ONNXIFI_STATUS_SUCCESS);\n+\n+  onnxMemoryFence input_fence;\n+  input_fence.event = nullptr;\n+  input_fence.type = ONNXIFI_SYNCHRONIZATION_EVENT;\n+  CAFFE_ENFORCE_EQ(\n+      lib_->onnxInitEvent(backend_, input_fence.event), ONNXIFI_STATUS_SUCCESS);\n+  onnxMemoryFence output_fence;\n+  output_fence.type = ONNXIFI_SYNCHRONIZATION_EVENT;\n+  output_fence.event = nullptr;\n+  CAFFE_ENFORCE_EQ(\n+      lib_->onnxInitEvent(backend_, output_fence.event),", "path": "caffe2/operators/onnxifi_op.cc", "position": null, "original_position": 118, "commit_id": "d441bb622ebb16bcc309e6ffb07b51ed43f161ce", "original_commit_id": "e79030fa8ea1cafe0b391bd15e5385c21d9d0c55", "user": {"login": "Maratyszcza", "id": 1093985, "node_id": "MDQ6VXNlcjEwOTM5ODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1093985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Maratyszcza", "html_url": "https://github.com/Maratyszcza", "followers_url": "https://api.github.com/users/Maratyszcza/followers", "following_url": "https://api.github.com/users/Maratyszcza/following{/other_user}", "gists_url": "https://api.github.com/users/Maratyszcza/gists{/gist_id}", "starred_url": "https://api.github.com/users/Maratyszcza/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Maratyszcza/subscriptions", "organizations_url": "https://api.github.com/users/Maratyszcza/orgs", "repos_url": "https://api.github.com/users/Maratyszcza/repos", "events_url": "https://api.github.com/users/Maratyszcza/events{/privacy}", "received_events_url": "https://api.github.com/users/Maratyszcza/received_events", "type": "User", "site_admin": false}, "body": "You should not initialize event for output fence. For ONNXIFI events, you just specify the type of the event, and `onnxRunGraph` populates it. BTW, if you wonder why such strange design, its because some system APIs (e.g. OpenGL) do not separate creation of the event from its submission to the queue.", "created_at": "2018-06-28T21:14:45Z", "updated_at": "2018-11-23T15:46:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/8749#discussion_r198988137", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8749", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/198988137"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8749#discussion_r198988137"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8749"}}, "body_html": "<p>You should not initialize event for output fence. For ONNXIFI events, you just specify the type of the event, and <code>onnxRunGraph</code> populates it. BTW, if you wonder why such strange design, its because some system APIs (e.g. OpenGL) do not separate creation of the event from its submission to the queue.</p>", "body_text": "You should not initialize event for output fence. For ONNXIFI events, you just specify the type of the event, and onnxRunGraph populates it. BTW, if you wonder why such strange design, its because some system APIs (e.g. OpenGL) do not separate creation of the event from its submission to the queue."}