{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6511", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6511/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6511/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6511/events", "html_url": "https://github.com/pytorch/pytorch/issues/6511", "id": 313425372, "node_id": "MDU6SXNzdWUzMTM0MjUzNzI=", "number": 6511, "title": "torch.irfft produces \"cuFFT error: CUFFT_ALLOC_FAILED\" when called after torch.rfft", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 897288569, "node_id": "MDU6TGFiZWw4OTcyODg1Njk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/pytorch", "name": "pytorch", "color": "f05732", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-04-11T17:51:00Z", "updated_at": "2018-06-05T17:55:31Z", "closed_at": "2018-06-05T17:55:31Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a></p>\n<p>The code below produces:</p>\n<pre><code>(25088, 4001, 2)\n\n  File \"bug.py\", line 28, in &lt;module&gt;\n    layer(bottom1, bottom2)\n  File \"/home/cvlab/vadim/.wigwam/prefix/python/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 371, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"bug.py\", line 20, in forward\n    cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\nRuntimeError: cuFFT error: CUFFT_ALLOC_FAILED\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CompactBilinearPooling</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_dim1</span>, <span class=\"pl-smi\">input_dim2</span>, <span class=\"pl-smi\">output_dim</span>, <span class=\"pl-smi\">sum_pool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>(CompactBilinearPooling, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.output_dim <span class=\"pl-k\">=</span> output_dim\n        <span class=\"pl-c1\">self</span>.sum_pool <span class=\"pl-k\">=</span> sum_pool\n\n        generate_sketch_matrix <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">rand_h</span>, <span class=\"pl-smi\">rand_s</span>, <span class=\"pl-smi\">input_dim</span>, <span class=\"pl-smi\">output_dim</span>: torch.sparse.FloatTensor(torch.stack([torch.arange(input_dim, <span class=\"pl-v\">out</span> <span class=\"pl-k\">=</span> torch.LongTensor()), rand_h.long()]), rand_s.float(), torch.Size([input_dim, output_dim])).to_dense()\n        <span class=\"pl-c1\">self</span>.sparse_sketch_matrix1 <span class=\"pl-k\">=</span> nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, <span class=\"pl-v\">size</span> <span class=\"pl-k\">=</span> (input_dim1,)), <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> torch.randint(<span class=\"pl-c1\">2</span>, <span class=\"pl-v\">size</span> <span class=\"pl-k\">=</span> (input_dim1,)) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>, input_dim1, output_dim))\n        <span class=\"pl-c1\">self</span>.sparse_sketch_matrix2 <span class=\"pl-k\">=</span> nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, <span class=\"pl-v\">size</span> <span class=\"pl-k\">=</span> (input_dim2,)), <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> torch.randint(<span class=\"pl-c1\">2</span>, <span class=\"pl-v\">size</span> <span class=\"pl-k\">=</span> (input_dim2,)) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>, input_dim2, output_dim))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bottom1</span>, <span class=\"pl-smi\">bottom2</span>):\n        sketch_1 <span class=\"pl-k\">=</span> bottom1.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>).contiguous().matmul(<span class=\"pl-c1\">self</span>.sparse_sketch_matrix1).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.output_dim)\n        sketch_2 <span class=\"pl-k\">=</span> bottom2.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>).contiguous().matmul(<span class=\"pl-c1\">self</span>.sparse_sketch_matrix2).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.output_dim)\n        fft1_real, fft1_imag <span class=\"pl-k\">=</span> torch.rfft(sketch_1, <span class=\"pl-c1\">1</span>).permute(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n        fft2_real, fft2_imag <span class=\"pl-k\">=</span> torch.rfft(sketch_2, <span class=\"pl-c1\">1</span>).permute(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n        fft_product <span class=\"pl-k\">=</span> torch.stack([fft1_real <span class=\"pl-k\">*</span> fft2_real <span class=\"pl-k\">-</span> fft1_imag <span class=\"pl-k\">*</span> fft2_imag, fft1_real <span class=\"pl-k\">*</span> fft2_imag <span class=\"pl-k\">-</span> fft1_imag <span class=\"pl-k\">*</span> fft2_real], <span class=\"pl-v\">dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">print</span>(fft_product.size())\n        cbp <span class=\"pl-k\">=</span> torch.irfft(fft_product, <span class=\"pl-c1\">1</span>).view(<span class=\"pl-c1\">len</span>(bottom1), bottom1.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>), bottom1.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">self</span>.output_dim) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.output_dim\n        <span class=\"pl-k\">return</span> cbp.sum(<span class=\"pl-v\">dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>).sum(<span class=\"pl-v\">dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.sum_pool <span class=\"pl-k\">else</span> cbp.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)\n\nbottom1 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">14</span>, <span class=\"pl-c1\">14</span>).cuda()\nbottom2 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">14</span>, <span class=\"pl-c1\">14</span>).cuda()\nlayer <span class=\"pl-k\">=</span> CompactBilinearPooling(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">8000</span>)\nlayer.cuda()\nlayer.train()\nlayer(bottom1, bottom2)</pre></div>\n<p>Works without error, but there seems a shape misalignment:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nfft_product <span class=\"pl-k\">=</span> torch.rfft(torch.cuda.FloatTensor(<span class=\"pl-c1\">25088</span>, <span class=\"pl-c1\">8000</span>), <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(fft_product.size())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (25088, 4001, 2)</span>\n<span class=\"pl-c1\">print</span>(torch.irfft(fft_product, <span class=\"pl-c1\">1</span>).size())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (25088, 8001)</span></pre></div>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: master</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version: 2.7</li>\n</ul>", "body_text": "@SsnL\nThe code below produces:\n(25088, 4001, 2)\n\n  File \"bug.py\", line 28, in <module>\n    layer(bottom1, bottom2)\n  File \"/home/cvlab/vadim/.wigwam/prefix/python/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 371, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"bug.py\", line 20, in forward\n    cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\nRuntimeError: cuFFT error: CUFFT_ALLOC_FAILED\n\nimport torch\nimport torch.nn as nn\n\nclass CompactBilinearPooling(nn.Module):\n    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool = True):\n        super(CompactBilinearPooling, self).__init__()\n        self.output_dim = output_dim\n        self.sum_pool = sum_pool\n\n        generate_sketch_matrix = lambda rand_h, rand_s, input_dim, output_dim: torch.sparse.FloatTensor(torch.stack([torch.arange(input_dim, out = torch.LongTensor()), rand_h.long()]), rand_s.float(), torch.Size([input_dim, output_dim])).to_dense()\n        self.sparse_sketch_matrix1 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim1,)), 2 * torch.randint(2, size = (input_dim1,)) - 1, input_dim1, output_dim))\n        self.sparse_sketch_matrix2 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim2,)), 2 * torch.randint(2, size = (input_dim2,)) - 1, input_dim2, output_dim))\n\n    def forward(self, bottom1, bottom2):\n        sketch_1 = bottom1.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix1).view(-1, self.output_dim)\n        sketch_2 = bottom2.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix2).view(-1, self.output_dim)\n        fft1_real, fft1_imag = torch.rfft(sketch_1, 1).permute(2, 0, 1)\n        fft2_real, fft2_imag = torch.rfft(sketch_2, 1).permute(2, 0, 1)\n        fft_product = torch.stack([fft1_real * fft2_real - fft1_imag * fft2_imag, fft1_real * fft2_imag - fft1_imag * fft2_real], dim = -1)\n        print(fft_product.size())\n        cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\n        return cbp.sum(dim = 1).sum(dim = 1) if self.sum_pool else cbp.permute(0, 3, 1, 2)\n\nbottom1 = torch.randn(128, 512, 14, 14).cuda()\nbottom2 = torch.randn(128, 512, 14, 14).cuda()\nlayer = CompactBilinearPooling(512, 512, 8000)\nlayer.cuda()\nlayer.train()\nlayer(bottom1, bottom2)\nWorks without error, but there seems a shape misalignment:\nimport torch\nfft_product = torch.rfft(torch.cuda.FloatTensor(25088, 8000), 1)\nprint(fft_product.size())\n# (25088, 4001, 2)\nprint(torch.irfft(fft_product, 1).size())\n# (25088, 8001)\n\nPyTorch or Caffe2: PyTorch\nOS: Ubuntu 16.04\nPyTorch version: master\nHow you installed PyTorch (conda, pip, source): source\nPython version: 2.7", "body": "@SsnL \r\n\r\nThe code below produces:\r\n```\r\n(25088, 4001, 2)\r\n\r\n  File \"bug.py\", line 28, in <module>\r\n    layer(bottom1, bottom2)\r\n  File \"/home/cvlab/vadim/.wigwam/prefix/python/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 371, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"bug.py\", line 20, in forward\r\n    cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\r\nRuntimeError: cuFFT error: CUFFT_ALLOC_FAILED\r\n```\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass CompactBilinearPooling(nn.Module):\r\n    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool = True):\r\n        super(CompactBilinearPooling, self).__init__()\r\n        self.output_dim = output_dim\r\n        self.sum_pool = sum_pool\r\n\r\n        generate_sketch_matrix = lambda rand_h, rand_s, input_dim, output_dim: torch.sparse.FloatTensor(torch.stack([torch.arange(input_dim, out = torch.LongTensor()), rand_h.long()]), rand_s.float(), torch.Size([input_dim, output_dim])).to_dense()\r\n        self.sparse_sketch_matrix1 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim1,)), 2 * torch.randint(2, size = (input_dim1,)) - 1, input_dim1, output_dim))\r\n        self.sparse_sketch_matrix2 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim2,)), 2 * torch.randint(2, size = (input_dim2,)) - 1, input_dim2, output_dim))\r\n\r\n    def forward(self, bottom1, bottom2):\r\n        sketch_1 = bottom1.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix1).view(-1, self.output_dim)\r\n        sketch_2 = bottom2.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix2).view(-1, self.output_dim)\r\n        fft1_real, fft1_imag = torch.rfft(sketch_1, 1).permute(2, 0, 1)\r\n        fft2_real, fft2_imag = torch.rfft(sketch_2, 1).permute(2, 0, 1)\r\n        fft_product = torch.stack([fft1_real * fft2_real - fft1_imag * fft2_imag, fft1_real * fft2_imag - fft1_imag * fft2_real], dim = -1)\r\n        print(fft_product.size())\r\n        cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\r\n        return cbp.sum(dim = 1).sum(dim = 1) if self.sum_pool else cbp.permute(0, 3, 1, 2)\r\n\r\nbottom1 = torch.randn(128, 512, 14, 14).cuda()\r\nbottom2 = torch.randn(128, 512, 14, 14).cuda()\r\nlayer = CompactBilinearPooling(512, 512, 8000)\r\nlayer.cuda()\r\nlayer.train()\r\nlayer(bottom1, bottom2)\r\n```\r\n\r\nWorks without error, but there seems a shape misalignment:\r\n```python\r\nimport torch\r\nfft_product = torch.rfft(torch.cuda.FloatTensor(25088, 8000), 1)\r\nprint(fft_product.size())\r\n# (25088, 4001, 2)\r\nprint(torch.irfft(fft_product, 1).size())\r\n# (25088, 8001)\r\n```\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: master\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 2.7"}