{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9572", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9572/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9572/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9572/events", "html_url": "https://github.com/pytorch/pytorch/issues/9572", "id": 342517831, "node_id": "MDU6SXNzdWUzNDI1MTc4MzE=", "number": 9572, "title": "[feature request] RNN Wishlist", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-07-18T22:45:30Z", "updated_at": "2018-07-26T23:26:38Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>PT has fairly basic options for RNNs, mainly due to keeping parity with cuDNN - something that should be alleviated by 1.0, which would allow a more flexible RNN API (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"206652810\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/711\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/711/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/711\">#711</a>). This issue details some possible options that we may want to provide. Sonnet's <a href=\"https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py\">gated_rnn file</a> provides a nice reference for all of these.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Customisable activation functions (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218770783\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1173\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1173/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1173\">#1173</a>). Would add a lot of flexibility, and needed to reproduce some works (see linked issue).</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Layer normalisation built in (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"315887577\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6760\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6760/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6760\">#6760</a>). The <a href=\"https://arxiv.org/pdf/1607.06450.pdf\" rel=\"nofollow\">original paper</a> provides 2 LSTM variants, and 1 GRU variant. TF provides a <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell\" rel=\"nofollow\">different module</a> for this. Pretty widely used due to its ability to improve stability in training, among other possible benefits.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Recurrent dropout (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"244425370\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2166\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2166/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2166\">#2166</a>). An RNN regulariser - see <a href=\"https://arxiv.org/pdf/1603.05118.pdf\" rel=\"nofollow\">original paper</a>. TF provides this in the <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell\" rel=\"nofollow\">layer norm LSTM</a>, but it could be applied as a generic wrapper for RNNs - see <a href=\"https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py\">Sonnet</a> for example.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Zoneout (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"202634918\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/561\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/561/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/561\">#561</a>). <a href=\"https://arxiv.org/pdf/1606.01305.pdf\" rel=\"nofollow\">Original paper</a>. This is again a generic RNN regulariser, implemented by a wrapper in <a href=\"https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py\">Sonnet</a>.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Configurable forget gate bias for LSTM (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"207903303\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/750\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/750/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/750\">#750</a>). Setting this to 1 is advocated for by the <a href=\"https://www.researchgate.net/profile/Felix_Gers/publication/12292425_Learning_to_Forget_Continual_Prediction_with_LSTM/links/5759414608ae9a9c954e84c5/Learning-to-Forget-Continual-Prediction-with-LSTM.pdf\" rel=\"nofollow\">original forget gate paper</a> - this makes the LSTM remember information until it learns to forget. Noticeably, this is the default in <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell\" rel=\"nofollow\">TF</a> and <a href=\"https://keras.io/layers/recurrent/#lstmcell\" rel=\"nofollow\">Keras</a>.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Peephole connections (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"203853311\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/630\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/630/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/630\">#630</a>). According to the <a href=\"http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf\" rel=\"nofollow\">original paper</a>, can help learn certain sequences.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> LSTM cell clipping. Provided in <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell\" rel=\"nofollow\">TF</a>, not on by default. <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf\" rel=\"nofollow\">Source</a> via <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a>.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Projection layer in LSTM. Allows larger units - see <a href=\"https://arxiv.org/pdf/1402.1128.pdf\" rel=\"nofollow\">original paper</a> - that are particularly relevant in NLP tasks. Provided in <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell\" rel=\"nofollow\">TF</a>.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Convolutional LSTM (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"233289931\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1706\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1706/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1706\">#1706</a>). <a href=\"https://arxiv.org/pdf/1506.04214.pdf\" rel=\"nofollow\">Original paper</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/ConvLSTMCell\" rel=\"nofollow\">TF module</a> - widely used component by now.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Gradient clipping at every timestep (see comments from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7424737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PetrochukM\">@PetrochukM</a> below for details). <a href=\"https://arxiv.org/pdf/1211.5063.pdf\" rel=\"nofollow\">Original paper</a>.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Better initialisations? Keras uses <a href=\"https://keras.io/layers/recurrent/\" rel=\"nofollow\">Glorot uniform for input-to-hidden, orthogonal for hidden-to-hidden weights, and zeros for the bias</a>, across all unit types (simple RNN, LSTM and GRU). This fits into the larger question of highlighting better weight initialisations across all PT modules, so may be out of place here.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Single bias? The current RNNs have two biases, which is completely redundant.</li>\n</ul>", "body_text": "PT has fairly basic options for RNNs, mainly due to keeping parity with cuDNN - something that should be alleviated by 1.0, which would allow a more flexible RNN API (#711). This issue details some possible options that we may want to provide. Sonnet's gated_rnn file provides a nice reference for all of these.\n\n Customisable activation functions (#1173). Would add a lot of flexibility, and needed to reproduce some works (see linked issue).\n Layer normalisation built in (#6760). The original paper provides 2 LSTM variants, and 1 GRU variant. TF provides a different module for this. Pretty widely used due to its ability to improve stability in training, among other possible benefits.\n Recurrent dropout (#2166). An RNN regulariser - see original paper. TF provides this in the layer norm LSTM, but it could be applied as a generic wrapper for RNNs - see Sonnet for example.\n Zoneout (#561). Original paper. This is again a generic RNN regulariser, implemented by a wrapper in Sonnet.\n Configurable forget gate bias for LSTM (#750). Setting this to 1 is advocated for by the original forget gate paper - this makes the LSTM remember information until it learns to forget. Noticeably, this is the default in TF and Keras.\n Peephole connections (#630). According to the original paper, can help learn certain sequences.\n LSTM cell clipping. Provided in TF, not on by default. Source via @ngimel.\n Projection layer in LSTM. Allows larger units - see original paper - that are particularly relevant in NLP tasks. Provided in TF.\n Convolutional LSTM (#1706). Original paper and TF module - widely used component by now.\n Gradient clipping at every timestep (see comments from @PetrochukM below for details). Original paper.\n Better initialisations? Keras uses Glorot uniform for input-to-hidden, orthogonal for hidden-to-hidden weights, and zeros for the bias, across all unit types (simple RNN, LSTM and GRU). This fits into the larger question of highlighting better weight initialisations across all PT modules, so may be out of place here.\n Single bias? The current RNNs have two biases, which is completely redundant.", "body": "PT has fairly basic options for RNNs, mainly due to keeping parity with cuDNN - something that should be alleviated by 1.0, which would allow a more flexible RNN API (https://github.com/pytorch/pytorch/issues/711). This issue details some possible options that we may want to provide. Sonnet's [gated_rnn file](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py) provides a nice reference for all of these.\r\n\r\n- [ ] Customisable activation functions (https://github.com/pytorch/pytorch/issues/1173). Would add a lot of flexibility, and needed to reproduce some works (see linked issue).\r\n- [ ] Layer normalisation built in (https://github.com/pytorch/pytorch/issues/6760). The [original paper](https://arxiv.org/pdf/1607.06450.pdf) provides 2 LSTM variants, and 1 GRU variant. TF provides a [different module](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell) for this. Pretty widely used due to its ability to improve stability in training, among other possible benefits.\r\n- [ ] Recurrent dropout (https://github.com/pytorch/pytorch/issues/2166). An RNN regulariser - see [original paper](https://arxiv.org/pdf/1603.05118.pdf). TF provides this in the [layer norm LSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell), but it could be applied as a generic wrapper for RNNs - see [Sonnet](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py) for example.\r\n- [ ] Zoneout (https://github.com/pytorch/pytorch/issues/561). [Original paper](https://arxiv.org/pdf/1606.01305.pdf). This is again a generic RNN regulariser, implemented by a wrapper in [Sonnet](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py).\r\n- [ ] Configurable forget gate bias for LSTM (https://github.com/pytorch/pytorch/issues/750). Setting this to 1 is advocated for by the [original forget gate paper](https://www.researchgate.net/profile/Felix_Gers/publication/12292425_Learning_to_Forget_Continual_Prediction_with_LSTM/links/5759414608ae9a9c954e84c5/Learning-to-Forget-Continual-Prediction-with-LSTM.pdf) - this makes the LSTM remember information until it learns to forget. Noticeably, this is the default in [TF](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) and [Keras](https://keras.io/layers/recurrent/#lstmcell).\r\n- [ ] Peephole connections (https://github.com/pytorch/pytorch/issues/630). According to the [original paper](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf), can help learn certain sequences.\r\n- [ ] LSTM cell clipping. Provided in [TF](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell), not on by default. [Source](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf) via @ngimel.\r\n- [ ] Projection layer in LSTM. Allows larger units - see [original paper](https://arxiv.org/pdf/1402.1128.pdf) - that are particularly relevant in NLP tasks. Provided in [TF](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell).\r\n- [ ] Convolutional LSTM (https://github.com/pytorch/pytorch/issues/1706). [Original paper](https://arxiv.org/pdf/1506.04214.pdf) and [TF module](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/ConvLSTMCell) - widely used component by now.\r\n- [ ] Gradient clipping at every timestep (see comments from @PetrochukM below for details). [Original paper](https://arxiv.org/pdf/1211.5063.pdf).\r\n- [ ] Better initialisations? Keras uses [Glorot uniform for input-to-hidden, orthogonal for hidden-to-hidden weights, and zeros for the bias](https://keras.io/layers/recurrent/), across all unit types (simple RNN, LSTM and GRU). This fits into the larger question of highlighting better weight initialisations across all PT modules, so may be out of place here.\r\n- [ ] Single bias? The current RNNs have two biases, which is completely redundant."}