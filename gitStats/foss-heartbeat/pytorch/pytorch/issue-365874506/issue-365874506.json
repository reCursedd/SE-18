{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12259", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12259/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12259/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12259/events", "html_url": "https://github.com/pytorch/pytorch/issues/12259", "id": 365874506, "node_id": "MDU6SXNzdWUzNjU4NzQ1MDY=", "number": 12259, "title": "Set Batchnorm weight scalar initialization to unit (not random uniform)", "user": {"login": "cstein06", "id": 7127316, "node_id": "MDQ6VXNlcjcxMjczMTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7127316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cstein06", "html_url": "https://github.com/cstein06", "followers_url": "https://api.github.com/users/cstein06/followers", "following_url": "https://api.github.com/users/cstein06/following{/other_user}", "gists_url": "https://api.github.com/users/cstein06/gists{/gist_id}", "starred_url": "https://api.github.com/users/cstein06/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cstein06/subscriptions", "organizations_url": "https://api.github.com/users/cstein06/orgs", "repos_url": "https://api.github.com/users/cstein06/repos", "events_url": "https://api.github.com/users/cstein06/events{/privacy}", "received_events_url": "https://api.github.com/users/cstein06/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-02T12:29:39Z", "updated_at": "2018-11-09T03:01:29Z", "closed_at": "2018-11-09T03:01:29Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Feature suggestion</h2>\n<p>Pytorch Batch normalization currently initializes it's scalar weight parameter with random uniform variables (<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/1d3f650ce4f1b781f03e8b4f250a25d5a8f819cc/torch/nn/modules/batchnorm.py#L46\">pytorch/torch/nn/modules/batchnorm.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 46\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/1d3f650ce4f1b781f03e8b4f250a25d5a8f819cc\">1d3f650</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L46\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"46\"></td>\n          <td id=\"LC46\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> init.uniform_(<span class=\"pl-c1\">self</span>.weight) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n). This initialization has no theoretical basis, while on the other hand it makes sense to use a fixed value of 1.</p>\n<p>Part of the usefulness of Batchnorm is that it can initialize the network easily, with inputs to hidden units normalized to zero norm and unit variance (original paper: <a href=\"https://arxiv.org/abs/1502.03167\" rel=\"nofollow\">https://arxiv.org/abs/1502.03167</a>) and it is also how recent studies use it (e.g. <a href=\"https://arxiv.org/abs/1706.02677\" rel=\"nofollow\">https://arxiv.org/abs/1706.02677</a>).</p>\n<h2>To change</h2>\n<p>From<br>\ninit.uniform_(self.weight)</p>\n<p>to<br>\ninit.ones_(self.weight)</p>", "body_text": "\ud83d\udc1b Feature suggestion\nPytorch Batch normalization currently initializes it's scalar weight parameter with random uniform variables (\n  \n    \n      pytorch/torch/nn/modules/batchnorm.py\n    \n    \n         Line 46\n      in\n      1d3f650\n    \n    \n    \n    \n\n        \n          \n           init.uniform_(self.weight) \n        \n    \n  \n\n). This initialization has no theoretical basis, while on the other hand it makes sense to use a fixed value of 1.\nPart of the usefulness of Batchnorm is that it can initialize the network easily, with inputs to hidden units normalized to zero norm and unit variance (original paper: https://arxiv.org/abs/1502.03167) and it is also how recent studies use it (e.g. https://arxiv.org/abs/1706.02677).\nTo change\nFrom\ninit.uniform_(self.weight)\nto\ninit.ones_(self.weight)", "body": "## \ud83d\udc1b Feature suggestion\r\n\r\nPytorch Batch normalization currently initializes it's scalar weight parameter with random uniform variables (https://github.com/pytorch/pytorch/blob/1d3f650ce4f1b781f03e8b4f250a25d5a8f819cc/torch/nn/modules/batchnorm.py#L46). This initialization has no theoretical basis, while on the other hand it makes sense to use a fixed value of 1. \r\n\r\nPart of the usefulness of Batchnorm is that it can initialize the network easily, with inputs to hidden units normalized to zero norm and unit variance (original paper: https://arxiv.org/abs/1502.03167) and it is also how recent studies use it (e.g. https://arxiv.org/abs/1706.02677).\r\n\r\n## To change\r\n\r\nFrom \r\ninit.uniform_(self.weight)\r\n\r\nto \r\ninit.ones_(self.weight)\r\n\r\n"}