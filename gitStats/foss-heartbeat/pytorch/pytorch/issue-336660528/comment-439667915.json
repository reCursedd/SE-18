{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/439667915", "html_url": "https://github.com/pytorch/pytorch/issues/8985#issuecomment-439667915", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8985", "id": 439667915, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTY2NzkxNQ==", "user": {"login": "askerlee", "id": 1575461, "node_id": "MDQ6VXNlcjE1NzU0NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1575461?v=4", "gravatar_id": "", "url": "https://api.github.com/users/askerlee", "html_url": "https://github.com/askerlee", "followers_url": "https://api.github.com/users/askerlee/followers", "following_url": "https://api.github.com/users/askerlee/following{/other_user}", "gists_url": "https://api.github.com/users/askerlee/gists{/gist_id}", "starred_url": "https://api.github.com/users/askerlee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/askerlee/subscriptions", "organizations_url": "https://api.github.com/users/askerlee/orgs", "repos_url": "https://api.github.com/users/askerlee/repos", "events_url": "https://api.github.com/users/askerlee/events{/privacy}", "received_events_url": "https://api.github.com/users/askerlee/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-18T04:57:11Z", "updated_at": "2018-11-18T04:59:03Z", "author_association": "NONE", "body_html": "<p>Below is another option of conditional batchnorm implemented by me, which doesn't share running_mean and running_var across domains. I tried to make it compatible with the old model checkpoints, so that we could plug it in a densenet/resnet/..., and still use pretrained models.</p>\n<pre><code>class CondBatchNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-05, momentum=0.05, \n                 affine=True, track_running_stats=True, num_domains=2):\n        super().__init__()\n        self.num_features = num_features\n        self.bns = [] # nn.ModuleList()\n        self.num_domains = num_domains\n        for i in range(self.num_domains):\n            self.bns.append(nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats))\n            self.bns[i].weight.data.fill_(1)\n            self.bns[i].bias.data.zero_()\n\n    def _load_from_state_dict(self, state_dict, prefix, metadata, strict, \n                            missing_keys, unexpected_keys, error_msgs):\n        # if bns are not in a ModuleList yet, then we are loading a cp of the traditional model, \n        # so we manually copy the same weights/bias/running_mean/running_var/num_batches_tracked to all bns\n        # if bns are in a ModuleList, it means we are loading a checkpoint of a multidomain model.\n        # bns not need to be manually initialized. pytorch will initialize ModuleList, which in turn initializes bns\n        if type(self.bns) == list:        \n            for i in range(self.num_domains):\n                self.bns[i]._load_from_state_dict( \\\n                            state_dict, prefix, metadata, strict, \n                            missing_keys, unexpected_keys, error_msgs )\n    \n    # default is in the first domain. \n    # if domains is not specified, fall back to the original bn's behavior\n    def forward(self, x, domains=None):\n        # convert from list to ModuleList, so that torch.save() will save parameters of bns\n        # Couldn't do the conversion in _load_from_state_dict; otherwise pytorch will \n        # directly access the elements in bns and try to load them, \n        # which will cause \"missing keys\" errors\n        if type(self.bns) == list:\n            self.bns = nn.ModuleList(self.bns)\n            if x.is_cuda:\n                self.bns.cuda()\n                \n        if domains is None:\n            domains = torch.zeros_like(x[:, 0, 0, 0]).long()\n        out = torch.zeros_like(x)\n        for i in range(self.num_domains):\n            if (domains == i).sum() &gt; 0:\n                x_dom = x[domains==i]\n                out[domains==i] = self.bns[i](x_dom)\n        return out\n</code></pre>", "body_text": "Below is another option of conditional batchnorm implemented by me, which doesn't share running_mean and running_var across domains. I tried to make it compatible with the old model checkpoints, so that we could plug it in a densenet/resnet/..., and still use pretrained models.\nclass CondBatchNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-05, momentum=0.05, \n                 affine=True, track_running_stats=True, num_domains=2):\n        super().__init__()\n        self.num_features = num_features\n        self.bns = [] # nn.ModuleList()\n        self.num_domains = num_domains\n        for i in range(self.num_domains):\n            self.bns.append(nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats))\n            self.bns[i].weight.data.fill_(1)\n            self.bns[i].bias.data.zero_()\n\n    def _load_from_state_dict(self, state_dict, prefix, metadata, strict, \n                            missing_keys, unexpected_keys, error_msgs):\n        # if bns are not in a ModuleList yet, then we are loading a cp of the traditional model, \n        # so we manually copy the same weights/bias/running_mean/running_var/num_batches_tracked to all bns\n        # if bns are in a ModuleList, it means we are loading a checkpoint of a multidomain model.\n        # bns not need to be manually initialized. pytorch will initialize ModuleList, which in turn initializes bns\n        if type(self.bns) == list:        \n            for i in range(self.num_domains):\n                self.bns[i]._load_from_state_dict( \\\n                            state_dict, prefix, metadata, strict, \n                            missing_keys, unexpected_keys, error_msgs )\n    \n    # default is in the first domain. \n    # if domains is not specified, fall back to the original bn's behavior\n    def forward(self, x, domains=None):\n        # convert from list to ModuleList, so that torch.save() will save parameters of bns\n        # Couldn't do the conversion in _load_from_state_dict; otherwise pytorch will \n        # directly access the elements in bns and try to load them, \n        # which will cause \"missing keys\" errors\n        if type(self.bns) == list:\n            self.bns = nn.ModuleList(self.bns)\n            if x.is_cuda:\n                self.bns.cuda()\n                \n        if domains is None:\n            domains = torch.zeros_like(x[:, 0, 0, 0]).long()\n        out = torch.zeros_like(x)\n        for i in range(self.num_domains):\n            if (domains == i).sum() > 0:\n                x_dom = x[domains==i]\n                out[domains==i] = self.bns[i](x_dom)\n        return out", "body": "Below is another option of conditional batchnorm implemented by me, which doesn't share running_mean and running_var across domains. I tried to make it compatible with the old model checkpoints, so that we could plug it in a densenet/resnet/..., and still use pretrained models.\r\n\r\n``` \r\nclass CondBatchNorm2d(nn.Module):\r\n    def __init__(self, num_features, eps=1e-05, momentum=0.05, \r\n                 affine=True, track_running_stats=True, num_domains=2):\r\n        super().__init__()\r\n        self.num_features = num_features\r\n        self.bns = [] # nn.ModuleList()\r\n        self.num_domains = num_domains\r\n        for i in range(self.num_domains):\r\n            self.bns.append(nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats))\r\n            self.bns[i].weight.data.fill_(1)\r\n            self.bns[i].bias.data.zero_()\r\n\r\n    def _load_from_state_dict(self, state_dict, prefix, metadata, strict, \r\n                            missing_keys, unexpected_keys, error_msgs):\r\n        # if bns are not in a ModuleList yet, then we are loading a cp of the traditional model, \r\n        # so we manually copy the same weights/bias/running_mean/running_var/num_batches_tracked to all bns\r\n        # if bns are in a ModuleList, it means we are loading a checkpoint of a multidomain model.\r\n        # bns not need to be manually initialized. pytorch will initialize ModuleList, which in turn initializes bns\r\n        if type(self.bns) == list:        \r\n            for i in range(self.num_domains):\r\n                self.bns[i]._load_from_state_dict( \\\r\n                            state_dict, prefix, metadata, strict, \r\n                            missing_keys, unexpected_keys, error_msgs )\r\n    \r\n    # default is in the first domain. \r\n    # if domains is not specified, fall back to the original bn's behavior\r\n    def forward(self, x, domains=None):\r\n        # convert from list to ModuleList, so that torch.save() will save parameters of bns\r\n        # Couldn't do the conversion in _load_from_state_dict; otherwise pytorch will \r\n        # directly access the elements in bns and try to load them, \r\n        # which will cause \"missing keys\" errors\r\n        if type(self.bns) == list:\r\n            self.bns = nn.ModuleList(self.bns)\r\n            if x.is_cuda:\r\n                self.bns.cuda()\r\n                \r\n        if domains is None:\r\n            domains = torch.zeros_like(x[:, 0, 0, 0]).long()\r\n        out = torch.zeros_like(x)\r\n        for i in range(self.num_domains):\r\n            if (domains == i).sum() > 0:\r\n                x_dom = x[domains==i]\r\n                out[domains==i] = self.bns[i](x_dom)\r\n        return out\r\n```"}