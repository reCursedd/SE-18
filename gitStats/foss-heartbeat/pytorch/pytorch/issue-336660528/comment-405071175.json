{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/405071175", "html_url": "https://github.com/pytorch/pytorch/issues/8985#issuecomment-405071175", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8985", "id": 405071175, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTA3MTE3NQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-15T06:44:46Z", "updated_at": "2018-07-15T06:44:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So here is the code I'm using, copypasting much of BatchNorm, a bit of that could also be achieved by subclassing.</p>\n<pre><code>class CategoricalConditionalBatchNorm(torch.nn.Module):\n    \"\"\"\n    Similar to batch norm, but with per-category weight and bias.\n    \"\"\"\n    def __init__(self, num_features, num_cats, eps=2e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super().__init__()\n        self.num_features = num_features\n        self.num_cats = num_cats\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\n            self.bias = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n            self.num_batches_tracked.zero_()\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        if self.affine:\n            self.weight.data.fill_(1.0)\n            self.bias.data.zero_()\n\n    def forward(self, input, cats):\n        exponential_average_factor = 0.0\n\n        if self.training and self.track_running_stats:\n            self.num_batches_tracked += 1\n            if self.momentum is None:  # use cumulative moving average\n                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n            else:  # use exponential moving average\n                exponential_average_factor = self.momentum\n\n        out = torch.nn.functional.batch_norm(\n            input, self.running_mean, self.running_var, None, None,\n            self.training or not self.track_running_stats,\n            exponential_average_factor, self.eps)\n        if self.affine:\n            shape = [input.size(0), self.num_features] + (input.dim() - 2) * [1]\n            weight = self.weight.index_select(0, cats).view(shape)\n            bias = self.bias.index_select(0, cats).view(shape)\n            out = out * weight + bias\n        return out\n\n    def extra_repr(self):\n        return '{num_features}, num_cats={num_cats}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n</code></pre>", "body_text": "So here is the code I'm using, copypasting much of BatchNorm, a bit of that could also be achieved by subclassing.\nclass CategoricalConditionalBatchNorm(torch.nn.Module):\n    \"\"\"\n    Similar to batch norm, but with per-category weight and bias.\n    \"\"\"\n    def __init__(self, num_features, num_cats, eps=2e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super().__init__()\n        self.num_features = num_features\n        self.num_cats = num_cats\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\n            self.bias = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n            self.num_batches_tracked.zero_()\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        if self.affine:\n            self.weight.data.fill_(1.0)\n            self.bias.data.zero_()\n\n    def forward(self, input, cats):\n        exponential_average_factor = 0.0\n\n        if self.training and self.track_running_stats:\n            self.num_batches_tracked += 1\n            if self.momentum is None:  # use cumulative moving average\n                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n            else:  # use exponential moving average\n                exponential_average_factor = self.momentum\n\n        out = torch.nn.functional.batch_norm(\n            input, self.running_mean, self.running_var, None, None,\n            self.training or not self.track_running_stats,\n            exponential_average_factor, self.eps)\n        if self.affine:\n            shape = [input.size(0), self.num_features] + (input.dim() - 2) * [1]\n            weight = self.weight.index_select(0, cats).view(shape)\n            bias = self.bias.index_select(0, cats).view(shape)\n            out = out * weight + bias\n        return out\n\n    def extra_repr(self):\n        return '{num_features}, num_cats={num_cats}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)", "body": "So here is the code I'm using, copypasting much of BatchNorm, a bit of that could also be achieved by subclassing.\r\n```\r\nclass CategoricalConditionalBatchNorm(torch.nn.Module):\r\n    \"\"\"\r\n    Similar to batch norm, but with per-category weight and bias.\r\n    \"\"\"\r\n    def __init__(self, num_features, num_cats, eps=2e-5, momentum=0.1, affine=True,\r\n                 track_running_stats=True):\r\n        super().__init__()\r\n        self.num_features = num_features\r\n        self.num_cats = num_cats\r\n        self.eps = eps\r\n        self.momentum = momentum\r\n        self.affine = affine\r\n        self.track_running_stats = track_running_stats\r\n        if self.affine:\r\n            self.weight = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\r\n            self.bias = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\r\n        else:\r\n            self.register_parameter('weight', None)\r\n            self.register_parameter('bias', None)\r\n        if self.track_running_stats:\r\n            self.register_buffer('running_mean', torch.zeros(num_features))\r\n            self.register_buffer('running_var', torch.ones(num_features))\r\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\r\n        else:\r\n            self.register_parameter('running_mean', None)\r\n            self.register_parameter('running_var', None)\r\n            self.register_parameter('num_batches_tracked', None)\r\n        self.reset_parameters()\r\n\r\n    def reset_running_stats(self):\r\n        if self.track_running_stats:\r\n            self.running_mean.zero_()\r\n            self.running_var.fill_(1)\r\n            self.num_batches_tracked.zero_()\r\n\r\n    def reset_parameters(self):\r\n        self.reset_running_stats()\r\n        if self.affine:\r\n            self.weight.data.fill_(1.0)\r\n            self.bias.data.zero_()\r\n\r\n    def forward(self, input, cats):\r\n        exponential_average_factor = 0.0\r\n\r\n        if self.training and self.track_running_stats:\r\n            self.num_batches_tracked += 1\r\n            if self.momentum is None:  # use cumulative moving average\r\n                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\r\n            else:  # use exponential moving average\r\n                exponential_average_factor = self.momentum\r\n\r\n        out = torch.nn.functional.batch_norm(\r\n            input, self.running_mean, self.running_var, None, None,\r\n            self.training or not self.track_running_stats,\r\n            exponential_average_factor, self.eps)\r\n        if self.affine:\r\n            shape = [input.size(0), self.num_features] + (input.dim() - 2) * [1]\r\n            weight = self.weight.index_select(0, cats).view(shape)\r\n            bias = self.bias.index_select(0, cats).view(shape)\r\n            out = out * weight + bias\r\n        return out\r\n\r\n    def extra_repr(self):\r\n        return '{num_features}, num_cats={num_cats}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\r\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\r\n```"}