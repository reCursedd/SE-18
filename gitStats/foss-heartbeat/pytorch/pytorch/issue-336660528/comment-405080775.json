{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/405080775", "html_url": "https://github.com/pytorch/pytorch/issues/8985#issuecomment-405080775", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8985", "id": 405080775, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTA4MDc3NQ==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-15T10:11:14Z", "updated_at": "2018-07-15T10:11:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here's the code I'm using, think it's better to reduce code reuse and just use the components we have already:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">ConditionalBatchNorm2d</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_features</span>, <span class=\"pl-smi\">num_classes</span>):\n    <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.num_features <span class=\"pl-k\">=</span> num_features\n    <span class=\"pl-c1\">self</span>.bn <span class=\"pl-k\">=</span> nn.BatchNorm2d(num_features, <span class=\"pl-v\">affine</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-c1\">self</span>.embed <span class=\"pl-k\">=</span> nn.Embedding(num_classes, num_features <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-c1\">self</span>.embed.weight.data[:, :num_features].normal_(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0.02</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialise scale at N(1, 0.02)</span>\n    <span class=\"pl-c1\">self</span>.embed.weight.data[:, num_features:].zero_()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialise bias at 0</span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n    out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.bn(x)\n    gamma, beta <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embed(y).chunk(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>)\n    out <span class=\"pl-k\">=</span> gamma.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.num_features, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> out <span class=\"pl-k\">+</span> beta.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.num_features, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> out</pre></div>", "body_text": "Here's the code I'm using, think it's better to reduce code reuse and just use the components we have already:\nclass ConditionalBatchNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes):\n    super().__init__()\n    self.num_features = num_features\n    self.bn = nn.BatchNorm2d(num_features, affine=False)\n    self.embed = nn.Embedding(num_classes, num_features * 2)\n    self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n    self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n\n  def forward(self, x, y):\n    out = self.bn(x)\n    gamma, beta = self.embed(y).chunk(2, 1)\n    out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n    return out", "body": "Here's the code I'm using, think it's better to reduce code reuse and just use the components we have already:\r\n\r\n```py\r\nclass ConditionalBatchNorm2d(nn.Module):\r\n  def __init__(self, num_features, num_classes):\r\n    super().__init__()\r\n    self.num_features = num_features\r\n    self.bn = nn.BatchNorm2d(num_features, affine=False)\r\n    self.embed = nn.Embedding(num_classes, num_features * 2)\r\n    self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\r\n    self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\r\n\r\n  def forward(self, x, y):\r\n    out = self.bn(x)\r\n    gamma, beta = self.embed(y).chunk(2, 1)\r\n    out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\r\n    return out\r\n```"}