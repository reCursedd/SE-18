{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6000", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6000/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6000/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6000/events", "html_url": "https://github.com/pytorch/pytorch/pull/6000", "id": 308429047, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc3MzM5NTQx", "number": 6000, "title": "Added parameter range checks for all optimizers", "user": {"login": "lazypanda1", "id": 35884075, "node_id": "MDQ6VXNlcjM1ODg0MDc1", "avatar_url": "https://avatars0.githubusercontent.com/u/35884075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lazypanda1", "html_url": "https://github.com/lazypanda1", "followers_url": "https://api.github.com/users/lazypanda1/followers", "following_url": "https://api.github.com/users/lazypanda1/following{/other_user}", "gists_url": "https://api.github.com/users/lazypanda1/gists{/gist_id}", "starred_url": "https://api.github.com/users/lazypanda1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lazypanda1/subscriptions", "organizations_url": "https://api.github.com/users/lazypanda1/orgs", "repos_url": "https://api.github.com/users/lazypanda1/repos", "events_url": "https://api.github.com/users/lazypanda1/events{/privacy}", "received_events_url": "https://api.github.com/users/lazypanda1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-03-26T04:22:55Z", "updated_at": "2018-11-23T15:41:11Z", "closed_at": "2018-03-28T09:22:24Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6000", "html_url": "https://github.com/pytorch/pytorch/pull/6000", "diff_url": "https://github.com/pytorch/pytorch/pull/6000.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6000.patch"}, "body_html": "<p>This PR adds parameter range checks to all optimizers to ensure that end-users do not end up providing invalid values to the optimizers and be confused by the output when there is no actual problem with their model.</p>\n<p>For example, running the following program produces <code>NaN</code>s in the output, due to invalid value of <code>rho</code> (&gt;1.0).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nN, D_in, H, D_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>\n\nx <span class=\"pl-k\">=</span> Variable(torch.randn(N, D_in))\ny <span class=\"pl-k\">=</span> Variable(torch.randn(N, D_out), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\nmodel <span class=\"pl-k\">=</span> torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\nloss_fn <span class=\"pl-k\">=</span> torch.nn.MSELoss(<span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-4</span>\noptimizer <span class=\"pl-k\">=</span> torch.optim.Adadelta(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>learning_rate, <span class=\"pl-v\">rho</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.1</span>)\n<span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n    y_pred <span class=\"pl-k\">=</span> model(x)\n    loss <span class=\"pl-k\">=</span> loss_fn(y_pred, y)\n    <span class=\"pl-c1\">print</span>(t, loss.data[<span class=\"pl-c1\">0</span>])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()</pre></div>\n<p>Output:</p>\n<pre><code>0 651.8707885742188\n1 nan\n</code></pre>\n<p>I tried adding constraints for all the parameters that I could infer from the corresponding articles, but I am still missing some. Please feel free to suggest what should be bound for the ones which are missing.</p>\n<p>This is similar to the bounds check which I added for <a href=\"https://github.com/pytorch/pytorch/pull/5147\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/5147/hovercard\"><code>Adam Optimizer</code></a></p>\n<p>I can also add tests if needed.</p>", "body_text": "This PR adds parameter range checks to all optimizers to ensure that end-users do not end up providing invalid values to the optimizers and be confused by the output when there is no actual problem with their model.\nFor example, running the following program produces NaNs in the output, due to invalid value of rho (>1.0).\nimport torch\nfrom torch.autograd import Variable\n\nN, D_in, H, D_out = 64, 1000, 100, 10\n\nx = Variable(torch.randn(N, D_in))\ny = Variable(torch.randn(N, D_out), requires_grad=False)\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\nloss_fn = torch.nn.MSELoss(size_average=False)\n\nlearning_rate = 1e-4\noptimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=1.1)\nfor t in range(2):\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    print(t, loss.data[0])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\nOutput:\n0 651.8707885742188\n1 nan\n\nI tried adding constraints for all the parameters that I could infer from the corresponding articles, but I am still missing some. Please feel free to suggest what should be bound for the ones which are missing.\nThis is similar to the bounds check which I added for Adam Optimizer\nI can also add tests if needed.", "body": "This PR adds parameter range checks to all optimizers to ensure that end-users do not end up providing invalid values to the optimizers and be confused by the output when there is no actual problem with their model.\r\n\r\nFor example, running the following program produces `NaN`s in the output, due to invalid value of `rho` (>1.0).\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\n\r\nx = Variable(torch.randn(N, D_in))\r\ny = Variable(torch.randn(N, D_out), requires_grad=False)\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(D_in, H),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Linear(H, D_out),\r\n)\r\nloss_fn = torch.nn.MSELoss(size_average=False)\r\n\r\nlearning_rate = 1e-4\r\noptimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=1.1)\r\nfor t in range(2):\r\n    y_pred = model(x)\r\n    loss = loss_fn(y_pred, y)\r\n    print(t, loss.data[0])\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\n\r\nOutput:\r\n```\r\n0 651.8707885742188\r\n1 nan\r\n```\r\n\r\nI tried adding constraints for all the parameters that I could infer from the corresponding articles, but I am still missing some. Please feel free to suggest what should be bound for the ones which are missing.\r\n\r\nThis is similar to the bounds check which I added for [`Adam Optimizer`](https://github.com/pytorch/pytorch/pull/5147)\r\n\r\nI can also add tests if needed."}