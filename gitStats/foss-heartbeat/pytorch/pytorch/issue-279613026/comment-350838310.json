{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/350838310", "html_url": "https://github.com/pytorch/pytorch/issues/4048#issuecomment-350838310", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4048", "id": 350838310, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDgzODMxMA==", "user": {"login": "rkaplan", "id": 1389330, "node_id": "MDQ6VXNlcjEzODkzMzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1389330?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rkaplan", "html_url": "https://github.com/rkaplan", "followers_url": "https://api.github.com/users/rkaplan/followers", "following_url": "https://api.github.com/users/rkaplan/following{/other_user}", "gists_url": "https://api.github.com/users/rkaplan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rkaplan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rkaplan/subscriptions", "organizations_url": "https://api.github.com/users/rkaplan/orgs", "repos_url": "https://api.github.com/users/rkaplan/repos", "events_url": "https://api.github.com/users/rkaplan/events{/privacy}", "received_events_url": "https://api.github.com/users/rkaplan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-11T19:53:15Z", "updated_at": "2017-12-11T19:53:52Z", "author_association": "NONE", "body_html": "<p>I prefer a dict interface to using regular attribute access with nn.Module because in my case, the names of the modules are only known at runtime. (They are dependent on the config file). I have a multi-headed network with several heads doing different tasks; each head is a Module that I'd like to keep as the value of a dictionary whose key is the task name.</p>\n<p>Furthermore, due to the way my data is labeled, I can only backprop through one head at a time. For each image I only have one of the tasks labeled. So each minibatch requires forwarding through a different subset of the network. This is a primary reason I'm using PyTorch, and I think multi-task learning setups like this will only get more common. To choose which subset of the network the data gets forwarded through, I use a string provided by the data loader: ideally the string would just be the key for the heads ModuleDict.</p>\n<p>It's possible to do this using getattr and setattr, but given the amount of magic PyTorch does for these functions in nn.Module under the hood, it's preferable to have a simple dict abstraction. It seems like one of PyTorch's design goals is first-class support for dynamic graphs -- if that's true then a ModuleDict is a natural addition.</p>", "body_text": "I prefer a dict interface to using regular attribute access with nn.Module because in my case, the names of the modules are only known at runtime. (They are dependent on the config file). I have a multi-headed network with several heads doing different tasks; each head is a Module that I'd like to keep as the value of a dictionary whose key is the task name.\nFurthermore, due to the way my data is labeled, I can only backprop through one head at a time. For each image I only have one of the tasks labeled. So each minibatch requires forwarding through a different subset of the network. This is a primary reason I'm using PyTorch, and I think multi-task learning setups like this will only get more common. To choose which subset of the network the data gets forwarded through, I use a string provided by the data loader: ideally the string would just be the key for the heads ModuleDict.\nIt's possible to do this using getattr and setattr, but given the amount of magic PyTorch does for these functions in nn.Module under the hood, it's preferable to have a simple dict abstraction. It seems like one of PyTorch's design goals is first-class support for dynamic graphs -- if that's true then a ModuleDict is a natural addition.", "body": "I prefer a dict interface to using regular attribute access with nn.Module because in my case, the names of the modules are only known at runtime. (They are dependent on the config file). I have a multi-headed network with several heads doing different tasks; each head is a Module that I'd like to keep as the value of a dictionary whose key is the task name.\r\n\r\nFurthermore, due to the way my data is labeled, I can only backprop through one head at a time. For each image I only have one of the tasks labeled. So each minibatch requires forwarding through a different subset of the network. This is a primary reason I'm using PyTorch, and I think multi-task learning setups like this will only get more common. To choose which subset of the network the data gets forwarded through, I use a string provided by the data loader: ideally the string would just be the key for the heads ModuleDict.\r\n\r\nIt's possible to do this using getattr and setattr, but given the amount of magic PyTorch does for these functions in nn.Module under the hood, it's preferable to have a simple dict abstraction. It seems like one of PyTorch's design goals is first-class support for dynamic graphs -- if that's true then a ModuleDict is a natural addition."}