{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7138", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7138/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7138/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7138/events", "html_url": "https://github.com/pytorch/pytorch/issues/7138", "id": 319251741, "node_id": "MDU6SXNzdWUzMTkyNTE3NDE=", "number": 7138, "title": "DataParallel spills memory to GPU #0", "user": {"login": "LinxiFan", "id": 3018029, "node_id": "MDQ6VXNlcjMwMTgwMjk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3018029?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LinxiFan", "html_url": "https://github.com/LinxiFan", "followers_url": "https://api.github.com/users/LinxiFan/followers", "following_url": "https://api.github.com/users/LinxiFan/following{/other_user}", "gists_url": "https://api.github.com/users/LinxiFan/gists{/gist_id}", "starred_url": "https://api.github.com/users/LinxiFan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LinxiFan/subscriptions", "organizations_url": "https://api.github.com/users/LinxiFan/orgs", "repos_url": "https://api.github.com/users/LinxiFan/repos", "events_url": "https://api.github.com/users/LinxiFan/events{/privacy}", "received_events_url": "https://api.github.com/users/LinxiFan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-01T16:38:16Z", "updated_at": "2018-05-01T18:06:53Z", "closed_at": "2018-05-01T18:06:53Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>nn.DataParallel uses memory on GPU 0 even if it's explicitly instructed to use 2 and 3.</p>\n<h2>Code example</h2>\n<p>No line in this code snippet touches <code>cuda:0</code>.</p>\n<pre><code>import os\nimport torch\nimport torch.nn as nn\n\nclass MM(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.block1 = nn.Linear(10, 20)\n\n        # wrap block2 in DataParallel\n        self.block2 = nn.Linear(20, 20)\n        self.block2 = nn.DataParallel(self.block2, device_ids=[3, 2])\n\n        self.block3 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        return x\n\ndev='cuda:3'\n\nx=torch.zeros((64, 10), device=dev)\nmm=MM()\nmm.to(device=dev)\n\ny=mm(x)\nos.system('nvidia-smi')\n</code></pre>\n<p>However, nvidia-smi output indicates that GPU 0 is allocated memory</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      2407      G   /usr/lib/xorg/Xorg                            14MiB |\n|    0      5848      C   python                                       306MiB |\n|    2      5848      C   python                                       324MiB |\n|    3      5848      C   python                                       324MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.2 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.61<br>\nGPU models and configuration:<br>\nGPU 0: Tesla K80<br>\nGPU 1: Tesla K80<br>\nGPU 2: Tesla K80<br>\nGPU 3: Tesla K80</p>\n<p>Nvidia driver version: 384.111<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] msgpack-numpy (0.4.2)<br>\n[pip3] numpy (1.13.3)<br>\n[pip3] numpydoc (0.6.0)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.1)<br>\n[pip3] torchx (0.0.1, /home/jimfan/code/torchx)<br>\n[conda] cuda80                    1.0                           0    soumith<br>\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch<br>\n[conda] torchx                    0.0.1                     </p>", "body_text": "Issue description\nnn.DataParallel uses memory on GPU 0 even if it's explicitly instructed to use 2 and 3.\nCode example\nNo line in this code snippet touches cuda:0.\nimport os\nimport torch\nimport torch.nn as nn\n\nclass MM(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.block1 = nn.Linear(10, 20)\n\n        # wrap block2 in DataParallel\n        self.block2 = nn.Linear(20, 20)\n        self.block2 = nn.DataParallel(self.block2, device_ids=[3, 2])\n\n        self.block3 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        return x\n\ndev='cuda:3'\n\nx=torch.zeros((64, 10), device=dev)\nmm=MM()\nmm.to(device=dev)\n\ny=mm(x)\nos.system('nvidia-smi')\n\nHowever, nvidia-smi output indicates that GPU 0 is allocated memory\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      2407      G   /usr/lib/xorg/Xorg                            14MiB |\n|    0      5848      C   python                                       306MiB |\n|    2      5848      C   python                                       324MiB |\n|    3      5848      C   python                                       324MiB |\n+-----------------------------------------------------------------------------+\n\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.2 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: Tesla K80\nGPU 1: Tesla K80\nGPU 2: Tesla K80\nGPU 3: Tesla K80\nNvidia driver version: 384.111\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\nVersions of relevant libraries:\n[pip3] msgpack-numpy (0.4.2)\n[pip3] numpy (1.13.3)\n[pip3] numpydoc (0.6.0)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[pip3] torchx (0.0.1, /home/jimfan/code/torchx)\n[conda] cuda80                    1.0                           0    soumith\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch\n[conda] torchx                    0.0.1", "body": "## Issue description\r\n\r\nnn.DataParallel uses memory on GPU 0 even if it's explicitly instructed to use 2 and 3.\r\n\r\n## Code example\r\n\r\nNo line in this code snippet touches `cuda:0`.\r\n\r\n```\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass MM(nn.Module):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.block1 = nn.Linear(10, 20)\r\n\r\n        # wrap block2 in DataParallel\r\n        self.block2 = nn.Linear(20, 20)\r\n        self.block2 = nn.DataParallel(self.block2, device_ids=[3, 2])\r\n\r\n        self.block3 = nn.Linear(20, 20)\r\n\r\n    def forward(self, x):\r\n        x = self.block1(x)\r\n        x = self.block2(x)\r\n        x = self.block3(x)\r\n        return x\r\n\r\ndev='cuda:3'\r\n\r\nx=torch.zeros((64, 10), device=dev)\r\nmm=MM()\r\nmm.to(device=dev)\r\n\r\ny=mm(x)\r\nos.system('nvidia-smi')\r\n```\r\n\r\nHowever, nvidia-smi output indicates that GPU 0 is allocated memory\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2407      G   /usr/lib/xorg/Xorg                            14MiB |\r\n|    0      5848      C   python                                       306MiB |\r\n|    2      5848      C   python                                       324MiB |\r\n|    3      5848      C   python                                       324MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.2 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy (0.4.2)\r\n[pip3] numpy (1.13.3)\r\n[pip3] numpydoc (0.6.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[pip3] torchx (0.0.1, /home/jimfan/code/torchx)\r\n[conda] cuda80                    1.0                           0    soumith\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n[conda] torchx                    0.0.1                     <pip>\r\n"}