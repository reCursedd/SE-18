{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3546", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3546/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3546/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3546/events", "html_url": "https://github.com/pytorch/pytorch/issues/3546", "id": 271984601, "node_id": "MDU6SXNzdWUyNzE5ODQ2MDE=", "number": 3546, "title": "Installing from source failing on Ubuntu 17.10", "user": {"login": "latkins", "id": 1080217, "node_id": "MDQ6VXNlcjEwODAyMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1080217?v=4", "gravatar_id": "", "url": "https://api.github.com/users/latkins", "html_url": "https://github.com/latkins", "followers_url": "https://api.github.com/users/latkins/followers", "following_url": "https://api.github.com/users/latkins/following{/other_user}", "gists_url": "https://api.github.com/users/latkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/latkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/latkins/subscriptions", "organizations_url": "https://api.github.com/users/latkins/orgs", "repos_url": "https://api.github.com/users/latkins/repos", "events_url": "https://api.github.com/users/latkins/events{/privacy}", "received_events_url": "https://api.github.com/users/latkins/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-11-07T21:10:52Z", "updated_at": "2017-12-12T14:03:05Z", "closed_at": "2017-11-09T10:09:28Z", "author_association": "NONE", "body_html": "<p>When attempting to install from source on a new machine, I seem to be running into some errors with ATen:</p>\n<pre><code>running install\nrunning build_deps\n-- The C compiler identification is GNU 4.8.5\n-- The CXX compiler identification is GNU 4.8.5\n-- Check for working C compiler: /usr/bin/gcc-4.8\n-- Check for working C compiler: /usr/bin/gcc-4.8 -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/g++-4.8\n-- Check for working CXX compiler: /usr/bin/g++-4.8 -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/liam/lib/pytorch/torch/lib/build/nccl\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/reduce.o\nCompiling src/reduce_scatter.cu               &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\n\tptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    &gt; /home/liam/lib/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: /home/liam/lib/pytorch/torch/lib/tmp_install/include/nccl.h\n-- The C compiler identification is GNU 4.8.5\n-- The CXX compiler identification is GNU 4.8.5\n-- Check for working C compiler: /usr/bin/gcc-4.8\n-- Check for working C compiler: /usr/bin/gcc-4.8 -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/g++-4.8\n-- Check for working CXX compiler: /usr/bin/g++-4.8 -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \n-- Autodetected CUDA architecture(s): 6.1\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\nCMake Warning (dev) at /usr/share/cmake-3.9/Modules/FindOpenMP.cmake:200 (if):\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n  details.  Use the cmake_policy command to set the policy and suppress this\n  warning.\n\n  Quoted variables like \"c\" will no longer be dereferenced when the policy is\n  set to NEW.  Since the policy is not set the OLD behavior will be used.\nCall Stack (most recent call first):\n  /usr/share/cmake-3.9/Modules/FindOpenMP.cmake:324 (_OPENMP_GET_FLAGS)\n  CMakeLists.txt:125 (FIND_PACKAGE)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n-- Found OpenMP_C: -fopenmp (found version \"3.1\") \n-- Found OpenMP_CXX: -fopenmp (found version \"3.1\") \n-- Compiling with OpenMP support\n-- MAGMA not found. Compiling without MAGMA support\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- Looking for cpuid.h\n-- Looking for cpuid.h - found\n-- Performing Test HAVE_GCC_GET_CPUID\n-- Performing Test HAVE_GCC_GET_CPUID - Success\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Success\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Success\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Success\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Success\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Success\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Success\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Success\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Success\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Success\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Success\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Success\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Success\n-- SSE2 Found\n-- SSE3 Found\n-- AVX Found\n-- AVX2 Found\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl - guide - pthread - m]\n--   Library mkl: not found\n-- MKL library not found\n-- Checking for [openblas]\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\n-- Checking for [openblas - pthread]\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\n-- Checking for [goto2 - gfortran]\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\n-- Checking for [goto2 - gfortran - pthread]\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\n-- Checking for [acml - gfortran]\n--   Library acml: BLAS_acml_LIBRARY-NOTFOUND\n-- Checking for [Accelerate]\n--   Library Accelerate: BLAS_Accelerate_LIBRARY-NOTFOUND\n-- Checking for [vecLib]\n--   Library vecLib: BLAS_vecLib_LIBRARY-NOTFOUND\n-- Checking for [ptf77blas - atlas - gfortran]\n--   Library ptf77blas: BLAS_ptf77blas_LIBRARY-NOTFOUND\n-- Checking for [blas]\n--   Library blas: /usr/lib/x86_64-linux-gnu/libblas.so\n-- Looking for sgemm_\n-- Looking for sgemm_ - found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (generic).\n-- Looking for cheev_\n-- Looking for cheev_ - found\n-- Found a library with LAPACK API. (generic)\nCMake Deprecation Warning at src/ATen/CMakeLists.txt:7 (CMAKE_POLICY):\n  The OLD behavior for policy CMP0026 will be removed from a future version\n  of CMake.\n\n  The cmake-policies(7) manual explains that the OLD behaviors of all\n  policies are deprecated and that a policy should be set to OLD only under\n  specific short-term circumstances.  Projects should be ported to the NEW\n  behavior and not rely on setting a policy to OLD.\n\n\n-- Using python found in /home/liam/anaconda/bin/python\n['/home/liam/lib/pytorch/aten/src/THNN/generic/THNN.h', '/home/liam/lib/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/liam/lib/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: {'bernoulli', 'bernoulli_'}\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/liam/lib/pytorch/torch/lib/build/aten\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\n[  0%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\n['/home/liam/lib/pytorch/aten/src/THNN/generic/THNN.h', '/home/liam/lib/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/liam/lib/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: {'bernoulli', 'bernoulli_'}\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu:105:155: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));\n                                                                                                                                                           ^\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu:135:155: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));\n                                                                                                                                                           ^\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/LookupTable.cu(25): warning: function \"__shfl(int, int, int)\"\n/usr/local/cuda/include/sm_30_intrinsics.hpp(152): here was declared deprecated (\"__shfl() is deprecated in favor of __shfl_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\n\n/home/liam/lib/pytorch/aten/src/THCUNN/LookupTable.cu(42): warning: function \"__any\"\n/usr/local/cuda/include/device_atomic_functions.h(180): here was declared deprecated (\"__any() is deprecated in favor of __any_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\n\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                      ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT &gt;= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:428: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                            ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:249:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:249:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\n                                                                                                                                                                                                                                                                                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,\n                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\n                                                                                                                                                                                                                                                                                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,\n                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\n                                                                                                                                                                                                                                                                                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH &gt;= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT &gt;= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                         ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:447: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size(state, gradOutput, input-&gt;nDimension, dimn, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaVolumetricMaxUnpooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                         ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:435: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size(state, gradOutput, input-&gt;nDimension, dimn, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                         ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:453: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size(state, gradOutput, input-&gt;nDimension, dimn, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\nScanning dependencies of target ATen\n[ 53%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CPUGenerator.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Utils.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Context.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o\n[ 56%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/aten/src/ATen/ExpandUtils.h:3,\n                 from /home/liam/lib/pytorch/aten/src/ATen/ExpandUtils.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/aten/src/ATen/DLConvertor.h:3,\n                 from /home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Scalar.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Storage.h:3,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharStorage.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharStorage.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteType.h:2,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteType.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;_Tp2&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteTensor.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteTensor.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Storage.h:3,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteStorage.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteStorage.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Formatting.h:4,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;_Tp2&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.h:7,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;_Tp2&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.h:7,\n                 from /home/liam/lib/pytorch/aten/src/ATen/CUDAGenerator.cpp:4:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;_Tp2&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1730: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp: In function \u2018at::Backend at::getATenBackend(const DLContext&amp;)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:68:60: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n       throw std::logic_error(\"Unsupported device_type: \" + std::to_string(ctx.device_type));\n                                                            ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp: In function \u2018at::ScalarType at::toScalarType(const DLDataType&amp;)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:84:62: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n           throw std::logic_error(\"Unsupported kUInt bits \" + std::to_string(dtype.bits));\n                                                              ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:102:61: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n           throw std::logic_error(\"Unsupported kInt bits \" + std::to_string(dtype.bits));\n                                                             ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:117:63: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n           throw std::logic_error(\"Unsupported kFloat bits \" + std::to_string(dtype.bits));\n                                                               ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:121:52: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n       throw std::logic_error(\"Unsupported code \" + std::to_string(dtype.code));\n                                                    ^\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1778: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o] Error 1\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharType.h:2,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharType.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;_Tp2&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp: In function \u2018std::tuple&lt;double, long int&gt; at::__printFormat(std::ostream&amp;, const at::Tensor&amp;)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:61:8: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\n     if(std::isfinite(z)) {\n        ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:61:8: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isfinite\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:69:10: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\n   while(!std::isfinite(self_p[offset])) {\n          ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:69:10: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isfinite\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:85:10: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\n       if(std::isfinite(z)) {\n          ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:85:10: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isfinite\u2019\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1898: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1706: recipe for target 'src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1826: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o] Error 1\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharTensor.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharTensor.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if&lt;(! std::is_integral&lt;From&gt;::value), bool&gt;::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity &amp;&amp; std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN &amp;&amp; std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n&lt;built-in&gt;:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1874: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1658: recipe for target 'src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1754: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1682: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Context.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Context.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1850: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1946: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1922: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o] Error 1\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\nMakefile:129: recipe for target 'all' failed\nmake: *** [all] Error 2\n</code></pre>", "body_text": "When attempting to install from source on a new machine, I seem to be running into some errors with ATen:\nrunning install\nrunning build_deps\n-- The C compiler identification is GNU 4.8.5\n-- The CXX compiler identification is GNU 4.8.5\n-- Check for working C compiler: /usr/bin/gcc-4.8\n-- Check for working C compiler: /usr/bin/gcc-4.8 -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/g++-4.8\n-- Check for working CXX compiler: /usr/bin/g++-4.8 -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/liam/lib/pytorch/torch/lib/build/nccl\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          > /home/liam/lib/pytorch/torch/lib/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/reduce.o\nCompiling src/reduce_scatter.cu               > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\n\tptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    > /home/liam/lib/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    > /home/liam/lib/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: /home/liam/lib/pytorch/torch/lib/tmp_install/include/nccl.h\n-- The C compiler identification is GNU 4.8.5\n-- The CXX compiler identification is GNU 4.8.5\n-- Check for working C compiler: /usr/bin/gcc-4.8\n-- Check for working C compiler: /usr/bin/gcc-4.8 -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/g++-4.8\n-- Check for working CXX compiler: /usr/bin/g++-4.8 -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \n-- Autodetected CUDA architecture(s): 6.1\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\nCMake Warning (dev) at /usr/share/cmake-3.9/Modules/FindOpenMP.cmake:200 (if):\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n  details.  Use the cmake_policy command to set the policy and suppress this\n  warning.\n\n  Quoted variables like \"c\" will no longer be dereferenced when the policy is\n  set to NEW.  Since the policy is not set the OLD behavior will be used.\nCall Stack (most recent call first):\n  /usr/share/cmake-3.9/Modules/FindOpenMP.cmake:324 (_OPENMP_GET_FLAGS)\n  CMakeLists.txt:125 (FIND_PACKAGE)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n-- Found OpenMP_C: -fopenmp (found version \"3.1\") \n-- Found OpenMP_CXX: -fopenmp (found version \"3.1\") \n-- Compiling with OpenMP support\n-- MAGMA not found. Compiling without MAGMA support\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- Looking for cpuid.h\n-- Looking for cpuid.h - found\n-- Performing Test HAVE_GCC_GET_CPUID\n-- Performing Test HAVE_GCC_GET_CPUID - Success\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Success\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Success\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Success\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Success\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Success\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Success\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Success\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Success\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Success\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Success\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Success\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Success\n-- SSE2 Found\n-- SSE3 Found\n-- AVX Found\n-- AVX2 Found\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel_lp64: not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\n--   Library mkl_intel: not found\n-- Checking for [mkl - guide - pthread - m]\n--   Library mkl: not found\n-- MKL library not found\n-- Checking for [openblas]\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\n-- Checking for [openblas - pthread]\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\n-- Checking for [goto2 - gfortran]\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\n-- Checking for [goto2 - gfortran - pthread]\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\n-- Checking for [acml - gfortran]\n--   Library acml: BLAS_acml_LIBRARY-NOTFOUND\n-- Checking for [Accelerate]\n--   Library Accelerate: BLAS_Accelerate_LIBRARY-NOTFOUND\n-- Checking for [vecLib]\n--   Library vecLib: BLAS_vecLib_LIBRARY-NOTFOUND\n-- Checking for [ptf77blas - atlas - gfortran]\n--   Library ptf77blas: BLAS_ptf77blas_LIBRARY-NOTFOUND\n-- Checking for [blas]\n--   Library blas: /usr/lib/x86_64-linux-gnu/libblas.so\n-- Looking for sgemm_\n-- Looking for sgemm_ - found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (generic).\n-- Looking for cheev_\n-- Looking for cheev_ - found\n-- Found a library with LAPACK API. (generic)\nCMake Deprecation Warning at src/ATen/CMakeLists.txt:7 (CMAKE_POLICY):\n  The OLD behavior for policy CMP0026 will be removed from a future version\n  of CMake.\n\n  The cmake-policies(7) manual explains that the OLD behaviors of all\n  policies are deprecated and that a policy should be set to OLD only under\n  specific short-term circumstances.  Projects should be ported to the NEW\n  behavior and not rely on setting a policy to OLD.\n\n\n-- Using python found in /home/liam/anaconda/bin/python\n['/home/liam/lib/pytorch/aten/src/THNN/generic/THNN.h', '/home/liam/lib/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/liam/lib/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: {'bernoulli', 'bernoulli_'}\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/liam/lib/pytorch/torch/lib/build/aten\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\n[  0%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\n['/home/liam/lib/pytorch/aten/src/THNN/generic/THNN.h', '/home/liam/lib/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/liam/lib/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: {'bernoulli', 'bernoulli_'}\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu:105:155: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n                                                                                                                                                           ^\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu:135:155: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n                                                                                                                                                           ^\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/LookupTable.cu(25): warning: function \"__shfl(int, int, int)\"\n/usr/local/cuda/include/sm_30_intrinsics.hpp(152): here was declared deprecated (\"__shfl() is deprecated in favor of __shfl_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\n\n/home/liam/lib/pytorch/aten/src/THCUNN/LookupTable.cu(42): warning: function \"__any\"\n/usr/local/cuda/include/device_atomic_functions.h(180): here was declared deprecated (\"__any() is deprecated in favor of __any_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\n\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                      ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:428: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                            ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:249:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:249:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = outputTime * inputSlices * batchSize;\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\n                                                                                                                                                                                                                                                                                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,\n                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\n                                                                                                                                                                                                                                                                                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,\n                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\n                                                                                                                                                                                                                                                                                                        ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\n                                             ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\n                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     if ((outputTime - 1)*dT >= inputTime + padT)\n                                           ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\n   int inputTime;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                         ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:447: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size(state, gradOutput, input->nDimension, dimn, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaVolumetricMaxUnpooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                         ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:435: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size(state, gradOutput, input->nDimension, dimn, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputWidth;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int inputHeight;\n     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                         ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n   int totalZ = inputTime * inputSlices * batchSize;\n                                          ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:453: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\n     THCUNN_check_dim_size(state, gradOutput, input->nDimension, dimn, inputSlices);\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\n   int inputSlices;\n     ^\nScanning dependencies of target ATen\n[ 53%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CPUGenerator.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Utils.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Context.cpp.o\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o\n[ 56%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/aten/src/ATen/ExpandUtils.h:3,\n                 from /home/liam/lib/pytorch/aten/src/ATen/ExpandUtils.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/aten/src/ATen/DLConvertor.h:3,\n                 from /home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Scalar.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Storage.h:3,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharStorage.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharStorage.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteType.h:2,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteType.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteTensor.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteTensor.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Storage.h:3,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteStorage.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteStorage.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Formatting.h:4,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.h:7,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.h:7,\n                 from /home/liam/lib/pytorch/aten/src/ATen/CUDAGenerator.cpp:4:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1730: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp: In function \u2018at::Backend at::getATenBackend(const DLContext&)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:68:60: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n       throw std::logic_error(\"Unsupported device_type: \" + std::to_string(ctx.device_type));\n                                                            ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp: In function \u2018at::ScalarType at::toScalarType(const DLDataType&)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:84:62: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n           throw std::logic_error(\"Unsupported kUInt bits \" + std::to_string(dtype.bits));\n                                                              ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:102:61: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n           throw std::logic_error(\"Unsupported kInt bits \" + std::to_string(dtype.bits));\n                                                             ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:117:63: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n           throw std::logic_error(\"Unsupported kFloat bits \" + std::to_string(dtype.bits));\n                                                               ^\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:121:52: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n       throw std::logic_error(\"Unsupported code \" + std::to_string(dtype.code));\n                                                    ^\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1778: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o] Error 1\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharType.h:2,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharType.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp: In function \u2018std::tuple<double, long int> at::__printFormat(std::ostream&, const at::Tensor&)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:61:8: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\n     if(std::isfinite(z)) {\n        ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:61:8: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isfinite\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:69:10: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\n   while(!std::isfinite(self_p[offset])) {\n          ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:69:10: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isfinite\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:85:10: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\n       if(std::isfinite(z)) {\n          ^\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:85:10: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isfinite\u2019\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1898: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1706: recipe for target 'src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1826: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o] Error 1\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharTensor.h:8,\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharTensor.cpp:1:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\n   if (limit::has_infinity && std::isinf(f)) {\n                              ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\n                                ^\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\n     msg += std::to_string(f);\n            ^\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1874: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1658: recipe for target 'src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1754: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1682: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Context.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Context.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1850: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1946: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o] Error 1\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1922: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o] Error 1\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\nMakefile:129: recipe for target 'all' failed\nmake: *** [all] Error 2", "body": "When attempting to install from source on a new machine, I seem to be running into some errors with ATen:\r\n\r\n```\r\nrunning install\r\nrunning build_deps\r\n-- The C compiler identification is GNU 4.8.5\r\n-- The CXX compiler identification is GNU 4.8.5\r\n-- Check for working C compiler: /usr/bin/gcc-4.8\r\n-- Check for working C compiler: /usr/bin/gcc-4.8 -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/g++-4.8\r\n-- Check for working CXX compiler: /usr/bin/g++-4.8 -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/liam/lib/pytorch/torch/lib/build/nccl\r\nScanning dependencies of target nccl\r\n[100%] Generating lib/libnccl.so\r\nGrabbing  src/nccl.h                          > /home/liam/lib/pytorch/torch/lib/build/nccl/include/nccl.h\r\nCompiling src/libwrap.cu                      > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/libwrap.o\r\nCompiling src/core.cu                         > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/core.o\r\nCompiling src/all_gather.cu                   > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/all_gather.o\r\nCompiling src/all_reduce.cu                   > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/all_reduce.o\r\nCompiling src/broadcast.cu                    > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/broadcast.o\r\nCompiling src/reduce.cu                       > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/reduce.o\r\nCompiling src/reduce_scatter.cu               > /home/liam/lib/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\n\tptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nLinking   libnccl.so.1.3.5                    > /home/liam/lib/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\r\nArchiving libnccl_static.a                    > /home/liam/lib/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\r\n[100%] Built target nccl\r\nInstall the project...\r\n-- Install configuration: \"Release\"\r\n-- Installing: /home/liam/lib/pytorch/torch/lib/tmp_install/include/nccl.h\r\n-- The C compiler identification is GNU 4.8.5\r\n-- The CXX compiler identification is GNU 4.8.5\r\n-- Check for working C compiler: /usr/bin/gcc-4.8\r\n-- Check for working C compiler: /usr/bin/gcc-4.8 -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/g++-4.8\r\n-- Check for working CXX compiler: /usr/bin/g++-4.8 -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \r\n-- Autodetected CUDA architecture(s): 6.1\r\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\nCMake Warning (dev) at /usr/share/cmake-3.9/Modules/FindOpenMP.cmake:200 (if):\r\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\r\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\r\n  details.  Use the cmake_policy command to set the policy and suppress this\r\n  warning.\r\n\r\n  Quoted variables like \"c\" will no longer be dereferenced when the policy is\r\n  set to NEW.  Since the policy is not set the OLD behavior will be used.\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake-3.9/Modules/FindOpenMP.cmake:324 (_OPENMP_GET_FLAGS)\r\n  CMakeLists.txt:125 (FIND_PACKAGE)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found OpenMP_C: -fopenmp (found version \"3.1\") \r\n-- Found OpenMP_CXX: -fopenmp (found version \"3.1\") \r\n-- Compiling with OpenMP support\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_SSE1_1\r\n-- Performing Test C_HAS_SSE1_1 - Success\r\n-- Performing Test C_HAS_SSE2_1\r\n-- Performing Test C_HAS_SSE2_1 - Success\r\n-- Performing Test C_HAS_SSE3_1\r\n-- Performing Test C_HAS_SSE3_1 - Failed\r\n-- Performing Test C_HAS_SSE3_2\r\n-- Performing Test C_HAS_SSE3_2 - Success\r\n-- Performing Test C_HAS_SSE4_1_1\r\n-- Performing Test C_HAS_SSE4_1_1 - Failed\r\n-- Performing Test C_HAS_SSE4_1_2\r\n-- Performing Test C_HAS_SSE4_1_2 - Success\r\n-- Performing Test C_HAS_SSE4_2_1\r\n-- Performing Test C_HAS_SSE4_2_1 - Failed\r\n-- Performing Test C_HAS_SSE4_2_2\r\n-- Performing Test C_HAS_SSE4_2_2 - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Success\r\n-- Performing Test CXX_HAS_SSE1_1\r\n-- Performing Test CXX_HAS_SSE1_1 - Success\r\n-- Performing Test CXX_HAS_SSE2_1\r\n-- Performing Test CXX_HAS_SSE2_1 - Success\r\n-- Performing Test CXX_HAS_SSE3_1\r\n-- Performing Test CXX_HAS_SSE3_1 - Failed\r\n-- Performing Test CXX_HAS_SSE3_2\r\n-- Performing Test CXX_HAS_SSE3_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_1_1\r\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_2\r\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_2_1\r\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_2\r\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Success\r\n-- SSE2 Found\r\n-- SSE3 Found\r\n-- AVX Found\r\n-- AVX2 Found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Failed\r\n-- Performing Test HAS_MSC_ATOMICS\r\n-- Performing Test HAS_MSC_ATOMICS - Failed\r\n-- Performing Test HAS_GCC_ATOMICS\r\n-- Performing Test HAS_GCC_ATOMICS - Success\r\n-- Atomics: using GCC intrinsics\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl - guide - pthread - m]\r\n--   Library mkl: not found\r\n-- MKL library not found\r\n-- Checking for [openblas]\r\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\r\n-- Checking for [openblas - pthread]\r\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\r\n-- Checking for [goto2 - gfortran]\r\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\r\n-- Checking for [goto2 - gfortran - pthread]\r\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\r\n-- Checking for [acml - gfortran]\r\n--   Library acml: BLAS_acml_LIBRARY-NOTFOUND\r\n-- Checking for [Accelerate]\r\n--   Library Accelerate: BLAS_Accelerate_LIBRARY-NOTFOUND\r\n-- Checking for [vecLib]\r\n--   Library vecLib: BLAS_vecLib_LIBRARY-NOTFOUND\r\n-- Checking for [ptf77blas - atlas - gfortran]\r\n--   Library ptf77blas: BLAS_ptf77blas_LIBRARY-NOTFOUND\r\n-- Checking for [blas]\r\n--   Library blas: /usr/lib/x86_64-linux-gnu/libblas.so\r\n-- Looking for sgemm_\r\n-- Looking for sgemm_ - found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (generic).\r\n-- Looking for cheev_\r\n-- Looking for cheev_ - found\r\n-- Found a library with LAPACK API. (generic)\r\nCMake Deprecation Warning at src/ATen/CMakeLists.txt:7 (CMAKE_POLICY):\r\n  The OLD behavior for policy CMP0026 will be removed from a future version\r\n  of CMake.\r\n\r\n  The cmake-policies(7) manual explains that the OLD behaviors of all\r\n  policies are deprecated and that a policy should be set to OLD only under\r\n  specific short-term circumstances.  Projects should be ported to the NEW\r\n  behavior and not rely on setting a policy to OLD.\r\n\r\n\r\n-- Using python found in /home/liam/anaconda/bin/python\r\n['/home/liam/lib/pytorch/aten/src/THNN/generic/THNN.h', '/home/liam/lib/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/liam/lib/pytorch/aten/src/ATen/nn.yaml']\r\nATen Excluded: {'bernoulli', 'bernoulli_'}\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\ndisable contrib because ATEN_NO_CONTRIB is set\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/liam/lib/pytorch/torch/lib/build/aten\r\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\r\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\r\n[  0%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\r\n[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\r\n['/home/liam/lib/pytorch/aten/src/THNN/generic/THNN.h', '/home/liam/lib/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/liam/lib/pytorch/aten/src/ATen/nn.yaml']\r\nATen Excluded: {'bernoulli', 'bernoulli_'}\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\r\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu:105:155: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n                                                                                                                                                           ^\r\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THC/THCBlas.cu:135:155: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n                                                                                                                                                           ^\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/LookupTable.cu(25): warning: function \"__shfl(int, int, int)\"\r\n/usr/local/cuda/include/sm_30_intrinsics.hpp(152): here was declared deprecated (\"__shfl() is deprecated in favor of __shfl_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\r\n\r\n/home/liam/lib/pytorch/aten/src/THCUNN/LookupTable.cu(42): warning: function \"__any\"\r\n/usr/local/cuda/include/device_atomic_functions.h(180): here was declared deprecated (\"__any() is deprecated in favor of __any_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\r\n\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:422: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                      ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:185:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:183:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:181:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:217:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                                           ^\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:428: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                            ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:249:26: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:249:43: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = outputTime * inputSlices * batchSize;\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\r\n                                                                                                                                                                                                                                                                                                        ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,\r\n                                        ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\r\n                                                                                                                                                                                                                                                                                                        ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:195:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:193:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:184:47: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     outputTime   = (int)(floor((float)(inputTime - (dilationT * (kT - 1) + 1) + 2*padT) / dT)) + 1;\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int64_t indicesSizeRaw[4] = { batchSize * inputSlices,\r\n                                        ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:235:40: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:103:296: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THError(\"Given input size: (%dx%dx%dx%d). Calculated output size: (%dx%dx%dx%d). Output size is too small\",\r\n                                                                                                                                                                                                                                                                                                        ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                             ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                           ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:407: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                       ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = inputTime * inputSlices * batchSize;\r\n                         ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = inputTime * inputSlices * batchSize;\r\n                                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:447: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size(state, gradOutput, input->nDimension, dimn, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaVolumetricMaxUnpooling_updateOutput(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = inputTime * inputSlices * batchSize;\r\n                         ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = inputTime * inputSlices * batchSize;\r\n                                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:435: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size(state, gradOutput, input->nDimension, dimn, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:78:5: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int inputWidth;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:77:5: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int inputHeight;\r\n     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = inputTime * inputSlices * batchSize;\r\n                         ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:25: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:139:42: warning: \u2018batchSize\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int totalZ = inputTime * inputSlices * batchSize;\r\n                                          ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu: In function \u2018void THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int)\u2019:\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:61:453: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size(state, gradOutput, input->nDimension, dimn, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^\r\n/home/liam/lib/pytorch/aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu:19:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^\r\nScanning dependencies of target ATen\r\n[ 53%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CPUGenerator.cpp.o\r\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o\r\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o\r\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o\r\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o\r\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Utils.cpp.o\r\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Context.cpp.o\r\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o\r\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o\r\n[ 56%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o\r\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o\r\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o\r\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/ExpandUtils.h:3,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/ExpandUtils.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/DLConvertor.h:3,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Scalar.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Storage.h:3,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharStorage.h:8,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharStorage.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteType.h:2,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteType.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteTensor.h:8,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteTensor.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Storage.h:3,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteStorage.h:8,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUByteStorage.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Formatting.h:4,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.h:7,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\n[ 57%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o\r\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/Context.h:7,\r\n                 from /home/liam/lib/pytorch/aten/src/ATen/CUDAGenerator.cpp:4:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1730: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp: In function \u2018at::Backend at::getATenBackend(const DLContext&)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:68:60: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n       throw std::logic_error(\"Unsupported device_type: \" + std::to_string(ctx.device_type));\r\n                                                            ^\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp: In function \u2018at::ScalarType at::toScalarType(const DLDataType&)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:84:62: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n           throw std::logic_error(\"Unsupported kUInt bits \" + std::to_string(dtype.bits));\r\n                                                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:102:61: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n           throw std::logic_error(\"Unsupported kInt bits \" + std::to_string(dtype.bits));\r\n                                                             ^\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:117:63: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n           throw std::logic_error(\"Unsupported kFloat bits \" + std::to_string(dtype.bits));\r\n                                                               ^\r\n/home/liam/lib/pytorch/aten/src/ATen/DLConvertor.cpp:121:52: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n       throw std::logic_error(\"Unsupported code \" + std::to_string(dtype.code));\r\n                                                    ^\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1778: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o] Error 1\r\nIn file included from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Type.h:10:0,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharType.h:2,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharType.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<_Tp2>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp: In function \u2018std::tuple<double, long int> at::__printFormat(std::ostream&, const at::Tensor&)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:61:8: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\r\n     if(std::isfinite(z)) {\r\n        ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:61:8: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isfinite\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:69:10: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\r\n   while(!std::isfinite(self_p[offset])) {\r\n          ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:69:10: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isfinite\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:85:10: error: \u2018__builtin_isfinite\u2019 is not a member of \u2018std\u2019\r\n       if(std::isfinite(z)) {\r\n          ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Formatting.cpp:85:10: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isfinite\u2019\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1898: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1706: recipe for target 'src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1826: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o] Error 1\r\nIn file included from /home/liam/lib/pytorch/aten/src/ATen/Scalar.h:10:0,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/Tensor.h:4,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharTensor.h:8,\r\n                 from /home/liam/lib/pytorch/torch/lib/build/aten/src/ATen/ATen/CPUCharTensor.cpp:1:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018typename std::enable_if<(! std::is_integral<From>::value), bool>::type at::overflows(From)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: error: \u2018__builtin_isinf_sign\u2019 is not a member of \u2018std\u2019\r\n   if (limit::has_infinity && std::isinf(f)) {\r\n                              ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:29:30: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isinf_sign\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: error: \u2018__builtin_isnan\u2019 is not a member of \u2018std\u2019\r\n   if (!limit::has_quiet_NaN && std::isnan(f)) {\r\n                                ^\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:32:32: note: suggested alternative:\r\n<built-in>:0:0: note:   \u2018__builtin_isnan\u2019\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h: In function \u2018To at::checked_convert(From, const char*)\u2019:\r\n/home/liam/lib/pytorch/aten/src/ATen/Half.h:43:12: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n     msg += std::to_string(f);\r\n            ^\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1874: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1658: recipe for target 'src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1754: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1682: recipe for target 'src/ATen/CMakeFiles/ATen.dir/Context.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/Context.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1850: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1946: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o] Error 1\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1922: recipe for target 'src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o] Error 1\r\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\r\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```"}