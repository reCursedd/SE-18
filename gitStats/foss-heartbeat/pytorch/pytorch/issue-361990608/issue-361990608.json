{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11881", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11881/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11881/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11881/events", "html_url": "https://github.com/pytorch/pytorch/issues/11881", "id": 361990608, "node_id": "MDU6SXNzdWUzNjE5OTA2MDg=", "number": 11881, "title": "DataParallel with Multi-GPU module (eg. 2 model replica on 4 GPU)", "user": {"login": "YHRen", "id": 2334880, "node_id": "MDQ6VXNlcjIzMzQ4ODA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2334880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YHRen", "html_url": "https://github.com/YHRen", "followers_url": "https://api.github.com/users/YHRen/followers", "following_url": "https://api.github.com/users/YHRen/following{/other_user}", "gists_url": "https://api.github.com/users/YHRen/gists{/gist_id}", "starred_url": "https://api.github.com/users/YHRen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YHRen/subscriptions", "organizations_url": "https://api.github.com/users/YHRen/orgs", "repos_url": "https://api.github.com/users/YHRen/repos", "events_url": "https://api.github.com/users/YHRen/events{/privacy}", "received_events_url": "https://api.github.com/users/YHRen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-20T02:04:58Z", "updated_at": "2018-09-21T02:56:19Z", "closed_at": "2018-09-21T02:56:19Z", "author_association": "NONE", "body_html": "<p>We are working on distributing a large model's weights on two GPUs and then use DataParallel to train the model.<br>\nSo, suppose we have 4 GPUs with <code>device_id = [0,1,2,3]</code>, there are two replicas of the model. One replica lives on device 0 and 1, and the other lives on 2 and 3.<br>\nDuring the training, we received the following error message:</p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"dpmp_simple_cnn.py\", line 93, in <br>\nloss.backward()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/tensor.py\", line 93, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/<strong>init</strong>.py\", line 90, in backward<br>\nallow_unreachable=True)  # allow_unreachable flag<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/function.py\", line 76, in apply<br>\nreturn self._forward_cls.backward(self, *args)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/_functions.py\", line 30, in backward<br>\nreturn (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/_functions.py\", line 41, in forward<br>\nreturn comm.reduce_add_coalesced(grads, destination)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/cuda/comm.py\", line 120, in reduce_add_coalesced<br>\nflat_result = reduce_add(flat_tensors, destination)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/cuda/comm.py\", line 73, in reduce_add<br>\nraise RuntimeError(\"reduce_add expects destination to be on the same GPU with one of the tensors\")<br>\nRuntimeError: reduce_add expects destination to be on the same GPU with one of the tensors</p>\n</blockquote>\n<p>We think the error traces back to the <code>reduce_add_coalesced()</code> function, where to improve the efficiency of the data transfer, all grads are flattened and packed into fix-sized (about 10MB by default) buffers.<br>\nIn our case, part of the grads is on the first device, and the other part on the second device, which leads to the above error.</p>\n<p>If we reduce the number of parameters of the model within the buffer size, the error will not occur, and the model converges.</p>\n<p>System Info: pytorch 0.4.1, python 2.7, on 4 V100 (32GB version).</p>\n<p>Here is a minimum code to reproduce the problem, where one can tune the size of the model via the variable <code>tmp</code> during the initialization. (tmp&lt;=5 will be a small model, and tmp&gt;=12 will be a large model).</p>\n<p>`import torch<br>\nimport torch.nn as nn<br>\nfrom collections import OrderedDict<br>\nfrom torchvision import datasets, transforms</p>\n<p>batch_size = 128<br>\ntrain_loader = torch.utils.data.DataLoader(<br>\ndatasets.MNIST('../data', train=True, download=True,<br>\ntransform=transforms.Compose([<br>\ntransforms.ToTensor(),<br>\ntransforms.Normalize((0.1307,), (0.3081,))<br>\n])),<br>\nbatch_size=batch_size, shuffle=True)</p>\n<p>class ConvBlck(nn.Module):<br>\ndef <strong>init</strong>(self, in_channel, out_channel, kernel_size, stride=1):<br>\nsuper(ConvBlck, self).<strong>init</strong>()<br>\nself.blck = nn.Sequential( OrderedDict([<br>\n('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),<br>\n('relu', nn.ReLU() )]) )<br>\ndef forward(self, x):<br>\nreturn self.blck(x)</p>\n<p>class SixConv(nn.Module):<br>\ndef <strong>init</strong>(self):<br>\nsuper(SixConv, self).<strong>init</strong>()<br>\nself.conv1 = ConvBlck(1,16,5,2)<br>\nself.conv2 = ConvBlck(16,32,5,2)<br>\nself.conv3 = ConvBlck(32,16,5,2)<br>\nself.conv4 = nn.Sequential( nn.AdaptiveAvgPool2d(1) , nn.Conv2d(16, 10, 1, 1) )</p>\n<pre><code>def forward(self, x):\n    x = self.conv1(x)\n    x = self.conv2(x)\n    next_index = x.device.index + 1\n    x = x.cuda(next_index)\n    tmp_conv3 = self.conv3.cuda(next_index)\n    tmp_conv4 = self.conv4.cuda(next_index)\n    x = tmp_conv3(x)\n    x = tmp_conv4(x)\n    x = x.cuda(next_index-1)\n    return x\n</code></pre>\n<p>model = SixConv()<br>\nmodel = nn.DataParallel(model,device_ids=[0,2]);<br>\ncrite = torch.nn.CrossEntropyLoss()<br>\noptim = torch.optim.Adam( model.parameters(), lr=0.001)<br>\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'<br>\nmodel.to( device )</p>\n<p>losses = []<br>\nfor x,y in train_loader:<br>\noptim.zero_grad()<br>\nx, y = x.to(device), y.to(device)<br>\npred = model(x)<br>\nloss = crite(pred.squeeze(), y)<br>\nloss.backward()<br>\noptim.step()<br>\nlosses.append(loss.item())<br>\nprint(losses)</p>\n<p>`</p>\n<p>I think, which might be wrong, the function <code>reduce_add_coalesced()</code> expects all the inputs are from the same device, which is not true in our case.<br>\nAny suggestions to work around this are welcome.<br>\nThank you in advance.</p>", "body_text": "We are working on distributing a large model's weights on two GPUs and then use DataParallel to train the model.\nSo, suppose we have 4 GPUs with device_id = [0,1,2,3], there are two replicas of the model. One replica lives on device 0 and 1, and the other lives on 2 and 3.\nDuring the training, we received the following error message:\n\nTraceback (most recent call last):\nFile \"dpmp_simple_cnn.py\", line 93, in \nloss.backward()\nFile \"/usr/local/lib/python2.7/dist-packages/torch/tensor.py\", line 93, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/init.py\", line 90, in backward\nallow_unreachable=True)  # allow_unreachable flag\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/function.py\", line 76, in apply\nreturn self._forward_cls.backward(self, *args)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/_functions.py\", line 30, in backward\nreturn (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/_functions.py\", line 41, in forward\nreturn comm.reduce_add_coalesced(grads, destination)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/cuda/comm.py\", line 120, in reduce_add_coalesced\nflat_result = reduce_add(flat_tensors, destination)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/cuda/comm.py\", line 73, in reduce_add\nraise RuntimeError(\"reduce_add expects destination to be on the same GPU with one of the tensors\")\nRuntimeError: reduce_add expects destination to be on the same GPU with one of the tensors\n\nWe think the error traces back to the reduce_add_coalesced() function, where to improve the efficiency of the data transfer, all grads are flattened and packed into fix-sized (about 10MB by default) buffers.\nIn our case, part of the grads is on the first device, and the other part on the second device, which leads to the above error.\nIf we reduce the number of parameters of the model within the buffer size, the error will not occur, and the model converges.\nSystem Info: pytorch 0.4.1, python 2.7, on 4 V100 (32GB version).\nHere is a minimum code to reproduce the problem, where one can tune the size of the model via the variable tmp during the initialization. (tmp<=5 will be a small model, and tmp>=12 will be a large model).\n`import torch\nimport torch.nn as nn\nfrom collections import OrderedDict\nfrom torchvision import datasets, transforms\nbatch_size = 128\ntrain_loader = torch.utils.data.DataLoader(\ndatasets.MNIST('../data', train=True, download=True,\ntransform=transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307,), (0.3081,))\n])),\nbatch_size=batch_size, shuffle=True)\nclass ConvBlck(nn.Module):\ndef init(self, in_channel, out_channel, kernel_size, stride=1):\nsuper(ConvBlck, self).init()\nself.blck = nn.Sequential( OrderedDict([\n('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n('relu', nn.ReLU() )]) )\ndef forward(self, x):\nreturn self.blck(x)\nclass SixConv(nn.Module):\ndef init(self):\nsuper(SixConv, self).init()\nself.conv1 = ConvBlck(1,16,5,2)\nself.conv2 = ConvBlck(16,32,5,2)\nself.conv3 = ConvBlck(32,16,5,2)\nself.conv4 = nn.Sequential( nn.AdaptiveAvgPool2d(1) , nn.Conv2d(16, 10, 1, 1) )\ndef forward(self, x):\n    x = self.conv1(x)\n    x = self.conv2(x)\n    next_index = x.device.index + 1\n    x = x.cuda(next_index)\n    tmp_conv3 = self.conv3.cuda(next_index)\n    tmp_conv4 = self.conv4.cuda(next_index)\n    x = tmp_conv3(x)\n    x = tmp_conv4(x)\n    x = x.cuda(next_index-1)\n    return x\n\nmodel = SixConv()\nmodel = nn.DataParallel(model,device_ids=[0,2]);\ncrite = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam( model.parameters(), lr=0.001)\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\nmodel.to( device )\nlosses = []\nfor x,y in train_loader:\noptim.zero_grad()\nx, y = x.to(device), y.to(device)\npred = model(x)\nloss = crite(pred.squeeze(), y)\nloss.backward()\noptim.step()\nlosses.append(loss.item())\nprint(losses)\n`\nI think, which might be wrong, the function reduce_add_coalesced() expects all the inputs are from the same device, which is not true in our case.\nAny suggestions to work around this are welcome.\nThank you in advance.", "body": "We are working on distributing a large model's weights on two GPUs and then use DataParallel to train the model. \r\nSo, suppose we have 4 GPUs with `device_id = [0,1,2,3]`, there are two replicas of the model. One replica lives on device 0 and 1, and the other lives on 2 and 3.\r\nDuring the training, we received the following error message:\r\n\r\n\r\n> Traceback (most recent call last):\r\n  File \"dpmp_simple_cnn.py\", line 93, in <module>\r\n    loss.backward()\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.py\", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/function.py\", line 76, in apply\r\n    return self._forward_cls.backward(self, *args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/_functions.py\", line 30, in backward\r\n    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/_functions.py\", line 41, in forward\r\n    return comm.reduce_add_coalesced(grads, destination)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/cuda/comm.py\", line 120, in reduce_add_coalesced\r\n    flat_result = reduce_add(flat_tensors, destination)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/cuda/comm.py\", line 73, in reduce_add\r\n    raise RuntimeError(\"reduce_add expects destination to be on the same GPU with one of the tensors\")\r\nRuntimeError: reduce_add expects destination to be on the same GPU with one of the tensors\r\n\r\nWe think the error traces back to the `reduce_add_coalesced()` function, where to improve the efficiency of the data transfer, all grads are flattened and packed into fix-sized (about 10MB by default) buffers. \r\nIn our case, part of the grads is on the first device, and the other part on the second device, which leads to the above error.\r\n\r\nIf we reduce the number of parameters of the model within the buffer size, the error will not occur, and the model converges. \r\n\r\nSystem Info: pytorch 0.4.1, python 2.7, on 4 V100 (32GB version).\r\n\r\nHere is a minimum code to reproduce the problem, where one can tune the size of the model via the variable `tmp` during the initialization. (tmp<=5 will be a small model, and tmp>=12 will be a large model).\r\n\r\n`import torch\r\nimport torch.nn as nn\r\nfrom collections import OrderedDict\r\nfrom torchvision import datasets, transforms\r\n\r\nbatch_size = 128\r\ntrain_loader = torch.utils.data.DataLoader(\r\n        datasets.MNIST('../data', train=True, download=True,\r\n                       transform=transforms.Compose([\r\n                           transforms.ToTensor(),\r\n                           transforms.Normalize((0.1307,), (0.3081,))\r\n                       ])),\r\nbatch_size=batch_size, shuffle=True)\r\n\r\nclass ConvBlck(nn.Module):\r\n    def __init__(self, in_channel, out_channel, kernel_size, stride=1):\r\n        super(ConvBlck, self).__init__()\r\n        self.blck = nn.Sequential( OrderedDict([\r\n            ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\r\n            ('relu', nn.ReLU() )]) )\r\n    def forward(self, x):\r\n        return self.blck(x)\r\n\r\nclass SixConv(nn.Module):\r\n    def __init__(self):\r\n        super(SixConv, self).__init__()\r\n        self.conv1 = ConvBlck(1,16,5,2)\r\n        self.conv2 = ConvBlck(16,32,5,2)\r\n        self.conv3 = ConvBlck(32,16,5,2)\r\n        self.conv4 = nn.Sequential( nn.AdaptiveAvgPool2d(1) , nn.Conv2d(16, 10, 1, 1) )\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        next_index = x.device.index + 1\r\n        x = x.cuda(next_index)\r\n        tmp_conv3 = self.conv3.cuda(next_index)\r\n        tmp_conv4 = self.conv4.cuda(next_index)\r\n        x = tmp_conv3(x)\r\n        x = tmp_conv4(x)\r\n        x = x.cuda(next_index-1)\r\n        return x\r\n\r\nmodel = SixConv()\r\nmodel = nn.DataParallel(model,device_ids=[0,2]);\r\ncrite = torch.nn.CrossEntropyLoss()\r\noptim = torch.optim.Adam( model.parameters(), lr=0.001)\r\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\r\nmodel.to( device )\r\n\r\nlosses = []\r\nfor x,y in train_loader:\r\n    optim.zero_grad()\r\n    x, y = x.to(device), y.to(device)\r\n    pred = model(x)\r\n    loss = crite(pred.squeeze(), y)\r\n    loss.backward()\r\n    optim.step()\r\n    losses.append(loss.item())\r\nprint(losses)\r\n\r\n`\r\n\r\nI think, which might be wrong, the function `reduce_add_coalesced()` expects all the inputs are from the same device, which is not true in our case. \r\nAny suggestions to work around this are welcome.\r\nThank you in advance. "}