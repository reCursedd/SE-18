{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423309483", "html_url": "https://github.com/pytorch/pytorch/issues/11881#issuecomment-423309483", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11881", "id": 423309483, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzMwOTQ4Mw==", "user": {"login": "YHRen", "id": 2334880, "node_id": "MDQ6VXNlcjIzMzQ4ODA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2334880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YHRen", "html_url": "https://github.com/YHRen", "followers_url": "https://api.github.com/users/YHRen/followers", "following_url": "https://api.github.com/users/YHRen/following{/other_user}", "gists_url": "https://api.github.com/users/YHRen/gists{/gist_id}", "starred_url": "https://api.github.com/users/YHRen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YHRen/subscriptions", "organizations_url": "https://api.github.com/users/YHRen/orgs", "repos_url": "https://api.github.com/users/YHRen/repos", "events_url": "https://api.github.com/users/YHRen/events{/privacy}", "received_events_url": "https://api.github.com/users/YHRen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T19:46:33Z", "updated_at": "2018-09-20T19:46:33Z", "author_association": "NONE", "body_html": "<p>The walk-around we found is to <code>DaraParallel</code> the head of the model and the tail of the model separately.<br>\nAnd wrap the data transfer in the <code>forward</code> function.<br>\nHere is a working example. Hopefully, someone finds this useful.<br>\nThanks. This issue can be closed.</p>\n<pre><code>\nclass ConvBlck(nn.Module):\n    def __init__(self, in_channel, out_channel, kernel_size, stride=1):\n        super(ConvBlck, self).__init__()\n        self.blck = nn.Sequential( OrderedDict([\n            ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n            ('relu', nn.ReLU() )]) )\n    def forward(self, x):\n        return self.blck(x)\n\nclass SixConv(nn.Module):\n    def __init__(self):\n        super(SixConv, self).__init__()\n        tmp = 5\n        self.conv1 = nn.Sequential( ConvBlck(1,1&lt;&lt;tmp,5,2), ConvBlck(1&lt;&lt;tmp,1&lt;&lt;(tmp+1),5,2) )\n        self.conv2 = nn.Sequential( ConvBlck(1&lt;&lt;(tmp+1),1&lt;&lt;tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1&lt;&lt;tmp, 10, 1, 1) )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        next_index = x.device.index + 1\n        x = x.cuda(next_index)\n        x = self.conv2(x)\n        return x\nmodel = SixConv()\nmodel.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\nmodel.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\ncrite = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam( model.parameters() , lr=0.001)\nmodel.conv1.to( 'cuda:0' )\nmodel.conv2.to( 'cuda:1' )\n\n</code></pre>", "body_text": "The walk-around we found is to DaraParallel the head of the model and the tail of the model separately.\nAnd wrap the data transfer in the forward function.\nHere is a working example. Hopefully, someone finds this useful.\nThanks. This issue can be closed.\n\nclass ConvBlck(nn.Module):\n    def __init__(self, in_channel, out_channel, kernel_size, stride=1):\n        super(ConvBlck, self).__init__()\n        self.blck = nn.Sequential( OrderedDict([\n            ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n            ('relu', nn.ReLU() )]) )\n    def forward(self, x):\n        return self.blck(x)\n\nclass SixConv(nn.Module):\n    def __init__(self):\n        super(SixConv, self).__init__()\n        tmp = 5\n        self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )\n        self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        next_index = x.device.index + 1\n        x = x.cuda(next_index)\n        x = self.conv2(x)\n        return x\nmodel = SixConv()\nmodel.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\nmodel.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\ncrite = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam( model.parameters() , lr=0.001)\nmodel.conv1.to( 'cuda:0' )\nmodel.conv2.to( 'cuda:1' )", "body": "The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. \r\nAnd wrap the data transfer in the `forward` function.\r\nHere is a working example. Hopefully, someone finds this useful. \r\nThanks. This issue can be closed. \r\n```\r\n\r\nclass ConvBlck(nn.Module):\r\n    def __init__(self, in_channel, out_channel, kernel_size, stride=1):\r\n        super(ConvBlck, self).__init__()\r\n        self.blck = nn.Sequential( OrderedDict([\r\n            ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\r\n            ('relu', nn.ReLU() )]) )\r\n    def forward(self, x):\r\n        return self.blck(x)\r\n\r\nclass SixConv(nn.Module):\r\n    def __init__(self):\r\n        super(SixConv, self).__init__()\r\n        tmp = 5\r\n        self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )\r\n        self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        next_index = x.device.index + 1\r\n        x = x.cuda(next_index)\r\n        x = self.conv2(x)\r\n        return x\r\nmodel = SixConv()\r\nmodel.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\r\nmodel.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\r\ncrite = torch.nn.CrossEntropyLoss()\r\noptim = torch.optim.Adam( model.parameters() , lr=0.001)\r\nmodel.conv1.to( 'cuda:0' )\r\nmodel.conv2.to( 'cuda:1' )\r\n\r\n```"}