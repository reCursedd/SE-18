{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/364673476", "html_url": "https://github.com/pytorch/pytorch/issues/4757#issuecomment-364673476", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4757", "id": 364673476, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDY3MzQ3Ng==", "user": {"login": "jpilaul", "id": 614861, "node_id": "MDQ6VXNlcjYxNDg2MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/614861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpilaul", "html_url": "https://github.com/jpilaul", "followers_url": "https://api.github.com/users/jpilaul/followers", "following_url": "https://api.github.com/users/jpilaul/following{/other_user}", "gists_url": "https://api.github.com/users/jpilaul/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpilaul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpilaul/subscriptions", "organizations_url": "https://api.github.com/users/jpilaul/orgs", "repos_url": "https://api.github.com/users/jpilaul/repos", "events_url": "https://api.github.com/users/jpilaul/events{/privacy}", "received_events_url": "https://api.github.com/users/jpilaul/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-10T17:21:40Z", "updated_at": "2018-02-10T17:21:40Z", "author_association": "NONE", "body_html": "<p>It's actually related to my seq_lengths vector in torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True). If batch_in is of size batch_size x max_sequence_length x embedding_dimension, we need max(seq_lengths) = max_sequence_length. In the previous version, I didn't get error when max(seq_lengths) &gt; max_sequence_length. In this update, I am getting the error that I have explained above. So, to conclude it is not related to the RNN but to the length vector.</p>", "body_text": "It's actually related to my seq_lengths vector in torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True). If batch_in is of size batch_size x max_sequence_length x embedding_dimension, we need max(seq_lengths) = max_sequence_length. In the previous version, I didn't get error when max(seq_lengths) > max_sequence_length. In this update, I am getting the error that I have explained above. So, to conclude it is not related to the RNN but to the length vector.", "body": "It's actually related to my seq_lengths vector in torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True). If batch_in is of size batch_size x max_sequence_length x embedding_dimension, we need max(seq_lengths) = max_sequence_length. In the previous version, I didn't get error when max(seq_lengths) > max_sequence_length. In this update, I am getting the error that I have explained above. So, to conclude it is not related to the RNN but to the length vector."}