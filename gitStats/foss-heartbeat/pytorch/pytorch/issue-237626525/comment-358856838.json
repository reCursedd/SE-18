{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358856838", "html_url": "https://github.com/pytorch/pytorch/issues/1868#issuecomment-358856838", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1868", "id": 358856838, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODg1NjgzOA==", "user": {"login": "yukw777", "id": 2057325, "node_id": "MDQ6VXNlcjIwNTczMjU=", "avatar_url": "https://avatars2.githubusercontent.com/u/2057325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yukw777", "html_url": "https://github.com/yukw777", "followers_url": "https://api.github.com/users/yukw777/followers", "following_url": "https://api.github.com/users/yukw777/following{/other_user}", "gists_url": "https://api.github.com/users/yukw777/gists{/gist_id}", "starred_url": "https://api.github.com/users/yukw777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yukw777/subscriptions", "organizations_url": "https://api.github.com/users/yukw777/orgs", "repos_url": "https://api.github.com/users/yukw777/repos", "events_url": "https://api.github.com/users/yukw777/events{/privacy}", "received_events_url": "https://api.github.com/users/yukw777/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T03:30:13Z", "updated_at": "2018-01-19T03:30:13Z", "author_association": "NONE", "body_html": "<p>My code had almost the same structure as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5655024\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Louis-Tian\">@Louis-Tian</a>'s example, and I was able to get around it by putting a lock where I instantiate my module in each thread. Working code below (pay attention to the lock)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> threading\n<span class=\"pl-k\">import</span> torch.functional <span class=\"pl-k\">as</span> f\n<span class=\"pl-k\">from</span> concurrent.futures <span class=\"pl-k\">import</span> ThreadPoolExecutor <span class=\"pl-k\">as</span> ThreadPool\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build</span>(<span class=\"pl-smi\">cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    nn <span class=\"pl-k\">=</span> torch.nn.Sequential(\n        torch.nn.Linear(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">1024</span>),\n        torch.nn.Linear(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">1</span>)\n    )\n\n    <span class=\"pl-k\">return</span> nn.cuda() <span class=\"pl-k\">if</span> cuda <span class=\"pl-k\">else</span> nn\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">nn</span>, <span class=\"pl-smi\">X</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>):\n    X <span class=\"pl-k\">=</span> torch.autograd.Variable(X)\n    y <span class=\"pl-k\">=</span> torch.autograd.Variable(y)\n    optim <span class=\"pl-k\">=</span> torch.optim.SGD(nn.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epoch):\n        yhat <span class=\"pl-k\">=</span> nn(X)\n        loss <span class=\"pl-k\">=</span> ((yhat <span class=\"pl-k\">-</span> y) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>).mean()\n        loss.backward()\n        optim.step()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">data</span>(<span class=\"pl-smi\">cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    X <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1024</span>)\n    y <span class=\"pl-k\">=</span> torch.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>))\n    <span class=\"pl-k\">return</span> (X.cuda(), y.cuda()) <span class=\"pl-k\">if</span> cuda <span class=\"pl-k\">else</span> (X, y)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">cpu_run</span>(<span class=\"pl-smi\">lock</span>):\n    <span class=\"pl-k\">with</span> lock:\n        nn <span class=\"pl-k\">=</span> build(<span class=\"pl-v\">cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    d <span class=\"pl-k\">=</span> data(<span class=\"pl-v\">cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    train(nn, <span class=\"pl-k\">*</span>d)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">thread_cpu_run</span>():\n    pool <span class=\"pl-k\">=</span> ThreadPool()\n    lock <span class=\"pl-k\">=</span> threading.Lock()\n    threads <span class=\"pl-k\">=</span> pool.map(cpu_run, [lock <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5</span>)])\n\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">list</span>(threads)\n\nthread_cpu_run()</pre></div>", "body_text": "My code had almost the same structure as @Louis-Tian's example, and I was able to get around it by putting a lock where I instantiate my module in each thread. Working code below (pay attention to the lock)\nimport torch\nimport threading\nimport torch.functional as f\nfrom concurrent.futures import ThreadPoolExecutor as ThreadPool\n\n\ndef build(cuda=False):\n    nn = torch.nn.Sequential(\n        torch.nn.Linear(1024, 1024),\n        torch.nn.Linear(1024, 1)\n    )\n\n    return nn.cuda() if cuda else nn\n\ndef train(nn, X, y, epoch=100):\n    X = torch.autograd.Variable(X)\n    y = torch.autograd.Variable(y)\n    optim = torch.optim.SGD(nn.parameters(), lr=0.1)\n    for i in range(epoch):\n        yhat = nn(X)\n        loss = ((yhat - y) ** 2).mean()\n        loss.backward()\n        optim.step()\n\ndef data(cuda=False):\n    X = torch.zeros(10, 1024)\n    y = torch.zeros((10, 1))\n    return (X.cuda(), y.cuda()) if cuda else (X, y)\n\ndef cpu_run(lock):\n    with lock:\n        nn = build(cuda=False)\n    d = data(cuda=False)\n    train(nn, *d)\n\ndef thread_cpu_run():\n    pool = ThreadPool()\n    lock = threading.Lock()\n    threads = pool.map(cpu_run, [lock for _ in range(5)])\n\n    return list(threads)\n\nthread_cpu_run()", "body": "My code had almost the same structure as @Louis-Tian's example, and I was able to get around it by putting a lock where I instantiate my module in each thread. Working code below (pay attention to the lock)\r\n\r\n```python\r\nimport torch\r\nimport threading\r\nimport torch.functional as f\r\nfrom concurrent.futures import ThreadPoolExecutor as ThreadPool\r\n\r\n\r\ndef build(cuda=False):\r\n    nn = torch.nn.Sequential(\r\n        torch.nn.Linear(1024, 1024),\r\n        torch.nn.Linear(1024, 1)\r\n    )\r\n\r\n    return nn.cuda() if cuda else nn\r\n\r\ndef train(nn, X, y, epoch=100):\r\n    X = torch.autograd.Variable(X)\r\n    y = torch.autograd.Variable(y)\r\n    optim = torch.optim.SGD(nn.parameters(), lr=0.1)\r\n    for i in range(epoch):\r\n        yhat = nn(X)\r\n        loss = ((yhat - y) ** 2).mean()\r\n        loss.backward()\r\n        optim.step()\r\n\r\ndef data(cuda=False):\r\n    X = torch.zeros(10, 1024)\r\n    y = torch.zeros((10, 1))\r\n    return (X.cuda(), y.cuda()) if cuda else (X, y)\r\n\r\ndef cpu_run(lock):\r\n    with lock:\r\n        nn = build(cuda=False)\r\n    d = data(cuda=False)\r\n    train(nn, *d)\r\n\r\ndef thread_cpu_run():\r\n    pool = ThreadPool()\r\n    lock = threading.Lock()\r\n    threads = pool.map(cpu_run, [lock for _ in range(5)])\r\n\r\n    return list(threads)\r\n\r\nthread_cpu_run()\r\n```"}