{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8025", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8025/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8025/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8025/events", "html_url": "https://github.com/pytorch/pytorch/issues/8025", "id": 328406579, "node_id": "MDU6SXNzdWUzMjg0MDY1Nzk=", "number": 8025, "title": "Exported onnx protobuf file is missing tensor information", "user": {"login": "mfojtak", "id": 13848117, "node_id": "MDQ6VXNlcjEzODQ4MTE3", "avatar_url": "https://avatars2.githubusercontent.com/u/13848117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfojtak", "html_url": "https://github.com/mfojtak", "followers_url": "https://api.github.com/users/mfojtak/followers", "following_url": "https://api.github.com/users/mfojtak/following{/other_user}", "gists_url": "https://api.github.com/users/mfojtak/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfojtak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfojtak/subscriptions", "organizations_url": "https://api.github.com/users/mfojtak/orgs", "repos_url": "https://api.github.com/users/mfojtak/repos", "events_url": "https://api.github.com/users/mfojtak/events{/privacy}", "received_events_url": "https://api.github.com/users/mfojtak/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-01T07:14:10Z", "updated_at": "2018-06-01T10:46:41Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Exported onnx protobuf file is missing tensor information.<br>\nHere is the onnx converted to json:</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>node<span class=\"pl-pds\">\"</span></span>: [\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>0<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>5<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>opType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Gemm<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>attribute<span class=\"pl-pds\">\"</span></span>: [\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>alpha<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1.0</span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>\n        },\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>beta<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1.0</span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>\n        },\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>broadcast<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>INT<span class=\"pl-pds\">\"</span></span>\n        },\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>transB<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>INT<span class=\"pl-pds\">\"</span></span>\n        }\n      ]\n      \n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>5<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>6<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>opType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Relu<span class=\"pl-pds\">\"</span></span>\n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>6<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>3<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>4<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>7<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>opType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Gemm<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>attribute<span class=\"pl-pds\">\"</span></span>: [\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>alpha<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1.0</span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>\n        },\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>beta<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1.0</span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>\n        },\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>broadcast<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>INT<span class=\"pl-pds\">\"</span></span>\n        },\n        {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>transB<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>INT<span class=\"pl-pds\">\"</span></span>\n        }\n      ]\n    }\n  ],\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>torch-jit-export<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>initializer<span class=\"pl-pds\">\"</span></span>: [\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dims<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>100<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1000<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dataType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>\n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dims<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>10<span class=\"pl-pds\">\"</span></span>\n      ],\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dataType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>4<span class=\"pl-pds\">\"</span></span>\n    }\n  ],\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>: [\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>0<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorType<span class=\"pl-pds\">\"</span></span>: {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elemType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>shape<span class=\"pl-pds\">\"</span></span>: {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim<span class=\"pl-pds\">\"</span></span>: [\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>64<span class=\"pl-pds\">\"</span></span>\n              },\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1000<span class=\"pl-pds\">\"</span></span>\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorType<span class=\"pl-pds\">\"</span></span>: {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elemType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>shape<span class=\"pl-pds\">\"</span></span>: {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim<span class=\"pl-pds\">\"</span></span>: [\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>100<span class=\"pl-pds\">\"</span></span>\n              },\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1000<span class=\"pl-pds\">\"</span></span>\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorType<span class=\"pl-pds\">\"</span></span>: {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elemType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>shape<span class=\"pl-pds\">\"</span></span>: {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim<span class=\"pl-pds\">\"</span></span>: [\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>100<span class=\"pl-pds\">\"</span></span>\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>3<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorType<span class=\"pl-pds\">\"</span></span>: {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elemType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>shape<span class=\"pl-pds\">\"</span></span>: {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim<span class=\"pl-pds\">\"</span></span>: [\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>10<span class=\"pl-pds\">\"</span></span>\n              },\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>100<span class=\"pl-pds\">\"</span></span>\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>4<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorType<span class=\"pl-pds\">\"</span></span>: {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elemType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>shape<span class=\"pl-pds\">\"</span></span>: {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim<span class=\"pl-pds\">\"</span></span>: [\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>10<span class=\"pl-pds\">\"</span></span>\n              }\n            ]\n          }\n        }\n      }\n    }\n  ],\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>: [\n    {\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>7<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorType<span class=\"pl-pds\">\"</span></span>: {\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elemType<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FLOAT<span class=\"pl-pds\">\"</span></span>,\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>shape<span class=\"pl-pds\">\"</span></span>: {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim<span class=\"pl-pds\">\"</span></span>: [\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>64<span class=\"pl-pds\">\"</span></span>\n              },\n              {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dimValue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>10<span class=\"pl-pds\">\"</span></span>\n              }\n            ]\n          }\n        }\n      }\n    }\n  ]\n}</pre></div>\n<p>And here's also output from onnx.helper.printable_graph(model.graph):</p>\n<pre><code>graph torch-jit-export (\n  %0[FLOAT, 64x1000]\n) initializers (\n  %1[FLOAT, 100x1000]\n  %2[FLOAT, 100]\n  %3[FLOAT, 10x100]\n  %4[FLOAT, 10]\n) {\n  %5 = Gemm[alpha = 1, beta = 1, broadcast = 1, transB = 1](%0, %1, %2)\n  %6 = Relu(%5)\n  %7 = Gemm[alpha = 1, beta = 1, broadcast = 1, transB = 1](%6, %3, %4)\n  return %7\n}\n</code></pre>\n<p>As you can see, the tensors 5, 6 are missing both in initializers and input/output.<br>\nThere is no way e.g. to find tensor information of Relu module input/output (tensors 5/6)</p>\n<p>Here is the verbose graph output for the model printed during export:</p>\n<pre><code>graph(%0 : Float(64, 1000)\n      %1 : Float(100, 1000)\n      %2 : Float(100)\n      %3 : Float(10, 100)\n      %4 : Float(10)) {\n  %5 : Float(64, 100) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%0, %1, %2), scope: Sequential/Linear[0]\n  %6 : Float(64, 100) = onnx::Relu(%5), scope: Sequential/ReLU[1]\n  %7 : Float(64, 10) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%6, %3, %4), scope: Sequential/Linear[2]\n  return (%7);\n}\n</code></pre>\n<p>This contains all info about tensors 5 and 6.<br>\nWhy is it not exported to ONNX?</p>\n<p>Am I missing something or is it a bug?</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.onnx\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> N is batch size; D_in is input dimension;</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> H is hidden dimension; D_out is output dimension.</span>\nN, D_in, H, D_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create random Tensors to hold inputs and outputs</span>\nx <span class=\"pl-k\">=</span> torch.randn(N, D_in)\ny <span class=\"pl-k\">=</span> torch.randn(N, D_out)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the nn package to define our model and loss function.</span>\nmodel <span class=\"pl-k\">=</span> torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\nloss_fn <span class=\"pl-k\">=</span> torch.nn.MSELoss(<span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the optim package to define an Optimizer that will update the weights of</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the model for us. Here we will use Adam; the optim package contains many other</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optimization algoriths. The first argument to the Adam constructor tells the</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optimizer which Tensors it should update.</span>\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-4</span>\noptimizer <span class=\"pl-k\">=</span> torch.optim.Adam(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>learning_rate)\n\ntorch.onnx.export(model, x, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test.onnx<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (GCC) 7.2.0<br>\nCMake version: version 3.10.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration: GPU 0: GeForce GTX 970<br>\nNvidia driver version: 387.26<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.3)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torchvision               0.2.1                     </p>", "body_text": "Issue description\nExported onnx protobuf file is missing tensor information.\nHere is the onnx converted to json:\n{\n  \"node\": [\n    {\n      \"input\": [\n        \"0\",\n        \"1\",\n        \"2\"\n      ],\n      \"output\": [\n        \"5\"\n      ],\n      \"opType\": \"Gemm\",\n      \"attribute\": [\n        {\n          \"name\": \"alpha\",\n          \"f\": 1.0,\n          \"type\": \"FLOAT\"\n        },\n        {\n          \"name\": \"beta\",\n          \"f\": 1.0,\n          \"type\": \"FLOAT\"\n        },\n        {\n          \"name\": \"broadcast\",\n          \"i\": \"1\",\n          \"type\": \"INT\"\n        },\n        {\n          \"name\": \"transB\",\n          \"i\": \"1\",\n          \"type\": \"INT\"\n        }\n      ]\n      \n    },\n    {\n      \"input\": [\n        \"5\"\n      ],\n      \"output\": [\n        \"6\"\n      ],\n      \"opType\": \"Relu\"\n    },\n    {\n      \"input\": [\n        \"6\",\n        \"3\",\n        \"4\"\n      ],\n      \"output\": [\n        \"7\"\n      ],\n      \"opType\": \"Gemm\",\n      \"attribute\": [\n        {\n          \"name\": \"alpha\",\n          \"f\": 1.0,\n          \"type\": \"FLOAT\"\n        },\n        {\n          \"name\": \"beta\",\n          \"f\": 1.0,\n          \"type\": \"FLOAT\"\n        },\n        {\n          \"name\": \"broadcast\",\n          \"i\": \"1\",\n          \"type\": \"INT\"\n        },\n        {\n          \"name\": \"transB\",\n          \"i\": \"1\",\n          \"type\": \"INT\"\n        }\n      ]\n    }\n  ],\n  \"name\": \"torch-jit-export\",\n  \"initializer\": [\n    {\n      \"dims\": [\n        \"100\",\n        \"1000\"\n      ],\n      \"dataType\": \"FLOAT\",\n      \"name\": \"1\"\n    },\n    {\n      \"dims\": [\n        \"10\"\n      ],\n      \"dataType\": \"FLOAT\",\n      \"name\": \"4\"\n    }\n  ],\n  \"input\": [\n    {\n      \"name\": \"0\",\n      \"type\": {\n        \"tensorType\": {\n          \"elemType\": \"FLOAT\",\n          \"shape\": {\n            \"dim\": [\n              {\n                \"dimValue\": \"64\"\n              },\n              {\n                \"dimValue\": \"1000\"\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"1\",\n      \"type\": {\n        \"tensorType\": {\n          \"elemType\": \"FLOAT\",\n          \"shape\": {\n            \"dim\": [\n              {\n                \"dimValue\": \"100\"\n              },\n              {\n                \"dimValue\": \"1000\"\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"2\",\n      \"type\": {\n        \"tensorType\": {\n          \"elemType\": \"FLOAT\",\n          \"shape\": {\n            \"dim\": [\n              {\n                \"dimValue\": \"100\"\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"3\",\n      \"type\": {\n        \"tensorType\": {\n          \"elemType\": \"FLOAT\",\n          \"shape\": {\n            \"dim\": [\n              {\n                \"dimValue\": \"10\"\n              },\n              {\n                \"dimValue\": \"100\"\n              }\n            ]\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"4\",\n      \"type\": {\n        \"tensorType\": {\n          \"elemType\": \"FLOAT\",\n          \"shape\": {\n            \"dim\": [\n              {\n                \"dimValue\": \"10\"\n              }\n            ]\n          }\n        }\n      }\n    }\n  ],\n  \"output\": [\n    {\n      \"name\": \"7\",\n      \"type\": {\n        \"tensorType\": {\n          \"elemType\": \"FLOAT\",\n          \"shape\": {\n            \"dim\": [\n              {\n                \"dimValue\": \"64\"\n              },\n              {\n                \"dimValue\": \"10\"\n              }\n            ]\n          }\n        }\n      }\n    }\n  ]\n}\nAnd here's also output from onnx.helper.printable_graph(model.graph):\ngraph torch-jit-export (\n  %0[FLOAT, 64x1000]\n) initializers (\n  %1[FLOAT, 100x1000]\n  %2[FLOAT, 100]\n  %3[FLOAT, 10x100]\n  %4[FLOAT, 10]\n) {\n  %5 = Gemm[alpha = 1, beta = 1, broadcast = 1, transB = 1](%0, %1, %2)\n  %6 = Relu(%5)\n  %7 = Gemm[alpha = 1, beta = 1, broadcast = 1, transB = 1](%6, %3, %4)\n  return %7\n}\n\nAs you can see, the tensors 5, 6 are missing both in initializers and input/output.\nThere is no way e.g. to find tensor information of Relu module input/output (tensors 5/6)\nHere is the verbose graph output for the model printed during export:\ngraph(%0 : Float(64, 1000)\n      %1 : Float(100, 1000)\n      %2 : Float(100)\n      %3 : Float(10, 100)\n      %4 : Float(10)) {\n  %5 : Float(64, 100) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%0, %1, %2), scope: Sequential/Linear[0]\n  %6 : Float(64, 100) = onnx::Relu(%5), scope: Sequential/ReLU[1]\n  %7 : Float(64, 10) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%6, %3, %4), scope: Sequential/Linear[2]\n  return (%7);\n}\n\nThis contains all info about tensors 5 and 6.\nWhy is it not exported to ONNX?\nAm I missing something or is it a bug?\nCode example\nimport torch\nimport torch.onnx\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Use the nn package to define our model and loss function.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\nloss_fn = torch.nn.MSELoss(size_average=False)\n\n# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. Here we will use Adam; the optim package contains many other\n# optimization algoriths. The first argument to the Adam constructor tells the\n# optimizer which Tensors it should update.\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\ntorch.onnx.export(model, x, \"test.onnx\", verbose=True)\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.3 LTS\nGCC version: (GCC) 7.2.0\nCMake version: version 3.10.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: GPU 0: GeForce GTX 970\nNvidia driver version: 387.26\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\nVersions of relevant libraries:\n[pip3] numpy (1.14.3)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] torch                     0.4.0                     \n[conda] torchvision               0.2.1", "body": "## Issue description\r\nExported onnx protobuf file is missing tensor information.\r\nHere is the onnx converted to json:\r\n```json\r\n{\r\n  \"node\": [\r\n    {\r\n      \"input\": [\r\n        \"0\",\r\n        \"1\",\r\n        \"2\"\r\n      ],\r\n      \"output\": [\r\n        \"5\"\r\n      ],\r\n      \"opType\": \"Gemm\",\r\n      \"attribute\": [\r\n        {\r\n          \"name\": \"alpha\",\r\n          \"f\": 1.0,\r\n          \"type\": \"FLOAT\"\r\n        },\r\n        {\r\n          \"name\": \"beta\",\r\n          \"f\": 1.0,\r\n          \"type\": \"FLOAT\"\r\n        },\r\n        {\r\n          \"name\": \"broadcast\",\r\n          \"i\": \"1\",\r\n          \"type\": \"INT\"\r\n        },\r\n        {\r\n          \"name\": \"transB\",\r\n          \"i\": \"1\",\r\n          \"type\": \"INT\"\r\n        }\r\n      ]\r\n      \r\n    },\r\n    {\r\n      \"input\": [\r\n        \"5\"\r\n      ],\r\n      \"output\": [\r\n        \"6\"\r\n      ],\r\n      \"opType\": \"Relu\"\r\n    },\r\n    {\r\n      \"input\": [\r\n        \"6\",\r\n        \"3\",\r\n        \"4\"\r\n      ],\r\n      \"output\": [\r\n        \"7\"\r\n      ],\r\n      \"opType\": \"Gemm\",\r\n      \"attribute\": [\r\n        {\r\n          \"name\": \"alpha\",\r\n          \"f\": 1.0,\r\n          \"type\": \"FLOAT\"\r\n        },\r\n        {\r\n          \"name\": \"beta\",\r\n          \"f\": 1.0,\r\n          \"type\": \"FLOAT\"\r\n        },\r\n        {\r\n          \"name\": \"broadcast\",\r\n          \"i\": \"1\",\r\n          \"type\": \"INT\"\r\n        },\r\n        {\r\n          \"name\": \"transB\",\r\n          \"i\": \"1\",\r\n          \"type\": \"INT\"\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"name\": \"torch-jit-export\",\r\n  \"initializer\": [\r\n    {\r\n      \"dims\": [\r\n        \"100\",\r\n        \"1000\"\r\n      ],\r\n      \"dataType\": \"FLOAT\",\r\n      \"name\": \"1\"\r\n    },\r\n    {\r\n      \"dims\": [\r\n        \"10\"\r\n      ],\r\n      \"dataType\": \"FLOAT\",\r\n      \"name\": \"4\"\r\n    }\r\n  ],\r\n  \"input\": [\r\n    {\r\n      \"name\": \"0\",\r\n      \"type\": {\r\n        \"tensorType\": {\r\n          \"elemType\": \"FLOAT\",\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"dimValue\": \"64\"\r\n              },\r\n              {\r\n                \"dimValue\": \"1000\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"1\",\r\n      \"type\": {\r\n        \"tensorType\": {\r\n          \"elemType\": \"FLOAT\",\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"dimValue\": \"100\"\r\n              },\r\n              {\r\n                \"dimValue\": \"1000\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"2\",\r\n      \"type\": {\r\n        \"tensorType\": {\r\n          \"elemType\": \"FLOAT\",\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"dimValue\": \"100\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"3\",\r\n      \"type\": {\r\n        \"tensorType\": {\r\n          \"elemType\": \"FLOAT\",\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"dimValue\": \"10\"\r\n              },\r\n              {\r\n                \"dimValue\": \"100\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"4\",\r\n      \"type\": {\r\n        \"tensorType\": {\r\n          \"elemType\": \"FLOAT\",\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"dimValue\": \"10\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  \"output\": [\r\n    {\r\n      \"name\": \"7\",\r\n      \"type\": {\r\n        \"tensorType\": {\r\n          \"elemType\": \"FLOAT\",\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"dimValue\": \"64\"\r\n              },\r\n              {\r\n                \"dimValue\": \"10\"\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\nAnd here's also output from onnx.helper.printable_graph(model.graph):\r\n```\r\ngraph torch-jit-export (\r\n  %0[FLOAT, 64x1000]\r\n) initializers (\r\n  %1[FLOAT, 100x1000]\r\n  %2[FLOAT, 100]\r\n  %3[FLOAT, 10x100]\r\n  %4[FLOAT, 10]\r\n) {\r\n  %5 = Gemm[alpha = 1, beta = 1, broadcast = 1, transB = 1](%0, %1, %2)\r\n  %6 = Relu(%5)\r\n  %7 = Gemm[alpha = 1, beta = 1, broadcast = 1, transB = 1](%6, %3, %4)\r\n  return %7\r\n}\r\n```\r\nAs you can see, the tensors 5, 6 are missing both in initializers and input/output.\r\nThere is no way e.g. to find tensor information of Relu module input/output (tensors 5/6)\r\n\r\nHere is the verbose graph output for the model printed during export:\r\n```\r\ngraph(%0 : Float(64, 1000)\r\n      %1 : Float(100, 1000)\r\n      %2 : Float(100)\r\n      %3 : Float(10, 100)\r\n      %4 : Float(10)) {\r\n  %5 : Float(64, 100) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%0, %1, %2), scope: Sequential/Linear[0]\r\n  %6 : Float(64, 100) = onnx::Relu(%5), scope: Sequential/ReLU[1]\r\n  %7 : Float(64, 10) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%6, %3, %4), scope: Sequential/Linear[2]\r\n  return (%7);\r\n}\r\n```\r\nThis contains all info about tensors 5 and 6.\r\nWhy is it not exported to ONNX?\r\n\r\nAm I missing something or is it a bug?\r\n\r\n\r\n\r\n## Code example\r\n```python\r\nimport torch\r\nimport torch.onnx\r\n\r\n# N is batch size; D_in is input dimension;\r\n# H is hidden dimension; D_out is output dimension.\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\n\r\n# Create random Tensors to hold inputs and outputs\r\nx = torch.randn(N, D_in)\r\ny = torch.randn(N, D_out)\r\n\r\n# Use the nn package to define our model and loss function.\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(D_in, H),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Linear(H, D_out),\r\n)\r\nloss_fn = torch.nn.MSELoss(size_average=False)\r\n\r\n# Use the optim package to define an Optimizer that will update the weights of\r\n# the model for us. Here we will use Adam; the optim package contains many other\r\n# optimization algoriths. The first argument to the Adam constructor tells the\r\n# optimizer which Tensors it should update.\r\nlearning_rate = 1e-4\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\ntorch.onnx.export(model, x, \"test.onnx\", verbose=True)\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (GCC) 7.2.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 970\r\nNvidia driver version: 387.26\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.3)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n"}