{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/400071244", "html_url": "https://github.com/pytorch/pytorch/issues/6351#issuecomment-400071244", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6351", "id": 400071244, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDA3MTI0NA==", "user": {"login": "dlmacedo", "id": 15001116, "node_id": "MDQ6VXNlcjE1MDAxMTE2", "avatar_url": "https://avatars2.githubusercontent.com/u/15001116?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlmacedo", "html_url": "https://github.com/dlmacedo", "followers_url": "https://api.github.com/users/dlmacedo/followers", "following_url": "https://api.github.com/users/dlmacedo/following{/other_user}", "gists_url": "https://api.github.com/users/dlmacedo/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlmacedo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlmacedo/subscriptions", "organizations_url": "https://api.github.com/users/dlmacedo/orgs", "repos_url": "https://api.github.com/users/dlmacedo/repos", "events_url": "https://api.github.com/users/dlmacedo/events{/privacy}", "received_events_url": "https://api.github.com/users/dlmacedo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-25T19:41:09Z", "updated_at": "2018-06-25T19:41:09Z", "author_association": "NONE", "body_html": "<p>I am NOT using numpy to dataloader.</p>\n<p>Since I set the PYTHON seed in my code and it is changed inside dataloader code (see bellow), I need to overwrite the PYTHON seed back again (since it is being changed inside the dataloader code) in the worker init (I am not changing NUMPY seed inside the worker_init):</p>\n<pre><code>def worker_init(worker_id):\n    random.seed(args.base_seed)\n\n</code></pre>\n<p>See below the dataloader code that changes the PYTHON (NOT numpy) seed:</p>\n<pre><code>def _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\n    global _use_shared_memory\n    _use_shared_memory = True\n\n    # Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal\n    # module's handlers are executed after Python returns from C low-level\n    # handlers, likely when the same fatal signal happened again already.\n    # https://docs.python.org/3/library/signal.html Sec. 18.8.1.1\n    _set_worker_signal_handlers()\n\n    torch.set_num_threads(1)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    if init_fn is not None:\n        init_fn(worker_id)\n\n</code></pre>\n<p>I guess we should not change the PYTHON seed inside the PyTorch dataloarder code.</p>", "body_text": "I am NOT using numpy to dataloader.\nSince I set the PYTHON seed in my code and it is changed inside dataloader code (see bellow), I need to overwrite the PYTHON seed back again (since it is being changed inside the dataloader code) in the worker init (I am not changing NUMPY seed inside the worker_init):\ndef worker_init(worker_id):\n    random.seed(args.base_seed)\n\n\nSee below the dataloader code that changes the PYTHON (NOT numpy) seed:\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\n    global _use_shared_memory\n    _use_shared_memory = True\n\n    # Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal\n    # module's handlers are executed after Python returns from C low-level\n    # handlers, likely when the same fatal signal happened again already.\n    # https://docs.python.org/3/library/signal.html Sec. 18.8.1.1\n    _set_worker_signal_handlers()\n\n    torch.set_num_threads(1)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    if init_fn is not None:\n        init_fn(worker_id)\n\n\nI guess we should not change the PYTHON seed inside the PyTorch dataloarder code.", "body": "I am NOT using numpy to dataloader.\r\n\r\nSince I set the PYTHON seed in my code and it is changed inside dataloader code (see bellow), I need to overwrite the PYTHON seed back again (since it is being changed inside the dataloader code) in the worker init (I am not changing NUMPY seed inside the worker_init):\r\n\r\n```\r\ndef worker_init(worker_id):\r\n    random.seed(args.base_seed)\r\n\r\n```\r\n\r\nSee below the dataloader code that changes the PYTHON (NOT numpy) seed:\r\n\r\n```\r\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\r\n    global _use_shared_memory\r\n    _use_shared_memory = True\r\n\r\n    # Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal\r\n    # module's handlers are executed after Python returns from C low-level\r\n    # handlers, likely when the same fatal signal happened again already.\r\n    # https://docs.python.org/3/library/signal.html Sec. 18.8.1.1\r\n    _set_worker_signal_handlers()\r\n\r\n    torch.set_num_threads(1)\r\n    random.seed(seed)\r\n    torch.manual_seed(seed)\r\n\r\n    if init_fn is not None:\r\n        init_fn(worker_id)\r\n\r\n```\r\n\r\nI guess we should not change the PYTHON seed inside the PyTorch dataloarder code.\r\n"}