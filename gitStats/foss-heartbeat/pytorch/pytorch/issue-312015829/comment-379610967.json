{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379610967", "html_url": "https://github.com/pytorch/pytorch/issues/6351#issuecomment-379610967", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6351", "id": 379610967, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTYxMDk2Nw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T02:34:35Z", "updated_at": "2018-04-09T02:34:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1168046\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lvdmaaten\">@lvdmaaten</a> You are right, that check is not guaranteeing much, only that a deterministic algorithm would be picked for a run, and another deterministic algo can be picked for another run. However, it is a legitimate question what guarantees should be provided. Even if a particular algo is chosen (like it is currently done for deterministic w/o benchmarking), it does not guarantee same results on the different architectures or between the different cudnn versions or cublas versions.<br>\nOn the same hardware with the same software stack it should be possible to pick deterministic algos without sacrificing performance in most cases, but that would likely require a user-level API directly specifying algo (lua torch had that), or reimplementing cudnnFind within a framework, like tensorflow does, because the way cudnnFind is implemented it is prone to selecting different algos on the different runs if their runtime is close, because all algos are run just once and variance is high.</p>", "body_text": "@lvdmaaten You are right, that check is not guaranteeing much, only that a deterministic algorithm would be picked for a run, and another deterministic algo can be picked for another run. However, it is a legitimate question what guarantees should be provided. Even if a particular algo is chosen (like it is currently done for deterministic w/o benchmarking), it does not guarantee same results on the different architectures or between the different cudnn versions or cublas versions.\nOn the same hardware with the same software stack it should be possible to pick deterministic algos without sacrificing performance in most cases, but that would likely require a user-level API directly specifying algo (lua torch had that), or reimplementing cudnnFind within a framework, like tensorflow does, because the way cudnnFind is implemented it is prone to selecting different algos on the different runs if their runtime is close, because all algos are run just once and variance is high.", "body": "@lvdmaaten You are right, that check is not guaranteeing much, only that a deterministic algorithm would be picked for a run, and another deterministic algo can be picked for another run. However, it is a legitimate question what guarantees should be provided. Even if a particular algo is chosen (like it is currently done for deterministic w/o benchmarking), it does not guarantee same results on the different architectures or between the different cudnn versions or cublas versions.\r\nOn the same hardware with the same software stack it should be possible to pick deterministic algos without sacrificing performance in most cases, but that would likely require a user-level API directly specifying algo (lua torch had that), or reimplementing cudnnFind within a framework, like tensorflow does, because the way cudnnFind is implemented it is prone to selecting different algos on the different runs if their runtime is close, because all algos are run just once and variance is high.   "}