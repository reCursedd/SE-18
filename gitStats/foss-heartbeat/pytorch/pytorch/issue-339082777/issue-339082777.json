{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9222", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9222/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9222/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9222/events", "html_url": "https://github.com/pytorch/pytorch/issues/9222", "id": 339082777, "node_id": "MDU6SXNzdWUzMzkwODI3Nzc=", "number": 9222, "title": "[feature request] Implementing Block Sparse Operations", "user": {"login": "rdspring1", "id": 3637896, "node_id": "MDQ6VXNlcjM2Mzc4OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/3637896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdspring1", "html_url": "https://github.com/rdspring1", "followers_url": "https://api.github.com/users/rdspring1/followers", "following_url": "https://api.github.com/users/rdspring1/following{/other_user}", "gists_url": "https://api.github.com/users/rdspring1/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdspring1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdspring1/subscriptions", "organizations_url": "https://api.github.com/users/rdspring1/orgs", "repos_url": "https://api.github.com/users/rdspring1/repos", "events_url": "https://api.github.com/users/rdspring1/events{/privacy}", "received_events_url": "https://api.github.com/users/rdspring1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/sparse", "name": "sparse", "color": "bfd4f2", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-06T21:59:51Z", "updated_at": "2018-11-06T18:16:59Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>TL;DR:</strong>  Implementing block-sparse operations for faster matrix-multiplication.<br>\nIs this something worth adding to PyTorch?</p>\n<p>Goals:</p>\n<ol>\n<li>Faster matrix-multiplication by taking advantage of block-sparsity</li>\n<li>Improve running time of large-scale LSTMs - Word Language or Sentiment Analysis Models</li>\n<li>Based on <a href=\"https://blog.openai.com/block-sparse-gpu-kernels/\" rel=\"nofollow\">https://blog.openai.com/block-sparse-gpu-kernels/</a></li>\n</ol>\n<p>New Features:</p>\n<ol>\n<li>LinearBSMM Layer \u2013 A plug &amp; play replacement for the standard linear layer where the user specifies the block size and sparsity pattern</li>\n<li>Helper functions to generate different sparsity patterns \u2013 Random, Barabasi Albert, Watts Strogatz</li>\n</ol>\n<p>Examples:<br>\n<strong>Original Linear Layer</strong></p>\n<pre><code>m = nn.Linear(20, 30)\ninput = torch.randn(128, 20)\noutput = m(input)\n</code></pre>\n<p><strong>Block Sparse Linear Layer</strong></p>\n<pre><code>D = 20\nN = 30\nm = linearBSMM.random(D, N, p=0.25, block_size=32)\ninput = torch.randn(128, D)\noutput = m(input)\n</code></pre>\n<p>OR</p>\n<pre><code>D = 20\nN = 30\nsparsity_pattern = linearBSMM.random(D, N, p=0.25, block_size=32)\nm = nn.LinearBSMM(D, N, sparsity_pattern)\ninput = torch.randn(128, D)\noutput = m(input)\n</code></pre>", "body_text": "TL;DR:  Implementing block-sparse operations for faster matrix-multiplication.\nIs this something worth adding to PyTorch?\nGoals:\n\nFaster matrix-multiplication by taking advantage of block-sparsity\nImprove running time of large-scale LSTMs - Word Language or Sentiment Analysis Models\nBased on https://blog.openai.com/block-sparse-gpu-kernels/\n\nNew Features:\n\nLinearBSMM Layer \u2013 A plug & play replacement for the standard linear layer where the user specifies the block size and sparsity pattern\nHelper functions to generate different sparsity patterns \u2013 Random, Barabasi Albert, Watts Strogatz\n\nExamples:\nOriginal Linear Layer\nm = nn.Linear(20, 30)\ninput = torch.randn(128, 20)\noutput = m(input)\n\nBlock Sparse Linear Layer\nD = 20\nN = 30\nm = linearBSMM.random(D, N, p=0.25, block_size=32)\ninput = torch.randn(128, D)\noutput = m(input)\n\nOR\nD = 20\nN = 30\nsparsity_pattern = linearBSMM.random(D, N, p=0.25, block_size=32)\nm = nn.LinearBSMM(D, N, sparsity_pattern)\ninput = torch.randn(128, D)\noutput = m(input)", "body": "**TL;DR:**  Implementing block-sparse operations for faster matrix-multiplication. \r\nIs this something worth adding to PyTorch?\r\n\r\nGoals:\r\n1. Faster matrix-multiplication by taking advantage of block-sparsity\r\n2. Improve running time of large-scale LSTMs - Word Language or Sentiment Analysis Models\r\n3. Based on https://blog.openai.com/block-sparse-gpu-kernels/\r\n\r\nNew Features:\r\n1. LinearBSMM Layer \u2013 A plug & play replacement for the standard linear layer where the user specifies the block size and sparsity pattern\r\n2. Helper functions to generate different sparsity patterns \u2013 Random, Barabasi Albert, Watts Strogatz\r\n\r\nExamples:\r\n**Original Linear Layer**\r\n```\r\nm = nn.Linear(20, 30)\r\ninput = torch.randn(128, 20)\r\noutput = m(input)\r\n```\r\n**Block Sparse Linear Layer**\r\n```\r\nD = 20\r\nN = 30\r\nm = linearBSMM.random(D, N, p=0.25, block_size=32)\r\ninput = torch.randn(128, D)\r\noutput = m(input)\r\n```\r\nOR\r\n```\r\nD = 20\r\nN = 30\r\nsparsity_pattern = linearBSMM.random(D, N, p=0.25, block_size=32)\r\nm = nn.LinearBSMM(D, N, sparsity_pattern)\r\ninput = torch.randn(128, D)\r\noutput = m(input)\r\n```"}