{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419489252", "html_url": "https://github.com/pytorch/pytorch/issues/7870#issuecomment-419489252", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7870", "id": 419489252, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTQ4OTI1Mg==", "user": {"login": "thlinh", "id": 1904846, "node_id": "MDQ6VXNlcjE5MDQ4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1904846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thlinh", "html_url": "https://github.com/thlinh", "followers_url": "https://api.github.com/users/thlinh/followers", "following_url": "https://api.github.com/users/thlinh/following{/other_user}", "gists_url": "https://api.github.com/users/thlinh/gists{/gist_id}", "starred_url": "https://api.github.com/users/thlinh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thlinh/subscriptions", "organizations_url": "https://api.github.com/users/thlinh/orgs", "repos_url": "https://api.github.com/users/thlinh/repos", "events_url": "https://api.github.com/users/thlinh/events{/privacy}", "received_events_url": "https://api.github.com/users/thlinh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T16:12:10Z", "updated_at": "2018-09-07T16:12:10Z", "author_association": "NONE", "body_html": "<p>I think I have a similar problem. I was trying to export a model including LSTM to ONNX. It generated an error like this:</p>\n<p>File \"run_simple.py\", line 281, in main</p>\n<pre><code>    torch.onnx.export(best_model, tmp_input, \"saved_models_simple/embed_toContext.onnx\",verbose=True)                           \n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/__init__.py\", line 26, in export\n    return utils.export(*args, **kwargs)\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 94, in export\n    operator_export_type=operator_export_type)\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 226, in _export\n    example_outputs, propagate)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 180, in _model_to_graph\n    graph = _optimize_graph(graph, operator_export_type)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 107, in _optimize_graph\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/__init__.py\", line 56, in _run_symbolic_method\n    return utils._run_symbolic_method(*args, **kwargs)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 291, in _run_symbolic_method\n    return symbolic_fn(*args)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/nn/utils/rnn.py\", line 158, in _onnx_symbolic_pack_padded_sequence\n    raise RuntimeError(\"ONNX export requires that the lengths passed \"\nRuntimeError: ONNX export requires that the lengths passed to pack_padded_sequence must be of type Int\n</code></pre>\n<p><strong>RuntimeError: ONNX export requires that the lengths passed to pack_padded_sequence must be of type Int</strong></p>\n<p>I have followed the debugger, and at L157-158 of python3.5/site-packages/torch/nn/utils/rnn.py we have<br>\n<code>if lengths.type().scalarType() != 'Int':</code><br>\nwith<br>\nlengths =<br>\n<code>90 defined in (%90 : Long(5), %91 : Long(5) = aten::sort[dim=-1, descending=1](%1), scope: BiDAF_simple/LSTM[context_LSTM])</code></p>\n<p>So <strong>Long</strong> cannot be accepted/converted as <strong>Int</strong> here?<br>\nCould anyone help me please! Many thanks in advance!!!</p>", "body_text": "I think I have a similar problem. I was trying to export a model including LSTM to ONNX. It generated an error like this:\nFile \"run_simple.py\", line 281, in main\n    torch.onnx.export(best_model, tmp_input, \"saved_models_simple/embed_toContext.onnx\",verbose=True)                           \n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/__init__.py\", line 26, in export\n    return utils.export(*args, **kwargs)\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 94, in export\n    operator_export_type=operator_export_type)\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 226, in _export\n    example_outputs, propagate)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 180, in _model_to_graph\n    graph = _optimize_graph(graph, operator_export_type)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 107, in _optimize_graph\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/__init__.py\", line 56, in _run_symbolic_method\n    return utils._run_symbolic_method(*args, **kwargs)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 291, in _run_symbolic_method\n    return symbolic_fn(*args)\n  File \"/home/py3env/lib/python3.5/site-packages/torch/nn/utils/rnn.py\", line 158, in _onnx_symbolic_pack_padded_sequence\n    raise RuntimeError(\"ONNX export requires that the lengths passed \"\nRuntimeError: ONNX export requires that the lengths passed to pack_padded_sequence must be of type Int\n\nRuntimeError: ONNX export requires that the lengths passed to pack_padded_sequence must be of type Int\nI have followed the debugger, and at L157-158 of python3.5/site-packages/torch/nn/utils/rnn.py we have\nif lengths.type().scalarType() != 'Int':\nwith\nlengths =\n90 defined in (%90 : Long(5), %91 : Long(5) = aten::sort[dim=-1, descending=1](%1), scope: BiDAF_simple/LSTM[context_LSTM])\nSo Long cannot be accepted/converted as Int here?\nCould anyone help me please! Many thanks in advance!!!", "body": "I think I have a similar problem. I was trying to export a model including LSTM to ONNX. It generated an error like this:\r\n\r\n  File \"run_simple.py\", line 281, in main\r\n```\r\n    torch.onnx.export(best_model, tmp_input, \"saved_models_simple/embed_toContext.onnx\",verbose=True)                           \r\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/__init__.py\", line 26, in export\r\n    return utils.export(*args, **kwargs)\r\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 94, in export\r\n    operator_export_type=operator_export_type)\r\n  File \"/home/k00375917/resnet-doml/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 226, in _export\r\n    example_outputs, propagate)\r\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 180, in _model_to_graph\r\n    graph = _optimize_graph(graph, operator_export_type)\r\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 107, in _optimize_graph\r\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/__init__.py\", line 56, in _run_symbolic_method\r\n    return utils._run_symbolic_method(*args, **kwargs)\r\n  File \"/home/py3env/lib/python3.5/site-packages/torch/onnx/utils.py\", line 291, in _run_symbolic_method\r\n    return symbolic_fn(*args)\r\n  File \"/home/py3env/lib/python3.5/site-packages/torch/nn/utils/rnn.py\", line 158, in _onnx_symbolic_pack_padded_sequence\r\n    raise RuntimeError(\"ONNX export requires that the lengths passed \"\r\nRuntimeError: ONNX export requires that the lengths passed to pack_padded_sequence must be of type Int\r\n```\r\n**RuntimeError: ONNX export requires that the lengths passed to pack_padded_sequence must be of type Int**\r\n\r\nI have followed the debugger, and at L157-158 of python3.5/site-packages/torch/nn/utils/rnn.py we have\r\n`if lengths.type().scalarType() != 'Int':`\r\nwith \r\nlengths = \r\n`90 defined in (%90 : Long(5), %91 : Long(5) = aten::sort[dim=-1, descending=1](%1), scope: BiDAF_simple/LSTM[context_LSTM])`\r\n\r\nSo **Long** cannot be accepted/converted as **Int** here?\r\nCould anyone help me please! Many thanks in advance!!!\r\n\r\n\r\n"}