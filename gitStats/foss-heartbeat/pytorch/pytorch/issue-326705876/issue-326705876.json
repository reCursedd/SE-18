{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7870", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7870/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7870/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7870/events", "html_url": "https://github.com/pytorch/pytorch/issues/7870", "id": 326705876, "node_id": "MDU6SXNzdWUzMjY3MDU4NzY=", "number": 7870, "title": "[PyTorch] [ONNX] RNN sequence lengths are exported as Int64 rather than Int32", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-26T03:24:42Z", "updated_at": "2018-09-07T16:12:10Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>The ONNX spec requires that RNN sequence lengths be 32-bit:</p>\n<pre><code>sequence_lens (optional) : T1\nT1 : tensor(int32)\n   Constrain seq_lens to integer tensor.\n</code></pre>\n<p>But PyTorch uses <code>LongTensor</code> (which converts to ONNX int64) internally for most integer tensors and exports the sequence lengths as a <code>LongTensor</code>:</p>\n<pre><code>&gt;&gt;&gt; class Test4(torch.nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.rnn = torch.nn.RNN(2, 2)\n...     def forward(self, x):\n...         return torch.nn.utils.rnn.pad_packed_sequence(self.rnn(torch.nn.utils.rnn.pack_padded_sequence(x, torch.tensor([5, 4, 2])))[0])\n... \n&gt;&gt;&gt; torch.onnx.export(Test4(), torch.rand(5, 3, 2), 'test.onnx', verbose=True)\ngraph(%0 : Float(5, 3, 2)\n      %1 : Float(2, 2)\n      %2 : Float(2, 2)\n      %3 : Float(2)\n      %4 : Float(2)) {\n  %5 : Long(3) = onnx::Constant[value= 5  4  2 [ CPULongTensor{3} ]](), scope: Test4\n  %6 : Dynamic = onnx::Concat[axis=0](%3, %4), scope: Test4/RNN[rnn]\n  %7 : Dynamic = onnx::Shape(%0)\n  %8 : Dynamic = onnx::Constant[value={1}]()\n  %9 : Dynamic = onnx::Gather(%7, %8)\n  %10 : Dynamic = onnx::Unsqueeze[axes=[0]](%9)\n  %11 : Dynamic = onnx::Constant[value={2}]()\n  %12 : Dynamic = onnx::Constant[value={1}]()\n  %13 : Dynamic = onnx::Unsqueeze[axes=[0]](%12)\n  %14 : Dynamic = onnx::Concat[axis=0](%13, %10, %11)\n  %15 : Dynamic = onnx::ConstantFill[input_as_shape=1](%14)\n  %16 : Float(11, 2), %17 : Float(1, 3, 2) = onnx::RNN[activations=[tanh], hidden_size=2](%0, %1, %2, %6, %5, %15), scope: Test4/RNN[rnn]\n  return (%16, %5);\n}\n</code></pre>\n<p>This seems to break Caffe2, at least for <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2212584\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ryanleary\">@ryanleary</a>? It's been fine for me in onnx-tensorflow, but TF happens to allow both in64 and int32 for the sequence length tensor in dynamic RNNs.</p>\n<pre><code>RuntimeError: [enforce fail at tensor.h:495] IsType&lt;T&gt;(). Tensor type mismatch, caller expects elements to be long while tensor contains long long Error from operator:\ninput: \"85\" input: \"OC2_DUMMY_1020/dummy_sequence_lens\" output: \"OC2_DUMMY_1020/input-reversed\" name: \"\" type: \"ReversePackedSegs\" device_option { device_type: 0 cuda_gpu_id: 0 }\n** while accessing input: OC2_DUMMY_1020/dummy_sequence_lens\n</code></pre>\n<p>(I also might just be missing something here)</p>", "body_text": "The ONNX spec requires that RNN sequence lengths be 32-bit:\nsequence_lens (optional) : T1\nT1 : tensor(int32)\n   Constrain seq_lens to integer tensor.\n\nBut PyTorch uses LongTensor (which converts to ONNX int64) internally for most integer tensors and exports the sequence lengths as a LongTensor:\n>>> class Test4(torch.nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.rnn = torch.nn.RNN(2, 2)\n...     def forward(self, x):\n...         return torch.nn.utils.rnn.pad_packed_sequence(self.rnn(torch.nn.utils.rnn.pack_padded_sequence(x, torch.tensor([5, 4, 2])))[0])\n... \n>>> torch.onnx.export(Test4(), torch.rand(5, 3, 2), 'test.onnx', verbose=True)\ngraph(%0 : Float(5, 3, 2)\n      %1 : Float(2, 2)\n      %2 : Float(2, 2)\n      %3 : Float(2)\n      %4 : Float(2)) {\n  %5 : Long(3) = onnx::Constant[value= 5  4  2 [ CPULongTensor{3} ]](), scope: Test4\n  %6 : Dynamic = onnx::Concat[axis=0](%3, %4), scope: Test4/RNN[rnn]\n  %7 : Dynamic = onnx::Shape(%0)\n  %8 : Dynamic = onnx::Constant[value={1}]()\n  %9 : Dynamic = onnx::Gather(%7, %8)\n  %10 : Dynamic = onnx::Unsqueeze[axes=[0]](%9)\n  %11 : Dynamic = onnx::Constant[value={2}]()\n  %12 : Dynamic = onnx::Constant[value={1}]()\n  %13 : Dynamic = onnx::Unsqueeze[axes=[0]](%12)\n  %14 : Dynamic = onnx::Concat[axis=0](%13, %10, %11)\n  %15 : Dynamic = onnx::ConstantFill[input_as_shape=1](%14)\n  %16 : Float(11, 2), %17 : Float(1, 3, 2) = onnx::RNN[activations=[tanh], hidden_size=2](%0, %1, %2, %6, %5, %15), scope: Test4/RNN[rnn]\n  return (%16, %5);\n}\n\nThis seems to break Caffe2, at least for @ryanleary? It's been fine for me in onnx-tensorflow, but TF happens to allow both in64 and int32 for the sequence length tensor in dynamic RNNs.\nRuntimeError: [enforce fail at tensor.h:495] IsType<T>(). Tensor type mismatch, caller expects elements to be long while tensor contains long long Error from operator:\ninput: \"85\" input: \"OC2_DUMMY_1020/dummy_sequence_lens\" output: \"OC2_DUMMY_1020/input-reversed\" name: \"\" type: \"ReversePackedSegs\" device_option { device_type: 0 cuda_gpu_id: 0 }\n** while accessing input: OC2_DUMMY_1020/dummy_sequence_lens\n\n(I also might just be missing something here)", "body": "The ONNX spec requires that RNN sequence lengths be 32-bit:\r\n```\r\nsequence_lens (optional) : T1\r\nT1 : tensor(int32)\r\n   Constrain seq_lens to integer tensor.\r\n```\r\nBut PyTorch uses `LongTensor` (which converts to ONNX int64) internally for most integer tensors and exports the sequence lengths as a `LongTensor`:\r\n```\r\n>>> class Test4(torch.nn.Module):\r\n...     def __init__(self):\r\n...         super().__init__()\r\n...         self.rnn = torch.nn.RNN(2, 2)\r\n...     def forward(self, x):\r\n...         return torch.nn.utils.rnn.pad_packed_sequence(self.rnn(torch.nn.utils.rnn.pack_padded_sequence(x, torch.tensor([5, 4, 2])))[0])\r\n... \r\n>>> torch.onnx.export(Test4(), torch.rand(5, 3, 2), 'test.onnx', verbose=True)\r\ngraph(%0 : Float(5, 3, 2)\r\n      %1 : Float(2, 2)\r\n      %2 : Float(2, 2)\r\n      %3 : Float(2)\r\n      %4 : Float(2)) {\r\n  %5 : Long(3) = onnx::Constant[value= 5  4  2 [ CPULongTensor{3} ]](), scope: Test4\r\n  %6 : Dynamic = onnx::Concat[axis=0](%3, %4), scope: Test4/RNN[rnn]\r\n  %7 : Dynamic = onnx::Shape(%0)\r\n  %8 : Dynamic = onnx::Constant[value={1}]()\r\n  %9 : Dynamic = onnx::Gather(%7, %8)\r\n  %10 : Dynamic = onnx::Unsqueeze[axes=[0]](%9)\r\n  %11 : Dynamic = onnx::Constant[value={2}]()\r\n  %12 : Dynamic = onnx::Constant[value={1}]()\r\n  %13 : Dynamic = onnx::Unsqueeze[axes=[0]](%12)\r\n  %14 : Dynamic = onnx::Concat[axis=0](%13, %10, %11)\r\n  %15 : Dynamic = onnx::ConstantFill[input_as_shape=1](%14)\r\n  %16 : Float(11, 2), %17 : Float(1, 3, 2) = onnx::RNN[activations=[tanh], hidden_size=2](%0, %1, %2, %6, %5, %15), scope: Test4/RNN[rnn]\r\n  return (%16, %5);\r\n}\r\n```\r\nThis seems to break Caffe2, at least for @ryanleary? It's been fine for me in onnx-tensorflow, but TF happens to allow both in64 and int32 for the sequence length tensor in dynamic RNNs.\r\n```\r\nRuntimeError: [enforce fail at tensor.h:495] IsType<T>(). Tensor type mismatch, caller expects elements to be long while tensor contains long long Error from operator:\r\ninput: \"85\" input: \"OC2_DUMMY_1020/dummy_sequence_lens\" output: \"OC2_DUMMY_1020/input-reversed\" name: \"\" type: \"ReversePackedSegs\" device_option { device_type: 0 cuda_gpu_id: 0 }\r\n** while accessing input: OC2_DUMMY_1020/dummy_sequence_lens\r\n```\r\n\r\n(I also might just be missing something here)"}