{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/318205878", "html_url": "https://github.com/pytorch/pytorch/issues/2209#issuecomment-318205878", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2209", "id": 318205878, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODIwNTg3OA==", "user": {"login": "WendyShang", "id": 4813789, "node_id": "MDQ6VXNlcjQ4MTM3ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4813789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WendyShang", "html_url": "https://github.com/WendyShang", "followers_url": "https://api.github.com/users/WendyShang/followers", "following_url": "https://api.github.com/users/WendyShang/following{/other_user}", "gists_url": "https://api.github.com/users/WendyShang/gists{/gist_id}", "starred_url": "https://api.github.com/users/WendyShang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WendyShang/subscriptions", "organizations_url": "https://api.github.com/users/WendyShang/orgs", "repos_url": "https://api.github.com/users/WendyShang/repos", "events_url": "https://api.github.com/users/WendyShang/events{/privacy}", "received_events_url": "https://api.github.com/users/WendyShang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T22:51:49Z", "updated_at": "2017-07-26T22:51:49Z", "author_association": "NONE", "body_html": "<p>I finally figured out a walkaround after trying many ways to debug. So the way I found to solve this problem is to adjust the batchnorm layer hyperparameters. Originally, I have epsilon = 1e-6 and momentum = 0.9; now I switched to the default value and it allows training (still waiting for final result, but looks promising so far). Another way is to perform gradient clipping but the quantitative results are very suboptimal. Thanks for all your help and suggestions! especially <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1384\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/justin\">@justin</a>.</p>", "body_text": "I finally figured out a walkaround after trying many ways to debug. So the way I found to solve this problem is to adjust the batchnorm layer hyperparameters. Originally, I have epsilon = 1e-6 and momentum = 0.9; now I switched to the default value and it allows training (still waiting for final result, but looks promising so far). Another way is to perform gradient clipping but the quantitative results are very suboptimal. Thanks for all your help and suggestions! especially @justin.", "body": "I finally figured out a walkaround after trying many ways to debug. So the way I found to solve this problem is to adjust the batchnorm layer hyperparameters. Originally, I have epsilon = 1e-6 and momentum = 0.9; now I switched to the default value and it allows training (still waiting for final result, but looks promising so far). Another way is to perform gradient clipping but the quantitative results are very suboptimal. Thanks for all your help and suggestions! especially @justin.  "}