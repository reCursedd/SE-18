{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328119450", "html_url": "https://github.com/pytorch/pytorch/issues/2209#issuecomment-328119450", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2209", "id": 328119450, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODExOTQ1MA==", "user": {"login": "WendyShang", "id": 4813789, "node_id": "MDQ6VXNlcjQ4MTM3ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4813789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WendyShang", "html_url": "https://github.com/WendyShang", "followers_url": "https://api.github.com/users/WendyShang/followers", "following_url": "https://api.github.com/users/WendyShang/following{/other_user}", "gists_url": "https://api.github.com/users/WendyShang/gists{/gist_id}", "starred_url": "https://api.github.com/users/WendyShang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WendyShang/subscriptions", "organizations_url": "https://api.github.com/users/WendyShang/orgs", "repos_url": "https://api.github.com/users/WendyShang/repos", "events_url": "https://api.github.com/users/WendyShang/events{/privacy}", "received_events_url": "https://api.github.com/users/WendyShang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-08T14:33:05Z", "updated_at": "2017-09-08T14:35:30Z", "author_association": "NONE", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=14334441\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anuragranj\">@anuragranj</a> , my conjecture is numerical issue, that is, the handling of numerical instability. for example, in your case, ReLU can much more easily blow up on the positive end but sigmoid is always constrained between 0 and 1. Similarly, in my case, I changed the eps for BN from 1e-6 to 1e-5, where 1e-6 works with Torch but PyTorch seems to need a much bigger buffer to prevent numerical instability.</p>\n<p>If you share your code privately with me, I can take a look at any possible walkaround :) but i am not sure if there is a universal fix for it.</p>", "body_text": "Hi, @anuragranj , my conjecture is numerical issue, that is, the handling of numerical instability. for example, in your case, ReLU can much more easily blow up on the positive end but sigmoid is always constrained between 0 and 1. Similarly, in my case, I changed the eps for BN from 1e-6 to 1e-5, where 1e-6 works with Torch but PyTorch seems to need a much bigger buffer to prevent numerical instability.\nIf you share your code privately with me, I can take a look at any possible walkaround :) but i am not sure if there is a universal fix for it.", "body": "Hi, @anuragranj , my conjecture is numerical issue, that is, the handling of numerical instability. for example, in your case, ReLU can much more easily blow up on the positive end but sigmoid is always constrained between 0 and 1. Similarly, in my case, I changed the eps for BN from 1e-6 to 1e-5, where 1e-6 works with Torch but PyTorch seems to need a much bigger buffer to prevent numerical instability. \r\n\r\nIf you share your code privately with me, I can take a look at any possible walkaround :) but i am not sure if there is a universal fix for it.  "}