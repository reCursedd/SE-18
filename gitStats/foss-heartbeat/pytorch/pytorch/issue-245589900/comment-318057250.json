{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/318057250", "html_url": "https://github.com/pytorch/pytorch/issues/2209#issuecomment-318057250", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2209", "id": 318057250, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODA1NzI1MA==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T13:45:19Z", "updated_at": "2017-07-26T13:45:19Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4813789\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/WendyShang\">@WendyShang</a> adding the KL Loss might be making some of the weights nan (which might be making the next iteration of BCELoss get nan targets). Now, the reason KL loss might be generating NaNs, I'm not entirely sure, but 0 * KLLoss will still propagate nans back in the gradients. (0 * nan = nan)</p>", "body_text": "@WendyShang adding the KL Loss might be making some of the weights nan (which might be making the next iteration of BCELoss get nan targets). Now, the reason KL loss might be generating NaNs, I'm not entirely sure, but 0 * KLLoss will still propagate nans back in the gradients. (0 * nan = nan)", "body": "@WendyShang adding the KL Loss might be making some of the weights nan (which might be making the next iteration of BCELoss get nan targets). Now, the reason KL loss might be generating NaNs, I'm not entirely sure, but 0 * KLLoss will still propagate nans back in the gradients. (0 * nan = nan)"}