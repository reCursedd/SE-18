{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4934", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4934/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4934/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4934/events", "html_url": "https://github.com/pytorch/pytorch/issues/4934", "id": 292744990, "node_id": "MDU6SXNzdWUyOTI3NDQ5OTA=", "number": 4934, "title": "[ppc64le] Seg fault with distributed training using multiple data loading workers", "user": {"login": "vaisaxena", "id": 35957509, "node_id": "MDQ6VXNlcjM1OTU3NTA5", "avatar_url": "https://avatars0.githubusercontent.com/u/35957509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vaisaxena", "html_url": "https://github.com/vaisaxena", "followers_url": "https://api.github.com/users/vaisaxena/followers", "following_url": "https://api.github.com/users/vaisaxena/following{/other_user}", "gists_url": "https://api.github.com/users/vaisaxena/gists{/gist_id}", "starred_url": "https://api.github.com/users/vaisaxena/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vaisaxena/subscriptions", "organizations_url": "https://api.github.com/users/vaisaxena/orgs", "repos_url": "https://api.github.com/users/vaisaxena/repos", "events_url": "https://api.github.com/users/vaisaxena/events{/privacy}", "received_events_url": "https://api.github.com/users/vaisaxena/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-30T11:17:26Z", "updated_at": "2018-02-12T10:56:39Z", "closed_at": "2018-02-12T10:56:39Z", "author_association": "NONE", "body_html": "<h2>Environment</h2>\n<pre><code>OS: RHEL 7.3\nArchitecture: ppc64le\nPyTorch version: 0.4.0a0+f94f572\nHow you installed PyTorch (conda, pip, source): source\nPython version: Python 3.6.2 :: Anaconda, Inc.\nCUDA/cuDNN version: 9.1/7.0.3\nGPU models and configuration: Nvidia Tesla P100\nGCC version  (if compiling from source): 4.8.5\nMPI: OpenMPI 2.1.1\n</code></pre>\n<h2>Problem</h2>\n<ol>\n<li>I ran the imagenet training example (<a href=\"https://github.com/pytorch/examples/tree/master/imagenet\">https://github.com/pytorch/examples/tree/master/imagenet</a>) in distributed mode (with OpenMPI) with non-zero data loading workers (&gt;0) in torch.utils.data.DataLoader. In this case, the application either hangs or seg faults after showing following warning from OpenMPI:</li>\n</ol>\n<p>[Output of mpirun  -np 4 -x LD_LIBRARY_PATH python main.py  -a resnet50 --dist-backend mpi --world-size 4 --epochs 2 -j 4 /gpfs/r92gpfs02/vaibhav/shared/small-imagenet]</p>\n<p>creating model 'resnet50'<br>\ncreating model 'resnet50'<br>\ncreating model 'resnet50'<br>\ncreating model 'resnet50'</p>\n<hr>\n<p>A process has executed an operation involving a call to the<br>\n\"fork()\" system call to create a child process.  Open MPI is currently<br>\noperating in a condition that could result in memory corruption or<br>\nother system errors; your job may hang, crash, or produce silent<br>\ndata corruption.  The use of fork() (or system() or other calls that<br>\ncreate child processes) is strongly discouraged.</p>\n<p>The process that invoked fork was:</p>\n<p>Local host:          [[36650,1],0] (PID 144066)</p>\n<p>If you are <em>absolutely sure</em> that your application will successfully<br>\nand correctly survive a call to fork(), you may disable this warning<br>\nby setting the mpi_warn_on_fork MCA parameter to 0.<br>\n--------------------------------------------------------------------------\"</p>\n<p>The complete stack trace of the seg fault is provided in the attached file debug_4node_j4_e2.log<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1677387/debug_4node_j4_e2.log\">debug_4node_j4_e2.log</a></p>\n<ol start=\"2\">\n<li>\n<p>The example works fine in serial mode (no MPI) with multiple data loading workers (running with -j &gt; 0)</p>\n</li>\n<li>\n<p>The example also works fine in distributed mode (with OpenMPI) with no separate data loading worker (running the application with -j 0). In this case all the data loading happens in the main process itself. There is no \"fork\" related warning from OpenMPI in this case.</p>\n</li>\n</ol>\n<p>So there seems to be some problem using OpenMPI with multiple data loading workers.</p>\n<p>I also noticed a warning in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L24\">https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L24</a> that DistributedDataParallel only works with \"nccl\" and \"gloo\", so I changed the imagenet/main.py to use torch.nn.DataParallel with a separate gradient average function instead of torch.nn.parallel.DistributedDataParallel in the distributed case (with MPI) but that also didn't help.</p>", "body_text": "Environment\nOS: RHEL 7.3\nArchitecture: ppc64le\nPyTorch version: 0.4.0a0+f94f572\nHow you installed PyTorch (conda, pip, source): source\nPython version: Python 3.6.2 :: Anaconda, Inc.\nCUDA/cuDNN version: 9.1/7.0.3\nGPU models and configuration: Nvidia Tesla P100\nGCC version  (if compiling from source): 4.8.5\nMPI: OpenMPI 2.1.1\n\nProblem\n\nI ran the imagenet training example (https://github.com/pytorch/examples/tree/master/imagenet) in distributed mode (with OpenMPI) with non-zero data loading workers (>0) in torch.utils.data.DataLoader. In this case, the application either hangs or seg faults after showing following warning from OpenMPI:\n\n[Output of mpirun  -np 4 -x LD_LIBRARY_PATH python main.py  -a resnet50 --dist-backend mpi --world-size 4 --epochs 2 -j 4 /gpfs/r92gpfs02/vaibhav/shared/small-imagenet]\ncreating model 'resnet50'\ncreating model 'resnet50'\ncreating model 'resnet50'\ncreating model 'resnet50'\n\nA process has executed an operation involving a call to the\n\"fork()\" system call to create a child process.  Open MPI is currently\noperating in a condition that could result in memory corruption or\nother system errors; your job may hang, crash, or produce silent\ndata corruption.  The use of fork() (or system() or other calls that\ncreate child processes) is strongly discouraged.\nThe process that invoked fork was:\nLocal host:          [[36650,1],0] (PID 144066)\nIf you are absolutely sure that your application will successfully\nand correctly survive a call to fork(), you may disable this warning\nby setting the mpi_warn_on_fork MCA parameter to 0.\n--------------------------------------------------------------------------\"\nThe complete stack trace of the seg fault is provided in the attached file debug_4node_j4_e2.log\ndebug_4node_j4_e2.log\n\n\nThe example works fine in serial mode (no MPI) with multiple data loading workers (running with -j > 0)\n\n\nThe example also works fine in distributed mode (with OpenMPI) with no separate data loading worker (running the application with -j 0). In this case all the data loading happens in the main process itself. There is no \"fork\" related warning from OpenMPI in this case.\n\n\nSo there seems to be some problem using OpenMPI with multiple data loading workers.\nI also noticed a warning in https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L24 that DistributedDataParallel only works with \"nccl\" and \"gloo\", so I changed the imagenet/main.py to use torch.nn.DataParallel with a separate gradient average function instead of torch.nn.parallel.DistributedDataParallel in the distributed case (with MPI) but that also didn't help.", "body": "## Environment\r\n\r\n    OS: RHEL 7.3\r\n    Architecture: ppc64le\r\n    PyTorch version: 0.4.0a0+f94f572\r\n    How you installed PyTorch (conda, pip, source): source\r\n    Python version: Python 3.6.2 :: Anaconda, Inc.\r\n    CUDA/cuDNN version: 9.1/7.0.3\r\n    GPU models and configuration: Nvidia Tesla P100\r\n    GCC version  (if compiling from source): 4.8.5\r\n    MPI: OpenMPI 2.1.1\r\n\r\n## Problem\r\n\r\n1. I ran the imagenet training example (https://github.com/pytorch/examples/tree/master/imagenet) in distributed mode (with OpenMPI) with non-zero data loading workers (>0) in torch.utils.data.DataLoader. In this case, the application either hangs or seg faults after showing following warning from OpenMPI:\r\n\r\n[Output of mpirun  -np 4 -x LD_LIBRARY_PATH python main.py  -a resnet50 --dist-backend mpi --world-size 4 --epochs 2 -j 4 /gpfs/r92gpfs02/vaibhav/shared/small-imagenet]\r\n\r\ncreating model 'resnet50'\r\ncreating model 'resnet50'\r\ncreating model 'resnet50'\r\ncreating model 'resnet50'\r\n\r\n--------------------------------------------------------------------------\r\nA process has executed an operation involving a call to the\r\n\"fork()\" system call to create a child process.  Open MPI is currently\r\noperating in a condition that could result in memory corruption or\r\nother system errors; your job may hang, crash, or produce silent\r\ndata corruption.  The use of fork() (or system() or other calls that\r\ncreate child processes) is strongly discouraged.\r\n\r\nThe process that invoked fork was:\r\n\r\n  Local host:          [[36650,1],0] (PID 144066)\r\n\r\nIf you are *absolutely sure* that your application will successfully\r\nand correctly survive a call to fork(), you may disable this warning\r\nby setting the mpi_warn_on_fork MCA parameter to 0.\r\n--------------------------------------------------------------------------\"\r\n\r\nThe complete stack trace of the seg fault is provided in the attached file debug_4node_j4_e2.log\r\n[debug_4node_j4_e2.log](https://github.com/pytorch/pytorch/files/1677387/debug_4node_j4_e2.log)\r\n\r\n2. The example works fine in serial mode (no MPI) with multiple data loading workers (running with -j > 0) \r\n\r\n3. The example also works fine in distributed mode (with OpenMPI) with no separate data loading worker (running the application with -j 0). In this case all the data loading happens in the main process itself. There is no \"fork\" related warning from OpenMPI in this case. \r\n\r\nSo there seems to be some problem using OpenMPI with multiple data loading workers. \r\n\r\nI also noticed a warning in https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L24 that DistributedDataParallel only works with \"nccl\" and \"gloo\", so I changed the imagenet/main.py to use torch.nn.DataParallel with a separate gradient average function instead of torch.nn.parallel.DistributedDataParallel in the distributed case (with MPI) but that also didn't help.  \r\n\r\n\r\n\r\n\r\n"}