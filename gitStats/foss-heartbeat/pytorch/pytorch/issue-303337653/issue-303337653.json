{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5627", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5627/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5627/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5627/events", "html_url": "https://github.com/pytorch/pytorch/issues/5627", "id": 303337653, "node_id": "MDU6SXNzdWUzMDMzMzc2NTM=", "number": 5627, "title": "Can not restart the training to obtain the same results", "user": {"login": "jyzhang-bjtu", "id": 10786236, "node_id": "MDQ6VXNlcjEwNzg2MjM2", "avatar_url": "https://avatars2.githubusercontent.com/u/10786236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jyzhang-bjtu", "html_url": "https://github.com/jyzhang-bjtu", "followers_url": "https://api.github.com/users/jyzhang-bjtu/followers", "following_url": "https://api.github.com/users/jyzhang-bjtu/following{/other_user}", "gists_url": "https://api.github.com/users/jyzhang-bjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jyzhang-bjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jyzhang-bjtu/subscriptions", "organizations_url": "https://api.github.com/users/jyzhang-bjtu/orgs", "repos_url": "https://api.github.com/users/jyzhang-bjtu/repos", "events_url": "https://api.github.com/users/jyzhang-bjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/jyzhang-bjtu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2018-03-08T02:52:42Z", "updated_at": "2018-03-13T08:35:47Z", "closed_at": "2018-03-13T08:35:47Z", "author_association": "NONE", "body_html": "<p>PyTorch version : 0.3.0</p>\n<p>I use adam or sgd with the default parameters to training my network. But I found some interesing results during the training restart. Loss function is torch.nn.mse.</p>\n<p>Algorithm sgd  :<br>\nFirst I training it with 110 batches, and save the model with torch.save(model.state_dict(), self.fn) at 100-th batch.</p>\n<p>If I training continuously with all 110 batches, the loss I got will be the following :</p>\n<p>2018-03-08 09:37:13 - root - INFO - SNR: 3dB Epoch[0].100  :     loss 1.2919E-03,   err_num=31756<br>\n2018-03-08 09:39:16 - root - INFO - SNR: 3dB Epoch[0].101  :     loss 1.2755E-03,   err_num=31606<br>\n2018-03-08 09:39:17 - root - INFO - SNR: 3dB Epoch[0].102  :     loss 1.1193E-03,   err_num=26931<br>\n2018-03-08 09:39:18 - root - INFO - SNR: 3dB Epoch[0].103  :     loss 1.1842E-03,   err_num=28907<br>\n2018-03-08 09:39:19 - root - INFO - SNR: 3dB Epoch[0].104  :     loss 1.3281E-03,   err_num=32834</p>\n<p>If I restart the training and reload the model with model.load_state_dict( torch.load(self.fn) ) at 100-th batch, then the loss I got will be the following :</p>\n<p>2018-03-08 09:40:51 - root - INFO - SNR: 3dB Epoch[0].100  :     loss 1.2919E-03,   err_num=31756<br>\n2018-03-08 09:41:03 - root - INFO - SNR: 3dB Epoch[0].101  :     loss 1.2755E-03,   err_num=31606<br>\n2018-03-08 09:41:04 - root - INFO - SNR: 3dB Epoch[0].102  :     loss 1.1193E-03,   err_num=26931<br>\n2018-03-08 09:41:06 - root - INFO - SNR: 3dB Epoch[0].103  :     loss 1.1842E-03,   err_num=28914<br>\n2018-03-08 09:41:07 - root - INFO - SNR: 3dB Epoch[0].104  :     loss 1.3282E-03,   err_num=32833</p>\n<p>for adam<br>\ncontinuous training<br>\n2018-03-06 14:50:10 - root - INFO -  Epoch[0].100  :     loss 0.001053,   err_num=26686.0<br>\n2018-03-06 14:50:11 - root - INFO -  Epoch[0].101  :     loss 0.001018,   err_num=25701.0<br>\n2018-03-06 14:50:12 - root - INFO -  Epoch[0].102  :     loss 0.0008684,   err_num=21734.0<br>\n2018-03-06 14:50:13 - root - INFO -  Epoch[0].103  :     loss 0.00094,   err_num=23426.0<br>\n2018-03-06 14:50:14 - root - INFO -  Epoch[0].104  :     loss 0.001076,   err_num=26678.0</p>\n<p>restart training<br>\n2018-03-08 11:10:09 - root - INFO -  Epoch[0].100  :     loss 1.0526E-03,   err_num=26686<br>\n2018-03-08 11:10:11 - root - INFO -  Epoch[0].101  :     loss 1.0135E-03,   err_num=25711<br>\n2018-03-08 11:10:13 - root - INFO -  Epoch[0].102  :     loss 8.6998E-04,   err_num=21581<br>\n2018-03-08 11:10:14 - root - INFO -  Epoch[0].103  :     loss 9.3137E-04,   err_num=23466<br>\n2018-03-08 11:10:15 - root - INFO -  Epoch[0].104  :     loss 1.0608E-03,   err_num=26306<br>\n2018-03-08 11:10:16 - root - INFO -  Epoch[0].105  :     loss 8.5917E-04,   err_num=21298</p>\n<p>It can be seen that loss of restart training is not same as the training with continuous training. I double check the dataset of these two case, which is all the same. Why? How did it happen?</p>\n<p>If I use the adam alogorithm, loss and err_num will be differenet starting from Epoch[0].101.</p>", "body_text": "PyTorch version : 0.3.0\nI use adam or sgd with the default parameters to training my network. But I found some interesing results during the training restart. Loss function is torch.nn.mse.\nAlgorithm sgd  :\nFirst I training it with 110 batches, and save the model with torch.save(model.state_dict(), self.fn) at 100-th batch.\nIf I training continuously with all 110 batches, the loss I got will be the following :\n2018-03-08 09:37:13 - root - INFO - SNR: 3dB Epoch[0].100  :     loss 1.2919E-03,   err_num=31756\n2018-03-08 09:39:16 - root - INFO - SNR: 3dB Epoch[0].101  :     loss 1.2755E-03,   err_num=31606\n2018-03-08 09:39:17 - root - INFO - SNR: 3dB Epoch[0].102  :     loss 1.1193E-03,   err_num=26931\n2018-03-08 09:39:18 - root - INFO - SNR: 3dB Epoch[0].103  :     loss 1.1842E-03,   err_num=28907\n2018-03-08 09:39:19 - root - INFO - SNR: 3dB Epoch[0].104  :     loss 1.3281E-03,   err_num=32834\nIf I restart the training and reload the model with model.load_state_dict( torch.load(self.fn) ) at 100-th batch, then the loss I got will be the following :\n2018-03-08 09:40:51 - root - INFO - SNR: 3dB Epoch[0].100  :     loss 1.2919E-03,   err_num=31756\n2018-03-08 09:41:03 - root - INFO - SNR: 3dB Epoch[0].101  :     loss 1.2755E-03,   err_num=31606\n2018-03-08 09:41:04 - root - INFO - SNR: 3dB Epoch[0].102  :     loss 1.1193E-03,   err_num=26931\n2018-03-08 09:41:06 - root - INFO - SNR: 3dB Epoch[0].103  :     loss 1.1842E-03,   err_num=28914\n2018-03-08 09:41:07 - root - INFO - SNR: 3dB Epoch[0].104  :     loss 1.3282E-03,   err_num=32833\nfor adam\ncontinuous training\n2018-03-06 14:50:10 - root - INFO -  Epoch[0].100  :     loss 0.001053,   err_num=26686.0\n2018-03-06 14:50:11 - root - INFO -  Epoch[0].101  :     loss 0.001018,   err_num=25701.0\n2018-03-06 14:50:12 - root - INFO -  Epoch[0].102  :     loss 0.0008684,   err_num=21734.0\n2018-03-06 14:50:13 - root - INFO -  Epoch[0].103  :     loss 0.00094,   err_num=23426.0\n2018-03-06 14:50:14 - root - INFO -  Epoch[0].104  :     loss 0.001076,   err_num=26678.0\nrestart training\n2018-03-08 11:10:09 - root - INFO -  Epoch[0].100  :     loss 1.0526E-03,   err_num=26686\n2018-03-08 11:10:11 - root - INFO -  Epoch[0].101  :     loss 1.0135E-03,   err_num=25711\n2018-03-08 11:10:13 - root - INFO -  Epoch[0].102  :     loss 8.6998E-04,   err_num=21581\n2018-03-08 11:10:14 - root - INFO -  Epoch[0].103  :     loss 9.3137E-04,   err_num=23466\n2018-03-08 11:10:15 - root - INFO -  Epoch[0].104  :     loss 1.0608E-03,   err_num=26306\n2018-03-08 11:10:16 - root - INFO -  Epoch[0].105  :     loss 8.5917E-04,   err_num=21298\nIt can be seen that loss of restart training is not same as the training with continuous training. I double check the dataset of these two case, which is all the same. Why? How did it happen?\nIf I use the adam alogorithm, loss and err_num will be differenet starting from Epoch[0].101.", "body": "PyTorch version : 0.3.0\r\n\r\nI use adam or sgd with the default parameters to training my network. But I found some interesing results during the training restart. Loss function is torch.nn.mse.\r\n\r\nAlgorithm sgd  : \r\n First I training it with 110 batches, and save the model with torch.save(model.state_dict(), self.fn) at 100-th batch.\r\n\r\nIf I training continuously with all 110 batches, the loss I got will be the following :\r\n\r\n2018-03-08 09:37:13 - root - INFO - SNR: 3dB Epoch[0].100  :     loss 1.2919E-03,   err_num=31756  \r\n2018-03-08 09:39:16 - root - INFO - SNR: 3dB Epoch[0].101  :     loss 1.2755E-03,   err_num=31606   \r\n2018-03-08 09:39:17 - root - INFO - SNR: 3dB Epoch[0].102  :     loss 1.1193E-03,   err_num=26931 \r\n2018-03-08 09:39:18 - root - INFO - SNR: 3dB Epoch[0].103  :     loss 1.1842E-03,   err_num=28907  \r\n2018-03-08 09:39:19 - root - INFO - SNR: 3dB Epoch[0].104  :     loss 1.3281E-03,   err_num=32834 \r\n\r\nIf I restart the training and reload the model with model.load_state_dict( torch.load(self.fn) ) at 100-th batch, then the loss I got will be the following : \r\n\r\n2018-03-08 09:40:51 - root - INFO - SNR: 3dB Epoch[0].100  :     loss 1.2919E-03,   err_num=31756 \r\n2018-03-08 09:41:03 - root - INFO - SNR: 3dB Epoch[0].101  :     loss 1.2755E-03,   err_num=31606 \r\n2018-03-08 09:41:04 - root - INFO - SNR: 3dB Epoch[0].102  :     loss 1.1193E-03,   err_num=26931 \r\n2018-03-08 09:41:06 - root - INFO - SNR: 3dB Epoch[0].103  :     loss 1.1842E-03,   err_num=28914  \r\n2018-03-08 09:41:07 - root - INFO - SNR: 3dB Epoch[0].104  :     loss 1.3282E-03,   err_num=32833 \r\n\r\nfor adam \r\ncontinuous training\r\n2018-03-06 14:50:10 - root - INFO -  Epoch[0].100  :     loss 0.001053,   err_num=26686.0 \r\n2018-03-06 14:50:11 - root - INFO -  Epoch[0].101  :     loss 0.001018,   err_num=25701.0 \r\n2018-03-06 14:50:12 - root - INFO -  Epoch[0].102  :     loss 0.0008684,   err_num=21734.0  \r\n2018-03-06 14:50:13 - root - INFO -  Epoch[0].103  :     loss 0.00094,   err_num=23426.0  \r\n2018-03-06 14:50:14 - root - INFO -  Epoch[0].104  :     loss 0.001076,   err_num=26678.0  \r\n\r\nrestart training\r\n2018-03-08 11:10:09 - root - INFO -  Epoch[0].100  :     loss 1.0526E-03,   err_num=26686  \r\n2018-03-08 11:10:11 - root - INFO -  Epoch[0].101  :     loss 1.0135E-03,   err_num=25711  \r\n2018-03-08 11:10:13 - root - INFO -  Epoch[0].102  :     loss 8.6998E-04,   err_num=21581  \r\n2018-03-08 11:10:14 - root - INFO -  Epoch[0].103  :     loss 9.3137E-04,   err_num=23466  \r\n2018-03-08 11:10:15 - root - INFO -  Epoch[0].104  :     loss 1.0608E-03,   err_num=26306  \r\n2018-03-08 11:10:16 - root - INFO -  Epoch[0].105  :     loss 8.5917E-04,   err_num=21298 \r\n\r\nIt can be seen that loss of restart training is not same as the training with continuous training. I double check the dataset of these two case, which is all the same. Why? How did it happen?\r\n\r\nIf I use the adam alogorithm, loss and err_num will be differenet starting from Epoch[0].101.\r\n\r\n"}