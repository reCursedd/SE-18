{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/371367387", "html_url": "https://github.com/pytorch/pytorch/issues/5627#issuecomment-371367387", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5627", "id": 371367387, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTM2NzM4Nw==", "user": {"login": "jyzhang-bjtu", "id": 10786236, "node_id": "MDQ6VXNlcjEwNzg2MjM2", "avatar_url": "https://avatars2.githubusercontent.com/u/10786236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jyzhang-bjtu", "html_url": "https://github.com/jyzhang-bjtu", "followers_url": "https://api.github.com/users/jyzhang-bjtu/followers", "following_url": "https://api.github.com/users/jyzhang-bjtu/following{/other_user}", "gists_url": "https://api.github.com/users/jyzhang-bjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jyzhang-bjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jyzhang-bjtu/subscriptions", "organizations_url": "https://api.github.com/users/jyzhang-bjtu/orgs", "repos_url": "https://api.github.com/users/jyzhang-bjtu/repos", "events_url": "https://api.github.com/users/jyzhang-bjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/jyzhang-bjtu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-08T03:31:02Z", "updated_at": "2018-03-08T03:32:45Z", "author_association": "NONE", "body_html": "<p>here is the part of code, nothing special</p>\n<pre><code>def fit_batch(self, da0 = None):\n    rc0 = 0\n     \n     # closure for optimization\n    def closure():\n        nonlocal rc0\n        self.optimizer.zero_grad()\n        \n        # get the model output     \n        mout0 = self.model(da0)\n                \n        # define a layer to select the data to be used\n        target0,out0 = self.datasel(da0,mout0) \n         \n        # define the loss \n        loss0 = self.loss(out0,target0)\n        \n        loss0.backward()            \n\n        # proc the grad if exist\n        if self.procgrad is not None:\n            self.procgrad.proc()            \n\n        # store the first eval output\n        if rc0 == 0 :            \n            self.mout0 = mout0\n            self.target0 = target0\n            self.out0 = out0\n            self.loss0 = loss0\n        \n        rc0 = rc0 + 1\n        \n        return loss0\n\n    self.optimizer.zero_grad()        \n    self.optimizer.step(closure)  \n\ndef fit_epoch(self):\n    \"\"\" fit fucntion\n    \"\"\"\n            \n    # initial batch\n    step0 = self.gb_step % (self.nEpo*self.nBchperEpo)\n\n    d0 = datetime.datetime.now()\n    for jj in range(step0, self.nEpo*self.nBchperEpo) :\n        \n        # For each epoch, setup the lr_scheduler and shuffle the dataloader\n        if jj % self.nBchperEpo == 0 :\n            if self.lr_scheduler is not None : \n                self.lr_scheduler.step()            \n            self.dataloader.shuffle()\n            logging.info(\"{}\".format(self.dataloader.ind_sample))\n        \n        ind0 = self.dataloader.ind_bch2sample(jj%self.nBchperEpo)\n        \n        self.da0 = self.dataloader.get_sample(ind_data = ind0 )      \n        \n        self.fit_batch(self.da0) \n        \n        if (jj%self.summarizer.freq_log==self.summarizer.freq_log-1): \n            d1 = datetime.datetime.now()                \n            self.summarize(dt0 = d1-d0)\n            d0 = d1\n\n        if (jj%self.collector.freq_save==self.collector.freq_save-1):\n            pydevd.settrace()\n            self.fit_save()    \n                                   \n                \n        if self.collector.isstop() :\n            return\n        \n        # increase the global step \n        self.gb_step = self.gb_step + 1            \n\ndef fit(self):\n    \n    # restart training\n    self.fit_restart()\n\n    ii_start = self.gb_step // (self.nEpo*self.nBchperEpo)\n        \n    # training \n    for ii in range( ii_start, self.nDaSet ):\n        \n        # setting the dataloader with the state ii\n        self.dataloader.set_state(ii)\n                           \n        # training             \n        self.fit_epoch()\n        \n        # save at the end of each data set\n        self.fit_save()\n        \n        # summary \n        self.summarize()\n</code></pre>", "body_text": "here is the part of code, nothing special\ndef fit_batch(self, da0 = None):\n    rc0 = 0\n     \n     # closure for optimization\n    def closure():\n        nonlocal rc0\n        self.optimizer.zero_grad()\n        \n        # get the model output     \n        mout0 = self.model(da0)\n                \n        # define a layer to select the data to be used\n        target0,out0 = self.datasel(da0,mout0) \n         \n        # define the loss \n        loss0 = self.loss(out0,target0)\n        \n        loss0.backward()            \n\n        # proc the grad if exist\n        if self.procgrad is not None:\n            self.procgrad.proc()            \n\n        # store the first eval output\n        if rc0 == 0 :            \n            self.mout0 = mout0\n            self.target0 = target0\n            self.out0 = out0\n            self.loss0 = loss0\n        \n        rc0 = rc0 + 1\n        \n        return loss0\n\n    self.optimizer.zero_grad()        \n    self.optimizer.step(closure)  \n\ndef fit_epoch(self):\n    \"\"\" fit fucntion\n    \"\"\"\n            \n    # initial batch\n    step0 = self.gb_step % (self.nEpo*self.nBchperEpo)\n\n    d0 = datetime.datetime.now()\n    for jj in range(step0, self.nEpo*self.nBchperEpo) :\n        \n        # For each epoch, setup the lr_scheduler and shuffle the dataloader\n        if jj % self.nBchperEpo == 0 :\n            if self.lr_scheduler is not None : \n                self.lr_scheduler.step()            \n            self.dataloader.shuffle()\n            logging.info(\"{}\".format(self.dataloader.ind_sample))\n        \n        ind0 = self.dataloader.ind_bch2sample(jj%self.nBchperEpo)\n        \n        self.da0 = self.dataloader.get_sample(ind_data = ind0 )      \n        \n        self.fit_batch(self.da0) \n        \n        if (jj%self.summarizer.freq_log==self.summarizer.freq_log-1): \n            d1 = datetime.datetime.now()                \n            self.summarize(dt0 = d1-d0)\n            d0 = d1\n\n        if (jj%self.collector.freq_save==self.collector.freq_save-1):\n            pydevd.settrace()\n            self.fit_save()    \n                                   \n                \n        if self.collector.isstop() :\n            return\n        \n        # increase the global step \n        self.gb_step = self.gb_step + 1            \n\ndef fit(self):\n    \n    # restart training\n    self.fit_restart()\n\n    ii_start = self.gb_step // (self.nEpo*self.nBchperEpo)\n        \n    # training \n    for ii in range( ii_start, self.nDaSet ):\n        \n        # setting the dataloader with the state ii\n        self.dataloader.set_state(ii)\n                           \n        # training             \n        self.fit_epoch()\n        \n        # save at the end of each data set\n        self.fit_save()\n        \n        # summary \n        self.summarize()", "body": "here is the part of code, nothing special\r\n\r\n    def fit_batch(self, da0 = None):\r\n        rc0 = 0\r\n         \r\n         # closure for optimization\r\n        def closure():\r\n            nonlocal rc0\r\n            self.optimizer.zero_grad()\r\n            \r\n            # get the model output     \r\n            mout0 = self.model(da0)\r\n                    \r\n            # define a layer to select the data to be used\r\n            target0,out0 = self.datasel(da0,mout0) \r\n             \r\n            # define the loss \r\n            loss0 = self.loss(out0,target0)\r\n            \r\n            loss0.backward()            \r\n\r\n            # proc the grad if exist\r\n            if self.procgrad is not None:\r\n                self.procgrad.proc()            \r\n\r\n            # store the first eval output\r\n            if rc0 == 0 :            \r\n                self.mout0 = mout0\r\n                self.target0 = target0\r\n                self.out0 = out0\r\n                self.loss0 = loss0\r\n            \r\n            rc0 = rc0 + 1\r\n            \r\n            return loss0\r\n\r\n        self.optimizer.zero_grad()        \r\n        self.optimizer.step(closure)  \r\n\r\n    def fit_epoch(self):\r\n        \"\"\" fit fucntion\r\n        \"\"\"\r\n                \r\n        # initial batch\r\n        step0 = self.gb_step % (self.nEpo*self.nBchperEpo)\r\n\r\n        d0 = datetime.datetime.now()\r\n        for jj in range(step0, self.nEpo*self.nBchperEpo) :\r\n            \r\n            # For each epoch, setup the lr_scheduler and shuffle the dataloader\r\n            if jj % self.nBchperEpo == 0 :\r\n                if self.lr_scheduler is not None : \r\n                    self.lr_scheduler.step()            \r\n                self.dataloader.shuffle()\r\n                logging.info(\"{}\".format(self.dataloader.ind_sample))\r\n            \r\n            ind0 = self.dataloader.ind_bch2sample(jj%self.nBchperEpo)\r\n            \r\n            self.da0 = self.dataloader.get_sample(ind_data = ind0 )      \r\n            \r\n            self.fit_batch(self.da0) \r\n            \r\n            if (jj%self.summarizer.freq_log==self.summarizer.freq_log-1): \r\n                d1 = datetime.datetime.now()                \r\n                self.summarize(dt0 = d1-d0)\r\n                d0 = d1\r\n    \r\n            if (jj%self.collector.freq_save==self.collector.freq_save-1):\r\n                pydevd.settrace()\r\n                self.fit_save()    \r\n                                       \r\n                    \r\n            if self.collector.isstop() :\r\n                return\r\n            \r\n            # increase the global step \r\n            self.gb_step = self.gb_step + 1            \r\n    \r\n    def fit(self):\r\n        \r\n        # restart training\r\n        self.fit_restart()\r\n\r\n        ii_start = self.gb_step // (self.nEpo*self.nBchperEpo)\r\n            \r\n        # training \r\n        for ii in range( ii_start, self.nDaSet ):\r\n            \r\n            # setting the dataloader with the state ii\r\n            self.dataloader.set_state(ii)\r\n                               \r\n            # training             \r\n            self.fit_epoch()\r\n            \r\n            # save at the end of each data set\r\n            self.fit_save()\r\n            \r\n            # summary \r\n            self.summarize()\r\n"}