{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/371423798", "html_url": "https://github.com/pytorch/pytorch/issues/5627#issuecomment-371423798", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5627", "id": 371423798, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTQyMzc5OA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-08T09:01:08Z", "updated_at": "2018-03-08T09:01:08Z", "author_association": "MEMBER", "body_html": "<p>Just setting the random seed to the same value won\u2019t let you resume the training deterministically. It\u2019s true that the numbers generated will be the same in both runs, but you don\u2019t start from scratch anymore, so you should skip a long prefix. A correct solution is to save RNG states.</p>", "body_text": "Just setting the random seed to the same value won\u2019t let you resume the training deterministically. It\u2019s true that the numbers generated will be the same in both runs, but you don\u2019t start from scratch anymore, so you should skip a long prefix. A correct solution is to save RNG states.", "body": "Just setting the random seed to the same value won\u2019t let you resume the training deterministically. It\u2019s true that the numbers generated will be the same in both runs, but you don\u2019t start from scratch anymore, so you should skip a long prefix. A correct solution is to save RNG states."}