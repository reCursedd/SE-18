{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165425923", "pull_request_review_id": 93361964, "id": 165425923, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NTQyNTkyMw==", "diff_hunk": "@@ -75,24 +93,32 @@ static value_set findAllRequiresGradNodes(\n   return requires_grad_set;\n }\n \n-static Value* allocZerosLike(Value *v) {\n-  static const Symbol constant_sym = \"constant\"_sym;\n-  static const Symbol is_zero_sym  = \"is_zero\"_sym;\n-  static const Symbol value_sym    = \"value\"_sym;\n+static Value* createZerosLike(Value *v) {\n   JIT_EXPECTM(v->hasType(), \"can't allocate zero gradient for a value without a type\");\n   Graph *graph = v->owningGraph();\n   auto type = v->type()->expect<TensorType>();\n   AutoGPU gpu_guard(type->device());\n \n   auto & at_type = type->device() == -1 ? at::CPU(type->scalarType()) : at::CUDA(type->scalarType());\n   auto zeros = at_type.zeros({1}).expand(type->sizes());\n-  Node *constant = graph->create(constant_sym)\n-                        ->t_(value_sym, zeros)\n-                        ->i_(is_zero_sym, 1);\n+  Node *constant = graph->createConstant(zeros)\n+                        ->i_(kis_zero, 1);\n   graph->appendNode(constant);\n   return constant->output();\n }\n \n+// any vjp input may be undefined, and we need to potentially replace it\n+// with a zero tensor of the right size if required.\n+// this function inserts a guard into the graph that does this replacement.\n+// ReplaceIfUndef(dv,c) replaces dv with c if dv is undef.\n+// During Graph specialization these guards will get removed when\n+// 'dv' is known to be undef, and the zeros will be propagated if possible.\n+static Value* createUndefGuard(Value * dv, Value * alternative) {\n+  Graph* graph = dv->owningGraph();\n+  Node * n = graph->create(kReplaceIfUndef, {dv, alternative});\n+  return graph->appendNode(n)->output();\n+}", "path": "torch/csrc/jit/autodiff.cpp", "position": 81, "original_position": 81, "commit_id": "50f1371ab9d4af3f9ffba960951d8178b326d11c", "original_commit_id": "b84e313451e285323db29add20e1602d1e51aab8", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I don't think we should be putting these annotations in the graph like this. Is there any reason why we want to pollute the graph with them instead of having a \"zero-propagation pass\" that just works its way from inputs that we tell it to be vjps? We already have this information in the `Gradient` struct.\r\n\r\nIs it because we want to avoid passing undefined tensors to ATen functions?", "created_at": "2018-02-01T17:17:40Z", "updated_at": "2018-11-23T15:38:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/4982#discussion_r165425923", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4982", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165425923"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4982#discussion_r165425923"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4982"}}, "body_html": "<p>I don't think we should be putting these annotations in the graph like this. Is there any reason why we want to pollute the graph with them instead of having a \"zero-propagation pass\" that just works its way from inputs that we tell it to be vjps? We already have this information in the <code>Gradient</code> struct.</p>\n<p>Is it because we want to avoid passing undefined tensors to ATen functions?</p>", "body_text": "I don't think we should be putting these annotations in the graph like this. Is there any reason why we want to pollute the graph with them instead of having a \"zero-propagation pass\" that just works its way from inputs that we tell it to be vjps? We already have this information in the Gradient struct.\nIs it because we want to avoid passing undefined tensors to ATen functions?"}