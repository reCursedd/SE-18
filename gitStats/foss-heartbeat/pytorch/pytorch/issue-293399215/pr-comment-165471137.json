{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165471137", "pull_request_review_id": 93417480, "id": 165471137, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NTQ3MTEzNw==", "diff_hunk": "@@ -75,24 +93,32 @@ static value_set findAllRequiresGradNodes(\n   return requires_grad_set;\n }\n \n-static Value* allocZerosLike(Value *v) {\n-  static const Symbol constant_sym = \"constant\"_sym;\n-  static const Symbol is_zero_sym  = \"is_zero\"_sym;\n-  static const Symbol value_sym    = \"value\"_sym;\n+static Value* createZerosLike(Value *v) {\n   JIT_EXPECTM(v->hasType(), \"can't allocate zero gradient for a value without a type\");\n   Graph *graph = v->owningGraph();\n   auto type = v->type()->expect<TensorType>();\n   AutoGPU gpu_guard(type->device());\n \n   auto & at_type = type->device() == -1 ? at::CPU(type->scalarType()) : at::CUDA(type->scalarType());\n   auto zeros = at_type.zeros({1}).expand(type->sizes());\n-  Node *constant = graph->create(constant_sym)\n-                        ->t_(value_sym, zeros)\n-                        ->i_(is_zero_sym, 1);\n+  Node *constant = graph->createConstant(zeros)\n+                        ->i_(kis_zero, 1);\n   graph->appendNode(constant);\n   return constant->output();\n }\n \n+// any vjp input may be undefined, and we need to potentially replace it\n+// with a zero tensor of the right size if required.\n+// this function inserts a guard into the graph that does this replacement.\n+// ReplaceIfUndef(dv,c) replaces dv with c if dv is undef.\n+// During Graph specialization these guards will get removed when\n+// 'dv' is known to be undef, and the zeros will be propagated if possible.\n+static Value* createUndefGuard(Value * dv, Value * alternative) {\n+  Graph* graph = dv->owningGraph();\n+  Node * n = graph->create(kReplaceIfUndef, {dv, alternative});\n+  return graph->appendNode(n)->output();\n+}", "path": "torch/csrc/jit/autodiff.cpp", "position": 81, "original_position": 81, "commit_id": "50f1371ab9d4af3f9ffba960951d8178b326d11c", "original_commit_id": "b84e313451e285323db29add20e1602d1e51aab8", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "It does not get removed in all cases. So it has to be in the graph to make the graph semantically correct. This is not \"polluting\" the graph. It is correctly specifying what should happen, and then cleaning up the trivial cases later using rewrite rules. This has to happen because the autograd and the autodiff handle Undefined tensors differently. ", "created_at": "2018-02-01T19:59:20Z", "updated_at": "2018-11-23T15:38:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/4982#discussion_r165471137", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4982", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165471137"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4982#discussion_r165471137"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4982"}}, "body_html": "<p>It does not get removed in all cases. So it has to be in the graph to make the graph semantically correct. This is not \"polluting\" the graph. It is correctly specifying what should happen, and then cleaning up the trivial cases later using rewrite rules. This has to happen because the autograd and the autodiff handle Undefined tensors differently.</p>", "body_text": "It does not get removed in all cases. So it has to be in the graph to make the graph semantically correct. This is not \"polluting\" the graph. It is correctly specifying what should happen, and then cleaning up the trivial cases later using rewrite rules. This has to happen because the autograd and the autodiff handle Undefined tensors differently.", "in_reply_to_id": 165425923}