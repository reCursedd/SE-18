{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/354744964", "html_url": "https://github.com/pytorch/pytorch/issues/4382#issuecomment-354744964", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4382", "id": 354744964, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDc0NDk2NA==", "user": {"login": "0phoff", "id": 11853089, "node_id": "MDQ6VXNlcjExODUzMDg5", "avatar_url": "https://avatars3.githubusercontent.com/u/11853089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0phoff", "html_url": "https://github.com/0phoff", "followers_url": "https://api.github.com/users/0phoff/followers", "following_url": "https://api.github.com/users/0phoff/following{/other_user}", "gists_url": "https://api.github.com/users/0phoff/gists{/gist_id}", "starred_url": "https://api.github.com/users/0phoff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0phoff/subscriptions", "organizations_url": "https://api.github.com/users/0phoff/orgs", "repos_url": "https://api.github.com/users/0phoff/repos", "events_url": "https://api.github.com/users/0phoff/events{/privacy}", "received_events_url": "https://api.github.com/users/0phoff/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-02T11:07:52Z", "updated_at": "2018-01-02T11:07:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So I tinkered around a bit with these workers (mostly just running stuff and printing out indices), and now I understand my idea will not work.</p>\n<p>If I understand everything correctly, you keep <strong>2 * workers</strong> number of batches in your queue. This means that if I would call a function in <em>put_indices()</em> to change some <em>multiprocessing</em> variables, the <strong>2 * workers</strong> batches that were already in the queue would also get affected by it, which is not what I want.<br>\nMoreover, batches that are in the middle of being processed, also get affected, and thus generate batches with half of the batch processed with the previous value and half of it with the next...</p>\n<p>So, my question remains unanswered...<br>\nIs there <em>- or will there ever be -</em> a way to communicate in a synchronized way with your workers?<br>\nI know this would probably involve letting the <em>index_queu</em> empty itself, change a variable, and repopulate it. This will in turn slow down the whole data fetching process, so I  can understand if you are not implementing this...<br>\nIf someone could help me find another solution to my <a href=\"https://discuss.pytorch.org/t/communicating-with-dataloader-workers/11473\" rel=\"nofollow\">problem</a>, that would be great! <g-emoji class=\"g-emoji\" alias=\"pray\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f64f.png\">\ud83d\ude4f</g-emoji></p>", "body_text": "So I tinkered around a bit with these workers (mostly just running stuff and printing out indices), and now I understand my idea will not work.\nIf I understand everything correctly, you keep 2 * workers number of batches in your queue. This means that if I would call a function in put_indices() to change some multiprocessing variables, the 2 * workers batches that were already in the queue would also get affected by it, which is not what I want.\nMoreover, batches that are in the middle of being processed, also get affected, and thus generate batches with half of the batch processed with the previous value and half of it with the next...\nSo, my question remains unanswered...\nIs there - or will there ever be - a way to communicate in a synchronized way with your workers?\nI know this would probably involve letting the index_queu empty itself, change a variable, and repopulate it. This will in turn slow down the whole data fetching process, so I  can understand if you are not implementing this...\nIf someone could help me find another solution to my problem, that would be great! \ud83d\ude4f", "body": "So I tinkered around a bit with these workers (mostly just running stuff and printing out indices), and now I understand my idea will not work.  \r\n\r\nIf I understand everything correctly, you keep **2 * workers** number of batches in your queue. This means that if I would call a function in _put_indices()_ to change some _multiprocessing_ variables, the **2 * workers** batches that were already in the queue would also get affected by it, which is not what I want.  \r\nMoreover, batches that are in the middle of being processed, also get affected, and thus generate batches with half of the batch processed with the previous value and half of it with the next...\r\n\r\nSo, my question remains unanswered...  \r\nIs there _- or will there ever be -_ a way to communicate in a synchronized way with your workers?  \r\nI know this would probably involve letting the _index_queu_ empty itself, change a variable, and repopulate it. This will in turn slow down the whole data fetching process, so I  can understand if you are not implementing this...  \r\nIf someone could help me find another solution to my [problem](https://discuss.pytorch.org/t/communicating-with-dataloader-workers/11473), that would be great! :pray:\r\n"}