{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/291886477", "html_url": "https://github.com/pytorch/pytorch/issues/922#issuecomment-291886477", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/922", "id": 291886477, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTg4NjQ3Nw==", "user": {"login": "pedropgusmao", "id": 847743, "node_id": "MDQ6VXNlcjg0Nzc0Mw==", "avatar_url": "https://avatars2.githubusercontent.com/u/847743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pedropgusmao", "html_url": "https://github.com/pedropgusmao", "followers_url": "https://api.github.com/users/pedropgusmao/followers", "following_url": "https://api.github.com/users/pedropgusmao/following{/other_user}", "gists_url": "https://api.github.com/users/pedropgusmao/gists{/gist_id}", "starred_url": "https://api.github.com/users/pedropgusmao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pedropgusmao/subscriptions", "organizations_url": "https://api.github.com/users/pedropgusmao/orgs", "repos_url": "https://api.github.com/users/pedropgusmao/repos", "events_url": "https://api.github.com/users/pedropgusmao/events{/privacy}", "received_events_url": "https://api.github.com/users/pedropgusmao/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-05T14:51:36Z", "updated_at": "2017-04-05T14:51:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm still digging, but I believe there is a problem in the interface between <code>torch.mul()</code> and the vector multiplication functions in <code>VSX.c</code>. Also I think functions in <code>VSX.c</code> are only called when multiplication does not use slices.</p>\n<p>I say this because the segmentation fault that emerges for torch's set of tests is actually generated from <code>test_abs</code> and not from <code>test_mul</code>.</p>\n<p>You can check this by running:</p>\n<pre><code>$ ptyhon path_to_pytorch/test/test_torch.py TestTorch.test_abs\nSegmentation fault (core dumped)\n</code></pre>\n<p>and</p>\n<pre><code>$ ptyhon path_to_pytorch/test/test_torch.py TestTorch.test_mul\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n</code></pre>\n<p>Now, if you check the code for each one of these tests you will see that <code>test_abs</code> uses <code>tensor.mul(value)</code>  while <code>test_mul</code> uses a <code>sliced_tensor[:,3].mul_(value)</code>.</p>\n<p>This become clearer if you add the following tests to the same script</p>\n<pre><code>def test_mul_slice_mat(self):\n    m1 = torch.ones(10,10)\n    m  = m1[:,0].mul(5)\n \ndef test_mul_vec(self):\n    m1 = torch.ones(10,1)\n    m = m1.mul(5)\n</code></pre>\n<p>and run them as before.</p>\n<p>At least for me, using the sliced version gives no error while the normal version it gives me segfault.</p>\n<p>Now, when compiling pytorch with a DEBUG flag (I guess I just exported <code>export DEBUG=1</code> before compiling it) and using <code>gdb</code> with a breakpoint at <code>path_to_pytorch/torch/lib/TH/vector/VSX.c:408</code> (inside the function but outside a loop) it lets me see that <code>test_abs</code> enters <code>THDoubleVector_muls_VSX</code> while <code>test_mul</code> doesn't .</p>\n<pre><code>(gan) user@minsky31:~/repositories/pytorch$ gdb python\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\n....\n(gdb) b /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408\nNo source file named /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (/home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408) pending.\n(gdb) run test/test_torch.py TestTorch.test_abs\nStarting program: /home/user/miniconda3/envs/gan/bin/python test/test_torch.py TestTorch.test_abs\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\n[New Thread 0x3fff156ef1a0 (LWP 3286)]\n\nThread 1 \"python\" hit Breakpoint 1, THDoubleVector_muls_VSX (y=0x10ba1e00, x=0x10b9fe00, n=70367515247288) at /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:411\n411         for (i = 0; i &lt;= n-24; i += 24)\n</code></pre>\n<p>and</p>\n<pre><code>(gan) user@minsky31:~/repositories/pytorch$ gdb python\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\n...\n(gdb) b /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408\nNo source file named /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (/home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408) pending.\n(gdb) run test/test_torch.py TestTorch.test_mul\nStarting program: /home/user/miniconda3/envs/gan/bin/python test/test_torch.py TestTorch.test_mul\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\n[New Thread 0x3fff156ef1a0 (LWP 3396)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n[Thread 0x3fff156ef1a0 (LWP 3396) exited]\n[Inferior 1 (process 3393) exited normally]\n</code></pre>\n<p>At this point, if we check the values of <code>n</code> and <code>y</code> inside <code>THDoubleVector_muls_VSX</code> (for test_abs),  we see some strange things:</p>\n<pre><code>(gdb) p n\n$1 = 70367515247288\n(gdb) p *x\n$2 = 0.69646918727084994\n(gdb) p *y\n$3 = 0\n(gdb) p *(y+9)\n$4 = 0\n(gdb) p *(y+10)\n$5 = 0\n(gdb) p *(x+9)\n$6 = 0.49111893260851502\n(gdb) p *(x+10)\n$7 = 0.42310646059922874\n(gdb) p *x\n$8 = 0.69646918727084994\n(gdb) p *(x+999)\n$9 = 0.56434643105603755\n(gdb) p *(x+1000)\n$10 = 0\n(gdb) p *(y+999)\n$11 = 0\n(gdb) p *(y+1000)\n$12 = 0\n(gdb) p *(y)\n$13 = 0\n</code></pre>\n<p>It is as if <code>x</code> is correctly passed (its size should be 1000 in test_abs); <code>y</code> is not correctly passed; and <code>n</code>, which I believe should be the length of each vector (expanded in the case of multiplication by constant), is just a huge number.</p>\n<p>If someone could point me to where <code>mul</code> calls <code>THDoubleVector_muls_VSX</code> and alike, I could keep digging.</p>", "body_text": "I'm still digging, but I believe there is a problem in the interface between torch.mul() and the vector multiplication functions in VSX.c. Also I think functions in VSX.c are only called when multiplication does not use slices.\nI say this because the segmentation fault that emerges for torch's set of tests is actually generated from test_abs and not from test_mul.\nYou can check this by running:\n$ ptyhon path_to_pytorch/test/test_torch.py TestTorch.test_abs\nSegmentation fault (core dumped)\n\nand\n$ ptyhon path_to_pytorch/test/test_torch.py TestTorch.test_mul\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n\nNow, if you check the code for each one of these tests you will see that test_abs uses tensor.mul(value)  while test_mul uses a sliced_tensor[:,3].mul_(value).\nThis become clearer if you add the following tests to the same script\ndef test_mul_slice_mat(self):\n    m1 = torch.ones(10,10)\n    m  = m1[:,0].mul(5)\n \ndef test_mul_vec(self):\n    m1 = torch.ones(10,1)\n    m = m1.mul(5)\n\nand run them as before.\nAt least for me, using the sliced version gives no error while the normal version it gives me segfault.\nNow, when compiling pytorch with a DEBUG flag (I guess I just exported export DEBUG=1 before compiling it) and using gdb with a breakpoint at path_to_pytorch/torch/lib/TH/vector/VSX.c:408 (inside the function but outside a loop) it lets me see that test_abs enters THDoubleVector_muls_VSX while test_mul doesn't .\n(gan) user@minsky31:~/repositories/pytorch$ gdb python\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\n....\n(gdb) b /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408\nNo source file named /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (/home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408) pending.\n(gdb) run test/test_torch.py TestTorch.test_abs\nStarting program: /home/user/miniconda3/envs/gan/bin/python test/test_torch.py TestTorch.test_abs\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\n[New Thread 0x3fff156ef1a0 (LWP 3286)]\n\nThread 1 \"python\" hit Breakpoint 1, THDoubleVector_muls_VSX (y=0x10ba1e00, x=0x10b9fe00, n=70367515247288) at /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:411\n411         for (i = 0; i <= n-24; i += 24)\n\nand\n(gan) user@minsky31:~/repositories/pytorch$ gdb python\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\n...\n(gdb) b /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408\nNo source file named /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (/home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408) pending.\n(gdb) run test/test_torch.py TestTorch.test_mul\nStarting program: /home/user/miniconda3/envs/gan/bin/python test/test_torch.py TestTorch.test_mul\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\n[New Thread 0x3fff156ef1a0 (LWP 3396)]\n.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n[Thread 0x3fff156ef1a0 (LWP 3396) exited]\n[Inferior 1 (process 3393) exited normally]\n\nAt this point, if we check the values of n and y inside THDoubleVector_muls_VSX (for test_abs),  we see some strange things:\n(gdb) p n\n$1 = 70367515247288\n(gdb) p *x\n$2 = 0.69646918727084994\n(gdb) p *y\n$3 = 0\n(gdb) p *(y+9)\n$4 = 0\n(gdb) p *(y+10)\n$5 = 0\n(gdb) p *(x+9)\n$6 = 0.49111893260851502\n(gdb) p *(x+10)\n$7 = 0.42310646059922874\n(gdb) p *x\n$8 = 0.69646918727084994\n(gdb) p *(x+999)\n$9 = 0.56434643105603755\n(gdb) p *(x+1000)\n$10 = 0\n(gdb) p *(y+999)\n$11 = 0\n(gdb) p *(y+1000)\n$12 = 0\n(gdb) p *(y)\n$13 = 0\n\nIt is as if x is correctly passed (its size should be 1000 in test_abs); y is not correctly passed; and n, which I believe should be the length of each vector (expanded in the case of multiplication by constant), is just a huge number.\nIf someone could point me to where mul calls THDoubleVector_muls_VSX and alike, I could keep digging.", "body": "I'm still digging, but I believe there is a problem in the interface between `torch.mul()` and the vector multiplication functions in `VSX.c`. Also I think functions in `VSX.c` are only called when multiplication does not use slices. \r\n\r\nI say this because the segmentation fault that emerges for torch's set of tests is actually generated from `test_abs` and not from `test_mul`.\r\n\r\nYou can check this by running:\r\n```\r\n$ ptyhon path_to_pytorch/test/test_torch.py TestTorch.test_abs\r\nSegmentation fault (core dumped)\r\n```\r\nand\r\n```\r\n$ ptyhon path_to_pytorch/test/test_torch.py TestTorch.test_mul\r\n.\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.001s\r\n\r\nOK\r\n```\r\n\r\nNow, if you check the code for each one of these tests you will see that `test_abs` uses `tensor.mul(value)`  while `test_mul` uses a `sliced_tensor[:,3].mul_(value)`.  \r\n\r\nThis become clearer if you add the following tests to the same script \r\n```\r\ndef test_mul_slice_mat(self):\r\n    m1 = torch.ones(10,10)\r\n    m  = m1[:,0].mul(5)\r\n \r\ndef test_mul_vec(self):\r\n    m1 = torch.ones(10,1)\r\n    m = m1.mul(5)\r\n```\r\nand run them as before. \r\n\r\nAt least for me, using the sliced version gives no error while the normal version it gives me segfault. \r\n\r\nNow, when compiling pytorch with a DEBUG flag (I guess I just exported `export DEBUG=1` before compiling it) and using `gdb` with a breakpoint at `path_to_pytorch/torch/lib/TH/vector/VSX.c:408` (inside the function but outside a loop) it lets me see that `test_abs` enters `THDoubleVector_muls_VSX` while `test_mul` doesn't . \r\n\r\n```\r\n(gan) user@minsky31:~/repositories/pytorch$ gdb python\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\r\n....\r\n(gdb) b /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408\r\nNo source file named /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c.\r\nMake breakpoint pending on future shared library load? (y or [n]) y\r\nBreakpoint 1 (/home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408) pending.\r\n(gdb) run test/test_torch.py TestTorch.test_abs\r\nStarting program: /home/user/miniconda3/envs/gan/bin/python test/test_torch.py TestTorch.test_abs\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x3fff156ef1a0 (LWP 3286)]\r\n\r\nThread 1 \"python\" hit Breakpoint 1, THDoubleVector_muls_VSX (y=0x10ba1e00, x=0x10b9fe00, n=70367515247288) at /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:411\r\n411         for (i = 0; i <= n-24; i += 24)\r\n``` \r\nand\r\n\r\n```\r\n(gan) user@minsky31:~/repositories/pytorch$ gdb python\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\r\n...\r\n(gdb) b /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408\r\nNo source file named /home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c.\r\nMake breakpoint pending on future shared library load? (y or [n]) y\r\nBreakpoint 1 (/home/user/repositories/pytorch/torch/lib/TH/vector/VSX.c:408) pending.\r\n(gdb) run test/test_torch.py TestTorch.test_mul\r\nStarting program: /home/user/miniconda3/envs/gan/bin/python test/test_torch.py TestTorch.test_mul\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x3fff156ef1a0 (LWP 3396)]\r\n.\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.001s\r\n\r\nOK\r\n[Thread 0x3fff156ef1a0 (LWP 3396) exited]\r\n[Inferior 1 (process 3393) exited normally]\r\n```\r\nAt this point, if we check the values of `n` and `y` inside `THDoubleVector_muls_VSX` (for test_abs),  we see some strange things:\r\n\r\n```\r\n(gdb) p n\r\n$1 = 70367515247288\r\n(gdb) p *x\r\n$2 = 0.69646918727084994\r\n(gdb) p *y\r\n$3 = 0\r\n(gdb) p *(y+9)\r\n$4 = 0\r\n(gdb) p *(y+10)\r\n$5 = 0\r\n(gdb) p *(x+9)\r\n$6 = 0.49111893260851502\r\n(gdb) p *(x+10)\r\n$7 = 0.42310646059922874\r\n(gdb) p *x\r\n$8 = 0.69646918727084994\r\n(gdb) p *(x+999)\r\n$9 = 0.56434643105603755\r\n(gdb) p *(x+1000)\r\n$10 = 0\r\n(gdb) p *(y+999)\r\n$11 = 0\r\n(gdb) p *(y+1000)\r\n$12 = 0\r\n(gdb) p *(y)\r\n$13 = 0\r\n```\r\nIt is as if `x` is correctly passed (its size should be 1000 in test_abs); `y` is not correctly passed; and `n`, which I believe should be the length of each vector (expanded in the case of multiplication by constant), is just a huge number. \r\n\r\nIf someone could point me to where `mul` calls `THDoubleVector_muls_VSX` and alike, I could keep digging. \r\n\r\n\r\n"}