{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4649", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4649/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4649/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4649/events", "html_url": "https://github.com/pytorch/pytorch/issues/4649", "id": 288298789, "node_id": "MDU6SXNzdWUyODgyOTg3ODk=", "number": 4649, "title": "\"grad\" is \"None\" on sliced section of Variable", "user": {"login": "benvcutilli", "id": 23709161, "node_id": "MDQ6VXNlcjIzNzA5MTYx", "avatar_url": "https://avatars1.githubusercontent.com/u/23709161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benvcutilli", "html_url": "https://github.com/benvcutilli", "followers_url": "https://api.github.com/users/benvcutilli/followers", "following_url": "https://api.github.com/users/benvcutilli/following{/other_user}", "gists_url": "https://api.github.com/users/benvcutilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/benvcutilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benvcutilli/subscriptions", "organizations_url": "https://api.github.com/users/benvcutilli/orgs", "repos_url": "https://api.github.com/users/benvcutilli/repos", "events_url": "https://api.github.com/users/benvcutilli/events{/privacy}", "received_events_url": "https://api.github.com/users/benvcutilli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-13T03:24:30Z", "updated_at": "2018-01-13T16:50:19Z", "closed_at": "2018-01-13T10:25:22Z", "author_association": "NONE", "body_html": "<p>If I had something like:</p>\n<pre><code>superVariable = torch.autograd.Variable(torch.ones(4,4))\nsubVariable = superVariable[1:3].cuda()\nnetwork.forward(subVariable).backward()\nprint(subVariable.grad)\n</code></pre>\n<p><code>subVariable.grad</code> will show as <code>None</code>, but <code>superVariable.grad</code> is not <code>None</code>. Not sure if this is a bug, a feature request, or if I am doing something wrong. I've checked that <code>subVariable.requires_grad</code> is <code>True</code>. This would be incredibly convenient in the case of if we have a lower-case \"v\" variable already dedicated to a mini batch, and we want the gradient w.r.t that mini batch.</p>\n<p>Sorry if I missed something important. I took a semi-close look at the documentation for any obvious explanation, but came up with nothing. Also, <code>torch.__version__</code> says <code>'0.3.0.post4'</code>.</p>", "body_text": "If I had something like:\nsuperVariable = torch.autograd.Variable(torch.ones(4,4))\nsubVariable = superVariable[1:3].cuda()\nnetwork.forward(subVariable).backward()\nprint(subVariable.grad)\n\nsubVariable.grad will show as None, but superVariable.grad is not None. Not sure if this is a bug, a feature request, or if I am doing something wrong. I've checked that subVariable.requires_grad is True. This would be incredibly convenient in the case of if we have a lower-case \"v\" variable already dedicated to a mini batch, and we want the gradient w.r.t that mini batch.\nSorry if I missed something important. I took a semi-close look at the documentation for any obvious explanation, but came up with nothing. Also, torch.__version__ says '0.3.0.post4'.", "body": "If I had something like:\r\n\r\n```\r\nsuperVariable = torch.autograd.Variable(torch.ones(4,4))\r\nsubVariable = superVariable[1:3].cuda()\r\nnetwork.forward(subVariable).backward()\r\nprint(subVariable.grad)\r\n```\r\n\r\n`subVariable.grad` will show as `None`, but `superVariable.grad` is not `None`. Not sure if this is a bug, a feature request, or if I am doing something wrong. I've checked that `subVariable.requires_grad` is `True`. This would be incredibly convenient in the case of if we have a lower-case \"v\" variable already dedicated to a mini batch, and we want the gradient w.r.t that mini batch.\r\n\r\nSorry if I missed something important. I took a semi-close look at the documentation for any obvious explanation, but came up with nothing. Also, `torch.__version__` says `'0.3.0.post4'`."}