{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12824", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12824/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12824/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12824/events", "html_url": "https://github.com/pytorch/pytorch/pull/12824", "id": 371619041, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI0MDI2Mjcw", "number": 12824, "title": "Speed up tensor.resize_(sizes) when tensor has correct size", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-10-18T16:47:14Z", "updated_at": "2018-11-23T15:53:23Z", "closed_at": "2018-10-26T04:11:02Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/12824", "html_url": "https://github.com/pytorch/pytorch/pull/12824", "diff_url": "https://github.com/pytorch/pytorch/pull/12824.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/12824.patch"}, "body_html": "<p>While using gbenchmark, I found <code>tensor.resize_({0})</code> would take 300ns<br>\nif tensor already has the correct size. This is important for<br>\n<code>at::empty({0})</code> perf because <code>at::empty</code> always calls <code>resize_</code>, which<br>\nin turn is a important for JIT perf: the fusion compiler creates empty<br>\ntensors and then <code>resize_</code>s them to computed sizes. Most of the 300ns is<br>\ndue to DeviceGuard (200ns)</p>\n<p>Summary of findings:</p>\n<ul>\n<li><code>at::empty({0}, cuda)</code>: 851ns</li>\n<li><code>empty_tensor.resize({0})</code>: 308ns</li>\n<li><code>DeviceGuard(tensor)</code>: ctor + dtor: 200ns (Going to look into this<br>\nnext because it impacts <code>resize_</code> perf).</li>\n<li>vdispatch overhead (<code>tensor.resize_()</code> vs<br>\n<code>at::native::resize__cuda(tensor)</code>): ~10ns</li>\n</ul>\n<p>This PR rips out the TH <code>resize_</code> implementation and adds it to ATen<br>\nwith the following modifications:</p>\n<ul>\n<li>DeviceGuard used only after the same-size check.</li>\n<li>Same-size check rewritten for simplicity. The new check doesn't<br>\naffect perf.</li>\n<li>empty_cpu / empty_cuda avoid the dispatch overhead to<br>\ntensor.resize_.</li>\n</ul>\n<p>Timing with this PR:</p>\n<ul>\n<li><code>at::empty({0}, cuda)</code>: 363ns</li>\n<li><code>empty_tensor.resize_({0})</code>: 17ns</li>\n</ul>\n<p>Future:</p>\n<ul>\n<li>Investigate <code>resize_(sizes)</code> slowness when <code>tensor.sizes() != sizes</code></li>\n<li>Should tell resize_as_ to use the new resize_ implementation...<br>\n(because resize_as_ is in TH, it is calling the old TH resize_)</li>\n</ul>", "body_text": "While using gbenchmark, I found tensor.resize_({0}) would take 300ns\nif tensor already has the correct size. This is important for\nat::empty({0}) perf because at::empty always calls resize_, which\nin turn is a important for JIT perf: the fusion compiler creates empty\ntensors and then resize_s them to computed sizes. Most of the 300ns is\ndue to DeviceGuard (200ns)\nSummary of findings:\n\nat::empty({0}, cuda): 851ns\nempty_tensor.resize({0}): 308ns\nDeviceGuard(tensor): ctor + dtor: 200ns (Going to look into this\nnext because it impacts resize_ perf).\nvdispatch overhead (tensor.resize_() vs\nat::native::resize__cuda(tensor)): ~10ns\n\nThis PR rips out the TH resize_ implementation and adds it to ATen\nwith the following modifications:\n\nDeviceGuard used only after the same-size check.\nSame-size check rewritten for simplicity. The new check doesn't\naffect perf.\nempty_cpu / empty_cuda avoid the dispatch overhead to\ntensor.resize_.\n\nTiming with this PR:\n\nat::empty({0}, cuda): 363ns\nempty_tensor.resize_({0}): 17ns\n\nFuture:\n\nInvestigate resize_(sizes) slowness when tensor.sizes() != sizes\nShould tell resize_as_ to use the new resize_ implementation...\n(because resize_as_ is in TH, it is calling the old TH resize_)", "body": "While using gbenchmark, I found `tensor.resize_({0})` would take 300ns\r\nif tensor already has the correct size. This is important for\r\n`at::empty({0})` perf because `at::empty` always calls `resize_`, which\r\nin turn is a important for JIT perf: the fusion compiler creates empty\r\ntensors and then `resize_`s them to computed sizes. Most of the 300ns is\r\ndue to DeviceGuard (200ns)\r\n\r\nSummary of findings:\r\n- `at::empty({0}, cuda)`: 851ns\r\n- `empty_tensor.resize({0})`: 308ns\r\n- `DeviceGuard(tensor)`: ctor + dtor: 200ns (Going to look into this\r\n  next because it impacts `resize_` perf).\r\n- vdispatch overhead (`tensor.resize_()` vs\r\n  `at::native::resize__cuda(tensor)`): ~10ns\r\n\r\nThis PR rips out the TH `resize_` implementation and adds it to ATen\r\nwith the following modifications:\r\n- DeviceGuard used only after the same-size check.\r\n- Same-size check rewritten for simplicity. The new check doesn't\r\naffect perf.\r\n- empty_cpu / empty_cuda avoid the dispatch overhead to\r\ntensor.resize_.\r\n\r\nTiming with this PR:\r\n- `at::empty({0}, cuda)`: 363ns\r\n- `empty_tensor.resize_({0})`: 17ns\r\n\r\nFuture:\r\n- Investigate `resize_(sizes)` slowness when `tensor.sizes() != sizes`\r\n- Should tell resize_as_ to use the new resize_ implementation...\r\n(because resize_as_ is in TH, it is calling the old TH resize_)\r\n\r\n"}