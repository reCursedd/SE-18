{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393669533", "html_url": "https://github.com/pytorch/pytorch/issues/7996#issuecomment-393669533", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7996", "id": 393669533, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzY2OTUzMw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-31T20:30:03Z", "updated_at": "2018-05-31T20:30:12Z", "author_association": "MEMBER", "body_html": "<p>So this is a side effect of a recent addition to our tracing system, which is that now we can capture dependencies on tensor sizes. The way it works is by making <code>.size()</code> return a tuple of PyTorch scalars instead of Python numbers, so we can attach trace metadata to them. Then, <code>_infer_size</code> tries to unpack them, but sees an unexpected type and fails. If we just want to get this working quickly, and don't care about preserving the semantics of <code>_infer_size</code> in traces (meaning they will be bound to a particular input size), then we can just patch the function to accept tensors as well (we have functions for that). If you'd like it to work in a generic way, we might need to have some special <code>symbolic_override</code>s or implement it as an autograd function.</p>\n<p>Let me know what you think!</p>", "body_text": "So this is a side effect of a recent addition to our tracing system, which is that now we can capture dependencies on tensor sizes. The way it works is by making .size() return a tuple of PyTorch scalars instead of Python numbers, so we can attach trace metadata to them. Then, _infer_size tries to unpack them, but sees an unexpected type and fails. If we just want to get this working quickly, and don't care about preserving the semantics of _infer_size in traces (meaning they will be bound to a particular input size), then we can just patch the function to accept tensors as well (we have functions for that). If you'd like it to work in a generic way, we might need to have some special symbolic_overrides or implement it as an autograd function.\nLet me know what you think!", "body": "So this is a side effect of a recent addition to our tracing system, which is that now we can capture dependencies on tensor sizes. The way it works is by making `.size()` return a tuple of PyTorch scalars instead of Python numbers, so we can attach trace metadata to them. Then, `_infer_size` tries to unpack them, but sees an unexpected type and fails. If we just want to get this working quickly, and don't care about preserving the semantics of `_infer_size` in traces (meaning they will be bound to a particular input size), then we can just patch the function to accept tensors as well (we have functions for that). If you'd like it to work in a generic way, we might need to have some special `symbolic_override`s or implement it as an autograd function.\r\n\r\nLet me know what you think!"}