{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13883", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13883/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13883/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13883/events", "html_url": "https://github.com/pytorch/pytorch/issues/13883", "id": 380054310, "node_id": "MDU6SXNzdWUzODAwNTQzMTA=", "number": 13883, "title": "Unable to do transfer tensors to GPU using `.cuda()` when using multiprocessing.Process with `fork`", "user": {"login": "Amir-Arsalan", "id": 6061721, "node_id": "MDQ6VXNlcjYwNjE3MjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/6061721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Amir-Arsalan", "html_url": "https://github.com/Amir-Arsalan", "followers_url": "https://api.github.com/users/Amir-Arsalan/followers", "following_url": "https://api.github.com/users/Amir-Arsalan/following{/other_user}", "gists_url": "https://api.github.com/users/Amir-Arsalan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Amir-Arsalan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Amir-Arsalan/subscriptions", "organizations_url": "https://api.github.com/users/Amir-Arsalan/orgs", "repos_url": "https://api.github.com/users/Amir-Arsalan/repos", "events_url": "https://api.github.com/users/Amir-Arsalan/events{/privacy}", "received_events_url": "https://api.github.com/users/Amir-Arsalan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-13T04:25:21Z", "updated_at": "2018-11-19T19:00:09Z", "closed_at": "2018-11-19T19:00:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I know there has were issues regarding this last year but I thought it should have got resolved by now after about a year. I wonder, has there been any updates on this? I want to be able to use <code>fork</code> instead of <code>spawn</code> for running my processes as my work depends heavily on the benefits that <code>fork</code>ing provides.</p>\n<p>I just noticed that I cannot transfer my tensors to GPU when in a <code>fork</code>ed process from the <code>multiprocessing.Process</code> module. I do not even do <code>import torch</code> before I call <code>multiprocessing.Process</code> process via the <code>.start()</code> function but I get the following error:</p>\n<pre><code>File \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 162, in _lazy_init\n   torch._C._cuda_init()\nRuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:51\n</code></pre>\n<p>So basically: 1) I run the process 2) do <code>import torch</code> inside of the <code>fork</code>ed process 3) call <code>torch.manual_seed()</code> and <code>torch.cuda.manual_seed()</code> functions inside the process 4) convert a Numpy tensor to a PyTorch tensor 5) transfer the PyTorch tensor to GPU via the <code>.cuda()</code> function. 6) The above error is thrown</p>\n<p>Update:<br>\nI noticed that some of the other packages that I have to import earlier on are probably calling the CUDA drivers and that's potentially why I am facing this issue. The strange thing is that although I manually called <code>torch.cuda.manual_seed()</code> before <code>fork</code>ing a process things are still find and I can do <code>.cuda()</code> but I cannot do this once I import the other packages in my workspace. I cannot change the pipeline of my work in terms of importing those packages and using them before doing <code>import torch</code> and I really need to find a way around this. I really hope there is gonna be a workaround for this. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "I know there has were issues regarding this last year but I thought it should have got resolved by now after about a year. I wonder, has there been any updates on this? I want to be able to use fork instead of spawn for running my processes as my work depends heavily on the benefits that forking provides.\nI just noticed that I cannot transfer my tensors to GPU when in a forked process from the multiprocessing.Process module. I do not even do import torch before I call multiprocessing.Process process via the .start() function but I get the following error:\nFile \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 162, in _lazy_init\n   torch._C._cuda_init()\nRuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:51\n\nSo basically: 1) I run the process 2) do import torch inside of the forked process 3) call torch.manual_seed() and torch.cuda.manual_seed() functions inside the process 4) convert a Numpy tensor to a PyTorch tensor 5) transfer the PyTorch tensor to GPU via the .cuda() function. 6) The above error is thrown\nUpdate:\nI noticed that some of the other packages that I have to import earlier on are probably calling the CUDA drivers and that's potentially why I am facing this issue. The strange thing is that although I manually called torch.cuda.manual_seed() before forking a process things are still find and I can do .cuda() but I cannot do this once I import the other packages in my workspace. I cannot change the pipeline of my work in terms of importing those packages and using them before doing import torch and I really need to find a way around this. I really hope there is gonna be a workaround for this. @ngimel @soumith", "body": "I know there has were issues regarding this last year but I thought it should have got resolved by now after about a year. I wonder, has there been any updates on this? I want to be able to use `fork` instead of `spawn` for running my processes as my work depends heavily on the benefits that `fork`ing provides.  \r\n\r\nI just noticed that I cannot transfer my tensors to GPU when in a `fork`ed process from the `multiprocessing.Process` module. I do not even do `import torch` before I call `multiprocessing.Process` process via the `.start()` function but I get the following error:\r\n\r\n ```\r\nFile \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 162, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:51\r\n```\r\n\r\nSo basically: 1) I run the process 2) do `import torch` inside of the `fork`ed process 3) call `torch.manual_seed()` and `torch.cuda.manual_seed()` functions inside the process 4) convert a Numpy tensor to a PyTorch tensor 5) transfer the PyTorch tensor to GPU via the `.cuda()` function. 6) The above error is thrown\r\n\r\nUpdate: \r\nI noticed that some of the other packages that I have to import earlier on are probably calling the CUDA drivers and that's potentially why I am facing this issue. The strange thing is that although I manually called `torch.cuda.manual_seed()` before `fork`ing a process things are still find and I can do `.cuda()` but I cannot do this once I import the other packages in my workspace. I cannot change the pipeline of my work in terms of importing those packages and using them before doing `import torch` and I really need to find a way around this. I really hope there is gonna be a workaround for this. @ngimel @soumith "}