{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/312445176", "html_url": "https://github.com/pytorch/pytorch/pull/1935#issuecomment-312445176", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1935", "id": 312445176, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjQ0NTE3Ng==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-01T17:32:23Z", "updated_at": "2017-07-01T17:32:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>What I've learned yesterday is that all 3 matrix multiplies in linear go through <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L19-L26\">https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L19-L26</a>, and even if output and add_matrix data pointers there are the same, after passing through wrappers t and r_ in THC are not the same, and memcopy here <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/generic/THCTensorMathBlas.cu#L245-L249\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/generic/THCTensorMathBlas.cu#L245-L249</a> is triggered. Before PR forward matrix multiply did not trigger the memcopy, both backward did. After PR, all three MMs do. That benchmark looks like it's close to being CPU bound, so nvprof is pretty much useless for timings, can be used only for the order in which kernels are invoked and kernel durations. (I'm getting .8 s instead of .4-.5 when running under nvprof). I've seen the difference only with the latest (8.0.88) cublas, that improved pre-PR time to .4. With 8.0.61 both pre-PR and post-PR is around .5 s (times are on P100).</p>", "body_text": "What I've learned yesterday is that all 3 matrix multiplies in linear go through https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L19-L26, and even if output and add_matrix data pointers there are the same, after passing through wrappers t and r_ in THC are not the same, and memcopy here https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/generic/THCTensorMathBlas.cu#L245-L249 is triggered. Before PR forward matrix multiply did not trigger the memcopy, both backward did. After PR, all three MMs do. That benchmark looks like it's close to being CPU bound, so nvprof is pretty much useless for timings, can be used only for the order in which kernels are invoked and kernel durations. (I'm getting .8 s instead of .4-.5 when running under nvprof). I've seen the difference only with the latest (8.0.88) cublas, that improved pre-PR time to .4. With 8.0.61 both pre-PR and post-PR is around .5 s (times are on P100).", "body": "What I've learned yesterday is that all 3 matrix multiplies in linear go through https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L19-L26, and even if output and add_matrix data pointers there are the same, after passing through wrappers t and r_ in THC are not the same, and memcopy here https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/generic/THCTensorMathBlas.cu#L245-L249 is triggered. Before PR forward matrix multiply did not trigger the memcopy, both backward did. After PR, all three MMs do. That benchmark looks like it's close to being CPU bound, so nvprof is pretty much useless for timings, can be used only for the order in which kernels are invoked and kernel durations. (I'm getting .8 s instead of .4-.5 when running under nvprof). I've seen the difference only with the latest (8.0.88) cublas, that improved pre-PR time to .4. With 8.0.61 both pre-PR and post-PR is around .5 s (times are on P100). "}