{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8641", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8641/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8641/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8641/events", "html_url": "https://github.com/pytorch/pytorch/pull/8641", "id": 333524591, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk1NzE1NzI1", "number": 8641, "title": "Allow autograd to work even when the shape of values cannot be determined", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-19T05:32:22Z", "updated_at": "2018-11-23T15:46:17Z", "closed_at": "2018-06-26T01:40:04Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8641", "html_url": "https://github.com/pytorch/pytorch/pull/8641", "diff_url": "https://github.com/pytorch/pytorch/pull/8641.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8641.patch"}, "body_html": "<p>This commit implements the solution proposed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"331817896\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8410\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8410/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8410\">#8410</a><br>\nto workaround the need to create zero tensors with the same shape as inputs.<br>\nIt introduces the concept of a LinearBlock which marks places in the code<br>\nwhere we know if all the inputs to the node are zero, then the outputs<br>\nto the node are also zero. Autodiff introduces LinearBlocks around<br>\nbackwards functions, which have this property. specializeUndef then<br>\npropagates Undef nodes using this information.</p>\n<p>Notes:</p>\n<ul>\n<li>Since we do not always specialize, we have a pass LowerLinearBlocks<br>\nthat replaces the block with an if statement that dynamically guards<br>\nthe Undef case.</li>\n<li>We introduce AutogradAdd which is addition that still works when<br>\nits inputs might be undefined. In cases where we specialize this will<br>\nget removed in favor of a normal add, but there are cases where<br>\ngradient graphs do not specialize (e.g. when they are not differentiable,<br>\nbut a derivative is required) so it is important for this op to be executable.</li>\n</ul>", "body_text": "This commit implements the solution proposed in #8410\nto workaround the need to create zero tensors with the same shape as inputs.\nIt introduces the concept of a LinearBlock which marks places in the code\nwhere we know if all the inputs to the node are zero, then the outputs\nto the node are also zero. Autodiff introduces LinearBlocks around\nbackwards functions, which have this property. specializeUndef then\npropagates Undef nodes using this information.\nNotes:\n\nSince we do not always specialize, we have a pass LowerLinearBlocks\nthat replaces the block with an if statement that dynamically guards\nthe Undef case.\nWe introduce AutogradAdd which is addition that still works when\nits inputs might be undefined. In cases where we specialize this will\nget removed in favor of a normal add, but there are cases where\ngradient graphs do not specialize (e.g. when they are not differentiable,\nbut a derivative is required) so it is important for this op to be executable.", "body": "This commit implements the solution proposed in https://github.com/pytorch/pytorch/issues/8410\r\nto workaround the need to create zero tensors with the same shape as inputs.\r\nIt introduces the concept of a LinearBlock which marks places in the code\r\nwhere we know if all the inputs to the node are zero, then the outputs\r\nto the node are also zero. Autodiff introduces LinearBlocks around\r\nbackwards functions, which have this property. specializeUndef then\r\npropagates Undef nodes using this information.\r\n\r\nNotes:\r\n* Since we do not always specialize, we have a pass LowerLinearBlocks\r\nthat replaces the block with an if statement that dynamically guards\r\nthe Undef case.\r\n* We introduce AutogradAdd which is addition that still works when\r\nits inputs might be undefined. In cases where we specialize this will\r\nget removed in favor of a normal add, but there are cases where\r\ngradient graphs do not specialize (e.g. when they are not differentiable,\r\nbut a derivative is required) so it is important for this op to be executable.\r\n\r\n"}