{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197217674", "pull_request_review_id": 130913481, "id": 197217674, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NzIxNzY3NA==", "diff_hunk": "@@ -409,70 +412,117 @@ struct GraphExecutorImpl {\n     return false;\n   }\n \n-\n-  // remove ReplaceIfUndef(v, replacement) nodes that consume inputs with 'v' if\n-  // the input is defined, and 'replacement' if it is not.\n-  // Note: this is a very limited pass. It looks at undefined inputs,\n-  // and cleans up ReplaceIfUndef nodes inserted by autodiff.\n+  // propagate undefined information through a gradient graph\n+  // Note: this is a very limited pass. It only propagates undefines for\n+  // operations generated by the symbolic autodiff code and cleans up\n+  // AutogradAdds when possible. Outputs of other nodes are conservatively\n+  // marked MaybeDefined and not optimized.\n   void specializeUndef(Graph & g, const ArgumentSpec & spec) {\n+    enum class State { Defined, Undefined, MaybeDefined };\n+    std::unordered_map<Value*, State> state;\n     for(size_t i = 0; i < spec.size(); i++) {\n-      std::vector<Value*> to_replace;\n-      // do not edit in place, since it invalidates uses iterator\n-      for(auto u : g.inputs()[i]->uses()) {\n-        if(u.user->kind() == prim::ReplaceIfUndef) {\n-          to_replace.push_back(u.user->output());\n-        }\n-      }\n-      for(auto v : to_replace) {\n-        // if it is defined, then we replace with 'v' if not,\n-        // we replace with 'replacement' which is normally just a zero tensor\n-        int idx = spec.tensorInfo(i).defined() ? 0 : 1;\n-        v->replaceAllUsesWith(v->node()->inputs()[idx]);\n-        v->node()->destroy();\n-      }\n+      state[g.inputs()[i]] = spec.tensorInfo(i).defined() ? State::Defined : State::Undefined;\n     }\n-  }\n-  // a + 0 -> a\n-  // 0 + a -> a\n-  void propagateZeros(Graph & g) {\n     for(auto it = g.nodes().begin(); it != g.nodes().end(); ++it) {\n-      if(it->kind() == aten::add && it->inputs().size() == 2 && at::Scalar(it->t(attr::alpha)).toDouble() == 1.0) {\n-        if(isZero(it->inputs()[0])) {\n-          it->output()->replaceAllUsesWith(it->inputs()[1]);\n-          it.destroyCurrent();\n-        } else if(isZero(it->inputs()[1])) {\n-          it->output()->replaceAllUsesWith(it->inputs()[0]);\n-          it.destroyCurrent();\n-        }\n+      auto n = *it;\n+      switch(n->kind()) {\n+        case prim::LinearBlock: {\n+          auto all_undefined =\n+              std::all_of(n->inputs().begin(), n->inputs().end(), [&](Value* v) {\n+                return state[v] == State::Undefined;\n+              });\n+          // Property 1: if all the gradInputs to the LinearBlock are undefined\n+          // then the gradOutputs are also zero and will be represented as undefined nodes\n+          if(all_undefined) {\n+            auto undef = g.createUndefined()->insertAfter(n)->output();\n+            for(auto o : n->outputs()) {\n+              o->replaceAllUsesWith(undef);\n+            }\n+          } else {\n+          // Property 2: LinearBlocks are required to correctly handle combinations\n+          // of defined and undefined inputs. They are expected to produce defined\n+          // output tensors in this case.\n+\n+            // Remove the LinearBlock, splicing its body back into the surrounding block\n+            auto body = n->blocks().at(0);\n+            for(auto input : n->inputs()){\n+              // we should never get into a situation when specializing a LinearBlock\n+              // where we do not know if a value is defined since at the top level\n+              // a gradient graph is composed of Linear nodes and AutogradAdds\n+              // and LinearNodes only appear in these graphs\n+              JIT_ASSERT(state[input] != State::MaybeDefined);\n+            }\n+            // hoist the nodes in the LinearBlock body to be before the linear block\n+            for(auto it = body->nodes().begin(); it != body->nodes().end();) {\n+              auto block_node = *it++;\n+              block_node->moveBefore(n);\n+            }\n+\n+            for(size_t i = 0; i < n->outputs().size(); ++i)\n+              n->outputs().at(i)->replaceAllUsesWith(body->outputs().at(i));\n+\n+            it.destroyCurrent();\n+          }\n+        } break;\n+        case prim::AutogradAdd: {\n+          auto a = n->input(0);\n+          auto b = n->input(1);\n+          // if one is undefined, we can just drop the add\n+          if(state[a] == State::Undefined) {\n+            // Undef + b == b\n+            n->output()->replaceAllUsesWith(b);\n+            it.destroyCurrent();\n+          } else if(state[b] == State::Undefined) {\n+            // a + Undef == a\n+            n->output()->replaceAllUsesWith(a);\n+            it.destroyCurrent();\n+          } else if(state[a] == State::Defined && state[b] == State::Defined) {\n+            // when both are defined, we can use a normal, optimizable add instruction\n+            WithInsertPoint guard(n);\n+            Value* new_add = toVar(a) + toVar(b);\n+            state[new_add] = State::Defined;\n+            n->output()->replaceAllUsesWith(new_add);\n+            it.destroyCurrent();\n+          } else {\n+            // otherwise we have conditionally-defined things, and we need\n+            // to actually run an AutogradAdd which will guard for undefs\n+            // so we leave the op as is\n+            state[n->output()] = State::MaybeDefined;\n+          }\n+        } break;\n+        case prim::Undefined: {\n+          state[n->output()] = State::Undefined;\n+        } break;\n+        default:\n+          for(auto o : n->outputs()) {\n+            state[o] = State::MaybeDefined;\n+          }", "path": "torch/csrc/jit/graph_executor.cpp", "position": null, "original_position": 136, "commit_id": "e2b3828276930eaf208705eb3d91047bc109eca1", "original_commit_id": "3a4b65f33f2eccef207875e60b0f7e22646bbdb7", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Assuming the argument in line 449 is correct, do we really need this default case? Maybe it's better to make it fail, so we don't get unexpected regressions in the future?", "created_at": "2018-06-21T17:33:54Z", "updated_at": "2018-11-23T15:46:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/8641#discussion_r197217674", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8641", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197217674"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8641#discussion_r197217674"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8641"}}, "body_html": "<p>Assuming the argument in line 449 is correct, do we really need this default case? Maybe it's better to make it fail, so we don't get unexpected regressions in the future?</p>", "body_text": "Assuming the argument in line 449 is correct, do we really need this default case? Maybe it's better to make it fail, so we don't get unexpected regressions in the future?"}