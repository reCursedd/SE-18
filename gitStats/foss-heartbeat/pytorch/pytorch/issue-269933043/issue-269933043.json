{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3390", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3390/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3390/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3390/events", "html_url": "https://github.com/pytorch/pytorch/issues/3390", "id": 269933043, "node_id": "MDU6SXNzdWUyNjk5MzMwNDM=", "number": 3390, "title": "High CPU use by clock_gettime syscall", "user": {"login": "shenberg", "id": 653972, "node_id": "MDQ6VXNlcjY1Mzk3Mg==", "avatar_url": "https://avatars3.githubusercontent.com/u/653972?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shenberg", "html_url": "https://github.com/shenberg", "followers_url": "https://api.github.com/users/shenberg/followers", "following_url": "https://api.github.com/users/shenberg/following{/other_user}", "gists_url": "https://api.github.com/users/shenberg/gists{/gist_id}", "starred_url": "https://api.github.com/users/shenberg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shenberg/subscriptions", "organizations_url": "https://api.github.com/users/shenberg/orgs", "repos_url": "https://api.github.com/users/shenberg/repos", "events_url": "https://api.github.com/users/shenberg/events{/privacy}", "received_events_url": "https://api.github.com/users/shenberg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-31T12:01:21Z", "updated_at": "2018-07-10T17:36:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I noticed, while training a network via <a href=\"https://github.com/abhiskk/fast-neural-style\">https://github.com/abhiskk/fast-neural-style</a>, that CPU use is consistently high in kernel time that is not IO wait. I suspected this was caused by excessive syscalls or context switches.</p>\n<p>My platform is Ubuntu 16.04.2, kernel 4.10, CUDA 8, cudnn 6 with a 1080 Ti</p>\n<p>To reproduce, I run <code>python3 neural_style/neural_style.py train --dataset /path/to/COCO2014 --vgg-model-dir vgg --batch-size 4 --save-model-dir saved-models --cuda 1</code></p>\n<p><code>htop</code> will show high kernel time use (in detailed mode, so it isn't confused with IO waits)</p>\n<p><code>strace -f -p &lt;PID&gt;</code> shows that the calls are for <code>CLOCK_MONOTONIC_RAW</code>, which is indeed unsupported in vDSO and falls back to a syscall (see <a href=\"http://elixir.free-electrons.com/linux/v4.10.17/source/arch/x86/entry/vdso/vclock_gettime.c#L245\" rel=\"nofollow\">http://elixir.free-electrons.com/linux/v4.10.17/source/arch/x86/entry/vdso/vclock_gettime.c#L245</a>). <code>argdist</code> for low overhead tracing shows ~2.5m calls per second (<code>sudo ./argdist -p &lt;PID&gt; -C 'p::sys_clock_gettime(clockid_t clk_id, struct timespec *tp):int'</code>)</p>\n<p>I started manually running parts of the code in the interpreter while sampling the amount of calls happening via <code>argdist</code> on the python interpreter, and the culprits are <code>.cuda()</code> and <code>.cpu()</code> calls on tensors, which trigger many calls to <code>clock_gettime()</code> each (amounts are inconsistent and range between hundreds and tens of thousands). I haven't gone down further, but it seems to me that if it's at all possible to replace <code>CLOCK_MONOTONIC_RAW</code> with <code>CLOCK_MONOTONIC</code>, it will give plenty of performance for such a minor change. Not sure if it's pytorch or CUDA that actually contains said code, though.</p>", "body_text": "I noticed, while training a network via https://github.com/abhiskk/fast-neural-style, that CPU use is consistently high in kernel time that is not IO wait. I suspected this was caused by excessive syscalls or context switches.\nMy platform is Ubuntu 16.04.2, kernel 4.10, CUDA 8, cudnn 6 with a 1080 Ti\nTo reproduce, I run python3 neural_style/neural_style.py train --dataset /path/to/COCO2014 --vgg-model-dir vgg --batch-size 4 --save-model-dir saved-models --cuda 1\nhtop will show high kernel time use (in detailed mode, so it isn't confused with IO waits)\nstrace -f -p <PID> shows that the calls are for CLOCK_MONOTONIC_RAW, which is indeed unsupported in vDSO and falls back to a syscall (see http://elixir.free-electrons.com/linux/v4.10.17/source/arch/x86/entry/vdso/vclock_gettime.c#L245). argdist for low overhead tracing shows ~2.5m calls per second (sudo ./argdist -p <PID> -C 'p::sys_clock_gettime(clockid_t clk_id, struct timespec *tp):int')\nI started manually running parts of the code in the interpreter while sampling the amount of calls happening via argdist on the python interpreter, and the culprits are .cuda() and .cpu() calls on tensors, which trigger many calls to clock_gettime() each (amounts are inconsistent and range between hundreds and tens of thousands). I haven't gone down further, but it seems to me that if it's at all possible to replace CLOCK_MONOTONIC_RAW with CLOCK_MONOTONIC, it will give plenty of performance for such a minor change. Not sure if it's pytorch or CUDA that actually contains said code, though.", "body": "I noticed, while training a network via https://github.com/abhiskk/fast-neural-style, that CPU use is consistently high in kernel time that is not IO wait. I suspected this was caused by excessive syscalls or context switches.\r\n\r\nMy platform is Ubuntu 16.04.2, kernel 4.10, CUDA 8, cudnn 6 with a 1080 Ti\r\n\r\nTo reproduce, I run `python3 neural_style/neural_style.py train --dataset /path/to/COCO2014 --vgg-model-dir vgg --batch-size 4 --save-model-dir saved-models --cuda 1`\r\n\r\n`htop` will show high kernel time use (in detailed mode, so it isn't confused with IO waits)\r\n\r\n`strace -f -p <PID>` shows that the calls are for `CLOCK_MONOTONIC_RAW`, which is indeed unsupported in vDSO and falls back to a syscall (see http://elixir.free-electrons.com/linux/v4.10.17/source/arch/x86/entry/vdso/vclock_gettime.c#L245). `argdist` for low overhead tracing shows ~2.5m calls per second (`sudo ./argdist -p <PID> -C 'p::sys_clock_gettime(clockid_t clk_id, struct timespec *tp):int'`)\r\n\r\nI started manually running parts of the code in the interpreter while sampling the amount of calls happening via `argdist` on the python interpreter, and the culprits are `.cuda()` and `.cpu()` calls on tensors, which trigger many calls to `clock_gettime()` each (amounts are inconsistent and range between hundreds and tens of thousands). I haven't gone down further, but it seems to me that if it's at all possible to replace `CLOCK_MONOTONIC_RAW` with `CLOCK_MONOTONIC`, it will give plenty of performance for such a minor change. Not sure if it's pytorch or CUDA that actually contains said code, though."}