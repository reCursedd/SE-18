{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435403964", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435403964", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435403964, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTQwMzk2NA==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-02T14:47:00Z", "updated_at": "2018-11-02T14:47:41Z", "author_association": "COLLABORATOR", "body_html": "<p>From my point of view, this is the expected behavior and the correct answer.<br>\nThis is the same discussion as <a href=\"https://github.com/pytorch/pytorch/issues/2421#issuecomment-354580257\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2421/hovercard\">this one</a>. The norm accept a subgradient of 0 at 0, but the decomposed version that take the square root of the sum of squares will return nan.</p>\n<p>Here [0, 1] is the correct gradient for the indexing op: since the first input is not used, it gets 0 and the second gets 1.<br>\nThen in the composition with sqrt will give you nan for the first entry.</p>\n<p>If you had a combined <code>Function</code> that was doing both ops (similar in spirit to the norm function above), then yes you could return [0,1]. But for the not-merged function, this is the correct result.</p>\n<p>In your case, I guess you should always do the selection first? What is the point on doing ops to just ignore them afterwards?</p>", "body_text": "From my point of view, this is the expected behavior and the correct answer.\nThis is the same discussion as this one. The norm accept a subgradient of 0 at 0, but the decomposed version that take the square root of the sum of squares will return nan.\nHere [0, 1] is the correct gradient for the indexing op: since the first input is not used, it gets 0 and the second gets 1.\nThen in the composition with sqrt will give you nan for the first entry.\nIf you had a combined Function that was doing both ops (similar in spirit to the norm function above), then yes you could return [0,1]. But for the not-merged function, this is the correct result.\nIn your case, I guess you should always do the selection first? What is the point on doing ops to just ignore them afterwards?", "body": "From my point of view, this is the expected behavior and the correct answer.\r\nThis is the same discussion as [this one](https://github.com/pytorch/pytorch/issues/2421#issuecomment-354580257). The norm accept a subgradient of 0 at 0, but the decomposed version that take the square root of the sum of squares will return nan.\r\n\r\nHere [0, 1] is the correct gradient for the indexing op: since the first input is not used, it gets 0 and the second gets 1.\r\nThen in the composition with sqrt will give you nan for the first entry.\r\n\r\nIf you had a combined `Function` that was doing both ops (similar in spirit to the norm function above), then yes you could return [0,1]. But for the not-merged function, this is the correct result.\r\n\r\n\r\nIn your case, I guess you should always do the selection first? What is the point on doing ops to just ignore them afterwards?"}