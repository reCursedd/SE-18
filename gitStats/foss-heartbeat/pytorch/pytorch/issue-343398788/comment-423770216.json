{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423770216", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-423770216", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 423770216, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzc3MDIxNg==", "user": {"login": "jwdink", "id": 6982295, "node_id": "MDQ6VXNlcjY5ODIyOTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/6982295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwdink", "html_url": "https://github.com/jwdink", "followers_url": "https://api.github.com/users/jwdink/followers", "following_url": "https://api.github.com/users/jwdink/following{/other_user}", "gists_url": "https://api.github.com/users/jwdink/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwdink/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwdink/subscriptions", "organizations_url": "https://api.github.com/users/jwdink/orgs", "repos_url": "https://api.github.com/users/jwdink/repos", "events_url": "https://api.github.com/users/jwdink/events{/privacy}", "received_events_url": "https://api.github.com/users/jwdink/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T20:14:11Z", "updated_at": "2018-09-22T20:14:11Z", "author_association": "NONE", "body_html": "<p>Just wanted to second this issue. For my use case (kalman-filters) the idea is that some of our data may be missing and that our kalman-filter can naturally handle this. When computing the loss, it wasn't obvious that these two statements would have such different effects:</p>\n<pre><code># leads to unexpected nan gradients:\nloss = pointwise_lossfun(pred, actual)\nloss[~torch.isnan(loss)].mean().backward()\n\n# works:\nloss = pointwise_lossfun(pred[~torch.isnan(actual)], actual[~torch.isnan(actual)])\nloss.mean().backward()\n</code></pre>", "body_text": "Just wanted to second this issue. For my use case (kalman-filters) the idea is that some of our data may be missing and that our kalman-filter can naturally handle this. When computing the loss, it wasn't obvious that these two statements would have such different effects:\n# leads to unexpected nan gradients:\nloss = pointwise_lossfun(pred, actual)\nloss[~torch.isnan(loss)].mean().backward()\n\n# works:\nloss = pointwise_lossfun(pred[~torch.isnan(actual)], actual[~torch.isnan(actual)])\nloss.mean().backward()", "body": "Just wanted to second this issue. For my use case (kalman-filters) the idea is that some of our data may be missing and that our kalman-filter can naturally handle this. When computing the loss, it wasn't obvious that these two statements would have such different effects:\r\n\r\n```\r\n# leads to unexpected nan gradients:\r\nloss = pointwise_lossfun(pred, actual)\r\nloss[~torch.isnan(loss)].mean().backward()\r\n\r\n# works:\r\nloss = pointwise_lossfun(pred[~torch.isnan(actual)], actual[~torch.isnan(actual)])\r\nloss.mean().backward()\r\n```"}