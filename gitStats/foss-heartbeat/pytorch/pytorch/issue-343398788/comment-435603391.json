{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435603391", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435603391", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435603391, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTYwMzM5MQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-03T16:56:02Z", "updated_at": "2018-11-03T16:56:02Z", "author_association": "COLLABORATOR", "body_html": "<p>Your <code>loss</code> is 1 so <code>mses.grad</code> is <code>tensor([0.2500, 0.0000, 0.2500, 0.2500, 0.2500])</code><br>\nNow the fomula for the Mean Square Error gradient is if you write <code>o = mse(x, y)</code><br>\n<code>do/dx = 2*(x-y)</code>. As you can see, if y is <code>nan</code> for an entry, then <code>pred.grad</code> will be nan for that entry. (you can see this in your hook for <code>pred</code>).<br>\nThen your model is doing: <code>o = model(p)</code> where you repeat <code>p</code> with the expand_as. So <code>do/dp = 1</code> (note that 1 is a vector here). and so <code>dloss/dp = dloss/do * do/dp = pred.grad * 1 = sum(pred.grad)</code>. So if any element of <code>pred.grad</code> is nan, the gradient will be nan, because <code>nan + number = nan</code>.</p>", "body_text": "Your loss is 1 so mses.grad is tensor([0.2500, 0.0000, 0.2500, 0.2500, 0.2500])\nNow the fomula for the Mean Square Error gradient is if you write o = mse(x, y)\ndo/dx = 2*(x-y). As you can see, if y is nan for an entry, then pred.grad will be nan for that entry. (you can see this in your hook for pred).\nThen your model is doing: o = model(p) where you repeat p with the expand_as. So do/dp = 1 (note that 1 is a vector here). and so dloss/dp = dloss/do * do/dp = pred.grad * 1 = sum(pred.grad). So if any element of pred.grad is nan, the gradient will be nan, because nan + number = nan.", "body": "Your `loss` is 1 so `mses.grad` is `tensor([0.2500, 0.0000, 0.2500, 0.2500, 0.2500])`\r\nNow the fomula for the Mean Square Error gradient is if you write `o = mse(x, y)`\r\n`do/dx = 2*(x-y)`. As you can see, if y is `nan` for an entry, then `pred.grad` will be nan for that entry. (you can see this in your hook for `pred`).\r\nThen your model is doing: `o = model(p)` where you repeat `p` with the expand_as. So `do/dp = 1` (note that 1 is a vector here). and so `dloss/dp = dloss/do * do/dp = pred.grad * 1 = sum(pred.grad)`. So if any element of `pred.grad` is nan, the gradient will be nan, because `nan + number = nan`."}