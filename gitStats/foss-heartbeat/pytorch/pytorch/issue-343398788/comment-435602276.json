{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435602276", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435602276", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435602276, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTYwMjI3Ng==", "user": {"login": "jwdink", "id": 6982295, "node_id": "MDQ6VXNlcjY5ODIyOTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/6982295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwdink", "html_url": "https://github.com/jwdink", "followers_url": "https://api.github.com/users/jwdink/followers", "following_url": "https://api.github.com/users/jwdink/following{/other_user}", "gists_url": "https://api.github.com/users/jwdink/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwdink/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwdink/subscriptions", "organizations_url": "https://api.github.com/users/jwdink/orgs", "repos_url": "https://api.github.com/users/jwdink/repos", "events_url": "https://api.github.com/users/jwdink/events{/privacy}", "received_events_url": "https://api.github.com/users/jwdink/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-03T16:41:48Z", "updated_at": "2018-11-03T16:42:38Z", "author_association": "NONE", "body_html": "<p>Here's a very simple example to make this more concrete:</p>\n<pre><code>import torch\nfrom torch import nn\nfrom numpy import nan\n\n# simple model for example:\nclass InterceptModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intercept = nn.Parameter(torch.tensor(1.0))\n        \n    def forward(self, x):\n        return self.intercept.expand_as(x)\n\n# loss fun:\ndef pointwise_mse(pred, actual):\n    return torch.pow(pred - actual, 2)\n\n# the target, has some missing values:\nactual = torch.randn(5) + 10.\nactual[1] = nan\n# actual.register_hook(lambda x: print(('actual',x))) \n# RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n\n# set up my model, generate predictions:\nmy_model = InterceptModel()\npred = my_model(actual)\npred.register_hook(lambda x: print(('pred',x)))\n\n# this is an elementwise function\nmses = pointwise_mse(pred, actual)\n\n# now i'm excluding the invalid gradients:\nloss = mses[actual==actual].mean()\nloss.register_hook(lambda x: print(('loss',x)))\n\n# why didn't excluding the invalid gradients work?\nloss.backward()\nmy_model.intercept.grad # returns `tensor(nan)`\n</code></pre>\n<p>I'm a little unclear what I'm looking for here.</p>\n<p>Again, the intuition is still that I've excluded the invalid gradients when constructing the <code>loss</code> scalar, so calling backward on this scalar shouldn't involve these gradients. It's not clear to my why these nans should propagate, in that case.</p>\n<p>Apologies for my confusion.</p>", "body_text": "Here's a very simple example to make this more concrete:\nimport torch\nfrom torch import nn\nfrom numpy import nan\n\n# simple model for example:\nclass InterceptModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intercept = nn.Parameter(torch.tensor(1.0))\n        \n    def forward(self, x):\n        return self.intercept.expand_as(x)\n\n# loss fun:\ndef pointwise_mse(pred, actual):\n    return torch.pow(pred - actual, 2)\n\n# the target, has some missing values:\nactual = torch.randn(5) + 10.\nactual[1] = nan\n# actual.register_hook(lambda x: print(('actual',x))) \n# RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n\n# set up my model, generate predictions:\nmy_model = InterceptModel()\npred = my_model(actual)\npred.register_hook(lambda x: print(('pred',x)))\n\n# this is an elementwise function\nmses = pointwise_mse(pred, actual)\n\n# now i'm excluding the invalid gradients:\nloss = mses[actual==actual].mean()\nloss.register_hook(lambda x: print(('loss',x)))\n\n# why didn't excluding the invalid gradients work?\nloss.backward()\nmy_model.intercept.grad # returns `tensor(nan)`\n\nI'm a little unclear what I'm looking for here.\nAgain, the intuition is still that I've excluded the invalid gradients when constructing the loss scalar, so calling backward on this scalar shouldn't involve these gradients. It's not clear to my why these nans should propagate, in that case.\nApologies for my confusion.", "body": "Here's a very simple example to make this more concrete:\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom numpy import nan\r\n\r\n# simple model for example:\r\nclass InterceptModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.intercept = nn.Parameter(torch.tensor(1.0))\r\n        \r\n    def forward(self, x):\r\n        return self.intercept.expand_as(x)\r\n\r\n# loss fun:\r\ndef pointwise_mse(pred, actual):\r\n    return torch.pow(pred - actual, 2)\r\n\r\n# the target, has some missing values:\r\nactual = torch.randn(5) + 10.\r\nactual[1] = nan\r\n# actual.register_hook(lambda x: print(('actual',x))) \r\n# RuntimeError: cannot register a hook on a tensor that doesn't require gradient\r\n\r\n# set up my model, generate predictions:\r\nmy_model = InterceptModel()\r\npred = my_model(actual)\r\npred.register_hook(lambda x: print(('pred',x)))\r\n\r\n# this is an elementwise function\r\nmses = pointwise_mse(pred, actual)\r\n\r\n# now i'm excluding the invalid gradients:\r\nloss = mses[actual==actual].mean()\r\nloss.register_hook(lambda x: print(('loss',x)))\r\n\r\n# why didn't excluding the invalid gradients work?\r\nloss.backward()\r\nmy_model.intercept.grad # returns `tensor(nan)`\r\n```\r\n\r\nI'm a little unclear what I'm looking for here. \r\n\r\nAgain, the intuition is still that I've excluded the invalid gradients when constructing the `loss` scalar, so calling backward on this scalar shouldn't involve these gradients. It's not clear to my why these nans should propagate, in that case.\r\n\r\nApologies for my confusion."}