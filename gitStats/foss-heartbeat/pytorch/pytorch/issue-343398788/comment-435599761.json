{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435599761", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435599761", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435599761, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTU5OTc2MQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-03T16:12:26Z", "updated_at": "2018-11-03T16:12:26Z", "author_association": "COLLABORATOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6982295\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jwdink\">@jwdink</a></p>\n<blockquote>\n<p>If I later exclude the invalid elements before aggregating, then I have excluded those invalid gradients.</p>\n</blockquote>\n<p>The \"exclusion\" operation, when backpropagating through it will give 0 gradient for the elements that were excluded. If you look at the function f: (x,y) -&gt; y it's gradient df/dx = 0.</p>\n<p>Now the question is how <code>pointwise_lossfun</code> will handle the backprop phase for an element that was nan initially and has a 0 grad_output.<br>\nI expect that you get nan's in your case because the backward of <code>pointwise_lossfun</code> will involve the original output (that is nan) and most certainly will give a grad_input that is nan for them.<br>\nThis nan will then propagate in your network.</p>\n<p>To check that, you can print the gradients returned by you loss function by registering hooks on <code>pred</code> and <code>actual</code></p>", "body_text": "@jwdink\n\nIf I later exclude the invalid elements before aggregating, then I have excluded those invalid gradients.\n\nThe \"exclusion\" operation, when backpropagating through it will give 0 gradient for the elements that were excluded. If you look at the function f: (x,y) -> y it's gradient df/dx = 0.\nNow the question is how pointwise_lossfun will handle the backprop phase for an element that was nan initially and has a 0 grad_output.\nI expect that you get nan's in your case because the backward of pointwise_lossfun will involve the original output (that is nan) and most certainly will give a grad_input that is nan for them.\nThis nan will then propagate in your network.\nTo check that, you can print the gradients returned by you loss function by registering hooks on pred and actual", "body": "@jwdink \r\n> If I later exclude the invalid elements before aggregating, then I have excluded those invalid gradients.\r\n\r\nThe \"exclusion\" operation, when backpropagating through it will give 0 gradient for the elements that were excluded. If you look at the function f: (x,y) -> y it's gradient df/dx = 0.\r\n\r\nNow the question is how `pointwise_lossfun` will handle the backprop phase for an element that was nan initially and has a 0 grad_output.\r\nI expect that you get nan's in your case because the backward of `pointwise_lossfun` will involve the original output (that is nan) and most certainly will give a grad_input that is nan for them.\r\nThis nan will then propagate in your network.\r\n\r\nTo check that, you can print the gradients returned by you loss function by registering hooks on `pred` and `actual`"}