{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435778663", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435778663", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435778663, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTc3ODY2Mw==", "user": {"login": "c-hofer", "id": 28672615, "node_id": "MDQ6VXNlcjI4NjcyNjE1", "avatar_url": "https://avatars2.githubusercontent.com/u/28672615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c-hofer", "html_url": "https://github.com/c-hofer", "followers_url": "https://api.github.com/users/c-hofer/followers", "following_url": "https://api.github.com/users/c-hofer/following{/other_user}", "gists_url": "https://api.github.com/users/c-hofer/gists{/gist_id}", "starred_url": "https://api.github.com/users/c-hofer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c-hofer/subscriptions", "organizations_url": "https://api.github.com/users/c-hofer/orgs", "repos_url": "https://api.github.com/users/c-hofer/repos", "events_url": "https://api.github.com/users/c-hofer/events{/privacy}", "received_events_url": "https://api.github.com/users/c-hofer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-05T07:30:27Z", "updated_at": "2018-11-05T07:30:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a><br>\nFirst, thanks again for your time.<br>\nIndeed I used <code>nan</code> instead of <code>inf</code> as I was thinking of values &lt; 0 too.  sry for the sloppiness ;).</p>\n<p>The proposed solution does not solve the proposed problem (unless I've misunderstood something completely). To be clear, an example</p>\n<pre><code>x_0 = torch.tensor([float(i-1) for i in range(5)], requires_grad=True)\nx_1 = x_0.sqrt() # this is the output of the client\n\nx_input = x_1 # assumption: we have only access to x_1 and x_0 is hidden from \n                    #  the perspective of my_excluding_function\nx_output = my_excluding_function(x_input)\nx_output.sum().backward()\n\n# expected: x_0.grad contains no nan \n</code></pre>\n<p>In  my understanding the proposed pattern solves the issue if we have detail information about<br>\n<code>x_input</code>, i.e. <code>x_0</code> and twists <code>grad</code> at the right position in the recursion such that the chain rule does not hurt you.<br>\nHowever what is needed is to modify the very definition of <code>multiplication</code> at the current top level<br>\nof the recursion in a way such that <code>nan * 0 = inf * 0 = 0</code>. In other words from the point of the \"exclusion\" operation upwards we do not care about the ancestors in the differentiation graph.<br>\nMathematically this reflects the idea that we always can add an unknown to the point wise definition of a function and the related partial differentation operation will yield 0 if it does not impact the pointwise value.</p>\n<pre><code>f(x_1, x_2) = x_1 ==&gt; d f / d x_2 = 0 \n</code></pre>\n<p>I totally understand that technically this may yield problems because mathematics usually does not care about its implementation ;).<br>\nAgain I want to point out that maybe this is not a big issue and not worth the time. I think i can work around the issue but it simply makes things less elegant as I'd like them to be ;) .</p>\n<p>Comment: If one thinks of batch-wise application the problem is maybe more obvious.  Let <code>xx_0 = torch.tensor([float(i+1) for i in range(5)], requires_grad=True)</code> be the \"second\" batch element. All gradients are  defined and <code>my_excluding_function</code> would not exclude anything. However, the gradients in the backward pass will be added and hence the <code>nan</code> in the first case, <code>x_0</code>  will the destroy the gradient. It wouldn't if the gradient wrt the excluded coordinates in <code>x_0</code> would be zero.</p>", "body_text": "@albanD\nFirst, thanks again for your time.\nIndeed I used nan instead of inf as I was thinking of values < 0 too.  sry for the sloppiness ;).\nThe proposed solution does not solve the proposed problem (unless I've misunderstood something completely). To be clear, an example\nx_0 = torch.tensor([float(i-1) for i in range(5)], requires_grad=True)\nx_1 = x_0.sqrt() # this is the output of the client\n\nx_input = x_1 # assumption: we have only access to x_1 and x_0 is hidden from \n                    #  the perspective of my_excluding_function\nx_output = my_excluding_function(x_input)\nx_output.sum().backward()\n\n# expected: x_0.grad contains no nan \n\nIn  my understanding the proposed pattern solves the issue if we have detail information about\nx_input, i.e. x_0 and twists grad at the right position in the recursion such that the chain rule does not hurt you.\nHowever what is needed is to modify the very definition of multiplication at the current top level\nof the recursion in a way such that nan * 0 = inf * 0 = 0. In other words from the point of the \"exclusion\" operation upwards we do not care about the ancestors in the differentiation graph.\nMathematically this reflects the idea that we always can add an unknown to the point wise definition of a function and the related partial differentation operation will yield 0 if it does not impact the pointwise value.\nf(x_1, x_2) = x_1 ==> d f / d x_2 = 0 \n\nI totally understand that technically this may yield problems because mathematics usually does not care about its implementation ;).\nAgain I want to point out that maybe this is not a big issue and not worth the time. I think i can work around the issue but it simply makes things less elegant as I'd like them to be ;) .\nComment: If one thinks of batch-wise application the problem is maybe more obvious.  Let xx_0 = torch.tensor([float(i+1) for i in range(5)], requires_grad=True) be the \"second\" batch element. All gradients are  defined and my_excluding_function would not exclude anything. However, the gradients in the backward pass will be added and hence the nan in the first case, x_0  will the destroy the gradient. It wouldn't if the gradient wrt the excluded coordinates in x_0 would be zero.", "body": "@albanD \r\nFirst, thanks again for your time. \r\nIndeed I used `nan` instead of `inf` as I was thinking of values < 0 too.  sry for the sloppiness ;). \r\n\r\nThe proposed solution does not solve the proposed problem (unless I've misunderstood something completely). To be clear, an example\r\n```\r\nx_0 = torch.tensor([float(i-1) for i in range(5)], requires_grad=True)\r\nx_1 = x_0.sqrt() # this is the output of the client\r\n\r\nx_input = x_1 # assumption: we have only access to x_1 and x_0 is hidden from \r\n                    #  the perspective of my_excluding_function\r\nx_output = my_excluding_function(x_input)\r\nx_output.sum().backward()\r\n\r\n# expected: x_0.grad contains no nan \r\n```\r\n\r\nIn  my understanding the proposed pattern solves the issue if we have detail information about \r\n`x_input`, i.e. `x_0` and twists `grad` at the right position in the recursion such that the chain rule does not hurt you. \r\nHowever what is needed is to modify the very definition of `multiplication` at the current top level \r\nof the recursion in a way such that `nan * 0 = inf * 0 = 0`. In other words from the point of the \"exclusion\" operation upwards we do not care about the ancestors in the differentiation graph. \r\nMathematically this reflects the idea that we always can add an unknown to the point wise definition of a function and the related partial differentation operation will yield 0 if it does not impact the pointwise value. \r\n```\r\nf(x_1, x_2) = x_1 ==> d f / d x_2 = 0 \r\n```\r\nI totally understand that technically this may yield problems because mathematics usually does not care about its implementation ;).\r\nAgain I want to point out that maybe this is not a big issue and not worth the time. I think i can work around the issue but it simply makes things less elegant as I'd like them to be ;) . \r\n\r\nComment: If one thinks of batch-wise application the problem is maybe more obvious.  Let `xx_0 = torch.tensor([float(i+1) for i in range(5)], requires_grad=True)` be the \"second\" batch element. All gradients are  defined and `my_excluding_function` would not exclude anything. However, the gradients in the backward pass will be added and hence the `nan` in the first case, `x_0`  will the destroy the gradient. It wouldn't if the gradient wrt the excluded coordinates in `x_0` would be zero. \r\n\r\n\r\n"}