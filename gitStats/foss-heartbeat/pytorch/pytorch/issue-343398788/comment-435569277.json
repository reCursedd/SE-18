{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435569277", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435569277", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435569277, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTU2OTI3Nw==", "user": {"login": "c-hofer", "id": 28672615, "node_id": "MDQ6VXNlcjI4NjcyNjE1", "avatar_url": "https://avatars2.githubusercontent.com/u/28672615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c-hofer", "html_url": "https://github.com/c-hofer", "followers_url": "https://api.github.com/users/c-hofer/followers", "following_url": "https://api.github.com/users/c-hofer/following{/other_user}", "gists_url": "https://api.github.com/users/c-hofer/gists{/gist_id}", "starred_url": "https://api.github.com/users/c-hofer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c-hofer/subscriptions", "organizations_url": "https://api.github.com/users/c-hofer/orgs", "repos_url": "https://api.github.com/users/c-hofer/repos", "events_url": "https://api.github.com/users/c-hofer/events{/privacy}", "received_events_url": "https://api.github.com/users/c-hofer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-03T08:09:27Z", "updated_at": "2018-11-03T12:35:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a><br>\nI'm sorry that I may have stated the problem confusingly. My concern is NOT the gradient for <code>sqrt(0)</code>.<br>\nIts just the symptom of a deeper issue with PyTorch's interpretation of the differential operator. The following is a little lengthy and may seem not of practical value hence I will motivate it with an example ;)</p>\n<h3>example</h3>\n<pre><code>import torch\nimport torch.nn as nn\n\nclass ParametrizedFunction(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = nn.Parameter(torch.tensor(1.0))\n        \n    def forward(self, x):\n        return x - self.theta.expand_as(x)\n\n# that's what I want to do ... (case 1)\nf_theta = ParametrizedFunction()\nx = torch.tensor([float(i) for i in range(10)])\nx = f_theta(x) # x == tensor([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])\nx = x.sqrt()\nx = x[x &gt; 0]\nx = x.sum()\nx.backward()\n\nf_theta.theta.grad # tensor(nan.) -&gt; I did not expect that!\n\n# that's what I have to do to get what I want to do ... (case 2)\nf_theta   = ParametrizedFunction()\nx_initial = torch.tensor([float(i) for i in range(10)])\n\nx    = f_theta(x_initial)\nx    = x.sqrt()\nmask = (x &gt; 0)\n\nx = x_initial[mask]\nx = f_theta(x)  # this is redundant and may be expensive!\nx = x.sqrt()\nx = x.sum()\nx.backward()\n\nf_theta.theta.grad # tensor(-2.1857) -&gt; now I'm happy but not efficient ;)\n</code></pre>\n<p>If we exclude coordinates of <code> f_theta(x_initial)</code> which yield an undefined gradient, internally the gradient is nevertheless updated with the value <code>nan</code>.<br>\nMy concern is that the value should be <code>0</code> as the above example then would also work in the first case.</p>\n<h3>formal justification (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a>)</h3>\n<p>(this is less verbose as I would like it to be, but math in markdown without mathjax is a pain in the ... )</p>\n<p>First of all what is your argumentation that the correct gradient of the mapping  is <code>nan</code>? The differential operation of the mapping <code>f(x) = sqrt(x)</code> for the value <code>0</code> in the first (and only) coordinate is simply <em>not defined</em>. This does not mean its value is <code>nan</code>.<br>\nFrom my perspective PyTorch uses the  <em>the convention</em> that the symbol  <code>nan</code> is assigned if the differential operation is not defined.<br>\nAssigning symbols to define otherwise undefined values is common practice in mathematics. For example in measure theory its natural to have the symbol <code>inf</code> which algebraically integrates in the real numbers by setting <code>inf + x = inf + x = inf</code> ( <code>inf - inf</code> is  not defined). The motivation for this convention is to formally simplify things.<br>\nHowever, usually this assigning symbols to undefined values tries to ensure that those values \"behave naturally\" (see example from measure theory above).</p>\n<p>I suggest (the convention) to use <code>0</code> as the value for undefined differential operations instead of <code>nan</code>.<br>\nAt least after slicing operations, as <code>nan</code> yields somehow \"unnatural\" behaviour, which is nicely reflected in<br>\nthe code example I stated at the very top.<br>\nTo be precise, what is happening above is that we contradict the common convention that<br>\n<code>d f / dx = 0</code> if <code>f</code> does not depend on <code>x</code>.</p>\n<p>Assigning <code>0</code> to undefined differential operations would comply to this behaviour if, e.g., <code>f'(0)</code> is not defined but <code>f(0)</code> is defined.<br>\nAs we automatically sum the gradients in the <code>grad</code> property, <code>nan</code> is an somehow unfortunate choice as <code>x + nan = nan</code>. I totally understand that from an implementation perspective this may be convinient as, e.g., we get an integrated check if one of the involved differentation operations is not defined. However, it has the mentioned side-effects which are from my point of view unwanted.</p>\n<p>From this perspective I think this is an important issue and as PyTorch is converging to its 1.0 release one should maybe change the convention or at least document it with all its side-effects.<br>\nI'm a mathematician and at least for me this behavior was unexpected and did not feel natural.<br>\nHowever I am not deep into the PyTorch sources and do not now all the implications.</p>\n<h1>Conclusion</h1>\n<p>Are there any reasons why we should prefer <code>nan</code> over <code>0</code> for undefined differential operations, which<br>\nare stronger than the drawbacks (for example the redundant call in the code example)?</p>", "body_text": "@soumith, @albanD\nI'm sorry that I may have stated the problem confusingly. My concern is NOT the gradient for sqrt(0).\nIts just the symptom of a deeper issue with PyTorch's interpretation of the differential operator. The following is a little lengthy and may seem not of practical value hence I will motivate it with an example ;)\nexample\nimport torch\nimport torch.nn as nn\n\nclass ParametrizedFunction(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = nn.Parameter(torch.tensor(1.0))\n        \n    def forward(self, x):\n        return x - self.theta.expand_as(x)\n\n# that's what I want to do ... (case 1)\nf_theta = ParametrizedFunction()\nx = torch.tensor([float(i) for i in range(10)])\nx = f_theta(x) # x == tensor([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])\nx = x.sqrt()\nx = x[x > 0]\nx = x.sum()\nx.backward()\n\nf_theta.theta.grad # tensor(nan.) -> I did not expect that!\n\n# that's what I have to do to get what I want to do ... (case 2)\nf_theta   = ParametrizedFunction()\nx_initial = torch.tensor([float(i) for i in range(10)])\n\nx    = f_theta(x_initial)\nx    = x.sqrt()\nmask = (x > 0)\n\nx = x_initial[mask]\nx = f_theta(x)  # this is redundant and may be expensive!\nx = x.sqrt()\nx = x.sum()\nx.backward()\n\nf_theta.theta.grad # tensor(-2.1857) -> now I'm happy but not efficient ;)\n\nIf we exclude coordinates of  f_theta(x_initial) which yield an undefined gradient, internally the gradient is nevertheless updated with the value nan.\nMy concern is that the value should be 0 as the above example then would also work in the first case.\nformal justification (@albanD)\n(this is less verbose as I would like it to be, but math in markdown without mathjax is a pain in the ... )\nFirst of all what is your argumentation that the correct gradient of the mapping  is nan? The differential operation of the mapping f(x) = sqrt(x) for the value 0 in the first (and only) coordinate is simply not defined. This does not mean its value is nan.\nFrom my perspective PyTorch uses the  the convention that the symbol  nan is assigned if the differential operation is not defined.\nAssigning symbols to define otherwise undefined values is common practice in mathematics. For example in measure theory its natural to have the symbol inf which algebraically integrates in the real numbers by setting inf + x = inf + x = inf ( inf - inf is  not defined). The motivation for this convention is to formally simplify things.\nHowever, usually this assigning symbols to undefined values tries to ensure that those values \"behave naturally\" (see example from measure theory above).\nI suggest (the convention) to use 0 as the value for undefined differential operations instead of nan.\nAt least after slicing operations, as nan yields somehow \"unnatural\" behaviour, which is nicely reflected in\nthe code example I stated at the very top.\nTo be precise, what is happening above is that we contradict the common convention that\nd f / dx = 0 if f does not depend on x.\nAssigning 0 to undefined differential operations would comply to this behaviour if, e.g., f'(0) is not defined but f(0) is defined.\nAs we automatically sum the gradients in the grad property, nan is an somehow unfortunate choice as x + nan = nan. I totally understand that from an implementation perspective this may be convinient as, e.g., we get an integrated check if one of the involved differentation operations is not defined. However, it has the mentioned side-effects which are from my point of view unwanted.\nFrom this perspective I think this is an important issue and as PyTorch is converging to its 1.0 release one should maybe change the convention or at least document it with all its side-effects.\nI'm a mathematician and at least for me this behavior was unexpected and did not feel natural.\nHowever I am not deep into the PyTorch sources and do not now all the implications.\nConclusion\nAre there any reasons why we should prefer nan over 0 for undefined differential operations, which\nare stronger than the drawbacks (for example the redundant call in the code example)?", "body": "@soumith, @albanD \r\nI'm sorry that I may have stated the problem confusingly. My concern is NOT the gradient for `sqrt(0)`. \r\nIts just the symptom of a deeper issue with PyTorch's interpretation of the differential operator. The following is a little lengthy and may seem not of practical value hence I will motivate it with an example ;)   \r\n### example \r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass ParametrizedFunction(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.theta = nn.Parameter(torch.tensor(1.0))\r\n        \r\n    def forward(self, x):\r\n        return x - self.theta.expand_as(x)\r\n\r\n# that's what I want to do ... (case 1)\r\nf_theta = ParametrizedFunction()\r\nx = torch.tensor([float(i) for i in range(10)])\r\nx = f_theta(x) # x == tensor([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])\r\nx = x.sqrt()\r\nx = x[x > 0]\r\nx = x.sum()\r\nx.backward()\r\n\r\nf_theta.theta.grad # tensor(nan.) -> I did not expect that!\r\n\r\n# that's what I have to do to get what I want to do ... (case 2)\r\nf_theta   = ParametrizedFunction()\r\nx_initial = torch.tensor([float(i) for i in range(10)])\r\n\r\nx    = f_theta(x_initial)\r\nx    = x.sqrt()\r\nmask = (x > 0)\r\n\r\nx = x_initial[mask]\r\nx = f_theta(x)  # this is redundant and may be expensive!\r\nx = x.sqrt()\r\nx = x.sum()\r\nx.backward()\r\n\r\nf_theta.theta.grad # tensor(-2.1857) -> now I'm happy but not efficient ;)\r\n```\r\n\r\nIf we exclude coordinates of ` f_theta(x_initial)` which yield an undefined gradient, internally the gradient is nevertheless updated with the value `nan`. \r\nMy concern is that the value should be `0` as the above example then would also work in the first case. \r\n\r\n### formal justification (@albanD) \r\n(this is less verbose as I would like it to be, but math in markdown without mathjax is a pain in the ... ) \r\n\r\nFirst of all what is your argumentation that the correct gradient of the mapping  is `nan`? The differential operation of the mapping `f(x) = sqrt(x)` for the value `0` in the first (and only) coordinate is simply *not defined*. This does not mean its value is `nan`. \r\nFrom my perspective PyTorch uses the  *the convention* that the symbol  `nan` is assigned if the differential operation is not defined. \r\nAssigning symbols to define otherwise undefined values is common practice in mathematics. For example in measure theory its natural to have the symbol `inf` which algebraically integrates in the real numbers by setting `inf + x = inf + x = inf` ( `inf - inf` is  not defined). The motivation for this convention is to formally simplify things. \r\nHowever, usually this assigning symbols to undefined values tries to ensure that those values \"behave naturally\" (see example from measure theory above). \r\n\r\nI suggest (the convention) to use `0` as the value for undefined differential operations instead of `nan`. \r\nAt least after slicing operations, as `nan` yields somehow \"unnatural\" behaviour, which is nicely reflected in \r\nthe code example I stated at the very top.\r\nTo be precise, what is happening above is that we contradict the common convention that \r\n```d f / dx = 0``` if `f` does not depend on `x`. \r\n\r\nAssigning `0` to undefined differential operations would comply to this behaviour if, e.g., `f'(0)` is not defined but `f(0)` is defined. \r\nAs we automatically sum the gradients in the `grad` property, `nan` is an somehow unfortunate choice as `x + nan = nan`. I totally understand that from an implementation perspective this may be convinient as, e.g., we get an integrated check if one of the involved differentation operations is not defined. However, it has the mentioned side-effects which are from my point of view unwanted. \r\n\r\nFrom this perspective I think this is an important issue and as PyTorch is converging to its 1.0 release one should maybe change the convention or at least document it with all its side-effects. \r\nI'm a mathematician and at least for me this behavior was unexpected and did not feel natural. \r\nHowever I am not deep into the PyTorch sources and do not now all the implications. \r\n\r\n# Conclusion\r\n\r\nAre there any reasons why we should prefer `nan` over `0` for undefined differential operations, which \r\nare stronger than the drawbacks (for example the redundant call in the code example)? \r\n\r\n\r\n\r\n"}