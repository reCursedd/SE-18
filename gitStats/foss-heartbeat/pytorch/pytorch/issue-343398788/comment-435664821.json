{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435664821", "html_url": "https://github.com/pytorch/pytorch/issues/9688#issuecomment-435664821", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9688", "id": 435664821, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTY2NDgyMQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-04T12:17:51Z", "updated_at": "2018-11-04T12:17:51Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi,</p>\n<p>After some discussion with other people I think I have we found a simple answer to the problem: \"The chain rule only works for differentiable functions, and you're working at a point where sqrt is not differentiable. Hell insues !\".<br>\nWe will write all of that down explicitly in the doc to reduce such problems.</p>\n<p>In your case the problem is not that <code>nan * 0 = nan</code>, only <code>inf * 0 = nan</code>.<br>\nAnyway, the proper way to deal with \"I don't want the gradient that the chain rule gives\" is hooks.<br>\nIn your case something like this should fix any use case you encounter:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> I did not run this code, there might be typos in it, sorry</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Your model</span>\nx <span class=\"pl-k\">=</span> f_theta(x)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">my_excluding_functions</span>(<span class=\"pl-smi\">x_input</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute the function</span>\n    x <span class=\"pl-k\">=</span> x_input.sqrt()\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute the exclusion mask</span>\n    mask <span class=\"pl-k\">=</span> x<span class=\"pl-k\">&gt;</span><span class=\"pl-c1\">0</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Mask the output</span>\n    output <span class=\"pl-k\">=</span> x[mask]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make sure the backward pass won't return nan</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> We use hook to modify the gradient computed for</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_input and make sure it won't considered masked elements</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">mask_hook</span>(<span class=\"pl-smi\">grad</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Never modify the input given by a hook</span>\n        grad <span class=\"pl-k\">=</span> grad.clone()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the gradients (don't multiply) of the part we did not keep to 0</span>\n        grad[<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>mask] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">return</span> grad\n    x_input.register_hook(mask_hook)\n    \n    <span class=\"pl-k\">return</span> output\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the function that fix the gradients.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The foward will be the same as:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> x = x.sqrt()</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> x = x[x &gt; 0]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> But the backward will mask out gradients so that</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> whatever is returned by the first function, it will</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> be zeroed out.</span>\nx <span class=\"pl-k\">=</span> my_excluding_functions(x)</pre></div>\n<p>Note that if the function is not element wise and change the input size, you can compute an input mask and an output mask, one to mask out the output during the forward pass and one to mask out gradients of the input during the backward pass.</p>\n<p>Hope this helps you make what you want !</p>", "body_text": "Hi,\nAfter some discussion with other people I think I have we found a simple answer to the problem: \"The chain rule only works for differentiable functions, and you're working at a point where sqrt is not differentiable. Hell insues !\".\nWe will write all of that down explicitly in the doc to reduce such problems.\nIn your case the problem is not that nan * 0 = nan, only inf * 0 = nan.\nAnyway, the proper way to deal with \"I don't want the gradient that the chain rule gives\" is hooks.\nIn your case something like this should fix any use case you encounter:\n# I did not run this code, there might be typos in it, sorry\n# Your model\nx = f_theta(x)\n\ndef my_excluding_functions(x_input):\n    # Compute the function\n    x = x_input.sqrt()\n    # Compute the exclusion mask\n    mask = x>0\n    # Mask the output\n    output = x[mask]\n    # Make sure the backward pass won't return nan\n    # We use hook to modify the gradient computed for\n    # grad_input and make sure it won't considered masked elements\n    def mask_hook(grad):\n        # Never modify the input given by a hook\n        grad = grad.clone()\n        # Set the gradients (don't multiply) of the part we did not keep to 0\n        grad[1-mask] = 0\n        return grad\n    x_input.register_hook(mask_hook)\n    \n    return output\n\n# Use the function that fix the gradients.\n# The foward will be the same as:\n# x = x.sqrt()\n# x = x[x > 0]\n# But the backward will mask out gradients so that\n# whatever is returned by the first function, it will\n# be zeroed out.\nx = my_excluding_functions(x)\nNote that if the function is not element wise and change the input size, you can compute an input mask and an output mask, one to mask out the output during the forward pass and one to mask out gradients of the input during the backward pass.\nHope this helps you make what you want !", "body": "Hi,\r\n\r\nAfter some discussion with other people I think I have we found a simple answer to the problem: \"The chain rule only works for differentiable functions, and you're working at a point where sqrt is not differentiable. Hell insues !\".\r\nWe will write all of that down explicitly in the doc to reduce such problems.\r\n\r\nIn your case the problem is not that `nan * 0 = nan`, only `inf * 0 = nan`.\r\nAnyway, the proper way to deal with \"I don't want the gradient that the chain rule gives\" is hooks.\r\nIn your case something like this should fix any use case you encounter:\r\n```python\r\n# I did not run this code, there might be typos in it, sorry\r\n# Your model\r\nx = f_theta(x)\r\n\r\ndef my_excluding_functions(x_input):\r\n    # Compute the function\r\n    x = x_input.sqrt()\r\n    # Compute the exclusion mask\r\n    mask = x>0\r\n    # Mask the output\r\n    output = x[mask]\r\n    # Make sure the backward pass won't return nan\r\n    # We use hook to modify the gradient computed for\r\n    # grad_input and make sure it won't considered masked elements\r\n    def mask_hook(grad):\r\n        # Never modify the input given by a hook\r\n        grad = grad.clone()\r\n        # Set the gradients (don't multiply) of the part we did not keep to 0\r\n        grad[1-mask] = 0\r\n        return grad\r\n    x_input.register_hook(mask_hook)\r\n    \r\n    return output\r\n\r\n# Use the function that fix the gradients.\r\n# The foward will be the same as:\r\n# x = x.sqrt()\r\n# x = x[x > 0]\r\n# But the backward will mask out gradients so that\r\n# whatever is returned by the first function, it will\r\n# be zeroed out.\r\nx = my_excluding_functions(x)\r\n```\r\n\r\nNote that if the function is not element wise and change the input size, you can compute an input mask and an output mask, one to mask out the output during the forward pass and one to mask out gradients of the input during the backward pass.\r\n\r\nHope this helps you make what you want !"}