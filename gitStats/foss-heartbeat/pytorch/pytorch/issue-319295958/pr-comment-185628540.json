{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/185628540", "pull_request_review_id": 117056844, "id": 185628540, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NTYyODU0MA==", "diff_hunk": "@@ -21,10 +21,6 @@\n   return: self\n   cname: resize\n   cpu_half: True\n-  before_call:\n-    THPUtils_assert(arg_self->storage->flag & TH_STORAGE_RESIZABLE,\n-      \"calling resize_ on a tensor that has non-resizable storage. Clone it first \"\n-      \"or create a new tensor instead.\");", "path": "aten/src/ATen/Declarations.cwrap", "position": 7, "original_position": 7, "commit_id": "7e888b3d399b171546039f723d1060f7a8a466b2", "original_commit_id": "7e888b3d399b171546039f723d1060f7a8a466b2", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "Looks like it disappeared when we merged variable and tensor :).  We were left with the less-strict version, which just checks that the storage isn't resized (i.e. that the numpy array and our tensor share the same storage).  @colesbury and I thought that was okay, because it follows the usual tensor resize semantics, but I could see an argument the other way (and clearly whoever put in the assert (you?) thought that way).\r\n\r\nAs an additional note, even if we kept this assert through the tensor/variable merge, we wouldn't uniformly enforce the strict version, because we added a bunch of other functions that reshape things (i.e. as_strided) that don't preserve the check.  So if we wanted the strict check, we have more work to do.", "created_at": "2018-05-02T20:28:43Z", "updated_at": "2018-11-23T15:43:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/7147#discussion_r185628540", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/185628540"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7147#discussion_r185628540"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7147"}}, "body_html": "<p>Looks like it disappeared when we merged variable and tensor :).  We were left with the less-strict version, which just checks that the storage isn't resized (i.e. that the numpy array and our tensor share the same storage).  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> and I thought that was okay, because it follows the usual tensor resize semantics, but I could see an argument the other way (and clearly whoever put in the assert (you?) thought that way).</p>\n<p>As an additional note, even if we kept this assert through the tensor/variable merge, we wouldn't uniformly enforce the strict version, because we added a bunch of other functions that reshape things (i.e. as_strided) that don't preserve the check.  So if we wanted the strict check, we have more work to do.</p>", "body_text": "Looks like it disappeared when we merged variable and tensor :).  We were left with the less-strict version, which just checks that the storage isn't resized (i.e. that the numpy array and our tensor share the same storage).  @colesbury and I thought that was okay, because it follows the usual tensor resize semantics, but I could see an argument the other way (and clearly whoever put in the assert (you?) thought that way).\nAs an additional note, even if we kept this assert through the tensor/variable merge, we wouldn't uniformly enforce the strict version, because we added a bunch of other functions that reshape things (i.e. as_strided) that don't preserve the check.  So if we wanted the strict check, we have more work to do.", "in_reply_to_id": 185452456}