{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9978", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9978/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9978/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9978/events", "html_url": "https://github.com/pytorch/pytorch/issues/9978", "id": 345489289, "node_id": "MDU6SXNzdWUzNDU0ODkyODk=", "number": 9978, "title": "Doing multiple Matrix Multiply using for loop vs doing a single Matrix Multiply concatenating the inputs, gives two different results", "user": {"login": "DiableJambe", "id": 13801024, "node_id": "MDQ6VXNlcjEzODAxMDI0", "avatar_url": "https://avatars3.githubusercontent.com/u/13801024?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DiableJambe", "html_url": "https://github.com/DiableJambe", "followers_url": "https://api.github.com/users/DiableJambe/followers", "following_url": "https://api.github.com/users/DiableJambe/following{/other_user}", "gists_url": "https://api.github.com/users/DiableJambe/gists{/gist_id}", "starred_url": "https://api.github.com/users/DiableJambe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DiableJambe/subscriptions", "organizations_url": "https://api.github.com/users/DiableJambe/orgs", "repos_url": "https://api.github.com/users/DiableJambe/repos", "events_url": "https://api.github.com/users/DiableJambe/events{/privacy}", "received_events_url": "https://api.github.com/users/DiableJambe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-29T00:21:58Z", "updated_at": "2018-07-29T00:31:42Z", "closed_at": "2018-07-29T00:31:01Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When performing matrix multiply on a 3d tensor, in two different ways, I get two different gradients when performing backward on the output . Please see code example below for exact problem statement.</p>\n<p>Apologize if there is something wrong with my understanding, but kindly help me to the right resource for the correct understanding.</p>\n<h2>Code example</h2>\n<pre><code>import torch\n\nx = torch.autograd.Variable(torch.FloatTensor(4,4,4));\nx.data.normal_(0.0,0.1);\n\ny = torch.autograd.Variable(torch.FloatTensor(4,5), requires_grad=True);\ny.data.normal_(0.0,0.1);\n\nx2 = torch.autograd.Variable(x.data);\nz  = torch.autograd.Variable(y.data, requires_grad=True);\n\nw1 = [];\n\nfor x_ in torch.unbind(x, dim=0):\n    w1.append(torch.matmul(x, y));\n\nw1 = torch.stack(w1);\n\nw2 = torch.matmul(x2.view(16,4), z).view(4,4,5);\n\nprint(torch.sum((w2 - w1)**2));\n\nloss1 = torch.sum((w1 - 5)**2);\n\nloss2 = torch.sum((w2 - 5)**2);\n\nloss1.backward();\n\nloss2.backward();\n\nprint(y.grad);\nprint(z.grad);\n</code></pre>\n<h2>Output</h2>\n<pre><code>Variable containing:\n 0\n[torch.FloatTensor of size (1,)]\n\nVariable containing:\n-10.6638 -10.6886 -10.9352 -10.5696 -10.7285\n-19.2051 -19.1448 -19.1945 -19.3385 -19.2048\n -9.0791  -9.0517  -8.9411  -9.1330  -9.0585\n -4.8962  -4.7247  -4.6640  -5.0115  -4.8927\n[torch.FloatTensor of size (4,5)]\n\nVariable containing:\n-2.6659 -2.6722 -2.7338 -2.6424 -2.6821\n-4.8013 -4.7862 -4.7986 -4.8346 -4.8012\n-2.2698 -2.2629 -2.2353 -2.2833 -2.2646\n-1.2240 -1.1812 -1.1660 -1.2529 -1.2232\n[torch.FloatTensor of size (4,5)]\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0a0+65fb885<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.44</p>\n<p>OS: CentOS Linux 7 (Core)<br>\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)<br>\nCMake version: version 3.9.4</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 7.5.17<br>\nGPU models and configuration:<br>\nGPU 0: TITAN Xp<br>\nGPU 1: TITAN Xp</p>\n<p>Nvidia driver version: 390.25<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/lib64/libcudnn.so.5.1.5<br>\n/usr/local/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.0)<br>\n[pip] torch (0.4.0a0+65fb885)<br>\n[conda] magma-cuda80              2.2.0                h39f1f8d_1    pytorch<br>\n[conda] torch                     0.4.0a0+65fb885           </p>", "body_text": "Issue description\nWhen performing matrix multiply on a 3d tensor, in two different ways, I get two different gradients when performing backward on the output . Please see code example below for exact problem statement.\nApologize if there is something wrong with my understanding, but kindly help me to the right resource for the correct understanding.\nCode example\nimport torch\n\nx = torch.autograd.Variable(torch.FloatTensor(4,4,4));\nx.data.normal_(0.0,0.1);\n\ny = torch.autograd.Variable(torch.FloatTensor(4,5), requires_grad=True);\ny.data.normal_(0.0,0.1);\n\nx2 = torch.autograd.Variable(x.data);\nz  = torch.autograd.Variable(y.data, requires_grad=True);\n\nw1 = [];\n\nfor x_ in torch.unbind(x, dim=0):\n    w1.append(torch.matmul(x, y));\n\nw1 = torch.stack(w1);\n\nw2 = torch.matmul(x2.view(16,4), z).view(4,4,5);\n\nprint(torch.sum((w2 - w1)**2));\n\nloss1 = torch.sum((w1 - 5)**2);\n\nloss2 = torch.sum((w2 - 5)**2);\n\nloss1.backward();\n\nloss2.backward();\n\nprint(y.grad);\nprint(z.grad);\n\nOutput\nVariable containing:\n 0\n[torch.FloatTensor of size (1,)]\n\nVariable containing:\n-10.6638 -10.6886 -10.9352 -10.5696 -10.7285\n-19.2051 -19.1448 -19.1945 -19.3385 -19.2048\n -9.0791  -9.0517  -8.9411  -9.1330  -9.0585\n -4.8962  -4.7247  -4.6640  -5.0115  -4.8927\n[torch.FloatTensor of size (4,5)]\n\nVariable containing:\n-2.6659 -2.6722 -2.7338 -2.6424 -2.6821\n-4.8013 -4.7862 -4.7986 -4.8346 -4.8012\n-2.2698 -2.2629 -2.2353 -2.2833 -2.2646\n-1.2240 -1.1812 -1.1660 -1.2529 -1.2232\n[torch.FloatTensor of size (4,5)]\n\nSystem Info\nPyTorch version: 0.4.0a0+65fb885\nIs debug build: No\nCUDA used to build PyTorch: 8.0.44\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: version 3.9.4\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 7.5.17\nGPU models and configuration:\nGPU 0: TITAN Xp\nGPU 1: TITAN Xp\nNvidia driver version: 390.25\ncuDNN version: Probably one of the following:\n/usr/local/lib64/libcudnn.so.5.1.5\n/usr/local/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.14.0)\n[pip] torch (0.4.0a0+65fb885)\n[conda] magma-cuda80              2.2.0                h39f1f8d_1    pytorch\n[conda] torch                     0.4.0a0+65fb885", "body": "## Issue description\r\n\r\nWhen performing matrix multiply on a 3d tensor, in two different ways, I get two different gradients when performing backward on the output . Please see code example below for exact problem statement.\r\n\r\nApologize if there is something wrong with my understanding, but kindly help me to the right resource for the correct understanding.\r\n\r\n## Code example\r\n```\r\nimport torch\r\n\r\nx = torch.autograd.Variable(torch.FloatTensor(4,4,4));\r\nx.data.normal_(0.0,0.1);\r\n\r\ny = torch.autograd.Variable(torch.FloatTensor(4,5), requires_grad=True);\r\ny.data.normal_(0.0,0.1);\r\n\r\nx2 = torch.autograd.Variable(x.data);\r\nz  = torch.autograd.Variable(y.data, requires_grad=True);\r\n\r\nw1 = [];\r\n\r\nfor x_ in torch.unbind(x, dim=0):\r\n    w1.append(torch.matmul(x, y));\r\n\r\nw1 = torch.stack(w1);\r\n\r\nw2 = torch.matmul(x2.view(16,4), z).view(4,4,5);\r\n\r\nprint(torch.sum((w2 - w1)**2));\r\n\r\nloss1 = torch.sum((w1 - 5)**2);\r\n\r\nloss2 = torch.sum((w2 - 5)**2);\r\n\r\nloss1.backward();\r\n\r\nloss2.backward();\r\n\r\nprint(y.grad);\r\nprint(z.grad);\r\n```\r\n## Output\r\n```\r\nVariable containing:\r\n 0\r\n[torch.FloatTensor of size (1,)]\r\n\r\nVariable containing:\r\n-10.6638 -10.6886 -10.9352 -10.5696 -10.7285\r\n-19.2051 -19.1448 -19.1945 -19.3385 -19.2048\r\n -9.0791  -9.0517  -8.9411  -9.1330  -9.0585\r\n -4.8962  -4.7247  -4.6640  -5.0115  -4.8927\r\n[torch.FloatTensor of size (4,5)]\r\n\r\nVariable containing:\r\n-2.6659 -2.6722 -2.7338 -2.6424 -2.6821\r\n-4.8013 -4.7862 -4.7986 -4.8346 -4.8012\r\n-2.2698 -2.2629 -2.2353 -2.2833 -2.2646\r\n-1.2240 -1.1812 -1.1660 -1.2529 -1.2232\r\n[torch.FloatTensor of size (4,5)]\r\n```\r\n## System Info\r\n\r\nPyTorch version: 0.4.0a0+65fb885\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.44\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 7.5.17\r\nGPU models and configuration:\r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\n\r\nNvidia driver version: 390.25\r\ncuDNN version: Probably one of the following:\r\n/usr/local/lib64/libcudnn.so.5.1.5\r\n/usr/local/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.0)\r\n[pip] torch (0.4.0a0+65fb885)\r\n[conda] magma-cuda80              2.2.0                h39f1f8d_1    pytorch\r\n[conda] torch                     0.4.0a0+65fb885           <pip>"}