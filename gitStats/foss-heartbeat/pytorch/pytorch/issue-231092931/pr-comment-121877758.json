{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121877758", "pull_request_review_id": 43940161, "id": 121877758, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMTg3Nzc1OA==", "diff_hunk": "@@ -33,10 +33,12 @@ auto AccumulateGrad::acc_inplace(std::shared_ptr<Variable>& grad,\n \n auto AccumulateGrad::apply(const variable_list& grads) -> variable_list {\n   // XXX: this method is not thread-safe!\n-  check_input_variables(\"AccumulateGrad\", grads, 1);\n-  auto var = variable.lock();\n+  check_input_variables(\"AccumulateGrad\", grads, 1, 0);\n   auto new_grad = grads[0];\n \n+  if (!new_grad) return {};", "path": "torch/csrc/autograd/functions/accumulate_grad.cpp", "position": 9, "original_position": 9, "commit_id": "e3cc7c83dc86803042b934d3c1863a7af1640a8d", "original_commit_id": "c2c9e3dc9d63ca19b940adca1ffe6fc6c04dbfba", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "This is actually used in this PR because for example, when `gI` requires grad [here](https://github.com/albanD/pytorch/blob/clean_conv_double_back/torch/csrc/autograd/functions/convolution.cpp#L480-L505), it is non zero only if `ggW` is provided. So it happens that we pass null grads to AccumulateGrad, I think this is more efficient than creating a new tensor full of 0s.", "created_at": "2017-06-14T07:55:58Z", "updated_at": "2018-11-23T15:33:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/1643#discussion_r121877758", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1643", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121877758"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1643#discussion_r121877758"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1643"}}, "body_html": "<p>This is actually used in this PR because for example, when <code>gI</code> requires grad <a href=\"https://github.com/albanD/pytorch/blob/clean_conv_double_back/torch/csrc/autograd/functions/convolution.cpp#L480-L505\">here</a>, it is non zero only if <code>ggW</code> is provided. So it happens that we pass null grads to AccumulateGrad, I think this is more efficient than creating a new tensor full of 0s.</p>", "body_text": "This is actually used in this PR because for example, when gI requires grad here, it is non zero only if ggW is provided. So it happens that we pass null grads to AccumulateGrad, I think this is more efficient than creating a new tensor full of 0s.", "in_reply_to_id": 121255042}