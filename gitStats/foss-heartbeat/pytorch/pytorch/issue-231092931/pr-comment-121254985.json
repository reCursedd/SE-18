{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121254985", "pull_request_review_id": 43295648, "id": 121254985, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMTI1NDk4NQ==", "diff_hunk": "@@ -1228,6 +1230,88 @@ def backward(self, grad_output):\n         c.backward(torch.ones(c.size()))\n         self.assertEqual(x.grad.data, torch.ones(x.size()))\n \n+    def run_conv_double_back_test(self, kern, stride, padding, chan_in, chan_out,\n+                                  batch_size, inp_size, dilation, no_weight, use_cuda=False):\n+        x = Variable(torch.randn(batch_size, chan_in, inp_size, inp_size), requires_grad=True)\n+        weight = Variable(torch.randn(chan_out, chan_in, kern, kern), requires_grad=True)\n+        bias = Variable(torch.randn(chan_out), requires_grad=True)\n+\n+        if use_cuda:\n+            x = x.cuda()\n+            weight = weight.cuda()\n+            bias = bias.cuda()\n+\n+        if no_weight:\n+            # Special case because transpose dilated convolution is not implemented\n+            def func(x, bias):\n+                # We disable cudnn during forward to avoid finite difference imprecision issues\n+                prev_cudnn = torch.backends.cudnn.enabled\n+                torch.backends.cudnn.enabled = False\n+                out = F.conv2d(x, weight, bias, stride, padding, dilation)\n+                torch.backends.cudnn.enabled = prev_cudnn", "path": "test/test_autograd.py", "position": null, "original_position": 32, "commit_id": "e3cc7c83dc86803042b934d3c1863a7af1640a8d", "original_commit_id": "c2c9e3dc9d63ca19b940adca1ffe6fc6c04dbfba", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is unsafe, because if an error happens inside `F.conv2d` this won't revert the cudnn setting. It'd be better to add this:\r\n```python\r\nfrom contextlib import contextmanager\r\n\r\n@contextmanager\r\ndef use_cudnn(should_use):\r\n    orig = torch.backends.cudnn.enabled\r\n    torch.backends.cudnn.enabled = should_use\r\n    try:\r\n        yield\r\n    finally:\r\n        torch.backends.cudnn.enabled = orig\r\n```\r\nand wrap the forward in a `with` statement", "created_at": "2017-06-10T10:58:50Z", "updated_at": "2018-11-23T15:33:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/1643#discussion_r121254985", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1643", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121254985"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1643#discussion_r121254985"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1643"}}, "body_html": "<p>This is unsafe, because if an error happens inside <code>F.conv2d</code> this won't revert the cudnn setting. It'd be better to add this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> contextlib <span class=\"pl-k\">import</span> contextmanager\n\n<span class=\"pl-en\">@contextmanager</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">use_cudnn</span>(<span class=\"pl-smi\">should_use</span>):\n    orig <span class=\"pl-k\">=</span> torch.backends.cudnn.enabled\n    torch.backends.cudnn.enabled <span class=\"pl-k\">=</span> should_use\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-k\">yield</span>\n    <span class=\"pl-k\">finally</span>:\n        torch.backends.cudnn.enabled <span class=\"pl-k\">=</span> orig</pre></div>\n<p>and wrap the forward in a <code>with</code> statement</p>", "body_text": "This is unsafe, because if an error happens inside F.conv2d this won't revert the cudnn setting. It'd be better to add this:\nfrom contextlib import contextmanager\n\n@contextmanager\ndef use_cudnn(should_use):\n    orig = torch.backends.cudnn.enabled\n    torch.backends.cudnn.enabled = should_use\n    try:\n        yield\n    finally:\n        torch.backends.cudnn.enabled = orig\nand wrap the forward in a with statement"}