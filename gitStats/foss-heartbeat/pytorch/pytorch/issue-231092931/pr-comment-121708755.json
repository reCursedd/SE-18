{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121708755", "pull_request_review_id": 43756769, "id": 121708755, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMTcwODc1NQ==", "diff_hunk": "@@ -33,10 +33,12 @@ auto AccumulateGrad::acc_inplace(std::shared_ptr<Variable>& grad,\n \n auto AccumulateGrad::apply(const variable_list& grads) -> variable_list {\n   // XXX: this method is not thread-safe!\n-  check_input_variables(\"AccumulateGrad\", grads, 1);\n-  auto var = variable.lock();\n+  check_input_variables(\"AccumulateGrad\", grads, 1, 0);\n   auto new_grad = grads[0];\n \n+  if (!new_grad) return {};", "path": "torch/csrc/autograd/functions/accumulate_grad.cpp", "position": 9, "original_position": 9, "commit_id": "e3cc7c83dc86803042b934d3c1863a7af1640a8d", "original_commit_id": "c2c9e3dc9d63ca19b940adca1ffe6fc6c04dbfba", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "You're right, but I'm wondering when was it needed in this PR. It seems that the only situation when Convolutions return NULL grads is when weights don't require grad. But this means that they shouldn't have a GradAccumulator. Or is it triggered for bias in ConvBackwardBackward?", "created_at": "2017-06-13T15:14:42Z", "updated_at": "2018-11-23T15:33:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/1643#discussion_r121708755", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1643", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121708755"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1643#discussion_r121708755"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1643"}}, "body_html": "<p>You're right, but I'm wondering when was it needed in this PR. It seems that the only situation when Convolutions return NULL grads is when weights don't require grad. But this means that they shouldn't have a GradAccumulator. Or is it triggered for bias in ConvBackwardBackward?</p>", "body_text": "You're right, but I'm wondering when was it needed in this PR. It seems that the only situation when Convolutions return NULL grads is when weights don't require grad. But this means that they shouldn't have a GradAccumulator. Or is it triggered for bias in ConvBackwardBackward?", "in_reply_to_id": 121255042}