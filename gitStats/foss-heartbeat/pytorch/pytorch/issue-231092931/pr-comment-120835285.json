{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/120835285", "pull_request_review_id": 42830714, "id": 120835285, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMDgzNTI4NQ==", "diff_hunk": "@@ -1214,6 +1215,91 @@ def backward(self, grad_output):\n         c.backward(torch.ones(c.size()))\n         self.assertEqual(x.grad.data, torch.ones(x.size()))\n \n+    def run_conv_double_back_test(self, kern, stride, padding, chan_in, chan_out,\n+                                  batch_size, inp_size, dilation, no_weight):\n+        x = Variable(torch.randn(batch_size, chan_in, inp_size, inp_size), requires_grad=True)\n+        weight = Variable(torch.randn(chan_out, chan_in, kern, kern), requires_grad=True)\n+        bias = Variable(torch.randn(chan_out), requires_grad=True)\n+\n+        if no_weight:\n+            # Special case because transpose dilated convolution is not implemented\n+            def func(x, bias):\n+                return F.conv2d(x, weight, bias, stride, padding, dilation)", "path": "test/test_autograd.py", "position": null, "original_position": 23, "commit_id": "e3cc7c83dc86803042b934d3c1863a7af1640a8d", "original_commit_id": "9260594e948b8476f64c6c232f5b734ca6b987da", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "Yes it is. We test gradients wrt the arguments of `func`. ", "created_at": "2017-06-08T09:05:47Z", "updated_at": "2018-11-23T15:33:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/1643#discussion_r120835285", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1643", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/120835285"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1643#discussion_r120835285"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1643"}}, "body_html": "<p>Yes it is. We test gradients wrt the arguments of <code>func</code>.</p>", "body_text": "Yes it is. We test gradients wrt the arguments of func.", "in_reply_to_id": 120581859}