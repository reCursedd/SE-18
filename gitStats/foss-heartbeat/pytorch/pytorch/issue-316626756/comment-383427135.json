{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383427135", "html_url": "https://github.com/pytorch/pytorch/issues/6850#issuecomment-383427135", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6850", "id": 383427135, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzQyNzEzNQ==", "user": {"login": "meder411", "id": 6818607, "node_id": "MDQ6VXNlcjY4MTg2MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6818607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meder411", "html_url": "https://github.com/meder411", "followers_url": "https://api.github.com/users/meder411/followers", "following_url": "https://api.github.com/users/meder411/following{/other_user}", "gists_url": "https://api.github.com/users/meder411/gists{/gist_id}", "starred_url": "https://api.github.com/users/meder411/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meder411/subscriptions", "organizations_url": "https://api.github.com/users/meder411/orgs", "repos_url": "https://api.github.com/users/meder411/repos", "events_url": "https://api.github.com/users/meder411/events{/privacy}", "received_events_url": "https://api.github.com/users/meder411/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-23T00:59:41Z", "updated_at": "2018-04-23T01:24:50Z", "author_association": "NONE", "body_html": "<p>Sorry, yes. These are accumulator functions. I am thinking about using them to accumulate averages rather than raw data, for example. Or accumulate maxima/minima.</p>\n<p>A key aspect of these function, though, are that you can have repeated indices that get accumulated over. For example, if the data being accumulated is</p>\n<pre><code>[0]\n[2]\n[2]\n</code></pre>\n<p>and your indices are <code>[0,1,1,2]</code>, you can call <code>index_add_()</code> on a vector of zeros, and get an output of</p>\n<pre><code>[0]\n[4]\n[2]\n...\n</code></pre>\n<p>When using <code>index_select()</code>, you can't accumulate. So let's say I have some kind of associated index-data mapping. Maybe each data point maps to its 5 nearest neighbors in another set of data points (e.g. a mapping resulting from K-Means association--new data and centroids). Perhaps I'd now want to recompute the means. I have a list of indices <code>Y</code> corresponding to closest centroids, and data tensor <code>X</code> in which each data point is associated with an index in that list <code>Y</code>. Without accumulation, <a href=\"http://pytorch.org/docs/master/torch.html#torch.index_select\" rel=\"nofollow\"><code>index_select()</code></a> doesn't work for this. Instead, an <code>index_mean_()</code> would provide an efficient way to recompute the centroids. In these cases, the in-place operation would be more like a <a href=\"http://pytorch.org/docs/master/tensors.html?highlight=index_put#torch.Tensor.index_put_\" rel=\"nofollow\"><code>index_put_()</code></a>, though, as it would overwrite the contents of the tensor it's called on.</p>\n<p>For a full example, say I have some bimodal data and I run K-Means for K=2.</p>\n<pre><code>C = torch.FloatTensor([[0,0,1], [1,0,0]]) # Initial centroids\nX = torch.FloatTensor([[0,0,1.2], [1.1,0,0], [0,0,0.9], [1.3,0,0]]) # Data\n# Do something to compute nearest centroids, resulting in Y below...:\nY = torch.LongTensor([0, 1, 0, 1]) # Indices corresponding to the closest centroid for each data point\n\nC.index_mean_(0, Y, X) # This would produce [[0,0,1.05], [1.2,0,0]]\n</code></pre>", "body_text": "Sorry, yes. These are accumulator functions. I am thinking about using them to accumulate averages rather than raw data, for example. Or accumulate maxima/minima.\nA key aspect of these function, though, are that you can have repeated indices that get accumulated over. For example, if the data being accumulated is\n[0]\n[2]\n[2]\n\nand your indices are [0,1,1,2], you can call index_add_() on a vector of zeros, and get an output of\n[0]\n[4]\n[2]\n...\n\nWhen using index_select(), you can't accumulate. So let's say I have some kind of associated index-data mapping. Maybe each data point maps to its 5 nearest neighbors in another set of data points (e.g. a mapping resulting from K-Means association--new data and centroids). Perhaps I'd now want to recompute the means. I have a list of indices Y corresponding to closest centroids, and data tensor X in which each data point is associated with an index in that list Y. Without accumulation, index_select() doesn't work for this. Instead, an index_mean_() would provide an efficient way to recompute the centroids. In these cases, the in-place operation would be more like a index_put_(), though, as it would overwrite the contents of the tensor it's called on.\nFor a full example, say I have some bimodal data and I run K-Means for K=2.\nC = torch.FloatTensor([[0,0,1], [1,0,0]]) # Initial centroids\nX = torch.FloatTensor([[0,0,1.2], [1.1,0,0], [0,0,0.9], [1.3,0,0]]) # Data\n# Do something to compute nearest centroids, resulting in Y below...:\nY = torch.LongTensor([0, 1, 0, 1]) # Indices corresponding to the closest centroid for each data point\n\nC.index_mean_(0, Y, X) # This would produce [[0,0,1.05], [1.2,0,0]]", "body": "Sorry, yes. These are accumulator functions. I am thinking about using them to accumulate averages rather than raw data, for example. Or accumulate maxima/minima.\r\n\r\nA key aspect of these function, though, are that you can have repeated indices that get accumulated over. For example, if the data being accumulated is\r\n\r\n    [0]\r\n    [2]\r\n    [2]\r\n\r\n\r\nand your indices are `[0,1,1,2]`, you can call `index_add_()` on a vector of zeros, and get an output of \r\n\r\n    [0]\r\n    [4]\r\n    [2]\r\n    ...\r\n\r\n\r\nWhen using `index_select()`, you can't accumulate. So let's say I have some kind of associated index-data mapping. Maybe each data point maps to its 5 nearest neighbors in another set of data points (e.g. a mapping resulting from K-Means association--new data and centroids). Perhaps I'd now want to recompute the means. I have a list of indices `Y` corresponding to closest centroids, and data tensor `X` in which each data point is associated with an index in that list `Y`. Without accumulation, [`index_select()`](http://pytorch.org/docs/master/torch.html#torch.index_select) doesn't work for this. Instead, an `index_mean_()` would provide an efficient way to recompute the centroids. In these cases, the in-place operation would be more like a [`index_put_()`](http://pytorch.org/docs/master/tensors.html?highlight=index_put#torch.Tensor.index_put_), though, as it would overwrite the contents of the tensor it's called on.\r\n\r\nFor a full example, say I have some bimodal data and I run K-Means for K=2.\r\n\r\n    C = torch.FloatTensor([[0,0,1], [1,0,0]]) # Initial centroids\r\n    X = torch.FloatTensor([[0,0,1.2], [1.1,0,0], [0,0,0.9], [1.3,0,0]]) # Data\r\n    # Do something to compute nearest centroids, resulting in Y below...:\r\n    Y = torch.LongTensor([0, 1, 0, 1]) # Indices corresponding to the closest centroid for each data point\r\n\r\n    C.index_mean_(0, Y, X) # This would produce [[0,0,1.05], [1.2,0,0]]\r\n\r\n  "}