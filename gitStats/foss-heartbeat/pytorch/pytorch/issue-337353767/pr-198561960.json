{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9089", "id": 198561960, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk4NTYxOTYw", "html_url": "https://github.com/pytorch/pytorch/pull/9089", "diff_url": "https://github.com/pytorch/pytorch/pull/9089.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9089.patch", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9089", "number": 9089, "state": "closed", "locked": false, "title": "Unify THAllocator and THCDeviceAllocator with at::Allocator ", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Stacked on #9087\r\n\r\n```\r\n    Unify THAllocator and THCDeviceAllocator with at::Allocator\r\n    \r\n    - at::Allocator is the type that \"survived\" this refactor, but\r\n      it grew a void* ctx argument.  This brings it in line\r\n      with the THAllocator interface.  Arguably, one might say that\r\n      void* ctx is bad design.  But...\r\n    \r\n        - We definitely need some sort of context for deleters,\r\n          because sometimes we need more information than the data\r\n          pointer itself to properly deallocate.  A simple example\r\n          is a dlpack tensor; to free the data, we need an\r\n          enclosing struct which contains the actual function pointer\r\n          to perform deletion.\r\n    \r\n        - So, it's quite plausible we can pack the deleter together\r\n          with its context data (e.g., an object with a function\r\n          pointer and its context).  This is not a bad idea, but this\r\n          patch is pretty long already, so we leave it for later.\r\n    \r\n    - THCDeviceAllocator has a few differences from at::Allocator:\r\n    \r\n        - It used to return a cudaError_t.  Now, allocators\r\n          are expected to check the error status immediately and throw\r\n          an exception if there was an error.  It turns out that this\r\n          is what was immediately done after all occurrences of\r\n          allocate/release, so it wasn't a big deal (although some\r\n          subsidiary interfaces had to themselves be converted to\r\n          not return cudaError_t).\r\n    \r\n          There is one notable exception to this, and it is how\r\n          we handle CUDA OOM: if this occurs, we attempt to return\r\n          unused memory to the system and try again.  This is now\r\n          handled by a catch-all try-catch block.\r\n    \r\n        - It used to take the CUDA stream to perform the allocation\r\n          on as an argument.  However, it turned out that all call\r\n          sites, this stream was the stream for the current device.\r\n          So we can push this into the allocator (and the choice,\r\n          in the future, could be made explicitly by twiddling\r\n          thread local state.)\r\n    \r\n        - It held two extra methods, emptyCache and cacheInfo, specifically\r\n          for interacting with some state in THCCachingAllocator.\r\n          But this \"generality\" was a lie, since THCCachingAllocator\r\n          was the only allocator that actually implemented these\r\n          methods, and there is actually a bunch of code in THC\r\n          which assumes that it is the caching allocator that is\r\n          the underlying allocator for CUDA allocations.  So I\r\n          folded these two methods into this interface as\r\n          THCCachingAllocator_emptyCache and THCCachingAllocator_cacheInfo.\r\n    \r\n        - It held its context directly inside the THCDeviceAllocator\r\n          struct.  This context has been moved out into whatever\r\n          is holding the at::Allocator*.\r\n\r\n    - I took the opportunity to replace all global variables with\r\n      getter functions; e.g., where you had &THCudaHostAllocator,\r\n      you now have getTHCudaHostAllocator()\r\n    \r\n    - Realloc is no more.  (I'm not sure how important it was for\r\n      the CPU case.  However, if it was important, we have to add\r\n      it back very carefully. There is no way to write a \"default\"\r\n      implementation of realloc using malloc and free, because some\r\n      mechanism of copying data from the old region to the new\r\n      region is needed, and \"how to copy data\" is definitely not\r\n      in the purview of an allocator.  Furthermore, implementations\r\n      of realloc usually have side conditions: they usually only\r\n      work on plain-old-data, because the reallocator doesn't know\r\n      what the type of the data it is copying are.)  It's a lot\r\n      simpler for the design if we give up the slight optimization\r\n      we get in the CPU case from deferring the reallocation to the\r\n      system malloc() implementation, so I've dropped it for now.\r\n    \r\n    - allocatorVoidPtr is no more!  Now it's just an at::Allocator*\r\n    \r\n    I tried not to make big semantic changes to any of the downstream\r\n    allocators in this patch (as it was getting big), but a lot of the\r\n    downstream allocators are designed a bit strangely, since they\r\n    were built under the assumption that TH/THC could not be changed.\r\n    They definitely merit refactoring.\r\n   \r\n```", "created_at": "2018-07-02T03:00:14Z", "updated_at": "2018-11-23T15:46:40Z", "closed_at": "2018-07-08T23:38:32Z", "merged_at": null, "merge_commit_sha": "189b207c863d9d01fef834a58d579dffc0382bc5", "assignee": null, "assignees": [], "requested_reviewers": [{"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "requested_teams": [], "labels": [], "milestone": null, "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9089/commits", "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9089/comments", "review_comment_url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9089/comments", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/53f0bade46b6b09175554b6d2eb9d8c9140fa6fe", "head": {"label": "ezyang:pr/unify-allocators", "ref": "pr/unify-allocators", "sha": "53f0bade46b6b09175554b6d2eb9d8c9140fa6fe", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "repo": {"id": 101798885, "node_id": "MDEwOlJlcG9zaXRvcnkxMDE3OTg4ODU=", "name": "pytorch", "full_name": "ezyang/pytorch", "private": false, "owner": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "html_url": "https://github.com/ezyang/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": true, "url": "https://api.github.com/repos/ezyang/pytorch", "forks_url": "https://api.github.com/repos/ezyang/pytorch/forks", "keys_url": "https://api.github.com/repos/ezyang/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/ezyang/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/ezyang/pytorch/teams", "hooks_url": "https://api.github.com/repos/ezyang/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/ezyang/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/ezyang/pytorch/events", "assignees_url": "https://api.github.com/repos/ezyang/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/ezyang/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/ezyang/pytorch/tags", "blobs_url": "https://api.github.com/repos/ezyang/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/ezyang/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/ezyang/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/ezyang/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/ezyang/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/ezyang/pytorch/languages", "stargazers_url": "https://api.github.com/repos/ezyang/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/ezyang/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/ezyang/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/ezyang/pytorch/subscription", "commits_url": "https://api.github.com/repos/ezyang/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/ezyang/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/ezyang/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/ezyang/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/ezyang/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/ezyang/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/ezyang/pytorch/merges", "archive_url": "https://api.github.com/repos/ezyang/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/ezyang/pytorch/downloads", "issues_url": "https://api.github.com/repos/ezyang/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/ezyang/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/ezyang/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/ezyang/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/ezyang/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/ezyang/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/ezyang/pytorch/deployments", "created_at": "2017-08-29T19:28:39Z", "updated_at": "2018-10-29T15:06:40Z", "pushed_at": "2018-11-21T22:30:09Z", "git_url": "git://github.com/ezyang/pytorch.git", "ssh_url": "git@github.com:ezyang/pytorch.git", "clone_url": "https://github.com/ezyang/pytorch.git", "svn_url": "https://github.com/ezyang/pytorch", "homepage": "http://pytorch.org", "size": 88254, "stargazers_count": 1, "watchers_count": 1, "language": "C++", "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 0, "mirror_url": null, "archived": false, "open_issues_count": 2, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 0, "open_issues": 2, "watchers": 1, "default_branch": "master"}}, "base": {"label": "pytorch:master", "ref": "master", "sha": "e3dbdb2a17827d011c0d407eb0c6683677c4c895", "user": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 65600975, "node_id": "MDEwOlJlcG9zaXRvcnk2NTYwMDk3NQ==", "name": "pytorch", "full_name": "pytorch/pytorch", "private": false, "owner": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/pytorch/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": false, "url": "https://api.github.com/repos/pytorch/pytorch", "forks_url": "https://api.github.com/repos/pytorch/pytorch/forks", "keys_url": "https://api.github.com/repos/pytorch/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/pytorch/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/pytorch/pytorch/teams", "hooks_url": "https://api.github.com/repos/pytorch/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/pytorch/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/pytorch/pytorch/events", "assignees_url": "https://api.github.com/repos/pytorch/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/pytorch/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/pytorch/pytorch/tags", "blobs_url": "https://api.github.com/repos/pytorch/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/pytorch/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/pytorch/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/pytorch/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/pytorch/pytorch/languages", "stargazers_url": "https://api.github.com/repos/pytorch/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/pytorch/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/pytorch/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/pytorch/pytorch/subscription", "commits_url": "https://api.github.com/repos/pytorch/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/pytorch/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/pytorch/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/pytorch/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/pytorch/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/pytorch/pytorch/merges", "archive_url": "https://api.github.com/repos/pytorch/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/pytorch/pytorch/downloads", "issues_url": "https://api.github.com/repos/pytorch/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/pytorch/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/pytorch/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/pytorch/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/pytorch/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/pytorch/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/pytorch/pytorch/deployments", "created_at": "2016-08-13T05:26:41Z", "updated_at": "2018-11-24T12:35:43Z", "pushed_at": "2018-11-24T12:42:01Z", "git_url": "git://github.com/pytorch/pytorch.git", "ssh_url": "git@github.com:pytorch/pytorch.git", "clone_url": "https://github.com/pytorch/pytorch.git", "svn_url": "https://github.com/pytorch/pytorch", "homepage": "http://pytorch.org", "size": 89656, "stargazers_count": 21589, "watchers_count": 21589, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 5153, "mirror_url": null, "archived": false, "open_issues_count": 2196, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 5153, "open_issues": 2196, "watchers": 21589, "default_branch": "master"}}, "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9089"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9089"}, "issue": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/9089"}, "comments": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/9089/comments"}, "review_comments": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9089/comments"}, "review_comment": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}"}, "commits": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9089/commits"}, "statuses": {"href": "https://api.github.com/repos/pytorch/pytorch/statuses/53f0bade46b6b09175554b6d2eb9d8c9140fa6fe"}}, "author_association": "CONTRIBUTOR", "body_html": "<p>Stacked on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"337343084\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9087\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9087/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9087\">#9087</a></p>\n<pre><code>    Unify THAllocator and THCDeviceAllocator with at::Allocator\n    \n    - at::Allocator is the type that \"survived\" this refactor, but\n      it grew a void* ctx argument.  This brings it in line\n      with the THAllocator interface.  Arguably, one might say that\n      void* ctx is bad design.  But...\n    \n        - We definitely need some sort of context for deleters,\n          because sometimes we need more information than the data\n          pointer itself to properly deallocate.  A simple example\n          is a dlpack tensor; to free the data, we need an\n          enclosing struct which contains the actual function pointer\n          to perform deletion.\n    \n        - So, it's quite plausible we can pack the deleter together\n          with its context data (e.g., an object with a function\n          pointer and its context).  This is not a bad idea, but this\n          patch is pretty long already, so we leave it for later.\n    \n    - THCDeviceAllocator has a few differences from at::Allocator:\n    \n        - It used to return a cudaError_t.  Now, allocators\n          are expected to check the error status immediately and throw\n          an exception if there was an error.  It turns out that this\n          is what was immediately done after all occurrences of\n          allocate/release, so it wasn't a big deal (although some\n          subsidiary interfaces had to themselves be converted to\n          not return cudaError_t).\n    \n          There is one notable exception to this, and it is how\n          we handle CUDA OOM: if this occurs, we attempt to return\n          unused memory to the system and try again.  This is now\n          handled by a catch-all try-catch block.\n    \n        - It used to take the CUDA stream to perform the allocation\n          on as an argument.  However, it turned out that all call\n          sites, this stream was the stream for the current device.\n          So we can push this into the allocator (and the choice,\n          in the future, could be made explicitly by twiddling\n          thread local state.)\n    \n        - It held two extra methods, emptyCache and cacheInfo, specifically\n          for interacting with some state in THCCachingAllocator.\n          But this \"generality\" was a lie, since THCCachingAllocator\n          was the only allocator that actually implemented these\n          methods, and there is actually a bunch of code in THC\n          which assumes that it is the caching allocator that is\n          the underlying allocator for CUDA allocations.  So I\n          folded these two methods into this interface as\n          THCCachingAllocator_emptyCache and THCCachingAllocator_cacheInfo.\n    \n        - It held its context directly inside the THCDeviceAllocator\n          struct.  This context has been moved out into whatever\n          is holding the at::Allocator*.\n\n    - I took the opportunity to replace all global variables with\n      getter functions; e.g., where you had &amp;THCudaHostAllocator,\n      you now have getTHCudaHostAllocator()\n    \n    - Realloc is no more.  (I'm not sure how important it was for\n      the CPU case.  However, if it was important, we have to add\n      it back very carefully. There is no way to write a \"default\"\n      implementation of realloc using malloc and free, because some\n      mechanism of copying data from the old region to the new\n      region is needed, and \"how to copy data\" is definitely not\n      in the purview of an allocator.  Furthermore, implementations\n      of realloc usually have side conditions: they usually only\n      work on plain-old-data, because the reallocator doesn't know\n      what the type of the data it is copying are.)  It's a lot\n      simpler for the design if we give up the slight optimization\n      we get in the CPU case from deferring the reallocation to the\n      system malloc() implementation, so I've dropped it for now.\n    \n    - allocatorVoidPtr is no more!  Now it's just an at::Allocator*\n    \n    I tried not to make big semantic changes to any of the downstream\n    allocators in this patch (as it was getting big), but a lot of the\n    downstream allocators are designed a bit strangely, since they\n    were built under the assumption that TH/THC could not be changed.\n    They definitely merit refactoring.\n   \n</code></pre>", "body_text": "Stacked on #9087\n    Unify THAllocator and THCDeviceAllocator with at::Allocator\n    \n    - at::Allocator is the type that \"survived\" this refactor, but\n      it grew a void* ctx argument.  This brings it in line\n      with the THAllocator interface.  Arguably, one might say that\n      void* ctx is bad design.  But...\n    \n        - We definitely need some sort of context for deleters,\n          because sometimes we need more information than the data\n          pointer itself to properly deallocate.  A simple example\n          is a dlpack tensor; to free the data, we need an\n          enclosing struct which contains the actual function pointer\n          to perform deletion.\n    \n        - So, it's quite plausible we can pack the deleter together\n          with its context data (e.g., an object with a function\n          pointer and its context).  This is not a bad idea, but this\n          patch is pretty long already, so we leave it for later.\n    \n    - THCDeviceAllocator has a few differences from at::Allocator:\n    \n        - It used to return a cudaError_t.  Now, allocators\n          are expected to check the error status immediately and throw\n          an exception if there was an error.  It turns out that this\n          is what was immediately done after all occurrences of\n          allocate/release, so it wasn't a big deal (although some\n          subsidiary interfaces had to themselves be converted to\n          not return cudaError_t).\n    \n          There is one notable exception to this, and it is how\n          we handle CUDA OOM: if this occurs, we attempt to return\n          unused memory to the system and try again.  This is now\n          handled by a catch-all try-catch block.\n    \n        - It used to take the CUDA stream to perform the allocation\n          on as an argument.  However, it turned out that all call\n          sites, this stream was the stream for the current device.\n          So we can push this into the allocator (and the choice,\n          in the future, could be made explicitly by twiddling\n          thread local state.)\n    \n        - It held two extra methods, emptyCache and cacheInfo, specifically\n          for interacting with some state in THCCachingAllocator.\n          But this \"generality\" was a lie, since THCCachingAllocator\n          was the only allocator that actually implemented these\n          methods, and there is actually a bunch of code in THC\n          which assumes that it is the caching allocator that is\n          the underlying allocator for CUDA allocations.  So I\n          folded these two methods into this interface as\n          THCCachingAllocator_emptyCache and THCCachingAllocator_cacheInfo.\n    \n        - It held its context directly inside the THCDeviceAllocator\n          struct.  This context has been moved out into whatever\n          is holding the at::Allocator*.\n\n    - I took the opportunity to replace all global variables with\n      getter functions; e.g., where you had &THCudaHostAllocator,\n      you now have getTHCudaHostAllocator()\n    \n    - Realloc is no more.  (I'm not sure how important it was for\n      the CPU case.  However, if it was important, we have to add\n      it back very carefully. There is no way to write a \"default\"\n      implementation of realloc using malloc and free, because some\n      mechanism of copying data from the old region to the new\n      region is needed, and \"how to copy data\" is definitely not\n      in the purview of an allocator.  Furthermore, implementations\n      of realloc usually have side conditions: they usually only\n      work on plain-old-data, because the reallocator doesn't know\n      what the type of the data it is copying are.)  It's a lot\n      simpler for the design if we give up the slight optimization\n      we get in the CPU case from deferring the reallocation to the\n      system malloc() implementation, so I've dropped it for now.\n    \n    - allocatorVoidPtr is no more!  Now it's just an at::Allocator*\n    \n    I tried not to make big semantic changes to any of the downstream\n    allocators in this patch (as it was getting big), but a lot of the\n    downstream allocators are designed a bit strangely, since they\n    were built under the assumption that TH/THC could not be changed.\n    They definitely merit refactoring.", "merged": false, "mergeable": false, "rebaseable": false, "mergeable_state": "dirty", "merged_by": null, "comments": 1, "review_comments": 6, "maintainer_can_modify": false, "commits": 2, "additions": 513, "deletions": 659, "changed_files": 46}