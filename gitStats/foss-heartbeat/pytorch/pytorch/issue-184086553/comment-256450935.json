{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/256450935", "html_url": "https://github.com/pytorch/pytorch/issues/144#issuecomment-256450935", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/144", "id": 256450935, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NjQ1MDkzNQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-26T19:23:16Z", "updated_at": "2016-10-26T19:23:16Z", "author_association": "MEMBER", "body_html": "<p>So actually I can't see any easy solution to this problem. The graph you create using in-place operations looks like this at the moment:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/4583066/19741362/a713c68a-9bc0-11e6-8194-4d5e047422e3.png\"><img src=\"https://cloud.githubusercontent.com/assets/4583066/19741362/a713c68a-9bc0-11e6-8194-4d5e047422e3.png\" alt=\"rysunek bez nazwy\" style=\"max-width:100%;\"></a><br>\nThe problem is that the two in-place columns are created and immediately discarded, so they're never included in the backward phase. A solution would need to move the original big Variable forward in the graph, after the inplace ops.</p>\n<p>I can see only one solution at the moment - introducing a new <code>VariableView</code> class. It would have to remember a whole sequence of operations that preserved the storage sharing and led to its creation (in your case, just a single Index, in a more complicated one, it could be sth like Index, Transpose, Index, ...). Then, once we mark a <code>VariableView</code> as dirty, we still increment a version count of the original tensor, but we also create a new graph node that will always backprop through the in-place branch.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/4583066/19741666/e0386ae6-9bc1-11e6-8735-c117207399e7.png\"><img src=\"https://cloud.githubusercontent.com/assets/4583066/19741666/e0386ae6-9bc1-11e6-8735-c117207399e7.png\" alt=\"rysunek bez nazwy-2\" style=\"max-width:100%;\"></a></p>\n<p><code>GradIndex</code> node would apply the same sequence of indexing operations that were saved in the VariableView (we'd probably need to come up with some compact format for them).</p>\n<p>The main two downsides of this approach is that it adds a lot of complexity to an already complicated piece of code (<code>_do_forward</code> of <code>Function</code>), and if the chain of in-place operations is is long, we'll have to do a lot of additional tensor indexing in the backward, which may make us even more CPU bound with newer GPUs.</p>", "body_text": "So actually I can't see any easy solution to this problem. The graph you create using in-place operations looks like this at the moment:\n\nThe problem is that the two in-place columns are created and immediately discarded, so they're never included in the backward phase. A solution would need to move the original big Variable forward in the graph, after the inplace ops.\nI can see only one solution at the moment - introducing a new VariableView class. It would have to remember a whole sequence of operations that preserved the storage sharing and led to its creation (in your case, just a single Index, in a more complicated one, it could be sth like Index, Transpose, Index, ...). Then, once we mark a VariableView as dirty, we still increment a version count of the original tensor, but we also create a new graph node that will always backprop through the in-place branch.\n\nGradIndex node would apply the same sequence of indexing operations that were saved in the VariableView (we'd probably need to come up with some compact format for them).\nThe main two downsides of this approach is that it adds a lot of complexity to an already complicated piece of code (_do_forward of Function), and if the chain of in-place operations is is long, we'll have to do a lot of additional tensor indexing in the backward, which may make us even more CPU bound with newer GPUs.", "body": "So actually I can't see any easy solution to this problem. The graph you create using in-place operations looks like this at the moment:\n![rysunek bez nazwy](https://cloud.githubusercontent.com/assets/4583066/19741362/a713c68a-9bc0-11e6-8194-4d5e047422e3.png)\nThe problem is that the two in-place columns are created and immediately discarded, so they're never included in the backward phase. A solution would need to move the original big Variable forward in the graph, after the inplace ops.\n\nI can see only one solution at the moment - introducing a new `VariableView` class. It would have to remember a whole sequence of operations that preserved the storage sharing and led to its creation (in your case, just a single Index, in a more complicated one, it could be sth like Index, Transpose, Index, ...). Then, once we mark a `VariableView` as dirty, we still increment a version count of the original tensor, but we also create a new graph node that will always backprop through the in-place branch.\n\n![rysunek bez nazwy-2](https://cloud.githubusercontent.com/assets/4583066/19741666/e0386ae6-9bc1-11e6-8735-c117207399e7.png)\n\n`GradIndex` node would apply the same sequence of indexing operations that were saved in the VariableView (we'd probably need to come up with some compact format for them).\n\nThe main two downsides of this approach is that it adds a lot of complexity to an already complicated piece of code (`_do_forward` of `Function`), and if the chain of in-place operations is is long, we'll have to do a lot of additional tensor indexing in the backward, which may make us even more CPU bound with newer GPUs.\n"}