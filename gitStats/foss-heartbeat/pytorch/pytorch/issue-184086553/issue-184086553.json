{"url": "https://api.github.com/repos/pytorch/pytorch/issues/144", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/144/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/144/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/144/events", "html_url": "https://github.com/pytorch/pytorch/issues/144", "id": 184086553, "node_id": "MDU6SXNzdWUxODQwODY1NTM=", "number": 144, "title": "inplace operations not raising error during backward", "user": {"login": "glample", "id": 8885556, "node_id": "MDQ6VXNlcjg4ODU1NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8885556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glample", "html_url": "https://github.com/glample", "followers_url": "https://api.github.com/users/glample/followers", "following_url": "https://api.github.com/users/glample/following{/other_user}", "gists_url": "https://api.github.com/users/glample/gists{/gist_id}", "starred_url": "https://api.github.com/users/glample/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glample/subscriptions", "organizations_url": "https://api.github.com/users/glample/orgs", "repos_url": "https://api.github.com/users/glample/repos", "events_url": "https://api.github.com/users/glample/events{/privacy}", "received_events_url": "https://api.github.com/users/glample/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-10-19T22:06:55Z", "updated_at": "2016-11-08T17:12:56Z", "closed_at": "2016-11-08T17:12:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Calling the backward function does not necessarily raise an error when inplace operations are made. This leads to incorrect gradients. Below is an example with an LSTM that computes the gates with and without inplace operations.</p>\n<pre><code>class LSTM(nn.Container):\n\n    def __init__(self, input_dim, hidden_dim):\n        super(LSTM, self).__init__(\n            i2h=nn.Linear(input_dim, 4 * hidden_dim),\n            h2h=nn.Linear(hidden_dim, 4 * hidden_dim)\n        )\n        self.c0 = Variable(torch.zeros(hidden_dim), requires_grad=True)\n        self.h0 = Variable(torch.zeros(hidden_dim), requires_grad=True)\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.reset()\n\n    def forward(self, x):\n        last_c, last_h = self.temp\n        if self.step == 0:\n            last_c = last_c.view(1, self.hidden_dim).expand(x.size(0), self.hidden_dim)\n            last_h = last_h.view(1, self.hidden_dim).expand(x.size(0), self.hidden_dim)\n        gates = self.i2h(x) + self.h2h(last_h)\n\n        ## works\n#         i_t = nn.Sigmoid()(gates[:, :self.hidden_dim])\n#         f_t = nn.Sigmoid()(gates[:, self.hidden_dim:2 * self.hidden_dim])\n#         o_t = nn.Sigmoid()(gates[:, 2 * self.hidden_dim:3 * self.hidden_dim])\n#         cell_input = nn.Tanh()(gates[:, 3 * self.hidden_dim:])\n\n        ## works, same results than above, as expected\n#         i_t, f_t, o_t = gates[:, :3 * self.hidden_dim].sigmoid().chunk(3, 1)\n#         cell_input = gates[:, 3 * self.hidden_dim:].tanh()\n\n        ## fails, although backward runs\n        gates[:, :3 * self.hidden_dim].sigmoid_()\n        gates[:, 3 * self.hidden_dim:].tanh_()\n        i_t, f_t, o_t, cell_input = gates.chunk(4, 1)\n\n        next_c = f_t * last_c + i_t * cell_input\n        next_h = o_t * next_c.tanh()\n        self.temp = [next_c, next_h]\n        self.step += 1\n        return next_h\n\n    def reset(self):\n        self.step = 0\n        self.temp = [self.c0, self.h0]\n\ntorch.manual_seed(0)\nrnn = LSTM(3, 4)\ninputs = [Variable(torch.FloatTensor(2, 3).normal_()) for _ in range(3)]\n\n# code below doesn't always output the same thing when LSTM uses inplace operations\nfor k, p in rnn.parameter_dict().items():\n    p.grad.fill_(0)\nfor x in inputs:\n    print(x.sum().data[0], rnn(x).sum().data[0])\nrnn.temp[-1].sum().backward()\nfor k, p in rnn.parameter_dict().items():\n    print(k, p.grad.sum())\n</code></pre>", "body_text": "Calling the backward function does not necessarily raise an error when inplace operations are made. This leads to incorrect gradients. Below is an example with an LSTM that computes the gates with and without inplace operations.\nclass LSTM(nn.Container):\n\n    def __init__(self, input_dim, hidden_dim):\n        super(LSTM, self).__init__(\n            i2h=nn.Linear(input_dim, 4 * hidden_dim),\n            h2h=nn.Linear(hidden_dim, 4 * hidden_dim)\n        )\n        self.c0 = Variable(torch.zeros(hidden_dim), requires_grad=True)\n        self.h0 = Variable(torch.zeros(hidden_dim), requires_grad=True)\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.reset()\n\n    def forward(self, x):\n        last_c, last_h = self.temp\n        if self.step == 0:\n            last_c = last_c.view(1, self.hidden_dim).expand(x.size(0), self.hidden_dim)\n            last_h = last_h.view(1, self.hidden_dim).expand(x.size(0), self.hidden_dim)\n        gates = self.i2h(x) + self.h2h(last_h)\n\n        ## works\n#         i_t = nn.Sigmoid()(gates[:, :self.hidden_dim])\n#         f_t = nn.Sigmoid()(gates[:, self.hidden_dim:2 * self.hidden_dim])\n#         o_t = nn.Sigmoid()(gates[:, 2 * self.hidden_dim:3 * self.hidden_dim])\n#         cell_input = nn.Tanh()(gates[:, 3 * self.hidden_dim:])\n\n        ## works, same results than above, as expected\n#         i_t, f_t, o_t = gates[:, :3 * self.hidden_dim].sigmoid().chunk(3, 1)\n#         cell_input = gates[:, 3 * self.hidden_dim:].tanh()\n\n        ## fails, although backward runs\n        gates[:, :3 * self.hidden_dim].sigmoid_()\n        gates[:, 3 * self.hidden_dim:].tanh_()\n        i_t, f_t, o_t, cell_input = gates.chunk(4, 1)\n\n        next_c = f_t * last_c + i_t * cell_input\n        next_h = o_t * next_c.tanh()\n        self.temp = [next_c, next_h]\n        self.step += 1\n        return next_h\n\n    def reset(self):\n        self.step = 0\n        self.temp = [self.c0, self.h0]\n\ntorch.manual_seed(0)\nrnn = LSTM(3, 4)\ninputs = [Variable(torch.FloatTensor(2, 3).normal_()) for _ in range(3)]\n\n# code below doesn't always output the same thing when LSTM uses inplace operations\nfor k, p in rnn.parameter_dict().items():\n    p.grad.fill_(0)\nfor x in inputs:\n    print(x.sum().data[0], rnn(x).sum().data[0])\nrnn.temp[-1].sum().backward()\nfor k, p in rnn.parameter_dict().items():\n    print(k, p.grad.sum())", "body": "Calling the backward function does not necessarily raise an error when inplace operations are made. This leads to incorrect gradients. Below is an example with an LSTM that computes the gates with and without inplace operations.\n\n```\nclass LSTM(nn.Container):\n\n    def __init__(self, input_dim, hidden_dim):\n        super(LSTM, self).__init__(\n            i2h=nn.Linear(input_dim, 4 * hidden_dim),\n            h2h=nn.Linear(hidden_dim, 4 * hidden_dim)\n        )\n        self.c0 = Variable(torch.zeros(hidden_dim), requires_grad=True)\n        self.h0 = Variable(torch.zeros(hidden_dim), requires_grad=True)\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.reset()\n\n    def forward(self, x):\n        last_c, last_h = self.temp\n        if self.step == 0:\n            last_c = last_c.view(1, self.hidden_dim).expand(x.size(0), self.hidden_dim)\n            last_h = last_h.view(1, self.hidden_dim).expand(x.size(0), self.hidden_dim)\n        gates = self.i2h(x) + self.h2h(last_h)\n\n        ## works\n#         i_t = nn.Sigmoid()(gates[:, :self.hidden_dim])\n#         f_t = nn.Sigmoid()(gates[:, self.hidden_dim:2 * self.hidden_dim])\n#         o_t = nn.Sigmoid()(gates[:, 2 * self.hidden_dim:3 * self.hidden_dim])\n#         cell_input = nn.Tanh()(gates[:, 3 * self.hidden_dim:])\n\n        ## works, same results than above, as expected\n#         i_t, f_t, o_t = gates[:, :3 * self.hidden_dim].sigmoid().chunk(3, 1)\n#         cell_input = gates[:, 3 * self.hidden_dim:].tanh()\n\n        ## fails, although backward runs\n        gates[:, :3 * self.hidden_dim].sigmoid_()\n        gates[:, 3 * self.hidden_dim:].tanh_()\n        i_t, f_t, o_t, cell_input = gates.chunk(4, 1)\n\n        next_c = f_t * last_c + i_t * cell_input\n        next_h = o_t * next_c.tanh()\n        self.temp = [next_c, next_h]\n        self.step += 1\n        return next_h\n\n    def reset(self):\n        self.step = 0\n        self.temp = [self.c0, self.h0]\n\ntorch.manual_seed(0)\nrnn = LSTM(3, 4)\ninputs = [Variable(torch.FloatTensor(2, 3).normal_()) for _ in range(3)]\n\n# code below doesn't always output the same thing when LSTM uses inplace operations\nfor k, p in rnn.parameter_dict().items():\n    p.grad.fill_(0)\nfor x in inputs:\n    print(x.sum().data[0], rnn(x).sum().data[0])\nrnn.temp[-1].sum().backward()\nfor k, p in rnn.parameter_dict().items():\n    print(k, p.grad.sum())\n```\n"}