{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150981942", "pull_request_review_id": 76616555, "id": 150981942, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDk4MTk0Mg==", "diff_hunk": "@@ -139,7 +140,6 @@ CopySlices::CopySlices(const Variable& base_var, TensorGeometry view, std::share\n   // to base instead of the view.\n   next_functions.resize(fn->next_functions.size());\n   next_functions[0] = std::make_pair(base_var.grad_fn(), base_var.output_nr());\n-  fn->next_functions[0] = next_functions[0];", "path": "torch/csrc/autograd/functions/tensor.cpp", "position": 24, "original_position": 24, "commit_id": "903209d964374b1e257559080f3dda3de150df2f", "original_commit_id": "6af829219cb4602df7d83be35baa96e7ada25a1c", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "No, the fn->next_functions are set correctly before it's passed to CopySlices. For example, `elu_` looks something like:\r\n\r\n```c++\r\nTensor VariableType::elu_(Tensor & self, Scalar alpha) const {\r\n    profiler::RecordFunction profiler(\"elu_\");\r\n    auto& self_ = unpack(self, \"self\", 0);\r\n    check_inplace(self);\r\n    std::shared_ptr<EluBackward> grad_fn;\r\n    auto flags = compute_flags({ self });\r\n    if (flags.requires_grad) {\r\n      grad_fn = std::make_shared<EluBackward>();\r\n      grad_fn->is_executable = true;\r\n      grad_fn->next_functions = compute_next_functions({ self });\r\n      grad_fn->alpha = alpha;\r\n    }\r\n   ...\r\n}\r\n```\r\n\r\nThe line `grad_fn->next_functions = ...` is the relevant one here.\r\n\r\nIf \"self\" is a view, then next_functions will point to something like AsStridedBackward or ViewBackward, which is what we want.\r\n\r\nThe test `test_elu_inplace_view` and `test_relu_inplace_view` tests this, since they save inputs or outputs. If the grad_fn's are incorrect, the double-backwards tests fail, which was previously the case.", "created_at": "2017-11-14T22:22:45Z", "updated_at": "2018-11-23T15:36:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/3679#discussion_r150981942", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3679", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150981942"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3679#discussion_r150981942"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3679"}}, "body_html": "<p>No, the fn-&gt;next_functions are set correctly before it's passed to CopySlices. For example, <code>elu_</code> looks something like:</p>\n<div class=\"highlight highlight-source-c++\"><pre>Tensor <span class=\"pl-en\">VariableType::elu_</span>(Tensor &amp; self, Scalar alpha) <span class=\"pl-k\">const</span> {\n    profiler::RecordFunction <span class=\"pl-smi\">profiler</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elu_<span class=\"pl-pds\">\"</span></span>);\n    <span class=\"pl-k\">auto</span>&amp; self_ = <span class=\"pl-c1\">unpack</span>(self, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>self<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">0</span>);\n    <span class=\"pl-c1\">check_inplace</span>(self);\n    std::shared_ptr&lt;EluBackward&gt; grad_fn;\n    <span class=\"pl-k\">auto</span> flags = <span class=\"pl-c1\">compute_flags</span>({ self });\n    <span class=\"pl-k\">if</span> (flags.<span class=\"pl-smi\">requires_grad</span>) {\n      grad_fn = std::make_shared&lt;EluBackward&gt;();\n      grad_fn-&gt;<span class=\"pl-smi\">is_executable</span> = <span class=\"pl-c1\">true</span>;\n      grad_fn-&gt;<span class=\"pl-smi\">next_functions</span> = <span class=\"pl-c1\">compute_next_functions</span>({ self });\n      grad_fn-&gt;<span class=\"pl-smi\">alpha</span> = alpha;\n    }\n   ...\n}</pre></div>\n<p>The line <code>grad_fn-&gt;next_functions = ...</code> is the relevant one here.</p>\n<p>If \"self\" is a view, then next_functions will point to something like AsStridedBackward or ViewBackward, which is what we want.</p>\n<p>The test <code>test_elu_inplace_view</code> and <code>test_relu_inplace_view</code> tests this, since they save inputs or outputs. If the grad_fn's are incorrect, the double-backwards tests fail, which was previously the case.</p>", "body_text": "No, the fn->next_functions are set correctly before it's passed to CopySlices. For example, elu_ looks something like:\nTensor VariableType::elu_(Tensor & self, Scalar alpha) const {\n    profiler::RecordFunction profiler(\"elu_\");\n    auto& self_ = unpack(self, \"self\", 0);\n    check_inplace(self);\n    std::shared_ptr<EluBackward> grad_fn;\n    auto flags = compute_flags({ self });\n    if (flags.requires_grad) {\n      grad_fn = std::make_shared<EluBackward>();\n      grad_fn->is_executable = true;\n      grad_fn->next_functions = compute_next_functions({ self });\n      grad_fn->alpha = alpha;\n    }\n   ...\n}\nThe line grad_fn->next_functions = ... is the relevant one here.\nIf \"self\" is a view, then next_functions will point to something like AsStridedBackward or ViewBackward, which is what we want.\nThe test test_elu_inplace_view and test_relu_inplace_view tests this, since they save inputs or outputs. If the grad_fn's are incorrect, the double-backwards tests fail, which was previously the case.", "in_reply_to_id": 150940503}