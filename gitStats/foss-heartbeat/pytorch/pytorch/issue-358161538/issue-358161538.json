{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11389", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11389/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11389/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11389/events", "html_url": "https://github.com/pytorch/pytorch/issues/11389", "id": 358161538, "node_id": "MDU6SXNzdWUzNTgxNjE1Mzg=", "number": 11389, "title": "[distributions] Torch distribution samplers slow on expanded parameters", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-07T18:08:15Z", "updated_at": "2018-09-10T17:27:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>We use expanded tensors as distribution parameters in many cases where we dynamically broadcast the parameters at runtime. While working on a related (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357782489\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11341\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11341/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11341\">#11341</a>) PR, I noticed that sampling can be slow when using expanded tensors as distribution parameters. I have narrowed this slowdown to the native torch samplers.</p>\n<p>While this is likely expected behavior, it raises the question of:<br>\n(a) whether we should be doing anything inside of distributions to ensure that parameter tensors are contiguous, and if so,<br>\n(b) under what conditions should we ensure contiguity - always by default, or have it be controllable by the user via an optional keyword argument. If we use the same instance to draw multiple samples, it is worth the one time cost of calling <code>.contiguous</code> on the distribution parameters (I think, given the relatively low overhead of <code>.contiguous</code>, we can probably make it the default).</p>\n<h2>Profiling code</h2>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">36</span>]: m1, s1 <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10000</span>), torch.ones(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10000</span>)\n\nIn [<span class=\"pl-c1\">37</span>]: m2, s2 <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">10000</span>).expand([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10000</span>]), torch.ones(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10000</span>).expand([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10000</span>])\n\nIn [<span class=\"pl-c1\">38</span>]: m3, s3 <span class=\"pl-k\">=</span> m2.contiguous(), s2.contiguous()\n\nIn [<span class=\"pl-c1\">39</span>]: m1.is_contiguous(), m2.is_contiguous(), m3.is_contiguous()\nOut[<span class=\"pl-c1\">39</span>]: (<span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">False</span>, <span class=\"pl-c1\">True</span>)\n\nIn [<span class=\"pl-c1\">40</span>]: <span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>n <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">10</span> torch.normal(m1, s1)\n<span class=\"pl-c1\">1.53</span> ms \u00b1 <span class=\"pl-c1\">47</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">10</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\nIn [<span class=\"pl-c1\">41</span>]: <span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>n <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">10</span> torch.normal(m2, s2)\n<span class=\"pl-c1\">1.98</span> ms \u00b1 <span class=\"pl-c1\">18</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">10</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\nIn [<span class=\"pl-c1\">42</span>]: <span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>n <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">10</span> torch.normal(m3, s3)\n<span class=\"pl-c1\">1.5</span> ms \u00b1 <span class=\"pl-c1\">36.6</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">10</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\nIn [<span class=\"pl-c1\">43</span>]: <span class=\"pl-k\">%</span>timeit m2.contiguous()\n<span class=\"pl-c1\">134</span> \u00b5s \u00b1 <span class=\"pl-c1\">1.25</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10000</span> loops each)</pre></div>\n<p>cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23639302\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vishwakftw\">@vishwakftw</a></p>", "body_text": "Issue description\nWe use expanded tensors as distribution parameters in many cases where we dynamically broadcast the parameters at runtime. While working on a related (#11341) PR, I noticed that sampling can be slow when using expanded tensors as distribution parameters. I have narrowed this slowdown to the native torch samplers.\nWhile this is likely expected behavior, it raises the question of:\n(a) whether we should be doing anything inside of distributions to ensure that parameter tensors are contiguous, and if so,\n(b) under what conditions should we ensure contiguity - always by default, or have it be controllable by the user via an optional keyword argument. If we use the same instance to draw multiple samples, it is worth the one time cost of calling .contiguous on the distribution parameters (I think, given the relatively low overhead of .contiguous, we can probably make it the default).\nProfiling code\nIn [36]: m1, s1 = torch.ones(2, 10, 10000), torch.ones(2, 10, 10000)\n\nIn [37]: m2, s2 = torch.ones(10000).expand([2, 10, 10000]), torch.ones(10, 10000).expand([2, 10, 10000])\n\nIn [38]: m3, s3 = m2.contiguous(), s2.contiguous()\n\nIn [39]: m1.is_contiguous(), m2.is_contiguous(), m3.is_contiguous()\nOut[39]: (True, False, True)\n\nIn [40]: %timeit -n 1000 -r 10 torch.normal(m1, s1)\n1.53 ms \u00b1 47 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 1000 loops each)\n\nIn [41]: %timeit -n 1000 -r 10 torch.normal(m2, s2)\n1.98 ms \u00b1 18 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 1000 loops each)\n\nIn [42]: %timeit -n 1000 -r 10 torch.normal(m3, s3)\n1.5 ms \u00b1 36.6 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 1000 loops each)\n\nIn [43]: %timeit m2.contiguous()\n134 \u00b5s \u00b1 1.25 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\ncc. @fritzo, @vishwakftw", "body": "## Issue description\r\nWe use expanded tensors as distribution parameters in many cases where we dynamically broadcast the parameters at runtime. While working on a related (https://github.com/pytorch/pytorch/pull/11341) PR, I noticed that sampling can be slow when using expanded tensors as distribution parameters. I have narrowed this slowdown to the native torch samplers.\r\n\r\nWhile this is likely expected behavior, it raises the question of:\r\n (a) whether we should be doing anything inside of distributions to ensure that parameter tensors are contiguous, and if so,\r\n (b) under what conditions should we ensure contiguity - always by default, or have it be controllable by the user via an optional keyword argument. If we use the same instance to draw multiple samples, it is worth the one time cost of calling `.contiguous` on the distribution parameters (I think, given the relatively low overhead of `.contiguous`, we can probably make it the default). \r\n\r\n## Profiling code\r\n```python\r\nIn [36]: m1, s1 = torch.ones(2, 10, 10000), torch.ones(2, 10, 10000)\r\n\r\nIn [37]: m2, s2 = torch.ones(10000).expand([2, 10, 10000]), torch.ones(10, 10000).expand([2, 10, 10000])\r\n\r\nIn [38]: m3, s3 = m2.contiguous(), s2.contiguous()\r\n\r\nIn [39]: m1.is_contiguous(), m2.is_contiguous(), m3.is_contiguous()\r\nOut[39]: (True, False, True)\r\n\r\nIn [40]: %timeit -n 1000 -r 10 torch.normal(m1, s1)\r\n1.53 ms \u00b1 47 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 1000 loops each)\r\n\r\nIn [41]: %timeit -n 1000 -r 10 torch.normal(m2, s2)\r\n1.98 ms \u00b1 18 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 1000 loops each)\r\n\r\nIn [42]: %timeit -n 1000 -r 10 torch.normal(m3, s3)\r\n1.5 ms \u00b1 36.6 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 1000 loops each)\r\n\r\nIn [43]: %timeit m2.contiguous()\r\n134 \u00b5s \u00b1 1.25 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\ncc. @fritzo, @vishwakftw \r\n"}