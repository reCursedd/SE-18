{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/311455482", "html_url": "https://github.com/pytorch/pytorch/pull/1792#issuecomment-311455482", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1792", "id": 311455482, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTQ1NTQ4Mg==", "user": {"login": "esube", "id": 7385894, "node_id": "MDQ6VXNlcjczODU4OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7385894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/esube", "html_url": "https://github.com/esube", "followers_url": "https://api.github.com/users/esube/followers", "following_url": "https://api.github.com/users/esube/following{/other_user}", "gists_url": "https://api.github.com/users/esube/gists{/gist_id}", "starred_url": "https://api.github.com/users/esube/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/esube/subscriptions", "organizations_url": "https://api.github.com/users/esube/orgs", "repos_url": "https://api.github.com/users/esube/repos", "events_url": "https://api.github.com/users/esube/events{/privacy}", "received_events_url": "https://api.github.com/users/esube/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T19:08:33Z", "updated_at": "2017-06-27T19:09:10Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=687194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alykhantejani\">@alykhantejani</a><br>\nSorry for the edited response above. I have changed the class and the function so that the forward accepts three inputs as I described above and I have it working locally. It would be nice to have this either in your PR as correction or on a separate PR if you think it is completely different.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">WBCELoss</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>(WBCELoss, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.size_average <span class=\"pl-k\">=</span> size_average\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-k\">return</span> binary_cross_entropy_with_logits(<span class=\"pl-c1\">input</span>, target, weight, <span class=\"pl-c1\">self</span>.size_average)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">binary_cross_entropy_with_logits</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n   \n    <span class=\"pl-k\">if</span> weight <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> weight.dim() <span class=\"pl-k\">!=</span> target.dim() <span class=\"pl-k\">and</span> target.dim() <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">1</span>:\n        weight <span class=\"pl-k\">=</span> weight.view(<span class=\"pl-c1\">1</span>, target.size(<span class=\"pl-c1\">1</span>)).expand_as(target)\n\n    neg_abs <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">input</span>.abs()\n    loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">input</span> <span class=\"pl-k\">*</span> target <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">+</span> neg_abs.exp()).log()\n\n    <span class=\"pl-k\">if</span> weight <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        loss <span class=\"pl-k\">=</span> loss <span class=\"pl-k\">*</span> weight\n\n    <span class=\"pl-k\">if</span> size_average:\n        <span class=\"pl-k\">return</span> loss.mean()\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> loss.sum()</pre></div>\n<p>Note: this expects the dataset loader to supply a minibatch of input, target and weight tensors to the loss function.</p>", "body_text": "@alykhantejani\nSorry for the edited response above. I have changed the class and the function so that the forward accepts three inputs as I described above and I have it working locally. It would be nice to have this either in your PR as correction or on a separate PR if you think it is completely different.\nclass WBCELoss(nn.Module):\n    def __init__(self, size_average=True):\n        super(WBCELoss, self).__init__()\n        self.size_average = size_average\n\n    def forward(self, input, target, weight=None):\n        return binary_cross_entropy_with_logits(input, target, weight, self.size_average)\ndef binary_cross_entropy_with_logits(input, target, weight=None, size_average=True):\n   \n    if weight is not None and weight.dim() != target.dim() and target.dim() != 1:\n        weight = weight.view(1, target.size(1)).expand_as(target)\n\n    neg_abs = - input.abs()\n    loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n\n    if weight is not None:\n        loss = loss * weight\n\n    if size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\nNote: this expects the dataset loader to supply a minibatch of input, target and weight tensors to the loss function.", "body": "@alykhantejani \r\nSorry for the edited response above. I have changed the class and the function so that the forward accepts three inputs as I described above and I have it working locally. It would be nice to have this either in your PR as correction or on a separate PR if you think it is completely different. \r\n\r\n```python\r\nclass WBCELoss(nn.Module):\r\n    def __init__(self, size_average=True):\r\n        super(WBCELoss, self).__init__()\r\n        self.size_average = size_average\r\n\r\n    def forward(self, input, target, weight=None):\r\n        return binary_cross_entropy_with_logits(input, target, weight, self.size_average)\r\n```\r\n\r\n```python\r\ndef binary_cross_entropy_with_logits(input, target, weight=None, size_average=True):\r\n   \r\n    if weight is not None and weight.dim() != target.dim() and target.dim() != 1:\r\n        weight = weight.view(1, target.size(1)).expand_as(target)\r\n\r\n    neg_abs = - input.abs()\r\n    loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\r\n\r\n    if weight is not None:\r\n        loss = loss * weight\r\n\r\n    if size_average:\r\n        return loss.mean()\r\n    else:\r\n        return loss.sum()\r\n```\r\nNote: this expects the dataset loader to supply a minibatch of input, target and weight tensors to the loss function.\r\n"}