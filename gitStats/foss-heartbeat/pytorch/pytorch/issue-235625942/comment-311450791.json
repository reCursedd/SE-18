{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/311450791", "html_url": "https://github.com/pytorch/pytorch/pull/1792#issuecomment-311450791", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1792", "id": 311450791, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTQ1MDc5MQ==", "user": {"login": "esube", "id": 7385894, "node_id": "MDQ6VXNlcjczODU4OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7385894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/esube", "html_url": "https://github.com/esube", "followers_url": "https://api.github.com/users/esube/followers", "following_url": "https://api.github.com/users/esube/following{/other_user}", "gists_url": "https://api.github.com/users/esube/gists{/gist_id}", "starred_url": "https://api.github.com/users/esube/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/esube/subscriptions", "organizations_url": "https://api.github.com/users/esube/orgs", "repos_url": "https://api.github.com/users/esube/repos", "events_url": "https://api.github.com/users/esube/events{/privacy}", "received_events_url": "https://api.github.com/users/esube/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T18:50:56Z", "updated_at": "2017-06-27T19:11:14Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=687194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alykhantejani\">@alykhantejani</a><br>\nThe following:</p>\n<div class=\"highlight highlight-source-python\"><pre>output <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.Tensor(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>).uniform_())\ntarget <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.Tensor(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">10</span>).uniform_())\nweight <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">10</span>).uniform_()\nloss_fn <span class=\"pl-k\">=</span> nn.BCEWithLogitsLoss(weight)\nloss <span class=\"pl-k\">=</span> loss_fn(output, target)</pre></div>\n<p>fails with error code:</p>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;stdin&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python2.7/site-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">206</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>\n    result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python2.7/site-packages/torch/nn/modules/loss.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">232</span>, <span class=\"pl-k\">in</span> forward\n    <span class=\"pl-k\">return</span> F.binary_cross_entropy_with_logits(<span class=\"pl-c1\">input</span>, target, Variable(<span class=\"pl-c1\">self</span>.weight), <span class=\"pl-c1\">self</span>.size_average)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python2.7/site-packages/torch/nn/functional.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">659</span>, <span class=\"pl-k\">in</span> binary_cross_entropy_with_logits\n    weight <span class=\"pl-k\">=</span> weight.view(<span class=\"pl-c1\">1</span>, target.size(<span class=\"pl-c1\">1</span>)).expand_as(target)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python2.7/site-packages/torch/autograd/variable.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">479</span>, <span class=\"pl-k\">in</span> view\n    <span class=\"pl-k\">return</span> View.apply(<span class=\"pl-c1\">self</span>, sizes)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python2.7/site-packages/torch/autograd/_functions/tensor.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">96</span>, <span class=\"pl-k\">in</span> forward\n    result <span class=\"pl-k\">=</span> i.view(<span class=\"pl-k\">*</span>sizes)\n<span class=\"pl-c1\">RuntimeError</span>: size <span class=\"pl-s\"><span class=\"pl-pds\">'</span>[1 x 10]<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">is</span> invalid <span class=\"pl-k\">for</span> <span class=\"pl-c1\">input</span> of <span class=\"pl-k\">with</span> <span class=\"pl-c1\">20</span> elements at pytorch<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span><span class=\"pl-c1\">TH</span><span class=\"pl-k\">/</span>THStorage.c:<span class=\"pl-c1\">41</span></pre></div>\n<p>In multi-label (not multi-class) classification tasks, there are conditions that you want 2D weight for 2D target vector to weigh each label of each sample separately such as in the case of extreme imbalance in positive and negative examples of multi-label learning.</p>\n<p>Note: I am not saying this loss function would work out of the box even if you fix the check as the weight is passed to the initializer than the forward function in the pytorch loss functions paradigm (the weight multiplication would fail). However, I locally have the function that it accepts batch of weights in the forward function just like the targets and perform the weight multiplication. I have this locally working and if needed can do a PR.</p>", "body_text": "@alykhantejani\nThe following:\noutput = torch.autograd.Variable(torch.Tensor(2, 10).uniform_())\ntarget = torch.autograd.Variable(torch.Tensor(2, 10).uniform_())\nweight = torch.Tensor(2,10).uniform_()\nloss_fn = nn.BCEWithLogitsLoss(weight)\nloss = loss_fn(output, target)\nfails with error code:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"python2.7/site-packages/torch/nn/modules/loss.py\", line 232, in forward\n    return F.binary_cross_entropy_with_logits(input, target, Variable(self.weight), self.size_average)\n  File \"python2.7/site-packages/torch/nn/functional.py\", line 659, in binary_cross_entropy_with_logits\n    weight = weight.view(1, target.size(1)).expand_as(target)\n  File \"python2.7/site-packages/torch/autograd/variable.py\", line 479, in view\n    return View.apply(self, sizes)\n  File \"python2.7/site-packages/torch/autograd/_functions/tensor.py\", line 96, in forward\n    result = i.view(*sizes)\nRuntimeError: size '[1 x 10]' is invalid for input of with 20 elements at pytorch/torch/lib/TH/THStorage.c:41\nIn multi-label (not multi-class) classification tasks, there are conditions that you want 2D weight for 2D target vector to weigh each label of each sample separately such as in the case of extreme imbalance in positive and negative examples of multi-label learning.\nNote: I am not saying this loss function would work out of the box even if you fix the check as the weight is passed to the initializer than the forward function in the pytorch loss functions paradigm (the weight multiplication would fail). However, I locally have the function that it accepts batch of weights in the forward function just like the targets and perform the weight multiplication. I have this locally working and if needed can do a PR.", "body": "@alykhantejani \r\nThe following:\r\n```python\r\noutput = torch.autograd.Variable(torch.Tensor(2, 10).uniform_())\r\ntarget = torch.autograd.Variable(torch.Tensor(2, 10).uniform_())\r\nweight = torch.Tensor(2,10).uniform_()\r\nloss_fn = nn.BCEWithLogitsLoss(weight)\r\nloss = loss_fn(output, target)\r\n```\r\n\r\nfails with error code:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"python2.7/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"python2.7/site-packages/torch/nn/modules/loss.py\", line 232, in forward\r\n    return F.binary_cross_entropy_with_logits(input, target, Variable(self.weight), self.size_average)\r\n  File \"python2.7/site-packages/torch/nn/functional.py\", line 659, in binary_cross_entropy_with_logits\r\n    weight = weight.view(1, target.size(1)).expand_as(target)\r\n  File \"python2.7/site-packages/torch/autograd/variable.py\", line 479, in view\r\n    return View.apply(self, sizes)\r\n  File \"python2.7/site-packages/torch/autograd/_functions/tensor.py\", line 96, in forward\r\n    result = i.view(*sizes)\r\nRuntimeError: size '[1 x 10]' is invalid for input of with 20 elements at pytorch/torch/lib/TH/THStorage.c:41\r\n```\r\n\r\nIn multi-label (not multi-class) classification tasks, there are conditions that you want 2D weight for 2D target vector to weigh each label of each sample separately such as in the case of extreme imbalance in positive and negative examples of multi-label learning.\r\n\r\nNote: I am not saying this loss function would work out of the box even if you fix the check as the weight is passed to the initializer than the forward function in the pytorch loss functions paradigm (the weight multiplication would fail). However, I locally have the function that it accepts batch of weights in the forward function just like the targets and perform the weight multiplication. I have this locally working and if needed can do a PR."}