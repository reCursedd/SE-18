{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12603", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12603/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12603/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12603/events", "html_url": "https://github.com/pytorch/pytorch/issues/12603", "id": 369444409, "node_id": "MDU6SXNzdWUzNjk0NDQ0MDk=", "number": 12603, "title": "DistributedDataParallel does not get the same grad at all.", "user": {"login": "acgtyrant", "id": 3921062, "node_id": "MDQ6VXNlcjM5MjEwNjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3921062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acgtyrant", "html_url": "https://github.com/acgtyrant", "followers_url": "https://api.github.com/users/acgtyrant/followers", "following_url": "https://api.github.com/users/acgtyrant/following{/other_user}", "gists_url": "https://api.github.com/users/acgtyrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/acgtyrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acgtyrant/subscriptions", "organizations_url": "https://api.github.com/users/acgtyrant/orgs", "repos_url": "https://api.github.com/users/acgtyrant/repos", "events_url": "https://api.github.com/users/acgtyrant/events{/privacy}", "received_events_url": "https://api.github.com/users/acgtyrant/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-10-12T08:11:43Z", "updated_at": "2018-11-14T02:41:02Z", "closed_at": "2018-11-14T02:41:02Z", "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>The docs notes that torch.nn.DistributedDataParallel will all_reduce all gradients at all machines so that all gradients are same. However, I use the <code>torch.nn.Conv2d()</code> and the <code>torch.nn.DistributedDataParallel(torch.nn.Conv2d()</code> at the same time, I find their grads are not same.</p>\n<h2>To Reproduce</h2>\n<pre><code>import os\nimport types\n\nimport pytest\nimport torch\nfrom torch.multiprocessing import Process\n\n\ndef init_processes(x, conv_weight_distributed, rank):\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    os.environ['RANK'] = str(rank)\n    os.environ['WORLD_SIZE'] = '2'\n    torch.cuda.set_device(rank)\n    torch.distributed.init_process_group(backend='nccl')\n    channels = x.size(1)\n    conv_distributed = torch.nn.parallel.DistributedDataParallel(\n            torch.nn.Conv2d(channels, channels, 1).cuda(),\n            device_ids=[rank])\n    conv_distributed.module.weight = torch.nn.Parameter(\n            conv_weight_distributed)\n    torch.nn.init.constant_(conv_distributed.module.bias, 0)\n    y = conv_distributed(x)\n    y.mean().backward()\n    # it to the dict manually\n    state_dict = {\n            'x': x,\n            'x_grad': x.grad,\n            'conv': conv_distributed.module,\n            'conv_weight_grad': conv_distributed.module.weight.grad,\n            'conv_bias_grad': conv_distributed.module.bias.grad,\n            'y': y,\n    }\n    torch.save(state_dict, '{}.pth'.format(rank))\n\n\n@pytest.fixture(scope='module')\ndef conv_instances():\n    torch.multiprocessing.set_start_method('spawn')\n\n    x_local = torch.randn(\n            (4, 2, 1, 1), device='cuda:0', requires_grad=True)\n    x_distributed = x_local.detach().cuda().requires_grad_(True)\n\n    channels = x_local.size(1)\n    conv_local = torch.nn.Conv2d(channels, channels, 1).cuda()\n    torch.nn.init.constant_(conv_local.bias, 0)\n    y_local = conv_local(x_local)\n    y_local.mean().backward()\n\n    WORLD_SIZE = 2\n    step = x_local.size(0) // WORLD_SIZE\n    x_0 = x_distributed.detach(\n            )[:step, :, :, :].clone().cuda(0).requires_grad_(True)\n    x_1 = x_distributed.detach(\n            )[step:, :, :, :].clone().cuda(1).requires_grad_(True)\n    conv_weight_distributed_0 = conv_local.weight.detach(\n            ).clone().cuda(0)\n    conv_weight_distributed_1 = conv_local.weight.detach(\n            ).clone().cuda(1)\n    process_1 = Process(\n            target=init_processes,\n            args=(x_0, conv_weight_distributed_0, 0))\n    process_2 = Process(\n            target=init_processes,\n            args=(x_1, conv_weight_distributed_1, 1))\n    process_1.start()\n    process_2.start()\n\n    process_1.join()\n    process_2.join()\n    conv_instance_local = types.SimpleNamespace()\n    conv_instance_local.x = x_local\n    conv_instance_local.conv = conv_local\n    conv_instance_local.y = y_local\n    conv_instance_distributed_0 = types.SimpleNamespace(\n            **torch.load('0.pth'))\n    conv_instance_distributed_1 = types.SimpleNamespace(\n            **torch.load('1.pth', map_location={'cuda:1': 'cuda:0'}))\n    yield (\n            conv_instance_local,\n            conv_instance_distributed_0,\n            conv_instance_distributed_1,\n    )\n    os.remove('0.pth')\n    os.remove('1.pth')\n\n\ndef allclose(x, y):\n    adiff = (x - y).abs()\n    if (y == 0).all():\n        rdiff = 'NaN'\n    else:\n        rdiff = (adiff / y).abs().max()\n    assert torch.allclose(x, y), (\n            'Tensor close check failed\\n'\n            '{}\\n'\n            '{}\\n'\n            'adiff={}\\n'\n            'rdiff={}\\n'\n    ).format(x, y, adiff, rdiff)\n\n\ndef test_x(conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    x_distributed = torch.cat(\n            (distributed_0.x, distributed_1.x))\n    allclose(local.x, x_distributed)\n\n\ndef test_parameters(conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    a = list(local.conv.parameters())\n    b = list(distributed_0.conv.parameters())\n    c = list(distributed_0.conv.parameters())\n    for x, y in zip(a, b):\n        allclose(x, y)\n    for x, y in zip(a, c):\n        allclose(x, y)\n\n\ndef test_conv_weight_grad_between_local_and_distributed(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            local.conv.weight.grad,\n            distributed_0.conv_weight_grad)\n\n\ndef test_conv_weight_grad_between_distributeds(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            distributed_0.conv_weight_grad,\n            distributed_1.conv_weight_grad)\n\n\ndef test_conv_bias_grad_between_local_and_distributed(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            local.conv.bias.grad,\n            distributed_0.conv_bias_grad)\n\n\ndef test_conv_bias_grad_between_distributeds(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            distributed_0.conv_bias_grad,\n            distributed_1.conv_bias_grad)\n\n\ndef test_y(conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    y_distributed = torch.cat((distributed_0.y, distributed_1.y))\n    allclose(local.y, y_distributed)\n</code></pre>\n<p>Name the above code as <code>test_conv2d.py</code> and put it in a virtual enviroment which has installed <code>torch</code> and <code>pytest</code>, run <code>pytest</code>, the later reports <code>x</code> and all parameters are same, but their gradients are not same because <code>test_conv_weight_grad_between_distributeds</code> and <code>test_conv_weight_grad_between_local_and_distributed</code> failed.</p>\n<h2>Expected behavior</h2>\n<p>All grads are same so that ll tests functions passed.</p>\n<h2>Environment</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 1.0.0.dev20181002\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.1 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN XPlease copy and paste the output from our\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\n(or fill out the checklist below manually).\n\nNvidia driver version: 390.77\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-3f9a723f.so.6.0.21\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n</code></pre>", "body_text": "\ud83d\udc1b Bug\nThe docs notes that torch.nn.DistributedDataParallel will all_reduce all gradients at all machines so that all gradients are same. However, I use the torch.nn.Conv2d() and the torch.nn.DistributedDataParallel(torch.nn.Conv2d() at the same time, I find their grads are not same.\nTo Reproduce\nimport os\nimport types\n\nimport pytest\nimport torch\nfrom torch.multiprocessing import Process\n\n\ndef init_processes(x, conv_weight_distributed, rank):\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    os.environ['RANK'] = str(rank)\n    os.environ['WORLD_SIZE'] = '2'\n    torch.cuda.set_device(rank)\n    torch.distributed.init_process_group(backend='nccl')\n    channels = x.size(1)\n    conv_distributed = torch.nn.parallel.DistributedDataParallel(\n            torch.nn.Conv2d(channels, channels, 1).cuda(),\n            device_ids=[rank])\n    conv_distributed.module.weight = torch.nn.Parameter(\n            conv_weight_distributed)\n    torch.nn.init.constant_(conv_distributed.module.bias, 0)\n    y = conv_distributed(x)\n    y.mean().backward()\n    # it to the dict manually\n    state_dict = {\n            'x': x,\n            'x_grad': x.grad,\n            'conv': conv_distributed.module,\n            'conv_weight_grad': conv_distributed.module.weight.grad,\n            'conv_bias_grad': conv_distributed.module.bias.grad,\n            'y': y,\n    }\n    torch.save(state_dict, '{}.pth'.format(rank))\n\n\n@pytest.fixture(scope='module')\ndef conv_instances():\n    torch.multiprocessing.set_start_method('spawn')\n\n    x_local = torch.randn(\n            (4, 2, 1, 1), device='cuda:0', requires_grad=True)\n    x_distributed = x_local.detach().cuda().requires_grad_(True)\n\n    channels = x_local.size(1)\n    conv_local = torch.nn.Conv2d(channels, channels, 1).cuda()\n    torch.nn.init.constant_(conv_local.bias, 0)\n    y_local = conv_local(x_local)\n    y_local.mean().backward()\n\n    WORLD_SIZE = 2\n    step = x_local.size(0) // WORLD_SIZE\n    x_0 = x_distributed.detach(\n            )[:step, :, :, :].clone().cuda(0).requires_grad_(True)\n    x_1 = x_distributed.detach(\n            )[step:, :, :, :].clone().cuda(1).requires_grad_(True)\n    conv_weight_distributed_0 = conv_local.weight.detach(\n            ).clone().cuda(0)\n    conv_weight_distributed_1 = conv_local.weight.detach(\n            ).clone().cuda(1)\n    process_1 = Process(\n            target=init_processes,\n            args=(x_0, conv_weight_distributed_0, 0))\n    process_2 = Process(\n            target=init_processes,\n            args=(x_1, conv_weight_distributed_1, 1))\n    process_1.start()\n    process_2.start()\n\n    process_1.join()\n    process_2.join()\n    conv_instance_local = types.SimpleNamespace()\n    conv_instance_local.x = x_local\n    conv_instance_local.conv = conv_local\n    conv_instance_local.y = y_local\n    conv_instance_distributed_0 = types.SimpleNamespace(\n            **torch.load('0.pth'))\n    conv_instance_distributed_1 = types.SimpleNamespace(\n            **torch.load('1.pth', map_location={'cuda:1': 'cuda:0'}))\n    yield (\n            conv_instance_local,\n            conv_instance_distributed_0,\n            conv_instance_distributed_1,\n    )\n    os.remove('0.pth')\n    os.remove('1.pth')\n\n\ndef allclose(x, y):\n    adiff = (x - y).abs()\n    if (y == 0).all():\n        rdiff = 'NaN'\n    else:\n        rdiff = (adiff / y).abs().max()\n    assert torch.allclose(x, y), (\n            'Tensor close check failed\\n'\n            '{}\\n'\n            '{}\\n'\n            'adiff={}\\n'\n            'rdiff={}\\n'\n    ).format(x, y, adiff, rdiff)\n\n\ndef test_x(conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    x_distributed = torch.cat(\n            (distributed_0.x, distributed_1.x))\n    allclose(local.x, x_distributed)\n\n\ndef test_parameters(conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    a = list(local.conv.parameters())\n    b = list(distributed_0.conv.parameters())\n    c = list(distributed_0.conv.parameters())\n    for x, y in zip(a, b):\n        allclose(x, y)\n    for x, y in zip(a, c):\n        allclose(x, y)\n\n\ndef test_conv_weight_grad_between_local_and_distributed(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            local.conv.weight.grad,\n            distributed_0.conv_weight_grad)\n\n\ndef test_conv_weight_grad_between_distributeds(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            distributed_0.conv_weight_grad,\n            distributed_1.conv_weight_grad)\n\n\ndef test_conv_bias_grad_between_local_and_distributed(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            local.conv.bias.grad,\n            distributed_0.conv_bias_grad)\n\n\ndef test_conv_bias_grad_between_distributeds(\n        conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    allclose(\n            distributed_0.conv_bias_grad,\n            distributed_1.conv_bias_grad)\n\n\ndef test_y(conv_instances):\n    local, distributed_0, distributed_1 = conv_instances\n    y_distributed = torch.cat((distributed_0.y, distributed_1.y))\n    allclose(local.y, y_distributed)\n\nName the above code as test_conv2d.py and put it in a virtual enviroment which has installed torch and pytest, run pytest, the later reports x and all parameters are same, but their gradients are not same because test_conv_weight_grad_between_distributeds and test_conv_weight_grad_between_local_and_distributed failed.\nExpected behavior\nAll grads are same so that ll tests functions passed.\nEnvironment\nCollecting environment information...\nPyTorch version: 1.0.0.dev20181002\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.1 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN XPlease copy and paste the output from our\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\n(or fill out the checklist below manually).\n\nNvidia driver version: 390.77\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-3f9a723f.so.6.0.21\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "## \ud83d\udc1b Bug\r\n\r\nThe docs notes that torch.nn.DistributedDataParallel will all_reduce all gradients at all machines so that all gradients are same. However, I use the `torch.nn.Conv2d()` and the `torch.nn.DistributedDataParallel(torch.nn.Conv2d()` at the same time, I find their grads are not same.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport os\r\nimport types\r\n\r\nimport pytest\r\nimport torch\r\nfrom torch.multiprocessing import Process\r\n\r\n\r\ndef init_processes(x, conv_weight_distributed, rank):\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    os.environ['RANK'] = str(rank)\r\n    os.environ['WORLD_SIZE'] = '2'\r\n    torch.cuda.set_device(rank)\r\n    torch.distributed.init_process_group(backend='nccl')\r\n    channels = x.size(1)\r\n    conv_distributed = torch.nn.parallel.DistributedDataParallel(\r\n            torch.nn.Conv2d(channels, channels, 1).cuda(),\r\n            device_ids=[rank])\r\n    conv_distributed.module.weight = torch.nn.Parameter(\r\n            conv_weight_distributed)\r\n    torch.nn.init.constant_(conv_distributed.module.bias, 0)\r\n    y = conv_distributed(x)\r\n    y.mean().backward()\r\n    # it to the dict manually\r\n    state_dict = {\r\n            'x': x,\r\n            'x_grad': x.grad,\r\n            'conv': conv_distributed.module,\r\n            'conv_weight_grad': conv_distributed.module.weight.grad,\r\n            'conv_bias_grad': conv_distributed.module.bias.grad,\r\n            'y': y,\r\n    }\r\n    torch.save(state_dict, '{}.pth'.format(rank))\r\n\r\n\r\n@pytest.fixture(scope='module')\r\ndef conv_instances():\r\n    torch.multiprocessing.set_start_method('spawn')\r\n\r\n    x_local = torch.randn(\r\n            (4, 2, 1, 1), device='cuda:0', requires_grad=True)\r\n    x_distributed = x_local.detach().cuda().requires_grad_(True)\r\n\r\n    channels = x_local.size(1)\r\n    conv_local = torch.nn.Conv2d(channels, channels, 1).cuda()\r\n    torch.nn.init.constant_(conv_local.bias, 0)\r\n    y_local = conv_local(x_local)\r\n    y_local.mean().backward()\r\n\r\n    WORLD_SIZE = 2\r\n    step = x_local.size(0) // WORLD_SIZE\r\n    x_0 = x_distributed.detach(\r\n            )[:step, :, :, :].clone().cuda(0).requires_grad_(True)\r\n    x_1 = x_distributed.detach(\r\n            )[step:, :, :, :].clone().cuda(1).requires_grad_(True)\r\n    conv_weight_distributed_0 = conv_local.weight.detach(\r\n            ).clone().cuda(0)\r\n    conv_weight_distributed_1 = conv_local.weight.detach(\r\n            ).clone().cuda(1)\r\n    process_1 = Process(\r\n            target=init_processes,\r\n            args=(x_0, conv_weight_distributed_0, 0))\r\n    process_2 = Process(\r\n            target=init_processes,\r\n            args=(x_1, conv_weight_distributed_1, 1))\r\n    process_1.start()\r\n    process_2.start()\r\n\r\n    process_1.join()\r\n    process_2.join()\r\n    conv_instance_local = types.SimpleNamespace()\r\n    conv_instance_local.x = x_local\r\n    conv_instance_local.conv = conv_local\r\n    conv_instance_local.y = y_local\r\n    conv_instance_distributed_0 = types.SimpleNamespace(\r\n            **torch.load('0.pth'))\r\n    conv_instance_distributed_1 = types.SimpleNamespace(\r\n            **torch.load('1.pth', map_location={'cuda:1': 'cuda:0'}))\r\n    yield (\r\n            conv_instance_local,\r\n            conv_instance_distributed_0,\r\n            conv_instance_distributed_1,\r\n    )\r\n    os.remove('0.pth')\r\n    os.remove('1.pth')\r\n\r\n\r\ndef allclose(x, y):\r\n    adiff = (x - y).abs()\r\n    if (y == 0).all():\r\n        rdiff = 'NaN'\r\n    else:\r\n        rdiff = (adiff / y).abs().max()\r\n    assert torch.allclose(x, y), (\r\n            'Tensor close check failed\\n'\r\n            '{}\\n'\r\n            '{}\\n'\r\n            'adiff={}\\n'\r\n            'rdiff={}\\n'\r\n    ).format(x, y, adiff, rdiff)\r\n\r\n\r\ndef test_x(conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    x_distributed = torch.cat(\r\n            (distributed_0.x, distributed_1.x))\r\n    allclose(local.x, x_distributed)\r\n\r\n\r\ndef test_parameters(conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    a = list(local.conv.parameters())\r\n    b = list(distributed_0.conv.parameters())\r\n    c = list(distributed_0.conv.parameters())\r\n    for x, y in zip(a, b):\r\n        allclose(x, y)\r\n    for x, y in zip(a, c):\r\n        allclose(x, y)\r\n\r\n\r\ndef test_conv_weight_grad_between_local_and_distributed(\r\n        conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    allclose(\r\n            local.conv.weight.grad,\r\n            distributed_0.conv_weight_grad)\r\n\r\n\r\ndef test_conv_weight_grad_between_distributeds(\r\n        conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    allclose(\r\n            distributed_0.conv_weight_grad,\r\n            distributed_1.conv_weight_grad)\r\n\r\n\r\ndef test_conv_bias_grad_between_local_and_distributed(\r\n        conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    allclose(\r\n            local.conv.bias.grad,\r\n            distributed_0.conv_bias_grad)\r\n\r\n\r\ndef test_conv_bias_grad_between_distributeds(\r\n        conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    allclose(\r\n            distributed_0.conv_bias_grad,\r\n            distributed_1.conv_bias_grad)\r\n\r\n\r\ndef test_y(conv_instances):\r\n    local, distributed_0, distributed_1 = conv_instances\r\n    y_distributed = torch.cat((distributed_0.y, distributed_1.y))\r\n    allclose(local.y, y_distributed)\r\n```\r\n\r\nName the above code as `test_conv2d.py` and put it in a virtual enviroment which has installed `torch` and `pytest`, run `pytest`, the later reports `x` and all parameters are same, but their gradients are not same because `test_conv_weight_grad_between_distributeds` and `test_conv_weight_grad_between_local_and_distributed` failed.\r\n\r\n## Expected behavior\r\n\r\nAll grads are same so that ll tests functions passed.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181002\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN XPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nNvidia driver version: 390.77\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-3f9a723f.so.6.0.21\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```"}