{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427724069", "html_url": "https://github.com/pytorch/pytorch/issues/12383#issuecomment-427724069", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12383", "id": 427724069, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzcyNDA2OQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-08T05:19:41Z", "updated_at": "2018-10-08T05:19:41Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I think it's a bug that the TensorOptions returned by Tensor::options don't include requires_grad, thanks for reporting that.</p>\n</blockquote>\n<p>Actually, I'm not sure... say you write something like this:</p>\n<pre><code>void f(Tensor x) {\n  auto y = at::zeros({2,2}, x.options());\n  do_stuff_to_it_mutably(y);\n  return x + y;\n}\n</code></pre>\n<p>You almost definitely didn't want to make <code>y</code> a leaf variable that accumulates grad. So <code>requires_grad</code> feels a bit different than scalar types and device, at least with the idiom here.</p>\n<p>But yes, I agree that everything would be better if Tensor and Variable merge in C++, and we're working on making this happen. You will be a much happier panda if you just use variables exclusively.</p>", "body_text": "I think it's a bug that the TensorOptions returned by Tensor::options don't include requires_grad, thanks for reporting that.\n\nActually, I'm not sure... say you write something like this:\nvoid f(Tensor x) {\n  auto y = at::zeros({2,2}, x.options());\n  do_stuff_to_it_mutably(y);\n  return x + y;\n}\n\nYou almost definitely didn't want to make y a leaf variable that accumulates grad. So requires_grad feels a bit different than scalar types and device, at least with the idiom here.\nBut yes, I agree that everything would be better if Tensor and Variable merge in C++, and we're working on making this happen. You will be a much happier panda if you just use variables exclusively.", "body": "> I think it's a bug that the TensorOptions returned by Tensor::options don't include requires_grad, thanks for reporting that.\r\n\r\nActually, I'm not sure... say you write something like this:\r\n\r\n```\r\nvoid f(Tensor x) {\r\n  auto y = at::zeros({2,2}, x.options());\r\n  do_stuff_to_it_mutably(y);\r\n  return x + y;\r\n}\r\n```\r\n\r\nYou almost definitely didn't want to make `y` a leaf variable that accumulates grad. So `requires_grad` feels a bit different than scalar types and device, at least with the idiom here.\r\n\r\nBut yes, I agree that everything would be better if Tensor and Variable merge in C++, and we're working on making this happen. You will be a much happier panda if you just use variables exclusively."}