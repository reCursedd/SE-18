{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388294713", "html_url": "https://github.com/pytorch/pytorch/issues/4186#issuecomment-388294713", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4186", "id": 388294713, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODI5NDcxMw==", "user": {"login": "mratsim", "id": 22738317, "node_id": "MDQ6VXNlcjIyNzM4MzE3", "avatar_url": "https://avatars3.githubusercontent.com/u/22738317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mratsim", "html_url": "https://github.com/mratsim", "followers_url": "https://api.github.com/users/mratsim/followers", "following_url": "https://api.github.com/users/mratsim/following{/other_user}", "gists_url": "https://api.github.com/users/mratsim/gists{/gist_id}", "starred_url": "https://api.github.com/users/mratsim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mratsim/subscriptions", "organizations_url": "https://api.github.com/users/mratsim/orgs", "repos_url": "https://api.github.com/users/mratsim/repos", "events_url": "https://api.github.com/users/mratsim/events{/privacy}", "received_events_url": "https://api.github.com/users/mratsim/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-11T08:15:20Z", "updated_at": "2018-05-11T09:46:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20233731\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mingfeima\">@mingfeima</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Here is my analysis on softmax/logsoftmax optimization. Taken from <a href=\"https://github.com/pytorch/pytorch/blob/a015d579ddce90831bb945bbefdd1aa1a3633f63/aten/src/ATen/native/SoftMax.cpp#L25-L77\">here</a>.</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">template</span>&lt;<span class=\"pl-k\">typename</span> <span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-k\">bool</span> LogSoftMax&gt;<span class=\"pl-c\"><span class=\"pl-c\">//</span>, template&lt;typename&gt; class Epilogue&gt;</span>\n<span class=\"pl-k\">void</span> <span class=\"pl-en\">host_softmax</span>(Tensor output, <span class=\"pl-k\">const</span> Tensor &amp; input_, <span class=\"pl-k\">const</span> <span class=\"pl-c1\">int64_t</span> dim_){\n  <span class=\"pl-k\">auto</span> input = input_.<span class=\"pl-c1\">contiguous</span>();\n  <span class=\"pl-c1\">int64_t</span> dim = <span class=\"pl-c1\">maybe_wrap_dim</span>(dim_, input.<span class=\"pl-c1\">dim</span>());\n  <span class=\"pl-k\">if</span> (input.<span class=\"pl-c1\">dim</span>() == <span class=\"pl-c1\">0</span>) input = input.<span class=\"pl-c1\">view</span>(<span class=\"pl-c1\">1</span>);\n  <span class=\"pl-c1\">AT_CHECK</span>(dim &gt;=<span class=\"pl-c1\">0</span> &amp;&amp; dim &lt; input.<span class=\"pl-c1\">dim</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim must be non-negative and less than input dimensions<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-c1\">uint64_t</span> outer_size = <span class=\"pl-c1\">1</span>;\n  <span class=\"pl-c1\">uint64_t</span> dim_size = input.<span class=\"pl-c1\">size</span>(dim);\n  <span class=\"pl-c1\">uint64_t</span> inner_size = <span class=\"pl-c1\">1</span>;\n  <span class=\"pl-k\">for</span> (<span class=\"pl-c1\">int64_t</span> i = <span class=\"pl-c1\">0</span>; i &lt; dim; ++i)\n    outer_size *= input.<span class=\"pl-c1\">size</span>(i);\n  <span class=\"pl-k\">for</span> (<span class=\"pl-c1\">int64_t</span> i = dim + <span class=\"pl-c1\">1</span>; i &lt; input.<span class=\"pl-c1\">dim</span>(); ++i)\n    inner_size *= input.<span class=\"pl-c1\">size</span>(i);\n  <span class=\"pl-c1\">uint64_t</span> dim_stride = inner_size;\n  <span class=\"pl-c1\">uint64_t</span> outer_stride = dim_size * dim_stride; \n  <span class=\"pl-c1\">scalar_t</span> *input_data_base  = input.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;();\n  <span class=\"pl-c1\">scalar_t</span> *output_data_base = output.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;();\n  LOG_SOFTMAX_SIZE_TYPE i, d;\n#<span class=\"pl-k\">pragma</span> omp parallel for private(i, d)\n  <span class=\"pl-k\">for</span> (i = <span class=\"pl-c1\">0</span>; i &lt; <span class=\"pl-c1\">LOG_SOFTMAX_CAST_TYPE</span> (outer_size * inner_size); i++)\n  {\n    <span class=\"pl-c1\">uint64_t</span> outer_idx = i / inner_size;  <span class=\"pl-c\"><span class=\"pl-c\">//</span> costly division</span>\n    <span class=\"pl-c1\">uint64_t</span> inner_idx = i % inner_size;\n    <span class=\"pl-c1\">scalar_t</span> *input_data  = input_data_base  + outer_idx * outer_stride + inner_idx;\n    <span class=\"pl-c1\">scalar_t</span> *output_data = output_data_base + outer_idx * outer_stride + inner_idx;\n\n    <span class=\"pl-c1\">scalar_t</span> max_input = -std::numeric_limits&lt;<span class=\"pl-c1\">scalar_t</span>&gt;::<span class=\"pl-c1\">max</span>();\n    <span class=\"pl-k\">for</span> (d = <span class=\"pl-c1\">0</span>; d &lt; LOG_SOFTMAX_CAST_TYPE dim_size; d++)       <span class=\"pl-c\"><span class=\"pl-c\">//</span> 1st loop over the data</span>\n      max_input = <span class=\"pl-c1\">std::max</span>(max_input, input_data[d * dim_stride]);\n\n    <span class=\"pl-k\">using</span> <span class=\"pl-c1\">accscalar_t</span> =  acc_type&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">false</span>&gt;; \n    <span class=\"pl-c1\">accscalar_t</span> tmpsum = <span class=\"pl-c1\">0</span>;\n    <span class=\"pl-k\">for</span> (d = <span class=\"pl-c1\">0</span>; d &lt; LOG_SOFTMAX_CAST_TYPE dim_size; d++){    <span class=\"pl-c\"><span class=\"pl-c\">//</span> 2nd loop over the data</span>\n      <span class=\"pl-c1\">scalar_t</span> z = <span class=\"pl-c1\">std::exp</span>(input_data[d * dim_stride] - max_input);\n      tmpsum += z;\n      <span class=\"pl-k\">if</span> (!LogSoftMax){ <span class=\"pl-c\"><span class=\"pl-c\">//</span> branching in a very tight loop</span>\n        output_data[d*dim_stride] = z;\n      }\n    }\n  \n    <span class=\"pl-k\">if</span> (LogSoftMax) <span class=\"pl-c\"><span class=\"pl-c\">//</span> branching in a tight loop</span>\n       tmpsum = max_input + <span class=\"pl-c1\">std::log</span>(tmpsum);\n    <span class=\"pl-k\">else</span>\n       tmpsum = <span class=\"pl-c1\">1</span> / tmpsum;\n\n    <span class=\"pl-k\">for</span> (d = <span class=\"pl-c1\">0</span>; d &lt; LOG_SOFTMAX_CAST_TYPE dim_size; d++)  <span class=\"pl-c\"><span class=\"pl-c\">//</span> 3rd loop over the data</span>\n      <span class=\"pl-k\">if</span> (LogSoftMax) <span class=\"pl-c\"><span class=\"pl-c\">//</span> branching in a very tight loop</span>\n         output_data[d * dim_stride] = input_data[d * dim_stride] - tmpsum;\n      <span class=\"pl-k\">else</span>\n         output_data[d * dim_stride] *= tmpsum; \n  }\n\n}</pre></div>\n<p>In the main OpenMP loop:</p>\n<ul>\n<li>you have 3 loops over the data</li>\n<li>branching in tight loops</li>\n<li>division/modulo which are quite expensive.</li>\n</ul>\n<p>Since logsoftmax/softmax is known at compile-time that branching can be templatized.<br>\nAlso you can compute the max and sum of exp in a streaming fashion (one pass similar to streaming mean/variance) <a href=\"http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html\" rel=\"nofollow\">as proposed in this blog post</a>. This is especially beneficial for language modelling where vocabulary size is huge.</p>\n<p>In my own tensor library, using NCHW layout, this is the resulting code, it is at least 25% faster than 3 for loops. Code is <a href=\"https://nim-lang.org/\" rel=\"nofollow\">in Nim</a> (a compiled-to-C/C++ and Javascript language with the syntax similar to Python).</p>\n<div class=\"highlight highlight-source-nim\"><pre><span class=\"pl-k\">proc</span> <span class=\"pl-en\">streaming_max_sumexp</span><span class=\"pl-k\">*</span>[T](t: <span class=\"pl-c1\">Tensor</span>[T])<span class=\"pl-k\">:</span> <span class=\"pl-k\">tuple</span>[max:T, sumexp: T] {.<span class=\"pl-e\">noSideEffect</span>, <span class=\"pl-e\">inline</span>.}<span class=\"pl-k\">=</span>\n  result.max = -<span class=\"pl-c1\">Inf</span>.T   <span class=\"pl-c\"><span class=\"pl-c\"># </span>will store the streaming max of the tensor</span>\n  result.sumexp = <span class=\"pl-c1\">0</span>.T   <span class=\"pl-c\"><span class=\"pl-c\"># </span>will store the streaming sum of exp of the tensor</span>\n\n  <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> t:\n    <span class=\"pl-k\">if</span> x &lt;= result.max:\n      result.sumexp += <span class=\"pl-c1\">exp</span>(x - result.max)\n    <span class=\"pl-k\">else</span>:\n      result.sumexp = result.sumexp * <span class=\"pl-c1\">exp</span>(result.max - x) + <span class=\"pl-c1\">1</span>\n      result.max = x\n\n<span class=\"pl-k\">proc</span> <span class=\"pl-en\">stable_softmax</span><span class=\"pl-k\">*</span>[T](x, max, sumexp: T)<span class=\"pl-k\">:</span> T {.<span class=\"pl-e\">noSideEffect</span>, <span class=\"pl-e\">inline</span>.}<span class=\"pl-k\">=</span>\n  <span class=\"pl-c\"><span class=\"pl-c\"># </span>Numerically stable streaming softmax helper</span>\n  result = <span class=\"pl-c1\">exp</span>(x - max) / sumexp\n\n<span class=\"pl-k\">proc</span> <span class=\"pl-en\">softmax</span><span class=\"pl-k\">*</span>[T](input: <span class=\"pl-c1\">Tensor</span>[T])<span class=\"pl-k\">:</span> <span class=\"pl-c1\">Tensor</span>[T] {.<span class=\"pl-e\">noInit</span>.} <span class=\"pl-k\">=</span>\n  <span class=\"pl-c\">## For each sample in a tensor:</span>\n  <span class=\"pl-c\">##   do an exponential normalization of each of its class features xi</span>\n  <span class=\"pl-c\">##   ``exp(xi) / \u2211i exp(xi)``</span>\n  <span class=\"pl-c\">##</span>\n  <span class=\"pl-c\">## Input:</span>\n  <span class=\"pl-c\">##   - A tensor of shape [batch_size, number_of_classes]</span>\n  <span class=\"pl-c\">## Output:</span>\n  <span class=\"pl-c\">##   - A tensor of shape [batch_size, number_of_classes]</span>\n\n  <span class=\"pl-k\">let</span> batch_size = input.shape[<span class=\"pl-c1\">0</span>]\n  result = <span class=\"pl-c1\">zeros_like</span>(input)\n\n  <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">0</span>||(batch_size-<span class=\"pl-c1\">1</span>)<span class=\"pl-k\">:</span> <span class=\"pl-c\"><span class=\"pl-c\"># </span>|| is a OpenMP parallel loop</span>\n    <span class=\"pl-k\">let</span> (max, sumexp) = input[i, _].streaming_max_sumexp\n\n    <span class=\"pl-k\">var</span> res_slice = result[i, _]\n\n    <span class=\"pl-c1\">apply2_inline</span>(res_slice, input[i, _]):  <span class=\"pl-c\"><span class=\"pl-c\"># </span>This is an inline Nim template for parallel tensor iteration</span>\n      <span class=\"pl-c1\">stable_softmax</span>(y, max, sumexp) <span class=\"pl-c\"><span class=\"pl-c\"># </span>y refers to the second argument of the template</span></pre></div>\n<p>Reference:</p>\n<ul>\n<li><a href=\"https://github.com/mratsim/Arraymancer/blob/249218eb64476c5aec34ca15d0dcf0955fbb418c/src/nn_primitives/nnp_softmax.nim#L21-L40\">Softmax</a></li>\n<li><a href=\"https://github.com/mratsim/Arraymancer/blob/249218eb64476c5aec34ca15d0dcf0955fbb418c/src/nn_primitives/private/p_logsumexp.nim#L30-L39\">Streaming_max_sumexp</a></li>\n<li><a href=\"https://github.com/mratsim/Arraymancer/blob/249218eb64476c5aec34ca15d0dcf0955fbb418c/src/nn_primitives/private/p_logsumexp.nim#L57-L59\">Stable softmax helper</a></li>\n</ul>\n<p>Regarding logsoftmax, a fused softmax_cross_entropy similar to Tensorflow's would be much faster than cross_entropy(logsoftmax(x)) as you can do everything in fewer loops.</p>", "body_text": "@mingfeima @apaszke Here is my analysis on softmax/logsoftmax optimization. Taken from here.\ntemplate<typename scalar_t, bool LogSoftMax>//, template<typename> class Epilogue>\nvoid host_softmax(Tensor output, const Tensor & input_, const int64_t dim_){\n  auto input = input_.contiguous();\n  int64_t dim = maybe_wrap_dim(dim_, input.dim());\n  if (input.dim() == 0) input = input.view(1);\n  AT_CHECK(dim >=0 && dim < input.dim(), \"dim must be non-negative and less than input dimensions\");\n  uint64_t outer_size = 1;\n  uint64_t dim_size = input.size(dim);\n  uint64_t inner_size = 1;\n  for (int64_t i = 0; i < dim; ++i)\n    outer_size *= input.size(i);\n  for (int64_t i = dim + 1; i < input.dim(); ++i)\n    inner_size *= input.size(i);\n  uint64_t dim_stride = inner_size;\n  uint64_t outer_stride = dim_size * dim_stride; \n  scalar_t *input_data_base  = input.data<scalar_t>();\n  scalar_t *output_data_base = output.data<scalar_t>();\n  LOG_SOFTMAX_SIZE_TYPE i, d;\n#pragma omp parallel for private(i, d)\n  for (i = 0; i < LOG_SOFTMAX_CAST_TYPE (outer_size * inner_size); i++)\n  {\n    uint64_t outer_idx = i / inner_size;  // costly division\n    uint64_t inner_idx = i % inner_size;\n    scalar_t *input_data  = input_data_base  + outer_idx * outer_stride + inner_idx;\n    scalar_t *output_data = output_data_base + outer_idx * outer_stride + inner_idx;\n\n    scalar_t max_input = -std::numeric_limits<scalar_t>::max();\n    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++)       // 1st loop over the data\n      max_input = std::max(max_input, input_data[d * dim_stride]);\n\n    using accscalar_t =  acc_type<scalar_t, false>; \n    accscalar_t tmpsum = 0;\n    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++){    // 2nd loop over the data\n      scalar_t z = std::exp(input_data[d * dim_stride] - max_input);\n      tmpsum += z;\n      if (!LogSoftMax){ // branching in a very tight loop\n        output_data[d*dim_stride] = z;\n      }\n    }\n  \n    if (LogSoftMax) // branching in a tight loop\n       tmpsum = max_input + std::log(tmpsum);\n    else\n       tmpsum = 1 / tmpsum;\n\n    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++)  // 3rd loop over the data\n      if (LogSoftMax) // branching in a very tight loop\n         output_data[d * dim_stride] = input_data[d * dim_stride] - tmpsum;\n      else\n         output_data[d * dim_stride] *= tmpsum; \n  }\n\n}\nIn the main OpenMP loop:\n\nyou have 3 loops over the data\nbranching in tight loops\ndivision/modulo which are quite expensive.\n\nSince logsoftmax/softmax is known at compile-time that branching can be templatized.\nAlso you can compute the max and sum of exp in a streaming fashion (one pass similar to streaming mean/variance) as proposed in this blog post. This is especially beneficial for language modelling where vocabulary size is huge.\nIn my own tensor library, using NCHW layout, this is the resulting code, it is at least 25% faster than 3 for loops. Code is in Nim (a compiled-to-C/C++ and Javascript language with the syntax similar to Python).\nproc streaming_max_sumexp*[T](t: Tensor[T]): tuple[max:T, sumexp: T] {.noSideEffect, inline.}=\n  result.max = -Inf.T   # will store the streaming max of the tensor\n  result.sumexp = 0.T   # will store the streaming sum of exp of the tensor\n\n  for x in t:\n    if x <= result.max:\n      result.sumexp += exp(x - result.max)\n    else:\n      result.sumexp = result.sumexp * exp(result.max - x) + 1\n      result.max = x\n\nproc stable_softmax*[T](x, max, sumexp: T): T {.noSideEffect, inline.}=\n  # Numerically stable streaming softmax helper\n  result = exp(x - max) / sumexp\n\nproc softmax*[T](input: Tensor[T]): Tensor[T] {.noInit.} =\n  ## For each sample in a tensor:\n  ##   do an exponential normalization of each of its class features xi\n  ##   ``exp(xi) / \u2211i exp(xi)``\n  ##\n  ## Input:\n  ##   - A tensor of shape [batch_size, number_of_classes]\n  ## Output:\n  ##   - A tensor of shape [batch_size, number_of_classes]\n\n  let batch_size = input.shape[0]\n  result = zeros_like(input)\n\n  for i in 0||(batch_size-1): # || is a OpenMP parallel loop\n    let (max, sumexp) = input[i, _].streaming_max_sumexp\n\n    var res_slice = result[i, _]\n\n    apply2_inline(res_slice, input[i, _]):  # This is an inline Nim template for parallel tensor iteration\n      stable_softmax(y, max, sumexp) # y refers to the second argument of the template\nReference:\n\nSoftmax\nStreaming_max_sumexp\nStable softmax helper\n\nRegarding logsoftmax, a fused softmax_cross_entropy similar to Tensorflow's would be much faster than cross_entropy(logsoftmax(x)) as you can do everything in fewer loops.", "body": "@mingfeima @apaszke Here is my analysis on softmax/logsoftmax optimization. Taken from [here](https://github.com/pytorch/pytorch/blob/a015d579ddce90831bb945bbefdd1aa1a3633f63/aten/src/ATen/native/SoftMax.cpp#L25-L77).\r\n\r\n```C++\r\ntemplate<typename scalar_t, bool LogSoftMax>//, template<typename> class Epilogue>\r\nvoid host_softmax(Tensor output, const Tensor & input_, const int64_t dim_){\r\n  auto input = input_.contiguous();\r\n  int64_t dim = maybe_wrap_dim(dim_, input.dim());\r\n  if (input.dim() == 0) input = input.view(1);\r\n  AT_CHECK(dim >=0 && dim < input.dim(), \"dim must be non-negative and less than input dimensions\");\r\n  uint64_t outer_size = 1;\r\n  uint64_t dim_size = input.size(dim);\r\n  uint64_t inner_size = 1;\r\n  for (int64_t i = 0; i < dim; ++i)\r\n    outer_size *= input.size(i);\r\n  for (int64_t i = dim + 1; i < input.dim(); ++i)\r\n    inner_size *= input.size(i);\r\n  uint64_t dim_stride = inner_size;\r\n  uint64_t outer_stride = dim_size * dim_stride; \r\n  scalar_t *input_data_base  = input.data<scalar_t>();\r\n  scalar_t *output_data_base = output.data<scalar_t>();\r\n  LOG_SOFTMAX_SIZE_TYPE i, d;\r\n#pragma omp parallel for private(i, d)\r\n  for (i = 0; i < LOG_SOFTMAX_CAST_TYPE (outer_size * inner_size); i++)\r\n  {\r\n    uint64_t outer_idx = i / inner_size;  // costly division\r\n    uint64_t inner_idx = i % inner_size;\r\n    scalar_t *input_data  = input_data_base  + outer_idx * outer_stride + inner_idx;\r\n    scalar_t *output_data = output_data_base + outer_idx * outer_stride + inner_idx;\r\n\r\n    scalar_t max_input = -std::numeric_limits<scalar_t>::max();\r\n    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++)       // 1st loop over the data\r\n      max_input = std::max(max_input, input_data[d * dim_stride]);\r\n\r\n    using accscalar_t =  acc_type<scalar_t, false>; \r\n    accscalar_t tmpsum = 0;\r\n    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++){    // 2nd loop over the data\r\n      scalar_t z = std::exp(input_data[d * dim_stride] - max_input);\r\n      tmpsum += z;\r\n      if (!LogSoftMax){ // branching in a very tight loop\r\n        output_data[d*dim_stride] = z;\r\n      }\r\n    }\r\n  \r\n    if (LogSoftMax) // branching in a tight loop\r\n       tmpsum = max_input + std::log(tmpsum);\r\n    else\r\n       tmpsum = 1 / tmpsum;\r\n\r\n    for (d = 0; d < LOG_SOFTMAX_CAST_TYPE dim_size; d++)  // 3rd loop over the data\r\n      if (LogSoftMax) // branching in a very tight loop\r\n         output_data[d * dim_stride] = input_data[d * dim_stride] - tmpsum;\r\n      else\r\n         output_data[d * dim_stride] *= tmpsum; \r\n  }\r\n\r\n}\r\n```\r\n\r\nIn the main OpenMP loop:\r\n  - you have 3 loops over the data\r\n  - branching in tight loops\r\n  - division/modulo which are quite expensive.\r\n\r\nSince logsoftmax/softmax is known at compile-time that branching can be templatized.\r\nAlso you can compute the max and sum of exp in a streaming fashion (one pass similar to streaming mean/variance) [as proposed in this blog post](http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html). This is especially beneficial for language modelling where vocabulary size is huge.\r\n\r\nIn my own tensor library, using NCHW layout, this is the resulting code, it is at least 25% faster than 3 for loops. Code is [in Nim](https://nim-lang.org/) (a compiled-to-C/C++ and Javascript language with the syntax similar to Python).\r\n\r\n```Nim\r\nproc streaming_max_sumexp*[T](t: Tensor[T]): tuple[max:T, sumexp: T] {.noSideEffect, inline.}=\r\n  result.max = -Inf.T   # will store the streaming max of the tensor\r\n  result.sumexp = 0.T   # will store the streaming sum of exp of the tensor\r\n\r\n  for x in t:\r\n    if x <= result.max:\r\n      result.sumexp += exp(x - result.max)\r\n    else:\r\n      result.sumexp = result.sumexp * exp(result.max - x) + 1\r\n      result.max = x\r\n\r\nproc stable_softmax*[T](x, max, sumexp: T): T {.noSideEffect, inline.}=\r\n  # Numerically stable streaming softmax helper\r\n  result = exp(x - max) / sumexp\r\n\r\nproc softmax*[T](input: Tensor[T]): Tensor[T] {.noInit.} =\r\n  ## For each sample in a tensor:\r\n  ##   do an exponential normalization of each of its class features xi\r\n  ##   ``exp(xi) / \u2211i exp(xi)``\r\n  ##\r\n  ## Input:\r\n  ##   - A tensor of shape [batch_size, number_of_classes]\r\n  ## Output:\r\n  ##   - A tensor of shape [batch_size, number_of_classes]\r\n\r\n  let batch_size = input.shape[0]\r\n  result = zeros_like(input)\r\n\r\n  for i in 0||(batch_size-1): # || is a OpenMP parallel loop\r\n    let (max, sumexp) = input[i, _].streaming_max_sumexp\r\n\r\n    var res_slice = result[i, _]\r\n\r\n    apply2_inline(res_slice, input[i, _]):  # This is an inline Nim template for parallel tensor iteration\r\n      stable_softmax(y, max, sumexp) # y refers to the second argument of the template\r\n```\r\n\r\nReference:\r\n  - [Softmax](https://github.com/mratsim/Arraymancer/blob/249218eb64476c5aec34ca15d0dcf0955fbb418c/src/nn_primitives/nnp_softmax.nim#L21-L40)\r\n  - [Streaming_max_sumexp](https://github.com/mratsim/Arraymancer/blob/249218eb64476c5aec34ca15d0dcf0955fbb418c/src/nn_primitives/private/p_logsumexp.nim#L30-L39)\r\n  - [Stable softmax helper](https://github.com/mratsim/Arraymancer/blob/249218eb64476c5aec34ca15d0dcf0955fbb418c/src/nn_primitives/private/p_logsumexp.nim#L57-L59)\r\n\r\nRegarding logsoftmax, a fused softmax_cross_entropy similar to Tensorflow's would be much faster than cross_entropy(logsoftmax(x)) as you can do everything in fewer loops."}