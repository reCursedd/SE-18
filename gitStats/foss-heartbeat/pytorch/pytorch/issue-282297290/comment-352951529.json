{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/352951529", "html_url": "https://github.com/pytorch/pytorch/issues/4186#issuecomment-352951529", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4186", "id": 352951529, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Mjk1MTUyOQ==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-20T03:01:45Z", "updated_at": "2017-12-20T03:01:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> . MKLDNN does have an internal memory layout, but the intention is to optimize convolution performance by reorder memory in a cache friendly manner, rather than  break down the native layout of a framework. Ground rule is that whenever you want to read a tensor outside  MKLDNN (e.g. dump the weight), it is in BCHW layout.</p>\n<p>So clear boundaries of MKLDNN supported layer (e.g. conv2d) and unsupported layer should be defined and MKLDNN internal format only flow inside MKLDNN supported layers. If you treat CPU as an computation device as GPU, the data flow is actually similar. GPU needs to do memcpy from host/device, CPU needs to do memory reorder inside/outside MKLDNN.</p>\n<p>Feedback from recent past NIPS indicates that there might be concerns that MKLDNN internal layout is opaque. The internal layout is not opaque actually, it represents a way of blocking the memory. Taking the commonly used <a href=\"https://github.com/01org/mkl-dnn/blob/master/include/mkldnn.hpp#L565\">nChw16c</a>, 16c indicates 16 floats along the channel dim for vectorization purpose (512-bit SIMD), C = number of channels / 16, nChw dims are used for blocking of OpenMP parallel. You can also directly read an internal layout memory based on BCHW index. MKLDNN will select the best fit layout based on input tensor size, algorithms(direct, winograd, etc.) and hardware configuration (256-bit or 512-bit) to assure the best performance on a given CPU.</p>\n<p>MKLDNN full package is not available on Anaconda yet at the moment and there is no recent plan.</p>", "body_text": "Hi, @soumith . MKLDNN does have an internal memory layout, but the intention is to optimize convolution performance by reorder memory in a cache friendly manner, rather than  break down the native layout of a framework. Ground rule is that whenever you want to read a tensor outside  MKLDNN (e.g. dump the weight), it is in BCHW layout.\nSo clear boundaries of MKLDNN supported layer (e.g. conv2d) and unsupported layer should be defined and MKLDNN internal format only flow inside MKLDNN supported layers. If you treat CPU as an computation device as GPU, the data flow is actually similar. GPU needs to do memcpy from host/device, CPU needs to do memory reorder inside/outside MKLDNN.\nFeedback from recent past NIPS indicates that there might be concerns that MKLDNN internal layout is opaque. The internal layout is not opaque actually, it represents a way of blocking the memory. Taking the commonly used nChw16c, 16c indicates 16 floats along the channel dim for vectorization purpose (512-bit SIMD), C = number of channels / 16, nChw dims are used for blocking of OpenMP parallel. You can also directly read an internal layout memory based on BCHW index. MKLDNN will select the best fit layout based on input tensor size, algorithms(direct, winograd, etc.) and hardware configuration (256-bit or 512-bit) to assure the best performance on a given CPU.\nMKLDNN full package is not available on Anaconda yet at the moment and there is no recent plan.", "body": "Hi, @soumith . MKLDNN does have an internal memory layout, but the intention is to optimize convolution performance by reorder memory in a cache friendly manner, rather than  break down the native layout of a framework. Ground rule is that whenever you want to read a tensor outside  MKLDNN (e.g. dump the weight), it is in BCHW layout.\r\n\r\nSo clear boundaries of MKLDNN supported layer (e.g. conv2d) and unsupported layer should be defined and MKLDNN internal format only flow inside MKLDNN supported layers. If you treat CPU as an computation device as GPU, the data flow is actually similar. GPU needs to do memcpy from host/device, CPU needs to do memory reorder inside/outside MKLDNN.\r\n\r\nFeedback from recent past NIPS indicates that there might be concerns that MKLDNN internal layout is opaque. The internal layout is not opaque actually, it represents a way of blocking the memory. Taking the commonly used [nChw16c](https://github.com/01org/mkl-dnn/blob/master/include/mkldnn.hpp#L565), 16c indicates 16 floats along the channel dim for vectorization purpose (512-bit SIMD), C = number of channels / 16, nChw dims are used for blocking of OpenMP parallel. You can also directly read an internal layout memory based on BCHW index. MKLDNN will select the best fit layout based on input tensor size, algorithms(direct, winograd, etc.) and hardware configuration (256-bit or 512-bit) to assure the best performance on a given CPU.\r\n\r\nMKLDNN full package is not available on Anaconda yet at the moment and there is no recent plan.\r\n"}