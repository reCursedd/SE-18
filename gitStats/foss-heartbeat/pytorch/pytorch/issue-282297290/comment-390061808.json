{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390061808", "html_url": "https://github.com/pytorch/pytorch/issues/4186#issuecomment-390061808", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4186", "id": 390061808, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDA2MTgwOA==", "user": {"login": "MlWoo", "id": 20226293, "node_id": "MDQ6VXNlcjIwMjI2Mjkz", "avatar_url": "https://avatars2.githubusercontent.com/u/20226293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MlWoo", "html_url": "https://github.com/MlWoo", "followers_url": "https://api.github.com/users/MlWoo/followers", "following_url": "https://api.github.com/users/MlWoo/following{/other_user}", "gists_url": "https://api.github.com/users/MlWoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/MlWoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MlWoo/subscriptions", "organizations_url": "https://api.github.com/users/MlWoo/orgs", "repos_url": "https://api.github.com/users/MlWoo/repos", "events_url": "https://api.github.com/users/MlWoo/events{/privacy}", "received_events_url": "https://api.github.com/users/MlWoo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-18T01:03:06Z", "updated_at": "2018-05-18T01:03:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a>  If I do not miss something, the vectorization optimization is applied to softmax and logsoftmax with the last deimension on a contiguous tensor. At first, I have to admit that calculating softmax and logsoftmax with the last deimension on a contiguous tensor is frequent in classifier. So the PR LGTM.</p>\n<p>However, there are some questions to be confirmed.</p>\n<ol>\n<li>As a benchmark, perf on non-contiguous tensor maybe needs to be provided. The code shows that softmax clones the tensor to make a new contiguous one first if handling with a non-contigous tensor.  I doubt the benefit from vectorization could not cover overhead of copy in some case, not to mention comparison with streaming the operation( the method provided by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22738317\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mratsim\">@mratsim</a>).</li>\n<li>If the dimension is not the last one, there are some problems which have been pointed out by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22738317\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mratsim\">@mratsim</a>.</li>\n</ol>\n<p>So there are some room to be better on these operations. But opt further on softmax is not urgent in consideration of the practice of classifier.</p>", "body_text": "@cpuhrsch  If I do not miss something, the vectorization optimization is applied to softmax and logsoftmax with the last deimension on a contiguous tensor. At first, I have to admit that calculating softmax and logsoftmax with the last deimension on a contiguous tensor is frequent in classifier. So the PR LGTM.\nHowever, there are some questions to be confirmed.\n\nAs a benchmark, perf on non-contiguous tensor maybe needs to be provided. The code shows that softmax clones the tensor to make a new contiguous one first if handling with a non-contigous tensor.  I doubt the benefit from vectorization could not cover overhead of copy in some case, not to mention comparison with streaming the operation( the method provided by @mratsim).\nIf the dimension is not the last one, there are some problems which have been pointed out by @mratsim.\n\nSo there are some room to be better on these operations. But opt further on softmax is not urgent in consideration of the practice of classifier.", "body": "@cpuhrsch  If I do not miss something, the vectorization optimization is applied to softmax and logsoftmax with the last deimension on a contiguous tensor. At first, I have to admit that calculating softmax and logsoftmax with the last deimension on a contiguous tensor is frequent in classifier. So the PR LGTM. \r\n\r\nHowever, there are some questions to be confirmed.\r\n1.  As a benchmark, perf on non-contiguous tensor maybe needs to be provided. The code shows that softmax clones the tensor to make a new contiguous one first if handling with a non-contigous tensor.  I doubt the benefit from vectorization could not cover overhead of copy in some case, not to mention comparison with streaming the operation( the method provided by @mratsim).  \r\n2. If the dimension is not the last one, there are some problems which have been pointed out by @mratsim. \r\n\r\nSo there are some room to be better on these operations. But opt further on softmax is not urgent in consideration of the practice of classifier.  "}