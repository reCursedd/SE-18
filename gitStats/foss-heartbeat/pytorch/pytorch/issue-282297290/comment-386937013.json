{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386937013", "html_url": "https://github.com/pytorch/pytorch/issues/4186#issuecomment-386937013", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4186", "id": 386937013, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjkzNzAxMw==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-07T01:59:49Z", "updated_at": "2018-05-07T01:59:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> we only PR mkldnn convolution at the moment but actually code for lot of other operators are already in our internal branch.</p>\n<p>The real problem is that mkldnn utilizes unique data layouts for optimization purpose, what we do for the convolution layer at now is 1) reorder from bchw to internal 2) do computation 3) reorder from internal to bchw. If we reorder everytime for every operator, only convolution is beneficial using mkldnn.</p>\n<p>So from the performance point of view, it is needed to cache the memory of mkldnn layout. Do reorder only once when entering and exiting for a consecutive mkldnn calls. This is what has been done on <a href=\"https://github.com/pytorch/pytorch/tree/master/caffe2/ideep\">caffe2/ideep</a>. (ideep is a wrapper of mkldnn). However this is against <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> initial proposal of reserving BCHW layout. So i can't simply follow what has been done on caffe2 or tensorflow...</p>\n<p>I'm still investigating methods for caching the mkldnn memory, we have several plans but haven't got a perfect one.</p>\n<p>Anyway, by vectoring exp and log is good enough at the stage, won't have serious performance gap with mkldnn solutions. Once we reach an agreement of caching mkldnn memory in ATen, both fwd/bwd of softmax will be provided.</p>", "body_text": "@cpuhrsch we only PR mkldnn convolution at the moment but actually code for lot of other operators are already in our internal branch.\nThe real problem is that mkldnn utilizes unique data layouts for optimization purpose, what we do for the convolution layer at now is 1) reorder from bchw to internal 2) do computation 3) reorder from internal to bchw. If we reorder everytime for every operator, only convolution is beneficial using mkldnn.\nSo from the performance point of view, it is needed to cache the memory of mkldnn layout. Do reorder only once when entering and exiting for a consecutive mkldnn calls. This is what has been done on caffe2/ideep. (ideep is a wrapper of mkldnn). However this is against @soumith initial proposal of reserving BCHW layout. So i can't simply follow what has been done on caffe2 or tensorflow...\nI'm still investigating methods for caching the mkldnn memory, we have several plans but haven't got a perfect one.\nAnyway, by vectoring exp and log is good enough at the stage, won't have serious performance gap with mkldnn solutions. Once we reach an agreement of caching mkldnn memory in ATen, both fwd/bwd of softmax will be provided.", "body": "@cpuhrsch we only PR mkldnn convolution at the moment but actually code for lot of other operators are already in our internal branch.\r\n\r\nThe real problem is that mkldnn utilizes unique data layouts for optimization purpose, what we do for the convolution layer at now is 1) reorder from bchw to internal 2) do computation 3) reorder from internal to bchw. If we reorder everytime for every operator, only convolution is beneficial using mkldnn.\r\n\r\nSo from the performance point of view, it is needed to cache the memory of mkldnn layout. Do reorder only once when entering and exiting for a consecutive mkldnn calls. This is what has been done on [caffe2/ideep](https://github.com/pytorch/pytorch/tree/master/caffe2/ideep). (ideep is a wrapper of mkldnn). However this is against @soumith initial proposal of reserving BCHW layout. So i can't simply follow what has been done on caffe2 or tensorflow...\r\n\r\nI'm still investigating methods for caching the mkldnn memory, we have several plans but haven't got a perfect one.\r\n\r\nAnyway, by vectoring exp and log is good enough at the stage, won't have serious performance gap with mkldnn solutions. Once we reach an agreement of caching mkldnn memory in ATen, both fwd/bwd of softmax will be provided.\r\n"}