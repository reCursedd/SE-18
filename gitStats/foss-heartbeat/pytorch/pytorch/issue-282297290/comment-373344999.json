{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373344999", "html_url": "https://github.com/pytorch/pytorch/issues/4186#issuecomment-373344999", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4186", "id": 373344999, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzM0NDk5OQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-15T11:28:03Z", "updated_at": "2018-03-15T11:28:24Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> It always spawns the threads, but I think it's not so much about them (they will be idle if you run on CPU only), but more about OMP having separate thread pools for forward and backward threads.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20233731\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mingfeima\">@mingfeima</a> thanks for the update. A few notes from our side that might be relevant:</p>\n<p>I looked into <code>logsoftmax</code> a few months back and concluded that it's heavily bottlenecked by functions like <code>exp</code> and <code>log</code> that prevent vectorization (I know <code>icc</code> can do it properly, but that doesn't solve it for us). Since then we've merged vectorized implementations of those functions into ATen or TH (can't remember now), so it should be relatively easy to get an order of magnitude improvement there.</p>\n<p>We also added support for generating fused CPU kernels on the fly to the PyTorch JIT, but didn't proceed with the work because we haven't seen any real benefits from these fusions just yet. Are there really cases where you can get a 50% speedup?</p>", "body_text": "@fmassa It always spawns the threads, but I think it's not so much about them (they will be idle if you run on CPU only), but more about OMP having separate thread pools for forward and backward threads.\n@mingfeima thanks for the update. A few notes from our side that might be relevant:\nI looked into logsoftmax a few months back and concluded that it's heavily bottlenecked by functions like exp and log that prevent vectorization (I know icc can do it properly, but that doesn't solve it for us). Since then we've merged vectorized implementations of those functions into ATen or TH (can't remember now), so it should be relatively easy to get an order of magnitude improvement there.\nWe also added support for generating fused CPU kernels on the fly to the PyTorch JIT, but didn't proceed with the work because we haven't seen any real benefits from these fusions just yet. Are there really cases where you can get a 50% speedup?", "body": "@fmassa It always spawns the threads, but I think it's not so much about them (they will be idle if you run on CPU only), but more about OMP having separate thread pools for forward and backward threads.\r\n\r\n@mingfeima thanks for the update. A few notes from our side that might be relevant:\r\n\r\nI looked into `logsoftmax` a few months back and concluded that it's heavily bottlenecked by functions like `exp` and `log` that prevent vectorization (I know `icc` can do it properly, but that doesn't solve it for us). Since then we've merged vectorized implementations of those functions into ATen or TH (can't remember now), so it should be relatively easy to get an order of magnitude improvement there.\r\n\r\nWe also added support for generating fused CPU kernels on the fly to the PyTorch JIT, but didn't proceed with the work because we haven't seen any real benefits from these fusions just yet. Are there really cases where you can get a 50% speedup?"}