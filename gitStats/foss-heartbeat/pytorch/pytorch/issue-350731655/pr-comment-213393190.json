{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213393190", "pull_request_review_id": 150220379, "id": 213393190, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzM5MzE5MA==", "diff_hunk": "@@ -183,9 +183,104 @@ static void prod_kernel_impl(Tensor& result, const Tensor& self, at::optional<in\n   });\n }\n \n+template<typename scalar_t>\n+struct NormReduction {\n+  static void apply(Tensor& res, const Tensor& self, Scalar p, at::optional<int64_t> dim) {\n+    auto out_ = res.data<scalar_t>();\n+    auto data_ = self.data<scalar_t>();\n+    auto numel = self.numel();\n+    float pval = 0.0;\n+    if (p.isIntegral()){\n+      pval = p.to<int64_t>();\n+    } else if (p.isFloatingPoint()) {\n+      pval = p.to<float>();\n+    }\n+    if (!dim.has_value()) {\n+      *out_ = reduce_all(data_, numel,  pval);\n+      return;\n+    }\n+    int64_t n = self.size(*dim);\n+    int64_t stride = self.stride(*dim);\n+    // A contiguous tensor does not need to hold a meaningful stride\n+    // if the corresponding size is 1\n+    if (n == 1) {\n+      stride = 1;\n+      for (int64_t i = self.ndimension() - 1; i > *dim; i--) {\n+        stride *= self.size(i);\n+      }\n+    }\n+    int64_t batch = numel / n;\n+    parallel_for(0, batch, 1, [=](int64_t begin, int64_t end) {\n+      for (int64_t bi = begin; bi < end; bi++) {\n+        int64_t b = bi / stride;\n+        int64_t i = bi % stride;\n+        const scalar_t* data = &data_[b * n * stride + i];\n+        out_[bi] = norm_calc(data, n, stride, pval);\n+      }\n+    });\n+  }\n+\n+  static scalar_t reduce_all(const scalar_t* data_, int64_t size,  float pval) {\n+    scalar_t sum = parallel_reduce(\n+      0,\n+      size,\n+      internal::GRAIN_SIZE,\n+      (scalar_t)0,\n+      [=](int64_t begin, int64_t end, scalar_t init) {\n+        const scalar_t* data = &data_[begin];\n+        int64_t n = end - begin;\n+        scalar_t result = norm_calc(data, n, 1, pval);\n+        return result;\n+      },\n+      std::plus<scalar_t>());\n+    return sum;\n+  }\n+\n+  static scalar_t norm_calc(const scalar_t* data, int64_t n, int64_t stride, float pval) {\n+        scalar_t result = 0.0;", "path": "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "position": null, "original_position": 58, "commit_id": "01620c5d75010435810c9b85584db9ff65cf5887", "original_commit_id": "062a70a3e9f6ccc9b3b14df2a865b9d80d01e7c1", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "It seems that the looks in this function can greatly benefit from the vectorized CPU instructions. \r\n\r\nFiles under `native/cpu` are compiled multiple times, once for AVX2 enabled CPUs, once for AVX (but not AVX2) enabled CPUs, and once for CPUs with neither enabled. We provide helper class `Vec256` (see `vec256.h` and `vml.h`) to enabling writing the same code for different instruction sets. Other files under `native/cpu` (except `TensorCompareKernel`) have some examples. Maybe you can look into using that?", "created_at": "2018-08-28T16:55:34Z", "updated_at": "2018-11-23T15:50:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/10535#discussion_r213393190", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10535", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213393190"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10535#discussion_r213393190"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10535"}}, "body_html": "<p>It seems that the looks in this function can greatly benefit from the vectorized CPU instructions.</p>\n<p>Files under <code>native/cpu</code> are compiled multiple times, once for AVX2 enabled CPUs, once for AVX (but not AVX2) enabled CPUs, and once for CPUs with neither enabled. We provide helper class <code>Vec256</code> (see <code>vec256.h</code> and <code>vml.h</code>) to enabling writing the same code for different instruction sets. Other files under <code>native/cpu</code> (except <code>TensorCompareKernel</code>) have some examples. Maybe you can look into using that?</p>", "body_text": "It seems that the looks in this function can greatly benefit from the vectorized CPU instructions.\nFiles under native/cpu are compiled multiple times, once for AVX2 enabled CPUs, once for AVX (but not AVX2) enabled CPUs, and once for CPUs with neither enabled. We provide helper class Vec256 (see vec256.h and vml.h) to enabling writing the same code for different instruction sets. Other files under native/cpu (except TensorCompareKernel) have some examples. Maybe you can look into using that?"}