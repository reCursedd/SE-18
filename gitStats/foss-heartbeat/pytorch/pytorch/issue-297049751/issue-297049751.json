{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5234", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5234/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5234/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5234/events", "html_url": "https://github.com/pytorch/pytorch/issues/5234", "id": 297049751, "node_id": "MDU6SXNzdWUyOTcwNDk3NTE=", "number": 5234, "title": "Exporting RNN in ONNX", "user": {"login": "arogozhnikov", "id": 6318811, "node_id": "MDQ6VXNlcjYzMTg4MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6318811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arogozhnikov", "html_url": "https://github.com/arogozhnikov", "followers_url": "https://api.github.com/users/arogozhnikov/followers", "following_url": "https://api.github.com/users/arogozhnikov/following{/other_user}", "gists_url": "https://api.github.com/users/arogozhnikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/arogozhnikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arogozhnikov/subscriptions", "organizations_url": "https://api.github.com/users/arogozhnikov/orgs", "repos_url": "https://api.github.com/users/arogozhnikov/repos", "events_url": "https://api.github.com/users/arogozhnikov/events{/privacy}", "received_events_url": "https://api.github.com/users/arogozhnikov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-14T10:46:41Z", "updated_at": "2018-02-15T06:46:44Z", "closed_at": "2018-02-15T06:46:44Z", "author_association": "NONE", "body_html": "<p>When submitting a bug report, please include the following information (where relevant):</p>\n<ul>\n<li>OS: ubuntu 16.04 (docker)</li>\n<li>PyTorch version: 0.3.1 and 0.3.0</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: cuda 9.0, cudnn 7</li>\n<li>GPU models and configuration: &lt; irrelevant &gt;</li>\n<li>GCC version (if compiling from source): &lt; irrelevant &gt;</li>\n</ul>\n<hr>\n<p>Hi, I'm trying to export simple rnn models from pytorch to onnx. All attempts failed with</p>\n<pre><code>RuntimeError: hack_onnx_rnn NYI\n</code></pre>\n<p>Problems arise when LSTM / RNN is in the graph.<br>\nBut <a href=\"http://pytorch.org/docs/0.3.0/onnx.html\" rel=\"nofollow\">http://pytorch.org/docs/0.3.0/onnx.html</a> says that RNN module is supported.</p>\n<p>Below in a simple example based on examples/word_language_model</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.onnx\n<span class=\"pl-k\">import</span> torchvision\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> numpy\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RNNModel</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Container module with an encoder, a recurrent module, and a decoder.<span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">ntoken</span>, <span class=\"pl-smi\">ninp</span>, <span class=\"pl-smi\">nhid</span>, <span class=\"pl-smi\">nlayers</span>, <span class=\"pl-smi\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>):\n        <span class=\"pl-c1\">super</span>(RNNModel, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.drop <span class=\"pl-k\">=</span> nn.Dropout(dropout)\n        <span class=\"pl-c1\">self</span>.encoder <span class=\"pl-k\">=</span> nn.Embedding(ntoken, ninp)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>         self.rnn = nn.LSTM(ninp, nhid, num_layers=nlayers, dropout=dropout)</span>\n        <span class=\"pl-c1\">self</span>.rnn <span class=\"pl-k\">=</span> nn.RNN(ninp, nhid, <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span>nlayers, <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span>dropout)\n        <span class=\"pl-c1\">self</span>.decoder <span class=\"pl-k\">=</span> nn.Linear(nhid, ntoken)\n\n        <span class=\"pl-c1\">self</span>.init_weights()\n        <span class=\"pl-c1\">self</span>.nlayers <span class=\"pl-k\">=</span> nlayers\n        <span class=\"pl-c1\">self</span>.nhid <span class=\"pl-k\">=</span> nhid\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">init_weights</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        initrange <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>\n        <span class=\"pl-c1\">self</span>.encoder.weight.data.uniform_(<span class=\"pl-k\">-</span>initrange, initrange)\n        <span class=\"pl-c1\">self</span>.decoder.bias.data.fill_(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-c1\">self</span>.decoder.weight.data.uniform_(<span class=\"pl-k\">-</span>initrange, initrange)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">hidden</span>):\n        emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.drop(<span class=\"pl-c1\">self</span>.encoder(<span class=\"pl-c1\">input</span>))\n        output, hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.rnn(emb, hidden)\n        output <span class=\"pl-k\">=</span> emb\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.drop(output)\n        decoded <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.decoder(output.view(output.size(<span class=\"pl-c1\">0</span>)<span class=\"pl-k\">*</span>output.size(<span class=\"pl-c1\">1</span>), output.size(<span class=\"pl-c1\">2</span>)))\n        <span class=\"pl-k\">return</span> decoded.view(output.size(<span class=\"pl-c1\">0</span>), output.size(<span class=\"pl-c1\">1</span>), decoded.size(<span class=\"pl-c1\">1</span>)), hidden\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">init_hidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">batch_size</span>):\n        weight <span class=\"pl-k\">=</span> <span class=\"pl-c1\">next</span>(<span class=\"pl-c1\">self</span>.parameters()).data\n        <span class=\"pl-k\">return</span> Variable(weight.new(<span class=\"pl-c1\">self</span>.nlayers, batch_size, <span class=\"pl-c1\">self</span>.nhid).zero_())\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> return (Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_()),</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>        Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_()))</span>\n\nmodel <span class=\"pl-k\">=</span> RNNModel(<span class=\"pl-v\">ntoken</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">ninp</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">nhid</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">nlayers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n\ndummy_input <span class=\"pl-k\">=</span> Variable(torch.from_numpy(numpy.arange(<span class=\"pl-c1\">256</span>)[:, <span class=\"pl-c1\">None</span>] <span class=\"pl-k\">%</span> <span class=\"pl-c1\">64</span>))\ndummy_state <span class=\"pl-k\">=</span> model.init_hidden(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> works well</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> output = model(dummy_input, dummy_state)[0].size()</span>\n\ntorch.onnx.export(model, (dummy_input, dummy_state), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>recurrent.proto<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>", "body_text": "When submitting a bug report, please include the following information (where relevant):\n\nOS: ubuntu 16.04 (docker)\nPyTorch version: 0.3.1 and 0.3.0\nHow you installed PyTorch (conda, pip, source): pip\nPython version: 3.6\nCUDA/cuDNN version: cuda 9.0, cudnn 7\nGPU models and configuration: < irrelevant >\nGCC version (if compiling from source): < irrelevant >\n\n\nHi, I'm trying to export simple rnn models from pytorch to onnx. All attempts failed with\nRuntimeError: hack_onnx_rnn NYI\n\nProblems arise when LSTM / RNN is in the graph.\nBut http://pytorch.org/docs/0.3.0/onnx.html says that RNN module is supported.\nBelow in a simple example based on examples/word_language_model\nfrom torch.autograd import Variable\nimport torch.onnx\nimport torchvision\nimport torch.nn as nn\nimport numpy\n\nclass RNNModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n#         self.rnn = nn.LSTM(ninp, nhid, num_layers=nlayers, dropout=dropout)\n        self.rnn = nn.RNN(ninp, nhid, num_layers=nlayers, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        self.init_weights()\n        self.nlayers = nlayers\n        self.nhid = nhid\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = emb\n        output = self.drop(output)\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        return Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_())\n        # return (Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_()),\n        #        Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_()))\n\nmodel = RNNModel(ntoken=64, ninp=128, nhid=128, nlayers=2, dropout=0.5)\n\ndummy_input = Variable(torch.from_numpy(numpy.arange(256)[:, None] % 64))\ndummy_state = model.init_hidden(batch_size=1)\n\n# works well\n# output = model(dummy_input, dummy_state)[0].size()\n\ntorch.onnx.export(model, (dummy_input, dummy_state), \"recurrent.proto\", verbose=True)", "body": "When submitting a bug report, please include the following information (where relevant):\r\n- OS: ubuntu 16.04 (docker)\r\n- PyTorch version: 0.3.1 and 0.3.0\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cuda 9.0, cudnn 7\r\n- GPU models and configuration: < irrelevant >\r\n- GCC version (if compiling from source): < irrelevant >\r\n\r\n---\r\n\r\nHi, I'm trying to export simple rnn models from pytorch to onnx. All attempts failed with \r\n```\r\nRuntimeError: hack_onnx_rnn NYI\r\n```\r\n\r\nProblems arise when LSTM / RNN is in the graph.\r\nBut http://pytorch.org/docs/0.3.0/onnx.html says that RNN module is supported.\r\n\r\n\r\nBelow in a simple example based on examples/word_language_model\r\n\r\n```python\r\nfrom torch.autograd import Variable\r\nimport torch.onnx\r\nimport torchvision\r\nimport torch.nn as nn\r\nimport numpy\r\n\r\nclass RNNModel(nn.Module):\r\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\r\n\r\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\r\n        super(RNNModel, self).__init__()\r\n        self.drop = nn.Dropout(dropout)\r\n        self.encoder = nn.Embedding(ntoken, ninp)\r\n#         self.rnn = nn.LSTM(ninp, nhid, num_layers=nlayers, dropout=dropout)\r\n        self.rnn = nn.RNN(ninp, nhid, num_layers=nlayers, dropout=dropout)\r\n        self.decoder = nn.Linear(nhid, ntoken)\r\n\r\n        self.init_weights()\r\n        self.nlayers = nlayers\r\n        self.nhid = nhid\r\n\r\n    def init_weights(self):\r\n        initrange = 0.1\r\n        self.encoder.weight.data.uniform_(-initrange, initrange)\r\n        self.decoder.bias.data.fill_(0)\r\n        self.decoder.weight.data.uniform_(-initrange, initrange)\r\n\r\n    def forward(self, input, hidden):\r\n        emb = self.drop(self.encoder(input))\r\n        output, hidden = self.rnn(emb, hidden)\r\n        output = emb\r\n        output = self.drop(output)\r\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\r\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\r\n\r\n    def init_hidden(self, batch_size):\r\n        weight = next(self.parameters()).data\r\n        return Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_())\r\n        # return (Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_()),\r\n        #        Variable(weight.new(self.nlayers, batch_size, self.nhid).zero_()))\r\n\r\nmodel = RNNModel(ntoken=64, ninp=128, nhid=128, nlayers=2, dropout=0.5)\r\n\r\ndummy_input = Variable(torch.from_numpy(numpy.arange(256)[:, None] % 64))\r\ndummy_state = model.init_hidden(batch_size=1)\r\n\r\n# works well\r\n# output = model(dummy_input, dummy_state)[0].size()\r\n\r\ntorch.onnx.export(model, (dummy_input, dummy_state), \"recurrent.proto\", verbose=True)\r\n```"}