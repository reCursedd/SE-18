{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9996", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9996/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9996/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9996/events", "html_url": "https://github.com/pytorch/pytorch/issues/9996", "id": 345565451, "node_id": "MDU6SXNzdWUzNDU1NjU0NTE=", "number": 9996, "title": "Error Sharing CUDA Models between Processes using `torch.multiprocessing`", "user": {"login": "yangky11", "id": 5431913, "node_id": "MDQ6VXNlcjU0MzE5MTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5431913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangky11", "html_url": "https://github.com/yangky11", "followers_url": "https://api.github.com/users/yangky11/followers", "following_url": "https://api.github.com/users/yangky11/following{/other_user}", "gists_url": "https://api.github.com/users/yangky11/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangky11/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangky11/subscriptions", "organizations_url": "https://api.github.com/users/yangky11/orgs", "repos_url": "https://api.github.com/users/yangky11/repos", "events_url": "https://api.github.com/users/yangky11/events{/privacy}", "received_events_url": "https://api.github.com/users/yangky11/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 696160151, "node_id": "MDU6TGFiZWw2OTYxNjAxNTE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/nightly-announce", "name": "nightly-announce", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 22, "created_at": "2018-07-29T21:48:02Z", "updated_at": "2018-11-23T05:22:45Z", "closed_at": "2018-08-10T20:56:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I got a runtime error when trying to share a CUDA model with a child process.</p>\n<p>The code I was running:</p>\n<pre><code>import torch\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\n\ndef train(model):\n  print(model)\n  # do something..\n\n\nif __name__ == '__main__':\n    model = nn.Linear(10, 1)\n    model.cuda()\n    model.share_memory()\n\n    mp.set_start_method('spawn')\n    p = mp.Process(target=train, args=(model,))\n    p.start()\n    p.join()\n</code></pre>\n<p>The error traceback:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 18, in &lt;module&gt;\n    p.start()\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 26, in __init__\n    self._launch(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n    reduction.dump(process_obj, fp)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 179, in reduce_storage\n    raise RuntimeError(\"Cannot pickle CUDA storage; try pickling a CUDA tensor instead\")\nRuntimeError: Cannot pickle CUDA storage; try pickling a CUDA tensor instead\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: CentOS Linux 7 (Core)<br>\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)<br>\nCMake version: version 3.9.4</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.61<br>\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti<br>\nNvidia driver version: 384.69<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.3)<br>\n[pip] numpydoc (0.7.0)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] cuda80                    1.0                  h205658b_0    pytorch<br>\n[conda] magma-cuda80              2.3.0                         1    pytorch<br>\n[conda] pytorch                   0.4.1           py36_cuda8.0.61_cudnn7.1.2_1  [cuda80]  pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "I got a runtime error when trying to share a CUDA model with a child process.\nThe code I was running:\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\n\ndef train(model):\n  print(model)\n  # do something..\n\n\nif __name__ == '__main__':\n    model = nn.Linear(10, 1)\n    model.cuda()\n    model.share_memory()\n\n    mp.set_start_method('spawn')\n    p = mp.Process(target=train, args=(model,))\n    p.start()\n    p.join()\n\nThe error traceback:\nTraceback (most recent call last):\n  File \"test.py\", line 18, in <module>\n    p.start()\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 26, in __init__\n    self._launch(process_obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n    reduction.dump(process_obj, fp)\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n  File \"/home/yangky/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 179, in reduce_storage\n    raise RuntimeError(\"Cannot pickle CUDA storage; try pickling a CUDA tensor instead\")\nRuntimeError: Cannot pickle CUDA storage; try pickling a CUDA tensor instead\n\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\nCMake version: version 3.9.4\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\nNvidia driver version: 384.69\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] numpydoc (0.7.0)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] cuda80                    1.0                  h205658b_0    pytorch\n[conda] magma-cuda80              2.3.0                         1    pytorch\n[conda] pytorch                   0.4.1           py36_cuda8.0.61_cudnn7.1.2_1  [cuda80]  pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "\r\nI got a runtime error when trying to share a CUDA model with a child process.\r\n\r\nThe code I was running:\r\n\r\n```\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torch.nn as nn\r\n\r\n\r\ndef train(model):\r\n  print(model)\r\n  # do something..\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = nn.Linear(10, 1)\r\n    model.cuda()\r\n    model.share_memory()\r\n\r\n    mp.set_start_method('spawn')\r\n    p = mp.Process(target=train, args=(model,))\r\n    p.start()\r\n    p.join()\r\n```\r\n\r\nThe error traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    p.start()\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 26, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"/home/yangky/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 179, in reduce_storage\r\n    raise RuntimeError(\"Cannot pickle CUDA storage; try pickling a CUDA tensor instead\")\r\nRuntimeError: Cannot pickle CUDA storage; try pickling a CUDA tensor instead\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 384.69\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda80                    1.0                  h205658b_0    pytorch\r\n[conda] magma-cuda80              2.3.0                         1    pytorch\r\n[conda] pytorch                   0.4.1           py36_cuda8.0.61_cudnn7.1.2_1  [cuda80]  pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n"}