{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/364004749", "html_url": "https://github.com/pytorch/pytorch/pull/5108#issuecomment-364004749", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5108", "id": 364004749, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDAwNDc0OQ==", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T05:13:26Z", "updated_at": "2018-02-08T05:13:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> For we need to synchronize all the gradients among GPU cards for one batch, GPU with small <code> batch-size</code> would wait for the GPUs with large <code>batch-size</code> to complete.</p>\n<p>e.g. There are 4 GPUs available.</p>\n<ol>\n<li><code>batch-size=80</code>, each card actually computes <code>80/4 = 20</code> samples for one batch.</li>\n<li><code>batch-size=81</code>, we have card 0,1,2 with <code>batch-size=81/4 = 21</code>. and card 3 with <code>batch-size = 18</code>. In this case, increasing batch-size to <code>84</code> reduces <em>the number of batches</em> (total samples / batch-size) while costing the same computing time during each batch.</li>\n</ol>", "body_text": "@apaszke For we need to synchronize all the gradients among GPU cards for one batch, GPU with small  batch-size would wait for the GPUs with large batch-size to complete.\ne.g. There are 4 GPUs available.\n\nbatch-size=80, each card actually computes 80/4 = 20 samples for one batch.\nbatch-size=81, we have card 0,1,2 with batch-size=81/4 = 21. and card 3 with batch-size = 18. In this case, increasing batch-size to 84 reduces the number of batches (total samples / batch-size) while costing the same computing time during each batch.", "body": "@apaszke For we need to synchronize all the gradients among GPU cards for one batch, GPU with small ` batch-size` would wait for the GPUs with large `batch-size` to complete.\r\n\r\ne.g. There are 4 GPUs available.\r\n1. `batch-size=80`, each card actually computes `80/4 = 20` samples for one batch.\r\n2. `batch-size=81`, we have card 0,1,2 with `batch-size=81/4 = 21`. and card 3 with `batch-size = 18`. In this case, increasing batch-size to `84` reduces *the number of batches* (total samples / batch-size) while costing the same computing time during each batch."}