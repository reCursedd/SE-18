{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205462153", "pull_request_review_id": 140734705, "id": 205462153, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTQ2MjE1Mw==", "diff_hunk": "@@ -13,40 +15,71 @@ namespace torch { namespace jit {\n using value_map = std::unordered_map<Value*, Value*>;\n using value_set = std::unordered_set<Value*>;\n \n-bool hasOneValuedInput(Node *n, torch::jit::Symbol name) {\n-  auto maybe_t = n->get<at::Scalar>(name);\n-  if (!maybe_t) return false;\n-  return maybe_t->toDouble() == 1.0;\n+void wrapDim(int64_t & dim, const std::vector<int64_t> & sizes) {\n+  if (dim < 0) {\n+    dim += sizes.size();\n+  }\n }\n \n bool isDifferentiable(Node * n) {\n-  static std::unordered_set<Symbol> differentiable_kinds = {\n-    aten::add, aten::sub, aten::mul, prim::Constant,\n-    aten::sigmoid, aten::tanh, aten::mm, aten::chunk, aten::split, aten::t, aten::neg,\n-    aten::unsqueeze, aten::expand, aten::addmm, aten::gt, aten::lt, aten::eq, aten::ne, aten::ge, aten::le, aten::type_as,\n-    aten::relu, aten::exp, prim::AutogradAdd\n+  static OperatorSet differentiable_ops = {\n+    \"aten::add(Tensor self, Tensor other, *, Scalar alpha) -> Tensor\",\n+    \"aten::add(Tensor self, Scalar other, *, Scalar alpha) -> Tensor\",\n+    \"aten::sub(Tensor self, Tensor other, *, Scalar alpha) -> Tensor\",\n+    \"aten::sub(Tensor self, Scalar other, *, Scalar alpha) -> Tensor\",\n+    \"aten::mul(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::mul(Tensor self, Scalar other) -> Tensor\",\n+    \"aten::sigmoid(Tensor self) -> Tensor\",\n+    \"aten::tanh(Tensor self) -> Tensor\",\n+    \"aten::relu(Tensor self) -> Tensor\",\n+    \"aten::exp(Tensor self) -> Tensor\",\n+    \"aten::t(Tensor self) -> Tensor\",\n+    \"aten::neg(Tensor self) -> Tensor\",\n+    \"aten::chunk(Tensor self, int chunks, int dim) -> Tensor[]\",\n+    \"aten::split(Tensor self, int split_size, int dim) -> Tensor[]\",\n+    \"aten::type_as(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::unsqueeze(Tensor self, int dim) -> Tensor\",\n+    \"aten::mm(Tensor self, Tensor mat2) -> Tensor\",\n+    \"aten::lt(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::le(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::gt(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::ge(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::eq(Tensor self, Tensor other) -> Tensor\",\n+    \"aten::ne(Tensor self, Tensor other) -> Tensor\"\n   };\n-  // TODO: check this more generally via schema\n-  // This check ensures that the `alpha` and `beta` attributes on this addmm\n-  // node are constant and equivalent to 1.0\n-  if (n->kind() == aten::addmm) {\n-    if (n->inputs().size() > 3)\n-      return false;\n-    if (!hasOneValuedInput(n, attr::alpha) || !hasOneValuedInput(n, attr::beta))\n-      return false;\n-  }\n-  auto isTensor = [](Value* v) { return v->type()->isSubtypeOf(*DynamicType::get()); };\n \n-  if(!std::all_of(n->inputs().begin(), n->inputs().end(), isTensor)\n-    || !std::all_of(n->outputs().begin(), n->outputs().end(), isTensor))\n-    return false;\n+  if (n->kind() == prim::Constant || n->kind() == prim::AutogradAdd)\n+    return true;\n+  if (differentiable_ops.find(n))\n+    return true;\n \n-  if (n->kind() == aten::type_as && !n->inputs().at(1)->isTensor()) {\n-    return false;\n+  if (n->matches(\"aten::type_as(Tensor self, Tensor other) -> Tensor\")) {\n+    return n->input(1)->type()->cast<TensorType>();\n+  }\n+  if (n->matches(\"aten::cat(Tensor[] tensors, int dim) -> Tensor\")) {\n+    if (!n->is_constant(attr::dim)) return false;\n+    for (Value * input : n->inputs().slice(0, n->inputs().size() - 1)) {\n+      if (!input->type()->cast<TensorType>()) return false;\n+    }\n+    return true;\n+  }\n+  if (n->matches(\"aten::squeeze(Tensor self) -> Tensor\")) {\n+    return n->input()->type()->cast<TensorType>();\n+  }\n+  if (n->matches(\"aten::squeeze(Tensor self, int dim) -> Tensor\")) {\n+    return n->namedInput(attr::self)->type()->cast<TensorType>() && n->is_constant(attr::dim);\n+  }\n+  if (n->matches(\"aten::expand(Tensor self, int[] size, *, int implicit) -> Tensor\")) {\n+    return n->is_constant(attr::size) && n->is_constant(attr::implicit);\n+  }\n+  if (n->matches(\"aten::view(Tensor self, int[] size) -> Tensor\") ||\n+      n->matches(\"aten::reshape(Tensor self, int[] shape) -> Tensor\")) {\n+    return n->namedInput(attr::self)->type()->cast<TensorType>();\n   }\n \n   // linear blocks may appear as inputs to graph executors, but they are removed\n   // before differentiation occurs\n+  // TODO (apaszke): is this really needed?", "path": "torch/csrc/jit/autodiff.cpp", "position": null, "original_position": 102, "commit_id": "1f52f39901e6d1f1d0d263b5025388a55194ecc6", "original_commit_id": "9ee72f1547825a8e27d9ca8a7f00ce31e9741aae", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I just don't understand under what conditions we would be checking differentiability of the `GradOf` blocks. Shouldn't their lowering be one of the first passes that we would possibly run? It should happen before `CreateAutodiffSubgraphs` and `Differentiate` for sure, right?", "created_at": "2018-07-26T13:45:54Z", "updated_at": "2018-11-23T15:48:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9807#discussion_r205462153", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9807", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205462153"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9807#discussion_r205462153"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9807"}}, "body_html": "<p>I just don't understand under what conditions we would be checking differentiability of the <code>GradOf</code> blocks. Shouldn't their lowering be one of the first passes that we would possibly run? It should happen before <code>CreateAutodiffSubgraphs</code> and <code>Differentiate</code> for sure, right?</p>", "body_text": "I just don't understand under what conditions we would be checking differentiability of the GradOf blocks. Shouldn't their lowering be one of the first passes that we would possibly run? It should happen before CreateAutodiffSubgraphs and Differentiate for sure, right?", "in_reply_to_id": 205294041}