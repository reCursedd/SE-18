{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205665193", "pull_request_review_id": 140981857, "id": 205665193, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTY2NTE5Mw==", "diff_hunk": "@@ -631,57 +712,55 @@ def abs(g, self):\n \n \n def pow(g, self, exponent):\n+    exponent = _maybe_get_scalar(exponent)\n     return g.op(\"Pow\", self, _if_scalar_type_as(exponent, self), **_broadcast_if_scalar(exponent))\n \n \n+@parse_args('v', 'f', 'f')\n def clamp(g, self, min, max):\n     return g.op(\"Clip\", self, min_f=min, max_f=max)\n \n \n+@parse_args('v', 'f')\n def clamp_min(g, self, min):\n     return g.op(\"Clip\", self, min_f=min)\n \n \n+@parse_args('v', 'f')\n def clamp_max(g, self, max):\n     return g.op(\"Clip\", self, max_f=max)\n \n \n # torch.max (same for torch.min) actually has two interfaces smashed together:\n # torch.max(x, dim, keepdim) and torch.max(x, y)\n-def max(g, self, *args, **kwargs):\n-    dim = kwargs.get(\"dim\", None)\n-    if dim is None and isinstance(args[0], numbers.Number):\n-        dim = args[0]\n-    if dim is not None:\n-        keepdim = kwargs.get(\"keepdim\", False)\n+def max(g, self, dim_or_y, keepdim=None):\n+    if keepdim is None:\n+        return g.op(\"Max\", self, dim_or_y)\n+    else:\n+        dim = _get_const(dim_or_y, 'i', 'dim')\n+        keepdim = _get_const(keepdim, 'i', 'keepdim')\n         # TODO: export it as ReduceMax\n         return g.op(\"ATen\",\n                     self,\n                     operator_s=\"max\",\n                     dim_i=dim,\n                     keepdim_i=keepdim,\n                     outputs=2)\n-    else:\n-        (other,) = args\n-        return g.op(\"Max\", self, other)\n \n \n-def min(g, self, *args, **kwargs):\n-    dim = kwargs.get(\"dim\", None)\n-    if dim is None and isinstance(args[0], numbers.Number):\n-        dim = args[0]\n-    if dim is not None:\n-        keepdim = kwargs.get(\"keepdim\", False)\n-        # TODO: export it as ReduceMin\n+def min(g, self, dim_or_y, keepdim=None):", "path": "torch/onnx/symbolic.py", "position": 460, "original_position": 460, "commit_id": "1f52f39901e6d1f1d0d263b5025388a55194ecc6", "original_commit_id": "1f52f39901e6d1f1d0d263b5025388a55194ecc6", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Why doesn't this symbolic use parse args?", "created_at": "2018-07-27T04:25:37Z", "updated_at": "2018-11-23T15:48:18Z", "html_url": "https://github.com/pytorch/pytorch/pull/9807#discussion_r205665193", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9807", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205665193"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9807#discussion_r205665193"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9807"}}, "body_html": "<p>Why doesn't this symbolic use parse args?</p>", "body_text": "Why doesn't this symbolic use parse args?"}