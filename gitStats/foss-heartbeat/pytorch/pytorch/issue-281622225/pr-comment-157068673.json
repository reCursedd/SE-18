{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157068673", "pull_request_review_id": 83653182, "id": 157068673, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzA2ODY3Mw==", "diff_hunk": "@@ -372,3 +381,71 @@ def forward_extended(self, *input):\n \n     def backward_extended(self, *grad_output):\n         raise NotImplementedError\n+\n+\n+def symbolic_override(symbolic_fn):\n+    \"\"\"\n+    Decorator to override ONNX export of the a given Python function with\n+    specified subgraph.\n+\n+    Effectively allows to attach symbolic() implementation to an arbitrary\n+    python function. Requirements to the decorated function:\n+     - being non-member function\n+     - positional inputs are Variables/Tensors or (nested) lists or tuples of\n+       them (similar requirement to NestedIOFunction)\n+     - outputs are similarly Variables/Tensors or (nested) lists or tuples of\n+       them\n+     - keyword arguments are of non-tensor type\n+     - note, that function does NOT have to be autograd.Function\n+\n+    Example usage:\n+\n+    ```\n+    def symb(g, x, y):\n+        return g.op('Sum', x, y[0], y[1])\n+\n+    @symbolic_override(symb)\n+    def foo(x, y):\n+        return x + y[0] + y[1]\n+    ```\n+    \"\"\"\n+\n+    def wrapper_maker(fn):\n+\n+        def wrapper(*args, **kwargs):\n+            output = fn(*args, **kwargs)\n+            flat_args = tuple(_iter_variables(args))\n+            if not any(map(_C._jit_is_tracing, flat_args)):", "path": "torch/autograd/function.py", "position": null, "original_position": 98, "commit_id": "40309481d832c37b8e3afe14677603f408e842f2", "original_commit_id": "f7b50a881138b5d079eaf3391237137ba0734aeb", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "In general I think that would be a reasonable assumption. If we don't know some Variables we insert Constant nodes, but I don't think you can export these to ONNX (at least you probably shouldn't be by default, because it's likely that we screwed up the tracing somehow).\r\n\r\nBTW, about reducing the cost of these checks. I realized that [we actually have a global flag](https://github.com/pytorch/pytorch/blob/master/torch/jit/__init__.py#L22-L25) that is on only if any thread (likely this one) is tracing. Checking this before you start flattening is enough to ensure we don't take a hit on perf in the regular case.", "created_at": "2017-12-14T21:37:27Z", "updated_at": "2018-11-23T15:37:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/4143#discussion_r157068673", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4143", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157068673"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4143#discussion_r157068673"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4143"}}, "body_html": "<p>In general I think that would be a reasonable assumption. If we don't know some Variables we insert Constant nodes, but I don't think you can export these to ONNX (at least you probably shouldn't be by default, because it's likely that we screwed up the tracing somehow).</p>\n<p>BTW, about reducing the cost of these checks. I realized that <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/jit/__init__.py#L22-L25\">we actually have a global flag</a> that is on only if any thread (likely this one) is tracing. Checking this before you start flattening is enough to ensure we don't take a hit on perf in the regular case.</p>", "body_text": "In general I think that would be a reasonable assumption. If we don't know some Variables we insert Constant nodes, but I don't think you can export these to ONNX (at least you probably shouldn't be by default, because it's likely that we screwed up the tracing somehow).\nBTW, about reducing the cost of these checks. I realized that we actually have a global flag that is on only if any thread (likely this one) is tracing. Checking this before you start flattening is enough to ensure we don't take a hit on perf in the regular case.", "in_reply_to_id": 156673108}