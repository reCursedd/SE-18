{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6316", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6316/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6316/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6316/events", "html_url": "https://github.com/pytorch/pytorch/issues/6316", "id": 311667887, "node_id": "MDU6SXNzdWUzMTE2Njc4ODc=", "number": 6316, "title": "Batchnorm2d : ValueError: expected 2D or 3D input (got 4D input)", "user": {"login": "sameerkhurana10", "id": 7698301, "node_id": "MDQ6VXNlcjc2OTgzMDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7698301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sameerkhurana10", "html_url": "https://github.com/sameerkhurana10", "followers_url": "https://api.github.com/users/sameerkhurana10/followers", "following_url": "https://api.github.com/users/sameerkhurana10/following{/other_user}", "gists_url": "https://api.github.com/users/sameerkhurana10/gists{/gist_id}", "starred_url": "https://api.github.com/users/sameerkhurana10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sameerkhurana10/subscriptions", "organizations_url": "https://api.github.com/users/sameerkhurana10/orgs", "repos_url": "https://api.github.com/users/sameerkhurana10/repos", "events_url": "https://api.github.com/users/sameerkhurana10/events{/privacy}", "received_events_url": "https://api.github.com/users/sameerkhurana10/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-05T15:51:21Z", "updated_at": "2018-04-05T16:14:09Z", "closed_at": "2018-04-05T16:14:09Z", "author_association": "NONE", "body_html": "<pre><code>class EncoderBig(nn.Module):\n    def __init__(self, z_dim, hidden_dim):\n        super(EncoderBig, self).__init__()\n        # setup the three linear transformations used                                                                                                                             \n        self.conv1 = nn.Conv2d(1, 64, (80,1), (1,1), padding=(0,0))\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64,  64, (1, 1), (1, 1), (0, 0))\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, (1, 4), (1,2), (0,1))\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128,  128, (1, 1), (1, 1), (0, 0))\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, (1,4), (1,2), (0,1))\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, (1,1), (1,1), (0,0))\n        self.bn6 = nn.BatchNorm2d(256)\n        self.fc = nn.Linear(256*5, hidden_dim)\n        self.bn4 = nn.BatchNorm1d(hidden_dim)\n        self.mu_layer = nn.Linear(hidden_dim, z_dim)\n        self.scale_layer = nn.Linear(hidden_dim, z_dim)\n        # setup the non-linearity                                                                                                                                                 \n        self.lrelu = nn.LeakyReLU(inplace=True)\n        self.relu = nn.ReLU(inplace=True)\n        #self.softplus = nn.Softplus()\n \n    def forward(self, x):\n        # then compute the hidden units                                                                                                                                           \n        # x is NCHW                                                                                                                                                               \n        x = self.lrelu(self.bn1(self.conv1(x)))\n        print(x.shape)\n        x = self.lrelu(self.bn2(self.conv2(x)))\n        print(x.shape)\n        x = self.lrelu(self.bn3(self.conv3(x)))\n        print(x.shape)\n        x = self.conv4(x)\n        print(x.shape)\n        x = self.lrelu(self.bn4(x))\n        #x = self.lrelu(self.bn4(self.conv4(x)))                                                                                                                                  \n        print(x.shape)\n        x = self.lrelu(self.bn5(self.conv5(x)))\n        x = self.lrelu(self.bn6(self.conv6(x)))\n        x = x.view(x.size(0), 256 * 5)\n        x = self.relu(self.fc(x))\n        # then return a mean vector and a (positive) square root covariance                                                                                                       \n        # each of size batch_size x z_dim                                                                                                                                         \n        z_loc = self.mu_layer(x)\n        z_scale = torch.exp(0.5 * self.scale_layer(x))\n        return z_loc, z_scale\n    \n</code></pre>\n<p>My code is as above.</p>\n<p>I get the error when executing the instruction <code>x = self.lrelu(self.bn4(x))</code> in forward.</p>\n<p>The error is:</p>\n<p><code>ValueError: expected 2D or 3D input (got 4D input)</code></p>\n<p>Tensor shapes till the point of the error:</p>\n<pre><code>torch.Size([256, 64, 1, 20])\ntorch.Size([256, 64, 1, 20])\ntorch.Size([256, 128, 1, 10])\ntorch.Size([256, 128, 1, 10])\n</code></pre>\n<p>pytorch version <code>0.4.0a0+1807bac</code></p>\n<p>Pytorch built from source with the head at commit <code>1807bac</code></p>", "body_text": "class EncoderBig(nn.Module):\n    def __init__(self, z_dim, hidden_dim):\n        super(EncoderBig, self).__init__()\n        # setup the three linear transformations used                                                                                                                             \n        self.conv1 = nn.Conv2d(1, 64, (80,1), (1,1), padding=(0,0))\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64,  64, (1, 1), (1, 1), (0, 0))\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, (1, 4), (1,2), (0,1))\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128,  128, (1, 1), (1, 1), (0, 0))\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, (1,4), (1,2), (0,1))\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, (1,1), (1,1), (0,0))\n        self.bn6 = nn.BatchNorm2d(256)\n        self.fc = nn.Linear(256*5, hidden_dim)\n        self.bn4 = nn.BatchNorm1d(hidden_dim)\n        self.mu_layer = nn.Linear(hidden_dim, z_dim)\n        self.scale_layer = nn.Linear(hidden_dim, z_dim)\n        # setup the non-linearity                                                                                                                                                 \n        self.lrelu = nn.LeakyReLU(inplace=True)\n        self.relu = nn.ReLU(inplace=True)\n        #self.softplus = nn.Softplus()\n \n    def forward(self, x):\n        # then compute the hidden units                                                                                                                                           \n        # x is NCHW                                                                                                                                                               \n        x = self.lrelu(self.bn1(self.conv1(x)))\n        print(x.shape)\n        x = self.lrelu(self.bn2(self.conv2(x)))\n        print(x.shape)\n        x = self.lrelu(self.bn3(self.conv3(x)))\n        print(x.shape)\n        x = self.conv4(x)\n        print(x.shape)\n        x = self.lrelu(self.bn4(x))\n        #x = self.lrelu(self.bn4(self.conv4(x)))                                                                                                                                  \n        print(x.shape)\n        x = self.lrelu(self.bn5(self.conv5(x)))\n        x = self.lrelu(self.bn6(self.conv6(x)))\n        x = x.view(x.size(0), 256 * 5)\n        x = self.relu(self.fc(x))\n        # then return a mean vector and a (positive) square root covariance                                                                                                       \n        # each of size batch_size x z_dim                                                                                                                                         \n        z_loc = self.mu_layer(x)\n        z_scale = torch.exp(0.5 * self.scale_layer(x))\n        return z_loc, z_scale\n    \n\nMy code is as above.\nI get the error when executing the instruction x = self.lrelu(self.bn4(x)) in forward.\nThe error is:\nValueError: expected 2D or 3D input (got 4D input)\nTensor shapes till the point of the error:\ntorch.Size([256, 64, 1, 20])\ntorch.Size([256, 64, 1, 20])\ntorch.Size([256, 128, 1, 10])\ntorch.Size([256, 128, 1, 10])\n\npytorch version 0.4.0a0+1807bac\nPytorch built from source with the head at commit 1807bac", "body": "```\r\nclass EncoderBig(nn.Module):\r\n    def __init__(self, z_dim, hidden_dim):\r\n        super(EncoderBig, self).__init__()\r\n        # setup the three linear transformations used                                                                                                                             \r\n        self.conv1 = nn.Conv2d(1, 64, (80,1), (1,1), padding=(0,0))\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.conv2 = nn.Conv2d(64,  64, (1, 1), (1, 1), (0, 0))\r\n        self.bn2 = nn.BatchNorm2d(64)\r\n        self.conv3 = nn.Conv2d(64, 128, (1, 4), (1,2), (0,1))\r\n        self.bn3 = nn.BatchNorm2d(128)\r\n        self.conv4 = nn.Conv2d(128,  128, (1, 1), (1, 1), (0, 0))\r\n        self.bn4 = nn.BatchNorm2d(128)\r\n        self.conv5 = nn.Conv2d(128, 256, (1,4), (1,2), (0,1))\r\n        self.bn5 = nn.BatchNorm2d(256)\r\n        self.conv6 = nn.Conv2d(256, 256, (1,1), (1,1), (0,0))\r\n        self.bn6 = nn.BatchNorm2d(256)\r\n        self.fc = nn.Linear(256*5, hidden_dim)\r\n        self.bn4 = nn.BatchNorm1d(hidden_dim)\r\n        self.mu_layer = nn.Linear(hidden_dim, z_dim)\r\n        self.scale_layer = nn.Linear(hidden_dim, z_dim)\r\n        # setup the non-linearity                                                                                                                                                 \r\n        self.lrelu = nn.LeakyReLU(inplace=True)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        #self.softplus = nn.Softplus()\r\n \r\n    def forward(self, x):\r\n        # then compute the hidden units                                                                                                                                           \r\n        # x is NCHW                                                                                                                                                               \r\n        x = self.lrelu(self.bn1(self.conv1(x)))\r\n        print(x.shape)\r\n        x = self.lrelu(self.bn2(self.conv2(x)))\r\n        print(x.shape)\r\n        x = self.lrelu(self.bn3(self.conv3(x)))\r\n        print(x.shape)\r\n        x = self.conv4(x)\r\n        print(x.shape)\r\n        x = self.lrelu(self.bn4(x))\r\n        #x = self.lrelu(self.bn4(self.conv4(x)))                                                                                                                                  \r\n        print(x.shape)\r\n        x = self.lrelu(self.bn5(self.conv5(x)))\r\n        x = self.lrelu(self.bn6(self.conv6(x)))\r\n        x = x.view(x.size(0), 256 * 5)\r\n        x = self.relu(self.fc(x))\r\n        # then return a mean vector and a (positive) square root covariance                                                                                                       \r\n        # each of size batch_size x z_dim                                                                                                                                         \r\n        z_loc = self.mu_layer(x)\r\n        z_scale = torch.exp(0.5 * self.scale_layer(x))\r\n        return z_loc, z_scale\r\n    \r\n```                   \r\n\r\nMy code is as above.\r\n\r\nI get the error when executing the instruction `x = self.lrelu(self.bn4(x))` in forward.\r\n\r\nThe error is:\r\n\r\n`ValueError: expected 2D or 3D input (got 4D input)`\r\n\r\nTensor shapes till the point of the error:\r\n\r\n```\r\ntorch.Size([256, 64, 1, 20])\r\ntorch.Size([256, 64, 1, 20])\r\ntorch.Size([256, 128, 1, 10])\r\ntorch.Size([256, 128, 1, 10])\r\n```                           \r\npytorch version `0.4.0a0+1807bac`\r\n\r\nPytorch built from source with the head at commit `1807bac`                                                                                         "}