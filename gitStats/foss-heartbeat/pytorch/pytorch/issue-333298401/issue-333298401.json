{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8602", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8602/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8602/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8602/events", "html_url": "https://github.com/pytorch/pytorch/issues/8602", "id": 333298401, "node_id": "MDU6SXNzdWUzMzMyOTg0MDE=", "number": 8602, "title": "Come with a better strategy for TensorArg (error reporting)", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-18T14:38:37Z", "updated_at": "2018-06-18T15:06:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>TensorArg (in <code>aten/src/ATen/TensorUtils.h</code>) is a class I came up with when I was porting CuDNN convolutions from Python to C++. The original goals were as follows:</p>\n<ol>\n<li>Create a set of utility functions to conveniently perform input checks (e.g., does it have the correct size; is it on the correct GPU, etc.)</li>\n<li>Give good error messages when these tests fail (e.g., give the name and the argument number of the argument that failed the test)</li>\n<li>Avoid copy-pasting error checking logic.</li>\n</ol>\n<p>Goals (2) and (3) are in tension with each other, especially for convolutions. Consider the following example: <code>convolution_forward</code>, and <code>transposed_convolution_backward</code>. Algorithmically, these two functions are equivalent (and indeed, both functions backend to the same underlying function), but from a UI perspective the functions are very different (e.g., arguments names differ). If you want to not copy-paste error checking logic, you need to put this logic in the underlying backend function, but if you want good error messages you have to track extra information on a per-Tensor basis so that you can accurately describe what the name/argument position of an argument is.</p>\n<p>TensorArg solves this problem by rewriting functions to pass <code>TensorArg</code> struct instead of <code>Tensor</code>, where <code>TensorArg</code> is augmented with extra information to track the name and position of an argument. Checking functions which make use of <code>TensorArg</code> can pull this information out directly when giving an error message.</p>\n<p>However, TensorArg falls short of some other goals which I subsequently realized were quite important:</p>\n<ol>\n<li><strong>Speed.</strong> Boxing tensors into <code>TensorArg</code> structs, while fairly cheap, resulted in a perceptible, cumulative performance slowdown. Ugh! Some of the cost may be due to additional dynamic allocations. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"278338908\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3958\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3958/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3958\">#3958</a> is the tracking issue.</li>\n<li><strong>Brevity.</strong> Suppose that I have a native function <code>f</code> which backends to <code>f_out</code>, which is also native. I cannot rewrite <code>f_out</code> to be <code>TensorArg</code>, because the calling convention for native functions requires <code>Tensor</code> arguments. So I have to write another backend function <code>_f_out</code>, and then have <code>f_out</code> and <code>f</code> call into it.</li>\n<li><strong>Compositionality.</strong> Essentially, the <code>TensorArg</code> strategy means we need a \"shadow\" hierarchy of TensorArg-consuming functions, which you call directly if you want to preserve error reporting. This means that if you ever need to go back in the direct Tensor interface, you cannot preserve error information.</li>\n</ol>\n<p>Given this problem, it seems unlikely that we can recommend TensorArg for wider use in the ATen codebase, which is a shame because it is still very difficult to do good error reporting when writing code in ATen.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6429851\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goldsborough\">@goldsborough</a></p>\n<hr>\n<p>Proposal: Thread-local state error reporting information</p>\n<p>Let's get rid of <code>TensorArg</code> so that we can achieve brevity and compositionality. In this case, how can we tell what the name of an argument is? We have thread-local error reporting information, which maintains a mapping (just an association list) from <code>TensorImpl*</code> to error information. Upon a failure, we perform a lookup in the mapping to resolve a <code>Tensor</code> into an actual argument name (or not, if we failed on an intermediate result; that's a bad outcome and means you didn't add enough error checking). The mapping is similar to how we handle input arguments in tracing.</p>\n<p>One limitation of this approach is that it deals poorly when exactly the same tensor is passed as two arguments, e.g., <code>add(x, x)</code>. In this case, we cannot tell purely from the identity of the TensorImpl whether or not this was the first argument or the second argument.</p>\n<hr>\n<p>Proposal: Exception handler transformers</p>\n<p>Let's get rid of <code>TensorArg</code> so that we can achieve brevity and compositionality. In this case, how can we tell what the name of an argument is? (Does this sound familiar? It's because it's the same as above). We don't: instead, we report whatever the local information of a function is and raise the exception.</p>\n<p>At each function call site, we instrument an exception handler which is responsible for \"rewriting\" the error message so it makes sense in its locality. So for example, if we have:</p>\n<pre><code>void foo(Tensor x, Tensor y) {\n  bar(y, x);\n}\n</code></pre>\n<p>We must somehow make this equivalent to the following:</p>\n<pre><code>void foo(Tensor x, Tensor y) {\n  try {\n    bar(y, x);\n  except (AtenError&amp; e) {\n    e.calling_pattern({{\"x\", 1}, {\"y\", 0}});\n    throw;\n  }\n}\n</code></pre>\n<p>or something equivalent, where we can reconstruct the original argument pattern by traversing the exception handlers.</p>\n<p>Limitation: It's not altogether clear how to automatically generate these wrappers without ... something like <code>TensorArg</code>, because the call pattern of <code>bar</code> matters, and that's not easily accessible from C++.</p>", "body_text": "TensorArg (in aten/src/ATen/TensorUtils.h) is a class I came up with when I was porting CuDNN convolutions from Python to C++. The original goals were as follows:\n\nCreate a set of utility functions to conveniently perform input checks (e.g., does it have the correct size; is it on the correct GPU, etc.)\nGive good error messages when these tests fail (e.g., give the name and the argument number of the argument that failed the test)\nAvoid copy-pasting error checking logic.\n\nGoals (2) and (3) are in tension with each other, especially for convolutions. Consider the following example: convolution_forward, and transposed_convolution_backward. Algorithmically, these two functions are equivalent (and indeed, both functions backend to the same underlying function), but from a UI perspective the functions are very different (e.g., arguments names differ). If you want to not copy-paste error checking logic, you need to put this logic in the underlying backend function, but if you want good error messages you have to track extra information on a per-Tensor basis so that you can accurately describe what the name/argument position of an argument is.\nTensorArg solves this problem by rewriting functions to pass TensorArg struct instead of Tensor, where TensorArg is augmented with extra information to track the name and position of an argument. Checking functions which make use of TensorArg can pull this information out directly when giving an error message.\nHowever, TensorArg falls short of some other goals which I subsequently realized were quite important:\n\nSpeed. Boxing tensors into TensorArg structs, while fairly cheap, resulted in a perceptible, cumulative performance slowdown. Ugh! Some of the cost may be due to additional dynamic allocations. #3958 is the tracking issue.\nBrevity. Suppose that I have a native function f which backends to f_out, which is also native. I cannot rewrite f_out to be TensorArg, because the calling convention for native functions requires Tensor arguments. So I have to write another backend function _f_out, and then have f_out and f call into it.\nCompositionality. Essentially, the TensorArg strategy means we need a \"shadow\" hierarchy of TensorArg-consuming functions, which you call directly if you want to preserve error reporting. This means that if you ever need to go back in the direct Tensor interface, you cannot preserve error information.\n\nGiven this problem, it seems unlikely that we can recommend TensorArg for wider use in the ATen codebase, which is a shame because it is still very difficult to do good error reporting when writing code in ATen.\nCC @goldsborough\n\nProposal: Thread-local state error reporting information\nLet's get rid of TensorArg so that we can achieve brevity and compositionality. In this case, how can we tell what the name of an argument is? We have thread-local error reporting information, which maintains a mapping (just an association list) from TensorImpl* to error information. Upon a failure, we perform a lookup in the mapping to resolve a Tensor into an actual argument name (or not, if we failed on an intermediate result; that's a bad outcome and means you didn't add enough error checking). The mapping is similar to how we handle input arguments in tracing.\nOne limitation of this approach is that it deals poorly when exactly the same tensor is passed as two arguments, e.g., add(x, x). In this case, we cannot tell purely from the identity of the TensorImpl whether or not this was the first argument or the second argument.\n\nProposal: Exception handler transformers\nLet's get rid of TensorArg so that we can achieve brevity and compositionality. In this case, how can we tell what the name of an argument is? (Does this sound familiar? It's because it's the same as above). We don't: instead, we report whatever the local information of a function is and raise the exception.\nAt each function call site, we instrument an exception handler which is responsible for \"rewriting\" the error message so it makes sense in its locality. So for example, if we have:\nvoid foo(Tensor x, Tensor y) {\n  bar(y, x);\n}\n\nWe must somehow make this equivalent to the following:\nvoid foo(Tensor x, Tensor y) {\n  try {\n    bar(y, x);\n  except (AtenError& e) {\n    e.calling_pattern({{\"x\", 1}, {\"y\", 0}});\n    throw;\n  }\n}\n\nor something equivalent, where we can reconstruct the original argument pattern by traversing the exception handlers.\nLimitation: It's not altogether clear how to automatically generate these wrappers without ... something like TensorArg, because the call pattern of bar matters, and that's not easily accessible from C++.", "body": "TensorArg (in `aten/src/ATen/TensorUtils.h`) is a class I came up with when I was porting CuDNN convolutions from Python to C++. The original goals were as follows:\r\n\r\n1. Create a set of utility functions to conveniently perform input checks (e.g., does it have the correct size; is it on the correct GPU, etc.)\r\n2. Give good error messages when these tests fail (e.g., give the name and the argument number of the argument that failed the test)\r\n3. Avoid copy-pasting error checking logic.\r\n\r\nGoals (2) and (3) are in tension with each other, especially for convolutions. Consider the following example: `convolution_forward`, and `transposed_convolution_backward`. Algorithmically, these two functions are equivalent (and indeed, both functions backend to the same underlying function), but from a UI perspective the functions are very different (e.g., arguments names differ). If you want to not copy-paste error checking logic, you need to put this logic in the underlying backend function, but if you want good error messages you have to track extra information on a per-Tensor basis so that you can accurately describe what the name/argument position of an argument is.\r\n\r\nTensorArg solves this problem by rewriting functions to pass `TensorArg` struct instead of `Tensor`, where `TensorArg` is augmented with extra information to track the name and position of an argument. Checking functions which make use of `TensorArg` can pull this information out directly when giving an error message.\r\n\r\nHowever, TensorArg falls short of some other goals which I subsequently realized were quite important:\r\n\r\n1. **Speed.** Boxing tensors into `TensorArg` structs, while fairly cheap, resulted in a perceptible, cumulative performance slowdown. Ugh! Some of the cost may be due to additional dynamic allocations. https://github.com/pytorch/pytorch/issues/3958 is the tracking issue.\r\n2. **Brevity.** Suppose that I have a native function `f` which backends to `f_out`, which is also native. I cannot rewrite `f_out` to be `TensorArg`, because the calling convention for native functions requires `Tensor` arguments. So I have to write another backend function `_f_out`, and then have `f_out` and `f` call into it.\r\n3. **Compositionality.** Essentially, the `TensorArg` strategy means we need a \"shadow\" hierarchy of TensorArg-consuming functions, which you call directly if you want to preserve error reporting. This means that if you ever need to go back in the direct Tensor interface, you cannot preserve error information.\r\n\r\nGiven this problem, it seems unlikely that we can recommend TensorArg for wider use in the ATen codebase, which is a shame because it is still very difficult to do good error reporting when writing code in ATen.\r\n\r\nCC @goldsborough \r\n\r\n----\r\n\r\nProposal: Thread-local state error reporting information\r\n\r\nLet's get rid of `TensorArg` so that we can achieve brevity and compositionality. In this case, how can we tell what the name of an argument is? We have thread-local error reporting information, which maintains a mapping (just an association list) from `TensorImpl*` to error information. Upon a failure, we perform a lookup in the mapping to resolve a `Tensor` into an actual argument name (or not, if we failed on an intermediate result; that's a bad outcome and means you didn't add enough error checking). The mapping is similar to how we handle input arguments in tracing.\r\n\r\nOne limitation of this approach is that it deals poorly when exactly the same tensor is passed as two arguments, e.g., `add(x, x)`. In this case, we cannot tell purely from the identity of the TensorImpl whether or not this was the first argument or the second argument. \r\n\r\n----\r\n\r\nProposal: Exception handler transformers\r\n\r\nLet's get rid of `TensorArg` so that we can achieve brevity and compositionality. In this case, how can we tell what the name of an argument is? (Does this sound familiar? It's because it's the same as above). We don't: instead, we report whatever the local information of a function is and raise the exception.\r\n\r\nAt each function call site, we instrument an exception handler which is responsible for \"rewriting\" the error message so it makes sense in its locality. So for example, if we have:\r\n\r\n```\r\nvoid foo(Tensor x, Tensor y) {\r\n  bar(y, x);\r\n}\r\n```\r\n\r\nWe must somehow make this equivalent to the following:\r\n\r\n```\r\nvoid foo(Tensor x, Tensor y) {\r\n  try {\r\n    bar(y, x);\r\n  except (AtenError& e) {\r\n    e.calling_pattern({{\"x\", 1}, {\"y\", 0}});\r\n    throw;\r\n  }\r\n}\r\n```\r\n\r\nor something equivalent, where we can reconstruct the original argument pattern by traversing the exception handlers.\r\n\r\nLimitation: It's not altogether clear how to automatically generate these wrappers without ... something like `TensorArg`, because the call pattern of `bar` matters, and that's not easily accessible from C++."}