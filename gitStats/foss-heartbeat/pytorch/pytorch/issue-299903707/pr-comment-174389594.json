{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174389594", "pull_request_review_id": 103732410, "id": 174389594, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDM4OTU5NA==", "diff_hunk": "@@ -23,6 +23,177 @@ void where_cpu(\n         ret_val = cond_val ? self_val : other_val;\n       });\n }\n+\n+template<template<typename T> class Comparator, typename scalar_out, typename scalar>\n+struct CmpOpTensor {\n+  static void apply(at::Tensor& ret, const at::Tensor& self, const at::Tensor& other) {\n+    at::CPU_tensor_apply3<scalar_out, scalar, scalar>(ret, self, other,\n+        [](scalar_out& ret_val, const scalar& self_val, const scalar& other_val) {\n+          ret_val = at::convert<scalar_out>(Comparator<scalar>()(self_val, other_val));\n+        }\n+    );\n+  }\n+};\n+\n+template<template<typename T> class Comparator, typename scalar_out, typename scalar>\n+struct CmpOp {\n+  static void apply(at::Tensor& ret, const at::Tensor& self, at::Scalar other) {\n+    auto other_val = other.to<scalar>();\n+    at::CPU_tensor_apply2<scalar_out, scalar>(ret, self,\n+        [other_val](scalar_out& ret_val, const scalar& self_val) {\n+          ret_val = at::convert<scalar_out>(Comparator<scalar>()(self_val, other_val));\n+      }\n+    );\n+  }\n+};\n+\n+// <=, >=, ==, != have special cases for integral tensors and floating scalars due to\n+// the floating scalars being automatically cast to integral types during the comparison\n+template<typename scalar_out, typename scalar>\n+struct CmpOp<std::less_equal, scalar_out, scalar> {\n+  static void apply(at::Tensor& ret, const at::Tensor& self, at::Scalar other) {\n+    auto other_val = other.to<scalar>();\n+\n+    if (isIntegralType(self.type().scalarType()) && other.isFloatingPoint()) {\n+      auto other_double = other.to<double>();\n+      auto other_long = other.to<int64_t>();\n+      if (other_double != other_long) {\n+        other_val = at::convert<scalar>(floor(other_double));\n+      }\n+    }\n+\n+    at::CPU_tensor_apply2<scalar_out, scalar>(ret, self,\n+        [other_val](scalar_out& ret_val, const scalar& self_val) {\n+          ret_val = at::convert<scalar_out>(std::less_equal<scalar>()(self_val, other_val));\n+      }\n+    );\n+  }\n+};\n+\n+template<typename scalar_out, typename scalar>\n+struct CmpOp<std::greater_equal, scalar_out, scalar> {\n+  static void apply(at::Tensor& ret, const at::Tensor& self, at::Scalar other) {\n+    auto other_val = other.to<scalar>();\n+\n+    if (isIntegralType(self.type().scalarType()) && other.isFloatingPoint()) {\n+      auto other_double = other.to<double>();\n+      auto other_long = other.to<int64_t>();\n+      if (other_double != other_long) {\n+        other_val = at::convert<scalar>(ceil(other_double));\n+      }\n+    }\n+\n+    at::CPU_tensor_apply2<scalar_out, scalar>(ret, self,\n+        [other_val](scalar_out& ret_val, const scalar& self_val) {\n+          ret_val = at::convert<scalar_out>(std::greater_equal<scalar>()(self_val, other_val));\n+      }\n+    );\n+  }\n+};\n+\n+template<typename scalar_out, typename scalar>\n+struct CmpOp<std::equal_to, scalar_out, scalar> {\n+  static void apply(at::Tensor& ret, const at::Tensor& self, at::Scalar other) {\n+    auto other_val = other.to<scalar>();\n+\n+    if (isIntegralType(self.type().scalarType()) && other.isFloatingPoint()) {\n+      auto other_double = other.to<double>();\n+      auto other_long = other.to<int64_t>();\n+      if (other_double != other_long) {\n+        ret.fill_(0);\n+      }\n+    } else {\n+      at::CPU_tensor_apply2<scalar_out, scalar>(ret, self,\n+          [other_val](scalar_out& ret_val, const scalar& self_val) {\n+            ret_val = at::convert<scalar_out>(std::equal_to<scalar>()(self_val, other_val));\n+        }\n+      );\n+    }\n+  }\n+};\n+\n+template<typename scalar_out, typename scalar>\n+struct CmpOp<std::not_equal_to, scalar_out, scalar> {\n+  static void apply(at::Tensor& ret, const at::Tensor& self, at::Scalar other) {\n+    auto other_val = other.to<scalar>();\n+\n+    if (isIntegralType(self.type().scalarType()) && other.isFloatingPoint()) {\n+      auto other_double = other.to<double>();\n+      auto other_long = other.to<int64_t>();\n+      if (other_double != other_long) {\n+        ret.fill_(1);\n+      }\n+    } else {\n+      at::CPU_tensor_apply2<scalar_out, scalar>(ret, self,\n+          [other_val](scalar_out& ret_val, const scalar& self_val) {\n+            ret_val = at::convert<scalar_out>(std::not_equal_to<scalar>()(self_val, other_val));\n+        }\n+      );\n+    }\n+  }\n+};\n+\n+template<template<typename T> class Comparator>\n+at::Tensor& cmp_out_cpu(at::Tensor& result, const at::Tensor& self, at::Scalar other, const char* op_name) {\n+  result.resize_(self.sizes());\n+  AT_DISPATCH_ALL_TYPES(self.type(), op_name, [&]() {\n+    CmpOp<Comparator, uint8_t, scalar_t>::apply(result, self, other);\n+  });\n+  return result;\n+}\n+\n+template<template<typename T> class Comparator>\n+at::Tensor& cmp_out_cpu(at::Tensor& result, const at::Tensor& self, const at::Tensor& other, const char* op_name) {", "path": "aten/src/ATen/native/TensorCompare.cpp", "position": null, "original_position": 124, "commit_id": "da6b60d7c716fb9b9fe5052edcdaf4204c8d3cef", "original_commit_id": "7e708e0a4094e330c4b9e5ac99b0774b1c824d35", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think this could use the `s_` mechanism, but I'm not 100% sure how to use it with native functions.", "created_at": "2018-03-14T09:00:28Z", "updated_at": "2018-11-23T15:40:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/5394#discussion_r174389594", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5394", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174389594"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5394#discussion_r174389594"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5394"}}, "body_html": "<p>I think this could use the <code>s_</code> mechanism, but I'm not 100% sure how to use it with native functions.</p>", "body_text": "I think this could use the s_ mechanism, but I'm not 100% sure how to use it with native functions."}