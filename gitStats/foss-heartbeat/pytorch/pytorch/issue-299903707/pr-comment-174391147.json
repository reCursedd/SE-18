{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174391147", "pull_request_review_id": 103732410, "id": 174391147, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDM5MTE0Nw==", "diff_hunk": "@@ -40,4 +215,148 @@ Tensor _s_where_cuda(\n   });\n   return ret;\n }\n+\n+Tensor lt_cuda(const Tensor& self, Scalar other) {\n+  return cmp_cuda<std::less>(self, other, \"lt\");\n+}\n+\n+Tensor gt_cuda(const Tensor& self, Scalar other) {\n+  return cmp_cuda<std::greater>(self, other, \"gt\");\n+}\n+\n+Tensor le_cuda(const Tensor& self, Scalar other) {\n+  return cmp_cuda<std::less_equal>(self, other, \"le\");\n+}\n+\n+Tensor ge_cuda(const Tensor& self, Scalar other) {\n+  return cmp_cuda<std::greater_equal>(self, other, \"ge\");\n+}\n+\n+Tensor eq_cuda(const Tensor& self, Scalar other) {\n+  return cmp_cuda<std::equal_to>(self, other, \"eq\");\n+}\n+\n+Tensor ne_cuda(const Tensor& self, Scalar other) {\n+  return cmp_cuda<std::not_equal_to>(self, other, \"ne\");\n+}\n+\n+Tensor& lt_out_cuda(Tensor& result, const Tensor& self, Scalar other) {\n+  return cmp_out_cuda<std::less>(result, self, other, \"lt\");\n+}\n+\n+Tensor& gt_out_cuda(Tensor& result, const Tensor& self, Scalar other) {\n+  return cmp_out_cuda<std::greater>(result, self, other, \"gt\");\n+}\n+\n+Tensor& le_out_cuda(Tensor& result, const Tensor& self, Scalar other) {\n+  return cmp_out_cuda<std::less_equal>(result, self, other, \"le\");\n+}\n+\n+Tensor& ge_out_cuda(Tensor& result, const Tensor& self, Scalar other) {\n+  return cmp_out_cuda<std::greater_equal>(result, self, other, \"ge\");\n+}\n+\n+Tensor& eq_out_cuda(Tensor& result, const Tensor& self, Scalar other) {\n+  return cmp_out_cuda<std::equal_to>(result, self, other, \"eq\");\n+}\n+\n+Tensor& ne_out_cuda(Tensor& result, const Tensor& self, Scalar other) {\n+  return cmp_out_cuda<std::not_equal_to>(result, self, other, \"ne\");\n+}\n+\n+Tensor lt_cuda(const Tensor& self, const Tensor& other) {\n+  return cmp_cuda<std::less>(self, other, \"lt\");\n+}\n+\n+Tensor gt_cuda(const Tensor& self, const Tensor& other) {\n+  return cmp_cuda<std::greater>(self, other, \"gt\");\n+}\n+\n+Tensor le_cuda(const Tensor& self, const Tensor& other) {\n+  return cmp_cuda<std::less_equal>(self, other, \"le\");\n+}\n+\n+Tensor ge_cuda(const Tensor& self, const Tensor& other) {\n+  return cmp_cuda<std::greater_equal>(self, other, \"le\");\n+}\n+\n+Tensor eq_cuda(const Tensor& self, const Tensor& other) {\n+  return cmp_cuda<std::equal_to>(self, other, \"eq\");\n+}\n+\n+Tensor ne_cuda(const Tensor& self, const Tensor& other) {\n+  return cmp_cuda<std::not_equal_to>(self, other, \"ne\");\n+}\n+\n+Tensor& lt_out_cuda(Tensor& result, const Tensor& self, const Tensor& other) {\n+  return cmp_out_cuda<std::less>(result, self, other, \"lt\");\n+}\n+\n+Tensor& gt_out_cuda(Tensor& result, const Tensor& self, const Tensor& other) {\n+  return cmp_out_cuda<std::greater>(result, self, other, \"gt\");\n+}\n+\n+Tensor& le_out_cuda(Tensor& result, const Tensor& self, const Tensor& other) {\n+  return cmp_out_cuda<std::less_equal>(result, self, other, \"le\");\n+}\n+\n+Tensor& ge_out_cuda(Tensor& result, const Tensor& self, const Tensor& other) {\n+  return cmp_out_cuda<std::greater_equal>(result, self, other, \"ge\");\n+}\n+\n+Tensor& eq_out_cuda(Tensor& result, const Tensor& self, const Tensor& other) {\n+  return cmp_out_cuda<std::equal_to>(result, self, other, \"eq\");\n+}\n+\n+Tensor& ne_out_cuda(Tensor& result, const Tensor& self, const Tensor& other) {\n+  return cmp_out_cuda<std::not_equal_to>(result, self, other, \"ne\");\n+}\n+\n+Tensor& lt_inplace_cuda(Tensor& self, Scalar other) {\n+  return cmp_inplace_cuda<std::less>(self, other, \"lt_\");\n+}\n+\n+Tensor& gt_inplace_cuda(Tensor& self, Scalar other) {\n+  return cmp_inplace_cuda<std::greater>(self, other, \"gt_\");\n+}\n+\n+Tensor& le_inplace_cuda(Tensor& self, Scalar other) {\n+  return cmp_inplace_cuda<std::less_equal>(self, other, \"le_\");\n+}\n+\n+Tensor& ge_inplace_cuda(Tensor& self, Scalar other) {\n+  return cmp_inplace_cuda<std::greater_equal>(self, other, \"ge_\");\n+}\n+\n+Tensor& eq_inplace_cuda(Tensor& self, Scalar other) {\n+  return cmp_inplace_cuda<std::equal_to>(self, other, \"eq_\");\n+}\n+\n+Tensor& ne_inplace_cuda(Tensor& self, Scalar other) {\n+  return cmp_inplace_cuda<std::not_equal_to>(self, other, \"ne_\");\n+}\n+\n+Tensor& lt_inplace_cuda(Tensor& self, const Tensor& other) {\n+  return cmp_inplace_cuda<std::less>(self, other, \"lt_\");\n+}\n+\n+Tensor& gt_inplace_cuda(Tensor& self, const Tensor& other) {\n+  return cmp_inplace_cuda<std::greater>(self, other, \"gt_\");\n+}\n+\n+Tensor& le_inplace_cuda(Tensor& self, const Tensor& other) {\n+  return cmp_inplace_cuda<std::less_equal>(self, other, \"le_\");\n+}\n+\n+Tensor& ge_inplace_cuda(Tensor& self, const Tensor& other) {\n+  return cmp_inplace_cuda<std::greater_equal>(self, other, \"ge_\");\n+}\n+\n+Tensor& eq_inplace_cuda(Tensor& self, const Tensor& other) {\n+  return cmp_inplace_cuda<std::equal_to>(self, other, \"eq_\");\n+}\n+\n+Tensor& ne_inplace_cuda(Tensor& self, const Tensor& other) {\n+  return cmp_inplace_cuda<std::not_equal_to>(self, other, \"ne_\");\n+}", "path": "aten/src/ATen/native/cuda/TensorCompare.cu", "position": null, "original_position": 341, "commit_id": "da6b60d7c716fb9b9fe5052edcdaf4204c8d3cef", "original_commit_id": "7e708e0a4094e330c4b9e5ac99b0774b1c824d35", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This file is almost the same as the CPU one, so it would be good to avoid duplication (it's very easy to make the special cases for op templates inconsistent between devices). Do we have any ways to do this?", "created_at": "2018-03-14T09:07:21Z", "updated_at": "2018-11-23T15:40:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/5394#discussion_r174391147", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5394", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174391147"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5394#discussion_r174391147"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5394"}}, "body_html": "<p>This file is almost the same as the CPU one, so it would be good to avoid duplication (it's very easy to make the special cases for op templates inconsistent between devices). Do we have any ways to do this?</p>", "body_text": "This file is almost the same as the CPU one, so it would be good to avoid duplication (it's very easy to make the special cases for op templates inconsistent between devices). Do we have any ways to do this?"}