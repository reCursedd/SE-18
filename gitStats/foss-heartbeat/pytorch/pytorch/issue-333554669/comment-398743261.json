{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398743261", "html_url": "https://github.com/pytorch/pytorch/pull/8647#issuecomment-398743261", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8647", "id": 398743261, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODc0MzI2MQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-20T13:12:01Z", "updated_at": "2018-06-20T13:12:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If you want a design that handles both global and thread local state, I suggest taking a look at <a href=\"https://fb.quip.com/w9G9AlEXbPlq\" rel=\"nofollow\">https://fb.quip.com/w9G9AlEXbPlq</a> (copy pasted below), which is a proposal of mine on how to put it together:</p>\n<h1>c10: The global context</h1>\n<p>This proposal is not slated for immediate implementation (instead, we will simply achieve API parity)</p>\n<h2>Motivation</h2>\n<p>In most real-world programs, there is a need for some sort of global state which can be easily accessed anywhere in the program. Common examples of such state include:</p>\n<ul>\n<li>Choice of CPU/CUDA memory allocator, and flags for said memory allocator</li>\n<li>Random number generator (PyTorch only; Caffe2 has per-operator RNGs)</li>\n<li>Cached information about the system (e.g., cudaDeviceProperties, number of available devices, etc)</li>\n<li>The dispatch table (projected in c10)</li>\n</ul>\n<p>Global state makes it difficult to have two copies of a library in memory, or run nearby programs with different settings for the context. Thus, there is a desire to encapsulate this state in some sort of context object, to allow for this.</p>\n<h2>Prior Art</h2>\n<p><em>PyTorch/ATen.</em> ATen has a Context object, which at present is a single static global variable which user code frequently access directly. Context objects are propagated because every Tensor stores a pointer to the Context object responsible for creating it; \u201cproper\u201d use says that you should use this particular Context object, although a lot of code that was subsequently written for ATen has not followed this constraint.</p>\n<p>We (Edward, Sam) don't think this model (context passing via Tensor objects) makes semantic sense for what a context object should \u201cbe\u201d. Additionally, it seems in practice very difficult to educate developers how to pass around context objects directly, when there is a very attractively named \u201cgetGlobalContext()\u201d function which they can use to synthesize a context from scratch.</p>\n<p><em>Caffe2.</em> Caffe2 simply uses global variables as necessary. For example, the CPUAllocator is a static unique pointer defined in caffe2/core/allocator.cc; additionally, Caffe2 makes use of gflags (or Caffe2's portable copy thereof), which programs the Caffe2 static registry, which is itself a static library. No context is propagated with actual tensors; instead, per-tensor configuration like the data deleter is stored via the shared_ptr.</p>\n<h2>Design Space</h2>\n<p>To determine how we handle the global context, we have to make a few decisions:</p>\n<ul>\n<li>Do we want to support having multiple copies of c10 in the same address space?</li>\n<li>Do we want to allow changes to the context in the middle of program execution (rather than solely at static initialization time)?</li>\n<li>Do we want to allow multiple changes to the context atomically?</li>\n<li>Should it be possible to change the context for all threads of execution?</li>\n</ul>\n<h2>Proposal</h2>\n<p>We offer the following proposal, given that we answered <em>YES</em> to each of the previous questions. Simpler designs are possible if you decide you don't care about some of these properties.</p>\n<p>The context is mediated by two levels of indirection: a thread-local context pointer, which points to an atomic memory cell containing a pointer to the actual struct. The Context struct itself is immutable.</p>\n<pre><code>#include &lt;memory&gt;\n#include &lt;atomic&gt;\n#include &lt;mutex&gt;\n#include &lt;cstdlib&gt;\n\n// immutable class which contains all of the context data\nclass ContextImpl {};\n// Unfortunately, we can't use std::shared_ptr&lt;ContextImpl&gt;, as atomic accesses to\n// this pointer are implemented with mutex. If this were possible to do, we could\n// also have accurate reference counts on ContextImpl, allowing us to deallocate\n// it when it became free.\nusing Context = std::atomic&lt;ContextImpl*&gt;;\nusing ContextPtr = std::shared_ptr&lt;Context&gt;;\n\nstatic thread_local ContextPtr thread_context = nullptr;\nstatic ContextImpl global_context;\nstatic ContextPtr global_context_ptr = nullptr;\nstatic std::mutex global_context_mutex;\n\n// THREAD STATE                         GLOBAL CONTEXT POINTER\n// TTTTTTTT (thread local state)        MMMMMMMM (mutex protected global state)\n//    \\ shared_ptr                      /\n//     \\---&gt; CONTEXT (MEMORY CELL) &lt;---/\n//            AAAAAAAA (atomically accessed word)\n//                  \\ pointer\n//                   \\--------&gt; CONTEXT IMPL\n//                              IIIIIIII (immutable memory)\n//                              IIIIIIII\n//                              IIIIIIII\n//                               ...\n\n\nContextPtr getGlobalContextSlow() {\n  std::lock_guard&lt;std::mutex&gt; guard(global_context_mutex);\n  if (!global_context_ptr) {\n    // Allocate JUST the mutable context memory cell.  This cell can\n    // be garbage collected when all references to it go away.\n    global_context_ptr = std::make_shared&lt;Context&gt;(&amp;global_context);\n  }\n  return global_context_ptr;\n};\n\nContextImpl* getContext() {\n  if (!thread_context) thread_context = getGlobalContextSlow();\n  return thread_context-&gt;load();\n}\n\n/* Fast path assembly looks like this:\n\ngetContext():\n  push rbp\n  push rbx\n  cmp BYTE PTR fs:__tls_guard@tpoff, 0\n  jne .L74 sub rsp, 24\n  ...\nL74:\n  mov rax, QWORD PTR fs:thread_context@tpoff\n  test rax, rax\n  je .L106\n  mov rax, QWORD PTR [rax]\n  add rsp, 24\n  pop rbx\n  pop rbp\n  ret\n*/\n</code></pre>\n<p><a href=\"https://godbolt.org/g/GPZPf6\" rel=\"nofollow\">https://godbolt.org/g/GPZPf6</a></p>\n<p>There are a few key characteristics of the implementation above:</p>\n<ul>\n<li>The most difficult thing to account for is how to atomically handle multiple changes to the Context in a thread safe manner. If you don't care about handling multiple changes, there is an obvious alternative: have the Context object itself provide atomic operations for setting/getting individual properties. We think that it may be necessary to apply multiple changes in practice; e.g., the allocator is typically two fields: a malloc() and a free() implementation. To make this possible,  the design makes the ContextImpl immutable, and introduces another level of indirection (now called Context) which is an atomic pointer which can be atomically changed when required. Now a swap of the context implementation can apply arbitrary changes without affecting existing readers.</li>\n<li>The thread local state is required to allow individual threads to locally change the context under use to something else. All threads default to the global state otherwise.</li>\n<li>As written, the above code requires the lifetime of ContextImpl objects to be managed externally to this mechanism. With atomic_shared_pointer (concurrency TS, now std::atomicstd::shared_ptr in C++20), we could efficiently also reference count ContextImpl objects, allowing them to be promptly destructed.</li>\n</ul>\n<h2>Open questions</h2>\n<ul>\n<li>The design of the context interacts closely with the design of the dispatch table, which lives in the context. In particular, an immutable context implies that the dispatch table must also be immutable. The alternative choice is to design a hash table which supports unsynchronized reads and rare, expensive writes. The primary hazard in this case is handling when the hash table needs to be resized; this implies you need some sort of pointer which can be swapped.\n<ul>\n<li>Zach has suggested that a good simplifying assumption is that, if an operation is added to the dispatch table at time X, any contexts from before X simply do not see the method. This allows for the implementation strategy where you have a direct, unsynchronized pointer to the dispatch table for method invocation on a Tensor, and a slower, global indirection to the dispatch table which can be updated atomically when changes are made.</li>\n</ul>\n</li>\n<li>Sebastian Messmer suggests that we should not have a global state pointer which we initialize threads with no setting for thread_context, and instead ask users to always reinitialize the state when they spawn new threads.\n<ul>\n<li>Ed's concern: don't want to have to call at::init() initially.</li>\n</ul>\n</li>\n<li>With the dispatch table, we would like operators to be registered immediately upon dlopen(). However, this poses a problem for thread-local dispatch tables: if dlopen() is called multiple times, the library is only loaded the first time. This means you can't \u201creload\u201d a library to get it into a second dispatch table; you'll have to do this manually itself. So it may make the most sense to make the dispatch table have a visibility equivalent to the dynamic linker, AKA be global state.</li>\n</ul>", "body_text": "If you want a design that handles both global and thread local state, I suggest taking a look at https://fb.quip.com/w9G9AlEXbPlq (copy pasted below), which is a proposal of mine on how to put it together:\nc10: The global context\nThis proposal is not slated for immediate implementation (instead, we will simply achieve API parity)\nMotivation\nIn most real-world programs, there is a need for some sort of global state which can be easily accessed anywhere in the program. Common examples of such state include:\n\nChoice of CPU/CUDA memory allocator, and flags for said memory allocator\nRandom number generator (PyTorch only; Caffe2 has per-operator RNGs)\nCached information about the system (e.g., cudaDeviceProperties, number of available devices, etc)\nThe dispatch table (projected in c10)\n\nGlobal state makes it difficult to have two copies of a library in memory, or run nearby programs with different settings for the context. Thus, there is a desire to encapsulate this state in some sort of context object, to allow for this.\nPrior Art\nPyTorch/ATen. ATen has a Context object, which at present is a single static global variable which user code frequently access directly. Context objects are propagated because every Tensor stores a pointer to the Context object responsible for creating it; \u201cproper\u201d use says that you should use this particular Context object, although a lot of code that was subsequently written for ATen has not followed this constraint.\nWe (Edward, Sam) don't think this model (context passing via Tensor objects) makes semantic sense for what a context object should \u201cbe\u201d. Additionally, it seems in practice very difficult to educate developers how to pass around context objects directly, when there is a very attractively named \u201cgetGlobalContext()\u201d function which they can use to synthesize a context from scratch.\nCaffe2. Caffe2 simply uses global variables as necessary. For example, the CPUAllocator is a static unique pointer defined in caffe2/core/allocator.cc; additionally, Caffe2 makes use of gflags (or Caffe2's portable copy thereof), which programs the Caffe2 static registry, which is itself a static library. No context is propagated with actual tensors; instead, per-tensor configuration like the data deleter is stored via the shared_ptr.\nDesign Space\nTo determine how we handle the global context, we have to make a few decisions:\n\nDo we want to support having multiple copies of c10 in the same address space?\nDo we want to allow changes to the context in the middle of program execution (rather than solely at static initialization time)?\nDo we want to allow multiple changes to the context atomically?\nShould it be possible to change the context for all threads of execution?\n\nProposal\nWe offer the following proposal, given that we answered YES to each of the previous questions. Simpler designs are possible if you decide you don't care about some of these properties.\nThe context is mediated by two levels of indirection: a thread-local context pointer, which points to an atomic memory cell containing a pointer to the actual struct. The Context struct itself is immutable.\n#include <memory>\n#include <atomic>\n#include <mutex>\n#include <cstdlib>\n\n// immutable class which contains all of the context data\nclass ContextImpl {};\n// Unfortunately, we can't use std::shared_ptr<ContextImpl>, as atomic accesses to\n// this pointer are implemented with mutex. If this were possible to do, we could\n// also have accurate reference counts on ContextImpl, allowing us to deallocate\n// it when it became free.\nusing Context = std::atomic<ContextImpl*>;\nusing ContextPtr = std::shared_ptr<Context>;\n\nstatic thread_local ContextPtr thread_context = nullptr;\nstatic ContextImpl global_context;\nstatic ContextPtr global_context_ptr = nullptr;\nstatic std::mutex global_context_mutex;\n\n// THREAD STATE                         GLOBAL CONTEXT POINTER\n// TTTTTTTT (thread local state)        MMMMMMMM (mutex protected global state)\n//    \\ shared_ptr                      /\n//     \\---> CONTEXT (MEMORY CELL) <---/\n//            AAAAAAAA (atomically accessed word)\n//                  \\ pointer\n//                   \\--------> CONTEXT IMPL\n//                              IIIIIIII (immutable memory)\n//                              IIIIIIII\n//                              IIIIIIII\n//                               ...\n\n\nContextPtr getGlobalContextSlow() {\n  std::lock_guard<std::mutex> guard(global_context_mutex);\n  if (!global_context_ptr) {\n    // Allocate JUST the mutable context memory cell.  This cell can\n    // be garbage collected when all references to it go away.\n    global_context_ptr = std::make_shared<Context>(&global_context);\n  }\n  return global_context_ptr;\n};\n\nContextImpl* getContext() {\n  if (!thread_context) thread_context = getGlobalContextSlow();\n  return thread_context->load();\n}\n\n/* Fast path assembly looks like this:\n\ngetContext():\n  push rbp\n  push rbx\n  cmp BYTE PTR fs:__tls_guard@tpoff, 0\n  jne .L74 sub rsp, 24\n  ...\nL74:\n  mov rax, QWORD PTR fs:thread_context@tpoff\n  test rax, rax\n  je .L106\n  mov rax, QWORD PTR [rax]\n  add rsp, 24\n  pop rbx\n  pop rbp\n  ret\n*/\n\nhttps://godbolt.org/g/GPZPf6\nThere are a few key characteristics of the implementation above:\n\nThe most difficult thing to account for is how to atomically handle multiple changes to the Context in a thread safe manner. If you don't care about handling multiple changes, there is an obvious alternative: have the Context object itself provide atomic operations for setting/getting individual properties. We think that it may be necessary to apply multiple changes in practice; e.g., the allocator is typically two fields: a malloc() and a free() implementation. To make this possible,  the design makes the ContextImpl immutable, and introduces another level of indirection (now called Context) which is an atomic pointer which can be atomically changed when required. Now a swap of the context implementation can apply arbitrary changes without affecting existing readers.\nThe thread local state is required to allow individual threads to locally change the context under use to something else. All threads default to the global state otherwise.\nAs written, the above code requires the lifetime of ContextImpl objects to be managed externally to this mechanism. With atomic_shared_pointer (concurrency TS, now std::atomicstd::shared_ptr in C++20), we could efficiently also reference count ContextImpl objects, allowing them to be promptly destructed.\n\nOpen questions\n\nThe design of the context interacts closely with the design of the dispatch table, which lives in the context. In particular, an immutable context implies that the dispatch table must also be immutable. The alternative choice is to design a hash table which supports unsynchronized reads and rare, expensive writes. The primary hazard in this case is handling when the hash table needs to be resized; this implies you need some sort of pointer which can be swapped.\n\nZach has suggested that a good simplifying assumption is that, if an operation is added to the dispatch table at time X, any contexts from before X simply do not see the method. This allows for the implementation strategy where you have a direct, unsynchronized pointer to the dispatch table for method invocation on a Tensor, and a slower, global indirection to the dispatch table which can be updated atomically when changes are made.\n\n\nSebastian Messmer suggests that we should not have a global state pointer which we initialize threads with no setting for thread_context, and instead ask users to always reinitialize the state when they spawn new threads.\n\nEd's concern: don't want to have to call at::init() initially.\n\n\nWith the dispatch table, we would like operators to be registered immediately upon dlopen(). However, this poses a problem for thread-local dispatch tables: if dlopen() is called multiple times, the library is only loaded the first time. This means you can't \u201creload\u201d a library to get it into a second dispatch table; you'll have to do this manually itself. So it may make the most sense to make the dispatch table have a visibility equivalent to the dynamic linker, AKA be global state.", "body": "If you want a design that handles both global and thread local state, I suggest taking a look at https://fb.quip.com/w9G9AlEXbPlq (copy pasted below), which is a proposal of mine on how to put it together:\r\n\r\n# c10: The global context\r\n\r\nThis proposal is not slated for immediate implementation (instead, we will simply achieve API parity)\r\n\r\n## Motivation\r\n\r\nIn most real-world programs, there is a need for some sort of global state which can be easily accessed anywhere in the program. Common examples of such state include:\r\n\r\n* Choice of CPU/CUDA memory allocator, and flags for said memory allocator\r\n* Random number generator (PyTorch only; Caffe2 has per-operator RNGs)\r\n* Cached information about the system (e.g., cudaDeviceProperties, number of available devices, etc)\r\n* The dispatch table (projected in c10)\r\n\r\nGlobal state makes it difficult to have two copies of a library in memory, or run nearby programs with different settings for the context. Thus, there is a desire to encapsulate this state in some sort of context object, to allow for this.\r\n\r\n## Prior Art\r\n\r\n*PyTorch/ATen.* ATen has a Context object, which at present is a single static global variable which user code frequently access directly. Context objects are propagated because every Tensor stores a pointer to the Context object responsible for creating it; \u201cproper\u201d use says that you should use this particular Context object, although a lot of code that was subsequently written for ATen has not followed this constraint.\r\n\r\nWe (Edward, Sam) don't think this model (context passing via Tensor objects) makes semantic sense for what a context object should \u201cbe\u201d. Additionally, it seems in practice very difficult to educate developers how to pass around context objects directly, when there is a very attractively named \u201cgetGlobalContext()\u201d function which they can use to synthesize a context from scratch.\r\n\r\n*Caffe2.* Caffe2 simply uses global variables as necessary. For example, the CPUAllocator is a static unique pointer defined in caffe2/core/allocator.cc; additionally, Caffe2 makes use of gflags (or Caffe2's portable copy thereof), which programs the Caffe2 static registry, which is itself a static library. No context is propagated with actual tensors; instead, per-tensor configuration like the data deleter is stored via the shared_ptr.\r\n\r\n## Design Space\r\n\r\nTo determine how we handle the global context, we have to make a few decisions:\r\n\r\n* Do we want to support having multiple copies of c10 in the same address space?\r\n* Do we want to allow changes to the context in the middle of program execution (rather than solely at static initialization time)?\r\n* Do we want to allow multiple changes to the context atomically?\r\n* Should it be possible to change the context for all threads of execution?\r\n\r\n## Proposal\r\n\r\nWe offer the following proposal, given that we answered *YES* to each of the previous questions. Simpler designs are possible if you decide you don't care about some of these properties.\r\n\r\nThe context is mediated by two levels of indirection: a thread-local context pointer, which points to an atomic memory cell containing a pointer to the actual struct. The Context struct itself is immutable.\r\n\r\n```\r\n#include <memory>\r\n#include <atomic>\r\n#include <mutex>\r\n#include <cstdlib>\r\n\r\n// immutable class which contains all of the context data\r\nclass ContextImpl {};\r\n// Unfortunately, we can't use std::shared_ptr<ContextImpl>, as atomic accesses to\r\n// this pointer are implemented with mutex. If this were possible to do, we could\r\n// also have accurate reference counts on ContextImpl, allowing us to deallocate\r\n// it when it became free.\r\nusing Context = std::atomic<ContextImpl*>;\r\nusing ContextPtr = std::shared_ptr<Context>;\r\n\r\nstatic thread_local ContextPtr thread_context = nullptr;\r\nstatic ContextImpl global_context;\r\nstatic ContextPtr global_context_ptr = nullptr;\r\nstatic std::mutex global_context_mutex;\r\n\r\n// THREAD STATE                         GLOBAL CONTEXT POINTER\r\n// TTTTTTTT (thread local state)        MMMMMMMM (mutex protected global state)\r\n//    \\ shared_ptr                      /\r\n//     \\---> CONTEXT (MEMORY CELL) <---/\r\n//            AAAAAAAA (atomically accessed word)\r\n//                  \\ pointer\r\n//                   \\--------> CONTEXT IMPL\r\n//                              IIIIIIII (immutable memory)\r\n//                              IIIIIIII\r\n//                              IIIIIIII\r\n//                               ...\r\n\r\n\r\nContextPtr getGlobalContextSlow() {\r\n  std::lock_guard<std::mutex> guard(global_context_mutex);\r\n  if (!global_context_ptr) {\r\n    // Allocate JUST the mutable context memory cell.  This cell can\r\n    // be garbage collected when all references to it go away.\r\n    global_context_ptr = std::make_shared<Context>(&global_context);\r\n  }\r\n  return global_context_ptr;\r\n};\r\n\r\nContextImpl* getContext() {\r\n  if (!thread_context) thread_context = getGlobalContextSlow();\r\n  return thread_context->load();\r\n}\r\n\r\n/* Fast path assembly looks like this:\r\n\r\ngetContext():\r\n  push rbp\r\n  push rbx\r\n  cmp BYTE PTR fs:__tls_guard@tpoff, 0\r\n  jne .L74 sub rsp, 24\r\n  ...\r\nL74:\r\n  mov rax, QWORD PTR fs:thread_context@tpoff\r\n  test rax, rax\r\n  je .L106\r\n  mov rax, QWORD PTR [rax]\r\n  add rsp, 24\r\n  pop rbx\r\n  pop rbp\r\n  ret\r\n*/\r\n```\r\n\r\nhttps://godbolt.org/g/GPZPf6\r\n\r\nThere are a few key characteristics of the implementation above:\r\n\r\n* The most difficult thing to account for is how to atomically handle multiple changes to the Context in a thread safe manner. If you don't care about handling multiple changes, there is an obvious alternative: have the Context object itself provide atomic operations for setting/getting individual properties. We think that it may be necessary to apply multiple changes in practice; e.g., the allocator is typically two fields: a malloc() and a free() implementation. To make this possible,  the design makes the ContextImpl immutable, and introduces another level of indirection (now called Context) which is an atomic pointer which can be atomically changed when required. Now a swap of the context implementation can apply arbitrary changes without affecting existing readers.\r\n* The thread local state is required to allow individual threads to locally change the context under use to something else. All threads default to the global state otherwise.\r\n* As written, the above code requires the lifetime of ContextImpl objects to be managed externally to this mechanism. With atomic_shared_pointer (concurrency TS, now std::atomic<std::shared_ptr> in C++20), we could efficiently also reference count ContextImpl objects, allowing them to be promptly destructed.\r\n\r\n## Open questions\r\n\r\n* The design of the context interacts closely with the design of the dispatch table, which lives in the context. In particular, an immutable context implies that the dispatch table must also be immutable. The alternative choice is to design a hash table which supports unsynchronized reads and rare, expensive writes. The primary hazard in this case is handling when the hash table needs to be resized; this implies you need some sort of pointer which can be swapped.\r\n    * Zach has suggested that a good simplifying assumption is that, if an operation is added to the dispatch table at time X, any contexts from before X simply do not see the method. This allows for the implementation strategy where you have a direct, unsynchronized pointer to the dispatch table for method invocation on a Tensor, and a slower, global indirection to the dispatch table which can be updated atomically when changes are made.\r\n* Sebastian Messmer suggests that we should not have a global state pointer which we initialize threads with no setting for thread_context, and instead ask users to always reinitialize the state when they spawn new threads.\r\n    * Ed's concern: don't want to have to call at::init() initially.\r\n* With the dispatch table, we would like operators to be registered immediately upon dlopen(). However, this poses a problem for thread-local dispatch tables: if dlopen() is called multiple times, the library is only loaded the first time. This means you can't \u201creload\u201d a library to get it into a second dispatch table; you'll have to do this manually itself. So it may make the most sense to make the dispatch table have a visibility equivalent to the dynamic linker, AKA be global state.\r\n\r\n"}