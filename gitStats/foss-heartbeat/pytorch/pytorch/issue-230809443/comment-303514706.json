{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303514706", "html_url": "https://github.com/pytorch/pytorch/issues/1631#issuecomment-303514706", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1631", "id": 303514706, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzUxNDcwNg==", "user": {"login": "jcjohnson", "id": 2718714, "node_id": "MDQ6VXNlcjI3MTg3MTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2718714?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcjohnson", "html_url": "https://github.com/jcjohnson", "followers_url": "https://api.github.com/users/jcjohnson/followers", "following_url": "https://api.github.com/users/jcjohnson/following{/other_user}", "gists_url": "https://api.github.com/users/jcjohnson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcjohnson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcjohnson/subscriptions", "organizations_url": "https://api.github.com/users/jcjohnson/orgs", "repos_url": "https://api.github.com/users/jcjohnson/repos", "events_url": "https://api.github.com/users/jcjohnson/events{/privacy}", "received_events_url": "https://api.github.com/users/jcjohnson/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-23T20:00:05Z", "updated_at": "2017-05-23T20:00:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems to be a problem with scatter. Gather backward is implemented using scatter:</p>\n<p><a href=\"https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/tensor.py#L511\">https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/tensor.py#L511</a></p>\n<p>CPU scatter just overwrites the destination:</p>\n<p><a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L465\">https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L465</a><br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L488\">https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L488</a></p>\n<p>Same story for GPU scatter:</p>\n<p><a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L126\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L126</a><br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L151\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L151</a></p>\n<p>I think we could add an optional flag to scatter to have it accumulate into the destination tensor rather then overwrite, then use the accumulation version of scatter in the gather backward pass.</p>\n<p>Unfortunately I don't think I'm familiar enough with the TH / THC guts to fix this myself.</p>", "body_text": "It seems to be a problem with scatter. Gather backward is implemented using scatter:\nhttps://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/tensor.py#L511\nCPU scatter just overwrites the destination:\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L465\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L488\nSame story for GPU scatter:\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L126\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L151\nI think we could add an optional flag to scatter to have it accumulate into the destination tensor rather then overwrite, then use the accumulation version of scatter in the gather backward pass.\nUnfortunately I don't think I'm familiar enough with the TH / THC guts to fix this myself.", "body": "It seems to be a problem with scatter. Gather backward is implemented using scatter:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/tensor.py#L511\r\n\r\nCPU scatter just overwrites the destination:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L465\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L488\r\n\r\nSame story for GPU scatter:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L126\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorScatterGather.cu#L151\r\n\r\nI think we could add an optional flag to scatter to have it accumulate into the destination tensor rather then overwrite, then use the accumulation version of scatter in the gather backward pass.\r\n\r\nUnfortunately I don't think I'm familiar enough with the TH / THC guts to fix this myself."}