{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/284097523", "html_url": "https://github.com/pytorch/pytorch/issues/914#issuecomment-284097523", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/914", "id": 284097523, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDA5NzUyMw==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-03T23:17:44Z", "updated_at": "2017-03-03T23:20:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I guess the idea is that parameters are Variables, so modules should be able to silently move their underlying Tensors (after first checking if they're already in acceptable memory positions). There's no state in the module or function, just a lightweight (if ugly) check at the beginning of the forward pass of the module that the parameter tensors have the correct memory relationship to each other. (It can't be in the Function because the Function doesn't know about Variables and can't reassign the variable's data attribute.)<br>\nThat would break any user code that assumes that the Tensors underlying a network's parameters will always stay in the same place in memory -- but that code would already be broken by someone calling .cuda(another_gpu) on the model.</p>", "body_text": "I guess the idea is that parameters are Variables, so modules should be able to silently move their underlying Tensors (after first checking if they're already in acceptable memory positions). There's no state in the module or function, just a lightweight (if ugly) check at the beginning of the forward pass of the module that the parameter tensors have the correct memory relationship to each other. (It can't be in the Function because the Function doesn't know about Variables and can't reassign the variable's data attribute.)\nThat would break any user code that assumes that the Tensors underlying a network's parameters will always stay in the same place in memory -- but that code would already be broken by someone calling .cuda(another_gpu) on the model.", "body": "I guess the idea is that parameters are Variables, so modules should be able to silently move their underlying Tensors (after first checking if they're already in acceptable memory positions). There's no state in the module or function, just a lightweight (if ugly) check at the beginning of the forward pass of the module that the parameter tensors have the correct memory relationship to each other. (It can't be in the Function because the Function doesn't know about Variables and can't reassign the variable's data attribute.)\r\nThat would break any user code that assumes that the Tensors underlying a network's parameters will always stay in the same place in memory -- but that code would already be broken by someone calling .cuda(another_gpu) on the model."}