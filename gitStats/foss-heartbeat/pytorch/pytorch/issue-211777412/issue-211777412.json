{"url": "https://api.github.com/repos/pytorch/pytorch/issues/914", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/914/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/914/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/914/events", "html_url": "https://github.com/pytorch/pytorch/issues/914", "id": 211777412, "node_id": "MDU6SXNzdWUyMTE3Nzc0MTI=", "number": 914, "title": "Manually unrolling cuDNN RNN OOM", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2017-03-03T19:16:43Z", "updated_at": "2017-07-24T22:30:26Z", "closed_at": "2017-07-24T22:30:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Manually unrolling cuDNN backend will cause memory usage to go sky high.</p>\n<p>Unrolled non-cuDNN pytorch takes ~1.8GB mem.<br>\nNon-unrolled cuDNN can take ~3GB mem.<br>\nManually unrolling over time in user script will take &gt;12GB mem.</p>\n<p>Important for attention models <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3359909\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bmccann\">@bmccann</a>. Repro code below.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> time <span class=\"pl-k\">import</span> sleep\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>torch.backends.cudnn.enabled=False</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>torch.cuda.set_device(1)</span>\n\ninput_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1024</span>\nhidden_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1024</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\ntime_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>\nlr   <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4e-2</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">initHidden</span>(<span class=\"pl-smi\">bsz</span>):\n    <span class=\"pl-k\">return</span> (Variable(torch.cuda.FloatTensor(<span class=\"pl-c1\">2</span>, bsz, hidden_size).zero_()),\n            Variable(torch.cuda.FloatTensor(<span class=\"pl-c1\">2</span>, bsz, hidden_size).zero_()))\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">resetHidden</span>(<span class=\"pl-smi\">hidden</span>):\n    <span class=\"pl-k\">return</span> (Variable(hidden[<span class=\"pl-c1\">0</span>].data.zero_()), Variable(hidden[<span class=\"pl-c1\">1</span>].data.zero_()))\n\nmodel <span class=\"pl-k\">=</span> nn.LSTM(input_size, hidden_size, <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>).cuda()\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(time_steps, batch_size, input_size).cuda()\ntarget <span class=\"pl-k\">=</span> torch.randn(time_steps, batch_size, hidden_size).cuda()\n\n\noptimizer <span class=\"pl-k\">=</span> optim.SGD(model.parameters(),\n                      <span class=\"pl-v\">lr</span> <span class=\"pl-k\">=</span> lr,\n                      <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>,\n                      <span class=\"pl-v\">dampening</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>,\n                      <span class=\"pl-v\">weight_decay</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n                     )\n\ncriterion <span class=\"pl-k\">=</span> nn.MSELoss().cuda()\n\nloss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\nhidden <span class=\"pl-k\">=</span> initHidden(batch_size)\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">input</span>)\ntarget <span class=\"pl-k\">=</span> Variable(target, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>):\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">9999</span>):\n        <span class=\"pl-c1\">print</span>(epoch)\n        loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        model.zero_grad()\n        optimizer.zero_grad()\n\n        hidden<span class=\"pl-k\">=</span>initHidden(batch_size)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>        output, hidden = model(input, hidden)</span>\n        outputs <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>)):\n            output, hidden <span class=\"pl-k\">=</span> model(<span class=\"pl-c1\">input</span>[j].view(<span class=\"pl-c1\">1</span>, <span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>[j].size()), hidden)\n            outputs.append(output)\n        outputs <span class=\"pl-k\">=</span> torch.cat(outputs, <span class=\"pl-c1\">0</span>)\n        output <span class=\"pl-k\">=</span> outputs\n\n        \n        loss <span class=\"pl-k\">=</span> criterion(output, target)\n        loss.backward(<span class=\"pl-v\">retain_variables</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n        \n        optimizer.step()\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Test ran in <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>( time.time() <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> seconds<span class=\"pl-pds\">\"</span></span>)</pre></div>", "body_text": "Manually unrolling cuDNN backend will cause memory usage to go sky high.\nUnrolled non-cuDNN pytorch takes ~1.8GB mem.\nNon-unrolled cuDNN can take ~3GB mem.\nManually unrolling over time in user script will take >12GB mem.\nImportant for attention models @bmccann. Repro code below.\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport time\nfrom torch.autograd import Variable\nfrom time import sleep\n\n#torch.backends.cudnn.enabled=False\n#torch.cuda.set_device(1)\n\ninput_size = 1024\nhidden_size = 1024*2\nbatch_size = 64\ntime_steps = 32*2\nlr   = 4e-2\n\ndef initHidden(bsz):\n    return (Variable(torch.cuda.FloatTensor(2, bsz, hidden_size).zero_()),\n            Variable(torch.cuda.FloatTensor(2, bsz, hidden_size).zero_()))\ndef resetHidden(hidden):\n    return (Variable(hidden[0].data.zero_()), Variable(hidden[1].data.zero_()))\n\nmodel = nn.LSTM(input_size, hidden_size, num_layers=2).cuda()\ninput = torch.randn(time_steps, batch_size, input_size).cuda()\ntarget = torch.randn(time_steps, batch_size, hidden_size).cuda()\n\n\noptimizer = optim.SGD(model.parameters(),\n                      lr = lr,\n                      momentum=0.9,\n                      dampening = 0.0,\n                      weight_decay = 0.0\n                     )\n\ncriterion = nn.MSELoss().cuda()\n\nloss = 0\n\nhidden = initHidden(batch_size)\ninput = Variable(input)\ntarget = Variable(target, requires_grad=False)\n\nfor i in range(1):\n    start = time.time()\n    for epoch in range(9999):\n        print(epoch)\n        loss = 0\n        model.zero_grad()\n        optimizer.zero_grad()\n\n        hidden=initHidden(batch_size)\n#        output, hidden = model(input, hidden)\n        outputs = []\n        for j in range(input.size(0)):\n            output, hidden = model(input[j].view(1, *input[j].size()), hidden)\n            outputs.append(output)\n        outputs = torch.cat(outputs, 0)\n        output = outputs\n\n        \n        loss = criterion(output, target)\n        loss.backward(retain_variables = True)\n        \n        optimizer.step()\n\n    print(\"Test ran in \" + str( time.time() - start) + \" seconds\")", "body": "Manually unrolling cuDNN backend will cause memory usage to go sky high. \r\n\r\nUnrolled non-cuDNN pytorch takes ~1.8GB mem.\r\nNon-unrolled cuDNN can take ~3GB mem.\r\nManually unrolling over time in user script will take >12GB mem.\r\n\r\nImportant for attention models @bmccann. Repro code below.\r\n\r\n```.py\r\nimport torch\r\nimport torch.optim as optim\r\nimport torch.nn as nn\r\nimport time\r\nfrom torch.autograd import Variable\r\nfrom time import sleep\r\n\r\n#torch.backends.cudnn.enabled=False\r\n#torch.cuda.set_device(1)\r\n\r\ninput_size = 1024\r\nhidden_size = 1024*2\r\nbatch_size = 64\r\ntime_steps = 32*2\r\nlr   = 4e-2\r\n\r\ndef initHidden(bsz):\r\n    return (Variable(torch.cuda.FloatTensor(2, bsz, hidden_size).zero_()),\r\n            Variable(torch.cuda.FloatTensor(2, bsz, hidden_size).zero_()))\r\ndef resetHidden(hidden):\r\n    return (Variable(hidden[0].data.zero_()), Variable(hidden[1].data.zero_()))\r\n\r\nmodel = nn.LSTM(input_size, hidden_size, num_layers=2).cuda()\r\ninput = torch.randn(time_steps, batch_size, input_size).cuda()\r\ntarget = torch.randn(time_steps, batch_size, hidden_size).cuda()\r\n\r\n\r\noptimizer = optim.SGD(model.parameters(),\r\n                      lr = lr,\r\n                      momentum=0.9,\r\n                      dampening = 0.0,\r\n                      weight_decay = 0.0\r\n                     )\r\n\r\ncriterion = nn.MSELoss().cuda()\r\n\r\nloss = 0\r\n\r\nhidden = initHidden(batch_size)\r\ninput = Variable(input)\r\ntarget = Variable(target, requires_grad=False)\r\n\r\nfor i in range(1):\r\n    start = time.time()\r\n    for epoch in range(9999):\r\n        print(epoch)\r\n        loss = 0\r\n        model.zero_grad()\r\n        optimizer.zero_grad()\r\n\r\n        hidden=initHidden(batch_size)\r\n#        output, hidden = model(input, hidden)\r\n        outputs = []\r\n        for j in range(input.size(0)):\r\n            output, hidden = model(input[j].view(1, *input[j].size()), hidden)\r\n            outputs.append(output)\r\n        outputs = torch.cat(outputs, 0)\r\n        output = outputs\r\n\r\n        \r\n        loss = criterion(output, target)\r\n        loss.backward(retain_variables = True)\r\n        \r\n        optimizer.step()\r\n\r\n    print(\"Test ran in \" + str( time.time() - start) + \" seconds\")\r\n```"}