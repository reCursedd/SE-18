{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2520", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2520/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2520/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2520/events", "html_url": "https://github.com/pytorch/pytorch/issues/2520", "id": 252391863, "node_id": "MDU6SXNzdWUyNTIzOTE4NjM=", "number": 2520, "title": "DistributedDataParallel doesn't exit properly; Leaves reduction threads running", "user": {"login": "ArEsKay3", "id": 6547143, "node_id": "MDQ6VXNlcjY1NDcxNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6547143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArEsKay3", "html_url": "https://github.com/ArEsKay3", "followers_url": "https://api.github.com/users/ArEsKay3/followers", "following_url": "https://api.github.com/users/ArEsKay3/following{/other_user}", "gists_url": "https://api.github.com/users/ArEsKay3/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArEsKay3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArEsKay3/subscriptions", "organizations_url": "https://api.github.com/users/ArEsKay3/orgs", "repos_url": "https://api.github.com/users/ArEsKay3/repos", "events_url": "https://api.github.com/users/ArEsKay3/events{/privacy}", "received_events_url": "https://api.github.com/users/ArEsKay3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-23T19:32:59Z", "updated_at": "2017-08-24T10:32:49Z", "closed_at": "2017-08-24T10:32:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When using the DDP module the process always hangs. I believe because the reduction threads are in a <code>while True:</code> loop.</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/4c69697d2acbbe8e418a0921464c09aaf09da82a/torch/nn/parallel/distributed.py#L313-L315\">pytorch/torch/nn/parallel/distributed.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 313 to 315\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/4c69697d2acbbe8e418a0921464c09aaf09da82a\">4c69697</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L313\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"313\"></td>\n          <td id=\"LC313\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">with</span> torch.cuda.device(device_ids[<span class=\"pl-c1\">0</span>]): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L314\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"314\"></td>\n          <td id=\"LC314\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L315\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"315\"></td>\n          <td id=\"LC315\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         _process_batch()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> just to have a clear scope</span> </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Repro Code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> ctypes\n\nlib <span class=\"pl-k\">=</span> ctypes.cdll.LoadLibrary(<span class=\"pl-c1\">None</span>)\n\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.pool <span class=\"pl-k\">=</span> nn.MaxPool2d(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">16</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">120</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">120</span>, <span class=\"pl-c1\">84</span>)\n        <span class=\"pl-c1\">self</span>.fc3 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">84</span>, <span class=\"pl-c1\">10</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>        self.bn1 = torch.nn.BatchNorm2d(6)</span>\n        <span class=\"pl-c1\">self</span>.bn2 <span class=\"pl-k\">=</span> torch.nn.BatchNorm2d(<span class=\"pl-c1\">16</span>)\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu((<span class=\"pl-c1\">self</span>.conv1(x))))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.bn2(<span class=\"pl-c1\">self</span>.conv2(x))))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">5</span>)\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc2(x))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>        x = self.fc3(x)</span>\n        <span class=\"pl-k\">return</span> x\n    \n    \nnet <span class=\"pl-k\">=</span> Net().cuda()\nworld_size<span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>\ndist_url <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tcp://224.66.41.62:23456<span class=\"pl-pds\">'</span></span>\ndist_backend <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>gloo<span class=\"pl-pds\">'</span></span>\n\ndist.init_process_group(<span class=\"pl-v\">backend</span><span class=\"pl-k\">=</span>dist_backend, <span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span>dist_url, <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>world_size)\ntorch.cuda.synchronize()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>initialized<span class=\"pl-pds\">\"</span></span>)\nnet <span class=\"pl-k\">=</span> torch.nn.parallel.DistributedDataParallel(net)\ntorch.cuda.synchronize()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>network created<span class=\"pl-pds\">\"</span></span>)\ninp <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>))\ntorch.cuda.synchronize()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input created<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>):\n    out <span class=\"pl-k\">=</span> net.forward(inp)\n    loss <span class=\"pl-k\">=</span> out.sum()\n    loss.backward()\n    torch.cuda.synchronize()\n\nlib.cudaProfilerStop()</pre></div>", "body_text": "When using the DDP module the process always hangs. I believe because the reduction threads are in a while True: loop.\n\n  \n    \n      pytorch/torch/nn/parallel/distributed.py\n    \n    \n        Lines 313 to 315\n      in\n      4c69697\n    \n    \n    \n    \n\n        \n          \n           with torch.cuda.device(device_ids[0]): \n        \n\n        \n          \n               while True: \n        \n\n        \n          \n                   _process_batch()  # just to have a clear scope \n        \n    \n  \n\n\nRepro Code:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport ctypes\n\nlib = ctypes.cdll.LoadLibrary(None)\n\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n#        self.bn1 = torch.nn.BatchNorm2d(6)\n        self.bn2 = torch.nn.BatchNorm2d(16)\n        \n    def forward(self, x):\n        x = self.pool(F.relu((self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n#        x = self.fc3(x)\n        return x\n    \n    \nnet = Net().cuda()\nworld_size=2\ndist_url = 'tcp://224.66.41.62:23456'\ndist_backend = 'gloo'\n\ndist.init_process_group(backend=dist_backend, init_method=dist_url, world_size=world_size)\ntorch.cuda.synchronize()\nprint(\"initialized\")\nnet = torch.nn.parallel.DistributedDataParallel(net)\ntorch.cuda.synchronize()\nprint(\"network created\")\ninp = Variable(torch.randn(64, 3, 32,32))\ntorch.cuda.synchronize()\nprint(\"input created\")\n\n\nfor i in range(3):\n    out = net.forward(inp)\n    loss = out.sum()\n    loss.backward()\n    torch.cuda.synchronize()\n\nlib.cudaProfilerStop()", "body": "\r\nWhen using the DDP module the process always hangs. I believe because the reduction threads are in a `while True:` loop.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4c69697d2acbbe8e418a0921464c09aaf09da82a/torch/nn/parallel/distributed.py#L313-L315\r\n\r\nRepro Code:\r\n\r\n``` python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.distributed as dist\r\nimport ctypes\r\n\r\nlib = ctypes.cdll.LoadLibrary(None)\r\n\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 6, 5)\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(6, 16, 5)\r\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n        self.fc2 = nn.Linear(120, 84)\r\n        self.fc3 = nn.Linear(84, 10)\r\n#        self.bn1 = torch.nn.BatchNorm2d(6)\r\n        self.bn2 = torch.nn.BatchNorm2d(16)\r\n        \r\n    def forward(self, x):\r\n        x = self.pool(F.relu((self.conv1(x))))\r\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\r\n        x = x.view(-1, 16 * 5 * 5)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n#        x = self.fc3(x)\r\n        return x\r\n    \r\n    \r\nnet = Net().cuda()\r\nworld_size=2\r\ndist_url = 'tcp://224.66.41.62:23456'\r\ndist_backend = 'gloo'\r\n\r\ndist.init_process_group(backend=dist_backend, init_method=dist_url, world_size=world_size)\r\ntorch.cuda.synchronize()\r\nprint(\"initialized\")\r\nnet = torch.nn.parallel.DistributedDataParallel(net)\r\ntorch.cuda.synchronize()\r\nprint(\"network created\")\r\ninp = Variable(torch.randn(64, 3, 32,32))\r\ntorch.cuda.synchronize()\r\nprint(\"input created\")\r\n\r\n\r\nfor i in range(3):\r\n    out = net.forward(inp)\r\n    loss = out.sum()\r\n    loss.backward()\r\n    torch.cuda.synchronize()\r\n\r\nlib.cudaProfilerStop()\r\n```"}