{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9221", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9221/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9221/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9221/events", "html_url": "https://github.com/pytorch/pytorch/issues/9221", "id": 339080988, "node_id": "MDU6SXNzdWUzMzkwODA5ODg=", "number": 9221, "title": "RNN weights are not Xavier-initialized", "user": {"login": "tavianator", "id": 1692591, "node_id": "MDQ6VXNlcjE2OTI1OTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1692591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tavianator", "html_url": "https://github.com/tavianator", "followers_url": "https://api.github.com/users/tavianator/followers", "following_url": "https://api.github.com/users/tavianator/following{/other_user}", "gists_url": "https://api.github.com/users/tavianator/gists{/gist_id}", "starred_url": "https://api.github.com/users/tavianator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tavianator/subscriptions", "organizations_url": "https://api.github.com/users/tavianator/orgs", "repos_url": "https://api.github.com/users/tavianator/repos", "events_url": "https://api.github.com/users/tavianator/events{/privacy}", "received_events_url": "https://api.github.com/users/tavianator/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-07-06T21:50:52Z", "updated_at": "2018-07-10T13:24:39Z", "closed_at": "2018-07-10T13:24:39Z", "author_association": "NONE", "body_html": "<p>The weights of the PyTorch RNN implementations (<code>torch.nn.LSTM</code>, <code>torch.nn.GRU</code>) are initialized with something that <a href=\"https://github.com/pytorch/pytorch/blob/7fbd57091dc39e86b2996ea132bbdcf7ae4997ce/torch/nn/modules/rnn.py#L115\">appears</a> to be like Xavier initialization, but isn't actually:</p>\n<pre><code>def reset_parameters(self):\n    stdv = 1.0 / math.sqrt(self.hidden_size)\n    for weight in self.parameters():\n        weight.data.uniform_(-stdv, stdv)\n</code></pre>\n<p>From the name <code>stdv</code> it looks like the code wants a distribution with that standard deviation.  But the uniform distribution \ud835\udce4(a, b) has standard deviation (b - a)^2 / 12, meaning the weights are initialized with variance 1/(3*N) instead of 1/N as is usually done for Xavier initialization of RNNs.  This could be accomplished like this:</p>\n<pre><code>def reset_parameters(self):\n    size = math.sqrt(3.0 / self.hidden_size)\n    for weight in self.parameters():\n        weight.data.uniform_(-size, size)\n</code></pre>\n<p>or possibly like this:</p>\n<pre><code>def reset_parameters(self):\n    stdv = 1.0 / math.sqrt(self.hidden_size)\n    for weight in self.parameters():\n        weight.data.normal_(0, stdv)\n</code></pre>\n<p>Also, I'm a bit surprised that the bias vectors receive this treatment, rather than being zero-initialized.</p>", "body_text": "The weights of the PyTorch RNN implementations (torch.nn.LSTM, torch.nn.GRU) are initialized with something that appears to be like Xavier initialization, but isn't actually:\ndef reset_parameters(self):\n    stdv = 1.0 / math.sqrt(self.hidden_size)\n    for weight in self.parameters():\n        weight.data.uniform_(-stdv, stdv)\n\nFrom the name stdv it looks like the code wants a distribution with that standard deviation.  But the uniform distribution \ud835\udce4(a, b) has standard deviation (b - a)^2 / 12, meaning the weights are initialized with variance 1/(3*N) instead of 1/N as is usually done for Xavier initialization of RNNs.  This could be accomplished like this:\ndef reset_parameters(self):\n    size = math.sqrt(3.0 / self.hidden_size)\n    for weight in self.parameters():\n        weight.data.uniform_(-size, size)\n\nor possibly like this:\ndef reset_parameters(self):\n    stdv = 1.0 / math.sqrt(self.hidden_size)\n    for weight in self.parameters():\n        weight.data.normal_(0, stdv)\n\nAlso, I'm a bit surprised that the bias vectors receive this treatment, rather than being zero-initialized.", "body": "The weights of the PyTorch RNN implementations (`torch.nn.LSTM`, `torch.nn.GRU`) are initialized with something that [appears](https://github.com/pytorch/pytorch/blob/7fbd57091dc39e86b2996ea132bbdcf7ae4997ce/torch/nn/modules/rnn.py#L115) to be like Xavier initialization, but isn't actually:\r\n\r\n    def reset_parameters(self):\r\n        stdv = 1.0 / math.sqrt(self.hidden_size)\r\n        for weight in self.parameters():\r\n            weight.data.uniform_(-stdv, stdv)\r\n\r\nFrom the name `stdv` it looks like the code wants a distribution with that standard deviation.  But the uniform distribution \ud835\udce4(a, b) has standard deviation (b - a)^2 / 12, meaning the weights are initialized with variance 1/(3*N) instead of 1/N as is usually done for Xavier initialization of RNNs.  This could be accomplished like this:\r\n\r\n    def reset_parameters(self):\r\n        size = math.sqrt(3.0 / self.hidden_size)\r\n        for weight in self.parameters():\r\n            weight.data.uniform_(-size, size)\r\n\r\nor possibly like this:\r\n\r\n    def reset_parameters(self):\r\n        stdv = 1.0 / math.sqrt(self.hidden_size)\r\n        for weight in self.parameters():\r\n            weight.data.normal_(0, stdv)\r\n\r\nAlso, I'm a bit surprised that the bias vectors receive this treatment, rather than being zero-initialized."}