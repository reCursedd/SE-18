{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/344022658", "html_url": "https://github.com/pytorch/pytorch/pull/3341#issuecomment-344022658", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3341", "id": 344022658, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NDAyMjY1OA==", "user": {"login": "nhynes", "id": 2353785, "node_id": "MDQ6VXNlcjIzNTM3ODU=", "avatar_url": "https://avatars2.githubusercontent.com/u/2353785?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nhynes", "html_url": "https://github.com/nhynes", "followers_url": "https://api.github.com/users/nhynes/followers", "following_url": "https://api.github.com/users/nhynes/following{/other_user}", "gists_url": "https://api.github.com/users/nhynes/gists{/gist_id}", "starred_url": "https://api.github.com/users/nhynes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nhynes/subscriptions", "organizations_url": "https://api.github.com/users/nhynes/orgs", "repos_url": "https://api.github.com/users/nhynes/repos", "events_url": "https://api.github.com/users/nhynes/events{/privacy}", "received_events_url": "https://api.github.com/users/nhynes/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T19:03:13Z", "updated_at": "2017-11-13T19:52:19Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>(or whatever is the standard approach for cuda-izing things)</p>\n</blockquote>\n<p>Probably something vaguely similar to</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-c1\">diff --git a/torch/nn/functional.py b/torch/nn/functional.py</span>\nindex 81654b2..55b272f 100644\n<span class=\"pl-md\">--- a/torch/nn/functional.py</span>\n<span class=\"pl-mi1\">+++ b/torch/nn/functional.py</span>\n<span class=\"pl-mdr\">@@ -764,7 +764,7 @@</span> def softmax(input, dim=None, _stacklevel=3):\n     return torch._C._nn.softmax(input, dim)\n\n\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>def sample_gumbel(shape, eps=1e-10):</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>def sample_gumbel(shape, eps=1e-10, out=None):</span>\n     \"\"\"\n     Sample from Gumbel(0, 1)\n\n<span class=\"pl-mdr\">@@ -772,7 +772,7 @@</span> def sample_gumbel(shape, eps=1e-10):\n     https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n     (MIT license)\n     \"\"\"\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    U = torch.rand(shape).float()</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)</span>\n     return - torch.log(eps - torch.log(U + eps))\n\n\n<span class=\"pl-mdr\">@@ -785,7 +785,7 @@</span> def gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n     (MIT license)\n     \"\"\"\n     dims = len(logits.size())\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    gumbel_noise = sample_gumbel(logits.size(), eps=eps)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    gumbel_noise = sample_gumbel(logits.size(), eps=eps, out=logits.data.new())</span>\n     y = logits + Variable(gumbel_noise)\n     return softmax(y / tau, dims - 1)\n\n<span class=\"pl-mdr\">@@ -816,7 +816,7 @@</span> def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\n         _, k = y_soft.data.max(-1)\n         # this bit is based on\n         # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>        y_hard = torch.FloatTensor(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        y_hard = logits.data.new(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)</span>\n         # this cool bit of code achieves two things:\n         # - makes the output value exactly one-hot (since we add then\n         #   subtract y_soft value)</pre></div>", "body_text": "(or whatever is the standard approach for cuda-izing things)\n\nProbably something vaguely similar to\ndiff --git a/torch/nn/functional.py b/torch/nn/functional.py\nindex 81654b2..55b272f 100644\n--- a/torch/nn/functional.py\n+++ b/torch/nn/functional.py\n@@ -764,7 +764,7 @@ def softmax(input, dim=None, _stacklevel=3):\n     return torch._C._nn.softmax(input, dim)\n\n\n-def sample_gumbel(shape, eps=1e-10):\n+def sample_gumbel(shape, eps=1e-10, out=None):\n     \"\"\"\n     Sample from Gumbel(0, 1)\n\n@@ -772,7 +772,7 @@ def sample_gumbel(shape, eps=1e-10):\n     https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n     (MIT license)\n     \"\"\"\n-    U = torch.rand(shape).float()\n+    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)\n     return - torch.log(eps - torch.log(U + eps))\n\n\n@@ -785,7 +785,7 @@ def gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n     (MIT license)\n     \"\"\"\n     dims = len(logits.size())\n-    gumbel_noise = sample_gumbel(logits.size(), eps=eps)\n+    gumbel_noise = sample_gumbel(logits.size(), eps=eps, out=logits.data.new())\n     y = logits + Variable(gumbel_noise)\n     return softmax(y / tau, dims - 1)\n\n@@ -816,7 +816,7 @@ def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\n         _, k = y_soft.data.max(-1)\n         # this bit is based on\n         # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5\n-        y_hard = torch.FloatTensor(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)\n+        y_hard = logits.data.new(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)\n         # this cool bit of code achieves two things:\n         # - makes the output value exactly one-hot (since we add then\n         #   subtract y_soft value)", "body": "> (or whatever is the standard approach for cuda-izing things)\r\n\r\nProbably something vaguely similar to\r\n```diff\r\ndiff --git a/torch/nn/functional.py b/torch/nn/functional.py\r\nindex 81654b2..55b272f 100644\r\n--- a/torch/nn/functional.py\r\n+++ b/torch/nn/functional.py\r\n@@ -764,7 +764,7 @@ def softmax(input, dim=None, _stacklevel=3):\r\n     return torch._C._nn.softmax(input, dim)\r\n\r\n\r\n-def sample_gumbel(shape, eps=1e-10):\r\n+def sample_gumbel(shape, eps=1e-10, out=None):\r\n     \"\"\"\r\n     Sample from Gumbel(0, 1)\r\n\r\n@@ -772,7 +772,7 @@ def sample_gumbel(shape, eps=1e-10):\r\n     https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\r\n     (MIT license)\r\n     \"\"\"\r\n-    U = torch.rand(shape).float()\r\n+    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)\r\n     return - torch.log(eps - torch.log(U + eps))\r\n\r\n\r\n@@ -785,7 +785,7 @@ def gumbel_softmax_sample(logits, tau=1, eps=1e-10):\r\n     (MIT license)\r\n     \"\"\"\r\n     dims = len(logits.size())\r\n-    gumbel_noise = sample_gumbel(logits.size(), eps=eps)\r\n+    gumbel_noise = sample_gumbel(logits.size(), eps=eps, out=logits.data.new())\r\n     y = logits + Variable(gumbel_noise)\r\n     return softmax(y / tau, dims - 1)\r\n\r\n@@ -816,7 +816,7 @@ def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\r\n         _, k = y_soft.data.max(-1)\r\n         # this bit is based on\r\n         # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5\r\n-        y_hard = torch.FloatTensor(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)\r\n+        y_hard = logits.data.new(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)\r\n         # this cool bit of code achieves two things:\r\n         # - makes the output value exactly one-hot (since we add then\r\n         #   subtract y_soft value)\r\n```"}