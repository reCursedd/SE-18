{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1509", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1509/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1509/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1509/events", "html_url": "https://github.com/pytorch/pytorch/issues/1509", "id": 226941106, "node_id": "MDU6SXNzdWUyMjY5NDExMDY=", "number": 1509, "title": "GPU memory consumption increases while training", "user": {"login": "EthanZhangYi", "id": 13479831, "node_id": "MDQ6VXNlcjEzNDc5ODMx", "avatar_url": "https://avatars3.githubusercontent.com/u/13479831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EthanZhangYi", "html_url": "https://github.com/EthanZhangYi", "followers_url": "https://api.github.com/users/EthanZhangYi/followers", "following_url": "https://api.github.com/users/EthanZhangYi/following{/other_user}", "gists_url": "https://api.github.com/users/EthanZhangYi/gists{/gist_id}", "starred_url": "https://api.github.com/users/EthanZhangYi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EthanZhangYi/subscriptions", "organizations_url": "https://api.github.com/users/EthanZhangYi/orgs", "repos_url": "https://api.github.com/users/EthanZhangYi/repos", "events_url": "https://api.github.com/users/EthanZhangYi/events{/privacy}", "received_events_url": "https://api.github.com/users/EthanZhangYi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-08T05:26:50Z", "updated_at": "2017-05-08T05:30:01Z", "closed_at": "2017-05-08T05:29:56Z", "author_association": "NONE", "body_html": "<p>Hello, all<br>\nI am new to Pytorch and I meet a strange GPU memory behavior while training a CNN model for semantic segmentation. Batchsize = 1, and there are totally 100 image-label pairs in trainset, thus 100 iterations per epoch. However the <strong>GPU memory consumption increases a lot at the first several iterations while training</strong>.</p>\n<p>[Platform] GTX TITAN X (12G), CUDA-7.5, cuDNN-5.0</p>\n<blockquote>\n<p>torch.backends.cudnn.enabled = False<br>\ntorch.backends.cudnn.benchmark = False</p>\n</blockquote>\n<p>Then GPU memory consumption is <strong>2934M -- 4413M -- 4433M -- 4537M -- 4537M -- 4537M</strong> at the first six iterations.</p>\n<blockquote>\n<p>torch.backends.cudnn.enabled = True<br>\ntorch.backends.cudnn.benchmark = True</p>\n</blockquote>\n<p>Then GPU memory consumption is <strong>1686M -- 1791M -- 1791M -- 1791M -- 1791M -- 1791M</strong> at the first six iterations.</p>\n<p><strong>Why GPU memory consumption increases while training, especially, increases so largely while no cuDNN? (In my opinion, GPU memory consumption won't increase while the CNN has been build and starts training)</strong></p>\n<p>Does anyone meet the same problem? Or could anyone give some help?</p>\n<p>This is the code snippet:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">train_loader</span>, <span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">criterion</span>, <span class=\"pl-smi\">optimizer</span>, <span class=\"pl-smi\">epoch</span>):\n    batch_time <span class=\"pl-k\">=</span> AverageMeter()\n    data_time <span class=\"pl-k\">=</span> AverageMeter()\n    losses <span class=\"pl-k\">=</span> AverageMeter()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> switch to train mode</span>\n    model.train()\n\n    end <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> i, (<span class=\"pl-c1\">input</span>, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> measure data loading time</span>\n        data_time.update(time.time() <span class=\"pl-k\">-</span> end)\n\n        target <span class=\"pl-k\">=</span> target.long()\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        target <span class=\"pl-k\">=</span> target.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        input_var <span class=\"pl-k\">=</span> torch.autograd.Variable(<span class=\"pl-c1\">input</span>)\n        target_var <span class=\"pl-k\">=</span> torch.autograd.Variable(target)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute output</span>\n        output <span class=\"pl-k\">=</span> model(input_var)\n        loss <span class=\"pl-k\">=</span> criterion(output, target_var)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> record loss</span>\n        losses.update(loss.data[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>))\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute gradient and do SGD step</span>\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> measure elapsed time</span>\n        batch_time.update(time.time() <span class=\"pl-k\">-</span> end)\n        end <span class=\"pl-k\">=</span> time.time()\n\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> args.print_freq <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Epoch: [<span class=\"pl-c1\">{0}</span>][<span class=\"pl-c1\">{1}</span>/<span class=\"pl-c1\">{2}</span>]<span class=\"pl-cce\">\\t</span><span class=\"pl-pds\">'</span></span>\n                  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Time <span class=\"pl-c1\">{batch_time.val<span class=\"pl-k\">:.3f</span>}</span> (<span class=\"pl-c1\">{batch_time.avg<span class=\"pl-k\">:.3f</span>}</span>)<span class=\"pl-cce\">\\t</span><span class=\"pl-pds\">'</span></span>\n                  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Data <span class=\"pl-c1\">{data_time.val<span class=\"pl-k\">:.3f</span>}</span> (<span class=\"pl-c1\">{data_time.avg<span class=\"pl-k\">:.3f</span>}</span>)<span class=\"pl-cce\">\\t</span><span class=\"pl-pds\">'</span></span>\n                  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loss <span class=\"pl-c1\">{loss.val<span class=\"pl-k\">:.4f</span>}</span> (<span class=\"pl-c1\">{loss.avg<span class=\"pl-k\">:.4f</span>}</span>)<span class=\"pl-pds\">'</span></span>.format(\n                   epoch, i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">len</span>(train_loader),\n                   <span class=\"pl-v\">batch_time</span><span class=\"pl-k\">=</span>batch_time,\n                   <span class=\"pl-v\">data_time</span><span class=\"pl-k\">=</span>data_time,\n                   <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>losses))</pre></div>", "body_text": "Hello, all\nI am new to Pytorch and I meet a strange GPU memory behavior while training a CNN model for semantic segmentation. Batchsize = 1, and there are totally 100 image-label pairs in trainset, thus 100 iterations per epoch. However the GPU memory consumption increases a lot at the first several iterations while training.\n[Platform] GTX TITAN X (12G), CUDA-7.5, cuDNN-5.0\n\ntorch.backends.cudnn.enabled = False\ntorch.backends.cudnn.benchmark = False\n\nThen GPU memory consumption is 2934M -- 4413M -- 4433M -- 4537M -- 4537M -- 4537M at the first six iterations.\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\n\nThen GPU memory consumption is 1686M -- 1791M -- 1791M -- 1791M -- 1791M -- 1791M at the first six iterations.\nWhy GPU memory consumption increases while training, especially, increases so largely while no cuDNN? (In my opinion, GPU memory consumption won't increase while the CNN has been build and starts training)\nDoes anyone meet the same problem? Or could anyone give some help?\nThis is the code snippet:\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.long()\n        input = input.cuda(async=True)\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # record loss\n        losses.update(loss.data[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n                   epoch, i+1, len(train_loader),\n                   batch_time=batch_time,\n                   data_time=data_time,\n                   loss=losses))", "body": "Hello, all\r\nI am new to Pytorch and I meet a strange GPU memory behavior while training a CNN model for semantic segmentation. Batchsize = 1, and there are totally 100 image-label pairs in trainset, thus 100 iterations per epoch. However the **GPU memory consumption increases a lot at the first several iterations while training**.\r\n\r\n[Platform] GTX TITAN X (12G), CUDA-7.5, cuDNN-5.0\r\n\r\n> torch.backends.cudnn.enabled = False\r\n> torch.backends.cudnn.benchmark = False\r\n\r\nThen GPU memory consumption is **2934M -- 4413M -- 4433M -- 4537M -- 4537M -- 4537M** at the first six iterations.\r\n\r\n> torch.backends.cudnn.enabled = True\r\n> torch.backends.cudnn.benchmark = True\r\n\r\nThen GPU memory consumption is **1686M -- 1791M -- 1791M -- 1791M -- 1791M -- 1791M** at the first six iterations.\r\n\r\n**Why GPU memory consumption increases while training, especially, increases so largely while no cuDNN? (In my opinion, GPU memory consumption won't increase while the CNN has been build and starts training)** \r\n\r\nDoes anyone meet the same problem? Or could anyone give some help?\r\n\r\nThis is the code snippet:\r\n\r\n```Python\r\ndef train(train_loader, model, criterion, optimizer, epoch):\r\n    batch_time = AverageMeter()\r\n    data_time = AverageMeter()\r\n    losses = AverageMeter()\r\n\r\n    # switch to train mode\r\n    model.train()\r\n\r\n    end = time.time()\r\n    for i, (input, target) in enumerate(train_loader):\r\n        # measure data loading time\r\n        data_time.update(time.time() - end)\r\n\r\n        target = target.long()\r\n        input = input.cuda(async=True)\r\n        target = target.cuda(async=True)\r\n        input_var = torch.autograd.Variable(input)\r\n        target_var = torch.autograd.Variable(target)\r\n\r\n        # compute output\r\n        output = model(input_var)\r\n        loss = criterion(output, target_var)\r\n\r\n        # record loss\r\n        losses.update(loss.data[0], input.size(0))\r\n\r\n        # compute gradient and do SGD step\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # measure elapsed time\r\n        batch_time.update(time.time() - end)\r\n        end = time.time()\r\n\r\n        if i % args.print_freq == 0:\r\n            print('Epoch: [{0}][{1}/{2}]\\t'\r\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\r\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\r\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\r\n                   epoch, i+1, len(train_loader),\r\n                   batch_time=batch_time,\r\n                   data_time=data_time,\r\n                   loss=losses))\r\n```"}