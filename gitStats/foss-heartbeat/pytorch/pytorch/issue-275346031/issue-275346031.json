{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3791", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3791/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3791/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3791/events", "html_url": "https://github.com/pytorch/pytorch/issues/3791", "id": 275346031, "node_id": "MDU6SXNzdWUyNzUzNDYwMzE=", "number": 3791, "title": "Make pytest stop printing docstrings in its default diagnostic output", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-11-20T12:54:51Z", "updated_at": "2017-11-21T09:56:52Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Sample:</p>\n<pre><code>_________________________________ TestNN.test_ConvTranspose3d_dilated_cuda ________________________[67/1868]\n\nself = &lt;test_nn.TestNN testMethod=test_ConvTranspose3d_dilated_cuda&gt;\ntest = &lt;test_nn.NewModuleTest object at 0x7f0b865920b8&gt;\n\n&gt;   setattr(TestNN, cuda_test_name, lambda self, test=test: test.test_cuda(self))\n\ntest_nn.py:3636: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncommon_nn.py:850: in test_cuda\n    gpu_gradInput = test_case._backward(gpu_module, gpu_input, gpu_output, gpu_gradOutput)\ntest_nn.py:237: in _backward\n    output.backward(grad_output, retain_graph=True)\n../torch/autograd/variable.py:168: in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nvariables = (Variable containing:\n(0 ,0 ,0 ,.,.) = \n -5.6280e-02 -1.3005e-01  3.2031e-01 -4.1098e-03  2.5253e-01  1.3095e-01\n  9.9...4e-01  1.4101e-01  3.8256e-01 -1.2257e-01  3.1663e-03 -4.4487e-01\n[torch.cuda.FloatTensor of size 1x3x6x9x6 (GPU 0)]\n,)\ngrad_variables = (Variable containing:\n(0 ,0 ,0 ,.,.) = \n   0   1   0   0   1   1\n   0   1   1   0   1   0\n   1   0   1   0   1   0\n   ...  1   0   1   1\n   0   0   0   0   1   1\n   0   1   1   1   1   0\n[torch.cuda.FloatTensor of size 1x3x6x9x6 (GPU 0)]\n,)\nretain_graph = True, create_graph = False, retain_variables = None\n\n    def backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None\n):\n        \"\"\"Computes the sum of gradients of given variables w.r.t. graph leaves.\n    \n        The graph is differentiated using the chain rule. If any of ``variables``\n        are non-scalar (i.e. their data has more than one element) and require\n        gradient, the function additionally requires specifying ``grad_variables``.\n</code></pre>\n<p>I guess it is trying to be helpfully chatty, but I don't need to see a big pile of docstring...</p>", "body_text": "Sample:\n_________________________________ TestNN.test_ConvTranspose3d_dilated_cuda ________________________[67/1868]\n\nself = <test_nn.TestNN testMethod=test_ConvTranspose3d_dilated_cuda>\ntest = <test_nn.NewModuleTest object at 0x7f0b865920b8>\n\n>   setattr(TestNN, cuda_test_name, lambda self, test=test: test.test_cuda(self))\n\ntest_nn.py:3636: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ncommon_nn.py:850: in test_cuda\n    gpu_gradInput = test_case._backward(gpu_module, gpu_input, gpu_output, gpu_gradOutput)\ntest_nn.py:237: in _backward\n    output.backward(grad_output, retain_graph=True)\n../torch/autograd/variable.py:168: in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nvariables = (Variable containing:\n(0 ,0 ,0 ,.,.) = \n -5.6280e-02 -1.3005e-01  3.2031e-01 -4.1098e-03  2.5253e-01  1.3095e-01\n  9.9...4e-01  1.4101e-01  3.8256e-01 -1.2257e-01  3.1663e-03 -4.4487e-01\n[torch.cuda.FloatTensor of size 1x3x6x9x6 (GPU 0)]\n,)\ngrad_variables = (Variable containing:\n(0 ,0 ,0 ,.,.) = \n   0   1   0   0   1   1\n   0   1   1   0   1   0\n   1   0   1   0   1   0\n   ...  1   0   1   1\n   0   0   0   0   1   1\n   0   1   1   1   1   0\n[torch.cuda.FloatTensor of size 1x3x6x9x6 (GPU 0)]\n,)\nretain_graph = True, create_graph = False, retain_variables = None\n\n    def backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None\n):\n        \"\"\"Computes the sum of gradients of given variables w.r.t. graph leaves.\n    \n        The graph is differentiated using the chain rule. If any of ``variables``\n        are non-scalar (i.e. their data has more than one element) and require\n        gradient, the function additionally requires specifying ``grad_variables``.\n\nI guess it is trying to be helpfully chatty, but I don't need to see a big pile of docstring...", "body": "Sample:\r\n\r\n```\r\n_________________________________ TestNN.test_ConvTranspose3d_dilated_cuda ________________________[67/1868]\r\n\r\nself = <test_nn.TestNN testMethod=test_ConvTranspose3d_dilated_cuda>\r\ntest = <test_nn.NewModuleTest object at 0x7f0b865920b8>\r\n\r\n>   setattr(TestNN, cuda_test_name, lambda self, test=test: test.test_cuda(self))\r\n\r\ntest_nn.py:3636: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ncommon_nn.py:850: in test_cuda\r\n    gpu_gradInput = test_case._backward(gpu_module, gpu_input, gpu_output, gpu_gradOutput)\r\ntest_nn.py:237: in _backward\r\n    output.backward(grad_output, retain_graph=True)\r\n../torch/autograd/variable.py:168: in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nvariables = (Variable containing:\r\n(0 ,0 ,0 ,.,.) = \r\n -5.6280e-02 -1.3005e-01  3.2031e-01 -4.1098e-03  2.5253e-01  1.3095e-01\r\n  9.9...4e-01  1.4101e-01  3.8256e-01 -1.2257e-01  3.1663e-03 -4.4487e-01\r\n[torch.cuda.FloatTensor of size 1x3x6x9x6 (GPU 0)]\r\n,)\r\ngrad_variables = (Variable containing:\r\n(0 ,0 ,0 ,.,.) = \r\n   0   1   0   0   1   1\r\n   0   1   1   0   1   0\r\n   1   0   1   0   1   0\r\n   ...  1   0   1   1\r\n   0   0   0   0   1   1\r\n   0   1   1   1   1   0\r\n[torch.cuda.FloatTensor of size 1x3x6x9x6 (GPU 0)]\r\n,)\r\nretain_graph = True, create_graph = False, retain_variables = None\r\n\r\n    def backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None\r\n):\r\n        \"\"\"Computes the sum of gradients of given variables w.r.t. graph leaves.\r\n    \r\n        The graph is differentiated using the chain rule. If any of ``variables``\r\n        are non-scalar (i.e. their data has more than one element) and require\r\n        gradient, the function additionally requires specifying ``grad_variables``.\r\n```\r\n\r\nI guess it is trying to be helpfully chatty, but I don't need to see a big pile of docstring..."}