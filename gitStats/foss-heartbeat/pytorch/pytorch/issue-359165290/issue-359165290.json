{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11532", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11532/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11532/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11532/events", "html_url": "https://github.com/pytorch/pytorch/issues/11532", "id": 359165290, "node_id": "MDU6SXNzdWUzNTkxNjUyOTA=", "number": 11532, "title": "[JIT][tracer] Slicing shape is specialized to tensor rank", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-11T18:24:52Z", "updated_at": "2018-09-11T18:24:53Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Example:</p>\n<pre><code>import torch\n\ndef fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:]), x[1:]), dim=0)\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\ntraced(torch.rand(3, 4, 5))\n</code></pre>\n<pre><code>graph(%0 : Float(3, 4)) {\n  %4 : int = prim::Constant[value=1]()\n  %5 : int = aten::size(%0, %4)\n  %6 : Long() = prim::NumToTensor(%5)\n  %7 : int = prim::TensorToNum(%6)\n  %8 : int = prim::Constant[value=1]()\n  %9 : int[] = prim::ListConstruct(%8, %7)\n  %10 : int = prim::Constant[value=6]()\n  %11 : int = prim::Constant[value=0]()\n  %12 : int[] = prim::Constant[value=[0, -1]]()\n  %13 : Float(1, 4) = aten::rand(%9, %10, %11, %12)\n  %14 : int = prim::Constant[value=0]()\n  %15 : int = prim::Constant[value=1]()\n  %16 : int = prim::Constant[value=9223372036854775807]()\n  %17 : int = prim::Constant[value=1]()\n  %18 : Float(2, 4) = aten::slice(%0, %14, %15, %16, %17)\n  %19 : Dynamic[] = prim::ListConstruct(%13, %18)\n  %20 : int = prim::Constant[value=0]()\n  %21 : Float(3, 4) = aten::cat(%19, %20)\n  return (%21);\n}\n</code></pre>\n<p>these <code>size()</code> calls we emit (e.g. <code>  %5 : int = aten::size(%0, %4)</code>) are specialized to the rank of the tensor we called <code>.shape</code> on</p>", "body_text": "Example:\nimport torch\n\ndef fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:]), x[1:]), dim=0)\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\ntraced(torch.rand(3, 4, 5))\n\ngraph(%0 : Float(3, 4)) {\n  %4 : int = prim::Constant[value=1]()\n  %5 : int = aten::size(%0, %4)\n  %6 : Long() = prim::NumToTensor(%5)\n  %7 : int = prim::TensorToNum(%6)\n  %8 : int = prim::Constant[value=1]()\n  %9 : int[] = prim::ListConstruct(%8, %7)\n  %10 : int = prim::Constant[value=6]()\n  %11 : int = prim::Constant[value=0]()\n  %12 : int[] = prim::Constant[value=[0, -1]]()\n  %13 : Float(1, 4) = aten::rand(%9, %10, %11, %12)\n  %14 : int = prim::Constant[value=0]()\n  %15 : int = prim::Constant[value=1]()\n  %16 : int = prim::Constant[value=9223372036854775807]()\n  %17 : int = prim::Constant[value=1]()\n  %18 : Float(2, 4) = aten::slice(%0, %14, %15, %16, %17)\n  %19 : Dynamic[] = prim::ListConstruct(%13, %18)\n  %20 : int = prim::Constant[value=0]()\n  %21 : Float(3, 4) = aten::cat(%19, %20)\n  return (%21);\n}\n\nthese size() calls we emit (e.g.   %5 : int = aten::size(%0, %4)) are specialized to the rank of the tensor we called .shape on", "body": "Example:\r\n\r\n```\r\nimport torch\r\n\r\ndef fill_row_zero(x):\r\n    x = torch.cat((torch.rand(1, *x.shape[1:]), x[1:]), dim=0)\r\n    return x\r\n\r\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\r\nprint(traced.graph)\r\ntraced(torch.rand(3, 4, 5))\r\n```\r\n\r\n\r\n```\r\ngraph(%0 : Float(3, 4)) {\r\n  %4 : int = prim::Constant[value=1]()\r\n  %5 : int = aten::size(%0, %4)\r\n  %6 : Long() = prim::NumToTensor(%5)\r\n  %7 : int = prim::TensorToNum(%6)\r\n  %8 : int = prim::Constant[value=1]()\r\n  %9 : int[] = prim::ListConstruct(%8, %7)\r\n  %10 : int = prim::Constant[value=6]()\r\n  %11 : int = prim::Constant[value=0]()\r\n  %12 : int[] = prim::Constant[value=[0, -1]]()\r\n  %13 : Float(1, 4) = aten::rand(%9, %10, %11, %12)\r\n  %14 : int = prim::Constant[value=0]()\r\n  %15 : int = prim::Constant[value=1]()\r\n  %16 : int = prim::Constant[value=9223372036854775807]()\r\n  %17 : int = prim::Constant[value=1]()\r\n  %18 : Float(2, 4) = aten::slice(%0, %14, %15, %16, %17)\r\n  %19 : Dynamic[] = prim::ListConstruct(%13, %18)\r\n  %20 : int = prim::Constant[value=0]()\r\n  %21 : Float(3, 4) = aten::cat(%19, %20)\r\n  return (%21);\r\n}\r\n```\r\n\r\nthese `size()` calls we emit (e.g. `  %5 : int = aten::size(%0, %4)`) are specialized to the rank of the tensor we called `.shape` on"}