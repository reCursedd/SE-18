{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11199", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11199/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11199/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11199/events", "html_url": "https://github.com/pytorch/pytorch/issues/11199", "id": 356407623, "node_id": "MDU6SXNzdWUzNTY0MDc2MjM=", "number": 11199, "title": "[feature request] PyTorch: Attention operation in NMT", "user": {"login": "xhzhao", "id": 17486215, "node_id": "MDQ6VXNlcjE3NDg2MjE1", "avatar_url": "https://avatars1.githubusercontent.com/u/17486215?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhzhao", "html_url": "https://github.com/xhzhao", "followers_url": "https://api.github.com/users/xhzhao/followers", "following_url": "https://api.github.com/users/xhzhao/following{/other_user}", "gists_url": "https://api.github.com/users/xhzhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhzhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhzhao/subscriptions", "organizations_url": "https://api.github.com/users/xhzhao/orgs", "repos_url": "https://api.github.com/users/xhzhao/repos", "events_url": "https://api.github.com/users/xhzhao/events{/privacy}", "received_events_url": "https://api.github.com/users/xhzhao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-03T08:36:13Z", "updated_at": "2018-09-04T16:32:18Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>hi<br>\nwe know that Attention mechanism is very popular in NMT domain, and i think it would be great to  support this operation in framework level. With this operation , we will get the following benefits:</p>\n<ul>\n<li>\n<p>reuse this op and simplify the code for the NMT applications</p>\n</li>\n<li>\n<p>more easy to do performance optimization for this op.</p>\n</li>\n</ul>\n<p>i think this operation should support 4 types of attention: dot, general, mlp, and multi-head attention.</p>\n<ul>\n<li>\n<p>dot, general, mlp is the traditional attention mechanism , see OpenNMT-py code <a href=\"https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py#L54\">here</a></p>\n</li>\n<li>\n<p>multi-head attention is for the Transformer, see this related issue <a href=\"https://github.com/pytorch/pytorch/issues/10459\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10459/hovercard\">here</a></p>\n</li>\n</ul>\n<p>i collected some reference implementation:<br>\n<a href=\"https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py\">https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py</a><br>\n<a href=\"https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/multi_headed_attn.py\">https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/multi_headed_attn.py</a><br>\n<a href=\"https://github.com/pytorch/fairseq/blob/master/fairseq/models/fconv.py#L287\">https://github.com/pytorch/fairseq/blob/master/fairseq/models/fconv.py#L287</a><br>\n<a href=\"https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L16\">https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L16</a><br>\n<a href=\"https://github.com/mlperf/reference/blob/master/rnn_translator/pytorch/seq2seq/models/attention.py\">https://github.com/mlperf/reference/blob/master/rnn_translator/pytorch/seq2seq/models/attention.py</a></p>\n<p>i understand that there maybe some variances for the attention operation, but i think we could extract the common part and make this new op support most of its variances.</p>", "body_text": "hi\nwe know that Attention mechanism is very popular in NMT domain, and i think it would be great to  support this operation in framework level. With this operation , we will get the following benefits:\n\n\nreuse this op and simplify the code for the NMT applications\n\n\nmore easy to do performance optimization for this op.\n\n\ni think this operation should support 4 types of attention: dot, general, mlp, and multi-head attention.\n\n\ndot, general, mlp is the traditional attention mechanism , see OpenNMT-py code here\n\n\nmulti-head attention is for the Transformer, see this related issue here\n\n\ni collected some reference implementation:\nhttps://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py\nhttps://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/multi_headed_attn.py\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/models/fconv.py#L287\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L16\nhttps://github.com/mlperf/reference/blob/master/rnn_translator/pytorch/seq2seq/models/attention.py\ni understand that there maybe some variances for the attention operation, but i think we could extract the common part and make this new op support most of its variances.", "body": "hi\r\nwe know that Attention mechanism is very popular in NMT domain, and i think it would be great to  support this operation in framework level. With this operation , we will get the following benefits:\r\n\r\n- reuse this op and simplify the code for the NMT applications\r\n\r\n- more easy to do performance optimization for this op.\r\n\r\ni think this operation should support 4 types of attention: dot, general, mlp, and multi-head attention.\r\n\r\n- dot, general, mlp is the traditional attention mechanism , see OpenNMT-py code [here](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py#L54)\r\n\r\n- multi-head attention is for the Transformer, see this related issue [here](https://github.com/pytorch/pytorch/issues/10459)\r\n\r\ni collected some reference implementation:\r\nhttps://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py \r\nhttps://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/multi_headed_attn.py\r\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/models/fconv.py#L287\r\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L16\r\nhttps://github.com/mlperf/reference/blob/master/rnn_translator/pytorch/seq2seq/models/attention.py\r\n\r\ni understand that there maybe some variances for the attention operation, but i think we could extract the common part and make this new op support most of its variances.\r\n"}