{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/366040034", "html_url": "https://github.com/pytorch/pytorch/issues/5263#issuecomment-366040034", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5263", "id": 366040034, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjA0MDAzNA==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-15T19:44:40Z", "updated_at": "2018-02-15T19:44:40Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Are we adding a notion of dtype to ATen? at::Type is an attractive candidate, but it distinguishes between Variable and Tensor where dtype does not.</p>\n</blockquote>\n<p>I don't see the reason to add dtype to ATen, at::Type covers everything.</p>\n<blockquote>\n<p>Are we accepting at::Type as an argument to functions? Any function which, at the Python side, supports foo(..., dtype=blah) should also support this ATen side as a Type argument. This means we should convert to and from dtype to at::Type in ATen.</p>\n</blockquote>\n<p>what do you mean by function here?  If you mean for native functions, I don't see why we wouldn't allow at::Type.  Note that we already have a \"hacky\" version of this with native functions that only depend on the type of the tensor parameter, e.g. is_floating_point.  That's for convenience in both PyTorch and ATen, so you can just write tensor.is_floating_point() instead of tensor.type().is_floating_point().</p>\n<blockquote>\n<p>How are we handling Variable versus Tensor? PyTorch frontend will resolve this problem by merging Variable and Tensor. In ATen, this can be resolved in two ways: if we use at::Type, there is no problem, as you are simply expected to pass an appropriate Variable type or Tensor type. If we introduce another notion of dtype, we need two copies of factory functions, one for Variables and one for Tensors.</p>\n</blockquote>\n<p>Use at::Type.</p>", "body_text": "Are we adding a notion of dtype to ATen? at::Type is an attractive candidate, but it distinguishes between Variable and Tensor where dtype does not.\n\nI don't see the reason to add dtype to ATen, at::Type covers everything.\n\nAre we accepting at::Type as an argument to functions? Any function which, at the Python side, supports foo(..., dtype=blah) should also support this ATen side as a Type argument. This means we should convert to and from dtype to at::Type in ATen.\n\nwhat do you mean by function here?  If you mean for native functions, I don't see why we wouldn't allow at::Type.  Note that we already have a \"hacky\" version of this with native functions that only depend on the type of the tensor parameter, e.g. is_floating_point.  That's for convenience in both PyTorch and ATen, so you can just write tensor.is_floating_point() instead of tensor.type().is_floating_point().\n\nHow are we handling Variable versus Tensor? PyTorch frontend will resolve this problem by merging Variable and Tensor. In ATen, this can be resolved in two ways: if we use at::Type, there is no problem, as you are simply expected to pass an appropriate Variable type or Tensor type. If we introduce another notion of dtype, we need two copies of factory functions, one for Variables and one for Tensors.\n\nUse at::Type.", "body": "> Are we adding a notion of dtype to ATen? at::Type is an attractive candidate, but it distinguishes between Variable and Tensor where dtype does not.\r\n\r\nI don't see the reason to add dtype to ATen, at::Type covers everything.\r\n\r\n> Are we accepting at::Type as an argument to functions? Any function which, at the Python side, supports foo(..., dtype=blah) should also support this ATen side as a Type argument. This means we should convert to and from dtype to at::Type in ATen.\r\n\r\nwhat do you mean by function here?  If you mean for native functions, I don't see why we wouldn't allow at::Type.  Note that we already have a \"hacky\" version of this with native functions that only depend on the type of the tensor parameter, e.g. is_floating_point.  That's for convenience in both PyTorch and ATen, so you can just write tensor.is_floating_point() instead of tensor.type().is_floating_point().\r\n\r\n> How are we handling Variable versus Tensor? PyTorch frontend will resolve this problem by merging Variable and Tensor. In ATen, this can be resolved in two ways: if we use at::Type, there is no problem, as you are simply expected to pass an appropriate Variable type or Tensor type. If we introduce another notion of dtype, we need two copies of factory functions, one for Variables and one for Tensors.\r\n\r\nUse at::Type."}