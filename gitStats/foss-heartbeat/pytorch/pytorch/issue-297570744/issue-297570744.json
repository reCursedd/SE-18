{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5263", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5263/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5263/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5263/events", "html_url": "https://github.com/pytorch/pytorch/issues/5263", "id": 297570744, "node_id": "MDU6SXNzdWUyOTc1NzA3NDQ=", "number": 5263, "title": "Discussion on ATen functions which take no parameters", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-02-15T19:37:37Z", "updated_at": "2018-04-24T20:18:06Z", "closed_at": "2018-04-24T20:18:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Consider <code>torch.zeros(*sizes)</code>, which creates a zero-filled tensor but doesn't specify what its type will be. This has been a great annoyance for our users, and as such our Python frontend is getting a new <code>dtype</code> parameter which will let you specify if you want a CPU double tensor or a CUDA float tensor, etc. Critically, however, there are still two versions of the function: a plain tensor one, and a Variable one. When PyTorch merges variables and tensors, there will only be one such one.</p>\n<p>I am opening this ticket to talk about what this will look like C++ ATen side.</p>\n<ul>\n<li>Are we adding a notion of <code>dtype</code> to ATen? <code>at::Type</code> is an attractive candidate, but it distinguishes between Variable and Tensor where dtype does not.</li>\n<li>Are we accepting <code>at::Type</code> as an argument to functions? Any function which, at the Python side, supports <code>foo(..., dtype=blah)</code> should also support this ATen side as a Type argument. This means we should convert to and from dtype to at::Type in ATen.</li>\n<li>How are we handling Variable versus Tensor? PyTorch frontend will resolve this problem by merging Variable and Tensor. In ATen, this can be resolved in two ways: if we use at::Type, there is no problem, as you are simply expected to pass an appropriate Variable type or Tensor type. If we introduce another notion of dtype, we need two copies of factory functions, one for Variables and one for Tensors.</li>\n</ul>\n<p>Other things I've missed? CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "Consider torch.zeros(*sizes), which creates a zero-filled tensor but doesn't specify what its type will be. This has been a great annoyance for our users, and as such our Python frontend is getting a new dtype parameter which will let you specify if you want a CPU double tensor or a CUDA float tensor, etc. Critically, however, there are still two versions of the function: a plain tensor one, and a Variable one. When PyTorch merges variables and tensors, there will only be one such one.\nI am opening this ticket to talk about what this will look like C++ ATen side.\n\nAre we adding a notion of dtype to ATen? at::Type is an attractive candidate, but it distinguishes between Variable and Tensor where dtype does not.\nAre we accepting at::Type as an argument to functions? Any function which, at the Python side, supports foo(..., dtype=blah) should also support this ATen side as a Type argument. This means we should convert to and from dtype to at::Type in ATen.\nHow are we handling Variable versus Tensor? PyTorch frontend will resolve this problem by merging Variable and Tensor. In ATen, this can be resolved in two ways: if we use at::Type, there is no problem, as you are simply expected to pass an appropriate Variable type or Tensor type. If we introduce another notion of dtype, we need two copies of factory functions, one for Variables and one for Tensors.\n\nOther things I've missed? CC @gchanan @colesbury", "body": "Consider `torch.zeros(*sizes)`, which creates a zero-filled tensor but doesn't specify what its type will be. This has been a great annoyance for our users, and as such our Python frontend is getting a new `dtype` parameter which will let you specify if you want a CPU double tensor or a CUDA float tensor, etc. Critically, however, there are still two versions of the function: a plain tensor one, and a Variable one. When PyTorch merges variables and tensors, there will only be one such one.\r\n\r\nI am opening this ticket to talk about what this will look like C++ ATen side.\r\n\r\n* Are we adding a notion of `dtype` to ATen? `at::Type` is an attractive candidate, but it distinguishes between Variable and Tensor where dtype does not.\r\n* Are we accepting `at::Type` as an argument to functions? Any function which, at the Python side, supports `foo(..., dtype=blah)` should also support this ATen side as a Type argument. This means we should convert to and from dtype to at::Type in ATen.\r\n* How are we handling Variable versus Tensor? PyTorch frontend will resolve this problem by merging Variable and Tensor. In ATen, this can be resolved in two ways: if we use at::Type, there is no problem, as you are simply expected to pass an appropriate Variable type or Tensor type. If we introduce another notion of dtype, we need two copies of factory functions, one for Variables and one for Tensors.\r\n\r\nOther things I've missed? CC @gchanan @colesbury "}