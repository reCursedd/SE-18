{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379805415", "html_url": "https://github.com/pytorch/pytorch/issues/6422#issuecomment-379805415", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6422", "id": 379805415, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTgwNTQxNQ==", "user": {"login": "shijieheping", "id": 25653707, "node_id": "MDQ6VXNlcjI1NjUzNzA3", "avatar_url": "https://avatars0.githubusercontent.com/u/25653707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shijieheping", "html_url": "https://github.com/shijieheping", "followers_url": "https://api.github.com/users/shijieheping/followers", "following_url": "https://api.github.com/users/shijieheping/following{/other_user}", "gists_url": "https://api.github.com/users/shijieheping/gists{/gist_id}", "starred_url": "https://api.github.com/users/shijieheping/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shijieheping/subscriptions", "organizations_url": "https://api.github.com/users/shijieheping/orgs", "repos_url": "https://api.github.com/users/shijieheping/repos", "events_url": "https://api.github.com/users/shijieheping/events{/privacy}", "received_events_url": "https://api.github.com/users/shijieheping/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T16:07:00Z", "updated_at": "2018-04-09T16:07:00Z", "author_association": "NONE", "body_html": "<p>I've reproduced the same issue with K80(dual core Kepler), file_store_path, with/without NCCL</p>\n<p>OS: Ubuntu 16.04, Linux 4.4.0-119-generic<br>\nPython version: 2.7.12<br>\nCUDA/cuDNN version: libcudnn7-dev_7.1.2.21-1+cuda9.1_amd64.deb<br>\nGPU models and configuration: K80<br>\nGCC version (if compiling from source): 5.4.0<br>\nMellanox OFED version: 4.3-1.0.1.0<br>\nNCCL: 2.1.15-1+cuda9.1</p>\n<p>root@<strong>u1</strong>:/dev/shm/lmdbexample# PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id <strong>0</strong> --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>root@<strong>u2</strong>:/dev/shm/lmdbexample# PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id <strong>1</strong> --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>On Node u1</p>\n<pre><code>root@u1:/dev/shm/lmdbexample#  PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 1 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0410 00:05:04.814718  8026 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:04.814929  8026 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:04.814939  8026 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0410 00:05:05.428273  8026 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3 \nE0410 00:05:05.428676  8026 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0410 00:05:05.428700  8026 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0426690578461 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0411059856415 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Parameter update function not defined --&gt; only forward\nINFO:resnet50_trainer:Starting epoch 0/10\nE0410 00:05:14.257479  8096 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\n</code></pre>\n<p>On Node u2</p>\n<pre><code>root@u2:/dev/shm/lmdbexample#  PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 0 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0410 00:05:06.052727 24038 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:06.053006 24038 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:06.053020 24038 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0410 00:05:06.892519 24038 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3 \nE0410 00:05:06.893355 24038 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0410 00:05:06.893406 24038 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0609338283539 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0527391433716 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Parameter update function not defined --&gt; only forward\nINFO:resnet50_trainer:Starting epoch 0/10\nE0410 00:05:14.254077 24091 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nWARNING:caffe2.python.workspace:Original python traceback for operator `820` in network `resnet50` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 615, in &lt;module&gt;\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\nTraceback (most recent call last):\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 615, in &lt;module&gt;\n    main()\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\n    Train(args)\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\n    explog\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\n    workspace.RunNet(train_model.net.Proto().name)\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 217, in RunNet\n    StringifyNetName(name), num_iter, allow_fail,\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 178, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\n\n</code></pre>", "body_text": "I've reproduced the same issue with K80(dual core Kepler), file_store_path, with/without NCCL\nOS: Ubuntu 16.04, Linux 4.4.0-119-generic\nPython version: 2.7.12\nCUDA/cuDNN version: libcudnn7-dev_7.1.2.21-1+cuda9.1_amd64.deb\nGPU models and configuration: K80\nGCC version (if compiling from source): 5.4.0\nMellanox OFED version: 4.3-1.0.1.0\nNCCL: 2.1.15-1+cuda9.1\nroot@u1:/dev/shm/lmdbexample# PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 0 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\nroot@u2:/dev/shm/lmdbexample# PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 1 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\nOn Node u1\nroot@u1:/dev/shm/lmdbexample#  PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 1 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0410 00:05:04.814718  8026 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:04.814929  8026 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:04.814939  8026 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0410 00:05:05.428273  8026 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \nE0410 00:05:05.428676  8026 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0410 00:05:05.428700  8026 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0426690578461 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0411059856415 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Parameter update function not defined --> only forward\nINFO:resnet50_trainer:Starting epoch 0/10\nE0410 00:05:14.257479  8096 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\n\nOn Node u2\nroot@u2:/dev/shm/lmdbexample#  PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 0 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0410 00:05:06.052727 24038 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:06.053006 24038 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0410 00:05:06.053020 24038 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0410 00:05:06.892519 24038 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \nE0410 00:05:06.893355 24038 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0410 00:05:06.893406 24038 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0609338283539 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0527391433716 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Parameter update function not defined --> only forward\nINFO:resnet50_trainer:Starting epoch 0/10\nE0410 00:05:14.254077 24091 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nWARNING:caffe2.python.workspace:Original python traceback for operator `820` in network `resnet50` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\nTraceback (most recent call last):\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\n    main()\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\n    Train(args)\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\n    explog\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\n    workspace.RunNet(train_model.net.Proto().name)\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 217, in RunNet\n    StringifyNetName(name), num_iter, allow_fail,\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 178, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"", "body": "I've reproduced the same issue with K80(dual core Kepler), file_store_path, with/without NCCL\r\n\r\nOS: Ubuntu 16.04, Linux 4.4.0-119-generic\r\nPython version: 2.7.12\r\nCUDA/cuDNN version: libcudnn7-dev_7.1.2.21-1+cuda9.1_amd64.deb\r\nGPU models and configuration: K80\r\nGCC version (if compiling from source): 5.4.0 \r\nMellanox OFED version: 4.3-1.0.1.0\r\nNCCL: 2.1.15-1+cuda9.1\r\n\r\n\r\n\r\n\r\nroot@**u1**:/dev/shm/lmdbexample# PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id **0** --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\nroot@**u2**:/dev/shm/lmdbexample# PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id **1** --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\n\r\n\r\nOn Node u1\r\n```\r\nroot@u1:/dev/shm/lmdbexample#  PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 1 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nE0410 00:05:04.814718  8026 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0410 00:05:04.814929  8026 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0410 00:05:04.814939  8026 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\r\nINFO:resnet50_trainer:Using epoch size: 9984\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nE0410 00:05:05.428273  8026 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \r\nE0410 00:05:05.428676  8026 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0410 00:05:05.428700  8026 operator.cc:465] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0426690578461 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0411059856415 secs\r\nINFO:resnet50_trainer:----- Create test net ----\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Parameter update function not defined --> only forward\r\nINFO:resnet50_trainer:Starting epoch 0/10\r\nE0410 00:05:14.257479  8096 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\n```\r\n\r\nOn Node u2\r\n```\r\nroot@u2:/dev/shm/lmdbexample#  PYTHONPATH=/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages LD_LIBRARY_PATH=/dev/shm/ibnoncclcaffe2/lib/ python /dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py --train_data /mnt/data/caffedata/imagenet_cars_boats_train --test_data /mnt/data/caffedata/imagenet_cars_boats_val --batch_size 64 --num_epochs 10 --epoch_size 10000 --num_gpus 2 --run_id 1 --num_shards 2 --shard_id 0 --file_store_path /mnt/fsp --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nE0410 00:05:06.052727 24038 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0410 00:05:06.053006 24038 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0410 00:05:06.053020 24038 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\r\nINFO:resnet50_trainer:Using epoch size: 9984\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nE0410 00:05:06.892519 24038 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \r\nE0410 00:05:06.893355 24038 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0410 00:05:06.893406 24038 operator.cc:465] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0609338283539 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0527391433716 secs\r\nINFO:resnet50_trainer:----- Create test net ----\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Parameter update function not defined --> only forward\r\nINFO:resnet50_trainer:Starting epoch 0/10\r\nE0410 00:05:14.254077 24091 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nWARNING:caffe2.python.workspace:Original python traceback for operator `820` in network `resnet50` in exception above (most recent call last):\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\r\nWARNING:caffe2.python.workspace:  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\r\nTraceback (most recent call last):\r\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\n    main()\r\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\n    Train(args)\r\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\r\n    explog\r\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\r\n    workspace.RunNet(train_model.net.Proto().name)\r\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 217, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File \"/dev/shm/ibnoncclcaffe2/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 178, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\n\r\n```\r\n\r\n"}