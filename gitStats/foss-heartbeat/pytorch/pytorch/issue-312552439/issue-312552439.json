{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6422", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6422/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6422/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6422/events", "html_url": "https://github.com/pytorch/pytorch/issues/6422", "id": 312552439, "node_id": "MDU6SXNzdWUzMTI1NTI0Mzk=", "number": 6422, "title": "[caffe2] Run resnet50_trainer.py error between 2 machines using GLOO/Redis and ibverbs", "user": {"login": "boriskovalev", "id": 31654570, "node_id": "MDQ6VXNlcjMxNjU0NTcw", "avatar_url": "https://avatars1.githubusercontent.com/u/31654570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boriskovalev", "html_url": "https://github.com/boriskovalev", "followers_url": "https://api.github.com/users/boriskovalev/followers", "following_url": "https://api.github.com/users/boriskovalev/following{/other_user}", "gists_url": "https://api.github.com/users/boriskovalev/gists{/gist_id}", "starred_url": "https://api.github.com/users/boriskovalev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boriskovalev/subscriptions", "organizations_url": "https://api.github.com/users/boriskovalev/orgs", "repos_url": "https://api.github.com/users/boriskovalev/repos", "events_url": "https://api.github.com/users/boriskovalev/events{/privacy}", "received_events_url": "https://api.github.com/users/boriskovalev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-09T14:24:27Z", "updated_at": "2018-07-26T13:04:50Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Caffe2<br>\nOS: Ubuntu 16.04<br>\nPython version: 2.7<br>\nCUDA/cuDNN version: 9.1 , 7.0.5<br>\nGPU models and configuration: P100<br>\nGCC version (if compiling from source):5.4.0<br>\nMellanox OFEDversion: 4.2.<br>\nNCCL 2.1.15</p>\n<p>I try to run resnet50_trainer.py on two nodes with several GPUs and ibverbs by:</p>\n<p>python /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --run_id 1 --epoch_size 10000 --num_epochs 10 --image_size 256 --num_gpus 2 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>python /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --run_id 1 --epoch_size 10000 --num_epochs 10 --image_size 256 --num_gpus 2 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p><strong>On first node:</strong><br>\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.<br>\nE0409 17:14:15.448786  5047 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nE0409 17:14:15.449039  5047 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nE0409 17:14:15.449051  5047 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nINFO:resnet50_trainer:Running on GPUs: [0, 1]<br>\nINFO:resnet50_trainer:Using epoch size: 9984<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nE0409 17:14:16.092067  5047 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3<br>\nE0409 17:14:16.092497  5047 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"<br>\nE0409 17:14:16.092525  5047 operator.cc:465] Returning empty results.<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0500400066376 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0491058826447 secs<br>\nINFO:resnet50_trainer:----- Create test net ----<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Parameter update function not defined --&gt; only forward<br>\nINFO:resnet50_trainer:Starting epoch 0/10<br>\nE0409 17:14:27.317248  5177 net_dag.cc:188] Exception from operator chain starting at 'comp_15_conv_3_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_4_cw\" input: \"gpu_0/comp_15_conv_3_w_grad\" input: \"gpu_1/comp_15_conv_3_w_grad\" output: \"gpu_0/comp_15_conv_3_w_grad\" output: \"gpu_1/comp_15_conv_3_w_grad\" name: \"comp_15_conv_3_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_3_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nE0409 17:14:27.324991  5184 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_2_s_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_6_cw\" input: \"gpu_0/comp_15_spatbn_2_s_grad\" input: \"gpu_1/comp_15_spatbn_2_s_grad\" output: \"gpu_0/comp_15_spatbn_2_s_grad\" output: \"gpu_1/comp_15_spatbn_2_s_grad\" name: \"comp_15_spatbn_2_s_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_s_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nE0409 17:14:27.363723  5178 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_2_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_7_cw\" input: \"gpu_0/comp_15_conv_2_w_grad\" input: \"gpu_1/comp_15_conv_2_w_grad\" output: \"gpu_0/comp_15_conv_2_w_grad\" output: \"gpu_1/comp_15_conv_2_w_grad\" name: \"comp_15_conv_2_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_2_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"</p>\n<p><strong>On second:</strong></p>\n<p>Ignoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.<br>\nE0409 17:14:16.832541  2188 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nE0409 17:14:16.832834  2188 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nE0409 17:14:16.832846  2188 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nINFO:resnet50_trainer:Running on GPUs: [0, 1]<br>\nINFO:resnet50_trainer:Using epoch size: 9984<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nE0409 17:14:17.516000  2188 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3<br>\nE0409 17:14:17.516786  2188 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"<br>\nE0409 17:14:17.516835  2188 operator.cc:465] Returning empty results.<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.057541847229 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0494668483734 secs<br>\nINFO:resnet50_trainer:----- Create test net ----<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Parameter update function not defined --&gt; only forward<br>\nINFO:resnet50_trainer:Starting epoch 0/10<br>\nE0409 17:14:27.317000  2299 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_2_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_5_cw\" input: \"gpu_0/comp_15_spatbn_2_b_grad\" input: \"gpu_1/comp_15_spatbn_2_b_grad\" output: \"gpu_0/comp_15_spatbn_2_b_grad\" output: \"gpu_1/comp_15_spatbn_2_b_grad\" name: \"comp_15_spatbn_2_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nE0409 17:14:27.325222  2295 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_3_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_4_cw\" input: \"gpu_0/comp_15_conv_3_w_grad\" input: \"gpu_1/comp_15_conv_3_w_grad\" output: \"gpu_0/comp_15_conv_3_w_grad\" output: \"gpu_1/comp_15_conv_3_w_grad\" name: \"comp_15_conv_3_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_3_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nE0409 17:14:27.365981  2285 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_2_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_7_cw\" input: \"gpu_0/comp_15_conv_2_w_grad\" input: \"gpu_1/comp_15_conv_2_w_grad\" output: \"gpu_0/comp_15_conv_2_w_grad\" output: \"gpu_1/comp_15_conv_2_w_grad\" name: \"comp_15_conv_2_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_2_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nOriginal python traceback for operator 829 in network <code>resnet50</code> in exception above (most recent call last):<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce<br>\nTraceback (most recent call last):<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <br>\nmain()<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main<br>\nTrain(args)<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train<br>\nexplog<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch<br>\nworkspace.RunNet(train_model.net.Proto().name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet<br>\nStringifyNetName(name), num_iter, allow_fail,<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept<br>\nreturn func(*args, **kwargs)<br>\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_5_cw\" input: \"gpu_0/comp_15_spatbn_2_b_grad\" input: \"gpu_1/comp_15_spatbn_2_b_grad\" output: \"gpu_0/comp_15_spatbn_2_b_grad\" output: \"gpu_1/comp_15_spatbn_2_b_grad\" name: \"comp_15_spatbn_2_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"</p>\n<p>Distributed TCP and locale multi GPUs is running properly.</p>\n<p>Thanks.</p>", "body_text": "Caffe2\nOS: Ubuntu 16.04\nPython version: 2.7\nCUDA/cuDNN version: 9.1 , 7.0.5\nGPU models and configuration: P100\nGCC version (if compiling from source):5.4.0\nMellanox OFEDversion: 4.2.\nNCCL 2.1.15\nI try to run resnet50_trainer.py on two nodes with several GPUs and ibverbs by:\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --run_id 1 --epoch_size 10000 --num_epochs 10 --image_size 256 --num_gpus 2 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --run_id 1 --epoch_size 10000 --num_epochs 10 --image_size 256 --num_gpus 2 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\nOn first node:\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0409 17:14:15.448786  5047 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0409 17:14:15.449039  5047 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0409 17:14:15.449051  5047 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0409 17:14:16.092067  5047 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0409 17:14:16.092497  5047 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0409 17:14:16.092525  5047 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0500400066376 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0491058826447 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Parameter update function not defined --> only forward\nINFO:resnet50_trainer:Starting epoch 0/10\nE0409 17:14:27.317248  5177 net_dag.cc:188] Exception from operator chain starting at 'comp_15_conv_3_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_4_cw\" input: \"gpu_0/comp_15_conv_3_w_grad\" input: \"gpu_1/comp_15_conv_3_w_grad\" output: \"gpu_0/comp_15_conv_3_w_grad\" output: \"gpu_1/comp_15_conv_3_w_grad\" name: \"comp_15_conv_3_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_3_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nE0409 17:14:27.324991  5184 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_2_s_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_6_cw\" input: \"gpu_0/comp_15_spatbn_2_s_grad\" input: \"gpu_1/comp_15_spatbn_2_s_grad\" output: \"gpu_0/comp_15_spatbn_2_s_grad\" output: \"gpu_1/comp_15_spatbn_2_s_grad\" name: \"comp_15_spatbn_2_s_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_s_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nE0409 17:14:27.363723  5178 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_2_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_7_cw\" input: \"gpu_0/comp_15_conv_2_w_grad\" input: \"gpu_1/comp_15_conv_2_w_grad\" output: \"gpu_0/comp_15_conv_2_w_grad\" output: \"gpu_1/comp_15_conv_2_w_grad\" name: \"comp_15_conv_2_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_2_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nOn second:\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0409 17:14:16.832541  2188 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0409 17:14:16.832834  2188 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0409 17:14:16.832846  2188 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0409 17:14:17.516000  2188 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0409 17:14:17.516786  2188 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0409 17:14:17.516835  2188 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.057541847229 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0494668483734 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Parameter update function not defined --> only forward\nINFO:resnet50_trainer:Starting epoch 0/10\nE0409 17:14:27.317000  2299 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_2_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_5_cw\" input: \"gpu_0/comp_15_spatbn_2_b_grad\" input: \"gpu_1/comp_15_spatbn_2_b_grad\" output: \"gpu_0/comp_15_spatbn_2_b_grad\" output: \"gpu_1/comp_15_spatbn_2_b_grad\" name: \"comp_15_spatbn_2_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nE0409 17:14:27.325222  2295 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_3_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_4_cw\" input: \"gpu_0/comp_15_conv_3_w_grad\" input: \"gpu_1/comp_15_conv_3_w_grad\" output: \"gpu_0/comp_15_conv_3_w_grad\" output: \"gpu_1/comp_15_conv_3_w_grad\" name: \"comp_15_conv_3_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_3_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nE0409 17:14:27.365981  2285 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_2_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_7_cw\" input: \"gpu_0/comp_15_conv_2_w_grad\" input: \"gpu_1/comp_15_conv_2_w_grad\" output: \"gpu_0/comp_15_conv_2_w_grad\" output: \"gpu_1/comp_15_conv_2_w_grad\" name: \"comp_15_conv_2_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_2_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nOriginal python traceback for operator 829 in network resnet50 in exception above (most recent call last):\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in \nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\nTraceback (most recent call last):\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in \nmain()\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nTrain(args)\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\nexplog\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\nworkspace.RunNet(train_model.net.Proto().name)\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet\nStringifyNetName(name), num_iter, allow_fail,\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept\nreturn func(*args, **kwargs)\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_5_cw\" input: \"gpu_0/comp_15_spatbn_2_b_grad\" input: \"gpu_1/comp_15_spatbn_2_b_grad\" output: \"gpu_0/comp_15_spatbn_2_b_grad\" output: \"gpu_1/comp_15_spatbn_2_b_grad\" name: \"comp_15_spatbn_2_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nDistributed TCP and locale multi GPUs is running properly.\nThanks.", "body": "Caffe2\r\nOS: Ubuntu 16.04\r\nPython version: 2.7\r\nCUDA/cuDNN version: 9.1 , 7.0.5\r\nGPU models and configuration: P100\r\nGCC version (if compiling from source):5.4.0\r\nMellanox OFEDversion: 4.2.\r\nNCCL 2.1.15\r\n\r\nI try to run resnet50_trainer.py on two nodes with several GPUs and ibverbs by:\r\n\r\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --run_id 1 --epoch_size 10000 --num_epochs 10 --image_size 256 --num_gpus 2 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --run_id 1 --epoch_size 10000 --num_epochs 10 --image_size 256 --num_gpus 2 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\n**On first node:**\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nE0409 17:14:15.448786  5047 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0409 17:14:15.449039  5047 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0409 17:14:15.449051  5047 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\r\nINFO:resnet50_trainer:Using epoch size: 9984\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nE0409 17:14:16.092067  5047 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \r\nE0409 17:14:16.092497  5047 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0409 17:14:16.092525  5047 operator.cc:465] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0500400066376 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0491058826447 secs\r\nINFO:resnet50_trainer:----- Create test net ----\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Parameter update function not defined --> only forward\r\nINFO:resnet50_trainer:Starting epoch 0/10\r\nE0409 17:14:27.317248  5177 net_dag.cc:188] Exception from operator chain starting at 'comp_15_conv_3_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_4_cw\" input: \"gpu_0/comp_15_conv_3_w_grad\" input: \"gpu_1/comp_15_conv_3_w_grad\" output: \"gpu_0/comp_15_conv_3_w_grad\" output: \"gpu_1/comp_15_conv_3_w_grad\" name: \"comp_15_conv_3_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_3_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nE0409 17:14:27.324991  5184 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_2_s_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_6_cw\" input: \"gpu_0/comp_15_spatbn_2_s_grad\" input: \"gpu_1/comp_15_spatbn_2_s_grad\" output: \"gpu_0/comp_15_spatbn_2_s_grad\" output: \"gpu_1/comp_15_spatbn_2_s_grad\" name: \"comp_15_spatbn_2_s_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_s_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nE0409 17:14:27.363723  5178 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_2_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_7_cw\" input: \"gpu_0/comp_15_conv_2_w_grad\" input: \"gpu_1/comp_15_conv_2_w_grad\" output: \"gpu_0/comp_15_conv_2_w_grad\" output: \"gpu_1/comp_15_conv_2_w_grad\" name: \"comp_15_conv_2_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_2_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\n\r\n**On second:**\r\n\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nE0409 17:14:16.832541  2188 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0409 17:14:16.832834  2188 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0409 17:14:16.832846  2188 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0, 1]\r\nINFO:resnet50_trainer:Using epoch size: 9984\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nE0409 17:14:17.516000  2188 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \r\nE0409 17:14:17.516786  2188 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0409 17:14:17.516835  2188 operator.cc:465] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.057541847229 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0494668483734 secs\r\nINFO:resnet50_trainer:----- Create test net ----\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Parameter update function not defined --> only forward\r\nINFO:resnet50_trainer:Starting epoch 0/10\r\nE0409 17:14:27.317000  2299 net_dag.cc:188] Exception from operator chain starting at 'comp_15_spatbn_2_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_5_cw\" input: \"gpu_0/comp_15_spatbn_2_b_grad\" input: \"gpu_1/comp_15_spatbn_2_b_grad\" output: \"gpu_0/comp_15_spatbn_2_b_grad\" output: \"gpu_1/comp_15_spatbn_2_b_grad\" name: \"comp_15_spatbn_2_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nE0409 17:14:27.325222  2295 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_3_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_4_cw\" input: \"gpu_0/comp_15_conv_3_w_grad\" input: \"gpu_1/comp_15_conv_3_w_grad\" output: \"gpu_0/comp_15_conv_3_w_grad\" output: \"gpu_1/comp_15_conv_3_w_grad\" name: \"comp_15_conv_3_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_3_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nE0409 17:14:27.365981  2285 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_conv_2_w_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_7_cw\" input: \"gpu_0/comp_15_conv_2_w_grad\" input: \"gpu_1/comp_15_conv_2_w_grad\" output: \"gpu_0/comp_15_conv_2_w_grad\" output: \"gpu_1/comp_15_conv_2_w_grad\" name: \"comp_15_conv_2_w_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_conv_2_w_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nOriginal python traceback for operator 829 in network `resnet50` in exception above (most recent call last):\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\r\nTraceback (most recent call last):\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\n    main()\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\n    Train(args)\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\r\n    explog\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\r\n    workspace.RunNet(train_model.net.Proto().name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_5_cw\" input: \"gpu_0/comp_15_spatbn_2_b_grad\" input: \"gpu_1/comp_15_spatbn_2_b_grad\" output: \"gpu_0/comp_15_spatbn_2_b_grad\" output: \"gpu_1/comp_15_spatbn_2_b_grad\" name: \"comp_15_spatbn_2_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_2_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\n\r\n\r\nDistributed TCP and locale multi GPUs is running properly.\r\n\r\nThanks.\r\n\r\n"}