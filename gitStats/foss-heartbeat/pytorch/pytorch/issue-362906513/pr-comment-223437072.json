{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223437072", "pull_request_review_id": 162563771, "id": 223437072, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzQzNzA3Mg==", "diff_hunk": "@@ -246,6 +281,200 @@ def handler(signum, frame):\n class _DataLoaderIter(object):\n     r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\n \n+    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n+    #\n+    # Preliminary:\n+    #\n+    # Our data model looks like this (queues are indicated with curly brackets):\n+    #\n+    #                main process                              ||\n+    #                     |                                    ||\n+    #               {index_queue}                              ||\n+    #                     |                                    ||\n+    #              worker processes                            ||     DATA\n+    #                     |                                    ||\n+    #            {worker_result_queue}                         ||     FLOW\n+    #                     |                                    ||\n+    #      pin_memory_thread of main process                   ||   DIRECTION\n+    #                     |                                    ||\n+    #               {data_queue}                               ||\n+    #                     |                                    ||\n+    #                data output                               \\/\n+    #\n+    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n+    #      `pin_memory=False`.\n+    #\n+    #\n+    # Terminating multiprocessing logic requires very careful design. In\n+    # particular, we need to make sure that\n+    #\n+    #   1. The iterator gracefully exits the workers when its last reference is\n+    #      gone.\n+    #\n+    #      In this case, the workers should be gracefully exited because the\n+    #      main process may still need to continue to run, and we want cleaning\n+    #      up code in the workers to be executed (e.g., releasing GPU memory).\n+    #      Naturally, we implement the shutdown logic in `__del__` of\n+    #      DataLoaderIterator.\n+    #\n+    #      We delay the discussion on the logic in this case until later.\n+    #\n+    #   2. The iterator exits the workers when the program ends without error.\n+    #\n+    #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n+    #\n+    #      When a process ends, it shuts the all its daemonic children down with\n+    #      a SIGTERM (instead of joining them without a timeout). Simiarly for\n+    #      threads, but by a different mechanism.\n+    #\n+    #      You may ask, why can't we make the workers non-daemonic, and\n+    #      gracefully exit using the same logic as we have in `__del__` when the\n+    #      iterator gets deleted (see 1 above)?\n+    #\n+    #      When a process exits, Python joins all its non-daemonic subprocesses,\n+    #      and terminates (via SIGTERM) all daemonic ones. This fact, together\n+    #      with a few implementation details of multiprocessing, forces us to\n+    #      make workers  daemonic. All of our problems arise when a DataLoader\n+    #      is used in a subprocess, and are caused by multiprocessing code\n+    #      which looks more or less like this:\n+    #\n+    #          try:\n+    #              your_function_using_a_dataloader()\n+    #          finally:\n+    #              multiprocessing.util._exit_function()\n+    #\n+    #      The joining/termination mentioned above happens inside\n+    #      `_exit_function()`. Now, if `your_function_using_a_dataloader()`\n+    #      throws, the stack trace stored in the exception will prevent the\n+    #      frame which uses `DataLoaderIter` to be freed. If the frame has any\n+    #      reference to the `DataLoaderIter` (e.g., in a method of the iter),\n+    #      its  `__del__`, which starts the shutdown procedure, will not be\n+    #      called. That, in turn, means that workers aren't notified. Attempting\n+    #      to join in `_exit_function` will then result in a hang.\n+    #\n+    #      For context, `_exit_function` is also registered as an `atexit` call.\n+    #      So it is unclear to me (@ssnl) why this is needed in a finally block.\n+    #      The code dates back to 2008 and there is no comment on the original\n+    #      PEP 371 or patch https://bugs.python.org/issue3050 (containing both\n+    #      the finally block and the `atexit` registration) that explains this.\n+    #\n+    #      Another choice is to just shutdown workers with logic in 1 above\n+    #      whenever we see an error in `next`. This isn't ideal because\n+    #        a. It prevents users from using try-catch to resume data loading.\n+    #        b. It doesn't prevent hanging if users have references to the\n+    #           iterator.\n+    #\n+    #   3. All processes exit if any of them die unexpectedly (e.g., error,\n+    #      fatal signals).\n+    #\n+    #      As shown above, the workers are set as daemonic children of the main\n+    #      process. However, automatic cleaning-up of such child processes only\n+    #      happens if the parent process exits gracefully (e.g., not via fatal\n+    #      signals like SIGKILL). So we must ensure that each process will exit\n+    #      even the process that should send/receive data to/from it were\n+    #      killed, i.e.,\n+    #\n+    #        a. A process won't hang when getting from a queue.\n+    #\n+    #           Even with carefully designed data dependencies (i.e., a `put()`\n+    #           always corresponding to a `get()`), hanging on `get()` can still\n+    #           happen when data in queue is corrupted (e.g., due to\n+    #           `cancel_join_thread` or unexpected exit).\n+    #\n+    #           For child exit, we register SIGCHLD handler on main process,\n+    #           which checks if any of the workers fail in the (Python) handler.\n+    #           See DataLoader.cpp.\n+    #\n+    #           For `.get()` calls where the sender(s) is not the workers, we\n+    #           guard them with timeouts, and check the status of the sender\n+    #           when timeout happens:\n+    #             + in the workers, the `ManagerWatchdog` class checks the main\n+    #               process status.\n+    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\n+    #               check `pin_memory_thread` status periodically until `.get()`\n+    #               returns or see that `pin_memory_thread` died.\n+    #\n+    #        b. A process won't hang when putting into a queue;\n+    #\n+    #           We use `mp.Queue` which has a separate background thread to put\n+    #           objects. The background thread is usually automatically joined\n+    #           when the process exits.\n+    #\n+    #           However, in case that the receiver has ended abruptly while\n+    #           reading from the pipe, the join will hang forever. Therefore,\n+    #           for both `worker_result_queue` (worker -> main process/pin_memory_thread)\n+    #           and each `index_queue` (main process -> worker), we use\n+    #           `q.cancel_join_thread()` in sender process before any `q.put` to\n+    #           prevent this automatic join.\n+    #\n+    #           Moreover, having all queues called `cancel_join_thread` makes\n+    #           implementing graceful shutdown logic in `__del__` much easier.\n+    #           It won't need to get from any queue, which would also need to be\n+    #           guarded by periodic status checks.\n+    #\n+    #           Note that this may leave corrupted data in the queue, but we\n+    #           don't care about the data anyways once we are shutting down.\n+    #\n+    #\n+    # Now let's get back to 1:\n+    #   how we gracefully exit the workers when the last reference to the\n+    #   iteartor is gone.\n+    #\n+    # To achieve this, we implement the following logic along with the design\n+    # choices mentioned above:\n+    #\n+    # [worker processes]\n+    #   While loader process is alive:\n+    #     Get from index_queue.\n+    #       If got a `None`, exit.\n+    #       If get anything else,\n+    #          Check `done_event`.\n+    #            If set, continue to next iteration\n+    #                    i.e., keep getting until see the `None`, then exit.\n+    #            Otherwise, process data.\n+    #       If timed out,\n+    #          No matter `done_event` is set (still need to see `None`) or not,\n+    #          must continue to next iteration .\n+    #\n+    # [pin_memory_thread]\n+    #   # No need to check main thread. If this thread is alive, the main loader\n+    #   # thread must be alive, because this thread is set as daemonic.\n+    #   While True:\n+    #     Get from index_queue.\n+    #       If got a `None`, exit.\n+    #       If get anything else,\n+    #          Check `done_event`.\n+    #            If set, continue to next iteration\n+    #                    i.e., keep getting until see the `None`, then exit.\n+    #            Otherwise, process data.\n+    #\n+    # [main process]\n+    #   In the DataLoader Iter's `__del__`\n+    #     a. Set `done_event` (shared with `pin_memory_thread` and workers).\n+    #\n+    #        Note: from here on, the workers & `pin_memory_thread` may exit at\n+    #              any time after they receive `None`.\n+    #\n+    #     b. Exit `pin_memory_thread`\n+    #          i.   Put `None` in `worker_result_queue`.\n+    #          ii.  Join the `pin_memory_thread`.\n+    #\n+    #     c. Exit the workers.\n+    #          i.   Put `None` in each worker's `index_queue`.\n+    #          ii.  Join the workers.", "path": "torch/utils/data/dataloader.py", "position": 370, "original_position": 363, "commit_id": "d3c4ffa092580d5b15ef2705bd3904f7b685e511", "original_commit_id": "398578667ab67c1b09c73456159eead852177cc1", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "I agree that it may go out of sync. But I still prefer having it here because it allows understanding how everything works without finding the code and scrolling back and forth, which is one of the reasons we have notes like this :) ", "created_at": "2018-10-08T17:10:35Z", "updated_at": "2018-11-23T15:52:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/11985#discussion_r223437072", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11985", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223437072"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11985#discussion_r223437072"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11985"}}, "body_html": "<p>I agree that it may go out of sync. But I still prefer having it here because it allows understanding how everything works without finding the code and scrolling back and forth, which is one of the reasons we have notes like this :)</p>", "body_text": "I agree that it may go out of sync. But I still prefer having it here because it allows understanding how everything works without finding the code and scrolling back and forth, which is one of the reasons we have notes like this :)", "in_reply_to_id": 223428404}