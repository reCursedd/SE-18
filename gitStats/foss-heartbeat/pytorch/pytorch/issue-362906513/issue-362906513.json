{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11985", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11985/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11985/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11985/events", "html_url": "https://github.com/pytorch/pytorch/pull/11985", "id": 362906513, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE3NDg4NDAx", "number": 11985, "title": "Prevent hanging in data loader altogether", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1002715609, "node_id": "MDU6TGFiZWwxMDAyNzE1NjA5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/blocker", "name": "blocker", "color": "b60205", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-09-23T02:11:10Z", "updated_at": "2018-11-23T15:52:36Z", "closed_at": "2018-10-09T16:55:47Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11985", "html_url": "https://github.com/pytorch/pytorch/pull/11985", "diff_url": "https://github.com/pytorch/pytorch/pull/11985.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11985.patch"}, "body_html": "<p>Test plan:</p>\n<p>Trained 16000 LeNets (total) on MNIST on 160 processes. Previously some of them will either hang during training or during program exiting. Now every process finishes successfully.</p>\n<pre><code>    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n    #\n    # Preliminary:\n    #\n    # Our data model looks like this (queues are indicated with curly brackets):\n    #\n    #                main process                              ||\n    #                     |                                    ||\n    #               {index_queue}                              ||\n    #                     |                                    ||\n    #              worker processes                            ||     DATA\n    #                     |                                    ||\n    #            {worker_result_queue}                         ||     FLOW\n    #                     |                                    ||\n    #      pin_memory_thread of main process                   ||   DIRECTION\n    #                     |                                    ||\n    #               {data_queue}                               ||\n    #                     |                                    ||\n    #                data output                               \\/\n    #\n    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n    #      `pin_memory=False`.\n    #\n    #\n    # Terminating multiprocessing logic requires very careful design. In\n    # particular, we need to make sure that\n    #\n    #   1. The iterator gracefully exits the workers when its last reference is\n    #      gone.\n    #\n    #      In this case, the workers should be gracefully exited because the\n    #      main process may still need to continue to run, and we want cleaning\n    #      up code in the workers to be executed (e.g., releasing GPU memory).\n    #      Naturally, we implement the shutdown logic in `__del__` of\n    #      DataLoaderIterator.\n    #\n    #      We delay the discussion on the logic in this case until later.\n    #\n    #   2. The iterator exits the workers when the problem ends\n    #\n    #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n    #      Doing so means that when the program ends, it shuts the workers down\n    #      with a SIGTERM. `pin_memory_thread` will exit too, but by a different\n    #      mechanism.\n    #\n    #      You may ask, why don't we just not set the workers as daemonic, and\n    #      gracefully exit using the same logic as we have in `__del__` when the\n    #      iterator gets deleted (see 1 above)? The answer requires a bit\n    #      understanding of Python multiprocessing design. As of Python 3.7, for\n    #      reasons I have yet to understand, in a subprocess, Python runs the\n    #      given function (e.g., the `target` argument passed to a `mp.Process`)\n    #      using this pattern (unrelated code removed for clarity):\n    #\n    #          # These are run the sub-process\n    #          try:\n    #              user_provided_function()\n    #          finally:\n    #              multiprocessing.util._exit_function()\n    #\n    #      In `_exit_function`, Python joins all non-daemonic subprocesses of\n    #      this process (which is a subprocess of a Python process itself), and\n    #      sends SIGTERM to the daemonic ones. Therefore, if a DataLoader is\n    #      used in a subprocess (i.e., used in `user_provided_function` above),\n    #      and an error is raised containing frames that references the\n    #      DataLoaderIter (Python exception traces keeps local objects in\n    #      relevant frames alive), workers will be joined in `_exit_function`\n    #      before the `__del__` is called (which starts the shutdown logic). And\n    #      unfortunately the DataLoaderIter process will hang. E.g., such errors\n    #      can be timeout, or arbitrary error if users hold a reference to an\n    #      iterator.\n    #\n    #      For context, `_exit_function` is also registered as an `atexit` call.\n    #      So I really don't understand the need to do this in a finally block\n    #      The code dates back to 2008 and there is no comment on the original\n    #      PEP 371 or patch https://bugs.python.org/issue3050 (containing both\n    #      the finally block and the `atexit` registration) that explains this.\n    #\n    #      Another choice is to just shutdown workers with logic in 1 above\n    #      whenever we see an error in `next`. This isn't ideal because\n    #        a. It prevents users from using try-catch to resume data loading.\n    #        b. It doesn't prevent hanging if users have references to the\n    #           iterator.\n    #\n    #   3. All processes exit if any of them die unexpectedly (e.g., error,\n    #      SIGKILL).\n    #\n    #      As shown above, the workers are set as daemonic children of the main\n    #      process. However, automatic cleaning-up of such child processes only\n    #      happen if the parent process exits gracefully (e.g., SIGTERM). So we\n    #      must ensure that each process will exit even the process that should\n    #      send/receive data to/from it were killed, i.e.,\n    #\n    #        a. A process won't hang when getting from a queue.\n    #\n    #           Even with carefully designed data dependencies (i.e., a `put()`\n    #           always corresponding to a `get()`), hanging on `get()` can still\n    #           happen when data in queue is corrupted (e.g., due to\n    #           `cancel_join_thread` or unexpected exit).\n    #\n    #           For child exit, we register SIGCHLD handler on main process,\n    #           which checks if any of the workers fail in the (Python) handler.\n    #           See DataLoader.cpp.\n    #\n    #           For `.get()` calls where the sender(s) is not the workers, we\n    #           guard them with timeouts, and check the status of the sender\n    #           when timeout happens:\n    #             + in the workers, the `ManagerWatchdog` class checks the main\n    #               process status.\n    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\n    #               check `pin_memory_thread` status periodically until `.get()`\n    #               returns or see that `pin_memory_thread` died.\n    #\n    #        b. A process won't hang when putting into a queue;\n    #\n    #           We use `mp.Queue` which has a separate background thread to put\n    #           objects. The background thread is usually automatically joined\n    #           when the process exits.\n    #\n    #           However, in case that the receiver has ended abruptly while\n    #           reading from the pipe, the join will hang forever. Therefore,\n    #           for both `worker_result_queue` (worker -&gt; main process/pin_memory_thread)\n    #           and each `index_queue` (main process -&gt; worker), we use\n    #           `q.cancel_join_thread()` in sender process before any `q.put` to\n    #           prevent this automatic join.\n    #\n    #           Moreover, having all queues called `cancel_join_thread` makes\n    #           implementing graceful shutdown logic in `__del__` much easier.\n    #           It won't need to get from any queue, which would also need to be\n    #           guarded by periodic status checks.\n    #\n    #           Note that this may leave corrupted data in the queue, but we\n    #           don't care about the data anyways once we are shutting down.\n    #\n    #\n    # Now let's get back to 1:\n    #   how we gracefully exit the workers when the last reference to the\n    #   iteartor is gone.\n    #\n    # To achieve this, we implement the following logic along with the design\n    # choices mentioned above:\n    #\n    # [pin_memory_thread] and [worker processes]\n    #   When getting from queues,\n    #     if get a `None`, exit.\n    #     if get anything else or time out, check `done_event`,\n    #        if set, keep getting until see the `None`, then exit.\n    #        otherwise, process the data.\n    #\n    # [main process]\n    #   In the DataLoader Iter's `__del__`\n    #     a. Set `done_event` (shared with `pin_memory_thread` and workers).\n    #\n    #        Note: from here on, the workers &amp; `pin_memory_thread` may exit at\n    #              any time after they receive `None`.\n    #\n    #     b. Exit `pin_memory_thread`\n    #          i.   Put `None` in `worker_result_queue`.\n    #          ii.  Join the `pin_memory_thread`.\n    #\n    #     c. Exit the workers.\n    #          i.   Put `None` in each worker's `index_queue`.\n    #          ii.  Join the workers.\n    #\n    #        Note: This has to be after (b) because it may leave corrupted data\n    #              in `worker_result_queue`, which `pin_memory_thread` reads\n    #              from.\n    #\n    #   Note: If `pin_memory=False`, there is no `pin_memory_thread` and (b)\n    #         can be omitted\n    #\n    # NB: `done_event`s isn't strictly needed. E.g., we can just check for\n    #     `None` from `index_queue`, but it allows us to skip wasting resources\n    #     processing indices already in `index_queue` if we are already shutting\n    #     down.\n</code></pre>\n<p>Original desc:</p>\n<p>In <code>DataLoaderIter</code> <code>__del__</code>, ensure that <code>None</code> is sent to <code>pin_memory_thread</code> before joining workers.</p>\n<p>Trace when interrupted at such a hang:</p>\n<pre><code> Exception ignored in: &lt;function _DataLoaderIter.__del__ at 0x7facf66760d0&gt;\n Traceback (most recent call last):\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 412, in __del__\n     self._shutdown_workers()\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 408, in _shutdown_worke\n\n     self.pin_memory_thread.join()\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1032, in join\n     self._wait_for_tstate_lock()\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n     elif lock.acquire(block, timeout):\n KeyboardInterrupt\n\n</code></pre>\n<p>The 1st commit solves majority of the hang, but uncovers another problem:</p>\n<pre><code>36: Exception ignored in: &lt;function _DataLoaderIter.__del__ at 0x7f214fa412f0&gt;\n36: Traceback (most recent call last):\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 416, in __\ndel__\n36:     self._shutdown_workers()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 401, in _s\nhutdown_workers\n36:     self.worker_result_queue.join_thread()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 145, in join_thread\n36:     self._jointhread()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n36:     res = self._callback(*self._args, **self._kwargs)\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n36:     thread.join()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1032, in join\n36:     self._wait_for_tstate_lock()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n36:     elif lock.acquire(block, timeout):\n36: KeyboardInterrupt\n</code></pre>", "body_text": "Test plan:\nTrained 16000 LeNets (total) on MNIST on 160 processes. Previously some of them will either hang during training or during program exiting. Now every process finishes successfully.\n    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n    #\n    # Preliminary:\n    #\n    # Our data model looks like this (queues are indicated with curly brackets):\n    #\n    #                main process                              ||\n    #                     |                                    ||\n    #               {index_queue}                              ||\n    #                     |                                    ||\n    #              worker processes                            ||     DATA\n    #                     |                                    ||\n    #            {worker_result_queue}                         ||     FLOW\n    #                     |                                    ||\n    #      pin_memory_thread of main process                   ||   DIRECTION\n    #                     |                                    ||\n    #               {data_queue}                               ||\n    #                     |                                    ||\n    #                data output                               \\/\n    #\n    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n    #      `pin_memory=False`.\n    #\n    #\n    # Terminating multiprocessing logic requires very careful design. In\n    # particular, we need to make sure that\n    #\n    #   1. The iterator gracefully exits the workers when its last reference is\n    #      gone.\n    #\n    #      In this case, the workers should be gracefully exited because the\n    #      main process may still need to continue to run, and we want cleaning\n    #      up code in the workers to be executed (e.g., releasing GPU memory).\n    #      Naturally, we implement the shutdown logic in `__del__` of\n    #      DataLoaderIterator.\n    #\n    #      We delay the discussion on the logic in this case until later.\n    #\n    #   2. The iterator exits the workers when the problem ends\n    #\n    #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n    #      Doing so means that when the program ends, it shuts the workers down\n    #      with a SIGTERM. `pin_memory_thread` will exit too, but by a different\n    #      mechanism.\n    #\n    #      You may ask, why don't we just not set the workers as daemonic, and\n    #      gracefully exit using the same logic as we have in `__del__` when the\n    #      iterator gets deleted (see 1 above)? The answer requires a bit\n    #      understanding of Python multiprocessing design. As of Python 3.7, for\n    #      reasons I have yet to understand, in a subprocess, Python runs the\n    #      given function (e.g., the `target` argument passed to a `mp.Process`)\n    #      using this pattern (unrelated code removed for clarity):\n    #\n    #          # These are run the sub-process\n    #          try:\n    #              user_provided_function()\n    #          finally:\n    #              multiprocessing.util._exit_function()\n    #\n    #      In `_exit_function`, Python joins all non-daemonic subprocesses of\n    #      this process (which is a subprocess of a Python process itself), and\n    #      sends SIGTERM to the daemonic ones. Therefore, if a DataLoader is\n    #      used in a subprocess (i.e., used in `user_provided_function` above),\n    #      and an error is raised containing frames that references the\n    #      DataLoaderIter (Python exception traces keeps local objects in\n    #      relevant frames alive), workers will be joined in `_exit_function`\n    #      before the `__del__` is called (which starts the shutdown logic). And\n    #      unfortunately the DataLoaderIter process will hang. E.g., such errors\n    #      can be timeout, or arbitrary error if users hold a reference to an\n    #      iterator.\n    #\n    #      For context, `_exit_function` is also registered as an `atexit` call.\n    #      So I really don't understand the need to do this in a finally block\n    #      The code dates back to 2008 and there is no comment on the original\n    #      PEP 371 or patch https://bugs.python.org/issue3050 (containing both\n    #      the finally block and the `atexit` registration) that explains this.\n    #\n    #      Another choice is to just shutdown workers with logic in 1 above\n    #      whenever we see an error in `next`. This isn't ideal because\n    #        a. It prevents users from using try-catch to resume data loading.\n    #        b. It doesn't prevent hanging if users have references to the\n    #           iterator.\n    #\n    #   3. All processes exit if any of them die unexpectedly (e.g., error,\n    #      SIGKILL).\n    #\n    #      As shown above, the workers are set as daemonic children of the main\n    #      process. However, automatic cleaning-up of such child processes only\n    #      happen if the parent process exits gracefully (e.g., SIGTERM). So we\n    #      must ensure that each process will exit even the process that should\n    #      send/receive data to/from it were killed, i.e.,\n    #\n    #        a. A process won't hang when getting from a queue.\n    #\n    #           Even with carefully designed data dependencies (i.e., a `put()`\n    #           always corresponding to a `get()`), hanging on `get()` can still\n    #           happen when data in queue is corrupted (e.g., due to\n    #           `cancel_join_thread` or unexpected exit).\n    #\n    #           For child exit, we register SIGCHLD handler on main process,\n    #           which checks if any of the workers fail in the (Python) handler.\n    #           See DataLoader.cpp.\n    #\n    #           For `.get()` calls where the sender(s) is not the workers, we\n    #           guard them with timeouts, and check the status of the sender\n    #           when timeout happens:\n    #             + in the workers, the `ManagerWatchdog` class checks the main\n    #               process status.\n    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\n    #               check `pin_memory_thread` status periodically until `.get()`\n    #               returns or see that `pin_memory_thread` died.\n    #\n    #        b. A process won't hang when putting into a queue;\n    #\n    #           We use `mp.Queue` which has a separate background thread to put\n    #           objects. The background thread is usually automatically joined\n    #           when the process exits.\n    #\n    #           However, in case that the receiver has ended abruptly while\n    #           reading from the pipe, the join will hang forever. Therefore,\n    #           for both `worker_result_queue` (worker -> main process/pin_memory_thread)\n    #           and each `index_queue` (main process -> worker), we use\n    #           `q.cancel_join_thread()` in sender process before any `q.put` to\n    #           prevent this automatic join.\n    #\n    #           Moreover, having all queues called `cancel_join_thread` makes\n    #           implementing graceful shutdown logic in `__del__` much easier.\n    #           It won't need to get from any queue, which would also need to be\n    #           guarded by periodic status checks.\n    #\n    #           Note that this may leave corrupted data in the queue, but we\n    #           don't care about the data anyways once we are shutting down.\n    #\n    #\n    # Now let's get back to 1:\n    #   how we gracefully exit the workers when the last reference to the\n    #   iteartor is gone.\n    #\n    # To achieve this, we implement the following logic along with the design\n    # choices mentioned above:\n    #\n    # [pin_memory_thread] and [worker processes]\n    #   When getting from queues,\n    #     if get a `None`, exit.\n    #     if get anything else or time out, check `done_event`,\n    #        if set, keep getting until see the `None`, then exit.\n    #        otherwise, process the data.\n    #\n    # [main process]\n    #   In the DataLoader Iter's `__del__`\n    #     a. Set `done_event` (shared with `pin_memory_thread` and workers).\n    #\n    #        Note: from here on, the workers & `pin_memory_thread` may exit at\n    #              any time after they receive `None`.\n    #\n    #     b. Exit `pin_memory_thread`\n    #          i.   Put `None` in `worker_result_queue`.\n    #          ii.  Join the `pin_memory_thread`.\n    #\n    #     c. Exit the workers.\n    #          i.   Put `None` in each worker's `index_queue`.\n    #          ii.  Join the workers.\n    #\n    #        Note: This has to be after (b) because it may leave corrupted data\n    #              in `worker_result_queue`, which `pin_memory_thread` reads\n    #              from.\n    #\n    #   Note: If `pin_memory=False`, there is no `pin_memory_thread` and (b)\n    #         can be omitted\n    #\n    # NB: `done_event`s isn't strictly needed. E.g., we can just check for\n    #     `None` from `index_queue`, but it allows us to skip wasting resources\n    #     processing indices already in `index_queue` if we are already shutting\n    #     down.\n\nOriginal desc:\nIn DataLoaderIter __del__, ensure that None is sent to pin_memory_thread before joining workers.\nTrace when interrupted at such a hang:\n Exception ignored in: <function _DataLoaderIter.__del__ at 0x7facf66760d0>\n Traceback (most recent call last):\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 412, in __del__\n     self._shutdown_workers()\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 408, in _shutdown_worke\n\n     self.pin_memory_thread.join()\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1032, in join\n     self._wait_for_tstate_lock()\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n     elif lock.acquire(block, timeout):\n KeyboardInterrupt\n\n\nThe 1st commit solves majority of the hang, but uncovers another problem:\n36: Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f214fa412f0>\n36: Traceback (most recent call last):\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 416, in __\ndel__\n36:     self._shutdown_workers()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 401, in _s\nhutdown_workers\n36:     self.worker_result_queue.join_thread()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 145, in join_thread\n36:     self._jointhread()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n36:     res = self._callback(*self._args, **self._kwargs)\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n36:     thread.join()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1032, in join\n36:     self._wait_for_tstate_lock()\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n36:     elif lock.acquire(block, timeout):\n36: KeyboardInterrupt", "body": "Test plan:\r\n\r\nTrained 16000 LeNets (total) on MNIST on 160 processes. Previously some of them will either hang during training or during program exiting. Now every process finishes successfully.\r\n\r\n```\r\n    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\r\n    #\r\n    # Preliminary:\r\n    #\r\n    # Our data model looks like this (queues are indicated with curly brackets):\r\n    #\r\n    #                main process                              ||\r\n    #                     |                                    ||\r\n    #               {index_queue}                              ||\r\n    #                     |                                    ||\r\n    #              worker processes                            ||     DATA\r\n    #                     |                                    ||\r\n    #            {worker_result_queue}                         ||     FLOW\r\n    #                     |                                    ||\r\n    #      pin_memory_thread of main process                   ||   DIRECTION\r\n    #                     |                                    ||\r\n    #               {data_queue}                               ||\r\n    #                     |                                    ||\r\n    #                data output                               \\/\r\n    #\r\n    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\r\n    #      `pin_memory=False`.\r\n    #\r\n    #\r\n    # Terminating multiprocessing logic requires very careful design. In\r\n    # particular, we need to make sure that\r\n    #\r\n    #   1. The iterator gracefully exits the workers when its last reference is\r\n    #      gone.\r\n    #\r\n    #      In this case, the workers should be gracefully exited because the\r\n    #      main process may still need to continue to run, and we want cleaning\r\n    #      up code in the workers to be executed (e.g., releasing GPU memory).\r\n    #      Naturally, we implement the shutdown logic in `__del__` of\r\n    #      DataLoaderIterator.\r\n    #\r\n    #      We delay the discussion on the logic in this case until later.\r\n    #\r\n    #   2. The iterator exits the workers when the problem ends\r\n    #\r\n    #      We set all workers and `pin_memory_thread` to have `daemon=True`.\r\n    #      Doing so means that when the program ends, it shuts the workers down\r\n    #      with a SIGTERM. `pin_memory_thread` will exit too, but by a different\r\n    #      mechanism.\r\n    #\r\n    #      You may ask, why don't we just not set the workers as daemonic, and\r\n    #      gracefully exit using the same logic as we have in `__del__` when the\r\n    #      iterator gets deleted (see 1 above)? The answer requires a bit\r\n    #      understanding of Python multiprocessing design. As of Python 3.7, for\r\n    #      reasons I have yet to understand, in a subprocess, Python runs the\r\n    #      given function (e.g., the `target` argument passed to a `mp.Process`)\r\n    #      using this pattern (unrelated code removed for clarity):\r\n    #\r\n    #          # These are run the sub-process\r\n    #          try:\r\n    #              user_provided_function()\r\n    #          finally:\r\n    #              multiprocessing.util._exit_function()\r\n    #\r\n    #      In `_exit_function`, Python joins all non-daemonic subprocesses of\r\n    #      this process (which is a subprocess of a Python process itself), and\r\n    #      sends SIGTERM to the daemonic ones. Therefore, if a DataLoader is\r\n    #      used in a subprocess (i.e., used in `user_provided_function` above),\r\n    #      and an error is raised containing frames that references the\r\n    #      DataLoaderIter (Python exception traces keeps local objects in\r\n    #      relevant frames alive), workers will be joined in `_exit_function`\r\n    #      before the `__del__` is called (which starts the shutdown logic). And\r\n    #      unfortunately the DataLoaderIter process will hang. E.g., such errors\r\n    #      can be timeout, or arbitrary error if users hold a reference to an\r\n    #      iterator.\r\n    #\r\n    #      For context, `_exit_function` is also registered as an `atexit` call.\r\n    #      So I really don't understand the need to do this in a finally block\r\n    #      The code dates back to 2008 and there is no comment on the original\r\n    #      PEP 371 or patch https://bugs.python.org/issue3050 (containing both\r\n    #      the finally block and the `atexit` registration) that explains this.\r\n    #\r\n    #      Another choice is to just shutdown workers with logic in 1 above\r\n    #      whenever we see an error in `next`. This isn't ideal because\r\n    #        a. It prevents users from using try-catch to resume data loading.\r\n    #        b. It doesn't prevent hanging if users have references to the\r\n    #           iterator.\r\n    #\r\n    #   3. All processes exit if any of them die unexpectedly (e.g., error,\r\n    #      SIGKILL).\r\n    #\r\n    #      As shown above, the workers are set as daemonic children of the main\r\n    #      process. However, automatic cleaning-up of such child processes only\r\n    #      happen if the parent process exits gracefully (e.g., SIGTERM). So we\r\n    #      must ensure that each process will exit even the process that should\r\n    #      send/receive data to/from it were killed, i.e.,\r\n    #\r\n    #        a. A process won't hang when getting from a queue.\r\n    #\r\n    #           Even with carefully designed data dependencies (i.e., a `put()`\r\n    #           always corresponding to a `get()`), hanging on `get()` can still\r\n    #           happen when data in queue is corrupted (e.g., due to\r\n    #           `cancel_join_thread` or unexpected exit).\r\n    #\r\n    #           For child exit, we register SIGCHLD handler on main process,\r\n    #           which checks if any of the workers fail in the (Python) handler.\r\n    #           See DataLoader.cpp.\r\n    #\r\n    #           For `.get()` calls where the sender(s) is not the workers, we\r\n    #           guard them with timeouts, and check the status of the sender\r\n    #           when timeout happens:\r\n    #             + in the workers, the `ManagerWatchdog` class checks the main\r\n    #               process status.\r\n    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\r\n    #               check `pin_memory_thread` status periodically until `.get()`\r\n    #               returns or see that `pin_memory_thread` died.\r\n    #\r\n    #        b. A process won't hang when putting into a queue;\r\n    #\r\n    #           We use `mp.Queue` which has a separate background thread to put\r\n    #           objects. The background thread is usually automatically joined\r\n    #           when the process exits.\r\n    #\r\n    #           However, in case that the receiver has ended abruptly while\r\n    #           reading from the pipe, the join will hang forever. Therefore,\r\n    #           for both `worker_result_queue` (worker -> main process/pin_memory_thread)\r\n    #           and each `index_queue` (main process -> worker), we use\r\n    #           `q.cancel_join_thread()` in sender process before any `q.put` to\r\n    #           prevent this automatic join.\r\n    #\r\n    #           Moreover, having all queues called `cancel_join_thread` makes\r\n    #           implementing graceful shutdown logic in `__del__` much easier.\r\n    #           It won't need to get from any queue, which would also need to be\r\n    #           guarded by periodic status checks.\r\n    #\r\n    #           Note that this may leave corrupted data in the queue, but we\r\n    #           don't care about the data anyways once we are shutting down.\r\n    #\r\n    #\r\n    # Now let's get back to 1:\r\n    #   how we gracefully exit the workers when the last reference to the\r\n    #   iteartor is gone.\r\n    #\r\n    # To achieve this, we implement the following logic along with the design\r\n    # choices mentioned above:\r\n    #\r\n    # [pin_memory_thread] and [worker processes]\r\n    #   When getting from queues,\r\n    #     if get a `None`, exit.\r\n    #     if get anything else or time out, check `done_event`,\r\n    #        if set, keep getting until see the `None`, then exit.\r\n    #        otherwise, process the data.\r\n    #\r\n    # [main process]\r\n    #   In the DataLoader Iter's `__del__`\r\n    #     a. Set `done_event` (shared with `pin_memory_thread` and workers).\r\n    #\r\n    #        Note: from here on, the workers & `pin_memory_thread` may exit at\r\n    #              any time after they receive `None`.\r\n    #\r\n    #     b. Exit `pin_memory_thread`\r\n    #          i.   Put `None` in `worker_result_queue`.\r\n    #          ii.  Join the `pin_memory_thread`.\r\n    #\r\n    #     c. Exit the workers.\r\n    #          i.   Put `None` in each worker's `index_queue`.\r\n    #          ii.  Join the workers.\r\n    #\r\n    #        Note: This has to be after (b) because it may leave corrupted data\r\n    #              in `worker_result_queue`, which `pin_memory_thread` reads\r\n    #              from.\r\n    #\r\n    #   Note: If `pin_memory=False`, there is no `pin_memory_thread` and (b)\r\n    #         can be omitted\r\n    #\r\n    # NB: `done_event`s isn't strictly needed. E.g., we can just check for\r\n    #     `None` from `index_queue`, but it allows us to skip wasting resources\r\n    #     processing indices already in `index_queue` if we are already shutting\r\n    #     down.\r\n```\r\n\r\nOriginal desc:\r\n\r\nIn `DataLoaderIter` `__del__`, ensure that `None` is sent to `pin_memory_thread` before joining workers.\r\n\r\n\r\nTrace when interrupted at such a hang:\r\n```\r\n Exception ignored in: <function _DataLoaderIter.__del__ at 0x7facf66760d0>\r\n Traceback (most recent call last):\r\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 412, in __del__\r\n     self._shutdown_workers()\r\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 408, in _shutdown_worke\r\n\r\n     self.pin_memory_thread.join()\r\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1032, in join\r\n     self._wait_for_tstate_lock()\r\n   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\r\n     elif lock.acquire(block, timeout):\r\n KeyboardInterrupt\r\n\r\n```\r\n\r\nThe 1st commit solves majority of the hang, but uncovers another problem:\r\n```\r\n36: Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f214fa412f0>\r\n36: Traceback (most recent call last):\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 416, in __\r\ndel__\r\n36:     self._shutdown_workers()\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 401, in _s\r\nhutdown_workers\r\n36:     self.worker_result_queue.join_thread()\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 145, in join_thread\r\n36:     self._jointhread()\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\r\n36:     res = self._callback(*self._args, **self._kwargs)\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\r\n36:     thread.join()\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1032, in join\r\n36:     self._wait_for_tstate_lock()\r\n36:   File \"/private/home/ssnl/miniconda3/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\r\n36:     elif lock.acquire(block, timeout):\r\n36: KeyboardInterrupt\r\n```"}