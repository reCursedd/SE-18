{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/222756450", "pull_request_review_id": 161714244, "id": 222756450, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMjc1NjQ1MA==", "diff_hunk": "@@ -246,6 +278,171 @@ def handler(signum, frame):\n class _DataLoaderIter(object):\n     r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\n \n+    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n+    #\n+    # Preliminary:\n+    #\n+    # Our data model looks like this (queues are indicated with curly brackets):\n+    #\n+    #                main process                              ||\n+    #                     |                                    ||\n+    #               {index_queue}                              ||\n+    #                     |                                    ||\n+    #              worker processes                            ||     DATA\n+    #                     |                                    ||\n+    #            {worker_result_queue}                         ||     FLOW\n+    #                     |                                    ||\n+    #      pin_memory_thread of main process                   ||   DIRECTION\n+    #                     |                                    ||\n+    #               {data_queue}                               ||\n+    #                     |                                    ||\n+    #                data output                               \\/\n+    #\n+    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n+    #      `pin_memory=False`.\n+    #\n+    #\n+    # Terminating multiprocessing logic requires very careful design. In\n+    # particular, we need to make sure that\n+    #\n+    #   1. The iterator gracefully exits the workers when its last reference is\n+    #      gone.\n+    #\n+    #      In this case, the workers should be gracefully exited because the\n+    #      main process may still need to continue to run, and we want cleaning\n+    #      up code in the workers to be executed (e.g., releasing GPU memory).\n+    #      Naturally, we implement the shutdown logic in `__del__` of\n+    #      DataLoaderIterator.\n+    #\n+    #      We delay the discussion on the logic in this case until later.\n+    #\n+    #   2. The iterator exits the workers when the problem ends\n+    #\n+    #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n+    #      Doing so means that when the program ends, it shuts the workers down\n+    #      with a SIGTERM. `pin_memory_thread` will exit too, but by a different\n+    #      mechanism.\n+    #\n+    #      You may ask, why don't we just not set the workers as daemonic, and\n+    #      gracefully exit using the same logic as we have in `__del__` when the\n+    #      iterator gets deleted (see 1 above)? The answer requires a bit\n+    #      understanding of Python multiprocessing design. As of Python 3.7, for\n+    #      reasons I have yet to understand, in a subprocess, Python runs the\n+    #      given function (e.g., the `target` argument passed to a `mp.Process`)\n+    #      using this pattern (unrelated code removed for clarity):\n+    #\n+    #          # These are run the sub-process\n+    #          try:\n+    #              user_provided_function()\n+    #          finally:\n+    #              multiprocessing.util._exit_function()\n+    #\n+    #      In `_exit_function`, Python joins all non-daemonic subprocesses of\n+    #      this process (which is a subprocess of a Python process itself), and\n+    #      sends SIGTERM to the daemonic ones. Therefore, if a DataLoader is\n+    #      used in a subprocess (i.e., used in `user_provided_function` above),\n+    #      and an error is raised containing frames that references the\n+    #      DataLoaderIter (Python exception traces keeps local objects in\n+    #      relevant frames alive), workers will be joined in `_exit_function`\n+    #      before the `__del__` is called (which starts the shutdown logic). And\n+    #      unfortunately the DataLoaderIter process will hang. E.g., such errors\n+    #      can be timeout, or arbitrary error if users hold a reference to an\n+    #      iterator.", "path": "torch/utils/data/dataloader.py", "position": null, "original_position": 250, "commit_id": "d3c4ffa092580d5b15ef2705bd3904f7b685e511", "original_commit_id": "9ce8d93d03726f874260cb5c132438243b810ffc", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think the main message of this paragraph is that we can't make them non-daemonic, because that causes hangs. Can we simply make all of this paragraph look like that:\r\n\r\nWhen a process exits, Python joins all its non-daemonic subprocesses, and terminates (via SIGTERM) all daemonic ones. This fact, together with a few implementation details of multiprocessing, forces us to make workers daemonic. All of our problems arise when a DataLoader is used in a subprocess, and are caused by multiprocessing code which looks more or less like this:\r\n```\r\ntry:\r\n    your_function_using_a_dataloader()\r\nfinally:\r\n    multiprocessing.util._exit_function()\r\n```\r\nThe joining/termination mentioned above happens inside `_exit_function()`. Now, if `your_function_using_a_dataloader()` throws, the stack trace stored in the exception will prevent the frame which uses `DataLoaderIter` to be freed. This means that its `__del__`, which starts the shutdown procedure, will not be called. That, in turn, means that workers aren't notified and attempts to join in `_exit_function` will result in a hang.", "created_at": "2018-10-04T17:17:55Z", "updated_at": "2018-11-23T15:52:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/11985#discussion_r222756450", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11985", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/222756450"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11985#discussion_r222756450"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11985"}}, "body_html": "<p>I think the main message of this paragraph is that we can't make them non-daemonic, because that causes hangs. Can we simply make all of this paragraph look like that:</p>\n<p>When a process exits, Python joins all its non-daemonic subprocesses, and terminates (via SIGTERM) all daemonic ones. This fact, together with a few implementation details of multiprocessing, forces us to make workers daemonic. All of our problems arise when a DataLoader is used in a subprocess, and are caused by multiprocessing code which looks more or less like this:</p>\n<pre><code>try:\n    your_function_using_a_dataloader()\nfinally:\n    multiprocessing.util._exit_function()\n</code></pre>\n<p>The joining/termination mentioned above happens inside <code>_exit_function()</code>. Now, if <code>your_function_using_a_dataloader()</code> throws, the stack trace stored in the exception will prevent the frame which uses <code>DataLoaderIter</code> to be freed. This means that its <code>__del__</code>, which starts the shutdown procedure, will not be called. That, in turn, means that workers aren't notified and attempts to join in <code>_exit_function</code> will result in a hang.</p>", "body_text": "I think the main message of this paragraph is that we can't make them non-daemonic, because that causes hangs. Can we simply make all of this paragraph look like that:\nWhen a process exits, Python joins all its non-daemonic subprocesses, and terminates (via SIGTERM) all daemonic ones. This fact, together with a few implementation details of multiprocessing, forces us to make workers daemonic. All of our problems arise when a DataLoader is used in a subprocess, and are caused by multiprocessing code which looks more or less like this:\ntry:\n    your_function_using_a_dataloader()\nfinally:\n    multiprocessing.util._exit_function()\n\nThe joining/termination mentioned above happens inside _exit_function(). Now, if your_function_using_a_dataloader() throws, the stack trace stored in the exception will prevent the frame which uses DataLoaderIter to be freed. This means that its __del__, which starts the shutdown procedure, will not be called. That, in turn, means that workers aren't notified and attempts to join in _exit_function will result in a hang."}