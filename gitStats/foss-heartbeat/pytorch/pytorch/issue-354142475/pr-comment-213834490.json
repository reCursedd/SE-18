{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213834490", "pull_request_review_id": 150755038, "id": 213834490, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzgzNDQ5MA==", "diff_hunk": "@@ -11,69 +16,377 @@ template <class Context>\n class SpatialBNOp : public Operator<Context> {\n  public:\n   USE_OPERATOR_CONTEXT_FUNCTIONS;\n+\n   SpatialBNOp(const OperatorDef& operator_def, Workspace* ws)\n       : Operator<Context>(operator_def, ws),\n-        is_test_(this->template GetSingleArgument<int>(OpSchema::Arg_IsTest, 0)),\n-        epsilon_(this->template GetSingleArgument<float>(\"epsilon\", 1e-5f)),\n-        momentum_(this->template GetSingleArgument<float>(\"momentum\", 0.9f)),\n+        OP_SINGLE_ARG(bool, OpSchema::Arg_IsTest, is_test_, false),\n+        OP_SINGLE_ARG(double, \"epsilon\", epsilon_, 1e-5),\n+        OP_SINGLE_ARG(float, \"momentum\", momentum_, 0.9f),\n         order_(StringToStorageOrder(\n-            this->template GetSingleArgument<string>(\"order\", \"NCHW\"))),\n-        num_batches_(this->template GetSingleArgument<int>(\"num_batches\", 1)) {\n-    // TODO(jiayq): update the input and output size checks.\n+            OperatorBase::GetSingleArgument<string>(\"order\", \"NCHW\"))),\n+        OP_SINGLE_ARG(int, \"num_batches\", num_batches_, 1) {\n+    CAFFE_ENFORCE_NE(\n+        order_,\n+        StorageOrder::UNKNOWN,\n+        \"order should be either \\\"NCHW\\\" or \\\"NHWC\\\".\");\n     CAFFE_ENFORCE(\n         (is_test_ && OutputSize() == 1) || (!is_test_ && OutputSize() == 5));\n     CAFFE_ENFORCE_GT(epsilon_, 0);\n     CAFFE_ENFORCE_GE(momentum_, 0);\n     CAFFE_ENFORCE_LE(momentum_, 1);\n   }\n-  ~SpatialBNOp() {}\n+\n+  virtual ~SpatialBNOp() = default;\n \n   bool RunOnDevice() override {\n+    return DispatchHelper<TensorTypes<float>>::call(this, Input(0));\n+  }\n+\n+  template <typename T>\n+  bool DoRunWithType() {\n+    const auto& X = Input(INPUT);\n+    const auto& scale = Input(SCALE);\n+    const auto& bias = Input(BIAS);\n+    auto* Y = Output(OUTPUT);\n+\n+    const int ndim = X.ndim();\n+    CAFFE_ENFORCE_GE(ndim, 3);\n+    const int N = X.dim32(0);\n+    const int C =\n+        (order_ == StorageOrder::NCHW ? X.dim32(1) : X.dim32(ndim - 1));\n+    const std::vector<int> X_dims(X.dims().cbegin(), X.dims().cend());\n+    const int HxW =\n+        std::accumulate(\n+            X_dims.cbegin() + 1, X_dims.cend(), 1, std::multiplies<int>()) /\n+        C;\n+    CAFFE_ENFORCE_EQ(scale.size(), C);\n+    CAFFE_ENFORCE_EQ(bias.size(), C);\n+\n+    Y->ResizeLike(X);\n+    const T* X_data = X.template data<T>();\n+    const T* scale_data = scale.template data<T>();\n+    const T* bias_data = bias.template data<T>();\n+    T* Y_data = Y->template mutable_data<T>();\n+    alpha_.Resize(C);\n+    beta_.Resize(C);\n+    T* alpha_data = alpha_.template mutable_data<T>();\n+    T* beta_data = beta_.template mutable_data<T>();\n+    if (is_test_) {\n+      if (N == 0) {\n+        return true;\n+      }\n+      const auto& mean = Input(EST_MEAN);\n+      const auto& var = Input(EST_VAR);\n+      CAFFE_ENFORCE_EQ(mean.size(), C);\n+      CAFFE_ENFORCE_EQ(var.size(), C);\n+      ComputeFusedParam<T>(\n+          C,\n+          scale_data,\n+          bias_data,\n+          mean.template data<T>(),\n+          var.template data<T>(),\n+          alpha_data,\n+          beta_data);\n+    } else {\n+      auto* saved_mean = Output(SAVED_MEAN);\n+      auto* saved_rstd = Output(SAVED_INV_STD);\n+      saved_mean->Resize(C);\n+      saved_rstd->Resize(C);\n+      T* saved_mean_data = saved_mean->template mutable_data<T>();\n+      T* saved_rstd_data = saved_rstd->template mutable_data<T>();\n+      auto* running_mean = Output(RUNNING_MEAN);\n+      auto* running_var = Output(RUNNING_VAR);\n+      if (running_mean->size() != C) {\n+        running_mean->Resize(C);\n+        math::Set<T, Context>(\n+            C, T(0), running_mean->template mutable_data<T>(), &context_);\n+      }\n+      if (running_var->size() != C) {\n+        running_var->Resize(C);\n+        math::Set<T, Context>(\n+            C, T(0), running_var->template mutable_data<T>(), &context_);\n+      }\n+      T* running_mean_data = running_mean->template mutable_data<T>();\n+      T* running_var_data = running_var->template mutable_data<T>();\n+      if (N == 0) {\n+        math::Set<T, Context>(C, T(0), saved_mean_data, &context_);\n+        math::Set<T, Context>(C, T(0), saved_rstd_data, &context_);\n+        return true;\n+      }\n+      if (num_batches_ > 1) {\n+        const auto& batch_mean_sum = Input(BATCH_MEAN_SUM);\n+        const auto& batch_var_sum = Input(BATCH_VAR_SUM);\n+        CAFFE_ENFORCE_EQ(batch_mean_sum.size(), C);\n+        CAFFE_ENFORCE_EQ(batch_var_sum.size(), C);\n+        ComputeBatchMoments<T>(\n+            N,\n+            C,\n+            HxW,\n+            batch_mean_sum.template data<T>(),\n+            batch_var_sum.template data<T>(),\n+            saved_mean_data,\n+            saved_rstd_data);\n+      } else {\n+        if (order_ == StorageOrder::NCHW) {\n+          const std::array<int, 3> dims = {N, C, HxW};\n+          const std::array<int, 3> axes = {0, 2};\n+          math::Moments<T, Context>(\n+              3,\n+              dims.data(),\n+              2,\n+              axes.data(),\n+              X_data,\n+              saved_mean_data,\n+              saved_rstd_data,\n+              &context_);\n+        } else {\n+          const std::array<int, 2> dims = {N * HxW, C};\n+          const int axis = 0;\n+          math::Moments<T, Context>(\n+              2,\n+              dims.data(),\n+              1,\n+              &axis,\n+              X_data,\n+              saved_mean_data,\n+              saved_rstd_data,\n+              &context_);\n+        }\n+      }\n+      ComputeRunningMomentsAndFusedParam<T>(\n+          C,\n+          scale_data,\n+          bias_data,\n+          saved_mean_data,\n+          saved_rstd_data,\n+          running_mean_data,\n+          running_var_data,\n+          saved_rstd_data,\n+          alpha_data,\n+          beta_data);\n+    }\n+    if (order_ == StorageOrder::NCHW) {\n+      math::AffineChannel<T, Context, StorageOrder::NCHW>(\n+          N, C, HxW, X_data, alpha_data, beta_data, Y_data, &context_);\n+    } else {\n+      math::AffineChannel<T, Context, StorageOrder::NHWC>(\n+          N, C, HxW, X_data, alpha_data, beta_data, Y_data, &context_);\n+    }\n+\n     return true;\n   }\n \n  protected:\n-  bool is_test_;\n+  template <typename T>\n+  void ComputeFusedParam(", "path": "caffe2/operators/spatial_batch_norm_op.h", "position": 198, "original_position": 186, "commit_id": "2a46f10abbe471e5e67a16273600edb3baccf1d7", "original_commit_id": "1d6906f2c2122b83c7aaf2a15f69f8f1fea73ab0", "user": {"login": "BIT-silence", "id": 3357667, "node_id": "MDQ6VXNlcjMzNTc2Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3357667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BIT-silence", "html_url": "https://github.com/BIT-silence", "followers_url": "https://api.github.com/users/BIT-silence/followers", "following_url": "https://api.github.com/users/BIT-silence/following{/other_user}", "gists_url": "https://api.github.com/users/BIT-silence/gists{/gist_id}", "starred_url": "https://api.github.com/users/BIT-silence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BIT-silence/subscriptions", "organizations_url": "https://api.github.com/users/BIT-silence/orgs", "repos_url": "https://api.github.com/users/BIT-silence/repos", "events_url": "https://api.github.com/users/BIT-silence/events{/privacy}", "received_events_url": "https://api.github.com/users/BIT-silence/received_events", "type": "User", "site_admin": false}, "body": "for the Forward pass, the actual formula is Y = (X - mu) / std * scale + bias. In order to reduce the number of computation, we fused it to alpha = scale / std, beta = bias - mu / std * scale. Then the formula is Y = alpha * X + beta. Here we are computing alpha and beta which is the fused param for the forward pass.", "created_at": "2018-08-29T21:06:56Z", "updated_at": "2018-11-23T15:50:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/10888#discussion_r213834490", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10888", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213834490"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10888#discussion_r213834490"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10888"}}, "body_html": "<p>for the Forward pass, the actual formula is Y = (X - mu) / std * scale + bias. In order to reduce the number of computation, we fused it to alpha = scale / std, beta = bias - mu / std * scale. Then the formula is Y = alpha * X + beta. Here we are computing alpha and beta which is the fused param for the forward pass.</p>", "body_text": "for the Forward pass, the actual formula is Y = (X - mu) / std * scale + bias. In order to reduce the number of computation, we fused it to alpha = scale / std, beta = bias - mu / std * scale. Then the formula is Y = alpha * X + beta. Here we are computing alpha and beta which is the fused param for the forward pass.", "in_reply_to_id": 213550440}