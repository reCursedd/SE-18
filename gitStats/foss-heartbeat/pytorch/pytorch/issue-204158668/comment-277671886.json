{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/277671886", "html_url": "https://github.com/pytorch/pytorch/issues/649#issuecomment-277671886", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/649", "id": 277671886, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NzY3MTg4Ng==", "user": {"login": "everwind", "id": 2656931, "node_id": "MDQ6VXNlcjI2NTY5MzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/2656931?v=4", "gravatar_id": "", "url": "https://api.github.com/users/everwind", "html_url": "https://github.com/everwind", "followers_url": "https://api.github.com/users/everwind/followers", "following_url": "https://api.github.com/users/everwind/following{/other_user}", "gists_url": "https://api.github.com/users/everwind/gists{/gist_id}", "starred_url": "https://api.github.com/users/everwind/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/everwind/subscriptions", "organizations_url": "https://api.github.com/users/everwind/orgs", "repos_url": "https://api.github.com/users/everwind/repos", "events_url": "https://api.github.com/users/everwind/events{/privacy}", "received_events_url": "https://api.github.com/users/everwind/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-06T12:53:18Z", "updated_at": "2017-02-06T12:53:18Z", "author_association": "NONE", "body_html": "<p>I implement the data_parallel with two inputs, but it does not work</p>\n<p>def data_parallel2(module, input1, input2, device_ids, output_device=None):<br>\n\"\"\"Evaluates module(input) in parallel across the GPUs given in device_ids.</p>\n<pre><code>This is the functional version of the DataParallel module. \n\nArgs:   \n    module: the module to evaluate in parallel\n    input: input to the module\n    device_ids: GPU ids on which to replicate module\n    output_device: GPU location of the output  Use -1 to indicate the CPU.\n        (default: device_ids[0])\nReturns:\n    a Variable containing the result of module(input) located on\n    output_device\n\"\"\"\nif not device_ids:\n    return module(input1, input2) \n\nif output_device is None:\n    output_device = device_ids[0]\n\nreplicas = replicate(module, device_ids)\ninput1s = scatter(input1, device_ids)\ninput2s = scatter(input2, device_ids)\nreplicas = replicas[:len(input1s)]\noutputs = parallel_apply2(replicas, input1s, input2s)\nreturn gather(outputs, output_device)\n</code></pre>\n<p>def parallel_apply2(modules, input1s, input2s):<br>\nassert len(modules) == len(input1s)<br>\n# Fast track<br>\nif len(modules) == 1:<br>\nreturn (modules[0](input1s[0], input2s[0]),)</p>\n<pre><code>lock = threading.Lock()\nresults = {}\n\ndef _worker(module, input1, input2, results, lock):\n    var_input1 = input1\n    var_input2 = input2\n    while not isinstance(var_input1, Variable):\n        var_input1 = var_input1[0]\n    while not isinstance(var_input2, Variable):\n        var_input2 = var_input2[0]\n    try:    \n        with torch.cuda.device_of(var_input1):\n            output = module(input1, input2) \n        with lock:\n            results[input1] = output\n    except Exception as e:\n        with lock:\n            results[input1] = e\n\nthreads = [threading.Thread(target=_worker,\n                            args=(module, input1, input2, results, lock))\n            for module, input1, input2 in zip(modules, input1s, input2s)]\n</code></pre>", "body_text": "I implement the data_parallel with two inputs, but it does not work\ndef data_parallel2(module, input1, input2, device_ids, output_device=None):\n\"\"\"Evaluates module(input) in parallel across the GPUs given in device_ids.\nThis is the functional version of the DataParallel module. \n\nArgs:   \n    module: the module to evaluate in parallel\n    input: input to the module\n    device_ids: GPU ids on which to replicate module\n    output_device: GPU location of the output  Use -1 to indicate the CPU.\n        (default: device_ids[0])\nReturns:\n    a Variable containing the result of module(input) located on\n    output_device\n\"\"\"\nif not device_ids:\n    return module(input1, input2) \n\nif output_device is None:\n    output_device = device_ids[0]\n\nreplicas = replicate(module, device_ids)\ninput1s = scatter(input1, device_ids)\ninput2s = scatter(input2, device_ids)\nreplicas = replicas[:len(input1s)]\noutputs = parallel_apply2(replicas, input1s, input2s)\nreturn gather(outputs, output_device)\n\ndef parallel_apply2(modules, input1s, input2s):\nassert len(modules) == len(input1s)\n# Fast track\nif len(modules) == 1:\nreturn (modules[0](input1s[0], input2s[0]),)\nlock = threading.Lock()\nresults = {}\n\ndef _worker(module, input1, input2, results, lock):\n    var_input1 = input1\n    var_input2 = input2\n    while not isinstance(var_input1, Variable):\n        var_input1 = var_input1[0]\n    while not isinstance(var_input2, Variable):\n        var_input2 = var_input2[0]\n    try:    \n        with torch.cuda.device_of(var_input1):\n            output = module(input1, input2) \n        with lock:\n            results[input1] = output\n    except Exception as e:\n        with lock:\n            results[input1] = e\n\nthreads = [threading.Thread(target=_worker,\n                            args=(module, input1, input2, results, lock))\n            for module, input1, input2 in zip(modules, input1s, input2s)]", "body": "I implement the data_parallel with two inputs, but it does not work\r\n\r\ndef data_parallel2(module, input1, input2, device_ids, output_device=None):\r\n    \"\"\"Evaluates module(input) in parallel across the GPUs given in device_ids.\r\n\r\n    This is the functional version of the DataParallel module. \r\n\r\n    Args:   \r\n        module: the module to evaluate in parallel\r\n        input: input to the module\r\n        device_ids: GPU ids on which to replicate module\r\n        output_device: GPU location of the output  Use -1 to indicate the CPU.\r\n            (default: device_ids[0])\r\n    Returns:\r\n        a Variable containing the result of module(input) located on\r\n        output_device\r\n    \"\"\"\r\n    if not device_ids:\r\n        return module(input1, input2) \r\n\r\n    if output_device is None:\r\n        output_device = device_ids[0]\r\n\r\n    replicas = replicate(module, device_ids)\r\n    input1s = scatter(input1, device_ids)\r\n    input2s = scatter(input2, device_ids)\r\n    replicas = replicas[:len(input1s)]\r\n    outputs = parallel_apply2(replicas, input1s, input2s)\r\n    return gather(outputs, output_device)\r\n\r\ndef parallel_apply2(modules, input1s, input2s):\r\n    assert len(modules) == len(input1s)\r\n    # Fast track\r\n    if len(modules) == 1:\r\n        return (modules[0](input1s[0], input2s[0]),)\r\n\r\n    lock = threading.Lock()\r\n    results = {}\r\n\r\n    def _worker(module, input1, input2, results, lock):\r\n        var_input1 = input1\r\n        var_input2 = input2\r\n        while not isinstance(var_input1, Variable):\r\n            var_input1 = var_input1[0]\r\n        while not isinstance(var_input2, Variable):\r\n            var_input2 = var_input2[0]\r\n        try:    \r\n            with torch.cuda.device_of(var_input1):\r\n                output = module(input1, input2) \r\n            with lock:\r\n                results[input1] = output\r\n        except Exception as e:\r\n            with lock:\r\n                results[input1] = e\r\n\r\n    threads = [threading.Thread(target=_worker,\r\n                                args=(module, input1, input2, results, lock))\r\n                for module, input1, input2 in zip(modules, input1s, input2s)]"}