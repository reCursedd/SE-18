{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396044900", "html_url": "https://github.com/pytorch/pytorch/issues/8316#issuecomment-396044900", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8316", "id": 396044900, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjA0NDkwMA==", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-10T12:21:44Z", "updated_at": "2018-06-10T21:29:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It works for batches of tensors. Please look at the documentation for <a href=\"https://pytorch.org/docs/master/nn.html#cosineembeddingloss\" rel=\"nofollow\"><code>CosineEmbeddingLoss</code></a>.</p>\n<p>Example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x1 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x2 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y <span class=\"pl-k\">=</span> torch.empty(<span class=\"pl-c1\">3</span>).bernoulli_().mul_(<span class=\"pl-c1\">2</span>).sub_(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> l <span class=\"pl-k\">=</span> torch.nn.CosineEmbeddingLoss()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">def</span> <span class=\"pl-en\">ang</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>):\n<span class=\"pl-c1\">...</span>     <span class=\"pl-k\">return</span> torch.dot(a, b) <span class=\"pl-k\">/</span> (torch.norm(a) <span class=\"pl-k\">*</span> torch.norm(b))\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> ang(x1[<span class=\"pl-c1\">0</span>], x2[<span class=\"pl-c1\">0</span>])\ntensor(<span class=\"pl-c1\">1.00000e-02</span> <span class=\"pl-k\">*</span>\n       <span class=\"pl-c1\">9.8725</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> ang(x1[<span class=\"pl-c1\">1</span>], x2[<span class=\"pl-c1\">1</span>])\ntensor(<span class=\"pl-c1\">1.00000e-02</span> <span class=\"pl-k\">*</span>\n       <span class=\"pl-c1\">7.6634</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> ang(x1[<span class=\"pl-c1\">2</span>], x2[<span class=\"pl-c1\">2</span>])\ntensor(<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.4407</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> By definition: the loss value should be mean of (1 - ang(x1[0], x2[0])), ang(x1[1], x2[1]) and 0</span>\n<span class=\"pl-c1\">...</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Which is approx 0.326</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> l(x1, x2, y).item() <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints 0.325969</span></pre></div>", "body_text": "It works for batches of tensors. Please look at the documentation for CosineEmbeddingLoss.\nExample:\n>>> x1 = torch.randn(3, 4)\n>>> x2 = torch.randn(3, 4)\n>>> y = torch.empty(3).bernoulli_().mul_(2).sub_(1)\n>>> l = torch.nn.CosineEmbeddingLoss()\n>>> def ang(a, b):\n...     return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n...\n>>> ang(x1[0], x2[0])\ntensor(1.00000e-02 *\n       9.8725)\n>>> ang(x1[1], x2[1])\ntensor(1.00000e-02 *\n       7.6634)\n>>> ang(x1[2], x2[2])\ntensor(-0.4407)\n>>> # By definition: the loss value should be mean of (1 - ang(x1[0], x2[0])), ang(x1[1], x2[1]) and 0\n... # Which is approx 0.326\n>>> l(x1, x2, y).item() # prints 0.325969", "body": "It works for batches of tensors. Please look at the documentation for [`CosineEmbeddingLoss`](https://pytorch.org/docs/master/nn.html#cosineembeddingloss).\r\n\r\nExample:\r\n```python\r\n>>> x1 = torch.randn(3, 4)\r\n>>> x2 = torch.randn(3, 4)\r\n>>> y = torch.empty(3).bernoulli_().mul_(2).sub_(1)\r\n>>> l = torch.nn.CosineEmbeddingLoss()\r\n>>> def ang(a, b):\r\n...     return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\r\n...\r\n>>> ang(x1[0], x2[0])\r\ntensor(1.00000e-02 *\r\n       9.8725)\r\n>>> ang(x1[1], x2[1])\r\ntensor(1.00000e-02 *\r\n       7.6634)\r\n>>> ang(x1[2], x2[2])\r\ntensor(-0.4407)\r\n>>> # By definition: the loss value should be mean of (1 - ang(x1[0], x2[0])), ang(x1[1], x2[1]) and 0\r\n... # Which is approx 0.326\r\n>>> l(x1, x2, y).item() # prints 0.325969\r\n```"}