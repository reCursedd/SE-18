{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9718", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9718/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9718/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9718/events", "html_url": "https://github.com/pytorch/pytorch/pull/9718", "id": 343733307, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAzMjk1ODM0", "number": 9718, "title": "Switch interpreter to use IValue's primitive int/floats (#9584)", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-23T17:54:19Z", "updated_at": "2018-07-23T21:08:59Z", "closed_at": "2018-07-23T21:08:59Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9718", "html_url": "https://github.com/pytorch/pytorch/pull/9718", "diff_url": "https://github.com/pytorch/pytorch/pull/9718.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9718.patch"}, "body_html": "<p>Summary:<br>\nThis patch switches the interpreter to use IValue's primitive numbers rather than tensors for computing on integers and floats. In addition to preparing the interpreter for first-class support of other types, this cleans up the handling of primitive numbers, making it possible to just use the normal operator overloading dispatch to find the right implementation for numbers. As a result of this change, a lot of other functionality needed to be updated since it was the first time we use non-tensors in a lot of places in the code base.</p>\n<p>Notes:</p>\n<ul>\n<li>Fixes code_template.py so that multi-line strings are indented correctly when used on a standalone line</li>\n<li>Cast operators (<code>int(x)</code>) now are functional. Some tests have addition conversions to integers because<br>\nwe no longer allow implicit tensor -&gt; integer conversions following the same convention as in python</li>\n<li>prim::ListConstruct/createList has been added to the interpreter for creating lists and this has<br>\nreplaced aten::stack for integers lists</li>\n<li>gen_jit_dispatch.py has been refactored so that non-tensor types use operators on IValues to extract<br>\nthe primitives</li>\n<li>IValue gains a .to method that is the equivalent of tensor_as but for IValue instead of at::Tensor</li>\n<li><code>constant_as&lt;T&gt;</code> is switched over to using IValues's <code>.to&lt;T&gt;</code> method, to make conversion from constant-&gt;IValue-&gt;C++ type<br>\nmore consistent. This functionality combined with <code>toIValue(Value*)</code> replaces the <code>tensor_as</code> and <code>as_tensor</code> family of functions.</li>\n<li>conditional expressions (if, loop) and operators related to them are now computed on integers rather than tensors</li>\n<li>IValue gains constructors for constructing from at::Scalar and converting to it. However, IValue itself will always store<br>\nthe scalars as a double or int64.</li>\n<li>To align with python 3 syntax, TK_INT, TK_FLOAT, and TK_BOOL have been removed from the parser, and int/float/bool are just treated as special identifiers in the compiler,<br>\nalong with print. These are represented as special sugared values with a <code>call</code> method implemented. For int/float/bool this implements casting behavior.</li>\n<li>Dropped shared_from_this from Type/Module. They were not needed and they making debugging harder because they internally throw/catch exceptions.</li>\n<li>Shape propagation has been updated to support running nodes that include floating point primitive types, this required some refactoring of internal functions.</li>\n<li>TensorToNum and NumToTensor have actual implementations as operators now</li>\n<li>regster_prim_ops now contains implementations of math operators for float/int primitive types, and for mixed (prim &lt;+&gt; tensor) versions. This removes the need for special handling in compiler.cpp</li>\n<li>Primitive math is now entirely handled by letting the compiler choose the right overloads. This removes tons of special casing in the compiler.</li>\n<li>incorporates eellison's change to allow casting from return values. Due to the addition of primitive support, the code need slight modifications, so I just pre-merged it here.</li>\n<li>stack.h gains generic vararg versions of push/pop that know how to convert to/from C++ types:</li>\n</ul>\n<pre><code>at::Tensor a;\nat::Scalar b;\npop(stack, a, b);\nat::Tensor c = a + b;\npush(stack, c);\n</code></pre>\n<p>apaszke<br>\nPull Request <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes pull request #9584.\">resolved</span>: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"342591504\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9584\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9584/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9584\">#9584</a></p>\n<p>Differential Revision: D8910546</p>", "body_text": "Summary:\nThis patch switches the interpreter to use IValue's primitive numbers rather than tensors for computing on integers and floats. In addition to preparing the interpreter for first-class support of other types, this cleans up the handling of primitive numbers, making it possible to just use the normal operator overloading dispatch to find the right implementation for numbers. As a result of this change, a lot of other functionality needed to be updated since it was the first time we use non-tensors in a lot of places in the code base.\nNotes:\n\nFixes code_template.py so that multi-line strings are indented correctly when used on a standalone line\nCast operators (int(x)) now are functional. Some tests have addition conversions to integers because\nwe no longer allow implicit tensor -> integer conversions following the same convention as in python\nprim::ListConstruct/createList has been added to the interpreter for creating lists and this has\nreplaced aten::stack for integers lists\ngen_jit_dispatch.py has been refactored so that non-tensor types use operators on IValues to extract\nthe primitives\nIValue gains a .to method that is the equivalent of tensor_as but for IValue instead of at::Tensor\nconstant_as<T> is switched over to using IValues's .to<T> method, to make conversion from constant->IValue->C++ type\nmore consistent. This functionality combined with toIValue(Value*) replaces the tensor_as and as_tensor family of functions.\nconditional expressions (if, loop) and operators related to them are now computed on integers rather than tensors\nIValue gains constructors for constructing from at::Scalar and converting to it. However, IValue itself will always store\nthe scalars as a double or int64.\nTo align with python 3 syntax, TK_INT, TK_FLOAT, and TK_BOOL have been removed from the parser, and int/float/bool are just treated as special identifiers in the compiler,\nalong with print. These are represented as special sugared values with a call method implemented. For int/float/bool this implements casting behavior.\nDropped shared_from_this from Type/Module. They were not needed and they making debugging harder because they internally throw/catch exceptions.\nShape propagation has been updated to support running nodes that include floating point primitive types, this required some refactoring of internal functions.\nTensorToNum and NumToTensor have actual implementations as operators now\nregster_prim_ops now contains implementations of math operators for float/int primitive types, and for mixed (prim <+> tensor) versions. This removes the need for special handling in compiler.cpp\nPrimitive math is now entirely handled by letting the compiler choose the right overloads. This removes tons of special casing in the compiler.\nincorporates eellison's change to allow casting from return values. Due to the addition of primitive support, the code need slight modifications, so I just pre-merged it here.\nstack.h gains generic vararg versions of push/pop that know how to convert to/from C++ types:\n\nat::Tensor a;\nat::Scalar b;\npop(stack, a, b);\nat::Tensor c = a + b;\npush(stack, c);\n\napaszke\nPull Request resolved: #9584\nDifferential Revision: D8910546", "body": "Summary:\nThis patch switches the interpreter to use IValue's primitive numbers rather than tensors for computing on integers and floats. In addition to preparing the interpreter for first-class support of other types, this cleans up the handling of primitive numbers, making it possible to just use the normal operator overloading dispatch to find the right implementation for numbers. As a result of this change, a lot of other functionality needed to be updated since it was the first time we use non-tensors in a lot of places in the code base.\n\nNotes:\n* Fixes code_template.py so that multi-line strings are indented correctly when used on a standalone line\n* Cast operators (`int(x)`) now are functional. Some tests have addition conversions to integers because\nwe no longer allow implicit tensor -> integer conversions following the same convention as in python\n* prim::ListConstruct/createList has been added to the interpreter for creating lists and this has\nreplaced aten::stack for integers lists\n* gen_jit_dispatch.py has been refactored so that non-tensor types use operators on IValues to extract\nthe primitives\n* IValue gains a .to<T> method that is the equivalent of tensor_as but for IValue instead of at::Tensor\n* `constant_as<T>` is switched over to using IValues's `.to<T>` method, to make conversion from constant->IValue->C++ type\nmore consistent. This functionality combined with `toIValue(Value*)` replaces the `tensor_as` and `as_tensor` family of functions.\n* conditional expressions (if, loop) and operators related to them are now computed on integers rather than tensors\n* IValue gains constructors for constructing from at::Scalar and converting to it. However, IValue itself will always store\nthe scalars as a double or int64.\n* To align with python 3 syntax, TK_INT, TK_FLOAT, and TK_BOOL have been removed from the parser, and int/float/bool are just treated as special identifiers in the compiler,\nalong with print. These are represented as special sugared values with a `call` method implemented. For int/float/bool this implements casting behavior.\n* Dropped shared_from_this from Type/Module. They were not needed and they making debugging harder because they internally throw/catch exceptions.\n* Shape propagation has been updated to support running nodes that include floating point primitive types, this required some refactoring of internal functions.\n* TensorToNum and NumToTensor have actual implementations as operators now\n* regster_prim_ops now contains implementations of math operators for float/int primitive types, and for mixed (prim <+> tensor) versions. This removes the need for special handling in compiler.cpp\n* Primitive math is now entirely handled by letting the compiler choose the right overloads. This removes tons of special casing in the compiler.\n* incorporates eellison's change to allow casting from return values. Due to the addition of primitive support, the code need slight modifications, so I just pre-merged it here.\n* stack.h gains generic vararg versions of push/pop that know how to convert to/from C++ types:\n\n```\nat::Tensor a;\nat::Scalar b;\npop(stack, a, b);\nat::Tensor c = a + b;\npush(stack, c);\n```\napaszke\nPull Request resolved: https://github.com/pytorch/pytorch/pull/9584\n\nDifferential Revision: D8910546\n"}