{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/172590449", "pull_request_review_id": 101633050, "id": 172590449, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MjU5MDQ0OQ==", "diff_hunk": "@@ -0,0 +1,166 @@\n+r\"\"\"\n+torch.distributed.launch is a helper module that spawns up multiple distributed\n+training processes on each of the training nodes.\n+\n+The utility can be used for single-node distributed training, in which one or\n+more processes per node will be spawned. The utility can be used for either\n+CPU training or GPU training. If the utility is used for GPU training,\n+each distributed process will be operating on a single GPU. This can achieve\n+well-improved single-node training performance. It can also be used in\n+multi-node distributed training, by spawning up multiple processes on each node\n+for well-improved multi-node distributed training performance as well.\n+This will especially be benefitial for systems with multiple Infiniband\n+interfaces that have direct-GPU support, since all of them can be utilized for\n+aggregated communication bandwidth.\n+\n+In both cases of single-node distributed training or multi-node distributed\n+training, this utility will launch the given number of processes per node\n+(``--nproc_per_node``). If used for GPU training, this number needs to be less\n+or euqal to the number of GPUs on the current system (``nproc_per_node``),\n+and each process will be operating on a single GPU from *GPU 0 to\n+GPU (nproc_per_node - 1)*.\n+\n+**How to use this module:**\n+\n+1. Single-Node multi-process distributed training\n+\n+  ``python -m torch.distributed.launch YOUR_TRAINING_SCRIPT.py (--arg1\n+  --arg2 --arg3 and all other arguments of your training script)``\n+\n+2. Multi-Node multi-process distributed training: (e.g. two nodes)\n+\n+    **Node 1**: *(IP: 192.168.1.1, and has a free port: 1234)*\n+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)``\n+    **Node 2**:\n+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)``\n+\n+3. To look up what optional arguments this module offers:\n+\n+        >>> python -m torch.distributed.launch --help\n+\n+**Important Notices:**\n+\n+1. This utilty and multi-process distributed (single-node or\n+multi-node) GPU training currently only achieves the best performance using\n+the NCCL distributed backend. Thus NCCL backend is the recommended backend to\n+use for GPU training.\n+\n+2. In your training program, you are supposed to parse the command-line\n+argument: ``--device=DEVICE_TO_RUN``, (which will be provided with this module)\n+and ensure that your code only runs on this device in your training program by:\n+\n+\n+    >>> torch.cuda.set_device(arg.device)  # before your code runs\n+\n+    or\n+\n+    >>> with torch.cuda.device(arg.device):\n+    >>>    # your code to run\n+\n+3. In your training program, you are supposed to call the following function\n+at the beginning to start the distributed backend. You need to make sure that\n+the init_method uses ``env://``, which is the only supported ``init_method``\n+by this module:\n+\n+    ``torch.distributed.init_process_group(backend='YOUR BACKEND',\n+    init_method='env://')``\n+\n+4. In your training program, you can either use regular distributed functions\n+or use DistributedDataParallel module. If you would like to use\n+DistributedDataParallel module, here is how to configure it.\n+\n+    ``model = torch.nn.parallel.DistributedDataParallel(model,\n+    device_ids=[arg.device], output_device=arg.device)``\n+\n+Please ensure that ``device_ids`` argument is set to be the only GPU device id\n+that your code will be operating on. In other words, the ``device_ids`` needs\n+to be ``[args.device]``, and ``output_device`` needs to be ``args.devices`` in\n+order to use this utility\n+\n+\"\"\"\n+\n+\n+import subprocess\n+import os\n+import socket\n+from argparse import ArgumentParser, REMAINDER\n+\n+import torch\n+\n+\n+def parse_args():\n+    \"\"\"\n+    Helper function parsing the command line options\n+    @retval ArgumentParser\n+    \"\"\"\n+    parser = ArgumentParser(description=\"PyTorch distributed training launch \"\n+                                        \"helper utilty that will spawn up \"\n+                                        \"multiple distributed processes\")\n+\n+    # Optional arguments for the launch helper\n+    parser.add_argument(\"--nnodes\", type=int, default=1,\n+                        help=\"The number of nodes to use for distributed \"\n+                             \"training\")\n+    parser.add_argument(\"--node_rank\", type=int, default=0,\n+                        help=\"The rank of the node for multi-node distributed \"\n+                             \"training\")\n+    parser.add_argument(\"--nproc_per_node\", type=int, default=1,\n+                        help=\"The number of processes to launch on each node, \"", "path": "torch/distributed/launch.py", "position": 126, "original_position": 114, "commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "original_commit_id": "5179d0a07a41fc23dc9d7c79b3470683c590066b", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "I don't see nproc_per_node being reset to the number of GPUs on the system?", "created_at": "2018-03-06T17:09:15Z", "updated_at": "2018-11-23T15:40:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/5348#discussion_r172590449", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5348", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/172590449"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5348#discussion_r172590449"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5348"}}, "body_html": "<p>I don't see nproc_per_node being reset to the number of GPUs on the system?</p>", "body_text": "I don't see nproc_per_node being reset to the number of GPUs on the system?"}