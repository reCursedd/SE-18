{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170906302", "pull_request_review_id": 99662380, "id": 170906302, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDkwNjMwMg==", "diff_hunk": "@@ -0,0 +1,172 @@\n+\"\"\"\n+torch.distributed.launch is a helper module that spawns up multiple distributed\n+training processes on each of the training nodes.\n+\n+The utility can be used for single-node distributed training, in which\n+each distributed process will be operating on a single GPU. This can achieve\n+well-improved single-node training performance. It can also be used in\n+multi-node distributed training, by spawning up multiple processes on each node\n+for well-improved multi-node distributed training performance as well.\n+This will especially be benefitial for systems with multiple Infiniband\n+interfaces since all of them can be utilized for aggregated communication\n+bandwidth.\n+\n+In both cases, this utilily will launch a given number of processes\n+per node (nproc_per_node, which defaults to the number of GPUs on the node).\n+This number needs to be less or euqal to the number of GPUs on the current\n+system, and each process will be operating on a single GPU from GPU 0 to\n+GPU nproc_per_node - 1.\n+\n+How to use this module:\n+\n+    (1) Single Node multi-proc distributed training:\n+        python -m torch.distributed.launch YOUR_TRAINING_SCRIPT.py (--arg1\n+        --arg2 --arg3 and all other arguments of your training script)\n+\n+    (2) Multi Node multi-proc distributed training: (e.g. two nodes)\n+\n+    NODE1: (IP: 192.168.1.1, and has a free port: 1234)\n+        python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --num_node=2 --rank_node=0 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)\n+    NODE2:\n+        python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --num_node=2 --rank_node=1 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)\n+\n+    (3) To look up what optional arguments this module offers:\n+        python -m torch.distributed.launch --help\n+\n+Important Notices:\n+\n+(1) This utilty and multi-process distributed (single node or\n+multi-node) GPU training currently only achieves the best performance using\n+the NCCL distributed backend. Thus NCCL backend is the recommended backend to\n+use for GPU training.\n+\n+(2) In your training program, you are supposed to parse the command-line\n+argument: --device=DEVICE_TO_RUN, (which will be provided with this module) and\n+ensure that your code only runs on this device in your training program by:\n+\n+    torch.cuda.set_device(arg.device)  # before your code runs\n+\n+    or\n+\n+    with torch.cuda.device(arg.device):\n+        # your code to run\n+\n+(3) In your training program, you are supposed to call the following function\n+at the beginning to start the distributed backend. You need to make sure that\n+the init_method uses \"env://\", which is the only supported init_method by this\n+module:\n+\n+    torch.distributed.init_process_group(backend='YOUR BACKEND',\n+                                         \"init_method='env://')\n+\n+(4) In your training program, you are supposed to convert your model to\n+DistributedDataParallel module using the following function. Please ensure\n+that device_ids argument is set to be the only GPU device id that your code\n+will be operating on. In other words, the device_ids needs to be [args.device]\n+in order to use this utility.\n+\n+    model = torch.nn.parallel.DistributedDataParallel(model,\n+                                                      device_ids=[arg.device])\n+\n+(5) For multi-node training, current we only support nodes with identical number\n+of GPUs. In other words, the number of GPUs on each node needs to be the same.\n+\n+\"\"\"\n+\n+\n+import subprocess\n+import os\n+import socket\n+from argparse import ArgumentParser, REMAINDER\n+\n+import torch\n+\n+\n+def parse_args():\n+    \"\"\"\n+    Helper function parsing the command line options\n+    @retval ArgumentParser\n+    \"\"\"\n+    parser = ArgumentParser(description=\"PyTorch distributed training launch \"\n+                                        \"helper utilty that will spawn up \"\n+                                        \"multiple distributed processes\")\n+\n+    # Optional arguments for the launch helper\n+    parser.add_argument(\"--num_node\", type=int, default=1,\n+                        help=\"The number of nodes to use for distributed \"\n+                             \"training\")\n+    parser.add_argument(\"--rank_node\", type=int, default=0,", "path": "torch/distributed/launch.py", "position": null, "original_position": 104, "commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "original_commit_id": "eaabab4e617d9890c90bc6f2b170729efb890507", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can you please change this to `--node_rank`?", "created_at": "2018-02-27T12:30:33Z", "updated_at": "2018-11-23T15:40:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/5348#discussion_r170906302", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5348", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170906302"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5348#discussion_r170906302"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5348"}}, "body_html": "<p>Can you please change this to <code>--node_rank</code>?</p>", "body_text": "Can you please change this to --node_rank?"}