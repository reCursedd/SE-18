{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170904993", "pull_request_review_id": 99662380, "id": 170904993, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDkwNDk5Mw==", "diff_hunk": "@@ -0,0 +1,172 @@\n+\"\"\"\n+torch.distributed.launch is a helper module that spawns up multiple distributed\n+training processes on each of the training nodes.\n+\n+The utility can be used for single-node distributed training, in which\n+each distributed process will be operating on a single GPU. This can achieve\n+well-improved single-node training performance. It can also be used in\n+multi-node distributed training, by spawning up multiple processes on each node\n+for well-improved multi-node distributed training performance as well.\n+This will especially be benefitial for systems with multiple Infiniband\n+interfaces since all of them can be utilized for aggregated communication\n+bandwidth.\n+\n+In both cases, this utilily will launch a given number of processes\n+per node (nproc_per_node, which defaults to the number of GPUs on the node).\n+This number needs to be less or euqal to the number of GPUs on the current\n+system, and each process will be operating on a single GPU from GPU 0 to\n+GPU nproc_per_node - 1.\n+\n+How to use this module:\n+\n+    (1) Single Node multi-proc distributed training:\n+        python -m torch.distributed.launch YOUR_TRAINING_SCRIPT.py (--arg1\n+        --arg2 --arg3 and all other arguments of your training script)\n+\n+    (2) Multi Node multi-proc distributed training: (e.g. two nodes)\n+\n+    NODE1: (IP: 192.168.1.1, and has a free port: 1234)\n+        python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --num_node=2 --rank_node=0 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)\n+    NODE2:\n+        python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --num_node=2 --rank_node=1 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)\n+\n+    (3) To look up what optional arguments this module offers:\n+        python -m torch.distributed.launch --help\n+\n+Important Notices:\n+\n+(1) This utilty and multi-process distributed (single node or\n+multi-node) GPU training currently only achieves the best performance using\n+the NCCL distributed backend. Thus NCCL backend is the recommended backend to\n+use for GPU training.\n+\n+(2) In your training program, you are supposed to parse the command-line\n+argument: --device=DEVICE_TO_RUN, (which will be provided with this module) and\n+ensure that your code only runs on this device in your training program by:\n+\n+    torch.cuda.set_device(arg.device)  # before your code runs\n+\n+    or\n+\n+    with torch.cuda.device(arg.device):\n+        # your code to run\n+\n+(3) In your training program, you are supposed to call the following function\n+at the beginning to start the distributed backend. You need to make sure that\n+the init_method uses \"env://\", which is the only supported init_method by this\n+module:\n+\n+    torch.distributed.init_process_group(backend='YOUR BACKEND',\n+                                         \"init_method='env://')", "path": "torch/distributed/launch.py", "position": null, "original_position": 66, "commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "original_commit_id": "eaabab4e617d9890c90bc6f2b170729efb890507", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "you have an unnecessary `\"` before `init_method`", "created_at": "2018-02-27T12:25:08Z", "updated_at": "2018-11-23T15:40:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/5348#discussion_r170904993", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5348", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170904993"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5348#discussion_r170904993"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5348"}}, "body_html": "<p>you have an unnecessary <code>\"</code> before <code>init_method</code></p>", "body_text": "you have an unnecessary \" before init_method"}