{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205933348", "pull_request_review_id": 141307950, "id": 205933348, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTkzMzM0OA==", "diff_hunk": "@@ -0,0 +1,179 @@\n+r\"\"\"\n+torch.distributed.launch is a helper module that spawns up multiple distributed\n+training processes on each of the training nodes.\n+\n+The utility can be used for single-node distributed training, in which one or\n+more processes per node will be spawned. The utility can be used for either\n+CPU training or GPU training. If the utility is used for GPU training,\n+each distributed process will be operating on a single GPU. This can achieve\n+well-improved single-node training performance. It can also be used in\n+multi-node distributed training, by spawning up multiple processes on each node\n+for well-improved multi-node distributed training performance as well.\n+This will especially be benefitial for systems with multiple Infiniband\n+interfaces that have direct-GPU support, since all of them can be utilized for\n+aggregated communication bandwidth.\n+\n+In both cases of single-node distributed training or multi-node distributed\n+training, this utility will launch the given number of processes per node\n+(``--nproc_per_node``). If used for GPU training, this number needs to be less\n+or euqal to the number of GPUs on the current system (``nproc_per_node``),\n+and each process will be operating on a single GPU from *GPU 0 to\n+GPU (nproc_per_node - 1)*.\n+\n+**How to use this module:**\n+\n+1. Single-Node multi-process distributed training\n+\n+  ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+  YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of\n+  your training script)``\n+\n+2. Multi-Node multi-process distributed training: (e.g. two nodes)\n+\n+    **Node 1**: *(IP: 192.168.1.1, and has a free port: 1234)*\n+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)``\n+    **Node 2**:\n+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)``\n+\n+3. To look up what optional arguments this module offers:\n+\n+        >>> python -m torch.distributed.launch --help\n+\n+**Important Notices:**\n+\n+1. This utilty and multi-process distributed (single-node or\n+multi-node) GPU training currently only achieves the best performance using\n+the NCCL distributed backend. Thus NCCL backend is the recommended backend to\n+use for GPU training.", "path": "torch/distributed/launch.py", "position": 53, "original_position": 53, "commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "original_commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "user": {"login": "acgtyrant", "id": 3921062, "node_id": "MDQ6VXNlcjM5MjEwNjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3921062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acgtyrant", "html_url": "https://github.com/acgtyrant", "followers_url": "https://api.github.com/users/acgtyrant/followers", "following_url": "https://api.github.com/users/acgtyrant/following{/other_user}", "gists_url": "https://api.github.com/users/acgtyrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/acgtyrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acgtyrant/subscriptions", "organizations_url": "https://api.github.com/users/acgtyrant/orgs", "repos_url": "https://api.github.com/users/acgtyrant/repos", "events_url": "https://api.github.com/users/acgtyrant/events{/privacy}", "received_events_url": "https://api.github.com/users/acgtyrant/received_events", "type": "User", "site_admin": false}, "body": "I noticed that we can allocate a rank per node while DistributedDataParallel supports to use all GPU at the node immediately, is use NUM_GPUS_YOU_HAVE ranks per node better than the former?", "created_at": "2018-07-28T05:14:07Z", "updated_at": "2018-11-23T15:48:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/5348#discussion_r205933348", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5348", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205933348"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5348#discussion_r205933348"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5348"}}, "body_html": "<p>I noticed that we can allocate a rank per node while DistributedDataParallel supports to use all GPU at the node immediately, is use NUM_GPUS_YOU_HAVE ranks per node better than the former?</p>", "body_text": "I noticed that we can allocate a rank per node while DistributedDataParallel supports to use all GPU at the node immediately, is use NUM_GPUS_YOU_HAVE ranks per node better than the former?"}