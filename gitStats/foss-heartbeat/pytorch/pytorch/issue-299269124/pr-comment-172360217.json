{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/172360217", "pull_request_review_id": 101364139, "id": 172360217, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MjM2MDIxNw==", "diff_hunk": "@@ -0,0 +1,172 @@\n+\"\"\"\n+torch.distributed.launch is a helper module that spawns up multiple distributed\n+training processes on each of the training nodes.\n+\n+The utility can be used for single-node distributed training, in which\n+each distributed process will be operating on a single GPU. This can achieve\n+well-improved single-node training performance. It can also be used in\n+multi-node distributed training, by spawning up multiple processes on each node\n+for well-improved multi-node distributed training performance as well.\n+This will especially be benefitial for systems with multiple Infiniband\n+interfaces since all of them can be utilized for aggregated communication\n+bandwidth.\n+\n+In both cases, this utilily will launch a given number of processes\n+per node (nproc_per_node, which defaults to the number of GPUs on the node).\n+This number needs to be less or euqal to the number of GPUs on the current\n+system, and each process will be operating on a single GPU from GPU 0 to\n+GPU nproc_per_node - 1.\n+\n+How to use this module:\n+\n+    (1) Single Node multi-proc distributed training:\n+        python -m torch.distributed.launch YOUR_TRAINING_SCRIPT.py (--arg1\n+        --arg2 --arg3 and all other arguments of your training script)\n+\n+    (2) Multi Node multi-proc distributed training: (e.g. two nodes)\n+\n+    NODE1: (IP: 192.168.1.1, and has a free port: 1234)\n+        python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --num_node=2 --rank_node=0 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)\n+    NODE2:\n+        python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --num_node=2 --rank_node=1 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)", "path": "torch/distributed/launch.py", "position": null, "original_position": 37, "commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "original_commit_id": "eaabab4e617d9890c90bc6f2b170729efb890507", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "I think the requirement here is that whether or not you you use the argument, you should always use env:// init method. Otherwise, it's gonna require the user to provide these process ranks, master_addr, etc, which is kind of against the idea of this script, to make it simple", "created_at": "2018-03-05T23:07:47Z", "updated_at": "2018-11-23T15:40:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/5348#discussion_r172360217", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5348", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/172360217"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5348#discussion_r172360217"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5348"}}, "body_html": "<p>I think the requirement here is that whether or not you you use the argument, you should always use env:// init method. Otherwise, it's gonna require the user to provide these process ranks, master_addr, etc, which is kind of against the idea of this script, to make it simple</p>", "body_text": "I think the requirement here is that whether or not you you use the argument, you should always use env:// init method. Otherwise, it's gonna require the user to provide these process ranks, master_addr, etc, which is kind of against the idea of this script, to make it simple", "in_reply_to_id": 170904782}