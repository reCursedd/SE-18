{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/173403441", "pull_request_review_id": 102589418, "id": 173403441, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MzQwMzQ0MQ==", "diff_hunk": "@@ -0,0 +1,168 @@\n+r\"\"\"\n+torch.distributed.launch is a helper module that spawns up multiple distributed\n+training processes on each of the training nodes.\n+\n+The utility can be used for single-node distributed training, in which one or\n+more processes per node will be spawned. The utility can be used for either\n+CPU training or GPU training. If the utility is used for GPU training,\n+each distributed process will be operating on a single GPU. This can achieve\n+well-improved single-node training performance. It can also be used in\n+multi-node distributed training, by spawning up multiple processes on each node\n+for well-improved multi-node distributed training performance as well.\n+This will especially be benefitial for systems with multiple Infiniband\n+interfaces that have direct-GPU support, since all of them can be utilized for\n+aggregated communication bandwidth.\n+\n+In both cases of single-node distributed training or multi-node distributed\n+training, this utility will launch the given number of processes per node\n+(``--nproc_per_node``). If used for GPU training, this number needs to be less\n+or euqal to the number of GPUs on the current system (``nproc_per_node``),\n+and each process will be operating on a single GPU from *GPU 0 to\n+GPU (nproc_per_node - 1)*.\n+\n+**How to use this module:**\n+\n+1. Single-Node multi-process distributed training\n+\n+  ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+  YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of\n+  your training script)``\n+\n+2. Multi-Node multi-process distributed training: (e.g. two nodes)\n+\n+    **Node 1**: *(IP: 192.168.1.1, and has a free port: 1234)*\n+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)``\n+    **Node 2**:\n+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n+        --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and\n+        all other arguments of your training script)``\n+\n+3. To look up what optional arguments this module offers:\n+\n+        >>> python -m torch.distributed.launch --help\n+\n+**Important Notices:**\n+\n+1. This utilty and multi-process distributed (single-node or\n+multi-node) GPU training currently only achieves the best performance using\n+the NCCL distributed backend. Thus NCCL backend is the recommended backend to\n+use for GPU training.\n+\n+2. In your training program, you are supposed to parse the command-line\n+argument: ``--device=DEVICE_TO_RUN``, (which will be provided with this module)\n+and ensure that your code only runs on this device in your training program by:\n+\n+\n+    >>> torch.cuda.set_device(arg.device)  # before your code runs\n+\n+    or\n+\n+    >>> with torch.cuda.device(arg.device):\n+    >>>    # your code to run\n+\n+3. In your training program, you are supposed to call the following function\n+at the beginning to start the distributed backend. You need to make sure that\n+the init_method uses ``env://``, which is the only supported ``init_method``\n+by this module:\n+\n+    ``torch.distributed.init_process_group(backend='YOUR BACKEND',\n+    init_method='env://')``\n+\n+4. In your training program, you can either use regular distributed functions\n+or use DistributedDataParallel module. If you would like to use\n+DistributedDataParallel module, here is how to configure it.\n+\n+    ``model = torch.nn.parallel.DistributedDataParallel(model,\n+    device_ids=[arg.device], output_device=arg.device)``\n+\n+Please ensure that ``device_ids`` argument is set to be the only GPU device id\n+that your code will be operating on. In other words, the ``device_ids`` needs\n+to be ``[args.device]``, and ``output_device`` needs to be ``args.devices`` in\n+order to use this utility\n+\n+\"\"\"\n+\n+\n+import subprocess\n+import os\n+import socket\n+from argparse import ArgumentParser, REMAINDER\n+\n+import torch\n+\n+\n+def parse_args():\n+    \"\"\"\n+    Helper function parsing the command line options\n+    @retval ArgumentParser\n+    \"\"\"\n+    parser = ArgumentParser(description=\"PyTorch distributed training launch \"\n+                                        \"helper utilty that will spawn up \"\n+                                        \"multiple distributed processes\")\n+\n+    # Optional arguments for the launch helper\n+    parser.add_argument(\"--nnodes\", type=int, default=1,\n+                        help=\"The number of nodes to use for distributed \"\n+                             \"training\")\n+    parser.add_argument(\"--node_rank\", type=int, default=0,\n+                        help=\"The rank of the node for multi-node distributed \"\n+                             \"training\")\n+    parser.add_argument(\"--nproc_per_node\", type=int, default=1,\n+                        help=\"The number of processes to launch on each node, \"\n+                             \"for GPU training, this is recommended to be set\"\n+                             \"to the number of GPUs in your system so that \"\n+                             \"each process can be bound to a single GPU.\")\n+    parser.add_argument(\"--master_addr\", default=\"127.0.0.1\", type=str,\n+                        help=\"Master node (rank 0)'s address, should be either \"\n+                             \"the IP address or the hostname of node 0, for \"\n+                             \"single node multi-proc training, the \"\n+                             \"--master_addr can simply be 127.0.0.1\")\n+    parser.add_argument(\"--master_port\", default=29500, type=int,\n+                        help=\"Master node (rank 0)'s free port that needs to \"\n+                             \"be used for communciation during distributed \"\n+                             \"training\")\n+\n+    # positional\n+    parser.add_argument(\"training_script\", type=str,\n+                        help=\"The full path to the single GPU training \"\n+                             \"program/script to be launched in parallel, \"\n+                             \"followed by all the arguments for the \"\n+                             \"training script\")\n+\n+    # rest from the training program\n+    parser.add_argument('training_script_args', nargs=REMAINDER)\n+    return parser.parse_args()\n+\n+\n+args = parse_args()\n+\n+# world size in terms of number of processes\n+dist_world_size = args.nproc_per_node * args.nnodes\n+\n+# set PyTorch distributed related environmental variables\n+current_env = os.environ.copy()\n+current_env[\"MASTER_ADDR\"] = args.master_addr\n+current_env[\"MASTER_PORT\"] = str(args.master_port)\n+current_env[\"WORLD_SIZE\"] = str(dist_world_size)\n+\n+processes = []\n+\n+for local_rank in range(0, args.nproc_per_node):\n+    # each process's rank\n+    dist_rank = args.nproc_per_node * args.node_rank + local_rank\n+    current_env[\"RANK\"] = str(dist_rank)\n+\n+    # spawn the processes\n+    cmd = [\"python\",\n+           args.training_script,\n+           \"--device={}\".format(local_rank)] + args.training_script_args", "path": "torch/distributed/launch.py", "position": null, "original_position": 162, "commit_id": "03b219badb2cbdbe3bbfb6df7dddaf3d70f1088e", "original_commit_id": "fb5189a1eb3c60528a2f1d9742f66bfd517a3e21", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Since this is effectively a `local_rank` (which is more general than `device` - your script doesn't even need to support CUDA), I think it would be a better idea to change the name of the arg to `local_rank` and request that user scripts accept this and use it to infer device IDs if they use CUDA.", "created_at": "2018-03-09T09:38:07Z", "updated_at": "2018-11-23T15:40:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/5348#discussion_r173403441", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5348", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/173403441"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5348#discussion_r173403441"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5348"}}, "body_html": "<p>Since this is effectively a <code>local_rank</code> (which is more general than <code>device</code> - your script doesn't even need to support CUDA), I think it would be a better idea to change the name of the arg to <code>local_rank</code> and request that user scripts accept this and use it to infer device IDs if they use CUDA.</p>", "body_text": "Since this is effectively a local_rank (which is more general than device - your script doesn't even need to support CUDA), I think it would be a better idea to change the name of the arg to local_rank and request that user scripts accept this and use it to infer device IDs if they use CUDA."}