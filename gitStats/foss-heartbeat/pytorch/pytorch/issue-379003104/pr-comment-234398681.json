{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234398681", "pull_request_review_id": 176043416, "id": 234398681, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDM5ODY4MQ==", "diff_hunk": "@@ -61,61 +61,114 @@ static bool sizes_match_except(IntList s1, IntList s2, int64_t dim_except /* sho\n   return true;\n }\n \n+// Check to see if the shape of tensors is compatible\n+// for being concatenated along a given dimension.\n+static void check_cat_sparse_dims(Tensor const &t,\n+  int64_t pos /* used only for debug messages */,\n+  IntList sizes,\n+  int64_t dim,\n+  int64_t wrapped,\n+  int64_t sparse_dim,\n+  int64_t dense_dim) {\n+    AT_CHECK(t.is_sparse(),\n+            \"Concatenating dense tensor at position \", pos, \" with sparse tensor(s) not supported.\");\n+    AT_CHECK(sizes_match_except(sizes, t.sizes(), wrapped),\n+            \"Concatenating tensor at position \", pos, \" of sizes \", t.sizes(), \" with tensor of sizes \", sizes,\n+            \" along dimension \", dim, \" not supported.\");\n+    AT_CHECK(t.sparse_dim() == sparse_dim && t.dense_dim() == dense_dim,\n+            \"Tensor at position \", pos, \" has dimension: sparse \", t.sparse_dim(), \", dense \", t.dense_dim(),\n+            \". Concatenating with tensor of dimensions \", sparse_dim, \", \", dense_dim, \" not supported.\");\n+}\n+\n static Tensor cat_sparse(TensorList tensors, int64_t dim) {\n   std::vector<Tensor> indices;\n   std::vector<Tensor> values;\n   int64_t wrapped = maybe_wrap_dim(dim, tensors[0].dim());\n   int64_t sparse_dim = tensors[0].sparse_dim();\n   int64_t dense_dim = tensors[0].dense_dim();\n-  // TODO - Make catting along dense dimensions work.\n-  // it's possible to do so,\n-  // but it involves creating a brand new values object\n-  // for each nonzero index in each input tensor\n-  // E.g.: catting [[1,2],[0,0]] and [[0,0],[3,4]]\n-  // yields [[1,2,0,0],[0,0,3,4]]\n-  AT_CHECK(wrapped < sparse_dim,\n-           \"Concatenating or stacking tensors of sparse dim \", sparse_dim, \"along non-sparse dimension \", dim, \" not supported.\");\n   IntList sizes = tensors[0].sizes();\n-  for (size_t i = 0; i < tensors.size(); ++i) {\n-    auto const &t = tensors[i];\n-    AT_CHECK(t.is_sparse(),\n-             \"Concatenating dense tensor at position \", i, \" with sparse tensor(s) not supported.\");\n-    AT_CHECK(sizes_match_except(sizes, t.sizes(), wrapped),\n-             \"Concatenating tensor at position \", i, \" of sizes \", t.sizes(), \" with tensor of sizes \", sizes,\n-             \" along dimension \", dim, \" not supported.\");\n-    AT_CHECK(t.sparse_dim() == sparse_dim && t.dense_dim() == dense_dim,\n-             \"Tensor at position \", i, \" has dimension: sparse \", t.sparse_dim(), \", dense \", t.dense_dim(),\n-             \". Concatenating with tensor of dimensions \", sparse_dim, \", \", dense_dim, \" not supported.\");\n-    indices.push_back(t._indices());\n-    values.push_back(t._values());\n-  }\n-  Tensor idxs = native::cat(indices, 1);\n-  Tensor vals = native::cat(values, 0);\n-  \n-  // We now need to move the indices of each\n-  // input tensor up along `dim` by an appropriate amount.\n-  // E.g., if t1 has indices [[2,3,4],[5,6,7]], \n-  // and sizes [10, 7]\n-  // then torch.cat((t1,t1,t1),1) should have indices\n-  // [[2,3,4,2,3,4,2,3,4],[5,6,7,12,13,14,19,20,21]],\n-  // so we need to increase idxs[1][3:6] by 7 \n-  // and idxs[1][6:9] by 14.\n-  int64_t col = 0;\n-  int64_t cumulative_offset = 0;\n-  for (size_t i = 0; i < tensors.size(); ++i) {\n-    auto const &t = tensors[i];\n-    int64_t this_piece_size = t._nnz();\n-    // cumulative_offset is zero for the first piece, so\n-    // don't waste time doing this operation unless i > 0.\n-    if (i > 0) {\n-      idxs[wrapped].narrow(0, col, this_piece_size) += cumulative_offset;\n+  if (wrapped < sparse_dim) {\n+    IntList sizes = tensors[0].sizes();\n+    for (size_t i = 0; i < tensors.size(); ++i) {\n+      auto const &t = tensors[i];\n+      check_cat_sparse_dims(t, i, sizes, dim, wrapped, sparse_dim, dense_dim);\n+      indices.push_back(t._indices());\n+      values.push_back(t._values());\n+    }\n+    Tensor idxs = native::cat(indices, 1);", "path": "aten/src/ATen/native/TensorShape.cpp", "position": null, "original_position": 79, "commit_id": "62e1322616aa52583dd4c4d183031a5f8bf1f504", "original_commit_id": "acc06d4e16cd3d9207892931b6506d497ab85eeb", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "body": "why not using `at::cat()` ?", "created_at": "2018-11-17T06:32:26Z", "updated_at": "2018-11-23T15:55:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/13761#discussion_r234398681", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13761", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234398681"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13761#discussion_r234398681"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13761"}}, "body_html": "<p>why not using <code>at::cat()</code> ?</p>", "body_text": "why not using at::cat() ?"}