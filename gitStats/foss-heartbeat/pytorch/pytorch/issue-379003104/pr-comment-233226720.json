{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233226720", "pull_request_review_id": 174593033, "id": 233226720, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzIyNjcyMA==", "diff_hunk": "@@ -61,61 +61,111 @@ static bool sizes_match_except(IntList s1, IntList s2, int64_t dim_except /* sho\n   return true;\n }\n \n+static void check_cat_sparse_dims(Tensor const &t,\n+  int64_t pos /* used only for debug messages */,\n+  IntList sizes,\n+  int64_t dim,\n+  int64_t wrapped,\n+  int64_t sparse_dim,\n+  int64_t dense_dim) {\n+    AT_CHECK(t.is_sparse(),\n+            \"Concatenating dense tensor at position \", pos, \" with sparse tensor(s) not supported.\");\n+    AT_CHECK(sizes_match_except(sizes, t.sizes(), wrapped),\n+            \"Concatenating tensor at position \", pos, \" of sizes \", t.sizes(), \" with tensor of sizes \", sizes,\n+            \" along dimension \", dim, \" not supported.\");\n+    AT_CHECK(t.sparse_dim() == sparse_dim && t.dense_dim() == dense_dim,\n+            \"Tensor at position \", pos, \" has dimension: sparse \", t.sparse_dim(), \", dense \", t.dense_dim(),\n+            \". Concatenating with tensor of dimensions \", sparse_dim, \", \", dense_dim, \" not supported.\");\n+}\n+\n static Tensor cat_sparse(TensorList tensors, int64_t dim) {\n   std::vector<Tensor> indices;\n   std::vector<Tensor> values;\n   int64_t wrapped = maybe_wrap_dim(dim, tensors[0].dim());\n   int64_t sparse_dim = tensors[0].sparse_dim();\n   int64_t dense_dim = tensors[0].dense_dim();\n-  // TODO - Make catting along dense dimensions work.\n-  // it's possible to do so,\n-  // but it involves creating a brand new values object\n-  // for each nonzero index in each input tensor\n-  // E.g.: catting [[1,2],[0,0]] and [[0,0],[3,4]]\n-  // yields [[1,2,0,0],[0,0,3,4]]\n-  AT_CHECK(wrapped < sparse_dim,\n-           \"Concatenating or stacking tensors of sparse dim \", sparse_dim, \"along non-sparse dimension \", dim, \" not supported.\");\n   IntList sizes = tensors[0].sizes();\n-  for (size_t i = 0; i < tensors.size(); ++i) {\n-    auto const &t = tensors[i];\n-    AT_CHECK(t.is_sparse(),\n-             \"Concatenating dense tensor at position \", i, \" with sparse tensor(s) not supported.\");\n-    AT_CHECK(sizes_match_except(sizes, t.sizes(), wrapped),\n-             \"Concatenating tensor at position \", i, \" of sizes \", t.sizes(), \" with tensor of sizes \", sizes,\n-             \" along dimension \", dim, \" not supported.\");\n-    AT_CHECK(t.sparse_dim() == sparse_dim && t.dense_dim() == dense_dim,\n-             \"Tensor at position \", i, \" has dimension: sparse \", t.sparse_dim(), \", dense \", t.dense_dim(),\n-             \". Concatenating with tensor of dimensions \", sparse_dim, \", \", dense_dim, \" not supported.\");\n-    indices.push_back(t._indices());\n-    values.push_back(t._values());\n-  }\n-  Tensor idxs = native::cat(indices, 1);\n-  Tensor vals = native::cat(values, 0);\n-  \n-  // We now need to move the indices of each\n-  // input tensor up along `dim` by an appropriate amount.\n-  // E.g., if t1 has indices [[2,3,4],[5,6,7]], \n-  // and sizes [10, 7]\n-  // then torch.cat((t1,t1,t1),1) should have indices\n-  // [[2,3,4,2,3,4,2,3,4],[5,6,7,12,13,14,19,20,21]],\n-  // so we need to increase idxs[1][3:6] by 7 \n-  // and idxs[1][6:9] by 14.\n-  int64_t col = 0;\n-  int64_t cumulative_offset = 0;\n-  for (size_t i = 0; i < tensors.size(); ++i) {\n-    auto const &t = tensors[i];\n-    int64_t this_piece_size = t._nnz();\n-    // cumulative_offset is zero for the first piece, so\n-    // don't waste time doing this operation unless i > 0.\n-    if (i > 0) {\n-      idxs[wrapped].narrow(0, col, this_piece_size) += cumulative_offset;\n+  if (wrapped < sparse_dim) {\n+    IntList sizes = tensors[0].sizes();\n+    for (size_t i = 0; i < tensors.size(); ++i) {\n+      auto const &t = tensors[i];\n+      check_cat_sparse_dims(t, i, sizes, dim, wrapped, sparse_dim, dense_dim);\n+      indices.push_back(t._indices());\n+      values.push_back(t._values());\n+    }\n+    Tensor idxs = native::cat(indices, 1);\n+    Tensor vals = native::cat(values, 0);\n+\n+    // We now need to move the indices of each\n+    // input tensor up along `dim` by an appropriate amount.\n+    // E.g., if t1 has indices [[2,3,4],[5,6,7]],\n+    // and sizes [10, 7]\n+    // then torch.cat((t1,t1,t1),1) should have indices\n+    // [[2,3,4,2,3,4,2,3,4],[5,6,7,12,13,14,19,20,21]],\n+    // so we need to increase idxs[1][3:6] by 7\n+    // and idxs[1][6:9] by 14.\n+    int64_t col = 0;\n+    int64_t cumulative_offset = 0;\n+    for (size_t i = 0; i < tensors.size(); ++i) {\n+      auto const &t = tensors[i];\n+      int64_t this_piece_size = t._nnz();\n+      // cumulative_offset is zero for the first piece, so\n+      // don't waste time doing this operation unless i > 0.\n+      if (i > 0) {\n+        idxs[wrapped].narrow(0, col, this_piece_size) += cumulative_offset;\n+      }\n+      cumulative_offset += t.size(wrapped);\n+      col += this_piece_size;\n+    }\n+    auto sizes_copy = sizes.vec();\n+    sizes_copy[wrapped] = cumulative_offset;\n+    return native::sparse_coo_tensor(idxs, vals, sizes_copy, tensors[0].options());\n+  }\n+  else {\n+    // Catting along a dense dimension requires us to create new values.\n+    // For illustration, consider the sparse 3d tensors t1 and t2,\n+    // given by t1 = [[[1,2],[3,4]], ... (zeros) ..., [[5,6],[7,8]]]\n+    // and t2 = [... (zeros) ..., [[9, 10], [11,12]], ... (zeros) ...],\n+    // Their concatenation along dimension 2 is:\n+    // [[[1,2,0,0],[3,4,0,0]], ... (zeros) ..., [[0,0,9,10],[0,0,11,12]], ... (zeros) ..., [[5,6,0,0],[7,8,0,0]]]\n+    //\n+    // Their values tensors are, respectively,\n+    // [[[1,2],[3,4]],[[5,6],[7,8]]] and [[[9,10],[11,12]]].\n+    //\n+    // and so the values tensor of their concatenation along dim 2 will be:\n+    // [[[1,2,0,0],[3,4,0,0]],[[5,6,0,0],[7,8,0,0]],[[0,0,9,10],[0,0,11,12]]]\n+    //\n+    // which we can get by taking the values tensor of each tensor, catting it with zeros of the appropriate size on the left and right,\n+    // and then catting all those results together.\n+\n+    // The dimension in each tensor's values object that corresponds to the overall dimension along which we're catting.\n+    int64_t values_dim = wrapped - sparse_dim + 1;\n+    // The final size along the catted dimension.\n+    int64_t total_size = std::accumulate(tensors.begin(), tensors.end(), 0, [values_dim](int64_t l, Tensor const &r) {\n+      return l + r._values().size(values_dim);\n+    });\n+    auto zeros_sizes = tensors[0]._values().sizes().vec();\n+    int64_t cumulative_size = 0;\n+    std::vector<Tensor> vals_pieces;\n+    std::vector<Tensor> idxs_pieces;\n+    for (size_t i = 0; i < tensors.size(); ++i) {\n+      auto const &t = tensors[i];\n+      check_cat_sparse_dims(t, i, sizes, dim, wrapped, sparse_dim, dense_dim);\n+      // dimension 0 of values corresponds to the number of values,\n+      // rather than to any logical dimension of the sparse tensor.\n+      zeros_sizes[0] = t._values().size(0);\n+      zeros_sizes[values_dim] = cumulative_size;\n+      cumulative_size += t._values().size(values_dim);\n+      auto z1 = native::zeros(zeros_sizes, t._values().options());\n+      zeros_sizes[values_dim] = total_size - cumulative_size;\n+      auto z2 = native::zeros(zeros_sizes, t._values().options());\n+      vals_pieces.push_back(native::cat({z1, t._values(), z2}, values_dim));\n+      idxs_pieces.push_back(t._indices());\n     }\n-    cumulative_offset += t.size(wrapped);\n-    col += this_piece_size;\n+    auto sizes_copy = sizes.vec();\n+    sizes_copy[wrapped] = total_size;\n+    return native::sparse_coo_tensor(native::cat(idxs_pieces, 1), native::cat(vals_pieces), sizes_copy, tensors[0].options());", "path": "aten/src/ATen/native/TensorShape.cpp", "position": 151, "original_position": 150, "commit_id": "62e1322616aa52583dd4c4d183031a5f8bf1f504", "original_commit_id": "a1b3a0850936566af154f60fcad36eb2189926ea", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "body": "does this mean catting a dense dim results to a uncoalesced sparse tensor? Should we add a note to that?", "created_at": "2018-11-13T21:23:27Z", "updated_at": "2018-11-23T15:54:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/13761#discussion_r233226720", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13761", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233226720"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13761#discussion_r233226720"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13761"}}, "body_html": "<p>does this mean catting a dense dim results to a uncoalesced sparse tensor? Should we add a note to that?</p>", "body_text": "does this mean catting a dense dim results to a uncoalesced sparse tensor? Should we add a note to that?"}