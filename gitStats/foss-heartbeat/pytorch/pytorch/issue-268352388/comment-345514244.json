{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/345514244", "html_url": "https://github.com/pytorch/pytorch/issues/3281#issuecomment-345514244", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3281", "id": 345514244, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTUxNDI0NA==", "user": {"login": "Arseny-N", "id": 3064867, "node_id": "MDQ6VXNlcjMwNjQ4Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3064867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Arseny-N", "html_url": "https://github.com/Arseny-N", "followers_url": "https://api.github.com/users/Arseny-N/followers", "following_url": "https://api.github.com/users/Arseny-N/following{/other_user}", "gists_url": "https://api.github.com/users/Arseny-N/gists{/gist_id}", "starred_url": "https://api.github.com/users/Arseny-N/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Arseny-N/subscriptions", "organizations_url": "https://api.github.com/users/Arseny-N/orgs", "repos_url": "https://api.github.com/users/Arseny-N/repos", "events_url": "https://api.github.com/users/Arseny-N/events{/privacy}", "received_events_url": "https://api.github.com/users/Arseny-N/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-19T12:52:12Z", "updated_at": "2017-11-19T12:52:12Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2560662\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinraison\">@martinraison</a> I've wrote the patch but fail to see how <code>DistributedDataParallel/DataParallel</code> could be affected by the changes in pin_memory. Maybe we both misunderstood each other -- my intention was to allow to write something like this</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">collate</span>(<span class=\"pl-smi\">batch</span>):\n  data, labels, dataset_ids <span class=\"pl-k\">=</span> <span class=\"pl-c1\">zip</span>(<span class=\"pl-k\">*</span>batch)\n  <span class=\"pl-k\">return</span> Batch(<span class=\"pl-v\">data</span><span class=\"pl-k\">=</span>torch.cat(data), <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">ids</span><span class=\"pl-k\">=</span>dataset_ids)\n\nvalidate_ds <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">collate_fn</span><span class=\"pl-k\">=</span>collate)\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> validate_ds:\n  output <span class=\"pl-k\">=</span> model(batch.data)\n  <span class=\"pl-c1\">...</span>\n  <span class=\"pl-c1\">print</span>(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">ids </span><span class=\"pl-c1\">{</span>batch.ids<span class=\"pl-c1\">}</span><span class=\"pl-s\"> Predicted </span><span class=\"pl-c1\">{</span>output<span class=\"pl-c1\">}</span><span class=\"pl-s\"> Labels </span><span class=\"pl-c1\">{</span>batch.labels<span class=\"pl-c1\">}</span><span class=\"pl-pds\">'</span>)\n  <span class=\"pl-c1\">...</span></pre></div>\n<hr>\n<p>Both  <code>DistributedDataParallel</code> and <code>DataParallel</code> operate on the <code>model</code> so the <code>namedtuple</code> returned by the <code>collate_fn</code> does not affect them, unless it is passed as an argument to the <code>model</code>. Am I overlooking something ?<br>\n<strong>PS</strong> Sorry it took so long, had some technical problems in setting up Pytorch in build mode.</p>", "body_text": "Hi @martinraison I've wrote the patch but fail to see how DistributedDataParallel/DataParallel could be affected by the changes in pin_memory. Maybe we both misunderstood each other -- my intention was to allow to write something like this\ndef collate(batch):\n  data, labels, dataset_ids = zip(*batch)\n  return Batch(data=torch.cat(data), labels=labels, ids=dataset_ids)\n\nvalidate_ds = DataLoader(dataset, collate_fn=collate)\n...\nfor batch in validate_ds:\n  output = model(batch.data)\n  ...\n  print(f'ids {batch.ids} Predicted {output} Labels {batch.labels}')\n  ...\n\nBoth  DistributedDataParallel and DataParallel operate on the model so the namedtuple returned by the collate_fn does not affect them, unless it is passed as an argument to the model. Am I overlooking something ?\nPS Sorry it took so long, had some technical problems in setting up Pytorch in build mode.", "body": "Hi @martinraison I've wrote the patch but fail to see how `DistributedDataParallel/DataParallel` could be affected by the changes in pin_memory. Maybe we both misunderstood each other -- my intention was to allow to write something like this\r\n```python\r\ndef collate(batch):\r\n  data, labels, dataset_ids = zip(*batch)\r\n  return Batch(data=torch.cat(data), labels=labels, ids=dataset_ids)\r\n\r\nvalidate_ds = DataLoader(dataset, collate_fn=collate)\r\n...\r\nfor batch in validate_ds:\r\n  output = model(batch.data)\r\n  ...\r\n  print(f'ids {batch.ids} Predicted {output} Labels {batch.labels}')\r\n  ...\r\n```\r\n-------\r\nBoth  `DistributedDataParallel` and `DataParallel` operate on the `model` so the `namedtuple` returned by the `collate_fn` does not affect them, unless it is passed as an argument to the `model`. Am I overlooking something ?\r\n**PS** Sorry it took so long, had some technical problems in setting up Pytorch in build mode."}