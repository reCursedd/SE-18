{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348975620", "html_url": "https://github.com/pytorch/pytorch/issues/3281#issuecomment-348975620", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3281", "id": 348975620, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODk3NTYyMA==", "user": {"login": "sergiogiro", "id": 4258800, "node_id": "MDQ6VXNlcjQyNTg4MDA=", "avatar_url": "https://avatars1.githubusercontent.com/u/4258800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sergiogiro", "html_url": "https://github.com/sergiogiro", "followers_url": "https://api.github.com/users/sergiogiro/followers", "following_url": "https://api.github.com/users/sergiogiro/following{/other_user}", "gists_url": "https://api.github.com/users/sergiogiro/gists{/gist_id}", "starred_url": "https://api.github.com/users/sergiogiro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sergiogiro/subscriptions", "organizations_url": "https://api.github.com/users/sergiogiro/orgs", "repos_url": "https://api.github.com/users/sergiogiro/repos", "events_url": "https://api.github.com/users/sergiogiro/events{/privacy}", "received_events_url": "https://api.github.com/users/sergiogiro/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-04T14:22:06Z", "updated_at": "2017-12-04T14:22:06Z", "author_association": "NONE", "body_html": "<p>Hi, I faced a related issue so I'll try to discuss here before raising a new issue (as I think that there's a solution that might fix both issues).</p>\n<p>Pinning memory with the current implementation might cause an unnecessary slowdown in performance. The root cause is that, when the dataloader pins a batch into memory, it calls the pin_memory_batch function, that iterates through the batch structure searching for tensors \"as possible\" (as long as elements are sequences or mappings). This search can create a lot of tight loops, in my case with penalties in performance of around 2x slower. In my particular case, I'm loading data from files and, in addition to the tensors, in the batch object I have lists of (non-numeric) data I use for post-processing. I don't think this scenario is very unusual. These lists contain one element for each value in the tensors, so it seems a lot of time is spent going through these massive lists and testing whether each value is a tensor or not. I can make the issue go away by wrapping the lists (or any data that is explored recursively) inside a holder, that is simply a class that is opaque to the search (ie, neither a sequence nor a mapping). The solution is not very elegant though.</p>\n<p>I was wondering if, in the same way that you can specify a collate function, we could implement an optional get_tensors_to_pin function, that given a batch would return, well, the elements that you want in pinned memory. This would work also for namedtuples (even if they're immutable, the tensor would be the same, it's just that .pin_memory() would be called on it). There'll be no need to \"reconstruct\" the batch object afterwards, as it wouldn't need to be deconstructed in the first place.</p>\n<p>This is my first \"contribution\" to pytorch. Do you think it's a feasible solution? Do you think it deserves a new issue of its own?</p>\n<p>Best,<br>\nSergio</p>", "body_text": "Hi, I faced a related issue so I'll try to discuss here before raising a new issue (as I think that there's a solution that might fix both issues).\nPinning memory with the current implementation might cause an unnecessary slowdown in performance. The root cause is that, when the dataloader pins a batch into memory, it calls the pin_memory_batch function, that iterates through the batch structure searching for tensors \"as possible\" (as long as elements are sequences or mappings). This search can create a lot of tight loops, in my case with penalties in performance of around 2x slower. In my particular case, I'm loading data from files and, in addition to the tensors, in the batch object I have lists of (non-numeric) data I use for post-processing. I don't think this scenario is very unusual. These lists contain one element for each value in the tensors, so it seems a lot of time is spent going through these massive lists and testing whether each value is a tensor or not. I can make the issue go away by wrapping the lists (or any data that is explored recursively) inside a holder, that is simply a class that is opaque to the search (ie, neither a sequence nor a mapping). The solution is not very elegant though.\nI was wondering if, in the same way that you can specify a collate function, we could implement an optional get_tensors_to_pin function, that given a batch would return, well, the elements that you want in pinned memory. This would work also for namedtuples (even if they're immutable, the tensor would be the same, it's just that .pin_memory() would be called on it). There'll be no need to \"reconstruct\" the batch object afterwards, as it wouldn't need to be deconstructed in the first place.\nThis is my first \"contribution\" to pytorch. Do you think it's a feasible solution? Do you think it deserves a new issue of its own?\nBest,\nSergio", "body": "Hi, I faced a related issue so I'll try to discuss here before raising a new issue (as I think that there's a solution that might fix both issues).\r\n\r\nPinning memory with the current implementation might cause an unnecessary slowdown in performance. The root cause is that, when the dataloader pins a batch into memory, it calls the pin_memory_batch function, that iterates through the batch structure searching for tensors \"as possible\" (as long as elements are sequences or mappings). This search can create a lot of tight loops, in my case with penalties in performance of around 2x slower. In my particular case, I'm loading data from files and, in addition to the tensors, in the batch object I have lists of (non-numeric) data I use for post-processing. I don't think this scenario is very unusual. These lists contain one element for each value in the tensors, so it seems a lot of time is spent going through these massive lists and testing whether each value is a tensor or not. I can make the issue go away by wrapping the lists (or any data that is explored recursively) inside a holder, that is simply a class that is opaque to the search (ie, neither a sequence nor a mapping). The solution is not very elegant though.\r\n\r\nI was wondering if, in the same way that you can specify a collate function, we could implement an optional get_tensors_to_pin function, that given a batch would return, well, the elements that you want in pinned memory. This would work also for namedtuples (even if they're immutable, the tensor would be the same, it's just that .pin_memory() would be called on it). There'll be no need to \"reconstruct\" the batch object afterwards, as it wouldn't need to be deconstructed in the first place.\r\n\r\nThis is my first \"contribution\" to pytorch. Do you think it's a feasible solution? Do you think it deserves a new issue of its own?\r\n\r\nBest,\r\nSergio"}