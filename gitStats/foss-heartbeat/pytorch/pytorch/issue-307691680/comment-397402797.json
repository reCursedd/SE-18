{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397402797", "html_url": "https://github.com/pytorch/pytorch/issues/5942#issuecomment-397402797", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5942", "id": 397402797, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzQwMjc5Nw==", "user": {"login": "bueche", "id": 40256397, "node_id": "MDQ6VXNlcjQwMjU2Mzk3", "avatar_url": "https://avatars3.githubusercontent.com/u/40256397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bueche", "html_url": "https://github.com/bueche", "followers_url": "https://api.github.com/users/bueche/followers", "following_url": "https://api.github.com/users/bueche/following{/other_user}", "gists_url": "https://api.github.com/users/bueche/gists{/gist_id}", "starred_url": "https://api.github.com/users/bueche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bueche/subscriptions", "organizations_url": "https://api.github.com/users/bueche/orgs", "repos_url": "https://api.github.com/users/bueche/repos", "events_url": "https://api.github.com/users/bueche/events{/privacy}", "received_events_url": "https://api.github.com/users/bueche/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T18:57:53Z", "updated_at": "2018-06-14T18:57:53Z", "author_association": "NONE", "body_html": "<p>Thanks. That did not solve the problem. I don't know if the original problem noted above is related to the test core dump, but for information the test failure happens here:</p>\n<pre><code>test_DoubleTensor_put_ (__main__.TestCuda) ... ok\ntest_DoubleTensor_put__accumulate (__main__.TestCuda) ... ok\ntest_DoubleTensor_put__empty (__main__.TestCuda) ... ok\ntest_DoubleTensor_qr_big (__main__.TestCuda) ... Segmentation fault (core dumped)\n\n</code></pre>\n<p>other tests fail as well:</p>\n<pre><code>Running test_autograd ...\n...........................................................................................s..................................................................................................................................................................................s.......................................................s........................................................................................................................................................................................................................................................................................................E...............................................................................................................................................................s............................\n======================================================================\nERROR: test_rnn_backward_to_input_but_not_parameters_cuda (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"test_autograd.py\", line 2301, in test_rnn_backward_to_input_but_not_parameters_cuda\n    l = torch.nn.LSTM(2, 3).to(dev)\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 370, in to\n    return self._apply(convert)\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 112, in _apply\n    self.flatten_parameters()\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 105, in flatten_parameters\n    self.batch_first, bool(self.bidirectional))\nRuntimeError: CUDNN_STATUS_NOT_INITIALIZED\n\n</code></pre>", "body_text": "Thanks. That did not solve the problem. I don't know if the original problem noted above is related to the test core dump, but for information the test failure happens here:\ntest_DoubleTensor_put_ (__main__.TestCuda) ... ok\ntest_DoubleTensor_put__accumulate (__main__.TestCuda) ... ok\ntest_DoubleTensor_put__empty (__main__.TestCuda) ... ok\ntest_DoubleTensor_qr_big (__main__.TestCuda) ... Segmentation fault (core dumped)\n\n\nother tests fail as well:\nRunning test_autograd ...\n...........................................................................................s..................................................................................................................................................................................s.......................................................s........................................................................................................................................................................................................................................................................................................E...............................................................................................................................................................s............................\n======================================================================\nERROR: test_rnn_backward_to_input_but_not_parameters_cuda (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"test_autograd.py\", line 2301, in test_rnn_backward_to_input_but_not_parameters_cuda\n    l = torch.nn.LSTM(2, 3).to(dev)\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 370, in to\n    return self._apply(convert)\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 112, in _apply\n    self.flatten_parameters()\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 105, in flatten_parameters\n    self.batch_first, bool(self.bidirectional))\nRuntimeError: CUDNN_STATUS_NOT_INITIALIZED", "body": "Thanks. That did not solve the problem. I don't know if the original problem noted above is related to the test core dump, but for information the test failure happens here:\r\n\r\n```\r\ntest_DoubleTensor_put_ (__main__.TestCuda) ... ok\r\ntest_DoubleTensor_put__accumulate (__main__.TestCuda) ... ok\r\ntest_DoubleTensor_put__empty (__main__.TestCuda) ... ok\r\ntest_DoubleTensor_qr_big (__main__.TestCuda) ... Segmentation fault (core dumped)\r\n\r\n```\r\nother tests fail as well:\r\n```\r\nRunning test_autograd ...\r\n...........................................................................................s..................................................................................................................................................................................s.......................................................s........................................................................................................................................................................................................................................................................................................E...............................................................................................................................................................s............................\r\n======================================================================\r\nERROR: test_rnn_backward_to_input_but_not_parameters_cuda (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_autograd.py\", line 2301, in test_rnn_backward_to_input_but_not_parameters_cuda\r\n    l = torch.nn.LSTM(2, 3).to(dev)\r\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 370, in to\r\n    return self._apply(convert)\r\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 112, in _apply\r\n    self.flatten_parameters()\r\n  File \"/home/bueche/miniconda2/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 105, in flatten_parameters\r\n    self.batch_first, bool(self.bidirectional))\r\nRuntimeError: CUDNN_STATUS_NOT_INITIALIZED\r\n\r\n```"}