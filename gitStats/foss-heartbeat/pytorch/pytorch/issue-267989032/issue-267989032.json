{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3256", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3256/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3256/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3256/events", "html_url": "https://github.com/pytorch/pytorch/issues/3256", "id": 267989032, "node_id": "MDU6SXNzdWUyNjc5ODkwMzI=", "number": 3256, "title": "A Multi-GPU LSTM script in 8 NVIDIA P100 card\uff0cwith 1 GPU is ok, but 8... stuck!", "user": {"login": "cobnut", "id": 16894322, "node_id": "MDQ6VXNlcjE2ODk0MzIy", "avatar_url": "https://avatars3.githubusercontent.com/u/16894322?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cobnut", "html_url": "https://github.com/cobnut", "followers_url": "https://api.github.com/users/cobnut/followers", "following_url": "https://api.github.com/users/cobnut/following{/other_user}", "gists_url": "https://api.github.com/users/cobnut/gists{/gist_id}", "starred_url": "https://api.github.com/users/cobnut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cobnut/subscriptions", "organizations_url": "https://api.github.com/users/cobnut/orgs", "repos_url": "https://api.github.com/users/cobnut/repos", "events_url": "https://api.github.com/users/cobnut/events{/privacy}", "received_events_url": "https://api.github.com/users/cobnut/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-24T11:05:04Z", "updated_at": "2018-03-09T07:19:09Z", "closed_at": "2018-03-09T07:19:09Z", "author_association": "NONE", "body_html": "<pre><code>class lstm(nn.Module):\n\t\n\tdef __init__(self,\n                     input_dim=None,\n\t\t     hidden_dim=None,\n\t\t     output_dim=None):\n\t\t\n            super(lstm, self).__init__()\n            self.input_dim = input_dim\n\t    self.hidden_dim = hidden_dim\n            self.output_dim = output_dim\n\t    self.initial_hidden = (None, None)\n            self.lstmcell = nn.LSTM(input_size=input_dim,\n\t\t\t\t    hidden_size=hidden_dim,\n\t\t\t\t    batch_first=True)\n\t    self.fc = nn.Linear(hidden_dim, output_dim)\n\n\tdef forward(self, x):\n\t    h0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim).cuda())\n\t    c0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim).cuda())\n\t    self.initial_hidden = (h0, c0)\n\t    output, _ = self.lstmcell(x, self.initial_hidden)\n\t    y = self.fc(output[:, -1, :])\n\t    return F.log_softmax(y)\n\n\ntrain_batch_size = 128\nvalid_batch_size = 1000\ntest_batch_size = 1000\n\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n\t\t\t      batch_size=train_batch_size,\n\t\t\t      shuffle=True,\n\t\t\t      num_workers=1,\n\t\t\t      drop_last=True)\n\n\nmodel = lstm(input_dim=28,\n\t     hidden_dim=128,\n             output_dim=28)\nmodel = nn.DataParallel(model)\n\n\nfor epoch_index in range(5):\n    model.train()\n    error_sum = 0.\n    for batch_index, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n\toptimizer.zero_grad()\n\toutputs = model(inputs)\n\terror = F.nll_loss(outputs, labels)\n\terror_sum += batch_average_error.data[0]\n\terror.backward()\n\toptimizer.step()\n</code></pre>", "body_text": "class lstm(nn.Module):\n\t\n\tdef __init__(self,\n                     input_dim=None,\n\t\t     hidden_dim=None,\n\t\t     output_dim=None):\n\t\t\n            super(lstm, self).__init__()\n            self.input_dim = input_dim\n\t    self.hidden_dim = hidden_dim\n            self.output_dim = output_dim\n\t    self.initial_hidden = (None, None)\n            self.lstmcell = nn.LSTM(input_size=input_dim,\n\t\t\t\t    hidden_size=hidden_dim,\n\t\t\t\t    batch_first=True)\n\t    self.fc = nn.Linear(hidden_dim, output_dim)\n\n\tdef forward(self, x):\n\t    h0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim).cuda())\n\t    c0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim).cuda())\n\t    self.initial_hidden = (h0, c0)\n\t    output, _ = self.lstmcell(x, self.initial_hidden)\n\t    y = self.fc(output[:, -1, :])\n\t    return F.log_softmax(y)\n\n\ntrain_batch_size = 128\nvalid_batch_size = 1000\ntest_batch_size = 1000\n\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n\t\t\t      batch_size=train_batch_size,\n\t\t\t      shuffle=True,\n\t\t\t      num_workers=1,\n\t\t\t      drop_last=True)\n\n\nmodel = lstm(input_dim=28,\n\t     hidden_dim=128,\n             output_dim=28)\nmodel = nn.DataParallel(model)\n\n\nfor epoch_index in range(5):\n    model.train()\n    error_sum = 0.\n    for batch_index, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n\toptimizer.zero_grad()\n\toutputs = model(inputs)\n\terror = F.nll_loss(outputs, labels)\n\terror_sum += batch_average_error.data[0]\n\terror.backward()\n\toptimizer.step()", "body": "```\r\nclass lstm(nn.Module):\r\n\t\r\n\tdef __init__(self,\r\n                     input_dim=None,\r\n\t\t     hidden_dim=None,\r\n\t\t     output_dim=None):\r\n\t\t\r\n            super(lstm, self).__init__()\r\n            self.input_dim = input_dim\r\n\t    self.hidden_dim = hidden_dim\r\n            self.output_dim = output_dim\r\n\t    self.initial_hidden = (None, None)\r\n            self.lstmcell = nn.LSTM(input_size=input_dim,\r\n\t\t\t\t    hidden_size=hidden_dim,\r\n\t\t\t\t    batch_first=True)\r\n\t    self.fc = nn.Linear(hidden_dim, output_dim)\r\n\r\n\tdef forward(self, x):\r\n\t    h0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim).cuda())\r\n\t    c0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim).cuda())\r\n\t    self.initial_hidden = (h0, c0)\r\n\t    output, _ = self.lstmcell(x, self.initial_hidden)\r\n\t    y = self.fc(output[:, -1, :])\r\n\t    return F.log_softmax(y)\r\n\r\n\r\ntrain_batch_size = 128\r\nvalid_batch_size = 1000\r\ntest_batch_size = 1000\r\n\r\n\r\ntrain_dataloader = DataLoader(dataset=train_dataset,\r\n\t\t\t      batch_size=train_batch_size,\r\n\t\t\t      shuffle=True,\r\n\t\t\t      num_workers=1,\r\n\t\t\t      drop_last=True)\r\n\r\n\r\nmodel = lstm(input_dim=28,\r\n\t     hidden_dim=128,\r\n             output_dim=28)\r\nmodel = nn.DataParallel(model)\r\n\r\n\r\nfor epoch_index in range(5):\r\n    model.train()\r\n    error_sum = 0.\r\n    for batch_index, (inputs, labels) in enumerate(train_dataloader):\r\n        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\r\n\toptimizer.zero_grad()\r\n\toutputs = model(inputs)\r\n\terror = F.nll_loss(outputs, labels)\r\n\terror_sum += batch_average_error.data[0]\r\n\terror.backward()\r\n\toptimizer.step()\r\n```"}