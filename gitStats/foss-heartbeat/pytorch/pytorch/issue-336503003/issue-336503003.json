{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8978", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8978/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8978/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8978/events", "html_url": "https://github.com/pytorch/pytorch/issues/8978", "id": 336503003, "node_id": "MDU6SXNzdWUzMzY1MDMwMDM=", "number": 8978, "title": "torch.jit.trace raises RuntimeError when torch.nn.functional.affine_grid is in the graph", "user": {"login": "nlgranger", "id": 3764009, "node_id": "MDQ6VXNlcjM3NjQwMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3764009?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nlgranger", "html_url": "https://github.com/nlgranger", "followers_url": "https://api.github.com/users/nlgranger/followers", "following_url": "https://api.github.com/users/nlgranger/following{/other_user}", "gists_url": "https://api.github.com/users/nlgranger/gists{/gist_id}", "starred_url": "https://api.github.com/users/nlgranger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nlgranger/subscriptions", "organizations_url": "https://api.github.com/users/nlgranger/orgs", "repos_url": "https://api.github.com/users/nlgranger/repos", "events_url": "https://api.github.com/users/nlgranger/events{/privacy}", "received_events_url": "https://api.github.com/users/nlgranger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-06-28T07:23:09Z", "updated_at": "2018-11-20T05:42:36Z", "closed_at": "2018-11-20T05:16:56Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p><code>torch.jit.trace</code> fails when <code>torch.nn.functional.affine_grid</code> exists in the graph, producing the following error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/granger/dev/retrynet/draft/scratch.py\", line 58, in &lt;module&gt;\n    torch.jit.trace(torch.rand(16, 1, 64, 64))(net)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/jit/__init__.py\", line 305, in wrapper\n    module._create_method_from_trace('forward', func, args)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/granger/dev/retrynet/draft/scratch.py\", line 42, in forward\n    grid = F.affine_grid(theta, x.size())\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/functional.py\", line 1921, in affine_grid\n    return vision.affine_grid_generator(theta, size)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/_functions/vision.py\", line 32, in affine_grid_generator\n    return AffineGridGenerator.apply(theta, size)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/_functions/vision.py\", line 115, in forward\n    grid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\nRuntimeError: torch/csrc/autograd/generated/VariableType.cpp:27570: transpose: Assertion `jit::tracer::ArgumentStash::empty()` failed.\n</code></pre>\n<p>I think to remember plotting a graph with a spatial transformer network in the past so it might be regression.</p>\n<h2>Code example</h2>\n<p>This sample code is very close to the pytorch turorial on Spatial Transformer Networks:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Localizer</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Localizer, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>)\n\n        nn.init.normal_(<span class=\"pl-c1\">self</span>.fc1.weight, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1e-5</span>)\n        <span class=\"pl-c1\">self</span>.fc2.bias.data.copy_(torch.tensor([<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">0</span>.]))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        x <span class=\"pl-k\">=</span> F.max_pool2d(x, <span class=\"pl-c1\">3</span>)\n        x <span class=\"pl-k\">=</span> F.relu(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2(x)\n        x <span class=\"pl-k\">=</span> F.max_pool2d(x, <span class=\"pl-c1\">2</span>)\n        x <span class=\"pl-k\">=</span> F.relu(x)\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">16</span>)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc1(x)\n        x <span class=\"pl-k\">=</span> F.relu(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc2(x)\n        <span class=\"pl-k\">return</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">STN</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">localizer</span>):\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">STN</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.localizer <span class=\"pl-k\">=</span> localizer\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        theta <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.localizer(x)\n        theta <span class=\"pl-k\">=</span> theta.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>)\n\n        grid <span class=\"pl-k\">=</span> F.affine_grid(theta, x.size())\n        x <span class=\"pl-k\">=</span> F.grid_sample(x, grid)\n\n        <span class=\"pl-k\">return</span> x\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> OK</span>\nnet <span class=\"pl-k\">=</span> Localizer()\ntorch.jit.trace(torch.rand(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>))(net)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> OK</span>\nnet <span class=\"pl-k\">=</span> STN(Localizer())\noutput <span class=\"pl-k\">=</span> net(torch.rand(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>))\n<span class=\"pl-c1\">print</span>(output.shape)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> KO</span>\ntorch.jit.trace(torch.rand(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>))(net)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.onnx.export(net, torch.rand(16, 1, 64, 64), \"mymodel\", export_params=True)</span></pre></div>\n<h2>System Info</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Arch Linux\nGCC version: (GCC) 8.1.1 20180531\nCMake version: version 3.11.4\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.5)\n[pip3] torch (0.4.0)\n[conda] Could not collect\n</code></pre>\n<p>The error was observed on <code>0.5.0a0+e62c3a4</code> as well.</p>", "body_text": "Issue description\ntorch.jit.trace fails when torch.nn.functional.affine_grid exists in the graph, producing the following error:\nTraceback (most recent call last):\n  File \"/home/granger/dev/retrynet/draft/scratch.py\", line 58, in <module>\n    torch.jit.trace(torch.rand(16, 1, 64, 64))(net)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/jit/__init__.py\", line 305, in wrapper\n    module._create_method_from_trace('forward', func, args)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/granger/dev/retrynet/draft/scratch.py\", line 42, in forward\n    grid = F.affine_grid(theta, x.size())\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/functional.py\", line 1921, in affine_grid\n    return vision.affine_grid_generator(theta, size)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/_functions/vision.py\", line 32, in affine_grid_generator\n    return AffineGridGenerator.apply(theta, size)\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/_functions/vision.py\", line 115, in forward\n    grid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\nRuntimeError: torch/csrc/autograd/generated/VariableType.cpp:27570: transpose: Assertion `jit::tracer::ArgumentStash::empty()` failed.\n\nI think to remember plotting a graph with a spatial transformer network in the past so it might be regression.\nCode example\nThis sample code is very close to the pytorch turorial on Spatial Transformer Networks:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Localizer(nn.Module):\n    def __init__(self):\n        super(Localizer, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, bias=False)\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, bias=False)\n        self.fc1 = nn.Linear(8 * 8 * 16, 32)\n        self.fc2 = nn.Linear(32, 2 * 3)\n\n        nn.init.normal_(self.fc1.weight, 0, 1e-5)\n        self.fc2.bias.data.copy_(torch.tensor([1., 0., 0., 0., 1., 0.]))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 3)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n        x = x.view(-1, 8 * 8 * 16)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x.view(-1, 2, 3)\n\n\nclass STN(nn.Module):\n    def __init__(self, localizer):\n        super(STN, self).__init__()\n\n        self.localizer = localizer\n\n    def forward(self, x):\n        theta = self.localizer(x)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n\n# OK\nnet = Localizer()\ntorch.jit.trace(torch.rand(16, 1, 64, 64))(net)\n\n# OK\nnet = STN(Localizer())\noutput = net(torch.rand(16, 1, 64, 64))\nprint(output.shape)\n\n# KO\ntorch.jit.trace(torch.rand(16, 1, 64, 64))(net)\n# torch.onnx.export(net, torch.rand(16, 1, 64, 64), \"mymodel\", export_params=True)\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Arch Linux\nGCC version: (GCC) 8.1.1 20180531\nCMake version: version 3.11.4\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.5)\n[pip3] torch (0.4.0)\n[conda] Could not collect\n\nThe error was observed on 0.5.0a0+e62c3a4 as well.", "body": "## Issue description\r\n\r\n`torch.jit.trace` fails when `torch.nn.functional.affine_grid` exists in the graph, producing the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/granger/dev/retrynet/draft/scratch.py\", line 58, in <module>\r\n    torch.jit.trace(torch.rand(16, 1, 64, 64))(net)\r\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/jit/__init__.py\", line 305, in wrapper\r\n    module._create_method_from_trace('forward', func, args)\r\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 468, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/granger/dev/retrynet/draft/scratch.py\", line 42, in forward\r\n    grid = F.affine_grid(theta, x.size())\r\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/functional.py\", line 1921, in affine_grid\r\n    return vision.affine_grid_generator(theta, size)\r\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/_functions/vision.py\", line 32, in affine_grid_generator\r\n    return AffineGridGenerator.apply(theta, size)\r\n  File \"/home/granger/dev/retrynet/venv/lib/python3.6/site-packages/torch/nn/_functions/vision.py\", line 115, in forward\r\n    grid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\r\nRuntimeError: torch/csrc/autograd/generated/VariableType.cpp:27570: transpose: Assertion `jit::tracer::ArgumentStash::empty()` failed.\r\n```\r\n\r\nI think to remember plotting a graph with a spatial transformer network in the past so it might be regression.\r\n\r\n## Code example\r\n\r\nThis sample code is very close to the pytorch turorial on Spatial Transformer Networks:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Localizer(nn.Module):\r\n    def __init__(self):\r\n        super(Localizer, self).__init__()\r\n\r\n        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, bias=False)\r\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, bias=False)\r\n        self.fc1 = nn.Linear(8 * 8 * 16, 32)\r\n        self.fc2 = nn.Linear(32, 2 * 3)\r\n\r\n        nn.init.normal_(self.fc1.weight, 0, 1e-5)\r\n        self.fc2.bias.data.copy_(torch.tensor([1., 0., 0., 0., 1., 0.]))\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = F.max_pool2d(x, 3)\r\n        x = F.relu(x)\r\n        x = self.conv2(x)\r\n        x = F.max_pool2d(x, 2)\r\n        x = F.relu(x)\r\n        x = x.view(-1, 8 * 8 * 16)\r\n        x = self.fc1(x)\r\n        x = F.relu(x)\r\n        x = self.fc2(x)\r\n        return x.view(-1, 2, 3)\r\n\r\n\r\nclass STN(nn.Module):\r\n    def __init__(self, localizer):\r\n        super(STN, self).__init__()\r\n\r\n        self.localizer = localizer\r\n\r\n    def forward(self, x):\r\n        theta = self.localizer(x)\r\n        theta = theta.view(-1, 2, 3)\r\n\r\n        grid = F.affine_grid(theta, x.size())\r\n        x = F.grid_sample(x, grid)\r\n\r\n        return x\r\n\r\n\r\n# OK\r\nnet = Localizer()\r\ntorch.jit.trace(torch.rand(16, 1, 64, 64))(net)\r\n\r\n# OK\r\nnet = STN(Localizer())\r\noutput = net(torch.rand(16, 1, 64, 64))\r\nprint(output.shape)\r\n\r\n# KO\r\ntorch.jit.trace(torch.rand(16, 1, 64, 64))(net)\r\n# torch.onnx.export(net, torch.rand(16, 1, 64, 64), \"mymodel\", export_params=True)\r\n```\r\n\r\n## System Info\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.1.1 20180531\r\nCMake version: version 3.11.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.5)\r\n[pip3] torch (0.4.0)\r\n[conda] Could not collect\r\n```\r\n\r\nThe error was observed on `0.5.0a0+e62c3a4` as well."}