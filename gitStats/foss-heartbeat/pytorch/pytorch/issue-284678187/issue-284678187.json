{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4361", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4361/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4361/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4361/events", "html_url": "https://github.com/pytorch/pytorch/issues/4361", "id": 284678187, "node_id": "MDU6SXNzdWUyODQ2NzgxODc=", "number": 4361, "title": "Model loader with cuda and data parallel", "user": {"login": "antspy", "id": 625297, "node_id": "MDQ6VXNlcjYyNTI5Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/625297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/antspy", "html_url": "https://github.com/antspy", "followers_url": "https://api.github.com/users/antspy/followers", "following_url": "https://api.github.com/users/antspy/following{/other_user}", "gists_url": "https://api.github.com/users/antspy/gists{/gist_id}", "starred_url": "https://api.github.com/users/antspy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/antspy/subscriptions", "organizations_url": "https://api.github.com/users/antspy/orgs", "repos_url": "https://api.github.com/users/antspy/repos", "events_url": "https://api.github.com/users/antspy/events{/privacy}", "received_events_url": "https://api.github.com/users/antspy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-27T08:52:11Z", "updated_at": "2018-10-10T05:33:01Z", "closed_at": "2017-12-28T15:24:12Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>it would be nice if the model loader would not care whether the weights were saved with cpu, cuda or data parallel and just accept everything in input.</p>\n<p>So if model is on cpu and weights were saved on data parallel, the weight are automatically transformed to cpu and loaded. If the model is on data parallel and the weights were saved from cuda, they are loaded correctly and automatically assigned to the data parallel model, and so on.</p>\n<p>It is of course possible to do this manually by simply transforming the dictionary keys and calling .cpu() or .cuda() on the weights, but I think it would be cleaner if it was inside load_state_dict.</p>\n<p>What do you think?</p>", "body_text": "Hi,\nit would be nice if the model loader would not care whether the weights were saved with cpu, cuda or data parallel and just accept everything in input.\nSo if model is on cpu and weights were saved on data parallel, the weight are automatically transformed to cpu and loaded. If the model is on data parallel and the weights were saved from cuda, they are loaded correctly and automatically assigned to the data parallel model, and so on.\nIt is of course possible to do this manually by simply transforming the dictionary keys and calling .cpu() or .cuda() on the weights, but I think it would be cleaner if it was inside load_state_dict.\nWhat do you think?", "body": "Hi, \r\n\r\nit would be nice if the model loader would not care whether the weights were saved with cpu, cuda or data parallel and just accept everything in input.\r\n\r\nSo if model is on cpu and weights were saved on data parallel, the weight are automatically transformed to cpu and loaded. If the model is on data parallel and the weights were saved from cuda, they are loaded correctly and automatically assigned to the data parallel model, and so on.\r\n\r\nIt is of course possible to do this manually by simply transforming the dictionary keys and calling .cpu() or .cuda() on the weights, but I think it would be cleaner if it was inside load_state_dict.\r\n\r\nWhat do you think?"}