{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357425267", "html_url": "https://github.com/pytorch/pytorch/issues/4361#issuecomment-357425267", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4361", "id": 357425267, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzQyNTI2Nw==", "user": {"login": "dhpollack", "id": 368699, "node_id": "MDQ6VXNlcjM2ODY5OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/368699?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dhpollack", "html_url": "https://github.com/dhpollack", "followers_url": "https://api.github.com/users/dhpollack/followers", "following_url": "https://api.github.com/users/dhpollack/following{/other_user}", "gists_url": "https://api.github.com/users/dhpollack/gists{/gist_id}", "starred_url": "https://api.github.com/users/dhpollack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dhpollack/subscriptions", "organizations_url": "https://api.github.com/users/dhpollack/orgs", "repos_url": "https://api.github.com/users/dhpollack/repos", "events_url": "https://api.github.com/users/dhpollack/events{/privacy}", "received_events_url": "https://api.github.com/users/dhpollack/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-13T10:43:59Z", "updated_at": "2018-01-13T10:43:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, I don't think you have to do any pattern matching.  One would just have to change the <code>state_dict</code> function to save the actual state of the module in DataParallel rather than saving from <code>self</code></p>\n<p>It seems pretty clear people have issues with this.  See discussions on the pytorch message board <a href=\"https://discuss.pytorch.org/t/dataparallel-optim-and-saving-correctness/4054\" rel=\"nofollow\">here</a> (where even <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> seems confused) and <a href=\"https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/\" rel=\"nofollow\">here</a></p>\n<p>The fix would be simple:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Module</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-c1\">...</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">destination</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">prefix</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">keep_vars</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        own_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.module <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(<span class=\"pl-c1\">self</span>, torch.nn.DataParallel) \\\n                    <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>\n        <span class=\"pl-k\">if</span> destination <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            destination <span class=\"pl-k\">=</span> OrderedDict()\n        <span class=\"pl-k\">for</span> name, param <span class=\"pl-k\">in</span> own_state._parameters.items():\n            <span class=\"pl-k\">if</span> param <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                destination[prefix <span class=\"pl-k\">+</span> name] <span class=\"pl-k\">=</span> param <span class=\"pl-k\">if</span> keep_vars <span class=\"pl-k\">else</span> param.data\n        <span class=\"pl-k\">for</span> name, buf <span class=\"pl-k\">in</span> own_state._buffers.items():\n            <span class=\"pl-k\">if</span> buf <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                destination[prefix <span class=\"pl-k\">+</span> name] <span class=\"pl-k\">=</span> buf\n        <span class=\"pl-k\">for</span> name, module <span class=\"pl-k\">in</span> own_state._modules.items():\n            <span class=\"pl-k\">if</span> module <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                module.state_dict(destination, prefix <span class=\"pl-k\">+</span> name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">keep_vars</span><span class=\"pl-k\">=</span>keep_vars)\n        <span class=\"pl-k\">return</span> destination\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">load_state_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">state_dict</span>, <span class=\"pl-smi\">strict</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        own_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.module.state_dict() <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(<span class=\"pl-c1\">self</span>, torch.nn.DataParallel) \\\n                    <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>.state_dict()\n        <span class=\"pl-k\">for</span> name, param <span class=\"pl-k\">in</span> state_dict.items():\n            <span class=\"pl-k\">if</span> name <span class=\"pl-k\">in</span> own_state:\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(param, Parameter):\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> backwards compatibility for serialized parameters</span>\n                    param <span class=\"pl-k\">=</span> param.data\n                <span class=\"pl-k\">try</span>:\n                    own_state[name].copy_(param)\n                <span class=\"pl-k\">except</span> <span class=\"pl-c1\">Exception</span>:\n                    <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>While copying the parameter named <span class=\"pl-c1\">{}</span>, <span class=\"pl-pds\">'</span></span>\n                                       <span class=\"pl-s\"><span class=\"pl-pds\">'</span>whose dimensions in the model are <span class=\"pl-c1\">{}</span> and <span class=\"pl-pds\">'</span></span>\n                                       <span class=\"pl-s\"><span class=\"pl-pds\">'</span>whose dimensions in the checkpoint are <span class=\"pl-c1\">{}</span>.<span class=\"pl-pds\">'</span></span>\n                                       .format(name, own_state[name].size(), param.size()))\n            <span class=\"pl-k\">elif</span> strict:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">KeyError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>unexpected key \"<span class=\"pl-c1\">{}</span>\" in state_dict<span class=\"pl-pds\">'</span></span>\n                               .format(name))\n        <span class=\"pl-k\">if</span> strict:\n            missing <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>(own_state.keys()) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">set</span>(state_dict.keys())\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(missing) <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">KeyError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>missing keys in state_dict: \"<span class=\"pl-c1\">{}</span>\"<span class=\"pl-pds\">'</span></span>.format(missing))</pre></div>\n<p>the main difference is that in <code>state_dict</code>, I'm exporting from <code>own_state</code> rather than always from <code>self</code>.  You'd just need to create a mini utility that people could use to remove the \"module.\" from the key names of previously saved DataParallel modules.</p>", "body_text": "@apaszke, I don't think you have to do any pattern matching.  One would just have to change the state_dict function to save the actual state of the module in DataParallel rather than saving from self\nIt seems pretty clear people have issues with this.  See discussions on the pytorch message board here (where even @soumith seems confused) and here\nThe fix would be simple:\nclass Module(object):\n    ...\n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        own_state = self.module if isinstance(self, torch.nn.DataParallel) \\\n                    else self\n        if destination is None:\n            destination = OrderedDict()\n        for name, param in own_state._parameters.items():\n            if param is not None:\n                destination[prefix + name] = param if keep_vars else param.data\n        for name, buf in own_state._buffers.items():\n            if buf is not None:\n                destination[prefix + name] = buf\n        for name, module in own_state._modules.items():\n            if module is not None:\n                module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)\n        return destination\n\n    def load_state_dict(self, state_dict, strict=True):\n        own_state = self.module.state_dict() if isinstance(self, torch.nn.DataParallel) \\\n                    else self.state_dict()\n        for name, param in state_dict.items():\n            if name in own_state:\n                if isinstance(param, Parameter):\n                    # backwards compatibility for serialized parameters\n                    param = param.data\n                try:\n                    own_state[name].copy_(param)\n                except Exception:\n                    raise RuntimeError('While copying the parameter named {}, '\n                                       'whose dimensions in the model are {} and '\n                                       'whose dimensions in the checkpoint are {}.'\n                                       .format(name, own_state[name].size(), param.size()))\n            elif strict:\n                raise KeyError('unexpected key \"{}\" in state_dict'\n                               .format(name))\n        if strict:\n            missing = set(own_state.keys()) - set(state_dict.keys())\n            if len(missing) > 0:\n                raise KeyError('missing keys in state_dict: \"{}\"'.format(missing))\nthe main difference is that in state_dict, I'm exporting from own_state rather than always from self.  You'd just need to create a mini utility that people could use to remove the \"module.\" from the key names of previously saved DataParallel modules.", "body": "@apaszke, I don't think you have to do any pattern matching.  One would just have to change the `state_dict` function to save the actual state of the module in DataParallel rather than saving from `self`\r\n\r\nIt seems pretty clear people have issues with this.  See discussions on the pytorch message board [here](https://discuss.pytorch.org/t/dataparallel-optim-and-saving-correctness/4054) (where even @soumith seems confused) and [here](https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/)\r\n\r\nThe fix would be simple:\r\n\r\n```python\r\nclass Module(object):\r\n    ...\r\n    def state_dict(self, destination=None, prefix='', keep_vars=False):\r\n        own_state = self.module if isinstance(self, torch.nn.DataParallel) \\\r\n                    else self\r\n        if destination is None:\r\n            destination = OrderedDict()\r\n        for name, param in own_state._parameters.items():\r\n            if param is not None:\r\n                destination[prefix + name] = param if keep_vars else param.data\r\n        for name, buf in own_state._buffers.items():\r\n            if buf is not None:\r\n                destination[prefix + name] = buf\r\n        for name, module in own_state._modules.items():\r\n            if module is not None:\r\n                module.state_dict(destination, prefix + name + '.', keep_vars=keep_vars)\r\n        return destination\r\n\r\n    def load_state_dict(self, state_dict, strict=True):\r\n        own_state = self.module.state_dict() if isinstance(self, torch.nn.DataParallel) \\\r\n                    else self.state_dict()\r\n        for name, param in state_dict.items():\r\n            if name in own_state:\r\n                if isinstance(param, Parameter):\r\n                    # backwards compatibility for serialized parameters\r\n                    param = param.data\r\n                try:\r\n                    own_state[name].copy_(param)\r\n                except Exception:\r\n                    raise RuntimeError('While copying the parameter named {}, '\r\n                                       'whose dimensions in the model are {} and '\r\n                                       'whose dimensions in the checkpoint are {}.'\r\n                                       .format(name, own_state[name].size(), param.size()))\r\n            elif strict:\r\n                raise KeyError('unexpected key \"{}\" in state_dict'\r\n                               .format(name))\r\n        if strict:\r\n            missing = set(own_state.keys()) - set(state_dict.keys())\r\n            if len(missing) > 0:\r\n                raise KeyError('missing keys in state_dict: \"{}\"'.format(missing))\r\n```\r\n\r\nthe main difference is that in `state_dict`, I'm exporting from `own_state` rather than always from `self`.  You'd just need to create a mini utility that people could use to remove the \"module.\" from the key names of previously saved DataParallel modules.\r\n"}