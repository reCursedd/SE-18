{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229466068", "pull_request_review_id": 169969747, "id": 229466068, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTQ2NjA2OA==", "diff_hunk": "@@ -0,0 +1,62 @@\n+#pragma once\n+\n+#include <torch/data/samplers/base.h>\n+#include <torch/data/samplers/custom_index.h>\n+#include <torch/tensor.h>\n+\n+#include <cstddef>\n+\n+namespace torch {\n+namespace serialize {\n+class InputArchive;\n+class OutputArchive;\n+} // namespace serialize\n+} // namespace torch\n+\n+namespace torch {\n+namespace data {\n+namespace samplers {\n+\n+/// A wrapper around a batch size value, which implements the `CustomIndex`\n+/// interface.\n+struct BatchSize : public CustomIndex {\n+  explicit BatchSize(size_t size);\n+  size_t size() const noexcept override;\n+  operator size_t() const noexcept;\n+  size_t size_;\n+};", "path": "torch/csrc/api/include/torch/data/samplers/stream.h", "position": 27, "original_position": 27, "commit_id": "ca8bf7cb1600b77f543744f5c0e5a5fd750be324", "original_commit_id": "bcf8a94cef28493b64a8ce603dcb30feb976a891", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I don't think this API is very well suited towards streaming datasets. Basically, I would think of such a dataset as an (potentially infinite) list that's read-once. Once you pop an element, it's gone forever. However, if there's a deterministic process that generates that stream, I'd imagine that no matter how I get to the `n`-th index, the sample that's returned will always be the same. If I understand correctly, this implementation simply ignores the indices completely, and takes whatever first `batch_size` elements at each step are available?\r\n\r\nThat would be fine if a user implemented this, but it's not fine in this case I think. It would be much better to represent a `CustomIndex` for this dataset as `base` offset + `batch_size` of elements that should be taken from the stream (and the stream asserts that at next iter `base` will point to `base + batch_size` from this one).", "created_at": "2018-10-30T20:07:11Z", "updated_at": "2018-11-23T15:53:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/12960#discussion_r229466068", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12960", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229466068"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12960#discussion_r229466068"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12960"}}, "body_html": "<p>I don't think this API is very well suited towards streaming datasets. Basically, I would think of such a dataset as an (potentially infinite) list that's read-once. Once you pop an element, it's gone forever. However, if there's a deterministic process that generates that stream, I'd imagine that no matter how I get to the <code>n</code>-th index, the sample that's returned will always be the same. If I understand correctly, this implementation simply ignores the indices completely, and takes whatever first <code>batch_size</code> elements at each step are available?</p>\n<p>That would be fine if a user implemented this, but it's not fine in this case I think. It would be much better to represent a <code>CustomIndex</code> for this dataset as <code>base</code> offset + <code>batch_size</code> of elements that should be taken from the stream (and the stream asserts that at next iter <code>base</code> will point to <code>base + batch_size</code> from this one).</p>", "body_text": "I don't think this API is very well suited towards streaming datasets. Basically, I would think of such a dataset as an (potentially infinite) list that's read-once. Once you pop an element, it's gone forever. However, if there's a deterministic process that generates that stream, I'd imagine that no matter how I get to the n-th index, the sample that's returned will always be the same. If I understand correctly, this implementation simply ignores the indices completely, and takes whatever first batch_size elements at each step are available?\nThat would be fine if a user implemented this, but it's not fine in this case I think. It would be much better to represent a CustomIndex for this dataset as base offset + batch_size of elements that should be taken from the stream (and the stream asserts that at next iter base will point to base + batch_size from this one)."}