{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204273132", "pull_request_review_id": 139321797, "id": 204273132, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDI3MzEzMg==", "diff_hunk": "@@ -905,6 +909,11 @@\n - func: logsumexp_out(Tensor result, Tensor self, int64_t dim, bool keepdim=False) -> Tensor\n   variants: function\n \n+- func: masked_scale(Tensor self, Tensor mask, double scale) -> Tensor", "path": "aten/src/ATen/native/native_functions.yaml", "position": null, "original_position": 15, "commit_id": "dabffecde0fd4da60bf71575f7e4ccefc2f56d76", "original_commit_id": "54a7d06c9b135b4dc166bd19fd7594aac6c5d779", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "Sure, will do `_masked_scale` and `_fused_dropout` - they'll still be available in python, right? I thought of using `where`, but it does not accept scalar argument as one of the tensors (it should, IMO, similar to comparison ops), and there's no way to scale in the same kernel, so it ends up being way slower. I totally agree this will be beside the point when randoms in jit are up and running, but until that, using more efficient dropout provides a few percent end-to-end speedup on some networks, so there's no reason not to use it. ", "created_at": "2018-07-23T03:13:22Z", "updated_at": "2018-11-23T15:47:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/9666#discussion_r204273132", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9666", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204273132"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9666#discussion_r204273132"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9666"}}, "body_html": "<p>Sure, will do <code>_masked_scale</code> and <code>_fused_dropout</code> - they'll still be available in python, right? I thought of using <code>where</code>, but it does not accept scalar argument as one of the tensors (it should, IMO, similar to comparison ops), and there's no way to scale in the same kernel, so it ends up being way slower. I totally agree this will be beside the point when randoms in jit are up and running, but until that, using more efficient dropout provides a few percent end-to-end speedup on some networks, so there's no reason not to use it.</p>", "body_text": "Sure, will do _masked_scale and _fused_dropout - they'll still be available in python, right? I thought of using where, but it does not accept scalar argument as one of the tensors (it should, IMO, similar to comparison ops), and there's no way to scale in the same kernel, so it ends up being way slower. I totally agree this will be beside the point when randoms in jit are up and running, but until that, using more efficient dropout provides a few percent end-to-end speedup on some networks, so there's no reason not to use it.", "in_reply_to_id": 204264233}