{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355820755", "html_url": "https://github.com/pytorch/pytorch/issues/830#issuecomment-355820755", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/830", "id": 355820755, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTgyMDc1NQ==", "user": {"login": "maor121", "id": 10461916, "node_id": "MDQ6VXNlcjEwNDYxOTE2", "avatar_url": "https://avatars3.githubusercontent.com/u/10461916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maor121", "html_url": "https://github.com/maor121", "followers_url": "https://api.github.com/users/maor121/followers", "following_url": "https://api.github.com/users/maor121/following{/other_user}", "gists_url": "https://api.github.com/users/maor121/gists{/gist_id}", "starred_url": "https://api.github.com/users/maor121/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maor121/subscriptions", "organizations_url": "https://api.github.com/users/maor121/orgs", "repos_url": "https://api.github.com/users/maor121/repos", "events_url": "https://api.github.com/users/maor121/events{/privacy}", "received_events_url": "https://api.github.com/users/maor121/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-07T12:50:57Z", "updated_at": "2018-01-07T12:55:00Z", "author_association": "NONE", "body_html": "<p>Hi, I thought I would contribute an idea taken from another framework: Dynet.</p>\n<p>Currently, implementing batching in sequences in Pytorch is a significant pain:</p>\n<ol>\n<li>Packed sequence alter the input order, it is undocumented in what way exactly, and unless you want to constantly unpack &amp; pack (inefficient), you would have a hard time running operations on it.</li>\n<li>Masking is slow.</li>\n<li>Batching sequences of the same length is possible, can be done manually, but damages shuffle randomization which hurts accuracy in the end, unless you use very small batch sizes.<br>\nI spent ALOT of time on this, it is a significant flow...</li>\n</ol>\n<p>Also, if you want to use custom LSTM in Pytorch, and use batching, it is also hard... You have to start diving into the source code.</p>\n<p>For those not familiar, dynet implements lazy evaluation. It means you iterate over the data in the loop, process it as you would, and later, on the call to forward. The data is batched together automaticly according to <a href=\"https://arxiv.org/abs/1705.07860\" rel=\"nofollow\">https://arxiv.org/abs/1705.07860</a></p>\n<p>This saves the hassle of organizing the data into batches manually, which is a pain....<br>\nMy suggestion for implementing this in Pytorch is as follows:</p>\n<ol>\n<li>Create classes, LazyModule, LazyTensor, LazyVariable</li>\n<li>LazyModule, on the forward function, will recieve a LazyVariable, which would have the same interface as a Variable. The dimension of the input, will NOT include batch dimension. It would be a single sequence. Operations on the Lazy object will be saved in a stack. To be performed later on the entire batch.</li>\n<li>On the call to final forward (evaluate\\backward..), LazyModule will have all the calls to forward stored somewhere, and will be able to batch the input and run the network on the data.</li>\n</ol>\n<p>This will save the hassle of using PackedSequence, masking, or manualy batching sequences of the same length. The idea is taken from dynet, and not my own.</p>\n<p>What do you think? Is this possible in Pytorch?</p>", "body_text": "Hi, I thought I would contribute an idea taken from another framework: Dynet.\nCurrently, implementing batching in sequences in Pytorch is a significant pain:\n\nPacked sequence alter the input order, it is undocumented in what way exactly, and unless you want to constantly unpack & pack (inefficient), you would have a hard time running operations on it.\nMasking is slow.\nBatching sequences of the same length is possible, can be done manually, but damages shuffle randomization which hurts accuracy in the end, unless you use very small batch sizes.\nI spent ALOT of time on this, it is a significant flow...\n\nAlso, if you want to use custom LSTM in Pytorch, and use batching, it is also hard... You have to start diving into the source code.\nFor those not familiar, dynet implements lazy evaluation. It means you iterate over the data in the loop, process it as you would, and later, on the call to forward. The data is batched together automaticly according to https://arxiv.org/abs/1705.07860\nThis saves the hassle of organizing the data into batches manually, which is a pain....\nMy suggestion for implementing this in Pytorch is as follows:\n\nCreate classes, LazyModule, LazyTensor, LazyVariable\nLazyModule, on the forward function, will recieve a LazyVariable, which would have the same interface as a Variable. The dimension of the input, will NOT include batch dimension. It would be a single sequence. Operations on the Lazy object will be saved in a stack. To be performed later on the entire batch.\nOn the call to final forward (evaluate\\backward..), LazyModule will have all the calls to forward stored somewhere, and will be able to batch the input and run the network on the data.\n\nThis will save the hassle of using PackedSequence, masking, or manualy batching sequences of the same length. The idea is taken from dynet, and not my own.\nWhat do you think? Is this possible in Pytorch?", "body": "Hi, I thought I would contribute an idea taken from another framework: Dynet.\r\n\r\nCurrently, implementing batching in sequences in Pytorch is a significant pain:\r\n1) Packed sequence alter the input order, it is undocumented in what way exactly, and unless you want to constantly unpack & pack (inefficient), you would have a hard time running operations on it.\r\n2) Masking is slow.\r\n3) Batching sequences of the same length is possible, can be done manually, but damages shuffle randomization which hurts accuracy in the end, unless you use very small batch sizes.\r\nI spent ALOT of time on this, it is a significant flow...\r\n\r\nAlso, if you want to use custom LSTM in Pytorch, and use batching, it is also hard... You have to start diving into the source code.\r\n\r\nFor those not familiar, dynet implements lazy evaluation. It means you iterate over the data in the loop, process it as you would, and later, on the call to forward. The data is batched together automaticly according to https://arxiv.org/abs/1705.07860 \r\n\r\nThis saves the hassle of organizing the data into batches manually, which is a pain....\r\nMy suggestion for implementing this in Pytorch is as follows:\r\n\r\n1) Create classes, LazyModule, LazyTensor, LazyVariable\r\n2) LazyModule, on the forward function, will recieve a LazyVariable, which would have the same interface as a Variable. The dimension of the input, will NOT include batch dimension. It would be a single sequence. Operations on the Lazy object will be saved in a stack. To be performed later on the entire batch.\r\n3) On the call to final forward (evaluate\\backward..), LazyModule will have all the calls to forward stored somewhere, and will be able to batch the input and run the network on the data. \r\n\r\nThis will save the hassle of using PackedSequence, masking, or manualy batching sequences of the same length. The idea is taken from dynet, and not my own.\r\n\r\nWhat do you think? Is this possible in Pytorch?\r\n  "}