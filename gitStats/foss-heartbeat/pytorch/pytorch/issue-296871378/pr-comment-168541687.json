{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168541687", "pull_request_review_id": 96869478, "id": 168541687, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODU0MTY4Nw==", "diff_hunk": "@@ -129,45 +127,46 @@ bool Eval::trySimpleEval(const variable_list& inputs, const variable_list& outpu\n   if (inherited_placeholders.size() != 0) return false;\n \n   auto& grad_fn = outputs[0].grad_fn();\n-  if (static_cast<std::size_t>(grad_fn->num_inputs) >= max_outputs) return false;\n-  if (static_cast<std::size_t>(grad_fn->num_inputs) != outputs.size()) return false;\n+  if (static_cast<std::size_t>(grad_fn->num_inputs()) >= max_outputs) return false;\n+  if (static_cast<std::size_t>(grad_fn->num_inputs()) != outputs.size()) return false;\n \n   // Check that all outputs have the same grad_fn and cover all its inputs\n   bitset_type output_nrs = 0;\n-  bitset_type expected_bitset = ((1 << grad_fn->num_inputs) - 1);\n+  bitset_type expected_bitset = ((1 << grad_fn->num_inputs()) - 1);\n   for (auto & output : outputs) {\n     if (output.grad_fn() != grad_fn) return false;\n     output_nrs |= (1 << output.output_nr());\n   }\n   if (output_nrs != expected_bitset) return false;\n \n-  // Check that grad_fn->next_functions matches the inputs exactly\n+  // Check that grad_fn's next_edges match the inputs exactly.\n   auto num_inputs = inputs.size();\n-  auto& grad_next_fns = grad_fn->next_functions;\n-  if (num_inputs != grad_next_fns.size()) return false;\n+  if (num_inputs != grad_fn->num_outputs()) return false;\n   for (std::size_t i = 0; i < num_inputs; ++i) {\n-    // Unfortunately, null edge pruning (see Note [Null-edge pruning]) applies to\n-    // autograd functions which would otherwise be eligible for the SimpleEval\n-    // optimization.  This makes everything more complicated, so for now we just don't\n-    // attempt the optimization in this case.  To fix it properly,\n-    // we'd need to filter grad_next_fns and outputs of apply in Eval::apply.\n-    // The check below tests if null edge pruning occurred.\n-    if (!inputs[i].defined() || !grad_next_fns[i].is_valid()) return false;\n-    if (grad_next_fns[i] != inputs[i].gradient_edge()) return false;\n+    const auto& next_grad_edge = grad_fn->next_edge(i);\n+    // Unfortunately, null edge pruning (see Note [Null-edge pruning]) applies\n+    // to autograd functions which would otherwise be eligible for the\n+    // SimpleEval optimization.  This makes everything more complicated, so for\n+    // now we just don't attempt the optimization in this case.  To fix it\n+    // properly, we'd need to filter grad_fn's output edges and outputs of\n+    // apply in Eval::apply. The check below tests if null edge pruning\n+    // occurred.\n+    if (!inputs[i].defined() || !next_grad_edge.is_valid()) return false;\n+    if (next_grad_edge != inputs[i].gradient_edge()) return false;\n   }\n \n   // Success! We still need to set up placeholders for next stages and to drop\n   // references to the graph.\n-  std::swap(next_functions, grad_next_fns);\n-  grad_next_fns.reserve(num_inputs);\n+  next_edges_.reserve(num_inputs);\n+  grad_fn->swap_next_edges(next_edges_);\n   placeholders.reserve(num_inputs);\n-  for (std::size_t i = 0; i < num_inputs; ++i) {\n-    auto placeholder = std::make_shared<EvalOutput>(next_functions[i]);\n-    grad_next_fns.emplace_back(placeholder, 0);\n+  for (const auto& input : next_edges_) {\n+    auto placeholder = std::make_shared<EvalOutput>(input);\n+    grad_fn->add_next_edge({placeholder, 0});", "path": "torch/csrc/autograd/functions/special.cpp", "position": 131, "original_position": 131, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "fbbfe675810f79e9d87f0239c892c3e16a23b976", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "One more thought when reading this code: `add_next_edge` and `add_gradient_edge` sound similar, and they can be easily swapped accidentally, because it's not immediately clear whether a `gradient_edge` is an incoming edge or an outgoing edge for a given function. Not sure how to solve it without making the names extremely verbose.\r\n\r\nEDIT: this only becomes more confusing later when you see things like these:\r\n```cpp\r\ngrad_fn->add_next_edge(input.gradient_edge());\r\n```\r\nI think I could have imagined this line written like this without any problem. It's not our current API, I'm just saying it's hard to tell the difference, because it looks like a syntactic sugar over the `add_gradient_edge` we have:\r\n```cpp\r\ngrad_fn->add_gradient_edge(input);\r\n```", "created_at": "2018-02-15T17:02:23Z", "updated_at": "2018-11-23T15:39:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168541687", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168541687"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168541687"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>One more thought when reading this code: <code>add_next_edge</code> and <code>add_gradient_edge</code> sound similar, and they can be easily swapped accidentally, because it's not immediately clear whether a <code>gradient_edge</code> is an incoming edge or an outgoing edge for a given function. Not sure how to solve it without making the names extremely verbose.</p>\n<p>EDIT: this only becomes more confusing later when you see things like these:</p>\n<div class=\"highlight highlight-source-c++\"><pre>grad_fn-&gt;<span class=\"pl-en\">add_next_edge</span>(input.gradient_edge());</pre></div>\n<p>I think I could have imagined this line written like this without any problem. It's not our current API, I'm just saying it's hard to tell the difference, because it looks like a syntactic sugar over the <code>add_gradient_edge</code> we have:</p>\n<div class=\"highlight highlight-source-c++\"><pre>grad_fn-&gt;<span class=\"pl-en\">add_gradient_edge</span>(input);</pre></div>", "body_text": "One more thought when reading this code: add_next_edge and add_gradient_edge sound similar, and they can be easily swapped accidentally, because it's not immediately clear whether a gradient_edge is an incoming edge or an outgoing edge for a given function. Not sure how to solve it without making the names extremely verbose.\nEDIT: this only becomes more confusing later when you see things like these:\ngrad_fn->add_next_edge(input.gradient_edge());\nI think I could have imagined this line written like this without any problem. It's not our current API, I'm just saying it's hard to tell the difference, because it looks like a syntactic sugar over the add_gradient_edge we have:\ngrad_fn->add_gradient_edge(input);"}