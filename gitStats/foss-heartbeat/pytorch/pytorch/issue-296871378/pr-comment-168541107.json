{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168541107", "pull_request_review_id": 96869478, "id": 168541107, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODU0MTEwNw==", "diff_hunk": "@@ -129,45 +127,46 @@ bool Eval::trySimpleEval(const variable_list& inputs, const variable_list& outpu\n   if (inherited_placeholders.size() != 0) return false;\n \n   auto& grad_fn = outputs[0].grad_fn();\n-  if (static_cast<std::size_t>(grad_fn->num_inputs) >= max_outputs) return false;\n-  if (static_cast<std::size_t>(grad_fn->num_inputs) != outputs.size()) return false;\n+  if (static_cast<std::size_t>(grad_fn->num_inputs()) >= max_outputs) return false;\n+  if (static_cast<std::size_t>(grad_fn->num_inputs()) != outputs.size()) return false;\n \n   // Check that all outputs have the same grad_fn and cover all its inputs\n   bitset_type output_nrs = 0;\n-  bitset_type expected_bitset = ((1 << grad_fn->num_inputs) - 1);\n+  bitset_type expected_bitset = ((1 << grad_fn->num_inputs()) - 1);\n   for (auto & output : outputs) {\n     if (output.grad_fn() != grad_fn) return false;\n     output_nrs |= (1 << output.output_nr());\n   }\n   if (output_nrs != expected_bitset) return false;\n \n-  // Check that grad_fn->next_functions matches the inputs exactly\n+  // Check that grad_fn's next_edges match the inputs exactly.\n   auto num_inputs = inputs.size();\n-  auto& grad_next_fns = grad_fn->next_functions;\n-  if (num_inputs != grad_next_fns.size()) return false;\n+  if (num_inputs != grad_fn->num_outputs()) return false;\n   for (std::size_t i = 0; i < num_inputs; ++i) {\n-    // Unfortunately, null edge pruning (see Note [Null-edge pruning]) applies to\n-    // autograd functions which would otherwise be eligible for the SimpleEval\n-    // optimization.  This makes everything more complicated, so for now we just don't\n-    // attempt the optimization in this case.  To fix it properly,\n-    // we'd need to filter grad_next_fns and outputs of apply in Eval::apply.\n-    // The check below tests if null edge pruning occurred.\n-    if (!inputs[i].defined() || !grad_next_fns[i].is_valid()) return false;\n-    if (grad_next_fns[i] != inputs[i].gradient_edge()) return false;\n+    const auto& next_grad_edge = grad_fn->next_edge(i);\n+    // Unfortunately, null edge pruning (see Note [Null-edge pruning]) applies\n+    // to autograd functions which would otherwise be eligible for the\n+    // SimpleEval optimization.  This makes everything more complicated, so for\n+    // now we just don't attempt the optimization in this case.  To fix it\n+    // properly, we'd need to filter grad_fn's output edges and outputs of\n+    // apply in Eval::apply. The check below tests if null edge pruning\n+    // occurred.\n+    if (!inputs[i].defined() || !next_grad_edge.is_valid()) return false;\n+    if (next_grad_edge != inputs[i].gradient_edge()) return false;\n   }\n \n   // Success! We still need to set up placeholders for next stages and to drop\n   // references to the graph.\n-  std::swap(next_functions, grad_next_fns);\n-  grad_next_fns.reserve(num_inputs);\n+  next_edges_.reserve(num_inputs);\n+  grad_fn->swap_next_edges(next_edges_);", "path": "torch/csrc/autograd/functions/special.cpp", "position": null, "original_position": 124, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "fbbfe675810f79e9d87f0239c892c3e16a23b976", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I have to say that this code became more confusing that it was. Can we reserve the `grad_next_fns` as we did before? It's weird that we allocate memory just to swap it a moment later.", "created_at": "2018-02-15T17:00:28Z", "updated_at": "2018-11-23T15:39:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168541107", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168541107"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168541107"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>I have to say that this code became more confusing that it was. Can we reserve the <code>grad_next_fns</code> as we did before? It's weird that we allocate memory just to swap it a moment later.</p>", "body_text": "I have to say that this code became more confusing that it was. Can we reserve the grad_next_fns as we did before? It's weird that we allocate memory just to swap it a moment later."}