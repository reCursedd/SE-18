{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169292206", "pull_request_review_id": 97791061, "id": 169292206, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2OTI5MjIwNg==", "diff_hunk": "@@ -23,164 +17,360 @@\n #include <cstdint>\n #include <initializer_list>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n namespace torch { namespace autograd {\n \n-struct Function;\n-struct Variable;\n struct Edge;\n+struct FunctionPostHook;\n+struct FunctionPreHook;\n \n using tensor_list = std::vector<at::Tensor>;\n using variable_list = std::vector<Variable>;\n-using function_list = std::vector<Edge>;\n+using edge_list = std::vector<Edge>;\n using saved_variable_list = std::vector<SavedVariable>;\n using IndexRange = std::pair<size_t, size_t>;\n \n-namespace detail {\n-struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {\n-  function_list next_functions;\n-  using IterArgs<MakeNextFunctionList>::operator();\n-  void operator()(const Variable& variable) {\n-    if (variable.defined()) {\n-      next_functions.push_back(variable.gradient_edge());\n-    } else {\n-      next_functions.emplace_back();\n-    }\n-  }\n-};\n-} // namespace detail\n-\n-// Returns true if any of the variables in the list require a gradient.\n-inline bool any_variable_requires_grad(const variable_list& variables) {\n-  return std::any_of(\n-      variables.begin(), variables.end(), [](const Variable& variable) {\n-        return variable.requires_grad();\n-      });\n-}\n-\n-template <typename... Variables>\n-function_list get_next_functions(Variables&&... variables) {\n-  if (!GradMode::is_enabled()) return {};\n-  detail::MakeNextFunctionList make;\n-  make.apply(std::forward<Variables>(variables)...);\n-  return std::move(make.next_functions);\n-}\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                               Function\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Function` is an abstract class that represents an operation taking zero\n+/// or more input `Variable`s and producing zero or more output `Variable`s. All\n+/// functions in PyTorch's autograd machinery derive from this class and\n+/// override its `apply` method. Instances of such subclasses will then be\n+/// invokeable via the call operator.\n+///\n+///                    Functions in the Autograd Graph\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// When viewing the autograd system as a graph, `Function`s are the vertices or\n+/// nodes, connected to each other via (directed) `Edge`s, which themselves are\n+/// represented via (`Function`, input_nr) pairs. `Variable`s are the outputs to\n+/// and inputs of `Function`s, and travel between these edges during execution\n+/// of the graph. When two or more `Edge`s (from different sources) point at the\n+/// same input to a `Function`, the values produced along all of these edges are\n+/// implicitly summed prior to being forwarded to the target `Function`.\n+///\n+///                              Hierarchy\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Subclasses will often represent forward passes (functions) or backward\n+/// passes (derivatives). Note, however, that due to the very general\n+/// definition of a `Function` taking *zero* or more inputs and producing\n+/// *zero* or more outputs, uses of `Function`s are flexible and extend beyond\n+/// purely mathematical operations. For example, the `AccumulateGrad` function\n+/// is a *sink*: it takes one input, but produces no outputs, instead adding\n+/// the input to its internal accumulator as a side effect. At the other\n+/// extreme, the `GraphRoot` function receives no inputs from other functions,\n+/// but produces multiple outputs.\n+///\n+///                              Interface\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// The most important method on `Function` is the call operator, which takes in\n+/// a list of variables and produces a list of variables. The precise size of\n+/// these lists can be determined with `num_inputs()` and `num_outputs()`.\n+/// `Function`s are stitched together via their `next_edge` interface, which let\n+/// you manipulate the set of outgoing edges of a `Function`. You can add an\n+/// edge with `add_next_edge()`, retrieve an edge with `next_edge(index)` and\n+/// iterate over them via the `next_edges()` method. Other methods exist for\n+/// integration with the JIT and other parts of PyTorch. Every `Function` has a\n+/// *sequence number* that increases monotonically in the order of `Function`\n+/// construction. It can be retrieved via the `sequence_nr()` method. Note that\n+/// this sequence number is *thread local*. This means that when `Function`s\n+/// `A`, `B` and `C` are created consecutively in the same thread, their\n+/// sequence numbers will be ordered `A` < `B` < `C`. If, however, `A` and `B`\n+/// are created in one thread and `C` is created in a new thread, `C` will have\n+/// a *lower* sequence number than `B`.\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n struct Function : std::enable_shared_from_this<Function> {\n-  static thread_local uint64_t function_counter;\n-\n-  Function() : time(function_counter++) {}\n-  Function(function_list&& next_functions_) : Function() {\n-    next_functions = std::move(next_functions_);\n-  }\n+ public:\n+  /// Construct a new `Function` with `num_inputs` inputs and the given\n+  /// `next_edges`.\n+  explicit Function(\n+      uint32_t num_inputs = 0,\n+      edge_list&& next_edges = edge_list())\n+      : sequence_nr_(next_sequence_nr_++),\n+        num_inputs_(num_inputs),\n+        next_edges_(std::move(next_edges)) {}\n \n+  /// Functions are neither copyable nor moveable.\n   Function(const Function& other) = delete;\n   Function(Function&& other) = delete;\n-  virtual ~Function() {}\n-\n-  // Implements the operation\n-  // NOTE: Don't call this function directly. Use operator() instead.\n-  virtual variable_list apply(const variable_list& inputs) = 0;\n-  variable_list tracedApply(variable_list inputs);\n+  Function& operator=(const Function& other) = delete;\n+  Function& operator=(Function&& other) = delete;\n+  virtual ~Function() = default;\n \n+  /// Evaluates the function on the given inputs and returns the result of the\n+  /// function call.\n   variable_list operator()(const variable_list& inputs) {\n     profiler::RecordFunction rec(this);\n     if (jit::tracer::isTracingVar(inputs)) {\n-      return tracedApply(inputs);\n+      return traced_apply(inputs);\n     }\n     return apply(inputs);\n   }\n \n-  // PyFunctions are not managed by shared_ptrs by default, but are bound to the\n-  // lifetime of their Python object instead.\n-  virtual std::shared_ptr<Function> getSharedPtr() {\n-    return shared_from_this();\n-  };\n-\n-  // Releases saved variables if the operation won't be reused\n-  virtual inline void releaseVariables() {}\n-  // called before a an apply if will release variables is going to be called\n-  // allows larger ops like InterpreterAutogradFunction\n-  // to incrementally release variables as they run\n-  virtual inline void willReleaseVariables() {}\n-  // Function name for debugging\n-  virtual std::string name();\n+  // Graph Connectivity API\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-  bool should_compute_output(size_t index) const {\n-    TORCH_ASSERTM(index < next_functions.size(), \"Index out of range\");\n-    return next_functions[index].is_valid();\n+  // Inputs\n+\n+  /// Increments the number of inputs of the function and returns the previous\n+  /// value.\n+  uint32_t bump_inputs() noexcept {\n+    return num_inputs_++;\n   }\n \n-  bool should_compute_any_outputs() const {\n-    for (size_t i = 0; i < next_functions.size(); ++i) {\n-      if (should_compute_output(i)) {\n-        return true;\n-      }\n-    }\n-    return false;\n+  void set_num_inputs(uint32_t num_inputs) noexcept {\n+    num_inputs_ = num_inputs;\n   }\n \n-  bool should_compute_output(std::initializer_list<size_t> idxs) const {\n-    return std::any_of(idxs.begin(), idxs.end(), [this](size_t i) {\n-      return should_compute_output(i);\n-    });\n+  uint32_t num_inputs() const noexcept {\n+    return num_inputs_;\n+  }\n+\n+  // Outputs (\"Next Edges\")\n+\n+  const Edge& next_edge(size_t index) const noexcept {\n+    return next_edges_[index];\n+  }\n+\n+  void set_next_edge(size_t index, Edge edge) {\n+    next_edges_[index] = std::move(edge);\n+  }\n+\n+  void add_next_edge(Edge edge) {\n+    next_edges_.push_back(std::move(edge));\n+  }\n+\n+  void set_next_edges(edge_list&& next_edges) {\n+    next_edges_ = std::move(next_edges);\n   }\n \n+  const edge_list& next_edges() const noexcept {\n+    return next_edges_;\n+  }\n+\n+  edge_list& next_edges() noexcept {\n+    return next_edges_;\n+  }\n+\n+  uint32_t num_outputs() const noexcept {\n+    return next_edges_.size();\n+  }\n+\n+  // Miscellaneous Methods\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// The unique sequence number of this `Function`.", "path": "torch/csrc/autograd/function.h", "position": null, "original_position": 242, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "d162ddcf5c0dbce3018ca1084c7999a40b59ba07", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "sequence numbers are not unique", "created_at": "2018-02-20T11:37:15Z", "updated_at": "2018-11-23T15:39:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r169292206", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169292206"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r169292206"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>sequence numbers are not unique</p>", "body_text": "sequence numbers are not unique"}