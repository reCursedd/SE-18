{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168596140", "pull_request_review_id": 96991988, "id": 168596140, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODU5NjE0MA==", "diff_hunk": "@@ -129,45 +127,46 @@ bool Eval::trySimpleEval(const variable_list& inputs, const variable_list& outpu\n   if (inherited_placeholders.size() != 0) return false;\n \n   auto& grad_fn = outputs[0].grad_fn();\n-  if (static_cast<std::size_t>(grad_fn->num_inputs) >= max_outputs) return false;\n-  if (static_cast<std::size_t>(grad_fn->num_inputs) != outputs.size()) return false;\n+  if (static_cast<std::size_t>(grad_fn->num_inputs()) >= max_outputs) return false;\n+  if (static_cast<std::size_t>(grad_fn->num_inputs()) != outputs.size()) return false;\n \n   // Check that all outputs have the same grad_fn and cover all its inputs\n   bitset_type output_nrs = 0;\n-  bitset_type expected_bitset = ((1 << grad_fn->num_inputs) - 1);\n+  bitset_type expected_bitset = ((1 << grad_fn->num_inputs()) - 1);\n   for (auto & output : outputs) {\n     if (output.grad_fn() != grad_fn) return false;\n     output_nrs |= (1 << output.output_nr());\n   }\n   if (output_nrs != expected_bitset) return false;\n \n-  // Check that grad_fn->next_functions matches the inputs exactly\n+  // Check that grad_fn's next_edges match the inputs exactly.\n   auto num_inputs = inputs.size();\n-  auto& grad_next_fns = grad_fn->next_functions;\n-  if (num_inputs != grad_next_fns.size()) return false;\n+  if (num_inputs != grad_fn->num_outputs()) return false;\n   for (std::size_t i = 0; i < num_inputs; ++i) {\n-    // Unfortunately, null edge pruning (see Note [Null-edge pruning]) applies to\n-    // autograd functions which would otherwise be eligible for the SimpleEval\n-    // optimization.  This makes everything more complicated, so for now we just don't\n-    // attempt the optimization in this case.  To fix it properly,\n-    // we'd need to filter grad_next_fns and outputs of apply in Eval::apply.\n-    // The check below tests if null edge pruning occurred.\n-    if (!inputs[i].defined() || !grad_next_fns[i].is_valid()) return false;\n-    if (grad_next_fns[i] != inputs[i].gradient_edge()) return false;\n+    const auto& next_grad_edge = grad_fn->next_edge(i);\n+    // Unfortunately, null edge pruning (see Note [Null-edge pruning]) applies\n+    // to autograd functions which would otherwise be eligible for the\n+    // SimpleEval optimization.  This makes everything more complicated, so for\n+    // now we just don't attempt the optimization in this case.  To fix it\n+    // properly, we'd need to filter grad_fn's output edges and outputs of\n+    // apply in Eval::apply. The check below tests if null edge pruning\n+    // occurred.\n+    if (!inputs[i].defined() || !next_grad_edge.is_valid()) return false;\n+    if (next_grad_edge != inputs[i].gradient_edge()) return false;\n   }\n \n   // Success! We still need to set up placeholders for next stages and to drop\n   // references to the graph.\n-  std::swap(next_functions, grad_next_fns);\n-  grad_next_fns.reserve(num_inputs);\n+  next_edges_.reserve(num_inputs);\n+  grad_fn->swap_next_edges(next_edges_);\n   placeholders.reserve(num_inputs);\n-  for (std::size_t i = 0; i < num_inputs; ++i) {\n-    auto placeholder = std::make_shared<EvalOutput>(next_functions[i]);\n-    grad_next_fns.emplace_back(placeholder, 0);\n+  for (const auto& input : next_edges_) {\n+    auto placeholder = std::make_shared<EvalOutput>(input);\n+    grad_fn->add_next_edge({placeholder, 0});", "path": "torch/csrc/autograd/functions/special.cpp", "position": 131, "original_position": 131, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "fbbfe675810f79e9d87f0239c892c3e16a23b976", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "I can call it `create_gradient_edge`. I had that name first but changed it to `add` because of the overload I added that takes an `Edge` (since it wasn't \"creating\" an Edge). Since I'm bringing back `set_gradient_edge`, the free function can `create` and edge. At least it'll be `create` instead of `add`", "created_at": "2018-02-15T20:19:35Z", "updated_at": "2018-11-23T15:39:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168596140", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168596140"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168596140"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>I can call it <code>create_gradient_edge</code>. I had that name first but changed it to <code>add</code> because of the overload I added that takes an <code>Edge</code> (since it wasn't \"creating\" an Edge). Since I'm bringing back <code>set_gradient_edge</code>, the free function can <code>create</code> and edge. At least it'll be <code>create</code> instead of <code>add</code></p>", "body_text": "I can call it create_gradient_edge. I had that name first but changed it to add because of the overload I added that takes an Edge (since it wasn't \"creating\" an Edge). Since I'm bringing back set_gradient_edge, the free function can create and edge. At least it'll be create instead of add", "in_reply_to_id": 168541687}