{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168537019", "pull_request_review_id": 96869478, "id": 168537019, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODUzNzAxOQ==", "diff_hunk": "@@ -23,164 +17,364 @@\n #include <cstdint>\n #include <initializer_list>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n namespace torch { namespace autograd {\n \n-struct Function;\n-struct Variable;\n struct Edge;\n+struct FunctionPostHook;\n+struct FunctionPreHook;\n \n using tensor_list = std::vector<at::Tensor>;\n using variable_list = std::vector<Variable>;\n-using function_list = std::vector<Edge>;\n+using edge_list = std::vector<Edge>;\n using saved_variable_list = std::vector<SavedVariable>;\n using IndexRange = std::pair<size_t, size_t>;\n \n-namespace detail {\n-struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {\n-  function_list next_functions;\n-  using IterArgs<MakeNextFunctionList>::operator();\n-  void operator()(const Variable& variable) {\n-    if (variable.defined()) {\n-      next_functions.push_back(variable.gradient_edge());\n-    } else {\n-      next_functions.emplace_back();\n-    }\n-  }\n-};\n-} // namespace detail\n-\n-// Returns true if any of the variables in the list require a gradient.\n-inline bool any_variable_requires_grad(const variable_list& variables) {\n-  return std::any_of(\n-      variables.begin(), variables.end(), [](const Variable& variable) {\n-        return variable.requires_grad();\n-      });\n-}\n-\n-template <typename... Variables>\n-function_list get_next_functions(Variables&&... variables) {\n-  if (!GradMode::is_enabled()) return {};\n-  detail::MakeNextFunctionList make;\n-  make.apply(std::forward<Variables>(variables)...);\n-  return std::move(make.next_functions);\n-}\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                               Function\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Function` is an abstract class that represents an operation taking zero\n+/// or more inputs and producing zero or more outputs. A function that wants to\n+/// interact with PyTorch's autograd machinery, i.e. take in `Variables` and be\n+/// connected to further `Functions` in a graph, should derive from this class\n+/// and override its `apply` method. Instances of such subclasses will then be\n+/// invokeable via the call operator.\n+///\n+///                    Functions in the Autograd Graph\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// When viewing the autograd system as a graph, `Function`s are the vertices\n+/// or nodes, connected to each other via (directed) `Edge`s, which themselves\n+/// are represented via the `Function` they are directed at and the particular\n+/// index of the edge when the Function is multivariate. `Variable`s are the\n+/// outputs to and inputs of `Function`s, and travel between these edges during\n+/// execution of the graph.\n+///\n+///                              Hierarchy\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Subclasses will often represent forward passes (functions) or backward\n+/// passes (derivatives). Note, however, that due to the very general\n+/// definition of a `Function` taking *zero* or more inputs and producing\n+/// *zero* or more outputs, uses of `Function`s are flexible and extend beyond\n+/// purely mathematical operations. For example, the `AccumulateGrad` function\n+/// is a *sink*: it takes one input, but produces no outputs, instead adding\n+/// the input to its internal accumulator as a side effect. At the other\n+/// extreme, the `GraphRoot` function receives no inputs from other functions,\n+/// but produces multiple outputs. Finally, the `Error` function takes neither\n+/// inputs nor outputs.\n+///\n+///                              Interface\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// The most important method on `Function` is the call operator, which takes\n+/// in a list of variables and produces a list of variables. The precise size\n+/// of these lists can be determined with `num_inputs()` and `num_outputs()`.\n+/// `Function`s are stitched together via their `next_edge` interface, which\n+/// let you manipulate the set of outgoing edges of a `Function`. You can add\n+/// an edge with `add_next_edge()`, retrieve an edge with `next_edge(index)`\n+/// and iterate over them via the `next_edges()` method. Other methods exist\n+/// for integration with the JIT and other parts of PyTorch. Every `Function`\n+/// also has a *sequence number* which increases monotonically in the order of\n+/// `Function` construction, which can be retrieved via the `sequence_number()`\n+/// getter.\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n struct Function : std::enable_shared_from_this<Function> {\n-  static thread_local uint64_t function_counter;\n+ public:\n+  /// Create a context edge for the JIT.\n+  static void set_up_context_edge(\n+      jit::Node* this_node,\n+      const variable_list& inputs,\n+      const variable_list& outputs);\n \n-  Function() : time(function_counter++) {}\n-  Function(function_list&& next_functions_) : Function() {\n-    next_functions = std::move(next_functions_);\n-  }\n+  /// Construct a new `Function` with `num_inputs` inputs and the given\n+  /// `next_edges`.\n+  explicit Function(\n+      uint32_t num_inputs = 0,\n+      edge_list&& next_edges = edge_list())\n+      : sequence_number_(function_counter_++),\n+        num_inputs_(num_inputs),\n+        next_edges_(std::move(next_edges)) {}\n \n+  /// Functions are neither copyable nor moveable.\n   Function(const Function& other) = delete;\n   Function(Function&& other) = delete;\n-  virtual ~Function() {}\n-\n-  // Implements the operation\n-  // NOTE: Don't call this function directly. Use operator() instead.\n-  virtual variable_list apply(const variable_list& inputs) = 0;\n-  variable_list tracedApply(variable_list inputs);\n+  Function& operator=(const Function& other) = delete;\n+  Function& operator=(Function&& other) = delete;\n+  virtual ~Function() = default;\n \n+  /// Evaluates the function on the given inputs and return the result of the\n+  /// function call.\n   variable_list operator()(const variable_list& inputs) {\n     profiler::RecordFunction rec(this);\n     if (jit::tracer::isTracingVar(inputs)) {\n-      return tracedApply(inputs);\n+      return traced_apply(inputs);\n     }\n     return apply(inputs);\n   }\n \n-  // PyFunctions are not managed by shared_ptrs by default, but are bound to the\n-  // lifetime of their Python object instead.\n-  virtual std::shared_ptr<Function> getSharedPtr() {\n-    return shared_from_this();\n-  };\n-\n-  // Releases saved variables if the operation won't be reused\n-  virtual inline void releaseVariables() {}\n-  // called before a an apply if will release variables is going to be called\n-  // allows larger ops like InterpreterAutogradFunction\n-  // to incrementally release variables as they run\n-  virtual inline void willReleaseVariables() {}\n-  // Function name for debugging\n-  virtual std::string name();\n+  // Graph Connectivity API\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  // Inputs\n \n-  bool should_compute_output(size_t index) const {\n-    TORCH_ASSERTM(index < next_functions.size(), \"Index out of range\");\n-    return next_functions[index].is_valid();\n+  /// Increments the number of inputs of the function and returns the previous\n+  /// value.\n+  uint32_t bump_inputs() noexcept {\n+    return num_inputs_++;\n   }\n \n-  bool should_compute_any_outputs() const {\n-    for (size_t i = 0; i < next_functions.size(); ++i) {\n-      if (should_compute_output(i)) {\n-        return true;\n-      }\n-    }\n-    return false;\n+  void set_num_inputs(uint32_t num_inputs) noexcept {\n+    num_inputs_ = num_inputs;\n   }\n \n-  bool should_compute_output(std::initializer_list<size_t> idxs) const {\n-    return std::any_of(idxs.begin(), idxs.end(), [this](size_t i) {\n-      return should_compute_output(i);\n-    });\n+  uint32_t num_inputs() const noexcept {\n+    return num_inputs_;\n+  }\n+\n+  // Outputs (\"Next Edges\")\n+\n+  const Edge& next_edge(size_t index) const noexcept {\n+    return next_edges_[index];\n+  }\n+\n+  void set_next_edge(size_t index, Edge edge) {\n+    next_edges_[index] = std::move(edge);\n+  }\n+\n+  void add_next_edge(Edge edge) {\n+    next_edges_.push_back(std::move(edge));\n+  }\n+\n+  void set_next_edges(edge_list&& next_edges) {\n+    next_edges_ = std::move(next_edges);\n+  }\n+\n+  void swap_next_edges(edge_list& new_edges) noexcept {\n+    std::swap(next_edges_, new_edges);\n+  }", "path": "torch/csrc/autograd/function.h", "position": null, "original_position": 231, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "fbbfe675810f79e9d87f0239c892c3e16a23b976", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Eh can we just have a non-const accessor for next_edges (if you want to discourage its usage just say it's `next_edges_mut()`)? It seems silly to implement a method for every random kind of modification you want to do to this list. All of the `set_*`, `swap_*` can be easily expressed as vector operations, we don't get any safety, and we get an explosion in the number of methods. I'm ok with leaving the `set_*` ones, but I think this one is a bit too much", "created_at": "2018-02-15T16:48:47Z", "updated_at": "2018-11-23T15:39:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168537019", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168537019"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168537019"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>Eh can we just have a non-const accessor for next_edges (if you want to discourage its usage just say it's <code>next_edges_mut()</code>)? It seems silly to implement a method for every random kind of modification you want to do to this list. All of the <code>set_*</code>, <code>swap_*</code> can be easily expressed as vector operations, we don't get any safety, and we get an explosion in the number of methods. I'm ok with leaving the <code>set_*</code> ones, but I think this one is a bit too much</p>", "body_text": "Eh can we just have a non-const accessor for next_edges (if you want to discourage its usage just say it's next_edges_mut())? It seems silly to implement a method for every random kind of modification you want to do to this list. All of the set_*, swap_* can be easily expressed as vector operations, we don't get any safety, and we get an explosion in the number of methods. I'm ok with leaving the set_* ones, but I think this one is a bit too much"}