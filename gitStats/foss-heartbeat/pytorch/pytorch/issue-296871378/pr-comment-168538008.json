{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168538008", "pull_request_review_id": 96869478, "id": 168538008, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODUzODAwOA==", "diff_hunk": "@@ -23,164 +17,364 @@\n #include <cstdint>\n #include <initializer_list>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n namespace torch { namespace autograd {\n \n-struct Function;\n-struct Variable;\n struct Edge;\n+struct FunctionPostHook;\n+struct FunctionPreHook;\n \n using tensor_list = std::vector<at::Tensor>;\n using variable_list = std::vector<Variable>;\n-using function_list = std::vector<Edge>;\n+using edge_list = std::vector<Edge>;\n using saved_variable_list = std::vector<SavedVariable>;\n using IndexRange = std::pair<size_t, size_t>;\n \n-namespace detail {\n-struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {\n-  function_list next_functions;\n-  using IterArgs<MakeNextFunctionList>::operator();\n-  void operator()(const Variable& variable) {\n-    if (variable.defined()) {\n-      next_functions.push_back(variable.gradient_edge());\n-    } else {\n-      next_functions.emplace_back();\n-    }\n-  }\n-};\n-} // namespace detail\n-\n-// Returns true if any of the variables in the list require a gradient.\n-inline bool any_variable_requires_grad(const variable_list& variables) {\n-  return std::any_of(\n-      variables.begin(), variables.end(), [](const Variable& variable) {\n-        return variable.requires_grad();\n-      });\n-}\n-\n-template <typename... Variables>\n-function_list get_next_functions(Variables&&... variables) {\n-  if (!GradMode::is_enabled()) return {};\n-  detail::MakeNextFunctionList make;\n-  make.apply(std::forward<Variables>(variables)...);\n-  return std::move(make.next_functions);\n-}\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                               Function\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Function` is an abstract class that represents an operation taking zero\n+/// or more inputs and producing zero or more outputs. A function that wants to\n+/// interact with PyTorch's autograd machinery, i.e. take in `Variables` and be\n+/// connected to further `Functions` in a graph, should derive from this class\n+/// and override its `apply` method. Instances of such subclasses will then be\n+/// invokeable via the call operator.\n+///\n+///                    Functions in the Autograd Graph\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// When viewing the autograd system as a graph, `Function`s are the vertices\n+/// or nodes, connected to each other via (directed) `Edge`s, which themselves\n+/// are represented via the `Function` they are directed at and the particular\n+/// index of the edge when the Function is multivariate. `Variable`s are the\n+/// outputs to and inputs of `Function`s, and travel between these edges during\n+/// execution of the graph.\n+///\n+///                              Hierarchy\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Subclasses will often represent forward passes (functions) or backward\n+/// passes (derivatives). Note, however, that due to the very general\n+/// definition of a `Function` taking *zero* or more inputs and producing\n+/// *zero* or more outputs, uses of `Function`s are flexible and extend beyond\n+/// purely mathematical operations. For example, the `AccumulateGrad` function\n+/// is a *sink*: it takes one input, but produces no outputs, instead adding\n+/// the input to its internal accumulator as a side effect. At the other\n+/// extreme, the `GraphRoot` function receives no inputs from other functions,\n+/// but produces multiple outputs. Finally, the `Error` function takes neither\n+/// inputs nor outputs.\n+///\n+///                              Interface\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// The most important method on `Function` is the call operator, which takes\n+/// in a list of variables and produces a list of variables. The precise size\n+/// of these lists can be determined with `num_inputs()` and `num_outputs()`.\n+/// `Function`s are stitched together via their `next_edge` interface, which\n+/// let you manipulate the set of outgoing edges of a `Function`. You can add\n+/// an edge with `add_next_edge()`, retrieve an edge with `next_edge(index)`\n+/// and iterate over them via the `next_edges()` method. Other methods exist\n+/// for integration with the JIT and other parts of PyTorch. Every `Function`\n+/// also has a *sequence number* which increases monotonically in the order of\n+/// `Function` construction, which can be retrieved via the `sequence_number()`\n+/// getter.\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n struct Function : std::enable_shared_from_this<Function> {\n-  static thread_local uint64_t function_counter;\n+ public:\n+  /// Create a context edge for the JIT.\n+  static void set_up_context_edge(\n+      jit::Node* this_node,\n+      const variable_list& inputs,\n+      const variable_list& outputs);\n \n-  Function() : time(function_counter++) {}\n-  Function(function_list&& next_functions_) : Function() {\n-    next_functions = std::move(next_functions_);\n-  }\n+  /// Construct a new `Function` with `num_inputs` inputs and the given\n+  /// `next_edges`.\n+  explicit Function(\n+      uint32_t num_inputs = 0,\n+      edge_list&& next_edges = edge_list())\n+      : sequence_number_(function_counter_++),\n+        num_inputs_(num_inputs),\n+        next_edges_(std::move(next_edges)) {}\n \n+  /// Functions are neither copyable nor moveable.\n   Function(const Function& other) = delete;\n   Function(Function&& other) = delete;\n-  virtual ~Function() {}\n-\n-  // Implements the operation\n-  // NOTE: Don't call this function directly. Use operator() instead.\n-  virtual variable_list apply(const variable_list& inputs) = 0;\n-  variable_list tracedApply(variable_list inputs);\n+  Function& operator=(const Function& other) = delete;\n+  Function& operator=(Function&& other) = delete;\n+  virtual ~Function() = default;\n \n+  /// Evaluates the function on the given inputs and return the result of the\n+  /// function call.\n   variable_list operator()(const variable_list& inputs) {\n     profiler::RecordFunction rec(this);\n     if (jit::tracer::isTracingVar(inputs)) {\n-      return tracedApply(inputs);\n+      return traced_apply(inputs);\n     }\n     return apply(inputs);\n   }\n \n-  // PyFunctions are not managed by shared_ptrs by default, but are bound to the\n-  // lifetime of their Python object instead.\n-  virtual std::shared_ptr<Function> getSharedPtr() {\n-    return shared_from_this();\n-  };\n-\n-  // Releases saved variables if the operation won't be reused\n-  virtual inline void releaseVariables() {}\n-  // called before a an apply if will release variables is going to be called\n-  // allows larger ops like InterpreterAutogradFunction\n-  // to incrementally release variables as they run\n-  virtual inline void willReleaseVariables() {}\n-  // Function name for debugging\n-  virtual std::string name();\n+  // Graph Connectivity API\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  // Inputs\n \n-  bool should_compute_output(size_t index) const {\n-    TORCH_ASSERTM(index < next_functions.size(), \"Index out of range\");\n-    return next_functions[index].is_valid();\n+  /// Increments the number of inputs of the function and returns the previous\n+  /// value.\n+  uint32_t bump_inputs() noexcept {\n+    return num_inputs_++;\n   }\n \n-  bool should_compute_any_outputs() const {\n-    for (size_t i = 0; i < next_functions.size(); ++i) {\n-      if (should_compute_output(i)) {\n-        return true;\n-      }\n-    }\n-    return false;\n+  void set_num_inputs(uint32_t num_inputs) noexcept {\n+    num_inputs_ = num_inputs;\n   }\n \n-  bool should_compute_output(std::initializer_list<size_t> idxs) const {\n-    return std::any_of(idxs.begin(), idxs.end(), [this](size_t i) {\n-      return should_compute_output(i);\n-    });\n+  uint32_t num_inputs() const noexcept {\n+    return num_inputs_;\n+  }\n+\n+  // Outputs (\"Next Edges\")\n+\n+  const Edge& next_edge(size_t index) const noexcept {\n+    return next_edges_[index];\n+  }\n+\n+  void set_next_edge(size_t index, Edge edge) {\n+    next_edges_[index] = std::move(edge);\n+  }\n+\n+  void add_next_edge(Edge edge) {\n+    next_edges_.push_back(std::move(edge));\n+  }\n+\n+  void set_next_edges(edge_list&& next_edges) {\n+    next_edges_ = std::move(next_edges);\n+  }\n+\n+  void swap_next_edges(edge_list& new_edges) noexcept {\n+    std::swap(next_edges_, new_edges);\n+  }\n+\n+  const edge_list& next_edges() const noexcept {\n+    return next_edges_;\n+  }\n+\n+  uint32_t num_outputs() const noexcept {\n+    return next_edges_.size();\n+  }\n+\n+  // Miscellaneous Methods\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// The unique sequence number of this `Function`.\n+  uint64_t sequence_number() const noexcept {\n+    return sequence_number_;\n+  }\n+\n+  /// Returns a shared pointer to `this`. `PyFunction`s are not managed by\n+  /// `shared_ptr`s by default, but are bound to the lifetime of their Python\n+  /// object instead.\n+  virtual std::shared_ptr<Function> get_shared_ptr() {\n+    return shared_from_this();\n   }\n \n+  /// Returns the name of the dynamic type of the function, for debugging.\n+  virtual std::string name();\n+\n+  /// Returns true if the particular output edge is active, and that particular\n+  /// output of this function should be computed.\n+  bool should_compute_output(size_t output_edge_index) const {\n+    TORCH_ASSERTM(output_edge_index < num_outputs(), \"Index out of range\");\n+    return next_edges_[output_edge_index].is_valid();\n+  }\n+\n+  /// Returns true if any of the output edges in any of the ranges are active.\n   bool should_compute_output(std::initializer_list<IndexRange> idxs) const {\n     return std::any_of(idxs.begin(), idxs.end(), [this](IndexRange range) {\n-      for (size_t i = range.first; i < range.second; i++) {\n-        if (should_compute_output(i)) return true;\n+      for (auto i = range.first; i < range.second; i++) {\n+        if (should_compute_output(i))\n+          return true;\n       }\n       return false;\n     });\n   }\n \n-  void set_next_functions(function_list&& next_functions) {\n-    this->next_functions = std::move(next_functions);\n+  jit::tracer::FunctionTracingState& tracing_state() noexcept {\n+    // Dereferencing will create the `TracingState` if the pointer is empty.\n+    return *tracing_state_;\n+  }\n+\n+  /// Returns the `PyObject` stored for this `Function` (for Python\n+  /// interaction).\n+  PyObject* pyobj() const noexcept {\n+    return pyobj_;\n+  }\n+\n+  /// Sets the `PyObject` stored for this `Function` (for Python interaction).\n+  void set_pyobj(PyObject* pyobj) noexcept {\n+    pyobj_ = pyobj;\n+  }\n+\n+  // Hook API\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  void add_post_hook(std::unique_ptr<FunctionPostHook>&& post_hook) {\n+    post_hooks_.push_back(std::move(post_hook));\n+  }\n+\n+  const std::vector<std::unique_ptr<FunctionPostHook>>& post_hooks() const\n+      noexcept {\n+    return post_hooks_;\n+  }\n+\n+  void clear_post_hooks() {\n+    post_hooks_.clear();\n+  }\n+\n+  void add_pre_hook(std::unique_ptr<FunctionPreHook>&& pre_hook) {\n+    pre_hooks_.push_back(std::move(pre_hook));\n   }\n \n-  // An op is traceable if all operations happening within apply() are performed\n-  // on autograd Variables (i.e. apply mostly instantiates and applies other functions).\n-  virtual inline bool is_traceable() { return false; };\n+  const std::vector<std::unique_ptr<FunctionPreHook>>& pre_hooks() const\n+      noexcept {\n+    return pre_hooks_;\n+  }\n+\n+  void clear_pre_hooks() {\n+    pre_hooks_.clear();\n+  }\n+\n+  // Customization Points for Subclasses\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// Releases saved variables if the operation won't be reused.\n+  virtual void release_variables() {}\n+\n+  /// Called before an apply if `release_variables()` is going to be called.\n+  /// Allows larger ops like `InterpreterAutogradFunction` to incrementally\n+  /// release variables as they run.\n+  virtual void will_release_variables() {}\n+\n+  /// Returns true if this function is traceable. An op is traceable if all\n+  /// operations happening within `apply()` are performed on autograd\n+  /// `Variables` (i.e. apply mostly instantiates and applies other functions).\n+  virtual bool is_traceable() {\n+    return false;\n+  }\n \n-  // An op is said to pass state transparently to backward, if the state consists\n-  // only of (Saved)Variables and only non-variable objects that parametrize the\n-  // operation in some way that defines the graph structure AND the backward function\n-  // is traceable. In particular, parametrization MUST NOT depend on the data\n-  // of any Variable.\n-  // TODO: it might be possible to handle cases where backward is non-traceable\n-  // but state passing could be considered transparent. This will probably depend\n-  // on saved_variable_list being mutable.\n-  // NOTE: this value matters only if is_traceable() returns false.\n-  virtual inline bool passes_state_transparently() { return false; };\n+  /// A `Function` is said to pass state transparently to backward, if the\n+  /// state consists only of (Saved)Variables and only non-variable objects\n+  /// that parameterize the operation in some way that defines the graph\n+  /// structure AND the backward function is traceable. In particular,\n+  /// parametrization MUST NOT depend on the data of any `Variable`.\n+  /// TODO: it might be possible to handle cases where backward is\n+  /// non-traceable but state passing could be considered transparent. This\n+  /// will probably depend on saved_variable_list being mutable.\n+  /// NOTE: this value matters only if is_traceable() returns false.\n+  virtual bool passes_state_transparently() {\n+    return false;\n+  }\n \n-  // Let's the JIT find inputs to apply that are not present explicitly in arguments.\n-  // Required only for functions that are not traceable, don't pass state to\n-  // backward transparently, and are not backwards closures of functions that don't\n-  // pass the state transparently. Which means that hopefully they will hardly ever\n-  // need to be implemented :)\n-  virtual inline std::unique_ptr<saved_variable_list> saved_variables() { return nullptr; }\n+  /// Returns `Variable`s saved by this `Function`.\n+  /// This let's the JIT find inputs to apply that are not present explicitly\n+  /// in arguments. Required only for functions that are not traceable, don't\n+  /// pass state to backward transparently, and are not backwards closures of\n+  /// functions that don't pass the state transparently. Which means that\n+  /// hopefully they will hardly ever need to be implemented :)\n+  virtual std::unique_ptr<saved_variable_list> saved_variables() {\n+    return nullptr;\n+  }\n+\n+ protected:\n+  /// Monotonically incrementing function counter to supply sequence numbers.", "path": "torch/csrc/autograd/function.h", "position": null, "original_position": 387, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "fbbfe675810f79e9d87f0239c892c3e16a23b976", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Same comment as previously", "created_at": "2018-02-15T16:51:24Z", "updated_at": "2018-11-23T15:39:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168538008", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168538008"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168538008"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>Same comment as previously</p>", "body_text": "Same comment as previously"}