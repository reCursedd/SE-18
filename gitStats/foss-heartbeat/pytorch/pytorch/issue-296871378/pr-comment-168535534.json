{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168535534", "pull_request_review_id": 96869478, "id": 168535534, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODUzNTUzNA==", "diff_hunk": "@@ -23,164 +17,364 @@\n #include <cstdint>\n #include <initializer_list>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n namespace torch { namespace autograd {\n \n-struct Function;\n-struct Variable;\n struct Edge;\n+struct FunctionPostHook;\n+struct FunctionPreHook;\n \n using tensor_list = std::vector<at::Tensor>;\n using variable_list = std::vector<Variable>;\n-using function_list = std::vector<Edge>;\n+using edge_list = std::vector<Edge>;\n using saved_variable_list = std::vector<SavedVariable>;\n using IndexRange = std::pair<size_t, size_t>;\n \n-namespace detail {\n-struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {\n-  function_list next_functions;\n-  using IterArgs<MakeNextFunctionList>::operator();\n-  void operator()(const Variable& variable) {\n-    if (variable.defined()) {\n-      next_functions.push_back(variable.gradient_edge());\n-    } else {\n-      next_functions.emplace_back();\n-    }\n-  }\n-};\n-} // namespace detail\n-\n-// Returns true if any of the variables in the list require a gradient.\n-inline bool any_variable_requires_grad(const variable_list& variables) {\n-  return std::any_of(\n-      variables.begin(), variables.end(), [](const Variable& variable) {\n-        return variable.requires_grad();\n-      });\n-}\n-\n-template <typename... Variables>\n-function_list get_next_functions(Variables&&... variables) {\n-  if (!GradMode::is_enabled()) return {};\n-  detail::MakeNextFunctionList make;\n-  make.apply(std::forward<Variables>(variables)...);\n-  return std::move(make.next_functions);\n-}\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                               Function\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Function` is an abstract class that represents an operation taking zero\n+/// or more inputs and producing zero or more outputs. A function that wants to\n+/// interact with PyTorch's autograd machinery, i.e. take in `Variables` and be\n+/// connected to further `Functions` in a graph, should derive from this class\n+/// and override its `apply` method. Instances of such subclasses will then be\n+/// invokeable via the call operator.\n+///\n+///                    Functions in the Autograd Graph\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// When viewing the autograd system as a graph, `Function`s are the vertices\n+/// or nodes, connected to each other via (directed) `Edge`s, which themselves\n+/// are represented via the `Function` they are directed at and the particular\n+/// index of the edge when the Function is multivariate. `Variable`s are the\n+/// outputs to and inputs of `Function`s, and travel between these edges during\n+/// execution of the graph.\n+///\n+///                              Hierarchy\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Subclasses will often represent forward passes (functions) or backward\n+/// passes (derivatives). Note, however, that due to the very general\n+/// definition of a `Function` taking *zero* or more inputs and producing\n+/// *zero* or more outputs, uses of `Function`s are flexible and extend beyond\n+/// purely mathematical operations. For example, the `AccumulateGrad` function\n+/// is a *sink*: it takes one input, but produces no outputs, instead adding\n+/// the input to its internal accumulator as a side effect. At the other\n+/// extreme, the `GraphRoot` function receives no inputs from other functions,\n+/// but produces multiple outputs. Finally, the `Error` function takes neither\n+/// inputs nor outputs.\n+///\n+///                              Interface\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// The most important method on `Function` is the call operator, which takes\n+/// in a list of variables and produces a list of variables. The precise size\n+/// of these lists can be determined with `num_inputs()` and `num_outputs()`.\n+/// `Function`s are stitched together via their `next_edge` interface, which\n+/// let you manipulate the set of outgoing edges of a `Function`. You can add\n+/// an edge with `add_next_edge()`, retrieve an edge with `next_edge(index)`\n+/// and iterate over them via the `next_edges()` method. Other methods exist\n+/// for integration with the JIT and other parts of PyTorch. Every `Function`\n+/// also has a *sequence number* which increases monotonically in the order of\n+/// `Function` construction, which can be retrieved via the `sequence_number()`\n+/// getter.\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n struct Function : std::enable_shared_from_this<Function> {\n-  static thread_local uint64_t function_counter;\n+ public:\n+  /// Create a context edge for the JIT.\n+  static void set_up_context_edge(\n+      jit::Node* this_node,\n+      const variable_list& inputs,\n+      const variable_list& outputs);", "path": "torch/csrc/autograd/function.h", "position": 298, "original_position": 125, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "fbbfe675810f79e9d87f0239c892c3e16a23b976", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can we put everything that's not the crucial part of the interface at the bottom? Very few people will be interested in this particular method", "created_at": "2018-02-15T16:44:41Z", "updated_at": "2018-11-23T15:39:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168535534", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168535534"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r168535534"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>Can we put everything that's not the crucial part of the interface at the bottom? Very few people will be interested in this particular method</p>", "body_text": "Can we put everything that's not the crucial part of the interface at the bottom? Very few people will be interested in this particular method"}