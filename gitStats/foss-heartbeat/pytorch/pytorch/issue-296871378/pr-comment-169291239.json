{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169291239", "pull_request_review_id": 97791061, "id": 169291239, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2OTI5MTIzOQ==", "diff_hunk": "@@ -23,164 +17,360 @@\n #include <cstdint>\n #include <initializer_list>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n namespace torch { namespace autograd {\n \n-struct Function;\n-struct Variable;\n struct Edge;\n+struct FunctionPostHook;\n+struct FunctionPreHook;\n \n using tensor_list = std::vector<at::Tensor>;\n using variable_list = std::vector<Variable>;\n-using function_list = std::vector<Edge>;\n+using edge_list = std::vector<Edge>;\n using saved_variable_list = std::vector<SavedVariable>;\n using IndexRange = std::pair<size_t, size_t>;\n \n-namespace detail {\n-struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {\n-  function_list next_functions;\n-  using IterArgs<MakeNextFunctionList>::operator();\n-  void operator()(const Variable& variable) {\n-    if (variable.defined()) {\n-      next_functions.push_back(variable.gradient_edge());\n-    } else {\n-      next_functions.emplace_back();\n-    }\n-  }\n-};\n-} // namespace detail\n-\n-// Returns true if any of the variables in the list require a gradient.\n-inline bool any_variable_requires_grad(const variable_list& variables) {\n-  return std::any_of(\n-      variables.begin(), variables.end(), [](const Variable& variable) {\n-        return variable.requires_grad();\n-      });\n-}\n-\n-template <typename... Variables>\n-function_list get_next_functions(Variables&&... variables) {\n-  if (!GradMode::is_enabled()) return {};\n-  detail::MakeNextFunctionList make;\n-  make.apply(std::forward<Variables>(variables)...);\n-  return std::move(make.next_functions);\n-}\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                               Function\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Function` is an abstract class that represents an operation taking zero\n+/// or more input `Variable`s and producing zero or more output `Variable`s. All\n+/// functions in PyTorch's autograd machinery derive from this class and\n+/// override its `apply` method. Instances of such subclasses will then be\n+/// invokeable via the call operator.\n+///\n+///                    Functions in the Autograd Graph\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// When viewing the autograd system as a graph, `Function`s are the vertices or\n+/// nodes, connected to each other via (directed) `Edge`s, which themselves are\n+/// represented via (`Function`, input_nr) pairs. `Variable`s are the outputs to\n+/// and inputs of `Function`s, and travel between these edges during execution\n+/// of the graph. When two or more `Edge`s (from different sources) point at the\n+/// same input to a `Function`, the values produced along all of these edges are\n+/// implicitly summed prior to being forwarded to the target `Function`.\n+///\n+///                              Hierarchy\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Subclasses will often represent forward passes (functions) or backward\n+/// passes (derivatives). Note, however, that due to the very general\n+/// definition of a `Function` taking *zero* or more inputs and producing\n+/// *zero* or more outputs, uses of `Function`s are flexible and extend beyond\n+/// purely mathematical operations. For example, the `AccumulateGrad` function\n+/// is a *sink*: it takes one input, but produces no outputs, instead adding\n+/// the input to its internal accumulator as a side effect. At the other", "path": "torch/csrc/autograd/function.h", "position": null, "original_position": 98, "commit_id": "2820cf70a401e759ae67f8169c15f64b6dd628d0", "original_commit_id": "d162ddcf5c0dbce3018ca1084c7999a40b59ba07", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "nit: there's no *internal accumulator* in AccumulateGrad. It takes it's `var` reference, traverses to its `.grad` attr, and adds it there.", "created_at": "2018-02-20T11:33:02Z", "updated_at": "2018-11-23T15:39:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/5221#discussion_r169291239", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5221", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169291239"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5221#discussion_r169291239"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5221"}}, "body_html": "<p>nit: there's no <em>internal accumulator</em> in AccumulateGrad. It takes it's <code>var</code> reference, traverses to its <code>.grad</code> attr, and adds it there.</p>", "body_text": "nit: there's no internal accumulator in AccumulateGrad. It takes it's var reference, traverses to its .grad attr, and adds it there."}