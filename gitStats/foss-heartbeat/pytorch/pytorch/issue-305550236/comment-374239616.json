{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/374239616", "html_url": "https://github.com/pytorch/pytorch/issues/5810#issuecomment-374239616", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5810", "id": 374239616, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDIzOTYxNg==", "user": {"login": "niwtr", "id": 10781892, "node_id": "MDQ6VXNlcjEwNzgxODky", "avatar_url": "https://avatars3.githubusercontent.com/u/10781892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/niwtr", "html_url": "https://github.com/niwtr", "followers_url": "https://api.github.com/users/niwtr/followers", "following_url": "https://api.github.com/users/niwtr/following{/other_user}", "gists_url": "https://api.github.com/users/niwtr/gists{/gist_id}", "starred_url": "https://api.github.com/users/niwtr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/niwtr/subscriptions", "organizations_url": "https://api.github.com/users/niwtr/orgs", "repos_url": "https://api.github.com/users/niwtr/repos", "events_url": "https://api.github.com/users/niwtr/events{/privacy}", "received_events_url": "https://api.github.com/users/niwtr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-19T14:51:38Z", "updated_at": "2018-03-19T14:51:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a> Many thanks, very enlightening! I'm following the same way with you and wrote a piece of workable code:</p>\n<pre><code>import torch\nfrom torch.distributions import Categorical\ndef heterogeneous_multinomial(probs, total_counts):\n    samples = torch.zeros(probs.shape[0], probs.shape[1] + 1)\n    total_count = total_counts.max().int().item()\n    cate_samples = Categorical(probs).sample((total_count,)).t() + 1\n    mask = (torch.arange(total_count).unsqueeze(0).expand_as(cate_samples) &lt; total_counts.unsqueeze(1)).long()\n    cate_samples *= mask\n    samples.scatter_add_(-1, cate_samples, torch.ones_like(cate_samples.float()))\n    samples = samples[..., 1:]\n    return samples\n</code></pre>\n<p>Here an example:</p>\n<pre><code>&gt;&gt;&gt; probs = torch.Tensor([[1/3., 1/3., 1/3.], [1/3., 1/3., 1/3.]])\n&gt;&gt;&gt; cs = torch.Tensor([3,5])\n&gt;&gt;&gt; x = heterogeneous_multinomial(probs, cs)\n&gt;&gt;&gt; print(x)\n\n 2  0  1\n 0  3  2\n[torch.FloatTensor of size (2,3)]\n</code></pre>\n<p>It's still slow enough, caching the <code>samples</code> and <code>mask</code> would help.</p>", "body_text": "@fritzo Many thanks, very enlightening! I'm following the same way with you and wrote a piece of workable code:\nimport torch\nfrom torch.distributions import Categorical\ndef heterogeneous_multinomial(probs, total_counts):\n    samples = torch.zeros(probs.shape[0], probs.shape[1] + 1)\n    total_count = total_counts.max().int().item()\n    cate_samples = Categorical(probs).sample((total_count,)).t() + 1\n    mask = (torch.arange(total_count).unsqueeze(0).expand_as(cate_samples) < total_counts.unsqueeze(1)).long()\n    cate_samples *= mask\n    samples.scatter_add_(-1, cate_samples, torch.ones_like(cate_samples.float()))\n    samples = samples[..., 1:]\n    return samples\n\nHere an example:\n>>> probs = torch.Tensor([[1/3., 1/3., 1/3.], [1/3., 1/3., 1/3.]])\n>>> cs = torch.Tensor([3,5])\n>>> x = heterogeneous_multinomial(probs, cs)\n>>> print(x)\n\n 2  0  1\n 0  3  2\n[torch.FloatTensor of size (2,3)]\n\nIt's still slow enough, caching the samples and mask would help.", "body": "@fritzo Many thanks, very enlightening! I'm following the same way with you and wrote a piece of workable code:\r\n```\r\nimport torch\r\nfrom torch.distributions import Categorical\r\ndef heterogeneous_multinomial(probs, total_counts):\r\n    samples = torch.zeros(probs.shape[0], probs.shape[1] + 1)\r\n    total_count = total_counts.max().int().item()\r\n    cate_samples = Categorical(probs).sample((total_count,)).t() + 1\r\n    mask = (torch.arange(total_count).unsqueeze(0).expand_as(cate_samples) < total_counts.unsqueeze(1)).long()\r\n    cate_samples *= mask\r\n    samples.scatter_add_(-1, cate_samples, torch.ones_like(cate_samples.float()))\r\n    samples = samples[..., 1:]\r\n    return samples\r\n```\r\nHere an example:\r\n```\r\n>>> probs = torch.Tensor([[1/3., 1/3., 1/3.], [1/3., 1/3., 1/3.]])\r\n>>> cs = torch.Tensor([3,5])\r\n>>> x = heterogeneous_multinomial(probs, cs)\r\n>>> print(x)\r\n\r\n 2  0  1\r\n 0  3  2\r\n[torch.FloatTensor of size (2,3)]\r\n```\r\nIt's still slow enough, caching the `samples` and `mask` would help."}