{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/374039253", "html_url": "https://github.com/pytorch/pytorch/issues/5810#issuecomment-374039253", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5810", "id": 374039253, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDAzOTI1Mw==", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-18T19:56:02Z", "updated_at": "2018-03-18T19:57:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10781892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/niwtr\">@niwtr</a> First note that <code>torch.multinomial</code> is confusingly named: it samples from a categorical/discrete distribution and does not perform the final aggregation needed to sample from a multinomial distribution. I will assume that you actually want to sample from a multinomial distribution.</p>\n<p>For an implementation of a vectorized multinomial sampler with homogeneous <code>total_count</code>, see <a href=\"https://github.com/probtorch/pytorch/blob/master/torch/distributions/multinomial.py\">torch.distributions.Multinomial.sample()</a>. The basic trick is to use <code>.scatter_add_()</code>. To implement heterogeneous <code>total_count</code>, you could can wrap <code>torch.multinomial</code> by adding an extra \"bogus\" category with probability zero, something like this (I haven't tried to run this code):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">heterogeneous_multinomial</span>(<span class=\"pl-smi\">probs</span>, <span class=\"pl-smi\">total_counts</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Draw a categorical sample from a distribution with an extra bogus bin.</span>\n    extended_probs <span class=\"pl-k\">=</span> torch.cat([torch.zeros(probs.shape[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span>,), probs], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    total_count = total_counts.max().item()\n    extended_categorical_sample = torch.multinomial(\n        extended_probs, total_counts.max().item())\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Move extra samples into the bogus bin.</span>\n    extended_categorical_sample <span class=\"pl-k\">*</span>= (torch.arange(total_count) <span class=\"pl-k\">&lt;</span> total_counts)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Aggregate categorical sample into a multinomial sample.</span>\n    extended_multinomial_sample = torch.zeros_like(extended_probs)\n    extended_multinomial_sample.scatter_add_(\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, extended_categorical_sample, torch.ones_like(extended_categorical_sample))\n    multinomial_sample = extended_multinomial_sample\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Discard the bogus bin.</span>\n    multinomial_sample = extended_multinomial_sample[<span class=\"pl-c1\">...</span>, <span class=\"pl-c1\">1</span>:]\n    <span class=\"pl-k\">assert</span> multinomial_sample.shape <span class=\"pl-k\">==</span> probs.shape\n    <span class=\"pl-k\">return</span> multinomial_sample</pre></div>", "body_text": "@niwtr First note that torch.multinomial is confusingly named: it samples from a categorical/discrete distribution and does not perform the final aggregation needed to sample from a multinomial distribution. I will assume that you actually want to sample from a multinomial distribution.\nFor an implementation of a vectorized multinomial sampler with homogeneous total_count, see torch.distributions.Multinomial.sample(). The basic trick is to use .scatter_add_(). To implement heterogeneous total_count, you could can wrap torch.multinomial by adding an extra \"bogus\" category with probability zero, something like this (I haven't tried to run this code):\ndef heterogeneous_multinomial(probs, total_counts):\n    # Draw a categorical sample from a distribution with an extra bogus bin.\n    extended_probs = torch.cat([torch.zeros(probs.shape[:-1] + (1,), probs], -1)\n    total_count = total_counts.max().item()\n    extended_categorical_sample = torch.multinomial(\n        extended_probs, total_counts.max().item())\n\n    # Move extra samples into the bogus bin.\n    extended_categorical_sample *= (torch.arange(total_count) < total_counts)\n\n    # Aggregate categorical sample into a multinomial sample.\n    extended_multinomial_sample = torch.zeros_like(extended_probs)\n    extended_multinomial_sample.scatter_add_(\n        -1, extended_categorical_sample, torch.ones_like(extended_categorical_sample))\n    multinomial_sample = extended_multinomial_sample\n\n    # Discard the bogus bin.\n    multinomial_sample = extended_multinomial_sample[..., 1:]\n    assert multinomial_sample.shape == probs.shape\n    return multinomial_sample", "body": "@niwtr First note that `torch.multinomial` is confusingly named: it samples from a categorical/discrete distribution and does not perform the final aggregation needed to sample from a multinomial distribution. I will assume that you actually want to sample from a multinomial distribution.\r\n\r\nFor an implementation of a vectorized multinomial sampler with homogeneous `total_count`, see [torch.distributions.Multinomial.sample()](https://github.com/probtorch/pytorch/blob/master/torch/distributions/multinomial.py). The basic trick is to use `.scatter_add_()`. To implement heterogeneous `total_count`, you could can wrap `torch.multinomial` by adding an extra \"bogus\" category with probability zero, something like this (I haven't tried to run this code):\r\n```py\r\ndef heterogeneous_multinomial(probs, total_counts):\r\n    # Draw a categorical sample from a distribution with an extra bogus bin.\r\n    extended_probs = torch.cat([torch.zeros(probs.shape[:-1] + (1,), probs], -1)\r\n    total_count = total_counts.max().item()\r\n    extended_categorical_sample = torch.multinomial(\r\n        extended_probs, total_counts.max().item())\r\n\r\n    # Move extra samples into the bogus bin.\r\n    extended_categorical_sample *= (torch.arange(total_count) < total_counts)\r\n\r\n    # Aggregate categorical sample into a multinomial sample.\r\n    extended_multinomial_sample = torch.zeros_like(extended_probs)\r\n    extended_multinomial_sample.scatter_add_(\r\n        -1, extended_categorical_sample, torch.ones_like(extended_categorical_sample))\r\n    multinomial_sample = extended_multinomial_sample\r\n\r\n    # Discard the bogus bin.\r\n    multinomial_sample = extended_multinomial_sample[..., 1:]\r\n    assert multinomial_sample.shape == probs.shape\r\n    return multinomial_sample\r\n```"}