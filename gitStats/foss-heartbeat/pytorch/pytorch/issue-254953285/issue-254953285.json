{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2610", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2610/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2610/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2610/events", "html_url": "https://github.com/pytorch/pytorch/issues/2610", "id": 254953285, "node_id": "MDU6SXNzdWUyNTQ5NTMyODU=", "number": 2610, "title": "DataParallel only can backwards when gpu id starts from 0", "user": {"login": "shuten-douji", "id": 23015186, "node_id": "MDQ6VXNlcjIzMDE1MTg2", "avatar_url": "https://avatars0.githubusercontent.com/u/23015186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuten-douji", "html_url": "https://github.com/shuten-douji", "followers_url": "https://api.github.com/users/shuten-douji/followers", "following_url": "https://api.github.com/users/shuten-douji/following{/other_user}", "gists_url": "https://api.github.com/users/shuten-douji/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuten-douji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuten-douji/subscriptions", "organizations_url": "https://api.github.com/users/shuten-douji/orgs", "repos_url": "https://api.github.com/users/shuten-douji/repos", "events_url": "https://api.github.com/users/shuten-douji/events{/privacy}", "received_events_url": "https://api.github.com/users/shuten-douji/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-04T07:09:58Z", "updated_at": "2017-09-04T10:28:00Z", "closed_at": "2017-09-04T10:28:00Z", "author_association": "NONE", "body_html": "<p>I'm trying to use dataparallel in multi-gpu, but get trouble in backward if I set the device_ids to [5, 6] other than [0, 1].Here's the code:</p>\n<p>`<br>\nimport torch<br>\nimport torch.nn as nn<br>\nfrom torch.autograd import Variable</p>\n<pre><code>class select(nn.Module):\n    def __init__(self):\n        super(select, self).__init__()\n        self.conv = nn.Conv2d(1, 1, kernel_size=1)\n\n    def forward(self, x):\n        print x\n        return self.conv(x)\n\nmodel = select().cuda(5)\nc = nn.L1Loss().cuda(5)\nmodel = nn.DataParallel(model, device_ids=[5, 6])\nx = Variable(torch.Tensor(torch.randn(2, 1, 3, 3)))\noutput = model(x)\ny = c(output, x.cuda(5, async=True))\nprint y\ny.backward()\n</code></pre>\n<p>`</p>\n<p>The error information is:</p>\n<p>Variable containing:<br>\n(0 ,0 ,.,.) =<br>\n0.0397 -0.7899  0.0385<br>\n-0.2171 -0.5417 -0.2214<br>\n0.1241  0.0371 -0.0182<br>\n[torch.cuda.FloatTensor of size 1x1x3x3 (GPU 5)]<br>\nVariable containing:<br>\n(0 ,0 ,.,.) =<br>\n0.5447 -0.9738  0.4060<br>\n-1.8595 -0.1073  0.2626<br>\n-1.5308 -0.8662  0.4490<br>\n[torch.cuda.FloatTensor of size 1x1x3x3 (GPU 6)]</p>\n<p>Variable containing:<br>\n1.1432<br>\n[torch.cuda.FloatTensor of size 1 (GPU 5)]</p>\n<p>Traceback (most recent call last):<br>\nFile \"/home/sby/workspace/Cnn3d/test/test_dataparallel.py\", line 21, in <br>\ny.backward()<br>\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br>\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/autograd/<strong>init</strong>.py\", line 98, in backward<br>\nvariables, grad_variables, retain_graph)<br>\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 25, in backward<br>\nreturn comm.reduce_add_coalesced(grad_outputs, self.input_device)<br>\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 122, in reduce_add_coalesced<br>\nresult = reduce_add(flattened, destination)<br>\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 92, in reduce_add<br>\nnccl.reduce(inputs, outputs, root=destination)<br>\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 161, in reduce<br>\nassert(root &gt;= 0 and root &lt; len(inputs))<br>\nAssertionError</p>\n<p>Process finished with exit code 1</p>\n<p>But the code runs well if it is like:<br>\n`<br>\nimport torch<br>\nimport torch.nn as nn<br>\nfrom torch.autograd import Variable</p>\n<pre><code>class select(nn.Module):\n    def __init__(self):\n        super(select, self).__init__()\n        self.conv = nn.Conv2d(1, 1, kernel_size=1)\n\n    def forward(self, x):\n        print x\n        return self.conv(x)\n\nmodel = select().cuda(0)\nc = nn.L1Loss().cuda(0)\nmodel = nn.DataParallel(model, device_ids=[0, 1])\nx = Variable(torch.Tensor(torch.randn(2, 1, 3, 3)))\noutput = model(x)\ny = c(output, x.cuda(0, async=True))\nprint y\ny.backward()\n</code></pre>\n<p>`</p>", "body_text": "I'm trying to use dataparallel in multi-gpu, but get trouble in backward if I set the device_ids to [5, 6] other than [0, 1].Here's the code:\n`\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nclass select(nn.Module):\n    def __init__(self):\n        super(select, self).__init__()\n        self.conv = nn.Conv2d(1, 1, kernel_size=1)\n\n    def forward(self, x):\n        print x\n        return self.conv(x)\n\nmodel = select().cuda(5)\nc = nn.L1Loss().cuda(5)\nmodel = nn.DataParallel(model, device_ids=[5, 6])\nx = Variable(torch.Tensor(torch.randn(2, 1, 3, 3)))\noutput = model(x)\ny = c(output, x.cuda(5, async=True))\nprint y\ny.backward()\n\n`\nThe error information is:\nVariable containing:\n(0 ,0 ,.,.) =\n0.0397 -0.7899  0.0385\n-0.2171 -0.5417 -0.2214\n0.1241  0.0371 -0.0182\n[torch.cuda.FloatTensor of size 1x1x3x3 (GPU 5)]\nVariable containing:\n(0 ,0 ,.,.) =\n0.5447 -0.9738  0.4060\n-1.8595 -0.1073  0.2626\n-1.5308 -0.8662  0.4490\n[torch.cuda.FloatTensor of size 1x1x3x3 (GPU 6)]\nVariable containing:\n1.1432\n[torch.cuda.FloatTensor of size 1 (GPU 5)]\nTraceback (most recent call last):\nFile \"/home/sby/workspace/Cnn3d/test/test_dataparallel.py\", line 21, in \ny.backward()\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/autograd/init.py\", line 98, in backward\nvariables, grad_variables, retain_graph)\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 25, in backward\nreturn comm.reduce_add_coalesced(grad_outputs, self.input_device)\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 122, in reduce_add_coalesced\nresult = reduce_add(flattened, destination)\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 92, in reduce_add\nnccl.reduce(inputs, outputs, root=destination)\nFile \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 161, in reduce\nassert(root >= 0 and root < len(inputs))\nAssertionError\nProcess finished with exit code 1\nBut the code runs well if it is like:\n`\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nclass select(nn.Module):\n    def __init__(self):\n        super(select, self).__init__()\n        self.conv = nn.Conv2d(1, 1, kernel_size=1)\n\n    def forward(self, x):\n        print x\n        return self.conv(x)\n\nmodel = select().cuda(0)\nc = nn.L1Loss().cuda(0)\nmodel = nn.DataParallel(model, device_ids=[0, 1])\nx = Variable(torch.Tensor(torch.randn(2, 1, 3, 3)))\noutput = model(x)\ny = c(output, x.cuda(0, async=True))\nprint y\ny.backward()\n\n`", "body": "I'm trying to use dataparallel in multi-gpu, but get trouble in backward if I set the device_ids to [5, 6] other than [0, 1].Here's the code:\r\n\r\n`\r\n    import torch\r\n    import torch.nn as nn\r\n    from torch.autograd import Variable\r\n\r\n    class select(nn.Module):\r\n        def __init__(self):\r\n            super(select, self).__init__()\r\n            self.conv = nn.Conv2d(1, 1, kernel_size=1)\r\n\r\n        def forward(self, x):\r\n            print x\r\n            return self.conv(x)\r\n\r\n    model = select().cuda(5)\r\n    c = nn.L1Loss().cuda(5)\r\n    model = nn.DataParallel(model, device_ids=[5, 6])\r\n    x = Variable(torch.Tensor(torch.randn(2, 1, 3, 3)))\r\n    output = model(x)\r\n    y = c(output, x.cuda(5, async=True))\r\n    print y\r\n    y.backward()\r\n`\r\n\r\nThe error information is:\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.0397 -0.7899  0.0385\r\n -0.2171 -0.5417 -0.2214\r\n  0.1241  0.0371 -0.0182\r\n[torch.cuda.FloatTensor of size 1x1x3x3 (GPU 5)]\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.5447 -0.9738  0.4060\r\n -1.8595 -0.1073  0.2626\r\n -1.5308 -0.8662  0.4490\r\n[torch.cuda.FloatTensor of size 1x1x3x3 (GPU 6)]\r\n\r\n\r\nVariable containing:\r\n 1.1432\r\n[torch.cuda.FloatTensor of size 1 (GPU 5)]\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/sby/workspace/Cnn3d/test/test_dataparallel.py\", line 21, in <module>\r\n    y.backward()\r\n  File \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 25, in backward\r\n    return comm.reduce_add_coalesced(grad_outputs, self.input_device)\r\n  File \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 122, in reduce_add_coalesced\r\n    result = reduce_add(flattened, destination)\r\n  File \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 92, in reduce_add\r\n    nccl.reduce(inputs, outputs, root=destination)\r\n  File \"/home/sby/anaconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 161, in reduce\r\n    assert(root >= 0 and root < len(inputs))\r\nAssertionError\r\n\r\nProcess finished with exit code 1\r\n\r\nBut the code runs well if it is like:\r\n`\r\n    import torch\r\n    import torch.nn as nn\r\n    from torch.autograd import Variable\r\n\r\n    class select(nn.Module):\r\n        def __init__(self):\r\n            super(select, self).__init__()\r\n            self.conv = nn.Conv2d(1, 1, kernel_size=1)\r\n\r\n        def forward(self, x):\r\n            print x\r\n            return self.conv(x)\r\n\r\n    model = select().cuda(0)\r\n    c = nn.L1Loss().cuda(0)\r\n    model = nn.DataParallel(model, device_ids=[0, 1])\r\n    x = Variable(torch.Tensor(torch.randn(2, 1, 3, 3)))\r\n    output = model(x)\r\n    y = c(output, x.cuda(0, async=True))\r\n    print y\r\n    y.backward()\r\n`"}