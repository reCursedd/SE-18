{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163385520", "pull_request_review_id": 90989916, "id": 163385520, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzM4NTUyMA==", "diff_hunk": "@@ -200,30 +186,43 @@ def once_differentiable(fn):\n \n     @functools.wraps(fn)\n     def wrapper(ctx, *args):\n-        tensor_args = [arg.data if isinstance(arg, Variable) else arg\n-                       for arg in args]\n-        outputs = fn(ctx, *tensor_args)\n-        # XXX: this is only an approximation of these flags - there's no way\n+        with torch.no_grad():\n+            outputs = fn(ctx, *args)\n+\n+        if not torch.is_grad_enabled():\n+            return outputs\n+\n+        # If any of the inputs have requires_grad=True, we force the outputs\n+        # to have requires_grad=True but point to a grad_fn which throws an\n+        # error message during (double) back-propagation.\n+        # XXX: this is only an approximation of requires_grad - there's no way\n         # to figure out if fn didn't use ctx.saved_variables and as a result\n         # some Variables might require grad, even if no args do.\n         # Unfortunately, this leads to unexpected error messages (\"no nodes\n         # require computing gradients\"), but I don't have a better idea.\n         # These functions would raise an error in backward anyway.\n-        requires_grad = any(arg.requires_grad if isinstance(arg, Variable) else False\n+        requires_grad = any(isinstance(arg, Variable) and arg.requires_grad\n                             for arg in args)\n-        if not torch.is_grad_enabled():\n-            def err_fn(*args):\n-                return args\n-        else:\n-            err_fn = torch._C._functions.DelayedError(\n-                b\"trying to differentiate twice a function that was marked\"\n-                b\"with @once_differentiable\")\n+        if not requires_grad:\n+            return outputs\n+\n+        err_fn = torch._C._functions.DelayedError(\n+            b\"trying to differentiate twice a function that was marked\"\n+            b\"with @once_differentiable\")\n+\n         if not isinstance(outputs, tuple):\n-            var = (Variable(outputs, requires_grad=requires_grad)\n-                   if outputs is not None else None)\n-            return err_fn(var)\n-        return err_fn(*[Variable(o, requires_grad=requires_grad) if o is not None else None\n-                      for o in outputs])\n+            outputs = (outputs,)\n+\n+        # Create aliases of each output that has requires_grad=True. We need\n+        # at least one of the inputs to err_fn to require grad so that the\n+        # output will have a grad_fn.\n+        def fake_requires_grad(var):\n+            if var is not None:\n+                var = var.detach()\n+                var.requires_grad = True", "path": "torch/autograd/function.py", "position": 91, "original_position": 90, "commit_id": "a52a6a4055e7bc0053fbc4c65f217b3cb39f0b29", "original_commit_id": "296394a9359ec2a3504b3bc155077d6a90bbf7e5", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "will they become leaves now? (and will accumulate grad)", "created_at": "2018-01-23T21:40:52Z", "updated_at": "2018-11-23T15:38:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/4786#discussion_r163385520", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4786", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163385520"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4786#discussion_r163385520"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4786"}}, "body_html": "<p>will they become leaves now? (and will accumulate grad)</p>", "body_text": "will they become leaves now? (and will accumulate grad)"}