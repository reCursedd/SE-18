{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163389455", "pull_request_review_id": 90996111, "id": 163389455, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzM4OTQ1NQ==", "diff_hunk": "@@ -200,30 +186,43 @@ def once_differentiable(fn):\n \n     @functools.wraps(fn)\n     def wrapper(ctx, *args):\n-        tensor_args = [arg.data if isinstance(arg, Variable) else arg\n-                       for arg in args]\n-        outputs = fn(ctx, *tensor_args)\n-        # XXX: this is only an approximation of these flags - there's no way\n+        with torch.no_grad():\n+            outputs = fn(ctx, *args)\n+\n+        if not torch.is_grad_enabled():\n+            return outputs\n+\n+        # If any of the inputs have requires_grad=True, we force the outputs\n+        # to have requires_grad=True but point to a grad_fn which throws an\n+        # error message during (double) back-propagation.\n+        # XXX: this is only an approximation of requires_grad - there's no way\n         # to figure out if fn didn't use ctx.saved_variables and as a result\n         # some Variables might require grad, even if no args do.\n         # Unfortunately, this leads to unexpected error messages (\"no nodes\n         # require computing gradients\"), but I don't have a better idea.\n         # These functions would raise an error in backward anyway.\n-        requires_grad = any(arg.requires_grad if isinstance(arg, Variable) else False\n+        requires_grad = any(isinstance(arg, Variable) and arg.requires_grad\n                             for arg in args)\n-        if not torch.is_grad_enabled():\n-            def err_fn(*args):\n-                return args\n-        else:\n-            err_fn = torch._C._functions.DelayedError(\n-                b\"trying to differentiate twice a function that was marked\"\n-                b\"with @once_differentiable\")\n+        if not requires_grad:\n+            return outputs\n+\n+        err_fn = torch._C._functions.DelayedError(\n+            b\"trying to differentiate twice a function that was marked\"\n+            b\"with @once_differentiable\")\n+\n         if not isinstance(outputs, tuple):\n-            var = (Variable(outputs, requires_grad=requires_grad)\n-                   if outputs is not None else None)\n-            return err_fn(var)\n-        return err_fn(*[Variable(o, requires_grad=requires_grad) if o is not None else None\n-                      for o in outputs])\n+            outputs = (outputs,)\n+\n+        # Create aliases of each output that has requires_grad=True. We need\n+        # at least one of the inputs to err_fn to require grad so that the\n+        # output will have a grad_fn.\n+        def fake_requires_grad(var):\n+            if var is not None:\n+                var = var.detach()\n+                var.requires_grad = True", "path": "torch/autograd/function.py", "position": 91, "original_position": 90, "commit_id": "a52a6a4055e7bc0053fbc4c65f217b3cb39f0b29", "original_commit_id": "296394a9359ec2a3504b3bc155077d6a90bbf7e5", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "`fake_requires_grad` creates leaf variables, but the thing that is returned is:\r\n\r\n ```\r\nerr_fn(*[fake_requires_grad(v) for v in outputs])\r\n```\r\n\r\nThe application of `err_fn` creates non-leaf Variables. Accumulate grad will never be called because the backward of `err_fn` throws an exception.", "created_at": "2018-01-23T21:55:09Z", "updated_at": "2018-11-23T15:38:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/4786#discussion_r163389455", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4786", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163389455"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4786#discussion_r163389455"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4786"}}, "body_html": "<p><code>fake_requires_grad</code> creates leaf variables, but the thing that is returned is:</p>\n<pre><code>err_fn(*[fake_requires_grad(v) for v in outputs])\n</code></pre>\n<p>The application of <code>err_fn</code> creates non-leaf Variables. Accumulate grad will never be called because the backward of <code>err_fn</code> throws an exception.</p>", "body_text": "fake_requires_grad creates leaf variables, but the thing that is returned is:\nerr_fn(*[fake_requires_grad(v) for v in outputs])\n\nThe application of err_fn creates non-leaf Variables. Accumulate grad will never be called because the backward of err_fn throws an exception.", "in_reply_to_id": 163385520}