{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163385229", "pull_request_review_id": 90989916, "id": 163385229, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzM4NTIyOQ==", "diff_hunk": "@@ -200,30 +186,43 @@ def once_differentiable(fn):\n \n     @functools.wraps(fn)\n     def wrapper(ctx, *args):\n-        tensor_args = [arg.data if isinstance(arg, Variable) else arg\n-                       for arg in args]\n-        outputs = fn(ctx, *tensor_args)\n-        # XXX: this is only an approximation of these flags - there's no way\n+        with torch.no_grad():\n+            outputs = fn(ctx, *args)\n+\n+        if not torch.is_grad_enabled():\n+            return outputs\n+\n+        # If any of the inputs have requires_grad=True, we force the outputs\n+        # to have requires_grad=True but point to a grad_fn which throws an\n+        # error message during (double) back-propagation.\n+        # XXX: this is only an approximation of requires_grad - there's no way\n         # to figure out if fn didn't use ctx.saved_variables and as a result\n         # some Variables might require grad, even if no args do.\n         # Unfortunately, this leads to unexpected error messages (\"no nodes\n         # require computing gradients\"), but I don't have a better idea.\n         # These functions would raise an error in backward anyway.\n-        requires_grad = any(arg.requires_grad if isinstance(arg, Variable) else False\n+        requires_grad = any(isinstance(arg, Variable) and arg.requires_grad\n                             for arg in args)\n-        if not torch.is_grad_enabled():\n-            def err_fn(*args):\n-                return args\n-        else:\n-            err_fn = torch._C._functions.DelayedError(\n-                b\"trying to differentiate twice a function that was marked\"\n-                b\"with @once_differentiable\")\n+        if not requires_grad:\n+            return outputs", "path": "torch/autograd/function.py", "position": 71, "original_position": 70, "commit_id": "a52a6a4055e7bc0053fbc4c65f217b3cb39f0b29", "original_commit_id": "296394a9359ec2a3504b3bc155077d6a90bbf7e5", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is a bit worrying for me. I've seen cases where running something with grad disabled, and later reusing those variables in context where grad is enabled triggers some assertions in SavedVariable, which should never fail. I'll open an issue for that", "created_at": "2018-01-23T21:39:46Z", "updated_at": "2018-11-23T15:38:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/4786#discussion_r163385229", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4786", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163385229"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4786#discussion_r163385229"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4786"}}, "body_html": "<p>This is a bit worrying for me. I've seen cases where running something with grad disabled, and later reusing those variables in context where grad is enabled triggers some assertions in SavedVariable, which should never fail. I'll open an issue for that</p>", "body_text": "This is a bit worrying for me. I've seen cases where running something with grad disabled, and later reusing those variables in context where grad is enabled triggers some assertions in SavedVariable, which should never fail. I'll open an issue for that"}