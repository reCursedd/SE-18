{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3824", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3824/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3824/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3824/events", "html_url": "https://github.com/pytorch/pytorch/issues/3824", "id": 275917710, "node_id": "MDU6SXNzdWUyNzU5MTc3MTA=", "number": 3824, "title": "Grad backward memory leak?", "user": {"login": "hudongloop", "id": 23180791, "node_id": "MDQ6VXNlcjIzMTgwNzkx", "avatar_url": "https://avatars2.githubusercontent.com/u/23180791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hudongloop", "html_url": "https://github.com/hudongloop", "followers_url": "https://api.github.com/users/hudongloop/followers", "following_url": "https://api.github.com/users/hudongloop/following{/other_user}", "gists_url": "https://api.github.com/users/hudongloop/gists{/gist_id}", "starred_url": "https://api.github.com/users/hudongloop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hudongloop/subscriptions", "organizations_url": "https://api.github.com/users/hudongloop/orgs", "repos_url": "https://api.github.com/users/hudongloop/repos", "events_url": "https://api.github.com/users/hudongloop/events{/privacy}", "received_events_url": "https://api.github.com/users/hudongloop/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-11-22T01:44:15Z", "updated_at": "2018-06-28T00:18:09Z", "closed_at": "2018-06-28T00:18:09Z", "author_association": "NONE", "body_html": "<p>Memory keeps growing at every iteration. My pytorch version 0.4.0a0+4d405a4.</p>\n<pre><code>import torch\nfrom torch.autograd import grad, Variable\nfrom torchvision import models\n\nmodel = models.resnet50().cuda()\nfor k in range(20):\n    x   = Variable(torch.rand(8, 3, 224, 224).cuda(), requires_grad=True)\n    dx, = grad(model(x).sum(), x, create_graph=True)\n    y = model(x + dx).sum()\n    y.backward()\n</code></pre>\n<p>This code is from <a href=\"https://github.com/pytorch/pytorch/issues/2361\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2361/hovercard\">Double backward memory leak</a>. It work well and no memory leak. But use</p>\n<pre><code>y = dx.mean()\ny.backward()\n</code></pre>\n<p>the memory leak is happen.</p>", "body_text": "Memory keeps growing at every iteration. My pytorch version 0.4.0a0+4d405a4.\nimport torch\nfrom torch.autograd import grad, Variable\nfrom torchvision import models\n\nmodel = models.resnet50().cuda()\nfor k in range(20):\n    x   = Variable(torch.rand(8, 3, 224, 224).cuda(), requires_grad=True)\n    dx, = grad(model(x).sum(), x, create_graph=True)\n    y = model(x + dx).sum()\n    y.backward()\n\nThis code is from Double backward memory leak. It work well and no memory leak. But use\ny = dx.mean()\ny.backward()\n\nthe memory leak is happen.", "body": "Memory keeps growing at every iteration. My pytorch version 0.4.0a0+4d405a4.\r\n```\r\nimport torch\r\nfrom torch.autograd import grad, Variable\r\nfrom torchvision import models\r\n\r\nmodel = models.resnet50().cuda()\r\nfor k in range(20):\r\n    x   = Variable(torch.rand(8, 3, 224, 224).cuda(), requires_grad=True)\r\n    dx, = grad(model(x).sum(), x, create_graph=True)\r\n    y = model(x + dx).sum()\r\n    y.backward()\r\n```\r\n\r\nThis code is from [Double backward memory leak](https://github.com/pytorch/pytorch/issues/2361). It work well and no memory leak. But use\r\n```\r\ny = dx.mean()\r\ny.backward()\r\n```\r\nthe memory leak is happen. "}