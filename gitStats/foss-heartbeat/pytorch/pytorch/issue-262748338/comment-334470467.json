{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/334470467", "html_url": "https://github.com/pytorch/pytorch/issues/2968#issuecomment-334470467", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2968", "id": 334470467, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDQ3MDQ2Nw==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-05T13:49:20Z", "updated_at": "2017-10-05T13:49:20Z", "author_association": "MEMBER", "body_html": "<p>To support <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> on non-efficient <code>float16</code> implementation on the CPU, here is a quick test showing that a basic <code>float16</code> operation in numpy is much slower than using <code>float32</code> or <code>float64</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float16<span class=\"pl-pds\">'</span></span>)\nb <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)\nc <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float64<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>and the results of <code>%timeit</code> for the <code>*</code> operation:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">%</span>timeit a <span class=\"pl-k\">*</span> a\n<span class=\"pl-c1\">100</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">7.83</span> ms per loop\nIn [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">%</span>timeit b <span class=\"pl-k\">*</span> b\n<span class=\"pl-c1\">1000</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">285</span> \u00b5s per loop\nIn [<span class=\"pl-c1\">3</span>]: <span class=\"pl-k\">%</span>timeit c <span class=\"pl-k\">*</span> c\n<span class=\"pl-c1\">1000</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">738</span> \u00b5s per loop</pre></div>\n<p>Which shows that <code>float16</code> is roughly 10x slower than <code>float64</code>, even though it has 4x less bytes.<br>\nRelevant <a href=\"https://stackoverflow.com/questions/15340781/python-numpy-data-types-performance\" rel=\"nofollow\">stack overflow thread</a>.</p>", "body_text": "To support @soumith on non-efficient float16 implementation on the CPU, here is a quick test showing that a basic float16 operation in numpy is much slower than using float32 or float64:\na = np.random.rand(100, 100, 100).astype('float16')\nb = np.random.rand(100, 100, 100).astype('float32')\nc = np.random.rand(100, 100, 100).astype('float64')\nand the results of %timeit for the * operation:\nIn [1]: %timeit a * a\n100 loops, best of 3: 7.83 ms per loop\nIn [2]: %timeit b * b\n1000 loops, best of 3: 285 \u00b5s per loop\nIn [3]: %timeit c * c\n1000 loops, best of 3: 738 \u00b5s per loop\nWhich shows that float16 is roughly 10x slower than float64, even though it has 4x less bytes.\nRelevant stack overflow thread.", "body": "To support @soumith on non-efficient `float16` implementation on the CPU, here is a quick test showing that a basic `float16` operation in numpy is much slower than using `float32` or `float64`:\r\n```python\r\na = np.random.rand(100, 100, 100).astype('float16')\r\nb = np.random.rand(100, 100, 100).astype('float32')\r\nc = np.random.rand(100, 100, 100).astype('float64')\r\n```\r\nand the results of `%timeit` for the `*` operation:\r\n```python\r\nIn [1]: %timeit a * a\r\n100 loops, best of 3: 7.83 ms per loop\r\nIn [2]: %timeit b * b\r\n1000 loops, best of 3: 285 \u00b5s per loop\r\nIn [3]: %timeit c * c\r\n1000 loops, best of 3: 738 \u00b5s per loop\r\n```\r\n\r\nWhich shows that `float16` is roughly 10x slower than `float64`, even though it has 4x less bytes.\r\nRelevant [stack overflow thread](https://stackoverflow.com/questions/15340781/python-numpy-data-types-performance)."}