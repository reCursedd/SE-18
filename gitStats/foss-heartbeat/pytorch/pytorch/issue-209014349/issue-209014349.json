{"url": "https://api.github.com/repos/pytorch/pytorch/issues/805", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/805/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/805/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/805/events", "html_url": "https://github.com/pytorch/pytorch/issues/805", "id": 209014349, "node_id": "MDU6SXNzdWUyMDkwMTQzNDk=", "number": 805, "title": "Maxout Layer", "user": {"login": "vzhong", "id": 1855260, "node_id": "MDQ6VXNlcjE4NTUyNjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1855260?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vzhong", "html_url": "https://github.com/vzhong", "followers_url": "https://api.github.com/users/vzhong/followers", "following_url": "https://api.github.com/users/vzhong/following{/other_user}", "gists_url": "https://api.github.com/users/vzhong/gists{/gist_id}", "starred_url": "https://api.github.com/users/vzhong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vzhong/subscriptions", "organizations_url": "https://api.github.com/users/vzhong/orgs", "repos_url": "https://api.github.com/users/vzhong/repos", "events_url": "https://api.github.com/users/vzhong/events{/privacy}", "received_events_url": "https://api.github.com/users/vzhong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-02-21T00:53:25Z", "updated_at": "2018-05-16T09:00:36Z", "closed_at": "2017-02-21T01:18:13Z", "author_association": "NONE", "body_html": "<p>Are there plans for a maxout layer? For example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Maxout</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">d_in</span>, <span class=\"pl-smi\">d_out</span>, <span class=\"pl-smi\">pool_size</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.d_in, <span class=\"pl-c1\">self</span>.d_out, <span class=\"pl-c1\">self</span>.pool_size <span class=\"pl-k\">=</span> d_in, d_out, pool_size\n        <span class=\"pl-c1\">self</span>.lin <span class=\"pl-k\">=</span> Linear(d_in, d_out <span class=\"pl-k\">*</span> pool_size)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(inputs.size())\n        shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.d_out\n        shape.append(<span class=\"pl-c1\">self</span>.pool_size)\n        last_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(shape) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lin(inputs)\n        m, i <span class=\"pl-k\">=</span> out.view(<span class=\"pl-k\">*</span>shape).max(last_dim)\n        <span class=\"pl-k\">return</span> m.squeeze(last_dim)</pre></div>", "body_text": "Are there plans for a maxout layer? For example:\nclass Maxout(nn.Module):\n\n    def __init__(self, d_in, d_out, pool_size):\n        super().__init__()\n        self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\n        self.lin = Linear(d_in, d_out * pool_size)\n\n    def forward(self, inputs):\n        shape = list(inputs.size())\n        shape[-1] = self.d_out\n        shape.append(self.pool_size)\n        last_dim = len(shape) - 1\n        out = self.lin(inputs)\n        m, i = out.view(*shape).max(last_dim)\n        return m.squeeze(last_dim)", "body": "Are there plans for a maxout layer? For example:\r\n\r\n```python\r\nclass Maxout(nn.Module):\r\n\r\n    def __init__(self, d_in, d_out, pool_size):\r\n        super().__init__()\r\n        self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\r\n        self.lin = Linear(d_in, d_out * pool_size)\r\n\r\n    def forward(self, inputs):\r\n        shape = list(inputs.size())\r\n        shape[-1] = self.d_out\r\n        shape.append(self.pool_size)\r\n        last_dim = len(shape) - 1\r\n        out = self.lin(inputs)\r\n        m, i = out.view(*shape).max(last_dim)\r\n        return m.squeeze(last_dim)\r\n```"}