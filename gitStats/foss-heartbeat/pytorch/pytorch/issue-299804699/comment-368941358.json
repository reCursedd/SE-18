{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/368941358", "html_url": "https://github.com/pytorch/pytorch/issues/5381#issuecomment-368941358", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5381", "id": 368941358, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODk0MTM1OA==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-27T16:38:08Z", "updated_at": "2018-02-27T16:38:08Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>By the same reasoning, ByteTensor is also confusing to users (because it's uint8) and we're going to deprecate this name, right? ;)</p>\n</blockquote>\n<p>I'm not against that, but it's at least <em>less</em> confusing than <code>torch.byte</code> because there is no np.ByteArray equivalent.</p>\n<blockquote>\n<p>BTW, I use ByteTensor all the time without any confusion because all I really need is some opaque blob of data whose size is counted in bytes, because I'm passing it on to cuDNN or something; in this case the signedness of the bytes themselves doesn't matter because I'm never going to interpret the bytes as numbers.</p>\n</blockquote>\n<p>I've always found this usage a little weird, it reminds me a bit of using CharStorages for strings in TH, i.e. just because something can hold something doesn't mean it should  (what is the cumprod of your blob?).  Why isn't this just some python byte blob object (I don't know the right one to use) that gets translated into some reasonable C++ class (pointer and size pair?)</p>\n<blockquote>\n<p>Can we make a use of torch.byte raise an error explaining why we don't want to let you use it, and how to use the one you actually want?</p>\n</blockquote>\n<p>I thought about that and I'm not totally against it, but it does seem a little...mean.  I.e. we give an error with a suggestion, but we don't just apply the suggestion (because we don't want them to be lazy and use <code>torch.byte</code>?  Well, that sounds a lot like deprecation, but if we do that we are introducing something and immediately deprecating it?</p>\n<blockquote>\n<p>What are the semantics of arithmetic operations on BoolTensor? The most natural mathematical interpretation is that boolean tensors are tensors of integers modulo two. But representing this with bytes is actively harmful for implementing these semantics correctly. So I really don't like calling it a BoolTensor unless we also work to make the ops correct (or exclude them.)</p>\n</blockquote>\n<p>Right, I'm not suggesting we end up in that state.  I'm just saying we treat ByteTensors as BoolTensors now (e.g. for boolean indexing), so <code>torch.byte</code> is again misleading.</p>", "body_text": "By the same reasoning, ByteTensor is also confusing to users (because it's uint8) and we're going to deprecate this name, right? ;)\n\nI'm not against that, but it's at least less confusing than torch.byte because there is no np.ByteArray equivalent.\n\nBTW, I use ByteTensor all the time without any confusion because all I really need is some opaque blob of data whose size is counted in bytes, because I'm passing it on to cuDNN or something; in this case the signedness of the bytes themselves doesn't matter because I'm never going to interpret the bytes as numbers.\n\nI've always found this usage a little weird, it reminds me a bit of using CharStorages for strings in TH, i.e. just because something can hold something doesn't mean it should  (what is the cumprod of your blob?).  Why isn't this just some python byte blob object (I don't know the right one to use) that gets translated into some reasonable C++ class (pointer and size pair?)\n\nCan we make a use of torch.byte raise an error explaining why we don't want to let you use it, and how to use the one you actually want?\n\nI thought about that and I'm not totally against it, but it does seem a little...mean.  I.e. we give an error with a suggestion, but we don't just apply the suggestion (because we don't want them to be lazy and use torch.byte?  Well, that sounds a lot like deprecation, but if we do that we are introducing something and immediately deprecating it?\n\nWhat are the semantics of arithmetic operations on BoolTensor? The most natural mathematical interpretation is that boolean tensors are tensors of integers modulo two. But representing this with bytes is actively harmful for implementing these semantics correctly. So I really don't like calling it a BoolTensor unless we also work to make the ops correct (or exclude them.)\n\nRight, I'm not suggesting we end up in that state.  I'm just saying we treat ByteTensors as BoolTensors now (e.g. for boolean indexing), so torch.byte is again misleading.", "body": "> By the same reasoning, ByteTensor is also confusing to users (because it's uint8) and we're going to deprecate this name, right? ;)\r\n\r\nI'm not against that, but it's at least _less_ confusing than `torch.byte` because there is no np.ByteArray equivalent.\r\n\r\n> BTW, I use ByteTensor all the time without any confusion because all I really need is some opaque blob of data whose size is counted in bytes, because I'm passing it on to cuDNN or something; in this case the signedness of the bytes themselves doesn't matter because I'm never going to interpret the bytes as numbers.\r\n\r\nI've always found this usage a little weird, it reminds me a bit of using CharStorages for strings in TH, i.e. just because something can hold something doesn't mean it should  (what is the cumprod of your blob?).  Why isn't this just some python byte blob object (I don't know the right one to use) that gets translated into some reasonable C++ class (pointer and size pair?)\r\n\r\n> Can we make a use of torch.byte raise an error explaining why we don't want to let you use it, and how to use the one you actually want?\r\n\r\nI thought about that and I'm not totally against it, but it does seem a little...mean.  I.e. we give an error with a suggestion, but we don't just apply the suggestion (because we don't want them to be lazy and use `torch.byte`?  Well, that sounds a lot like deprecation, but if we do that we are introducing something and immediately deprecating it?\r\n\r\n> What are the semantics of arithmetic operations on BoolTensor? The most natural mathematical interpretation is that boolean tensors are tensors of integers modulo two. But representing this with bytes is actively harmful for implementing these semantics correctly. So I really don't like calling it a BoolTensor unless we also work to make the ops correct (or exclude them.)\r\n\r\nRight, I'm not suggesting we end up in that state.  I'm just saying we treat ByteTensors as BoolTensors now (e.g. for boolean indexing), so `torch.byte` is again misleading."}