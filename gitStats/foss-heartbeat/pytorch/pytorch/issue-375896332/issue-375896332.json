{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13384", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13384/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13384/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13384/events", "html_url": "https://github.com/pytorch/pytorch/issues/13384", "id": 375896332, "node_id": "MDU6SXNzdWUzNzU4OTYzMzI=", "number": 13384, "title": "Fuser can incorrectly fuse comparison ops at the end of fusion groups", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-31T10:33:55Z", "updated_at": "2018-11-20T17:23:25Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>Currently the fuser hopes that when it will fuse an op that has an <code>int8</code> output type, it will be followed by a <code>type_as</code> (as is the case in e.g. many ReLU implementations). However, it's not guaranteed to be the case, and since our codegen only supports <code>float</code> math, it can sometimes change the run time type of the values:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">z</span>):\n    <span class=\"pl-k\">return</span> ((x <span class=\"pl-k\">+</span> y) <span class=\"pl-k\">/</span> z) <span class=\"pl-k\">&gt;=</span> z\n\ninputs <span class=\"pl-k\">=</span> [torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>)]\nreal_output <span class=\"pl-k\">=</span> f(<span class=\"pl-k\">*</span>inputs)\nfs <span class=\"pl-k\">=</span> torch.jit.script(f)\noutput <span class=\"pl-k\">=</span> fs(<span class=\"pl-k\">*</span>inputs)\n<span class=\"pl-c1\">print</span>(fs.graph_for(<span class=\"pl-k\">*</span>inputs))\n<span class=\"pl-c1\">print</span>(output)\n<span class=\"pl-c1\">print</span>(real_output)</pre></div>\n<p>This inconsistency is very unfortunate and I will need to remove it in one of my upcoming fuser refactors. We can safely make those ops fusable again once we expand the codegen to work on arbitrary scalar types.</p>", "body_text": "Currently the fuser hopes that when it will fuse an op that has an int8 output type, it will be followed by a type_as (as is the case in e.g. many ReLU implementations). However, it's not guaranteed to be the case, and since our codegen only supports float math, it can sometimes change the run time type of the values:\nimport torch\n\ndef f(x, y, z):\n    return ((x + y) / z) >= z\n\ninputs = [torch.randn(2, 2, device='cuda') for _ in range(3)]\nreal_output = f(*inputs)\nfs = torch.jit.script(f)\noutput = fs(*inputs)\nprint(fs.graph_for(*inputs))\nprint(output)\nprint(real_output)\nThis inconsistency is very unfortunate and I will need to remove it in one of my upcoming fuser refactors. We can safely make those ops fusable again once we expand the codegen to work on arbitrary scalar types.", "body": "Currently the fuser hopes that when it will fuse an op that has an `int8` output type, it will be followed by a `type_as` (as is the case in e.g. many ReLU implementations). However, it's not guaranteed to be the case, and since our codegen only supports `float` math, it can sometimes change the run time type of the values:\r\n\r\n```python\r\nimport torch\r\n\r\ndef f(x, y, z):\r\n    return ((x + y) / z) >= z\r\n\r\ninputs = [torch.randn(2, 2, device='cuda') for _ in range(3)]\r\nreal_output = f(*inputs)\r\nfs = torch.jit.script(f)\r\noutput = fs(*inputs)\r\nprint(fs.graph_for(*inputs))\r\nprint(output)\r\nprint(real_output)\r\n```\r\n\r\nThis inconsistency is very unfortunate and I will need to remove it in one of my upcoming fuser refactors. We can safely make those ops fusable again once we expand the codegen to work on arbitrary scalar types."}