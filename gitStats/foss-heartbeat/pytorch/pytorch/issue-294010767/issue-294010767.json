{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5021", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5021/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5021/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5021/events", "html_url": "https://github.com/pytorch/pytorch/pull/5021", "id": 294010767, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY2ODk0MTE3", "number": 5021, "title": "[WIP] Add XPU abstraction (Input Requested)", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-02-02T20:17:33Z", "updated_at": "2018-05-19T18:57:44Z", "closed_at": "2018-05-18T13:13:01Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5021", "html_url": "https://github.com/pytorch/pytorch/pull/5021", "diff_url": "https://github.com/pytorch/pytorch/pull/5021.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5021.patch"}, "body_html": "<h3>Overview</h3>\n<p>This PR is a proposal to add the class <code>XPU</code> to pytorch. Currently the APIs used to write CPU and GPU code are different. It is tricky to parameterize them via a function argument. This creates unnecessarily branches in the code and increases the difficulty of testing and writing robust code. To solve this I've developed the class XPU that creates a common interface allowing seamless switching between using a CPU, GPU, or even multiple GPUs.</p>\n<p>The basic idea is that when constructing a XPU you give it either None to represent CPU mode and an integer or list of integers to support GPU mode. The classmethod <code>cast('auto')</code> attempts to be smart and respects command line args if they are given, otherwise it choose sthe GPU with the lowest amount of memory if one exists, or the CPU.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> xpu <span class=\"pl-k\">=</span> XPU(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>auto<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>Then to move data onto either the cpu or gpu you run:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> xpu.move(data)</pre></div>\n<p>Instead of one of the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> data.cpu()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> data.gpu(<span class=\"pl-c1\">0</span>)</pre></div>\n<p>Furthemore, <code>XPU</code> provides a convenience method for creating variables</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> var1 <span class=\"pl-k\">=</span> xpu.variable(data1)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> I've also found it helpful to have a wrapper that processes more than one at a time (but maybe that design isn't clean, it could be removed)</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> var2, var3, <span class=\"pl-k\">*</span>varsN <span class=\"pl-k\">=</span> xpu.variables(data2, data3, <span class=\"pl-k\">*</span>datasN)</pre></div>\n<p>Lastly, to seamlessly support multiple GPUs, instead of calling <code>xpu.move</code> on a model, we mount it.</p>\n<div class=\"highlight highlight-source-python\"><pre>model <span class=\"pl-k\">=</span> xpu.mount(model)</pre></div>\n<p>This is effectively the same as move unless the XPU is multiple GPUs, in which case it wraps model with a <code>DataParallel</code> object. (This requires the creation of a DataSerial wrapper to provide a consistent API).</p>\n<h3>Status</h3>\n<p>Currently, I've just dumped the <code>xpu_device.py</code> file that I use in my scripts into the torch module. I put a few unit tests in that same file, but its by no means fully tested. I want to see what other people have to say and hear comments, criticisms, and suggestions before I work on integration.</p>", "body_text": "Overview\nThis PR is a proposal to add the class XPU to pytorch. Currently the APIs used to write CPU and GPU code are different. It is tricky to parameterize them via a function argument. This creates unnecessarily branches in the code and increases the difficulty of testing and writing robust code. To solve this I've developed the class XPU that creates a common interface allowing seamless switching between using a CPU, GPU, or even multiple GPUs.\nThe basic idea is that when constructing a XPU you give it either None to represent CPU mode and an integer or list of integers to support GPU mode. The classmethod cast('auto') attempts to be smart and respects command line args if they are given, otherwise it choose sthe GPU with the lowest amount of memory if one exists, or the CPU.\n>>> xpu = XPU('auto')\nThen to move data onto either the cpu or gpu you run:\n>>> xpu.move(data)\nInstead of one of the following:\n>>> data.cpu()\n>>> data.gpu(0)\nFurthemore, XPU provides a convenience method for creating variables\n>>> var1 = xpu.variable(data1)\n>>> # I've also found it helpful to have a wrapper that processes more than one at a time (but maybe that design isn't clean, it could be removed)\n>>> var2, var3, *varsN = xpu.variables(data2, data3, *datasN)\nLastly, to seamlessly support multiple GPUs, instead of calling xpu.move on a model, we mount it.\nmodel = xpu.mount(model)\nThis is effectively the same as move unless the XPU is multiple GPUs, in which case it wraps model with a DataParallel object. (This requires the creation of a DataSerial wrapper to provide a consistent API).\nStatus\nCurrently, I've just dumped the xpu_device.py file that I use in my scripts into the torch module. I put a few unit tests in that same file, but its by no means fully tested. I want to see what other people have to say and hear comments, criticisms, and suggestions before I work on integration.", "body": "### Overview\r\n\r\nThis PR is a proposal to add the class `XPU` to pytorch. Currently the APIs used to write CPU and GPU code are different. It is tricky to parameterize them via a function argument. This creates unnecessarily branches in the code and increases the difficulty of testing and writing robust code. To solve this I've developed the class XPU that creates a common interface allowing seamless switching between using a CPU, GPU, or even multiple GPUs. \r\n\r\nThe basic idea is that when constructing a XPU you give it either None to represent CPU mode and an integer or list of integers to support GPU mode. The classmethod `cast('auto')` attempts to be smart and respects command line args if they are given, otherwise it choose sthe GPU with the lowest amount of memory if one exists, or the CPU.  \r\n\r\n```python\r\n>>> xpu = XPU('auto')\r\n```\r\n\r\nThen to move data onto either the cpu or gpu you run:\r\n\r\n```python\r\n>>> xpu.move(data)\r\n```\r\n\r\nInstead of one of the following:\r\n \r\n```python\r\n>>> data.cpu()\r\n>>> data.gpu(0)\r\n```\r\n\r\nFurthemore, `XPU` provides a convenience method for creating variables\r\n\r\n```python \r\n>>> var1 = xpu.variable(data1)\r\n>>> # I've also found it helpful to have a wrapper that processes more than one at a time (but maybe that design isn't clean, it could be removed)\r\n>>> var2, var3, *varsN = xpu.variables(data2, data3, *datasN)\r\n```\r\n\r\nLastly, to seamlessly support multiple GPUs, instead of calling `xpu.move` on a model, we mount it.\r\n\r\n```python\r\nmodel = xpu.mount(model)\r\n```\r\n\r\nThis is effectively the same as move unless the XPU is multiple GPUs, in which case it wraps model with a `DataParallel` object. (This requires the creation of a DataSerial wrapper to provide a consistent API). \r\n\r\n### Status\r\n\r\nCurrently, I've just dumped the `xpu_device.py` file that I use in my scripts into the torch module. I put a few unit tests in that same file, but its by no means fully tested. I want to see what other people have to say and hear comments, criticisms, and suggestions before I work on integration. "}