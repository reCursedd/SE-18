{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/135902644", "pull_request_review_id": 59368240, "id": 135902644, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNTkwMjY0NA==", "diff_hunk": "@@ -0,0 +1,350 @@\n+import torch\n+import torch.jit\n+import torch.nn as nn\n+import unittest\n+from torch.autograd import Variable, Function\n+from common import TestCase, run_tests\n+\n+try:\n+    import torchvision\n+    HAS_TORCHVISION = True\n+except ImportError:\n+    HAS_TORCHVISION = False\n+\n+try:\n+    import toffee\n+    import google.protobuf.text_format\n+    HAS_TOFFEE = True\n+except ImportError:\n+    HAS_TOFFEE = False\n+\n+toffee_only = unittest.skipIf(not HAS_TOFFEE, \"no toffee support library\")\n+skipIfNoTorchVision = unittest.skipIf(not HAS_TORCHVISION, \"no torchvision\")\n+\n+\n+class TestJit(TestCase):\n+    maxDiff = None\n+\n+    def test_simple(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        trace = torch._C._tracer_enter((x, y))\n+        z = torch.sigmoid(torch.tanh(x * (x + y)))\n+        torch._C._tracer_exit((z,))\n+        torch._C._jit_pass_lint(trace)\n+        torch._C._jit_pass_init(trace)\n+        torch._C._jit_pass_lint(trace)\n+        torch._C._jit_pass_fuse(trace)\n+        torch._C._jit_pass_lint(trace)\n+\n+        self.assertExpected(str(trace))\n+\n+    def assertToffeeExpected(self, binary_pb, subname=None):\n+        graph_def = toffee.GraphProto.FromString(binary_pb)\n+        self.assertExpected(google.protobuf.text_format.MessageToString(graph_def, float_format='.15g'), subname)\n+\n+    @toffee_only\n+    def test_export(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        trace = torch._C._tracer_enter((x, y))\n+        z = -torch.sigmoid(torch.tanh(x * (x + y)))\n+        torch._C._tracer_exit((z,))\n+        torch._C._jit_pass_lint(trace)\n+        self.assertToffeeExpected(trace.export())\n+\n+    @toffee_only\n+    def test_export_view(self):\n+        x = Variable(torch.Tensor([0]), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(lambda x: x.view(1, 1), x)\n+        self.assertToffeeExpected(trace.export())\n+\n+    @toffee_only\n+    def test_export_data(self):\n+        x = Variable(torch.Tensor([[1, 2], [3, 4]]), requires_grad=True)\n+        y = Variable(torch.Tensor([[1, 2], [3, 4]]), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(lambda x, y: -torch.sigmoid(torch.tanh(x * (x + y))), x, y)\n+        initializers = [x.data]\n+        self.assertToffeeExpected(trace.export(initializers))\n+\n+    def test_lstm(self):\n+        # Careful: don't use fused backend (enabled with CUDA)\n+        # Pasted from test_LSTM_cell\n+        input = Variable(torch.randn(3, 10))\n+        hx = Variable(torch.randn(3, 20))\n+        cx = Variable(torch.randn(3, 20))\n+        trace, _ = torch.jit.record_trace(\n+            nn.LSTMCell(10, 20), input, (hx, cx))\n+        torch._C._jit_pass_lint(trace)\n+        torch._C._jit_pass_init(trace)\n+        torch._C._jit_pass_lint(trace)\n+        torch._C._jit_pass_fuse(trace)\n+        torch._C._jit_pass_lint(trace)\n+        self.assertExpected(str(trace))\n+\n+    def test_function_as_argument(self):\n+        # Careful: don't use fused backend (enabled with CUDA)\n+        # Pasted from test_LSTM_cell\n+        input = Variable(torch.randn(3, 10))\n+        hx = Variable(torch.randn(3, 20))\n+        cx = Variable(torch.randn(3, 20))\n+        lstm = nn.LSTMCell(10, 20)\n+\n+        def a_function(a, b):\n+            return lstm(a, b)\n+        trace, _ = torch.jit.record_trace(\n+            a_function, input, (hx, cx), parameters=lstm.parameters())\n+        torch._C._jit_pass_lint(trace)\n+        torch._C._jit_pass_init(trace)\n+        torch._C._jit_pass_lint(trace)\n+        torch._C._jit_pass_fuse(trace)\n+        torch._C._jit_pass_lint(trace)\n+        self.assertExpected(str(trace))\n+\n+    def test_verify(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        def doit(x, y):\n+            return torch.sigmoid(torch.tanh(x * (x + y)))\n+\n+        traced = torch.jit.traced(\n+            doit, enabled=True, verify=True, time=True, optimize=False)\n+        z = traced(x, y)\n+        z2 = traced(x, y)\n+        self.assertEqual(z, torch.sigmoid(torch.tanh(x * (x + y))))\n+        self.assertEqual(z, z2)\n+\n+    def test_traced_function(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        def doit(x, y):\n+            return torch.sigmoid(torch.tanh(x * (x + y)))\n+\n+        traced = torch.jit.traced(doit)\n+        z = traced(x, y)\n+        z2 = traced(x, y)\n+        self.assertEqual(z, torch.sigmoid(torch.tanh(x * (x + y))))\n+        self.assertEqual(z, z2)\n+\n+    def test_disabled_traced_function(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        def doit(x, y):\n+            return torch.sigmoid(torch.tanh(x * (x + y)))\n+\n+        traced = torch.jit.traced(doit, enabled=False)\n+        z = traced(x, y)\n+        z2 = traced(x, y)\n+        self.assertEqual(z, torch.sigmoid(torch.tanh(x * (x + y))))\n+        self.assertEqual(z, z2)\n+\n+    def test_traced_module(self):\n+        input = Variable(torch.randn(3, 10))\n+        hx = Variable(torch.randn(3, 20))\n+        cx = Variable(torch.randn(3, 20))\n+        lstm = nn.LSTMCell(10, 20)\n+        lstm = torch.jit.traced(lstm, verify=True)\n+\n+        out = lstm(input, (hx, cx))\n+        out2 = lstm(input, (hx, cx))\n+        self.assertEqual(out, out2)\n+\n+    def test_autograd_closure(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        trace = torch._C._tracer_enter((x, y))\n+\n+        z = torch.sigmoid(x * (x + y))\n+        w = torch.abs(x * x * x + y) + Variable(torch.ones(1))\n+\n+        torch._C._tracer_exit((z, w))\n+        torch._C._jit_pass_lint(trace)\n+\n+        (z * w).backward()\n+        torch._C._jit_pass_dco(trace)\n+        torch._C._jit_pass_lint(trace)\n+\n+        x_grad = x.grad.data.clone()\n+        x.grad.data.zero_()\n+\n+        function = torch._C._jit_createAutogradClosure(trace)\n+        torch._C._jit_pass_lint(trace)\n+        z2, w2 = function()(x, y)\n+        (z2 * w2).backward()\n+        self.assertEqual(z, z2)\n+        self.assertEqual(w, w2)\n+        self.assertEqual(x.grad.data, x_grad)\n+\n+    def test_constant(self):\n+        x = Variable(torch.randn(2, 2), requires_grad=True)\n+\n+        trace = torch._C._tracer_enter((x,))\n+\n+        y = Variable(torch.diag(torch.Tensor([2, 2])))\n+        z = x.matmul(y)\n+\n+        torch._C._tracer_exit((z,))\n+        function = torch._C._jit_createAutogradClosure(trace)\n+\n+        z2 = function()(x)\n+        self.assertEqual(z, z2)\n+\n+        y.data.fill_(1000)  # make sure the data has been cloned\n+\n+        x2 = Variable(torch.ones(2, 2) * 2, requires_grad=True)\n+        z3 = function()(x2)\n+        self.assertEqual(z3.data, torch.ones(2, 2) * 4)\n+\n+    def test_c_function(self):\n+        x = Variable(torch.randn(1, 3, 10, 10))\n+        m = nn.Conv2d(3, 8, 3, 1)\n+\n+        trace = torch._C._tracer_enter((x,) + tuple(m.parameters()))\n+        y = m(x)\n+        torch._C._tracer_exit((y,))\n+        self.assertExpected(str(trace))\n+\n+    def test_legacy_fail(self):\n+\n+        class Legacy(Function):\n+            def forward(self, x):\n+                return x\n+\n+            def backward(self, grad_output):\n+                return grad_output\n+\n+        x = Variable(torch.Tensor([0]), requires_grad=True)\n+        trace = torch._C._tracer_enter((x,))\n+        self.assertRaises(RuntimeError, lambda: Legacy()(x))\n+        torch._C._tracer_exit((x,))\n+\n+    def test_inplace_transplant(self):\n+        x = Variable(torch.Tensor([0]), requires_grad=True)\n+        trace = torch._C._tracer_enter((x,))\n+        y = x.clone()\n+        y.add_(2)\n+        y.add_(3)\n+        torch._C._tracer_exit((y,))\n+        self.assertExpected(str(trace))\n+\n+    def test_backward(self):\n+        a = Variable(torch.randn(2, 2), requires_grad=True)\n+        b = Variable(torch.randn(2, 2), requires_grad=True)\n+\n+        x = a\n+        y = a * b\n+\n+        trace = torch._C._tracer_enter((x, y))\n+        z = y * 2 * x\n+        torch._C._tracer_exit((z,))\n+        torch._C._jit_pass_lint(trace)\n+\n+        # Run first backward\n+        grad, = torch.autograd.grad(z, x, Variable(torch.ones(2, 2), requires_grad=True), create_graph=True)\n+        torch._C._jit_pass_lint(trace)\n+\n+        # Run second backward\n+        grad.sum().backward(create_graph=True)\n+        torch._C._jit_pass_lint(trace)\n+\n+        # Run dead code elimination to remove unused trace nodes\n+        torch._C._jit_pass_dco(trace)\n+        self.assertExpected(str(trace))\n+\n+    def test_python_ir(self):\n+        x = Variable(torch.Tensor([0.4]), requires_grad=True)\n+        y = Variable(torch.Tensor([0.7]), requires_grad=True)\n+\n+        def doit(x, y):\n+            return torch.sigmoid(torch.tanh(x * (x + y)))\n+\n+        traced, _ = torch.jit.record_trace(doit, x, y)\n+        g = torch._C._jit_get_graph(traced)\n+        g2 = torch._C.Graph()\n+        g_to_g2 = {}\n+        for node in g.inputs():\n+            g_to_g2[node] = g2.addInput()\n+        for node in g.nodes():\n+            if node.kind() == \"PythonOp\":\n+                n_ = g2.create(node.pyname(),\n+                               [g_to_g2[i] for i in node.inputs()]) \\\n+                    .setType(node.typeOption()) \\\n+                    .s_(\"note\", \"from_pyop\") \\\n+                    .i_(\"some_value\", len(node.scalar_args()))\n+                assert(n_.i(\"some_value\") == len(node.scalar_args()))\n+            else:\n+                n_ = g2.createClone(node, lambda x: g_to_g2[x])\n+                assert(n_.kindOf(\"Offset\") == \"i\")\n+\n+            g_to_g2[node] = g2.appendNode(n_)\n+\n+        for node in g.outputs():\n+            g2.registerOutput(g_to_g2[node])\n+\n+        t_node = g2.create(\"TensorTest\").t_(\"a\", torch.ones([2, 2]))\n+        assert(t_node.attributeNames() == [\"a\"])\n+        g2.appendNode(t_node)\n+        assert(torch.equal(torch.ones([2, 2]), t_node.t(\"a\")))\n+        self.assertExpected(str(g2))\n+\n+    def test_cpp(self):\n+        torch._C._jit_run_cpp_tests()\n+\n+    def test_batchnorm(self):\n+        x = Variable(torch.randn(2, 2).fill_(1.0), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(nn.BatchNorm2d(2), x)\n+        self.assertExpected(str(trace))\n+\n+    @toffee_only\n+    def test_batchnorm_export(self):\n+        x = Variable(torch.randn(2, 2).fill_(1.0), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(nn.BatchNorm2d(2), x)\n+        self.assertToffeeExpected(trace.export())\n+\n+    def test_batchnorm_verify(self):\n+        bn = torch.jit.traced(nn.BatchNorm2d(1), enabled=True, verify=True)\n+        x = Variable(torch.randn(5, 1))\n+        z = bn(x)\n+        z2 = bn(x)\n+        self.assertEqual(z, z2)\n+\n+    def test_conv(self):\n+        x = Variable(torch.randn(20, 16, 50, 40).fill_(1.0), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(nn.Conv2d(16, 13, 3, bias=False), x)\n+        self.assertExpected(str(trace))\n+\n+    @toffee_only\n+    def test_conv_export(self):\n+        x = Variable(torch.randn(20, 16, 50, 40).fill_(1.0), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(nn.Conv2d(16, 13, 3, bias=False), x)\n+        self.assertToffeeExpected(trace.export())\n+\n+    @toffee_only\n+    def test_maxpool_export(self):\n+        x = Variable(torch.randn(20, 16, 50))\n+        trace, _ = torch.jit.record_trace(nn.MaxPool1d(3, stride=2), x)\n+        self.assertToffeeExpected(trace.export())\n+\n+    @skipIfNoTorchVision\n+    def test_alexnet(self):\n+        x = Variable(torch.randn(10, 3, 224, 224).fill_(1.0), requires_grad=True)\n+        trace, _ = torch.jit.record_trace(torchvision.models.AlexNet(), x)\n+        self.assertExpected(str(trace))\n+        # NB: Purposely NOT testing protobuf export here\n+\n+    @skipIfNoTorchVision\n+    def test_densenet(self):\n+        x = Variable(torch.randn(10, 3, 224, 224).fill_(1.0), requires_grad=True)\n+        dense121 = torchvision.models.DenseNet(num_init_features=64, growth_rate=32,\n+                                               block_config=(6, 12, 24, 16))\n+        trace, _ = torch.jit.record_trace(dense121, x)\n+        # Densenet trace is pretty large, so we don't assert on it", "path": "test/test_jit.py", "position": null, "original_position": 347, "commit_id": "9d9a63c2860b98e0b323b578af23c4a4a00796ab", "original_commit_id": "652c70025e6aca99d828240869277bb2364799e3", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This test shouldn't be there. It's going to take forever to run this test on CPU and it doesn't even assert anything", "created_at": "2017-08-29T20:28:09Z", "updated_at": "2018-11-23T15:34:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/2565#discussion_r135902644", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2565", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/135902644"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2565#discussion_r135902644"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2565"}}, "body_html": "<p>This test shouldn't be there. It's going to take forever to run this test on CPU and it doesn't even assert anything</p>", "body_text": "This test shouldn't be there. It's going to take forever to run this test on CPU and it doesn't even assert anything"}