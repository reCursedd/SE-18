{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2565", "id": 138264544, "node_id": "MDExOlB1bGxSZXF1ZXN0MTM4MjY0NTQ0", "html_url": "https://github.com/pytorch/pytorch/pull/2565", "diff_url": "https://github.com/pytorch/pytorch/pull/2565.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2565.patch", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2565", "number": 2565, "state": "closed", "locked": false, "title": "A model exporter for PyTorch", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "This is a big PR. I don't recommend looking at the commits individually, although you may find it useful to blame lines here and there. (Naming disclaimer: the project uses \"toffee\" in some places, but this will need to be renamed.)\r\n\r\nA lot of folks contributed to this PR. Shout outs to @zdevito, @apaszke, @prigoyal, @killeent, @houseroad; as well as indirect contributions from @Yangqing, @bddppq and @jerryzh168.\r\n\r\n**User facing interface.** At the moment, most functionality is made available from `torch.jit` and `torch.toffee` modules. Some refactoring is on going for these interfaces; `torch.jit` will be made into a private namespace and `torch.toffee` will grow end-to-end conversion functions.\r\n\r\n**Key new pieces of code.**\r\n\r\n* **The compiler IR.** There is a new, mutable, SSA, in-memory compiler IR representation which represents a computation in PyTorch. We maintain accurate def-use chains. The IR is defined in `torch/csrc/jit/ir.h`. Some unusual things about the IR:\r\n  * Deep learning networks generally have multiple inputs AND outputs. However, computation nodes in our IR only have a single output; instead, we project out the particular output we are interested in using a Select node. The \"Select invariant\" states that any multiple return node (that's most nodes) always has exactly as many Select nodes as outputs. There are no first-class tuples in the language.\r\n  * Computations in the graph are associated with stages, specifying whether or not they occurred in forwards/backwards/backwards-backwards. A computation in some stage cannot depend on a computation from a later stage; in fact, earlier stage computations will always be earlier in the IR than later stage computations.\r\n  * Nodes in the graph are dynamically typed; while there are some special nodes (CppOp and PythonOp), most nodes are represented as strings with arbitrary dictionaries of attribute names to attribute values. Names are represented as interned strings in `torch/csrc/jit/interned_strings.h`; attribute dynamic dictionaries implemented in `torch/csrc/jit/attributes.h`.\r\n  * There is a linter to enforce invariants. Run the linter early and often!\r\n  * There are Python bindings using pybind11 for the IR in `torch/csrc/jit/python_ir.h`. These bindings are used in the definition of exporters for autograd functions.\r\n* **The forward tracer.** The forward tracer instruments invocations to autograd Functions in the forward pass and records the execution into the compiler IR. \r\n  * We implemented this tracing logic twice: once for C++ functions, and once for Python functions. The key functions are `tracedApply` in `torch/csrc/autograd/function.cpp` (C++ side) and `trace_create` in `torch/csrc/autograd/python_function.cpp`. Each of these functions are traced as PythonOps and CppOps.\r\n  * There is a calling convention change for C++ autograd functions; instead of writing `Transpose(0, 1).apply({ggI})[0]` you write `apply_fn<Transpose>(0, 1)(ggI)`.\r\n  * The tracer can only trace what code was actually executed; e.g., it only sees the conditional branch that was taken.\r\n  * Variables are marked as participating in a trace or not. A variable cannot participate in multiple traces simultaneously. You MUST dispose of a trace object (GC-wise) before you can use it again in another trace.\r\n* **The Toffee exporter.** The Toffee exporter takes a raw trace from the forward tracer (consisting of PythonOps and CppOps) and converts it to a Toffee format IR, which can then be exported to protobufs. The code lives in `torch/csrc/toffee/export.cpp`. Feel free to ignore the logic inside the `passes_state_transparently` conditional; this is for handling backwards traces.\r\n  * The way transformations are implemented is for each original op, we invoke the `primspec` associated with it (a new method associated with autograd Functions). This primspec takes as input the Toffee graph being constructed, and the inputs to the node (in the new Toffee IR graph), and extends the graph with the Toffee version of itself. Most primspecs are in Python, but C++ ops like BatchNorm and Convolution are done in C++.\r\n  * After this transformation, we export it to protobufs using the nanopb library. We've checked in the nanopb-protoc generated files for Toffee IR in `torch/csrc/toffee.pb.h` and `cpp`; nanopb has a pretty user-unfriendly interface, so there is a small wrapper around it in `torch/csrc/toffee.h` which simulates the Google protobuf C++ interface. See 619e20dee57e9642a8357bdb5f23e51f8b88d05e how it works.\r\n  * The manually checked in autogen files can be updated with `gen_toffee.sh`. If there are major changes to the protobuf, you may also have to update the C++ wrapper code.\r\n\r\n**Experimental code.** There are some experimental pieces which are not exercised by the model exporter, but we'd like to merge in. I don't recommend spending too much time on these pieces of code; they are at high risk of being rewritten/being heavily refactored, and they shouldn't affect anything in PyTorch proper.\r\n\r\n* **Optimization passes.** We have optimization passes for:\r\n  * An \"init\" pass (soon to be removed; `torch/crsc/jit/init_pass.cpp`)\r\n  * Dead code elimination (`torch/csrc/jit/dead_code_elimination.cpp`)\r\n  * Operator fusion (`torch/csrc/jit/graph_fusion.cpp`). Once operators are fused into fusion groups, the fusion compiler (`torch/csrc/jit/fusion_compiler.cpp`) handles actually compiling this to CUDA. This has only been lightly tested and is expected to be broken at the moment.\r\n* **The trace executor.** The trace executor takes a compiler IR trace, and converts it back into an autograd closure which we can then execute using the autograd engine. The main functionality for this is in `torch/crsc/autograd/functions/jit_closure.cpp`.\r\n* **The backwards tracer.** The backwards tracer is responsible for intercepting invocations of backward functions which were traced, and recording their backward traces to the trace (so forwards and backwards can be cooptimized.) A good chunk of the functionality here is implemented in `torch/csrc/autograd/functions/special.cpp`\r\n  * One significant source of complexity in the implementation is handling autograd functions whose backward passes are not traceable. In this case, we must fall back on the old PyTorch style (namely, generate an autograd trace when we compute forwards, and run that trace in backwards).\r\n\r\n**Miscellaneous changes.** (In order GitHub renders files)\r\n\r\n* New dependencies: pybind11 and nanopb. Pybind11 is a pretty nice wrapper for Python that can handle many conversions automatically. We intend keep migrating over PyTorch's Python wrapping code to use Pybind11 when applicable. Nanopb is a very small protobuf library; we use it to generate protobufs for the exporter. This prevents users from having to go through the pain of getting protobufs to work.\r\n* Submodules. PyTorch repo now has submodules: gloo (pre-existing), pybind11 and nanopb. The model for when to subtree versus when to submodule is primarily a question of how likely we will need to apply local patches to a project in PyTorch, versus being able to go through the upstream cycle. There is also a technical reason why gloo was submoduled, see #2426\r\n* Expect tests in the test suite. A new test method `assertExpected` lets you assert that a string matches some file which we have saved to disk. If the format changes, you can simply pass `--accept` to accept the new output. This makes it easy to see before and after in diffs.\r\n* A new test suite for the JIT. While the JIT is not a public facing component in this diff, it is a key part of the export pipeline, and so tests for it are in test_jit.py\r\n* There are C++ ports of chunk and split added to `torch/csrc/autograd/functions/tensor.cpp`\r\n* `torch/csrc/autograd/engine.cpp` Autograd is now reentrant; you can call autograd backward from within a backwards function. The implementation is the diff to engine.cpp, and our strategy is to just call back into the scheduling loop so we continue to service requests for the worker thread even when we have kicked off a subgraph task.\r\n* `torch/csrc/autograd/engine.cpp` Autograd callbacks have been generalized to support both pre and post callbacks. A pre callback is called before a function is invoked and can be used to rewrite inputs; a post callback is called after the function is invoked and we have output callbacks.\r\n* New `TemplateEnv` class in `torch/csrc/jit/code_template.h` which can be used to format C++ code fragments. It supports key-based template variables and can handle indentation correctly.\r\n* New utility `auto_unique_ptr` which constructs its object on the first dereference (rather than eagerly)\r\n* New utility `ResourceGuard`, an RAII class which can be explicitly released within its lexical scope, or automatically released when the scope ends.\r\n* New utility `fmap` which applies function to all elements of a vector (functionally, so no mutation)\r\n* New parameter to `state_dict()`, `keep_vars`, which causes `state_dict` to return parameters as Variables rather than tensors. This is used internally by the tracer.", "created_at": "2017-08-29T19:30:08Z", "updated_at": "2018-11-23T15:34:32Z", "closed_at": "2017-09-05T21:48:56Z", "merged_at": "2017-09-05T21:48:56Z", "merge_commit_sha": "4fc54af0104e49bf6113c53a064368e7ef2fed18", "assignee": null, "assignees": [], "requested_reviewers": [], "requested_teams": [], "labels": [], "milestone": null, "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2565/commits", "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2565/comments", "review_comment_url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2565/comments", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/9d9a63c2860b98e0b323b578af23c4a4a00796ab", "head": {"label": "ezyang:jit_clean", "ref": "jit_clean", "sha": "9d9a63c2860b98e0b323b578af23c4a4a00796ab", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "repo": {"id": 101798885, "node_id": "MDEwOlJlcG9zaXRvcnkxMDE3OTg4ODU=", "name": "pytorch", "full_name": "ezyang/pytorch", "private": false, "owner": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "html_url": "https://github.com/ezyang/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": true, "url": "https://api.github.com/repos/ezyang/pytorch", "forks_url": "https://api.github.com/repos/ezyang/pytorch/forks", "keys_url": "https://api.github.com/repos/ezyang/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/ezyang/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/ezyang/pytorch/teams", "hooks_url": "https://api.github.com/repos/ezyang/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/ezyang/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/ezyang/pytorch/events", "assignees_url": "https://api.github.com/repos/ezyang/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/ezyang/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/ezyang/pytorch/tags", "blobs_url": "https://api.github.com/repos/ezyang/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/ezyang/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/ezyang/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/ezyang/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/ezyang/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/ezyang/pytorch/languages", "stargazers_url": "https://api.github.com/repos/ezyang/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/ezyang/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/ezyang/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/ezyang/pytorch/subscription", "commits_url": "https://api.github.com/repos/ezyang/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/ezyang/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/ezyang/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/ezyang/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/ezyang/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/ezyang/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/ezyang/pytorch/merges", "archive_url": "https://api.github.com/repos/ezyang/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/ezyang/pytorch/downloads", "issues_url": "https://api.github.com/repos/ezyang/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/ezyang/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/ezyang/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/ezyang/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/ezyang/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/ezyang/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/ezyang/pytorch/deployments", "created_at": "2017-08-29T19:28:39Z", "updated_at": "2018-10-29T15:06:40Z", "pushed_at": "2018-11-21T22:30:09Z", "git_url": "git://github.com/ezyang/pytorch.git", "ssh_url": "git@github.com:ezyang/pytorch.git", "clone_url": "https://github.com/ezyang/pytorch.git", "svn_url": "https://github.com/ezyang/pytorch", "homepage": "http://pytorch.org", "size": 88254, "stargazers_count": 1, "watchers_count": 1, "language": "C++", "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 0, "mirror_url": null, "archived": false, "open_issues_count": 2, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 0, "open_issues": 2, "watchers": 1, "default_branch": "master"}}, "base": {"label": "pytorch:master", "ref": "master", "sha": "42448cf07f921aa1ca2f45d1c056ac025aeb9c1d", "user": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 65600975, "node_id": "MDEwOlJlcG9zaXRvcnk2NTYwMDk3NQ==", "name": "pytorch", "full_name": "pytorch/pytorch", "private": false, "owner": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/pytorch/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": false, "url": "https://api.github.com/repos/pytorch/pytorch", "forks_url": "https://api.github.com/repos/pytorch/pytorch/forks", "keys_url": "https://api.github.com/repos/pytorch/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/pytorch/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/pytorch/pytorch/teams", "hooks_url": "https://api.github.com/repos/pytorch/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/pytorch/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/pytorch/pytorch/events", "assignees_url": "https://api.github.com/repos/pytorch/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/pytorch/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/pytorch/pytorch/tags", "blobs_url": "https://api.github.com/repos/pytorch/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/pytorch/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/pytorch/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/pytorch/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/pytorch/pytorch/languages", "stargazers_url": "https://api.github.com/repos/pytorch/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/pytorch/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/pytorch/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/pytorch/pytorch/subscription", "commits_url": "https://api.github.com/repos/pytorch/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/pytorch/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/pytorch/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/pytorch/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/pytorch/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/pytorch/pytorch/merges", "archive_url": "https://api.github.com/repos/pytorch/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/pytorch/pytorch/downloads", "issues_url": "https://api.github.com/repos/pytorch/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/pytorch/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/pytorch/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/pytorch/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/pytorch/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/pytorch/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/pytorch/pytorch/deployments", "created_at": "2016-08-13T05:26:41Z", "updated_at": "2018-11-24T05:35:41Z", "pushed_at": "2018-11-24T05:34:07Z", "git_url": "git://github.com/pytorch/pytorch.git", "ssh_url": "git@github.com:pytorch/pytorch.git", "clone_url": "https://github.com/pytorch/pytorch.git", "svn_url": "https://github.com/pytorch/pytorch", "homepage": "http://pytorch.org", "size": 89651, "stargazers_count": 21577, "watchers_count": 21577, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 5149, "mirror_url": null, "archived": false, "open_issues_count": 2193, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 5149, "open_issues": 2193, "watchers": 21577, "default_branch": "master"}}, "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2565"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2565"}, "issue": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/2565"}, "comments": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/2565/comments"}, "review_comments": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2565/comments"}, "review_comment": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}"}, "commits": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2565/commits"}, "statuses": {"href": "https://api.github.com/repos/pytorch/pytorch/statuses/9d9a63c2860b98e0b323b578af23c4a4a00796ab"}}, "author_association": "CONTRIBUTOR", "body_html": "<p>This is a big PR. I don't recommend looking at the commits individually, although you may find it useful to blame lines here and there. (Naming disclaimer: the project uses \"toffee\" in some places, but this will need to be renamed.)</p>\n<p>A lot of folks contributed to this PR. Shout outs to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13488275\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prigoyal\">@prigoyal</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4529377\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/killeent\">@killeent</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30275821\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/houseroad\">@houseroad</a>; as well as indirect contributions from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=551151\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Yangqing\">@Yangqing</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9300575\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bddppq\">@bddppq</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4958441\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jerryzh168\">@jerryzh168</a>.</p>\n<p><strong>User facing interface.</strong> At the moment, most functionality is made available from <code>torch.jit</code> and <code>torch.toffee</code> modules. Some refactoring is on going for these interfaces; <code>torch.jit</code> will be made into a private namespace and <code>torch.toffee</code> will grow end-to-end conversion functions.</p>\n<p><strong>Key new pieces of code.</strong></p>\n<ul>\n<li><strong>The compiler IR.</strong> There is a new, mutable, SSA, in-memory compiler IR representation which represents a computation in PyTorch. We maintain accurate def-use chains. The IR is defined in <code>torch/csrc/jit/ir.h</code>. Some unusual things about the IR:\n<ul>\n<li>Deep learning networks generally have multiple inputs AND outputs. However, computation nodes in our IR only have a single output; instead, we project out the particular output we are interested in using a Select node. The \"Select invariant\" states that any multiple return node (that's most nodes) always has exactly as many Select nodes as outputs. There are no first-class tuples in the language.</li>\n<li>Computations in the graph are associated with stages, specifying whether or not they occurred in forwards/backwards/backwards-backwards. A computation in some stage cannot depend on a computation from a later stage; in fact, earlier stage computations will always be earlier in the IR than later stage computations.</li>\n<li>Nodes in the graph are dynamically typed; while there are some special nodes (CppOp and PythonOp), most nodes are represented as strings with arbitrary dictionaries of attribute names to attribute values. Names are represented as interned strings in <code>torch/csrc/jit/interned_strings.h</code>; attribute dynamic dictionaries implemented in <code>torch/csrc/jit/attributes.h</code>.</li>\n<li>There is a linter to enforce invariants. Run the linter early and often!</li>\n<li>There are Python bindings using pybind11 for the IR in <code>torch/csrc/jit/python_ir.h</code>. These bindings are used in the definition of exporters for autograd functions.</li>\n</ul>\n</li>\n<li><strong>The forward tracer.</strong> The forward tracer instruments invocations to autograd Functions in the forward pass and records the execution into the compiler IR.\n<ul>\n<li>We implemented this tracing logic twice: once for C++ functions, and once for Python functions. The key functions are <code>tracedApply</code> in <code>torch/csrc/autograd/function.cpp</code> (C++ side) and <code>trace_create</code> in <code>torch/csrc/autograd/python_function.cpp</code>. Each of these functions are traced as PythonOps and CppOps.</li>\n<li>There is a calling convention change for C++ autograd functions; instead of writing <code>Transpose(0, 1).apply({ggI})[0]</code> you write <code>apply_fn&lt;Transpose&gt;(0, 1)(ggI)</code>.</li>\n<li>The tracer can only trace what code was actually executed; e.g., it only sees the conditional branch that was taken.</li>\n<li>Variables are marked as participating in a trace or not. A variable cannot participate in multiple traces simultaneously. You MUST dispose of a trace object (GC-wise) before you can use it again in another trace.</li>\n</ul>\n</li>\n<li><strong>The Toffee exporter.</strong> The Toffee exporter takes a raw trace from the forward tracer (consisting of PythonOps and CppOps) and converts it to a Toffee format IR, which can then be exported to protobufs. The code lives in <code>torch/csrc/toffee/export.cpp</code>. Feel free to ignore the logic inside the <code>passes_state_transparently</code> conditional; this is for handling backwards traces.\n<ul>\n<li>The way transformations are implemented is for each original op, we invoke the <code>primspec</code> associated with it (a new method associated with autograd Functions). This primspec takes as input the Toffee graph being constructed, and the inputs to the node (in the new Toffee IR graph), and extends the graph with the Toffee version of itself. Most primspecs are in Python, but C++ ops like BatchNorm and Convolution are done in C++.</li>\n<li>After this transformation, we export it to protobufs using the nanopb library. We've checked in the nanopb-protoc generated files for Toffee IR in <code>torch/csrc/toffee.pb.h</code> and <code>cpp</code>; nanopb has a pretty user-unfriendly interface, so there is a small wrapper around it in <code>torch/csrc/toffee.h</code> which simulates the Google protobuf C++ interface. See <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/619e20dee57e9642a8357bdb5f23e51f8b88d05e/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/619e20dee57e9642a8357bdb5f23e51f8b88d05e\"><tt>619e20d</tt></a> how it works.</li>\n<li>The manually checked in autogen files can be updated with <code>gen_toffee.sh</code>. If there are major changes to the protobuf, you may also have to update the C++ wrapper code.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Experimental code.</strong> There are some experimental pieces which are not exercised by the model exporter, but we'd like to merge in. I don't recommend spending too much time on these pieces of code; they are at high risk of being rewritten/being heavily refactored, and they shouldn't affect anything in PyTorch proper.</p>\n<ul>\n<li><strong>Optimization passes.</strong> We have optimization passes for:\n<ul>\n<li>An \"init\" pass (soon to be removed; <code>torch/crsc/jit/init_pass.cpp</code>)</li>\n<li>Dead code elimination (<code>torch/csrc/jit/dead_code_elimination.cpp</code>)</li>\n<li>Operator fusion (<code>torch/csrc/jit/graph_fusion.cpp</code>). Once operators are fused into fusion groups, the fusion compiler (<code>torch/csrc/jit/fusion_compiler.cpp</code>) handles actually compiling this to CUDA. This has only been lightly tested and is expected to be broken at the moment.</li>\n</ul>\n</li>\n<li><strong>The trace executor.</strong> The trace executor takes a compiler IR trace, and converts it back into an autograd closure which we can then execute using the autograd engine. The main functionality for this is in <code>torch/crsc/autograd/functions/jit_closure.cpp</code>.</li>\n<li><strong>The backwards tracer.</strong> The backwards tracer is responsible for intercepting invocations of backward functions which were traced, and recording their backward traces to the trace (so forwards and backwards can be cooptimized.) A good chunk of the functionality here is implemented in <code>torch/csrc/autograd/functions/special.cpp</code>\n<ul>\n<li>One significant source of complexity in the implementation is handling autograd functions whose backward passes are not traceable. In this case, we must fall back on the old PyTorch style (namely, generate an autograd trace when we compute forwards, and run that trace in backwards).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Miscellaneous changes.</strong> (In order GitHub renders files)</p>\n<ul>\n<li>New dependencies: pybind11 and nanopb. Pybind11 is a pretty nice wrapper for Python that can handle many conversions automatically. We intend keep migrating over PyTorch's Python wrapping code to use Pybind11 when applicable. Nanopb is a very small protobuf library; we use it to generate protobufs for the exporter. This prevents users from having to go through the pain of getting protobufs to work.</li>\n<li>Submodules. PyTorch repo now has submodules: gloo (pre-existing), pybind11 and nanopb. The model for when to subtree versus when to submodule is primarily a question of how likely we will need to apply local patches to a project in PyTorch, versus being able to go through the upstream cycle. There is also a technical reason why gloo was submoduled, see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"250361814\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2426\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2426/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2426\">#2426</a></li>\n<li>Expect tests in the test suite. A new test method <code>assertExpected</code> lets you assert that a string matches some file which we have saved to disk. If the format changes, you can simply pass <code>--accept</code> to accept the new output. This makes it easy to see before and after in diffs.</li>\n<li>A new test suite for the JIT. While the JIT is not a public facing component in this diff, it is a key part of the export pipeline, and so tests for it are in test_jit.py</li>\n<li>There are C++ ports of chunk and split added to <code>torch/csrc/autograd/functions/tensor.cpp</code></li>\n<li><code>torch/csrc/autograd/engine.cpp</code> Autograd is now reentrant; you can call autograd backward from within a backwards function. The implementation is the diff to engine.cpp, and our strategy is to just call back into the scheduling loop so we continue to service requests for the worker thread even when we have kicked off a subgraph task.</li>\n<li><code>torch/csrc/autograd/engine.cpp</code> Autograd callbacks have been generalized to support both pre and post callbacks. A pre callback is called before a function is invoked and can be used to rewrite inputs; a post callback is called after the function is invoked and we have output callbacks.</li>\n<li>New <code>TemplateEnv</code> class in <code>torch/csrc/jit/code_template.h</code> which can be used to format C++ code fragments. It supports key-based template variables and can handle indentation correctly.</li>\n<li>New utility <code>auto_unique_ptr</code> which constructs its object on the first dereference (rather than eagerly)</li>\n<li>New utility <code>ResourceGuard</code>, an RAII class which can be explicitly released within its lexical scope, or automatically released when the scope ends.</li>\n<li>New utility <code>fmap</code> which applies function to all elements of a vector (functionally, so no mutation)</li>\n<li>New parameter to <code>state_dict()</code>, <code>keep_vars</code>, which causes <code>state_dict</code> to return parameters as Variables rather than tensors. This is used internally by the tracer.</li>\n</ul>", "body_text": "This is a big PR. I don't recommend looking at the commits individually, although you may find it useful to blame lines here and there. (Naming disclaimer: the project uses \"toffee\" in some places, but this will need to be renamed.)\nA lot of folks contributed to this PR. Shout outs to @zdevito, @apaszke, @prigoyal, @killeent, @houseroad; as well as indirect contributions from @Yangqing, @bddppq and @jerryzh168.\nUser facing interface. At the moment, most functionality is made available from torch.jit and torch.toffee modules. Some refactoring is on going for these interfaces; torch.jit will be made into a private namespace and torch.toffee will grow end-to-end conversion functions.\nKey new pieces of code.\n\nThe compiler IR. There is a new, mutable, SSA, in-memory compiler IR representation which represents a computation in PyTorch. We maintain accurate def-use chains. The IR is defined in torch/csrc/jit/ir.h. Some unusual things about the IR:\n\nDeep learning networks generally have multiple inputs AND outputs. However, computation nodes in our IR only have a single output; instead, we project out the particular output we are interested in using a Select node. The \"Select invariant\" states that any multiple return node (that's most nodes) always has exactly as many Select nodes as outputs. There are no first-class tuples in the language.\nComputations in the graph are associated with stages, specifying whether or not they occurred in forwards/backwards/backwards-backwards. A computation in some stage cannot depend on a computation from a later stage; in fact, earlier stage computations will always be earlier in the IR than later stage computations.\nNodes in the graph are dynamically typed; while there are some special nodes (CppOp and PythonOp), most nodes are represented as strings with arbitrary dictionaries of attribute names to attribute values. Names are represented as interned strings in torch/csrc/jit/interned_strings.h; attribute dynamic dictionaries implemented in torch/csrc/jit/attributes.h.\nThere is a linter to enforce invariants. Run the linter early and often!\nThere are Python bindings using pybind11 for the IR in torch/csrc/jit/python_ir.h. These bindings are used in the definition of exporters for autograd functions.\n\n\nThe forward tracer. The forward tracer instruments invocations to autograd Functions in the forward pass and records the execution into the compiler IR.\n\nWe implemented this tracing logic twice: once for C++ functions, and once for Python functions. The key functions are tracedApply in torch/csrc/autograd/function.cpp (C++ side) and trace_create in torch/csrc/autograd/python_function.cpp. Each of these functions are traced as PythonOps and CppOps.\nThere is a calling convention change for C++ autograd functions; instead of writing Transpose(0, 1).apply({ggI})[0] you write apply_fn<Transpose>(0, 1)(ggI).\nThe tracer can only trace what code was actually executed; e.g., it only sees the conditional branch that was taken.\nVariables are marked as participating in a trace or not. A variable cannot participate in multiple traces simultaneously. You MUST dispose of a trace object (GC-wise) before you can use it again in another trace.\n\n\nThe Toffee exporter. The Toffee exporter takes a raw trace from the forward tracer (consisting of PythonOps and CppOps) and converts it to a Toffee format IR, which can then be exported to protobufs. The code lives in torch/csrc/toffee/export.cpp. Feel free to ignore the logic inside the passes_state_transparently conditional; this is for handling backwards traces.\n\nThe way transformations are implemented is for each original op, we invoke the primspec associated with it (a new method associated with autograd Functions). This primspec takes as input the Toffee graph being constructed, and the inputs to the node (in the new Toffee IR graph), and extends the graph with the Toffee version of itself. Most primspecs are in Python, but C++ ops like BatchNorm and Convolution are done in C++.\nAfter this transformation, we export it to protobufs using the nanopb library. We've checked in the nanopb-protoc generated files for Toffee IR in torch/csrc/toffee.pb.h and cpp; nanopb has a pretty user-unfriendly interface, so there is a small wrapper around it in torch/csrc/toffee.h which simulates the Google protobuf C++ interface. See 619e20d how it works.\nThe manually checked in autogen files can be updated with gen_toffee.sh. If there are major changes to the protobuf, you may also have to update the C++ wrapper code.\n\n\n\nExperimental code. There are some experimental pieces which are not exercised by the model exporter, but we'd like to merge in. I don't recommend spending too much time on these pieces of code; they are at high risk of being rewritten/being heavily refactored, and they shouldn't affect anything in PyTorch proper.\n\nOptimization passes. We have optimization passes for:\n\nAn \"init\" pass (soon to be removed; torch/crsc/jit/init_pass.cpp)\nDead code elimination (torch/csrc/jit/dead_code_elimination.cpp)\nOperator fusion (torch/csrc/jit/graph_fusion.cpp). Once operators are fused into fusion groups, the fusion compiler (torch/csrc/jit/fusion_compiler.cpp) handles actually compiling this to CUDA. This has only been lightly tested and is expected to be broken at the moment.\n\n\nThe trace executor. The trace executor takes a compiler IR trace, and converts it back into an autograd closure which we can then execute using the autograd engine. The main functionality for this is in torch/crsc/autograd/functions/jit_closure.cpp.\nThe backwards tracer. The backwards tracer is responsible for intercepting invocations of backward functions which were traced, and recording their backward traces to the trace (so forwards and backwards can be cooptimized.) A good chunk of the functionality here is implemented in torch/csrc/autograd/functions/special.cpp\n\nOne significant source of complexity in the implementation is handling autograd functions whose backward passes are not traceable. In this case, we must fall back on the old PyTorch style (namely, generate an autograd trace when we compute forwards, and run that trace in backwards).\n\n\n\nMiscellaneous changes. (In order GitHub renders files)\n\nNew dependencies: pybind11 and nanopb. Pybind11 is a pretty nice wrapper for Python that can handle many conversions automatically. We intend keep migrating over PyTorch's Python wrapping code to use Pybind11 when applicable. Nanopb is a very small protobuf library; we use it to generate protobufs for the exporter. This prevents users from having to go through the pain of getting protobufs to work.\nSubmodules. PyTorch repo now has submodules: gloo (pre-existing), pybind11 and nanopb. The model for when to subtree versus when to submodule is primarily a question of how likely we will need to apply local patches to a project in PyTorch, versus being able to go through the upstream cycle. There is also a technical reason why gloo was submoduled, see #2426\nExpect tests in the test suite. A new test method assertExpected lets you assert that a string matches some file which we have saved to disk. If the format changes, you can simply pass --accept to accept the new output. This makes it easy to see before and after in diffs.\nA new test suite for the JIT. While the JIT is not a public facing component in this diff, it is a key part of the export pipeline, and so tests for it are in test_jit.py\nThere are C++ ports of chunk and split added to torch/csrc/autograd/functions/tensor.cpp\ntorch/csrc/autograd/engine.cpp Autograd is now reentrant; you can call autograd backward from within a backwards function. The implementation is the diff to engine.cpp, and our strategy is to just call back into the scheduling loop so we continue to service requests for the worker thread even when we have kicked off a subgraph task.\ntorch/csrc/autograd/engine.cpp Autograd callbacks have been generalized to support both pre and post callbacks. A pre callback is called before a function is invoked and can be used to rewrite inputs; a post callback is called after the function is invoked and we have output callbacks.\nNew TemplateEnv class in torch/csrc/jit/code_template.h which can be used to format C++ code fragments. It supports key-based template variables and can handle indentation correctly.\nNew utility auto_unique_ptr which constructs its object on the first dereference (rather than eagerly)\nNew utility ResourceGuard, an RAII class which can be explicitly released within its lexical scope, or automatically released when the scope ends.\nNew utility fmap which applies function to all elements of a vector (functionally, so no mutation)\nNew parameter to state_dict(), keep_vars, which causes state_dict to return parameters as Variables rather than tensors. This is used internally by the tracer.", "merged": true, "mergeable": null, "rebaseable": null, "mergeable_state": "unknown", "merged_by": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "comments": 3, "review_comments": 39, "maintainer_can_modify": false, "commits": 235, "additions": 8609, "deletions": 176, "changed_files": 123}