{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/136246121", "pull_request_review_id": 59755971, "id": 136246121, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNjI0NjEyMQ==", "diff_hunk": "@@ -0,0 +1,214 @@\n+import torch.autograd.function as function\n+import torch._C\n+from torch.autograd import Variable\n+from torch.nn import Module\n+import itertools\n+import types\n+import contextlib\n+import os\n+# Example how to use:\n+#\n+# import torch.jit\n+# model = model.RNNModel(args.model, ...)\n+# model = torch.jit.traced(model)\n+\n+\n+def flatten(x):\n+    \"\"\"\n+    Flatten an arbitrarily nested structure of Variables into\n+    a tuple of Variables.\n+    \"\"\"\n+    return tuple(function._iter_variables(x))\n+\n+\n+@contextlib.contextmanager\n+def _fork_rng(enabled=True):\n+    \"\"\"\n+    Forks the RNG, so that when you return, the RNG is reset\n+    to the state that it was previously in.  This is important\n+    if we are evaluating a trace twice, and it incorporates\n+    randomness: if we don't reset the seed, we might get totally\n+    different results!\n+\n+    TODO: Randomness in models is a big problem for reproduceability,\n+    because it means if we start executing models out of order,\n+    they may behave differently.  Interesting question is whether\n+    or not backwards pass ever has randomness.  I hope not.\n+    \"\"\"\n+    if not enabled:\n+        yield\n+        return\n+\n+    cpu_rng_state = torch.get_rng_state()\n+    gpu_rng_state = None\n+    if torch.cuda.is_available():\n+        gpu_rng_state = torch.cuda.get_rng_state()\n+\n+    yield\n+\n+    torch.set_rng_state(cpu_rng_state)\n+    if gpu_rng_state is not None:\n+        torch.cuda.set_rng_state(gpu_rng_state)", "path": "torch/jit.py", "position": 51, "original_position": 51, "commit_id": "9d9a63c2860b98e0b323b578af23c4a4a00796ab", "original_commit_id": "652c70025e6aca99d828240869277bb2364799e3", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Indeed, this seems to be the case. Should we add a torch.cuda helper for setting all of the states? (Maybe set_rng_state should set everything, and there should be a set_current_gpu_rng_state...)", "created_at": "2017-08-31T05:00:25Z", "updated_at": "2018-11-23T15:34:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/2565#discussion_r136246121", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2565", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/136246121"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2565#discussion_r136246121"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2565"}}, "body_html": "<p>Indeed, this seems to be the case. Should we add a torch.cuda helper for setting all of the states? (Maybe set_rng_state should set everything, and there should be a set_current_gpu_rng_state...)</p>", "body_text": "Indeed, this seems to be the case. Should we add a torch.cuda helper for setting all of the states? (Maybe set_rng_state should set everything, and there should be a set_current_gpu_rng_state...)", "in_reply_to_id": 135918743}