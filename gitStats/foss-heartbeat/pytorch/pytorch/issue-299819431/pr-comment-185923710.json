{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/185923710", "pull_request_review_id": 117413995, "id": 185923710, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NTkyMzcxMA==", "diff_hunk": "@@ -1,239 +1,394 @@\n import math\n+import types\n import random\n+import warnings\n \n import torch\n \n+##########################\n+# base initializer\n+##########################\n \n-def calculate_gain(nonlinearity, param=None):\n-    \"\"\"Return the recommended gain value for the given nonlinearity function.\n-    The values are as follows:\n \n-    ============ ==========================================\n-    nonlinearity gain\n-    ============ ==========================================\n-    linear       :math:`1`\n-    conv{1,2,3}d :math:`1`\n-    sigmoid      :math:`1`\n-    tanh         :math:`5 / 3`\n-    relu         :math:`\\sqrt{2}`\n-    leaky_relu   :math:`\\sqrt{2 / (1 + negative\\_slope^2)}`\n-    ============ ==========================================\n+class Initializer(object):\n+    \"\"\"\n+    Base class for all initializations.\n+    \"\"\"\n+    def __new__(cls, *non_tensor_args, **non_tensor_kwargs):\n+        first = None\n+        if len(non_tensor_args) > 0:\n+            first = non_tensor_args[0]\n+        if first is None:\n+            return object.__new__(cls)\n+        if isinstance(first, (torch.Tensor, torch.autograd.Variable)):\n+            return cls(*non_tensor_args[1:], **non_tensor_kwargs)(first)\n+        else:\n+            return object.__new__(cls)\n+\n+##########################\n+# initializers\n+##########################\n+\n+\n+class Ones(Initializer):\n+    r\"\"\"Fills the input Tensor with ones.\n \n     Args:\n-        nonlinearity: the nonlinear function (`nn.functional` name)\n-        param: optional parameter for the nonlinear function\n+        tensor: an n-dimensional `torch.Tensor`\n \n     Examples:\n-        >>> gain = nn.init.calculate_gain('leaky_relu')\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nn.init.ones_(w)\n+        >>> ones_init = nn.init.ones_()\n+        >>> ones_init(w)\n     \"\"\"\n-    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']\n-    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n-        return 1\n-    elif nonlinearity == 'tanh':\n-        return 5.0 / 3\n-    elif nonlinearity == 'relu':\n-        return math.sqrt(2.0)\n-    elif nonlinearity == 'leaky_relu':\n-        if param is None:\n-            negative_slope = 0.01\n-        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n-            # True/False are instances of int, hence check above\n-            negative_slope = param\n-        else:\n-            raise ValueError(\"negative_slope {} not a valid number\".format(param))\n-        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n-    else:\n-        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n+    def __call__(self, tensor):\n+        with torch.no_grad():\n+            return tensor.fill_(1)\n \n \n-def uniform(tensor, a=0, b=1):\n-    \"\"\"Fills the input Tensor or Variable with values drawn from the uniform\n-    distribution :math:`U(a, b)`.\n+class Zeros(Initializer):\n+    r\"\"\"Fills the input Tensor with zeros.\n \n     Args:\n-        tensor: an n-dimensional torch.Tensor or autograd.Variable\n-        a: the lower bound of the uniform distribution\n-        b: the upper bound of the uniform distribution\n+        tensor: an n-dimensional `torch.Tensor`\n \n     Examples:\n         >>> w = torch.Tensor(3, 5)\n-        >>> nn.init.uniform(w)\n+        >>> nn.init.zeros_(w)\n+        >>> zeros_init = nn.init.zeros_()\n+        >>> zeros_init(w)\n     \"\"\"\n-    with torch.no_grad():\n-        return tensor.uniform_(a, b)\n+    def __call__(self, tensor):\n+        with torch.no_grad():\n+            return tensor.fill_(0)\n \n \n-def normal(tensor, mean=0, std=1):\n-    \"\"\"Fills the input Tensor or Variable with values drawn from the normal\n-    distribution :math:`N(mean, std)`.\n+class Constant(Initializer):\n+    r\"\"\"Fills the input Tensor with the value :math:`\\text{val}`.\n \n     Args:\n-        tensor: an n-dimensional torch.Tensor or autograd.Variable\n-        mean: the mean of the normal distribution\n-        std: the standard deviation of the normal distribution\n+        tensor: an n-dimensional `torch.Tensor`\n+        val: the value to fill the tensor with\n \n     Examples:\n         >>> w = torch.Tensor(3, 5)\n-        >>> nn.init.normal(w)\n+        >>> nn.init.constant_(w, 0.3)\n+        >>> const_init = nn.init.constant_(0.3)\n+        >>> const_init(w)\n     \"\"\"\n-    with torch.no_grad():\n-        return tensor.normal_(mean, std)\n+    def __init__(self, val):\n+        self.val = val\n \n+    def __call__(self, tensor):\n+        with torch.no_grad():\n+            return tensor.fill_(self.val)\n \n-def constant(tensor, val):\n-    \"\"\"Fills the input Tensor or Variable with the value `val`.\n+\n+class Eye(Initializer):\n+    r\"\"\"Fills the 2-dimensional input `Tensor` with the identity\n+    matrix. Preserves the identity of the inputs in `Linear` layers, where as\n+    many inputs are preserved as possible.\n \n     Args:\n-        tensor: an n-dimensional torch.Tensor or autograd.Variable\n-        val: the value to fill the tensor with\n+        tensor: a 2-dimensional `torch.Tensor`\n \n     Examples:\n         >>> w = torch.Tensor(3, 5)\n-        >>> nn.init.constant(w, 0.3)\n+        >>> nn.init.eye_(w)\n+        >>> eye_init = nn.init.eye_()\n+        >>> eye_init(w)\n     \"\"\"\n-    with torch.no_grad():\n-        return tensor.fill_(val)\n+    def __call__(self, tensor):\n+        if tensor.ndimension() != 2:\n+            raise ValueError(\"Only tensors with 2 dimensions are supported\")\n \n+        with torch.no_grad():\n+            torch.eye(*tensor.shape, out=tensor)\n+        return tensor\n \n-def eye(tensor):\n-    \"\"\"Fills the 2-dimensional input Tensor or Variable with the identity\n-    matrix. Preserves the identity of the inputs in Linear layers, where as\n-    many inputs are preserved as possible.\n+\n+class Orthogonal(Initializer):\n+    r\"\"\"Fills the input `Tensor` with a (semi) orthogonal matrix, as\n+    described in \"Exact solutions to the nonlinear dynamics of learning in deep\n+    linear neural networks\" - Saxe, A. et al. (2013). The input tensor must have\n+    at least 2 dimensions, and for tensors with more than 2 dimensions the\n+    trailing dimensions are flattened.\n \n     Args:\n-        tensor: a 2-dimensional torch.Tensor or autograd.Variable\n+        tensor: an n-dimensional `torch.Tensor`, where :math:`n \\geq 2`\n+        gain: optional scaling factor\n \n     Examples:\n         >>> w = torch.Tensor(3, 5)\n-        >>> nn.init.eye(w)\n+        >>> nn.init.orthogonal_(w)\n+        >>> orth_init = nn.init.orthogonal_(5)\n+        >>> orth_init(w)\n     \"\"\"\n-    if tensor.ndimension() != 2:\n-        raise ValueError(\"Only tensors with 2 dimensions are supported\")\n+    def __init__(self, gain=1):\n+        self.gain = gain\n \n-    with torch.no_grad():\n-        torch.eye(*tensor.shape, out=tensor)\n-    return tensor\n+    def __call__(self, tensor):\n+        if tensor.ndimension() < 2:\n+            raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n \n+        rows = tensor.size(0)\n+        cols = tensor[0].numel()\n+        flattened = tensor.new(rows, cols).normal_(0, 1)\n \n-def dirac(tensor):\n-    \"\"\"Fills the {3, 4, 5}-dimensional input Tensor or Variable with the Dirac\n-    delta function. Preserves the identity of the inputs in Convolutional\n-    layers, where as many input channels are preserved as possible.\n+        if rows < cols:\n+            flattened.t_()\n+\n+        q, r = torch.qr(flattened)\n+        d = torch.diag(r, 0)\n+        ph = d.sign()\n+        q *= ph\n+\n+        if rows < cols:\n+            q.t_()\n+\n+        with torch.no_grad():\n+            tensor.view_as(q).copy_(q)\n+            tensor.mul_(self.gain)\n+        return tensor\n+\n+\n+class Uniform(Initializer):\n+    r\"\"\"Fills the input Tensor with values drawn from the uniform\n+    distribution :math:`\\mathcal{U}(a, b)`.\n \n     Args:\n-        tensor: a {3, 4, 5}-dimensional torch.Tensor or autograd.Variable\n+        tensor: an n-dimensional `torch.Tensor`\n+        a: the lower bound of the uniform distribution\n+        b: the upper bound of the uniform distribution\n \n     Examples:\n-        >>> w = torch.Tensor(3, 16, 5, 5)\n-        >>> nn.init.dirac(w)\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nn.init.uniform_(w, 4, 5)\n+        >>> uniform_init = nn.init.uniform_(4, 5)\n+        >>> uniform_init(w)\n     \"\"\"\n-    dimensions = tensor.ndimension()\n-    if dimensions not in [3, 4, 5]:\n-        raise ValueError(\"Only tensors with 3, 4, or 5 dimensions are supported\")\n+    def __init__(self, a=0, b=1):\n+        self.a = a\n+        self.b = b\n \n-    sizes = tensor.size()\n-    min_dim = min(sizes[0], sizes[1])\n-    with torch.no_grad():\n-        tensor.zero_()\n+    def __call__(self, tensor):\n+        with torch.no_grad():\n+            return tensor.uniform_(self.a, self.b)\n \n-        for d in range(min_dim):\n-            if dimensions == 3:  # Temporal convolution\n-                tensor[d, d, tensor.size(2) // 2] = 1\n-            elif dimensions == 4:  # Spatial convolution\n-                tensor[d, d, tensor.size(2) // 2, tensor.size(3) // 2] = 1\n-            else:  # Volumetric convolution\n-                tensor[d, d, tensor.size(2) // 2, tensor.size(3) // 2, tensor.size(4) // 2] = 1\n-    return tensor\n \n+class Normal(Initializer):\n+    r\"\"\"Fills the input Tensor with values drawn from the normal\n+    distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std})`.\n \n-def _calculate_fan_in_and_fan_out(tensor):\n-    dimensions = tensor.ndimension()\n-    if dimensions < 2:\n-        raise ValueError(\"Fan in and fan out can not be computed for tensor with less than 2 dimensions\")\n+    Args:\n+        tensor: an n-dimensional `torch.Tensor`\n+        mean: the mean of the normal distribution\n+        std: the standard deviation of the normal distribution\n \n-    if dimensions == 2:  # Linear\n-        fan_in = tensor.size(1)\n-        fan_out = tensor.size(0)\n-    else:\n-        num_input_fmaps = tensor.size(1)\n-        num_output_fmaps = tensor.size(0)\n-        receptive_field_size = 1\n-        if tensor.dim() > 2:\n-            receptive_field_size = tensor[0][0].numel()\n-        fan_in = num_input_fmaps * receptive_field_size\n-        fan_out = num_output_fmaps * receptive_field_size\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nn.init.normal_(w)\n+        >>> normal_init = nn.init.normal_()\n+        >>> normal_init(w)\n+    \"\"\"\n+    def __init__(self, mean=0, std=1):\n+        self.mean = mean\n+        self.std = std\n+\n+    def __call__(self, tensor):\n+        with torch.no_grad():\n+            return tensor.normal_(self.mean, self.std)\n \n-    return fan_in, fan_out\n \n+class Dirac(Initializer):\n+    r\"\"\"Fills the {3, 4, 5}-dimensional input `Tensor` with the Dirac\n+    delta function. Preserves the identity of the inputs in `Convolutional`\n+    layers, where as many input channels are preserved as possible.\n+\n+    Args:\n+        tensor: a {3, 4, 5}-dimensional `torch.Tensor`\n \n-def xavier_uniform(tensor, gain=1):\n-    \"\"\"Fills the input Tensor or Variable with values according to the method\n+    Examples:\n+        >>> w = torch.Tensor(3, 16, 5, 5)\n+        >>> nn.init.dirac_(w)\n+        >>> dirac_init = nn.init.dirac_()\n+        >>> dirac_init(w)\n+    \"\"\"\n+    def __call__(self, tensor):\n+        dimensions = tensor.ndimension()\n+        if dimensions not in [3, 4, 5]:\n+            raise ValueError(\"Only tensors with 3, 4, or 5 dimensions are supported\")\n+\n+        sizes = tensor.size()\n+        min_dim = min(sizes[0], sizes[1])\n+        with torch.no_grad():\n+            tensor.zero_()\n+\n+            for d in range(min_dim):\n+                if dimensions == 3:  # Temporal convolution\n+                    tensor[d, d, tensor.size(2) // 2] = 1\n+                elif dimensions == 4:  # Spatial convolution\n+                    tensor[d, d, tensor.size(2) // 2, tensor.size(3) // 2] = 1\n+                else:  # Volumetric convolution\n+                    tensor[d, d, tensor.size(2) // 2, tensor.size(3) // 2, tensor.size(4) // 2] = 1\n+        return tensor\n+\n+\n+class Sparse(Initializer):\n+    r\"\"\"Fills the 2D input `Tensor` as a sparse matrix, where the\n+    non-zero elements will be drawn from the normal distribution\n+    :math:`\\mathcal{N}(0, 0.01)`, as described in \"Deep learning via\n+    Hessian-free optimization\" - Martens, J. (2010).\n+\n+    Args:\n+        tensor: an n-dimensional `torch.Tensor`\n+        sparsity: The fraction of elements in each column to be set to zero\n+        std: the standard deviation of the normal distribution used to generate\n+            the non-zero values\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nn.init.sparse_(w, sparsity=0.1)\n+        >>> sparse_init = nn.init.sparse_(sparsity=0.1)\n+        >>> sparse_init(w)\n+    \"\"\"\n+    def __init__(self, sparsity, std=0.01):\n+        self.sparsity = sparsity\n+        self.std = std\n+\n+    def __call__(self, tensor):\n+        if tensor.ndimension() != 2:\n+            raise ValueError(\"Only tensors with 2 dimensions are supported\")\n+\n+        rows, cols = tensor.shape\n+        num_zeros = int(math.ceil(rows * self.sparsity))\n+\n+        with torch.no_grad():\n+            tensor.normal_(0, self.std)\n+            for col_idx in range(cols):\n+                row_indices = list(range(rows))\n+                random.shuffle(row_indices)\n+                zero_indices = row_indices[:num_zeros]\n+                for row_idx in zero_indices:\n+                    tensor[row_idx, col_idx] = 0", "path": "torch/nn/init.py", "position": 384, "original_position": 384, "commit_id": "1b8eecd6b4922b1b50e970563bc446ac4b997e9f", "original_commit_id": "1b8eecd6b4922b1b50e970563bc446ac4b997e9f", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "Could you rebase on top of master? A PR that optimized this operation has been recently merged", "created_at": "2018-05-03T20:16:39Z", "updated_at": "2018-11-23T15:43:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/5382#discussion_r185923710", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5382", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/185923710"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5382#discussion_r185923710"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5382"}}, "body_html": "<p>Could you rebase on top of master? A PR that optimized this operation has been recently merged</p>", "body_text": "Could you rebase on top of master? A PR that optimized this operation has been recently merged"}