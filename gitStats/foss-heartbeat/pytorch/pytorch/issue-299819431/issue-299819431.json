{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5382", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5382/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5382/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5382/events", "html_url": "https://github.com/pytorch/pytorch/pull/5382", "id": 299819431, "node_id": "MDExOlB1bGxSZXF1ZXN0MTcxMDkzMzc5", "number": 5382, "title": "Weight & Bias initializers for Linear, Conv, RNN", "user": {"login": "kevinzakka", "id": 10518920, "node_id": "MDQ6VXNlcjEwNTE4OTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/10518920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kevinzakka", "html_url": "https://github.com/kevinzakka", "followers_url": "https://api.github.com/users/kevinzakka/followers", "following_url": "https://api.github.com/users/kevinzakka/following{/other_user}", "gists_url": "https://api.github.com/users/kevinzakka/gists{/gist_id}", "starred_url": "https://api.github.com/users/kevinzakka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kevinzakka/subscriptions", "organizations_url": "https://api.github.com/users/kevinzakka/orgs", "repos_url": "https://api.github.com/users/kevinzakka/repos", "events_url": "https://api.github.com/users/kevinzakka/events{/privacy}", "received_events_url": "https://api.github.com/users/kevinzakka/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-02-23T19:12:49Z", "updated_at": "2018-11-23T15:43:36Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5382", "html_url": "https://github.com/pytorch/pytorch/pull/5382", "diff_url": "https://github.com/pytorch/pytorch/pull/5382.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5382.patch"}, "body_html": "<p>As mentioned in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"299693776\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5370\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/5370/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/5370\">#5370</a>, here's what adding weight and bias string args to some of the layers could look like.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> use a string</span>\nx <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-v\">weight_init</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>xavier_uniform<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">bias_init</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>zeros<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> use a function</span>\nx <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-v\">weight_init</span><span class=\"pl-k\">=</span>init.uniform(<span class=\"pl-k\">-</span><span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>))\nx <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-v\">weight_init</span><span class=\"pl-k\">=</span>init.xavier_uniform(<span class=\"pl-v\">gain</span><span class=\"pl-k\">=</span>init.calculate_gain(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tanh<span class=\"pl-pds\">'</span></span>)))</pre></div>\n<p>Example of backwards-compatibility:</p>\n<div class=\"highlight highlight-source-python\"><pre>w <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\nnn.init.constant(w, <span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints [[1, 1], [1, 1]]</span>\nnn.init.Constant(<span class=\"pl-c1\">0</span>)(w) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints [[0, 0], [0, 0]]</span></pre></div>\n<p>Finally, test to show that new implementation of <code>kaiming</code> and <code>xavier</code> work:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> same size input and output -&gt; in = out = average -&gt; approx same std</span>\nW <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">700</span>, <span class=\"pl-c1\">700</span>)\nxavier <span class=\"pl-k\">=</span> nn.init.xavier_uniform(W.clone(), <span class=\"pl-v\">gain</span><span class=\"pl-k\">=</span>nn.init.calculate_gain(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nkaiming <span class=\"pl-k\">=</span> nn.init.kaiming_uniform(W.clone())\n<span class=\"pl-c1\">print</span>(torch.std(xavier)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints 0.053412361240471495</span>\n<span class=\"pl-c1\">print</span>(torch.std(kaiming)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints 0.05350749617735303</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> diff size input and output -&gt; but fan set to avg -&gt; approx same std</span>\nW <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">700</span>, <span class=\"pl-c1\">500</span>)\nxavier <span class=\"pl-k\">=</span> nn.init.xavier_uniform(W.clone(), <span class=\"pl-v\">gain</span><span class=\"pl-k\">=</span>nn.init.calculate_gain(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nkaiming <span class=\"pl-k\">=</span> nn.init.kaiming_uniform(W.clone(), <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fan_avg<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">print</span>(torch.std(xavier)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints 0.057737772865026314</span>\n<span class=\"pl-c1\">print</span>(torch.std(kaiming)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints 0.057677882183886975</span></pre></div>\n<p>Also added <code>selu</code> support to <code>calculate_gain</code>.</p>", "body_text": "As mentioned in #5370, here's what adding weight and bias string args to some of the layers could look like.\n# use a string\nx = nn.Linear(784, 512, weight_init='xavier_uniform', bias_init='zeros')\n\n# use a function\nx = nn.Linear(784, 512, weight_init=init.uniform(-5, 5))\nx = nn.Linear(784, 512, weight_init=init.xavier_uniform(gain=init.calculate_gain('tanh')))\nExample of backwards-compatibility:\nw = torch.randn(2, 2)\nnn.init.constant(w, 1) # prints [[1, 1], [1, 1]]\nnn.init.Constant(0)(w) # prints [[0, 0], [0, 0]]\nFinally, test to show that new implementation of kaiming and xavier work:\n# same size input and output -> in = out = average -> approx same std\nW = torch.randn(700, 700)\nxavier = nn.init.xavier_uniform(W.clone(), gain=nn.init.calculate_gain('relu'))\nkaiming = nn.init.kaiming_uniform(W.clone())\nprint(torch.std(xavier)) # prints 0.053412361240471495\nprint(torch.std(kaiming)) # prints 0.05350749617735303\n\n# diff size input and output -> but fan set to avg -> approx same std\nW = torch.randn(700, 500)\nxavier = nn.init.xavier_uniform(W.clone(), gain=nn.init.calculate_gain('relu'))\nkaiming = nn.init.kaiming_uniform(W.clone(), mode=\"fan_avg\")\nprint(torch.std(xavier)) # prints 0.057737772865026314\nprint(torch.std(kaiming)) # prints 0.057677882183886975\nAlso added selu support to calculate_gain.", "body": "As mentioned in #5370, here's what adding weight and bias string args to some of the layers could look like.\r\n```python\r\n# use a string\r\nx = nn.Linear(784, 512, weight_init='xavier_uniform', bias_init='zeros')\r\n\r\n# use a function\r\nx = nn.Linear(784, 512, weight_init=init.uniform(-5, 5))\r\nx = nn.Linear(784, 512, weight_init=init.xavier_uniform(gain=init.calculate_gain('tanh')))\r\n```\r\n\r\nExample of backwards-compatibility:\r\n```python\r\nw = torch.randn(2, 2)\r\nnn.init.constant(w, 1) # prints [[1, 1], [1, 1]]\r\nnn.init.Constant(0)(w) # prints [[0, 0], [0, 0]]\r\n```\r\n\r\nFinally, test to show that new implementation of `kaiming` and `xavier` work:\r\n```python\r\n# same size input and output -> in = out = average -> approx same std\r\nW = torch.randn(700, 700)\r\nxavier = nn.init.xavier_uniform(W.clone(), gain=nn.init.calculate_gain('relu'))\r\nkaiming = nn.init.kaiming_uniform(W.clone())\r\nprint(torch.std(xavier)) # prints 0.053412361240471495\r\nprint(torch.std(kaiming)) # prints 0.05350749617735303\r\n\r\n# diff size input and output -> but fan set to avg -> approx same std\r\nW = torch.randn(700, 500)\r\nxavier = nn.init.xavier_uniform(W.clone(), gain=nn.init.calculate_gain('relu'))\r\nkaiming = nn.init.kaiming_uniform(W.clone(), mode=\"fan_avg\")\r\nprint(torch.std(xavier)) # prints 0.057737772865026314\r\nprint(torch.std(kaiming)) # prints 0.057677882183886975\r\n```\r\n\r\nAlso added `selu` support to `calculate_gain`."}