{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/310720095", "html_url": "https://github.com/pytorch/pytorch/issues/1886#issuecomment-310720095", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1886", "id": 310720095, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDcyMDA5NQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-23T17:02:42Z", "updated_at": "2017-06-23T17:02:42Z", "author_association": "MEMBER", "body_html": "<p>I'm not aware of any techniques that define derivatives of functions over sets that have no limit points (e.g. all discrete spaces, including integers). Of course it's possible to define derivatives of approximations of bitwise ops (mathematically, bitwise ops have different semantics depending on what's your encoding), however you still need floating-point values to compute the gradients. In <code>torch.autograd</code>, the gradient has to be of a type matching the data used in forward computation, so we can't allow integer tensors there. Even if it doesn't crash, I won't guarantee that what you're seeing in <code>.grad</code> is correct (because it's undefined in this setting).</p>", "body_text": "I'm not aware of any techniques that define derivatives of functions over sets that have no limit points (e.g. all discrete spaces, including integers). Of course it's possible to define derivatives of approximations of bitwise ops (mathematically, bitwise ops have different semantics depending on what's your encoding), however you still need floating-point values to compute the gradients. In torch.autograd, the gradient has to be of a type matching the data used in forward computation, so we can't allow integer tensors there. Even if it doesn't crash, I won't guarantee that what you're seeing in .grad is correct (because it's undefined in this setting).", "body": "I'm not aware of any techniques that define derivatives of functions over sets that have no limit points (e.g. all discrete spaces, including integers). Of course it's possible to define derivatives of approximations of bitwise ops (mathematically, bitwise ops have different semantics depending on what's your encoding), however you still need floating-point values to compute the gradients. In `torch.autograd`, the gradient has to be of a type matching the data used in forward computation, so we can't allow integer tensors there. Even if it doesn't crash, I won't guarantee that what you're seeing in `.grad` is correct (because it's undefined in this setting)."}