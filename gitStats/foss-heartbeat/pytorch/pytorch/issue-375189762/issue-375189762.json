{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13263", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13263/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13263/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13263/events", "html_url": "https://github.com/pytorch/pytorch/pull/13263", "id": 375189762, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI2NjkzMTM5", "number": 13263, "title": "Native batch norm", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-10-29T19:56:21Z", "updated_at": "2018-11-07T05:14:54Z", "closed_at": "2018-11-07T04:07:28Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13263", "html_url": "https://github.com/pytorch/pytorch/pull/13263", "diff_url": "https://github.com/pytorch/pytorch/pull/13263.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13263.patch"}, "body_html": "<ul>\n<li>Move batch norm from TH(CU)NN to native</li>\n<li>Speedups in many cases (e.g. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"363064888\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12006\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12006/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12006\">#12006</a>) for CUDA due to new block/grid layout and Welford-type mean/variance calculations (the latter for training mode)</li>\n<li>It splits the forward kernel in two pieces and reuses the evaluation kernel for the transformation.</li>\n<li>We change the meaning of save_mean and save_invstd (aka save_var) to accscalar to maintain reasonable precision.</li>\n</ul>\n<p>Compared to the ill-fated <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"367171904\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12368\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12368/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12368\">#12368</a></p>\n<ul>\n<li>I changed the CPU kernel to not call <code>.sum()</code> from within parallel for. This seemed to have caused the breakage (NaN-results) in TestModels.test_dcgan_netG (thank you <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30275821\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/houseroad\">@houseroad</a> for the repro, errors in assessment of the fix are my own)</li>\n<li>I updated the Half-&gt;Float upcasting in tensors to go through <code>t.type().scalarType()</code> instead of <code>t.dtype()</code>.</li>\n<li>I have merged master</li>\n</ul>", "body_text": "Move batch norm from TH(CU)NN to native\nSpeedups in many cases (e.g. #12006) for CUDA due to new block/grid layout and Welford-type mean/variance calculations (the latter for training mode)\nIt splits the forward kernel in two pieces and reuses the evaluation kernel for the transformation.\nWe change the meaning of save_mean and save_invstd (aka save_var) to accscalar to maintain reasonable precision.\n\nCompared to the ill-fated #12368\n\nI changed the CPU kernel to not call .sum() from within parallel for. This seemed to have caused the breakage (NaN-results) in TestModels.test_dcgan_netG (thank you @houseroad for the repro, errors in assessment of the fix are my own)\nI updated the Half->Float upcasting in tensors to go through t.type().scalarType() instead of t.dtype().\nI have merged master", "body": "- Move batch norm from TH(CU)NN to native\r\n- Speedups in many cases (e.g. #12006) for CUDA due to new block/grid layout and Welford-type mean/variance calculations (the latter for training mode)\r\n- It splits the forward kernel in two pieces and reuses the evaluation kernel for the transformation.\r\n- We change the meaning of save_mean and save_invstd (aka save_var) to accscalar to maintain reasonable precision.\r\n\r\nCompared to the ill-fated #12368 \r\n- I changed the CPU kernel to not call `.sum()` from within parallel for. This seemed to have caused the breakage (NaN-results) in TestModels.test_dcgan_netG (thank you @houseroad for the repro, errors in assessment of the fix are my own)\r\n- I updated the Half->Float upcasting in tensors to go through `t.type().scalarType()` instead of `t.dtype()`.\r\n- I have merged master\r\n"}