{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348747794", "html_url": "https://github.com/pytorch/pytorch/pull/3986#issuecomment-348747794", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3986", "id": 348747794, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODc0Nzc5NA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-03T08:09:53Z", "updated_at": "2017-12-03T08:09:53Z", "author_association": "MEMBER", "body_html": "<p>Since \"momentum correction\" is not something standard yet, and <code>step</code> is meant to have the same interface across all optimizers, I'd rather not merge this PR. A different argument is that if someone uses multiple parameter groups, the correction might need to be different for each group, so this still doesn't work in this form. I think it's best to just make the buffer names we use in our optimizers consistent and part of the public interface, and make such things part of the training scripts. In case of imagenet example, one could just make the LR scheduling function decay momentum (mutate the optimizer) if it decays LR. We do that for LR already.</p>\n<p>I'll close this for now, but feel free to continue the discussion here if you disagree.</p>", "body_text": "Since \"momentum correction\" is not something standard yet, and step is meant to have the same interface across all optimizers, I'd rather not merge this PR. A different argument is that if someone uses multiple parameter groups, the correction might need to be different for each group, so this still doesn't work in this form. I think it's best to just make the buffer names we use in our optimizers consistent and part of the public interface, and make such things part of the training scripts. In case of imagenet example, one could just make the LR scheduling function decay momentum (mutate the optimizer) if it decays LR. We do that for LR already.\nI'll close this for now, but feel free to continue the discussion here if you disagree.", "body": "Since \"momentum correction\" is not something standard yet, and `step` is meant to have the same interface across all optimizers, I'd rather not merge this PR. A different argument is that if someone uses multiple parameter groups, the correction might need to be different for each group, so this still doesn't work in this form. I think it's best to just make the buffer names we use in our optimizers consistent and part of the public interface, and make such things part of the training scripts. In case of imagenet example, one could just make the LR scheduling function decay momentum (mutate the optimizer) if it decays LR. We do that for LR already.\r\n\r\nI'll close this for now, but feel free to continue the discussion here if you disagree."}