{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/312899250", "html_url": "https://github.com/pytorch/pytorch/issues/1193#issuecomment-312899250", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1193", "id": 312899250, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjg5OTI1MA==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-04T15:01:24Z", "updated_at": "2017-07-04T15:01:24Z", "author_association": "COLLABORATOR", "body_html": "<p>I think there is a slight problem with how the <code>out=</code> interface works.</p>\n<ul>\n<li>What you expect here is that the result of the computation should be placed in the given tensor. You expect it to be equivalent to:</li>\n</ul>\n<pre><code>t = func(args)\n</code></pre>\n<ul>\n<li>But what is implemented is actually different. It expects a tensor that will be used to store the output. The behavior is actually equivalent to:</li>\n</ul>\n<pre><code>tmp = func(args)\nt.resize_as(tmp)\nt.copy_(tmp)\n</code></pre>\n<p>Since a <code>resize</code> is called on the tensor, you cannot expect the result to be written inplace in whatever Tensor was given. In this case you observe a side effect of the <code>resize</code> where you resize a tensor of size (3, 3) to (1, 3, 3) which completely invalidates the storage/stride of the original tensor.</p>\n<p>The question is: do we want to change this to implement the first behavior, because you can trigger the same kind of error for many other function (min/max/mean for example)?<br>\nThis will prevent usage where a buffer tensor (which currently has unknown size and content) as argument for out as this tensor should be resized by the user before giving it to the function's <code>out</code> kwarg.</p>", "body_text": "I think there is a slight problem with how the out= interface works.\n\nWhat you expect here is that the result of the computation should be placed in the given tensor. You expect it to be equivalent to:\n\nt = func(args)\n\n\nBut what is implemented is actually different. It expects a tensor that will be used to store the output. The behavior is actually equivalent to:\n\ntmp = func(args)\nt.resize_as(tmp)\nt.copy_(tmp)\n\nSince a resize is called on the tensor, you cannot expect the result to be written inplace in whatever Tensor was given. In this case you observe a side effect of the resize where you resize a tensor of size (3, 3) to (1, 3, 3) which completely invalidates the storage/stride of the original tensor.\nThe question is: do we want to change this to implement the first behavior, because you can trigger the same kind of error for many other function (min/max/mean for example)?\nThis will prevent usage where a buffer tensor (which currently has unknown size and content) as argument for out as this tensor should be resized by the user before giving it to the function's out kwarg.", "body": "I think there is a slight problem with how the `out=` interface works.\r\n* What you expect here is that the result of the computation should be placed in the given tensor. You expect it to be equivalent to:\r\n```\r\nt = func(args)\r\n```\r\n\r\n* But what is implemented is actually different. It expects a tensor that will be used to store the output. The behavior is actually equivalent to:\r\n```\r\ntmp = func(args)\r\nt.resize_as(tmp)\r\nt.copy_(tmp)\r\n```\r\nSince a `resize` is called on the tensor, you cannot expect the result to be written inplace in whatever Tensor was given. In this case you observe a side effect of the `resize` where you resize a tensor of size (3, 3) to (1, 3, 3) which completely invalidates the storage/stride of the original tensor.\r\n\r\n\r\nThe question is: do we want to change this to implement the first behavior, because you can trigger the same kind of error for many other function (min/max/mean for example)?\r\nThis will prevent usage where a buffer tensor (which currently has unknown size and content) as argument for out as this tensor should be resized by the user before giving it to the function's `out` kwarg."}