{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13957", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13957/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13957/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13957/events", "html_url": "https://github.com/pytorch/pytorch/issues/13957", "id": 380558670, "node_id": "MDU6SXNzdWUzODA1NTg2NzA=", "number": 13957, "title": "leakage in CPU inverse()", "user": {"login": "jongwook", "id": 266841, "node_id": "MDQ6VXNlcjI2Njg0MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/266841?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jongwook", "html_url": "https://github.com/jongwook", "followers_url": "https://api.github.com/users/jongwook/followers", "following_url": "https://api.github.com/users/jongwook/following{/other_user}", "gists_url": "https://api.github.com/users/jongwook/gists{/gist_id}", "starred_url": "https://api.github.com/users/jongwook/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jongwook/subscriptions", "organizations_url": "https://api.github.com/users/jongwook/orgs", "repos_url": "https://api.github.com/users/jongwook/repos", "events_url": "https://api.github.com/users/jongwook/events{/privacy}", "received_events_url": "https://api.github.com/users/jongwook/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1002728630, "node_id": "MDU6TGFiZWwxMDAyNzI4NjMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/1.0", "name": "1.0", "color": "f9db31", "default": false}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-14T06:46:12Z", "updated_at": "2018-11-19T19:01:45Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Repeatedly calling <code>torch.inverse()</code> makes memory leakage, which gets severer when the computation graph is complex.</p>\n<h2>To Reproduce</h2>\n<p>Minimum working example below; the reported memory usage increases linearly.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> resource <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n\n\nS <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nX <span class=\"pl-k\">=</span> torch.randn(S, S, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span>(i, <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\t</span><span class=\"pl-pds\">'</span></span>, getrusage(<span class=\"pl-c1\">RUSAGE_SELF</span>).ru_maxrss, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>kB used<span class=\"pl-pds\">'</span></span>)\n\n    Z <span class=\"pl-k\">=</span> X.sum(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">True</span>) <span class=\"pl-k\">+</span> torch.linspace(<span class=\"pl-k\">-</span>S, S, S)\n    Y <span class=\"pl-k\">=</span> X <span class=\"pl-k\">*</span> torch.erf(Z).sum(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">True</span>)\n    loss <span class=\"pl-k\">=</span> Y.t().mm(Y).inverse().sum() <span class=\"pl-k\">+</span> Y.sum()\n    loss.backward()</pre></div>\n<p>Below is the output; exact number might differ each run, but the memory usage always increases linearly. This will eventually hog all of available memory and cause problems in long training runs.</p>\n<pre><code>0 \t 139564 kB used\n100 \t 157264 kB used\n200 \t 157988 kB used\n300 \t 158888 kB used\n400 \t 159676 kB used\n500 \t 160392 kB used\n600 \t 161788 kB used\n700 \t 162140 kB used\n800 \t 162892 kB used\n900 \t 163800 kB used\n1000 \t 164964 kB used\n...\n</code></pre>\n<p>(Calling <code>backward()</code> is just for demonstration purposes, and the problem persists with no autograd at all)</p>\n<h2>Expected behavior</h2>\n<p>The memory usage should stay constant at a certain level.</p>\n<h2>Environment</h2>\n<p>(CUDA info below should be irrelevant since I'm only using CPU tensors in the example above)</p>\n<pre><code>PyTorch version: 1.0.0.dev20181113\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\n\nNvidia driver version: 384.111\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/cuda-9.0/lib64/libcudnn.so\n/usr/local/cuda-9.0/lib64/libcudnn.so.5\n/usr/local/cuda-9.0/lib64/libcudnn.so.5.1.10\n/usr/local/cuda-9.0/lib64/libcudnn.so.6\n/usr/local/cuda-9.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.3\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.1)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.1)\n[pip3] torchvision (0.2.1)\n[conda] pytorch-nightly           1.0.0.dev20181113 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\n</code></pre>\n<h2>Additional context</h2>\n<p>It's a regression from 0.4.1, which runs the example above without problems. The same code runs fine on CUDA tensors.</p>", "body_text": "\ud83d\udc1b Bug\nRepeatedly calling torch.inverse() makes memory leakage, which gets severer when the computation graph is complex.\nTo Reproduce\nMinimum working example below; the reported memory usage increases linearly.\nimport torch\nfrom resource import *\n\n\nS = 256\nX = torch.randn(S, S, requires_grad=True)\n\nfor i in range(10000):\n    if i % 100 == 0:\n        print(i, '\\t', getrusage(RUSAGE_SELF).ru_maxrss, 'kB used')\n\n    Z = X.sum(1, True) + torch.linspace(-S, S, S)\n    Y = X * torch.erf(Z).sum(1, True)\n    loss = Y.t().mm(Y).inverse().sum() + Y.sum()\n    loss.backward()\nBelow is the output; exact number might differ each run, but the memory usage always increases linearly. This will eventually hog all of available memory and cause problems in long training runs.\n0 \t 139564 kB used\n100 \t 157264 kB used\n200 \t 157988 kB used\n300 \t 158888 kB used\n400 \t 159676 kB used\n500 \t 160392 kB used\n600 \t 161788 kB used\n700 \t 162140 kB used\n800 \t 162892 kB used\n900 \t 163800 kB used\n1000 \t 164964 kB used\n...\n\n(Calling backward() is just for demonstration purposes, and the problem persists with no autograd at all)\nExpected behavior\nThe memory usage should stay constant at a certain level.\nEnvironment\n(CUDA info below should be irrelevant since I'm only using CPU tensors in the example above)\nPyTorch version: 1.0.0.dev20181113\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\n\nNvidia driver version: 384.111\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/cuda-9.0/lib64/libcudnn.so\n/usr/local/cuda-9.0/lib64/libcudnn.so.5\n/usr/local/cuda-9.0/lib64/libcudnn.so.5.1.10\n/usr/local/cuda-9.0/lib64/libcudnn.so.6\n/usr/local/cuda-9.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.3\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.1)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.1)\n[pip3] torchvision (0.2.1)\n[conda] pytorch-nightly           1.0.0.dev20181113 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\n\nAdditional context\nIt's a regression from 0.4.1, which runs the example above without problems. The same code runs fine on CUDA tensors.", "body": "## \ud83d\udc1b Bug\r\n\r\nRepeatedly calling `torch.inverse()` makes memory leakage, which gets severer when the computation graph is complex.\r\n\r\n## To Reproduce\r\n\r\nMinimum working example below; the reported memory usage increases linearly.\r\n\r\n```python\r\nimport torch\r\nfrom resource import *\r\n\r\n\r\nS = 256\r\nX = torch.randn(S, S, requires_grad=True)\r\n\r\nfor i in range(10000):\r\n    if i % 100 == 0:\r\n        print(i, '\\t', getrusage(RUSAGE_SELF).ru_maxrss, 'kB used')\r\n\r\n    Z = X.sum(1, True) + torch.linspace(-S, S, S)\r\n    Y = X * torch.erf(Z).sum(1, True)\r\n    loss = Y.t().mm(Y).inverse().sum() + Y.sum()\r\n    loss.backward()\r\n```\r\n\r\nBelow is the output; exact number might differ each run, but the memory usage always increases linearly. This will eventually hog all of available memory and cause problems in long training runs.\r\n\r\n```\r\n0 \t 139564 kB used\r\n100 \t 157264 kB used\r\n200 \t 157988 kB used\r\n300 \t 158888 kB used\r\n400 \t 159676 kB used\r\n500 \t 160392 kB used\r\n600 \t 161788 kB used\r\n700 \t 162140 kB used\r\n800 \t 162892 kB used\r\n900 \t 163800 kB used\r\n1000 \t 164964 kB used\r\n...\r\n```\r\n\r\n(Calling `backward()` is just for demonstration purposes, and the problem persists with no autograd at all)\r\n\r\n## Expected behavior\r\n\r\nThe memory usage should stay constant at a certain level.\r\n\r\n## Environment\r\n\r\n(CUDA info below should be irrelevant since I'm only using CPU tensors in the example above)\r\n\r\n```\r\nPyTorch version: 1.0.0.dev20181113\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.0/lib64/libcudnn.so\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.1)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.1)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch-nightly           1.0.0.dev20181113 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\nIt's a regression from 0.4.1, which runs the example above without problems. The same code runs fine on CUDA tensors."}