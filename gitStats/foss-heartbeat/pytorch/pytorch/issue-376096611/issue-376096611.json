{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13402", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13402/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13402/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13402/events", "html_url": "https://github.com/pytorch/pytorch/issues/13402", "id": 376096611, "node_id": "MDU6SXNzdWUzNzYwOTY2MTE=", "number": 13402, "title": "batch_norm doesn't bump version counter of running stats", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-31T18:12:32Z", "updated_at": "2018-10-31T18:38:15Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>This can cause correctness problems if you mix <code>eval</code> and <code>training</code> modes because <code>eval</code> mode backward needs the running stats, and <code>training</code> mode forward updates the running stats, even if the <code>training</code> mode result is unused. E.g.,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\nbn <span class=\"pl-k\">=</span> torch.nn.BatchNorm2d(<span class=\"pl-c1\">3</span>).double()\nx <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).double().requires_grad_()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n  bn.running_mean.data.fill_(<span class=\"pl-c1\">0</span>)\n  bn.running_var.data.fill_(<span class=\"pl-c1\">1</span>)\n  bn.eval(); y0 <span class=\"pl-k\">=</span> bn(x)\n  bn.train(); unused <span class=\"pl-k\">=</span> bn(x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> running buffers are changed in-place here</span>\n  <span class=\"pl-k\">return</span> y0\n\ntorch.autograd.gradcheck(f, (x,))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> fails gradcheck</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">g</span>(<span class=\"pl-smi\">x</span>):\n  bn.running_mean.data.fill_(<span class=\"pl-c1\">0</span>)\n  bn.running_var.data.fill_(<span class=\"pl-c1\">1</span>)\n  bn.eval(); y0 <span class=\"pl-k\">=</span> bn(x)\n  bn.running_mean <span class=\"pl-k\">=</span> bn.running_mean.clone()\n  bn.running_var <span class=\"pl-k\">=</span> bn.running_var.clone()\n  bn.train(); unused <span class=\"pl-k\">=</span> bn(x)\n  <span class=\"pl-k\">return</span> y0\n\ntorch.autograd.gradcheck(g, (x,))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> passes</span>\n</pre></div>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "This can cause correctness problems if you mix eval and training modes because eval mode backward needs the running stats, and training mode forward updates the running stats, even if the training mode result is unused. E.g.,\nimport torch\n\nbn = torch.nn.BatchNorm2d(3).double()\nx = torch.randn(2, 3, 1, 1).double().requires_grad_()\n\n\ndef f(x):\n  bn.running_mean.data.fill_(0)\n  bn.running_var.data.fill_(1)\n  bn.eval(); y0 = bn(x)\n  bn.train(); unused = bn(x)  # running buffers are changed in-place here\n  return y0\n\ntorch.autograd.gradcheck(f, (x,))  # fails gradcheck\n\ndef g(x):\n  bn.running_mean.data.fill_(0)\n  bn.running_var.data.fill_(1)\n  bn.eval(); y0 = bn(x)\n  bn.running_mean = bn.running_mean.clone()\n  bn.running_var = bn.running_var.clone()\n  bn.train(); unused = bn(x)\n  return y0\n\ntorch.autograd.gradcheck(g, (x,))  # passes\n\ncc @colesbury", "body": "This can cause correctness problems if you mix `eval` and `training` modes because `eval` mode backward needs the running stats, and `training` mode forward updates the running stats, even if the `training` mode result is unused. E.g.,\r\n\r\n```py\r\nimport torch\r\n\r\nbn = torch.nn.BatchNorm2d(3).double()\r\nx = torch.randn(2, 3, 1, 1).double().requires_grad_()\r\n\r\n\r\ndef f(x):\r\n  bn.running_mean.data.fill_(0)\r\n  bn.running_var.data.fill_(1)\r\n  bn.eval(); y0 = bn(x)\r\n  bn.train(); unused = bn(x)  # running buffers are changed in-place here\r\n  return y0\r\n\r\ntorch.autograd.gradcheck(f, (x,))  # fails gradcheck\r\n\r\ndef g(x):\r\n  bn.running_mean.data.fill_(0)\r\n  bn.running_var.data.fill_(1)\r\n  bn.eval(); y0 = bn(x)\r\n  bn.running_mean = bn.running_mean.clone()\r\n  bn.running_var = bn.running_var.clone()\r\n  bn.train(); unused = bn(x)\r\n  return y0\r\n\r\ntorch.autograd.gradcheck(g, (x,))  # passes\r\n\r\n```\r\n\r\ncc @colesbury "}