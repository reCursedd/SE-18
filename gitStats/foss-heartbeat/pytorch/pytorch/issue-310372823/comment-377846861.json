{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377846861", "html_url": "https://github.com/pytorch/pytorch/issues/6178#issuecomment-377846861", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6178", "id": 377846861, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Nzg0Njg2MQ==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-02T03:36:05Z", "updated_at": "2018-04-02T03:36:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the report, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3686634\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gursimar\">@gursimar</a>. This bug has already been fixed on master. It comes with a nice error message:</p>\n<pre><code>In [3]: import torch.nn as nn\n\nIn [4]: from torch.autograd import Variable\n\nIn [5]: lstm = nn.LSTM(20,30).cuda()\n   ...: X = Variable(torch.rand(1,100,20)).cuda()\n   ...: hidden_temp = Variable(torch.rand(1,100,30)).cuda()\n   ...: hidden = (hidden_temp, hidden_temp)\n   ...: _,_ = lstm(X.transpose(0,1),hidden)\n   ...:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-5-27aa7be44afa&gt; in &lt;module&gt;()\n      3 hidden_temp = Variable(torch.rand(1,100,30)).cuda()\n      4 hidden = (hidden_temp, hidden_temp)\n----&gt; 5 _,_ = lstm(X.transpose(0,1),hidden)\n\n~/pytorch/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    369             result = self._slow_forward(*input, **kwargs)\n    370         else:\n--&gt; 371             result = self.forward(*input, **kwargs)\n    372         for hook in self._forward_hooks.values():\n    373             hook_result = hook(self, input, result)\n\n~/pytorch/torch/nn/modules/rnn.py in forward(self, input, hx)\n    177             flat_weight = None\n    178\n--&gt; 179         self.check_forward_args(input, hx, batch_sizes)\n    180         func = self._backend.RNN(\n    181             self.mode,\n\n~/pytorch/torch/nn/modules/rnn.py in check_forward_args(self, input, hidden, batch_size\ns)\n    145         if self.mode == 'LSTM':\n    146             check_hidden_size(hidden[0], expected_hidden_size,\n--&gt; 147                               'Expected hidden[0] size {}, got {}')\n    148             check_hidden_size(hidden[1], expected_hidden_size,\n    149                               'Expected hidden[1] size {}, got {}')\n\n~/pytorch/torch/nn/modules/rnn.py in check_hidden_size(hx, expected_hidden_size, msg)\n    141         def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden si\nze {}, got {}'):\n    142             if tuple(hx.size()) != expected_hidden_size:\n--&gt; 143                 raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.si\nze())))\n    144\n    145         if self.mode == 'LSTM':\n\nRuntimeError: Expected hidden[0] size (1, 1, 30), got (1, 100, 30)\n\n</code></pre>", "body_text": "Thanks for the report, @gursimar. This bug has already been fixed on master. It comes with a nice error message:\nIn [3]: import torch.nn as nn\n\nIn [4]: from torch.autograd import Variable\n\nIn [5]: lstm = nn.LSTM(20,30).cuda()\n   ...: X = Variable(torch.rand(1,100,20)).cuda()\n   ...: hidden_temp = Variable(torch.rand(1,100,30)).cuda()\n   ...: hidden = (hidden_temp, hidden_temp)\n   ...: _,_ = lstm(X.transpose(0,1),hidden)\n   ...:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-27aa7be44afa> in <module>()\n      3 hidden_temp = Variable(torch.rand(1,100,30)).cuda()\n      4 hidden = (hidden_temp, hidden_temp)\n----> 5 _,_ = lstm(X.transpose(0,1),hidden)\n\n~/pytorch/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    369             result = self._slow_forward(*input, **kwargs)\n    370         else:\n--> 371             result = self.forward(*input, **kwargs)\n    372         for hook in self._forward_hooks.values():\n    373             hook_result = hook(self, input, result)\n\n~/pytorch/torch/nn/modules/rnn.py in forward(self, input, hx)\n    177             flat_weight = None\n    178\n--> 179         self.check_forward_args(input, hx, batch_sizes)\n    180         func = self._backend.RNN(\n    181             self.mode,\n\n~/pytorch/torch/nn/modules/rnn.py in check_forward_args(self, input, hidden, batch_size\ns)\n    145         if self.mode == 'LSTM':\n    146             check_hidden_size(hidden[0], expected_hidden_size,\n--> 147                               'Expected hidden[0] size {}, got {}')\n    148             check_hidden_size(hidden[1], expected_hidden_size,\n    149                               'Expected hidden[1] size {}, got {}')\n\n~/pytorch/torch/nn/modules/rnn.py in check_hidden_size(hx, expected_hidden_size, msg)\n    141         def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden si\nze {}, got {}'):\n    142             if tuple(hx.size()) != expected_hidden_size:\n--> 143                 raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.si\nze())))\n    144\n    145         if self.mode == 'LSTM':\n\nRuntimeError: Expected hidden[0] size (1, 1, 30), got (1, 100, 30)", "body": "Thanks for the report, @gursimar. This bug has already been fixed on master. It comes with a nice error message:\r\n```\r\nIn [3]: import torch.nn as nn\r\n\r\nIn [4]: from torch.autograd import Variable\r\n\r\nIn [5]: lstm = nn.LSTM(20,30).cuda()\r\n   ...: X = Variable(torch.rand(1,100,20)).cuda()\r\n   ...: hidden_temp = Variable(torch.rand(1,100,30)).cuda()\r\n   ...: hidden = (hidden_temp, hidden_temp)\r\n   ...: _,_ = lstm(X.transpose(0,1),hidden)\r\n   ...:\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-27aa7be44afa> in <module>()\r\n      3 hidden_temp = Variable(torch.rand(1,100,30)).cuda()\r\n      4 hidden = (hidden_temp, hidden_temp)\r\n----> 5 _,_ = lstm(X.transpose(0,1),hidden)\r\n\r\n~/pytorch/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    369             result = self._slow_forward(*input, **kwargs)\r\n    370         else:\r\n--> 371             result = self.forward(*input, **kwargs)\r\n    372         for hook in self._forward_hooks.values():\r\n    373             hook_result = hook(self, input, result)\r\n\r\n~/pytorch/torch/nn/modules/rnn.py in forward(self, input, hx)\r\n    177             flat_weight = None\r\n    178\r\n--> 179         self.check_forward_args(input, hx, batch_sizes)\r\n    180         func = self._backend.RNN(\r\n    181             self.mode,\r\n\r\n~/pytorch/torch/nn/modules/rnn.py in check_forward_args(self, input, hidden, batch_size\r\ns)\r\n    145         if self.mode == 'LSTM':\r\n    146             check_hidden_size(hidden[0], expected_hidden_size,\r\n--> 147                               'Expected hidden[0] size {}, got {}')\r\n    148             check_hidden_size(hidden[1], expected_hidden_size,\r\n    149                               'Expected hidden[1] size {}, got {}')\r\n\r\n~/pytorch/torch/nn/modules/rnn.py in check_hidden_size(hx, expected_hidden_size, msg)\r\n    141         def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden si\r\nze {}, got {}'):\r\n    142             if tuple(hx.size()) != expected_hidden_size:\r\n--> 143                 raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.si\r\nze())))\r\n    144\r\n    145         if self.mode == 'LSTM':\r\n\r\nRuntimeError: Expected hidden[0] size (1, 1, 30), got (1, 100, 30)\r\n\r\n``` "}