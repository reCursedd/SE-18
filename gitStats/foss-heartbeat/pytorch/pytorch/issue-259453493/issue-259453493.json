{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2816", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2816/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2816/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2816/events", "html_url": "https://github.com/pytorch/pytorch/issues/2816", "id": 259453493, "node_id": "MDU6SXNzdWUyNTk0NTM0OTM=", "number": 2816, "title": "Certain point-wise GPU operations with broadcasting are up to 12x slower going from v0.2.0 to master", "user": {"login": "ducksoup", "id": 1279573, "node_id": "MDQ6VXNlcjEyNzk1NzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1279573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ducksoup", "html_url": "https://github.com/ducksoup", "followers_url": "https://api.github.com/users/ducksoup/followers", "following_url": "https://api.github.com/users/ducksoup/following{/other_user}", "gists_url": "https://api.github.com/users/ducksoup/gists{/gist_id}", "starred_url": "https://api.github.com/users/ducksoup/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ducksoup/subscriptions", "organizations_url": "https://api.github.com/users/ducksoup/orgs", "repos_url": "https://api.github.com/users/ducksoup/repos", "events_url": "https://api.github.com/users/ducksoup/events{/privacy}", "received_events_url": "https://api.github.com/users/ducksoup/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/performance", "name": "performance", "color": "f9d0c4", "default": false}], "state": "open", "locked": false, "assignee": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2017-09-21T10:45:39Z", "updated_at": "2017-11-03T05:29:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Some point-wise tensor operations are up to 12 times slower on master (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/2b9765ad02b666d623566935b385fc3d058ad33d/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/2b9765ad02b666d623566935b385fc3d058ad33d\"><tt>2b9765a</tt></a>) compared to v0.2.0 when one of the operands is broadcasted.</p>\n<p>We are able to consistently reproduce this issue with the following code (note that the same issue appears also for point-wise sum and division):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> time\n\nN, C, H, W <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>\nrepetitions <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\nx <span class=\"pl-k\">=</span> torch.randn(N, C, H, W).cuda()\nw <span class=\"pl-k\">=</span> torch.randn(C).cuda()\n\nt0 <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(repetitions):\n    y <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> w.view(<span class=\"pl-c1\">1</span>, C, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n    torch.cuda.synchronize()\n<span class=\"pl-c1\">print</span> (time.time() <span class=\"pl-k\">-</span> t0) <span class=\"pl-k\">/</span> repetitions</pre></div>\n<p>A typical output is as follows:</p>\n<pre><code>(pytorch_master) $ python timeit.py \n0.0461528801918\n\n(pytorch_v0.2) $ python timeit.py \n0.00369760036469\n</code></pre>\n<p>The slow-down seems to be related to the axis that <code>x</code> and <code>w</code> have in common, in fact, when using</p>\n<div class=\"highlight highlight-source-python\"><pre>w <span class=\"pl-k\">=</span> torch.randn(N).cuda()\n<span class=\"pl-c1\">...</span>\ny <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> w.view(N, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)</pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-python\"><pre>w <span class=\"pl-k\">=</span> torch.randn(W).cuda()\n<span class=\"pl-c1\">...</span>\ny <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> w.view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, W)</pre></div>\n<p>there is no noticeable slow-down, while</p>\n<div class=\"highlight highlight-source-python\"><pre>w <span class=\"pl-k\">=</span> torch.randn(H).cuda()\n<span class=\"pl-c1\">...</span>\ny <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> w.view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, H, <span class=\"pl-c1\">1</span>)</pre></div>\n<p>produces the same behavior as the script above.</p>\n<p>This has been tested on different systems using CUDA 8.0 and a variety of GPUs, including: TITAN X, TITAN Xp and GTX 1050.</p>", "body_text": "Some point-wise tensor operations are up to 12 times slower on master (2b9765a) compared to v0.2.0 when one of the operands is broadcasted.\nWe are able to consistently reproduce this issue with the following code (note that the same issue appears also for point-wise sum and division):\nimport torch\nimport time\n\nN, C, H, W = 64, 256, 64, 64\nrepetitions = 100\n\nx = torch.randn(N, C, H, W).cuda()\nw = torch.randn(C).cuda()\n\nt0 = time.time()\nfor _ in range(repetitions):\n    y = x * w.view(1, C, 1, 1)\n    torch.cuda.synchronize()\nprint (time.time() - t0) / repetitions\nA typical output is as follows:\n(pytorch_master) $ python timeit.py \n0.0461528801918\n\n(pytorch_v0.2) $ python timeit.py \n0.00369760036469\n\nThe slow-down seems to be related to the axis that x and w have in common, in fact, when using\nw = torch.randn(N).cuda()\n...\ny = x * w.view(N, 1, 1, 1)\nor\nw = torch.randn(W).cuda()\n...\ny = x * w.view(1, 1, 1, W)\nthere is no noticeable slow-down, while\nw = torch.randn(H).cuda()\n...\ny = x * w.view(1, 1, H, 1)\nproduces the same behavior as the script above.\nThis has been tested on different systems using CUDA 8.0 and a variety of GPUs, including: TITAN X, TITAN Xp and GTX 1050.", "body": "Some point-wise tensor operations are up to 12 times slower on master (https://github.com/pytorch/pytorch/commit/2b9765ad02b666d623566935b385fc3d058ad33d) compared to v0.2.0 when one of the operands is broadcasted.\r\n\r\nWe are able to consistently reproduce this issue with the following code (note that the same issue appears also for point-wise sum and division):\r\n```python\r\nimport torch\r\nimport time\r\n\r\nN, C, H, W = 64, 256, 64, 64\r\nrepetitions = 100\r\n\r\nx = torch.randn(N, C, H, W).cuda()\r\nw = torch.randn(C).cuda()\r\n\r\nt0 = time.time()\r\nfor _ in range(repetitions):\r\n    y = x * w.view(1, C, 1, 1)\r\n    torch.cuda.synchronize()\r\nprint (time.time() - t0) / repetitions\r\n```\r\n\r\nA typical output is as follows:\r\n```\r\n(pytorch_master) $ python timeit.py \r\n0.0461528801918\r\n\r\n(pytorch_v0.2) $ python timeit.py \r\n0.00369760036469\r\n```\r\n\r\nThe slow-down seems to be related to the axis that `x` and `w` have in common, in fact, when using\r\n```python\r\nw = torch.randn(N).cuda()\r\n...\r\ny = x * w.view(N, 1, 1, 1)\r\n```\r\nor\r\n```python\r\nw = torch.randn(W).cuda()\r\n...\r\ny = x * w.view(1, 1, 1, W)\r\n```\r\nthere is no noticeable slow-down, while\r\n```python\r\nw = torch.randn(H).cuda()\r\n...\r\ny = x * w.view(1, 1, H, 1)\r\n```\r\nproduces the same behavior as the script above.\r\n\r\nThis has been tested on different systems using CUDA 8.0 and a variety of GPUs, including: TITAN X, TITAN Xp and GTX 1050."}