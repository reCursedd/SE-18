{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341481512", "html_url": "https://github.com/pytorch/pytorch/issues/2816#issuecomment-341481512", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2816", "id": 341481512, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTQ4MTUxMg==", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-02T16:36:38Z", "updated_at": "2017-11-02T16:36:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4556044\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Stonesjtu\">@Stonesjtu</a> Keeping data into register space is not inherently bad. In fact it can often be a very useful optimization which is why the compiler has a tendency to do so. THCTensorInfo, however, requires &gt;200B. <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L46-L49\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L46-L49</a> In THCApply we are trying to put 4, 512 blocks per SM (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L24\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L24</a>) Which means that if we try to put THCTensorInfo in register space on every thread we're asking for 409KB of register space. Considering register space / SM is 256KB this clearly can't be stored there. Instead it is stored in thread local memory (gets sent to global memory). <a href=\"http://developer.download.nvidia.com/CUDA/training/register_spilling.pdf\" rel=\"nofollow\">http://developer.download.nvidia.com/CUDA/training/register_spilling.pdf</a></p>\n<p>There for every block is storing 200B * 512 threads of memory into global memory before it even gets started. Keeping in mind that each block is responsible for 2 reads and 1 write (assuming THCApply 2) or 12B*512 threads.</p>\n<p>The reason that switching from a tensor of (1, C, 1, 1) to (N, C, 1, 1) with first 2 dimensions contiguous helps is that the backend 'collapses' N, C, 1, 1 tensor into a 2D tensor therefore using THCTensor's 2D templated indexing. <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L484-L548\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L484-L548</a> which templates <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L228-L248\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L228-L248</a> .</p>\n<p>Since Dims in this loop <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L236\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L236</a> is a template parameter, the compiler now knows exactly what is needed from the THC tensor structure (not a runtime variable as in -1 case) it will decide to not try and put it in thread local memory.</p>", "body_text": "@Stonesjtu Keeping data into register space is not inherently bad. In fact it can often be a very useful optimization which is why the compiler has a tendency to do so. THCTensorInfo, however, requires >200B. https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L46-L49 In THCApply we are trying to put 4, 512 blocks per SM (https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L24) Which means that if we try to put THCTensorInfo in register space on every thread we're asking for 409KB of register space. Considering register space / SM is 256KB this clearly can't be stored there. Instead it is stored in thread local memory (gets sent to global memory). http://developer.download.nvidia.com/CUDA/training/register_spilling.pdf\nThere for every block is storing 200B * 512 threads of memory into global memory before it even gets started. Keeping in mind that each block is responsible for 2 reads and 1 write (assuming THCApply 2) or 12B*512 threads.\nThe reason that switching from a tensor of (1, C, 1, 1) to (N, C, 1, 1) with first 2 dimensions contiguous helps is that the backend 'collapses' N, C, 1, 1 tensor into a 2D tensor therefore using THCTensor's 2D templated indexing. https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L484-L548 which templates https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L228-L248 .\nSince Dims in this loop https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L236 is a template parameter, the compiler now knows exactly what is needed from the THC tensor structure (not a runtime variable as in -1 case) it will decide to not try and put it in thread local memory.", "body": "@Stonesjtu Keeping data into register space is not inherently bad. In fact it can often be a very useful optimization which is why the compiler has a tendency to do so. THCTensorInfo, however, requires >200B. https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L46-L49 In THCApply we are trying to put 4, 512 blocks per SM (https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L24) Which means that if we try to put THCTensorInfo in register space on every thread we're asking for 409KB of register space. Considering register space / SM is 256KB this clearly can't be stored there. Instead it is stored in thread local memory (gets sent to global memory). http://developer.download.nvidia.com/CUDA/training/register_spilling.pdf\r\n\r\nThere for every block is storing 200B * 512 threads of memory into global memory before it even gets started. Keeping in mind that each block is responsible for 2 reads and 1 write (assuming THCApply 2) or 12B*512 threads.\r\n\r\nThe reason that switching from a tensor of (1, C, 1, 1) to (N, C, 1, 1) with first 2 dimensions contiguous helps is that the backend 'collapses' N, C, 1, 1 tensor into a 2D tensor therefore using THCTensor's 2D templated indexing. https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCApply.cuh#L484-L548 which templates https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L228-L248 .\r\n\r\nSince Dims in this loop https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorInfo.cuh#L236 is a template parameter, the compiler now knows exactly what is needed from the THC tensor structure (not a runtime variable as in -1 case) it will decide to not try and put it in thread local memory.\r\n"}