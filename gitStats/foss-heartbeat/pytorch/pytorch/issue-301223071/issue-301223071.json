{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5479", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5479/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5479/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5479/events", "html_url": "https://github.com/pytorch/pytorch/issues/5479", "id": 301223071, "node_id": "MDU6SXNzdWUzMDEyMjMwNzE=", "number": 5479, "title": "torch.autograd.grad output variable has no grad_fn", "user": {"login": "seerdecker", "id": 12588992, "node_id": "MDQ6VXNlcjEyNTg4OTky", "avatar_url": "https://avatars0.githubusercontent.com/u/12588992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seerdecker", "html_url": "https://github.com/seerdecker", "followers_url": "https://api.github.com/users/seerdecker/followers", "following_url": "https://api.github.com/users/seerdecker/following{/other_user}", "gists_url": "https://api.github.com/users/seerdecker/gists{/gist_id}", "starred_url": "https://api.github.com/users/seerdecker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seerdecker/subscriptions", "organizations_url": "https://api.github.com/users/seerdecker/orgs", "repos_url": "https://api.github.com/users/seerdecker/repos", "events_url": "https://api.github.com/users/seerdecker/events{/privacy}", "received_events_url": "https://api.github.com/users/seerdecker/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-28T23:25:15Z", "updated_at": "2018-03-01T14:11:38Z", "closed_at": "2018-03-01T12:12:53Z", "author_association": "NONE", "body_html": "<p>See the code below. I would expect d.grad_fn not to be None.</p>\n<pre><code>a = torch.autograd.Variable(torch.FloatTensor([[1, 2, 3], [4, 5, 6]]), requires_grad=1)\nb = torch.mul(a, 2)\nc = b.sum()\nd = torch.autograd.grad(c, b, torch.FloatTensor([2]), create_graph=True)[0]\n\nprint(\"d gradients are fine\")\nprint(d)\n\nprint(\"d.grad_fn is None\")\nprint(d.grad_fn)\n\nprint(\"Boom\")\nd.sum().backward()\n</code></pre>\n<p>Testing on Linux, Pytorch 0.3.1, Python 3.6.</p>", "body_text": "See the code below. I would expect d.grad_fn not to be None.\na = torch.autograd.Variable(torch.FloatTensor([[1, 2, 3], [4, 5, 6]]), requires_grad=1)\nb = torch.mul(a, 2)\nc = b.sum()\nd = torch.autograd.grad(c, b, torch.FloatTensor([2]), create_graph=True)[0]\n\nprint(\"d gradients are fine\")\nprint(d)\n\nprint(\"d.grad_fn is None\")\nprint(d.grad_fn)\n\nprint(\"Boom\")\nd.sum().backward()\n\nTesting on Linux, Pytorch 0.3.1, Python 3.6.", "body": "See the code below. I would expect d.grad_fn not to be None.\r\n```\r\na = torch.autograd.Variable(torch.FloatTensor([[1, 2, 3], [4, 5, 6]]), requires_grad=1)\r\nb = torch.mul(a, 2)\r\nc = b.sum()\r\nd = torch.autograd.grad(c, b, torch.FloatTensor([2]), create_graph=True)[0]\r\n\r\nprint(\"d gradients are fine\")\r\nprint(d)\r\n\r\nprint(\"d.grad_fn is None\")\r\nprint(d.grad_fn)\r\n\r\nprint(\"Boom\")\r\nd.sum().backward()\r\n```\r\n\r\nTesting on Linux, Pytorch 0.3.1, Python 3.6."}