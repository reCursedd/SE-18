{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369603223", "html_url": "https://github.com/pytorch/pytorch/issues/5479#issuecomment-369603223", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5479", "id": 369603223, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTYwMzIyMw==", "user": {"login": "seerdecker", "id": 12588992, "node_id": "MDQ6VXNlcjEyNTg4OTky", "avatar_url": "https://avatars0.githubusercontent.com/u/12588992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seerdecker", "html_url": "https://github.com/seerdecker", "followers_url": "https://api.github.com/users/seerdecker/followers", "following_url": "https://api.github.com/users/seerdecker/following{/other_user}", "gists_url": "https://api.github.com/users/seerdecker/gists{/gist_id}", "starred_url": "https://api.github.com/users/seerdecker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seerdecker/subscriptions", "organizations_url": "https://api.github.com/users/seerdecker/orgs", "repos_url": "https://api.github.com/users/seerdecker/repos", "events_url": "https://api.github.com/users/seerdecker/events{/privacy}", "received_events_url": "https://api.github.com/users/seerdecker/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-01T14:11:38Z", "updated_at": "2018-03-01T14:11:38Z", "author_association": "NONE", "body_html": "<p><code>dC/dB</code> exists but is zero.</p>\n<p>Note that this code doesn't throw even though the gradient is zero<br>\n<code>(torch.autograd.Variable(torch.FloatTensor([[1, 2, 3], [4, 5, 6]]), requires_grad=1)*0).sum().backward()</code></p>\n<p>I'll argue that the current semantics are not very helpful to the user when debugging. If you change the loss and cause the gradient to become zero, the code throws. It's kind of like having to write <code>if (ptr) free(ptr)</code> instead of <code>free(ptr)</code>. Not a big deal, but annoying.</p>", "body_text": "dC/dB exists but is zero.\nNote that this code doesn't throw even though the gradient is zero\n(torch.autograd.Variable(torch.FloatTensor([[1, 2, 3], [4, 5, 6]]), requires_grad=1)*0).sum().backward()\nI'll argue that the current semantics are not very helpful to the user when debugging. If you change the loss and cause the gradient to become zero, the code throws. It's kind of like having to write if (ptr) free(ptr) instead of free(ptr). Not a big deal, but annoying.", "body": "`dC/dB` exists but is zero.\r\n\r\nNote that this code doesn't throw even though the gradient is zero\r\n`(torch.autograd.Variable(torch.FloatTensor([[1, 2, 3], [4, 5, 6]]), requires_grad=1)*0).sum().backward()`\r\n\r\nI'll argue that the current semantics are not very helpful to the user when debugging. If you change the loss and cause the gradient to become zero, the code throws. It's kind of like having to write `if (ptr) free(ptr)` instead of `free(ptr)`. Not a big deal, but annoying."}