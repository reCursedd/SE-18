{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295376319", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-295376319", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 295376319, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTM3NjMxOQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-19T18:19:23Z", "updated_at": "2017-04-19T18:19:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is an example test case for optimization when we have sparse gradients, done by \"knocking out\" the x or y coordinate on the computed gradient and putting the rest in a sparse gradient (I did it in a kind of janky way; ways to improve this code stylistically much appreciated!) When we get around to testing with weight decay I'll have to replace the assertEqual with a more approximate test, since the strategy we plan on using won't give us exactly the same results as the dense version.</p>\n<pre><code>    def _test_rosenbrock_sparse(self, constructor):\n        params_t = torch.Tensor([1.5, 1.5])\n\n        params   = Variable(torch.Tensor([1.5, 1.5]), requires_grad=True)\n        params_c = Variable(torch.Tensor([1.5, 1.5]), requires_grad=True)\n        optimizer   = constructor([params])\n        optimizer_c = constructor([params_c])\n\n        solution = torch.Tensor([1, 1])\n        initial_dist = params.data.dist(solution)\n\n        def eval(params, sparse_grad, w):\n            optimizer.zero_grad()\n            loss = rosenbrock(params)\n            loss.backward()\n            # params.grad.data now has the computed gradient. Turn\n            # it into a sparse tensor\n            params.grad.data.copy_(drosenbrock(params.data))\n            # This is really goofy\n            if w:\n                i = torch.LongTensor([[0]])\n                v = torch.DoubleTensor([params.grad.data[0]])\n            else:\n                i = torch.LongTensor([[1]])\n                v = torch.DoubleTensor([params.grad.data[1]])\n            x = sparse.DoubleTensor(i, v, torch.Size([2]))\n            if sparse_grad:\n                params.grad.data = x\n            else:\n                params.grad.data = x.to_dense()\n            return loss\n\n        for i in range(2000):\n            w = torch.rand(1)[0] &gt; 0.5\n            optimizer.step(functools.partial(eval, params, True, w))\n            optimizer_c.step(functools.partial(eval, params_c, False, w))\n            self.assertEqual(params.data, params_c.data)\n\n        self.assertLessEqual(params.data.dist(solution), initial_dist)\n</code></pre>", "body_text": "Here is an example test case for optimization when we have sparse gradients, done by \"knocking out\" the x or y coordinate on the computed gradient and putting the rest in a sparse gradient (I did it in a kind of janky way; ways to improve this code stylistically much appreciated!) When we get around to testing with weight decay I'll have to replace the assertEqual with a more approximate test, since the strategy we plan on using won't give us exactly the same results as the dense version.\n    def _test_rosenbrock_sparse(self, constructor):\n        params_t = torch.Tensor([1.5, 1.5])\n\n        params   = Variable(torch.Tensor([1.5, 1.5]), requires_grad=True)\n        params_c = Variable(torch.Tensor([1.5, 1.5]), requires_grad=True)\n        optimizer   = constructor([params])\n        optimizer_c = constructor([params_c])\n\n        solution = torch.Tensor([1, 1])\n        initial_dist = params.data.dist(solution)\n\n        def eval(params, sparse_grad, w):\n            optimizer.zero_grad()\n            loss = rosenbrock(params)\n            loss.backward()\n            # params.grad.data now has the computed gradient. Turn\n            # it into a sparse tensor\n            params.grad.data.copy_(drosenbrock(params.data))\n            # This is really goofy\n            if w:\n                i = torch.LongTensor([[0]])\n                v = torch.DoubleTensor([params.grad.data[0]])\n            else:\n                i = torch.LongTensor([[1]])\n                v = torch.DoubleTensor([params.grad.data[1]])\n            x = sparse.DoubleTensor(i, v, torch.Size([2]))\n            if sparse_grad:\n                params.grad.data = x\n            else:\n                params.grad.data = x.to_dense()\n            return loss\n\n        for i in range(2000):\n            w = torch.rand(1)[0] > 0.5\n            optimizer.step(functools.partial(eval, params, True, w))\n            optimizer_c.step(functools.partial(eval, params_c, False, w))\n            self.assertEqual(params.data, params_c.data)\n\n        self.assertLessEqual(params.data.dist(solution), initial_dist)", "body": "Here is an example test case for optimization when we have sparse gradients, done by \"knocking out\" the x or y coordinate on the computed gradient and putting the rest in a sparse gradient (I did it in a kind of janky way; ways to improve this code stylistically much appreciated!) When we get around to testing with weight decay I'll have to replace the assertEqual with a more approximate test, since the strategy we plan on using won't give us exactly the same results as the dense version.\r\n\r\n```\r\n    def _test_rosenbrock_sparse(self, constructor):\r\n        params_t = torch.Tensor([1.5, 1.5])\r\n\r\n        params   = Variable(torch.Tensor([1.5, 1.5]), requires_grad=True)\r\n        params_c = Variable(torch.Tensor([1.5, 1.5]), requires_grad=True)\r\n        optimizer   = constructor([params])\r\n        optimizer_c = constructor([params_c])\r\n\r\n        solution = torch.Tensor([1, 1])\r\n        initial_dist = params.data.dist(solution)\r\n\r\n        def eval(params, sparse_grad, w):\r\n            optimizer.zero_grad()\r\n            loss = rosenbrock(params)\r\n            loss.backward()\r\n            # params.grad.data now has the computed gradient. Turn\r\n            # it into a sparse tensor\r\n            params.grad.data.copy_(drosenbrock(params.data))\r\n            # This is really goofy\r\n            if w:\r\n                i = torch.LongTensor([[0]])\r\n                v = torch.DoubleTensor([params.grad.data[0]])\r\n            else:\r\n                i = torch.LongTensor([[1]])\r\n                v = torch.DoubleTensor([params.grad.data[1]])\r\n            x = sparse.DoubleTensor(i, v, torch.Size([2]))\r\n            if sparse_grad:\r\n                params.grad.data = x\r\n            else:\r\n                params.grad.data = x.to_dense()\r\n            return loss\r\n\r\n        for i in range(2000):\r\n            w = torch.rand(1)[0] > 0.5\r\n            optimizer.step(functools.partial(eval, params, True, w))\r\n            optimizer_c.step(functools.partial(eval, params_c, False, w))\r\n            self.assertEqual(params.data, params_c.data)\r\n\r\n        self.assertLessEqual(params.data.dist(solution), initial_dist)\r\n```"}