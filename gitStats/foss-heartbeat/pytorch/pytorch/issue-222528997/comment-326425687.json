{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326425687", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-326425687", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 326425687, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjQyNTY4Nw==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T21:29:24Z", "updated_at": "2017-08-31T21:29:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>My 2c:</p>\n<p>I thought about it a bit and this naive lazy update kind of makes sense for Adam. If you apply <em>all</em> of the updates (m, v, p) sparsely, then indeed this should behave pretty similarly (probably even superior to) dense Adam. I would argue that you shouldn't be using a momentum method with sparse updates, but whatever...</p>\n<p>Weight decay is different. It's a component of the loss, and if you do it sparsely, you are no longer computing weight decay, you're computing something else. There's a very good reason why weight decay should be applied to all parameters: the whole point of L2 regularization is that it should drive down the value of parameters g_i for which dL/dg_i is small. If you have a parameter which is almost always \"sparse\" (i.e. dL/dg_i is 0 most of the time), then it should be pushed down strongly (i.e. weight decay should be applied at each gradient update, although this can be done in an amortized fashion to save on compute).</p>", "body_text": "My 2c:\nI thought about it a bit and this naive lazy update kind of makes sense for Adam. If you apply all of the updates (m, v, p) sparsely, then indeed this should behave pretty similarly (probably even superior to) dense Adam. I would argue that you shouldn't be using a momentum method with sparse updates, but whatever...\nWeight decay is different. It's a component of the loss, and if you do it sparsely, you are no longer computing weight decay, you're computing something else. There's a very good reason why weight decay should be applied to all parameters: the whole point of L2 regularization is that it should drive down the value of parameters g_i for which dL/dg_i is small. If you have a parameter which is almost always \"sparse\" (i.e. dL/dg_i is 0 most of the time), then it should be pushed down strongly (i.e. weight decay should be applied at each gradient update, although this can be done in an amortized fashion to save on compute).", "body": "My 2c:\r\n\r\nI thought about it a bit and this naive lazy update kind of makes sense for Adam. If you apply *all* of the updates (m, v, p) sparsely, then indeed this should behave pretty similarly (probably even superior to) dense Adam. I would argue that you shouldn't be using a momentum method with sparse updates, but whatever...\r\n\r\nWeight decay is different. It's a component of the loss, and if you do it sparsely, you are no longer computing weight decay, you're computing something else. There's a very good reason why weight decay should be applied to all parameters: the whole point of L2 regularization is that it should drive down the value of parameters g_i for which dL/dg_i is small. If you have a parameter which is almost always \"sparse\" (i.e. dL/dg_i is 0 most of the time), then it should be pushed down strongly (i.e. weight decay should be applied at each gradient update, although this can be done in an amortized fashion to save on compute)."}