{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326705658", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-326705658", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 326705658, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjcwNTY1OA==", "user": {"login": "jph00", "id": 346999, "node_id": "MDQ6VXNlcjM0Njk5OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/346999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jph00", "html_url": "https://github.com/jph00", "followers_url": "https://api.github.com/users/jph00/followers", "following_url": "https://api.github.com/users/jph00/following{/other_user}", "gists_url": "https://api.github.com/users/jph00/gists{/gist_id}", "starred_url": "https://api.github.com/users/jph00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jph00/subscriptions", "organizations_url": "https://api.github.com/users/jph00/orgs", "repos_url": "https://api.github.com/users/jph00/repos", "events_url": "https://api.github.com/users/jph00/events{/privacy}", "received_events_url": "https://api.github.com/users/jph00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-01T23:46:25Z", "updated_at": "2017-09-01T23:46:25Z", "author_association": "NONE", "body_html": "<p>Ugh, I spent the last 4 hours trying to create a clear example, and ended up finding that the key issue is actually in the difference between torchtext and keras padding (keras pre-pads by default, torchtext post-pads). I had no idea this could be such a big issue - but it fully explains the differences I was seeing! So sorry to waste your time with my incorrect finger-pointing at the sparse updates.</p>\n<p>I'm not going to fully back-track and say that updates to just the embeddings that were used isn't a good idea; I don't have the data either way on this. I hope sometime after the next course to have time to go back over this and do some proper experiments.</p>\n<p>I will say however that the fact that keras/tf handles these models so quickly regardless of weight decay, adam, etc, is pretty damn cool, although I'm not really sure how they do it!</p>", "body_text": "Ugh, I spent the last 4 hours trying to create a clear example, and ended up finding that the key issue is actually in the difference between torchtext and keras padding (keras pre-pads by default, torchtext post-pads). I had no idea this could be such a big issue - but it fully explains the differences I was seeing! So sorry to waste your time with my incorrect finger-pointing at the sparse updates.\nI'm not going to fully back-track and say that updates to just the embeddings that were used isn't a good idea; I don't have the data either way on this. I hope sometime after the next course to have time to go back over this and do some proper experiments.\nI will say however that the fact that keras/tf handles these models so quickly regardless of weight decay, adam, etc, is pretty damn cool, although I'm not really sure how they do it!", "body": "Ugh, I spent the last 4 hours trying to create a clear example, and ended up finding that the key issue is actually in the difference between torchtext and keras padding (keras pre-pads by default, torchtext post-pads). I had no idea this could be such a big issue - but it fully explains the differences I was seeing! So sorry to waste your time with my incorrect finger-pointing at the sparse updates.\r\n\r\nI'm not going to fully back-track and say that updates to just the embeddings that were used isn't a good idea; I don't have the data either way on this. I hope sometime after the next course to have time to go back over this and do some proper experiments.\r\n\r\nI will say however that the fact that keras/tf handles these models so quickly regardless of weight decay, adam, etc, is pretty damn cool, although I'm not really sure how they do it!"}