{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295917539", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-295917539", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 295917539, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTkxNzUzOQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T21:17:12Z", "updated_at": "2017-04-21T14:03:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Now that I feel that I have a much better grasp on what is going on with the optimizers, let me give an expanded \"state of play\":</p>\n<ul>\n<li>\n<p>Adagrad is already converted to use sparse updates. <a href=\"https://github.com/pytorch/pytorch/pull/735/files#diff-533671a4db549248f40d40c6b5473916\">https://github.com/pytorch/pytorch/pull/735/files#diff-533671a4db549248f40d40c6b5473916</a>  This made the code a bit more convoluted, since the naive implementation converts the sparse tensors into their dense representations too early when it calls addcmul (it would be nice to make this nicer, though it is not obvious to me what strategy should be taken here.) However, adagrad is a \"nice\" algorithm, in the sense that when the gradient is sparse, the final update is sparse as well.</p>\n</li>\n<li>\n<p>Adam, however, the updates to the parameters are NOT sparse, even when the gradients are sparse, because it accumulates moments, which means a full update to the moments (to decay the old values) and the parameters (This was also observed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2560662\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinraison\">@martinraison</a>). Indeed, TF has had to grapple with this issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"121453300\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/464\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/464/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/464\">tensorflow/tensorflow#464</a>; at the time this bug was written, TF's Adam implementation did a dense update, causing performance problems for other users <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"197234618\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6460\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6460/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6460\">tensorflow/tensorflow#6460</a>. Eventually, to solve this problem, they merged a \"lazy Adam optimizer\" <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/819a690d/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/819a690d\">tensorflow/tensorflow@<tt>819a690</tt></a> . The way this optimizer works is (1) only moments that show up in the gradient get updated, and (2) only those portions of the gradient get applied to the parameters.</p>\n<p><del>We could just implement this algorithm directly, but Adam's original suggestion about how to handle spare updates with weight decay got me thinking about how we might be able to play similar tricks in Adam's case, by applying the deferred momentum updates when we actually update the parameter in question (it's just a geometric series.)</del> That's not a good idea: the point of momentum is to apply an extra force until the gradient tells us to stop. If we're training on sparse data and batch up momentum, you could easily shoot way past the optimal point before you got more information from gradient to go the other way. So we probably do want to go as TF goes.</p>\n</li>\n<li>\n<p>Reading about Nesterov, it seems to me that this trick should also apply when we are applying deferred updates from weight decay / momentum / etc. The big problem is that unlike with Nesterov, we don't know which deferred updates we are going to apply until we've computed our gradient once. If there was a way to quickly find out what the <em>indices</em> of the sparse update would be (without bothering to compute the entire gradient), we could first immediately apply any pending momentum/weight decay updates, and THEN compute the gradient on the new point (just like as with Nesterov.) I don't know enough about the autograd mechanism to know how easy or hard this would be to do.</p>\n</li>\n<li>\n<p>A simple matter: lazy weight decay should apply all optimizers.  It will be annoying if we have to reimplement it each time, so a mixin is probably in order.</p>\n</li>\n</ul>\n<p>It will probably be interesting to see what TF is doing for all of the other optimizers we have.</p>", "body_text": "Now that I feel that I have a much better grasp on what is going on with the optimizers, let me give an expanded \"state of play\":\n\n\nAdagrad is already converted to use sparse updates. https://github.com/pytorch/pytorch/pull/735/files#diff-533671a4db549248f40d40c6b5473916  This made the code a bit more convoluted, since the naive implementation converts the sparse tensors into their dense representations too early when it calls addcmul (it would be nice to make this nicer, though it is not obvious to me what strategy should be taken here.) However, adagrad is a \"nice\" algorithm, in the sense that when the gradient is sparse, the final update is sparse as well.\n\n\nAdam, however, the updates to the parameters are NOT sparse, even when the gradients are sparse, because it accumulates moments, which means a full update to the moments (to decay the old values) and the parameters (This was also observed by @martinraison). Indeed, TF has had to grapple with this issue tensorflow/tensorflow#464; at the time this bug was written, TF's Adam implementation did a dense update, causing performance problems for other users tensorflow/tensorflow#6460. Eventually, to solve this problem, they merged a \"lazy Adam optimizer\" tensorflow/tensorflow@819a690 . The way this optimizer works is (1) only moments that show up in the gradient get updated, and (2) only those portions of the gradient get applied to the parameters.\nWe could just implement this algorithm directly, but Adam's original suggestion about how to handle spare updates with weight decay got me thinking about how we might be able to play similar tricks in Adam's case, by applying the deferred momentum updates when we actually update the parameter in question (it's just a geometric series.) That's not a good idea: the point of momentum is to apply an extra force until the gradient tells us to stop. If we're training on sparse data and batch up momentum, you could easily shoot way past the optimal point before you got more information from gradient to go the other way. So we probably do want to go as TF goes.\n\n\nReading about Nesterov, it seems to me that this trick should also apply when we are applying deferred updates from weight decay / momentum / etc. The big problem is that unlike with Nesterov, we don't know which deferred updates we are going to apply until we've computed our gradient once. If there was a way to quickly find out what the indices of the sparse update would be (without bothering to compute the entire gradient), we could first immediately apply any pending momentum/weight decay updates, and THEN compute the gradient on the new point (just like as with Nesterov.) I don't know enough about the autograd mechanism to know how easy or hard this would be to do.\n\n\nA simple matter: lazy weight decay should apply all optimizers.  It will be annoying if we have to reimplement it each time, so a mixin is probably in order.\n\n\nIt will probably be interesting to see what TF is doing for all of the other optimizers we have.", "body": "Now that I feel that I have a much better grasp on what is going on with the optimizers, let me give an expanded \"state of play\":\r\n\r\n* Adagrad is already converted to use sparse updates. https://github.com/pytorch/pytorch/pull/735/files#diff-533671a4db549248f40d40c6b5473916  This made the code a bit more convoluted, since the naive implementation converts the sparse tensors into their dense representations too early when it calls addcmul (it would be nice to make this nicer, though it is not obvious to me what strategy should be taken here.) However, adagrad is a \"nice\" algorithm, in the sense that when the gradient is sparse, the final update is sparse as well.\r\n\r\n* Adam, however, the updates to the parameters are NOT sparse, even when the gradients are sparse, because it accumulates moments, which means a full update to the moments (to decay the old values) and the parameters (This was also observed by @martinraison). Indeed, TF has had to grapple with this issue https://github.com/tensorflow/tensorflow/issues/464; at the time this bug was written, TF's Adam implementation did a dense update, causing performance problems for other users https://github.com/tensorflow/tensorflow/issues/6460. Eventually, to solve this problem, they merged a \"lazy Adam optimizer\" https://github.com/tensorflow/tensorflow/commit/819a690d . The way this optimizer works is (1) only moments that show up in the gradient get updated, and (2) only those portions of the gradient get applied to the parameters.\r\n\r\n  ~~We could just implement this algorithm directly, but Adam's original suggestion about how to handle spare updates with weight decay got me thinking about how we might be able to play similar tricks in Adam's case, by applying the deferred momentum updates when we actually update the parameter in question (it's just a geometric series.)~~ That's not a good idea: the point of momentum is to apply an extra force until the gradient tells us to stop. If we're training on sparse data and batch up momentum, you could easily shoot way past the optimal point before you got more information from gradient to go the other way. So we probably do want to go as TF goes.\r\n\r\n* Reading about Nesterov, it seems to me that this trick should also apply when we are applying deferred updates from weight decay / momentum / etc. The big problem is that unlike with Nesterov, we don't know which deferred updates we are going to apply until we've computed our gradient once. If there was a way to quickly find out what the *indices* of the sparse update would be (without bothering to compute the entire gradient), we could first immediately apply any pending momentum/weight decay updates, and THEN compute the gradient on the new point (just like as with Nesterov.) I don't know enough about the autograd mechanism to know how easy or hard this would be to do.\r\n\r\n* A simple matter: lazy weight decay should apply all optimizers.  It will be annoying if we have to reimplement it each time, so a mixin is probably in order.\r\n\r\nIt will probably be interesting to see what TF is doing for all of the other optimizers we have."}