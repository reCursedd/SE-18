{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285/events", "html_url": "https://github.com/pytorch/pytorch/issues/1285", "id": 222528997, "node_id": "MDU6SXNzdWUyMjI1Mjg5OTc=", "number": 1285, "title": "sparse optimizers", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/sparse", "name": "sparse", "color": "bfd4f2", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-04-18T20:03:04Z", "updated_at": "2018-09-12T21:03:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Now that sparse tensors are mostly working (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218344414\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1147\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/1147/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/1147\">#1147</a>), it would be awesome if all the optimizers worked with sparse tensors. This requires some cleverness to do amortized updates to the parameters. For example, for weight decay, you would do something like (pseudocode):</p>\n<pre><code># apply weight decay\n# dp is sparse\nn_t++\nfor i in dp.indices():\n  p[i] *= (1 - lr * wd)^(n_t - n[i])\n  n[i] = n_t\n</code></pre>\n<p>Note this isn't exactly equivalent (to be equivalent you'd need to apply the weight decay before the forward pass, not after backwards), but it's a good approximation. You can do the same thing for momentum.</p>\n<p>I'm guessing the same thing works for Adam/Adamax as well but I haven't worked through the equations. <a href=\"https://arxiv.org/pdf/1412.6980.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1412.6980.pdf</a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> expressed interest in working on this.</p>", "body_text": "Now that sparse tensors are mostly working (#1147), it would be awesome if all the optimizers worked with sparse tensors. This requires some cleverness to do amortized updates to the parameters. For example, for weight decay, you would do something like (pseudocode):\n# apply weight decay\n# dp is sparse\nn_t++\nfor i in dp.indices():\n  p[i] *= (1 - lr * wd)^(n_t - n[i])\n  n[i] = n_t\n\nNote this isn't exactly equivalent (to be equivalent you'd need to apply the weight decay before the forward pass, not after backwards), but it's a good approximation. You can do the same thing for momentum.\nI'm guessing the same thing works for Adam/Adamax as well but I haven't worked through the equations. https://arxiv.org/pdf/1412.6980.pdf\n@ezyang expressed interest in working on this.", "body": "Now that sparse tensors are mostly working (https://github.com/pytorch/pytorch/pull/1147), it would be awesome if all the optimizers worked with sparse tensors. This requires some cleverness to do amortized updates to the parameters. For example, for weight decay, you would do something like (pseudocode):\r\n\r\n```\r\n# apply weight decay\r\n# dp is sparse\r\nn_t++\r\nfor i in dp.indices():\r\n  p[i] *= (1 - lr * wd)^(n_t - n[i])\r\n  n[i] = n_t\r\n```\r\nNote this isn't exactly equivalent (to be equivalent you'd need to apply the weight decay before the forward pass, not after backwards), but it's a good approximation. You can do the same thing for momentum. \r\n\r\nI'm guessing the same thing works for Adam/Adamax as well but I haven't worked through the equations. https://arxiv.org/pdf/1412.6980.pdf\r\n\r\n@ezyang expressed interest in working on this."}