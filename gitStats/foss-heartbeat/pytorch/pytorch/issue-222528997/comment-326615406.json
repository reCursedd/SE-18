{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326615406", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-326615406", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 326615406, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYxNTQwNg==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-01T15:47:15Z", "updated_at": "2017-09-01T15:47:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=346999\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jph00\">@jph00</a> thanks for the anecdotal results, that's really good to know. Are you sure that it's LazyAdam in particular that leads to good embedding layers (vs. some other TF vs PT difference)? I.e. have you trained the same TF with DenseAdam / Adagrad / SGD and observed substantially worse embeddings?</p>\n<p>To clarify, you say you use sparse momentum + weight decay, but I don't see any weight decay option in <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/819a690d/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/819a690d\">tensorflow/tensorflow@<tt>819a690</tt></a> , so how are you applying weight decay? Is it dense or sparse?</p>\n<p>Re: my thoughts, I guess my main request is that if S is a sparse tensor then it should be the case that <code>F(S) = F(toDense(S))</code> for all operators <code>F</code> in pytorch, unless specially marked otherwise. So weight decay shouldn't magically be different for sparse tensors. But if there's some other optimizer that works better for embeddings, by all means lets implement it. (Note: maybe we shouldn't assume that any sparse gradient comes from an embedding...)</p>", "body_text": "@jph00 thanks for the anecdotal results, that's really good to know. Are you sure that it's LazyAdam in particular that leads to good embedding layers (vs. some other TF vs PT difference)? I.e. have you trained the same TF with DenseAdam / Adagrad / SGD and observed substantially worse embeddings?\nTo clarify, you say you use sparse momentum + weight decay, but I don't see any weight decay option in tensorflow/tensorflow@819a690 , so how are you applying weight decay? Is it dense or sparse?\nRe: my thoughts, I guess my main request is that if S is a sparse tensor then it should be the case that F(S) = F(toDense(S)) for all operators F in pytorch, unless specially marked otherwise. So weight decay shouldn't magically be different for sparse tensors. But if there's some other optimizer that works better for embeddings, by all means lets implement it. (Note: maybe we shouldn't assume that any sparse gradient comes from an embedding...)", "body": "@jph00 thanks for the anecdotal results, that's really good to know. Are you sure that it's LazyAdam in particular that leads to good embedding layers (vs. some other TF vs PT difference)? I.e. have you trained the same TF with DenseAdam / Adagrad / SGD and observed substantially worse embeddings? \r\n\r\nTo clarify, you say you use sparse momentum + weight decay, but I don't see any weight decay option in https://github.com/tensorflow/tensorflow/commit/819a690d , so how are you applying weight decay? Is it dense or sparse?\r\n\r\nRe: my thoughts, I guess my main request is that if S is a sparse tensor then it should be the case that `F(S) = F(toDense(S))` for all operators `F` in pytorch, unless specially marked otherwise. So weight decay shouldn't magically be different for sparse tensors. But if there's some other optimizer that works better for embeddings, by all means lets implement it. (Note: maybe we shouldn't assume that any sparse gradient comes from an embedding...)"}