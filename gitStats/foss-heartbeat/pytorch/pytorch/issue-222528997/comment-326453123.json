{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326453123", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-326453123", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 326453123, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjQ1MzEyMw==", "user": {"login": "jph00", "id": 346999, "node_id": "MDQ6VXNlcjM0Njk5OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/346999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jph00", "html_url": "https://github.com/jph00", "followers_url": "https://api.github.com/users/jph00/followers", "following_url": "https://api.github.com/users/jph00/following{/other_user}", "gists_url": "https://api.github.com/users/jph00/gists{/gist_id}", "starred_url": "https://api.github.com/users/jph00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jph00/subscriptions", "organizations_url": "https://api.github.com/users/jph00/orgs", "repos_url": "https://api.github.com/users/jph00/repos", "events_url": "https://api.github.com/users/jph00/events{/privacy}", "received_events_url": "https://api.github.com/users/jph00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-01T00:13:58Z", "updated_at": "2017-09-01T00:13:58Z", "author_association": "NONE", "body_html": "<p>Well... I do want to point out that the Tensorflow implementation works <em>extremely</em> well for embedding layers. It's very helpful to have both momentum methods and weight decay in embedding layers, but the current pytorch sparse approach doesn't work at all in this case. If I remove weight decay and use adagrad (which works with sparse layers) I don't get good results. With tensorflow (using keras) I've been able to build text classification (sentiment analysis) models that are far more accurate, have less parameters, and train more quickly and reliably using their approach to sparse updates. To be clear: they're far better regardless of whether I'm using weight decay - their approach to sparse updates seems to be giving much better results (and I've confirmed that other details like initializers are the same).</p>\n<p>The goal here shouldn't be to perform only as well as dense adam (which performs very poorly in this case), but as well as tensorflow's sparse adam (and sparse momentum, and sparse weight decay, etc).</p>\n<p>Whilst I'm not sure about <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a>'s theory, I do think that a lazy approach should at least be benchmarked against the simple gradient masking approach to see when/if it's actually better. If it's not better for real examples, then it's probably not a good idea.</p>\n<p>Just my $0.02...</p>", "body_text": "Well... I do want to point out that the Tensorflow implementation works extremely well for embedding layers. It's very helpful to have both momentum methods and weight decay in embedding layers, but the current pytorch sparse approach doesn't work at all in this case. If I remove weight decay and use adagrad (which works with sparse layers) I don't get good results. With tensorflow (using keras) I've been able to build text classification (sentiment analysis) models that are far more accurate, have less parameters, and train more quickly and reliably using their approach to sparse updates. To be clear: they're far better regardless of whether I'm using weight decay - their approach to sparse updates seems to be giving much better results (and I've confirmed that other details like initializers are the same).\nThe goal here shouldn't be to perform only as well as dense adam (which performs very poorly in this case), but as well as tensorflow's sparse adam (and sparse momentum, and sparse weight decay, etc).\nWhilst I'm not sure about @adamlerer's theory, I do think that a lazy approach should at least be benchmarked against the simple gradient masking approach to see when/if it's actually better. If it's not better for real examples, then it's probably not a good idea.\nJust my $0.02...", "body": "Well... I do want to point out that the Tensorflow implementation works _extremely_ well for embedding layers. It's very helpful to have both momentum methods and weight decay in embedding layers, but the current pytorch sparse approach doesn't work at all in this case. If I remove weight decay and use adagrad (which works with sparse layers) I don't get good results. With tensorflow (using keras) I've been able to build text classification (sentiment analysis) models that are far more accurate, have less parameters, and train more quickly and reliably using their approach to sparse updates. To be clear: they're far better regardless of whether I'm using weight decay - their approach to sparse updates seems to be giving much better results (and I've confirmed that other details like initializers are the same).\r\n\r\nThe goal here shouldn't be to perform only as well as dense adam (which performs very poorly in this case), but as well as tensorflow's sparse adam (and sparse momentum, and sparse weight decay, etc).\r\n\r\nWhilst I'm not sure about @adamlerer's theory, I do think that a lazy approach should at least be benchmarked against the simple gradient masking approach to see when/if it's actually better. If it's not better for real examples, then it's probably not a good idea.\r\n\r\nJust my $0.02..."}