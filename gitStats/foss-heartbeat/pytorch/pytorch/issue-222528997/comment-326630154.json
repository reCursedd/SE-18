{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326630154", "html_url": "https://github.com/pytorch/pytorch/issues/1285#issuecomment-326630154", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1285", "id": 326630154, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYzMDE1NA==", "user": {"login": "jph00", "id": 346999, "node_id": "MDQ6VXNlcjM0Njk5OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/346999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jph00", "html_url": "https://github.com/jph00", "followers_url": "https://api.github.com/users/jph00/followers", "following_url": "https://api.github.com/users/jph00/following{/other_user}", "gists_url": "https://api.github.com/users/jph00/gists{/gist_id}", "starred_url": "https://api.github.com/users/jph00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jph00/subscriptions", "organizations_url": "https://api.github.com/users/jph00/orgs", "repos_url": "https://api.github.com/users/jph00/repos", "events_url": "https://api.github.com/users/jph00/events{/privacy}", "received_events_url": "https://api.github.com/users/jph00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-01T16:49:17Z", "updated_at": "2017-09-01T16:49:17Z", "author_association": "NONE", "body_html": "<p>It's definitely not just lazyadam - I'm guessing it's just sparse_apply in general. But I don't know the exact source of the difference; at this point all I can say for sure is that keras, with tf backend, trains simple models with embeddings extremely quickly and extremely accurately compared to pytorch, and allows the use of weight decay and any optimizer. I've only briefly scanned the keras code, but it seems to generally be just passing stuff down to tf with minimal fiddling, so I was guessing that the reason must be in how tf is handling the embeddings. But I don't know that for sure.</p>\n<p>I also noticed that setting sparse=True in pytorch resulted in better results for simple models than setting sparse=False. So even in pytorch something about that approach is resulting in improvements.</p>\n<p>Which is to say: trying to match dense results doesn't seem like it should be the goal here...</p>", "body_text": "It's definitely not just lazyadam - I'm guessing it's just sparse_apply in general. But I don't know the exact source of the difference; at this point all I can say for sure is that keras, with tf backend, trains simple models with embeddings extremely quickly and extremely accurately compared to pytorch, and allows the use of weight decay and any optimizer. I've only briefly scanned the keras code, but it seems to generally be just passing stuff down to tf with minimal fiddling, so I was guessing that the reason must be in how tf is handling the embeddings. But I don't know that for sure.\nI also noticed that setting sparse=True in pytorch resulted in better results for simple models than setting sparse=False. So even in pytorch something about that approach is resulting in improvements.\nWhich is to say: trying to match dense results doesn't seem like it should be the goal here...", "body": "It's definitely not just lazyadam - I'm guessing it's just sparse_apply in general. But I don't know the exact source of the difference; at this point all I can say for sure is that keras, with tf backend, trains simple models with embeddings extremely quickly and extremely accurately compared to pytorch, and allows the use of weight decay and any optimizer. I've only briefly scanned the keras code, but it seems to generally be just passing stuff down to tf with minimal fiddling, so I was guessing that the reason must be in how tf is handling the embeddings. But I don't know that for sure.\r\n\r\nI also noticed that setting sparse=True in pytorch resulted in better results for simple models than setting sparse=False. So even in pytorch something about that approach is resulting in improvements.\r\n\r\nWhich is to say: trying to match dense results doesn't seem like it should be the goal here..."}