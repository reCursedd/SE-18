{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11141", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11141/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11141/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11141/events", "html_url": "https://github.com/pytorch/pytorch/issues/11141", "id": 355983250, "node_id": "MDU6SXNzdWUzNTU5ODMyNTA=", "number": 11141, "title": "JIT fusion difference between module and functional interfaces", "user": {"login": "slayton58", "id": 4992598, "node_id": "MDQ6VXNlcjQ5OTI1OTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/4992598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slayton58", "html_url": "https://github.com/slayton58", "followers_url": "https://api.github.com/users/slayton58/followers", "following_url": "https://api.github.com/users/slayton58/following{/other_user}", "gists_url": "https://api.github.com/users/slayton58/gists{/gist_id}", "starred_url": "https://api.github.com/users/slayton58/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slayton58/subscriptions", "organizations_url": "https://api.github.com/users/slayton58/orgs", "repos_url": "https://api.github.com/users/slayton58/repos", "events_url": "https://api.github.com/users/slayton58/events{/privacy}", "received_events_url": "https://api.github.com/users/slayton58/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-31T13:35:24Z", "updated_at": "2018-09-04T15:32:42Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>There is a difference in the fused code generated when using the functional <code>(x + y).relu_()</code> and the module <code>relu = nn.ReLU(inplace=True); relu(x+y)</code></p>\n<h2>Code example</h2>\n<pre><code>import torch\nimport torch.jit\nimport torch.nn.modules as nn\n\ndef functional_add_relu(x, y):\n    return (x + y).relu_()\n\ndef nn_add_relu(x, y):\n    relu = nn.ReLU(inplace=True)\n    return relu(x + y)\n\nx = torch.Tensor(128, 3, 300, 300).cuda().uniform_()\ny = torch.Tensor(128, 3, 300, 300).cuda().uniform_()\n\ntraced_fn1 = torch.jit.trace(x, y)(functional_add_relu)\ntraced_fn1(x, y)\n\ntraced_fn2 = torch.jit.trace(x, y)(nn_add_relu)\ntraced_fn2(x, y)\n\nprint(\"Functional:\")\nprint(traced_fn1.graph_for(x, y))\nprint(\"Module:\")\nprint(traced_fn2.graph_for(x, y))\n</code></pre>\n<p>Output:</p>\n<pre><code>Functional:\ngraph(%0 : Float(*, *, *, *)\n      %1 : Float(*, *, *, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %5 : Float(*, *, *, *) = prim::FusionGroup_0[device=0](%0, %1)\n  return (%5);\n}\nwith prim::FusionGroup_0 = graph(%2 : Float(*, *, *, *)\n      %3 : Float(*, *, *, *)) {\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(*, *, *, *) = aten::add(%2, %3, %4)\n  %1 : Float(*, *, *, *) = aten::relu(%5)\n  return (%1);\n}\n\nModule:\ngraph(%0 : Float(*, *, *, *)\n      %1 : Float(*, *, *, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(*, *, *, *) = aten::add(%0, %1, %2)\n  %4 : int = prim::Constant[value=0]()\n  %6 : Dynamic = aten::threshold(%3, %4, %4)\n  return (%6);\n}\n</code></pre>\n<p>Note that fusion only occurred in the functional <code>(x + y).relu_()</code> case.</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.5.0a0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 10.0.122</p>\n<p>OS: Ubuntu 16.04.5 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 10.0.122</p>", "body_text": "Issue description\nThere is a difference in the fused code generated when using the functional (x + y).relu_() and the module relu = nn.ReLU(inplace=True); relu(x+y)\nCode example\nimport torch\nimport torch.jit\nimport torch.nn.modules as nn\n\ndef functional_add_relu(x, y):\n    return (x + y).relu_()\n\ndef nn_add_relu(x, y):\n    relu = nn.ReLU(inplace=True)\n    return relu(x + y)\n\nx = torch.Tensor(128, 3, 300, 300).cuda().uniform_()\ny = torch.Tensor(128, 3, 300, 300).cuda().uniform_()\n\ntraced_fn1 = torch.jit.trace(x, y)(functional_add_relu)\ntraced_fn1(x, y)\n\ntraced_fn2 = torch.jit.trace(x, y)(nn_add_relu)\ntraced_fn2(x, y)\n\nprint(\"Functional:\")\nprint(traced_fn1.graph_for(x, y))\nprint(\"Module:\")\nprint(traced_fn2.graph_for(x, y))\n\nOutput:\nFunctional:\ngraph(%0 : Float(*, *, *, *)\n      %1 : Float(*, *, *, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %5 : Float(*, *, *, *) = prim::FusionGroup_0[device=0](%0, %1)\n  return (%5);\n}\nwith prim::FusionGroup_0 = graph(%2 : Float(*, *, *, *)\n      %3 : Float(*, *, *, *)) {\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(*, *, *, *) = aten::add(%2, %3, %4)\n  %1 : Float(*, *, *, *) = aten::relu(%5)\n  return (%1);\n}\n\nModule:\ngraph(%0 : Float(*, *, *, *)\n      %1 : Float(*, *, *, *)) {\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(*, *, *, *) = aten::add(%0, %1, %2)\n  %4 : int = prim::Constant[value=0]()\n  %6 : Dynamic = aten::threshold(%3, %4, %4)\n  return (%6);\n}\n\nNote that fusion only occurred in the functional (x + y).relu_() case.\nSystem Info\nPyTorch version: 0.5.0a0\nIs debug build: No\nCUDA used to build PyTorch: 10.0.122\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 10.0.122", "body": "## Issue description\r\n\r\nThere is a difference in the fused code generated when using the functional `(x + y).relu_()` and the module `relu = nn.ReLU(inplace=True); relu(x+y)`\r\n\r\n## Code example\r\n\r\n```\r\nimport torch\r\nimport torch.jit\r\nimport torch.nn.modules as nn\r\n\r\ndef functional_add_relu(x, y):\r\n    return (x + y).relu_()\r\n\r\ndef nn_add_relu(x, y):\r\n    relu = nn.ReLU(inplace=True)\r\n    return relu(x + y)\r\n\r\nx = torch.Tensor(128, 3, 300, 300).cuda().uniform_()\r\ny = torch.Tensor(128, 3, 300, 300).cuda().uniform_()\r\n\r\ntraced_fn1 = torch.jit.trace(x, y)(functional_add_relu)\r\ntraced_fn1(x, y)\r\n\r\ntraced_fn2 = torch.jit.trace(x, y)(nn_add_relu)\r\ntraced_fn2(x, y)\r\n\r\nprint(\"Functional:\")\r\nprint(traced_fn1.graph_for(x, y))\r\nprint(\"Module:\")\r\nprint(traced_fn2.graph_for(x, y))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nFunctional:\r\ngraph(%0 : Float(*, *, *, *)\r\n      %1 : Float(*, *, *, *)) {\r\n  %2 : int = prim::Constant[value=1]()\r\n  %5 : Float(*, *, *, *) = prim::FusionGroup_0[device=0](%0, %1)\r\n  return (%5);\r\n}\r\nwith prim::FusionGroup_0 = graph(%2 : Float(*, *, *, *)\r\n      %3 : Float(*, *, *, *)) {\r\n  %4 : int = prim::Constant[value=1]()\r\n  %5 : Float(*, *, *, *) = aten::add(%2, %3, %4)\r\n  %1 : Float(*, *, *, *) = aten::relu(%5)\r\n  return (%1);\r\n}\r\n\r\nModule:\r\ngraph(%0 : Float(*, *, *, *)\r\n      %1 : Float(*, *, *, *)) {\r\n  %2 : int = prim::Constant[value=1]()\r\n  %3 : Float(*, *, *, *) = aten::add(%0, %1, %2)\r\n  %4 : int = prim::Constant[value=0]()\r\n  %6 : Dynamic = aten::threshold(%3, %4, %4)\r\n  return (%6);\r\n}\r\n```\r\n\r\nNote that fusion only occurred in the functional `(x + y).relu_()` case.\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.5.0a0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.122\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.122\r\n\r\n"}