{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8652", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8652/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8652/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8652/events", "html_url": "https://github.com/pytorch/pytorch/issues/8652", "id": 333723619, "node_id": "MDU6SXNzdWUzMzM3MjM2MTk=", "number": 8652, "title": "Batch Sampler and Dataset indexing too restricted", "user": {"login": "flauted", "id": 29395172, "node_id": "MDQ6VXNlcjI5Mzk1MTcy", "avatar_url": "https://avatars2.githubusercontent.com/u/29395172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flauted", "html_url": "https://github.com/flauted", "followers_url": "https://api.github.com/users/flauted/followers", "following_url": "https://api.github.com/users/flauted/following{/other_user}", "gists_url": "https://api.github.com/users/flauted/gists{/gist_id}", "starred_url": "https://api.github.com/users/flauted/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flauted/subscriptions", "organizations_url": "https://api.github.com/users/flauted/orgs", "repos_url": "https://api.github.com/users/flauted/repos", "events_url": "https://api.github.com/users/flauted/events{/privacy}", "received_events_url": "https://api.github.com/users/flauted/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-19T15:28:20Z", "updated_at": "2018-06-27T23:07:33Z", "closed_at": "2018-06-27T23:07:33Z", "author_association": "NONE", "body_html": "<p>I was trying to use a Dataloader with a sampler of strings to index my dataset. The default <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L104\">BatchSampler</a> threw me an error that essentially str cannot be casted to int. The cast in question happens on line 139 (currently):</p>\n<pre><code>batch.append(int(idx))\n</code></pre>\n<p>I copied the source for the Batch Loader from master, removed the cast, and everything worked as expected.</p>\n<p><a href=\"https://pytorch.org/docs/stable/data.html\" rel=\"nofollow\"><code>torch.utils.data</code></a> <code>DataLoader</code> doesn't suggest that samplers should return integers, and the various Samplers just say list of indices. An index set doesn't have to be integers. Now <code>Dataset</code> <em>does</em> say <code>__getitem__</code> should support <strong>integer</strong> indexing, but why?</p>\n<p>Really, the choice of sampler (including choosing by default the default sampler) drives whether a Dataset must be indexable by integers. On the other hand, sometimes it's more sensible to \"key\" the dataset. For example, the sampler could \"request\" the a filename be loaded. Or, the actual data could be organized in a dict (like I have in my examples below).</p>\n<p>Along the way, I noticed that BatchLoader is now asserting that <code>sampler</code> is a <code>Sampler</code>. (This isn't the case in 0.4.0, just master). I don't really understand why. Why not duck type that <code>sampler</code> is iterable? Just a try-except around the for-loop that catches TypeError and returns whatever type of error. It breaks the example if I'm not mistaken, and excludes a very useful way to unittest.</p>\n<h2>MWE</h2>\n<pre><code>from string import ascii_lowercase\nimport torch\nfrom torch._six import int_classes as _int_classes\nfrom torch.utils.data import sampler, DataLoader\n\n\n# CHANGES to BatchSampler in master: Removing type cast and removing sampler\n# explicit type-check, adding iterable duck-type\nclass BatchSampler(sampler.Sampler):\n    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n    Example:\n        &gt;&gt;&gt; list(BatchSampler(range(10), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        &gt;&gt;&gt; list(BatchSampler(range(10), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    \"\"\"\n    def __init__(self, sampler, batch_size, drop_last):\n        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\n                batch_size &lt;= 0:\n            raise ValueError(\"batch_size should be a positive integeral value, \"\n                             \"but got batch_size={}\".format(batch_size))\n        if not isinstance(drop_last, bool):\n            raise ValueError(\"drop_last should be a boolean value, but got \"\n                             \"drop_last={}\".format(drop_last))\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        try:\n            for idx in self.sampler:\n                batch.append(idx)\n                if len(batch) == self.batch_size:\n                    yield batch\n                    batch = []\n            if len(batch) &gt; 0 and not self.drop_last:\n                yield batch\n        except TypeError:\n            raise ValueError(\"sampler should be an iterable, \"\n                                       \"but got sampler={}\".format(sampler))\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n\n\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\nloader = DataLoader(dataset_stub, batch_sampler=BatchSampler(sampler, 1, True))\n\nfor elem in loader:\n    print(elem)\n</code></pre>\n<h2>M.should-be-W.E.:</h2>\n<pre><code>from string import ascii_lowercase\nimport torch\nfrom torch.utils.data import sampler, DataLoader\n\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\nloader = DataLoader(dataset_stub, sampler=sampler)\n\nfor elem in loader:\n    print(elem)\n</code></pre>", "body_text": "I was trying to use a Dataloader with a sampler of strings to index my dataset. The default BatchSampler threw me an error that essentially str cannot be casted to int. The cast in question happens on line 139 (currently):\nbatch.append(int(idx))\n\nI copied the source for the Batch Loader from master, removed the cast, and everything worked as expected.\ntorch.utils.data DataLoader doesn't suggest that samplers should return integers, and the various Samplers just say list of indices. An index set doesn't have to be integers. Now Dataset does say __getitem__ should support integer indexing, but why?\nReally, the choice of sampler (including choosing by default the default sampler) drives whether a Dataset must be indexable by integers. On the other hand, sometimes it's more sensible to \"key\" the dataset. For example, the sampler could \"request\" the a filename be loaded. Or, the actual data could be organized in a dict (like I have in my examples below).\nAlong the way, I noticed that BatchLoader is now asserting that sampler is a Sampler. (This isn't the case in 0.4.0, just master). I don't really understand why. Why not duck type that sampler is iterable? Just a try-except around the for-loop that catches TypeError and returns whatever type of error. It breaks the example if I'm not mistaken, and excludes a very useful way to unittest.\nMWE\nfrom string import ascii_lowercase\nimport torch\nfrom torch._six import int_classes as _int_classes\nfrom torch.utils.data import sampler, DataLoader\n\n\n# CHANGES to BatchSampler in master: Removing type cast and removing sampler\n# explicit type-check, adding iterable duck-type\nclass BatchSampler(sampler.Sampler):\n    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n    Example:\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    \"\"\"\n    def __init__(self, sampler, batch_size, drop_last):\n        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\n                batch_size <= 0:\n            raise ValueError(\"batch_size should be a positive integeral value, \"\n                             \"but got batch_size={}\".format(batch_size))\n        if not isinstance(drop_last, bool):\n            raise ValueError(\"drop_last should be a boolean value, but got \"\n                             \"drop_last={}\".format(drop_last))\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        try:\n            for idx in self.sampler:\n                batch.append(idx)\n                if len(batch) == self.batch_size:\n                    yield batch\n                    batch = []\n            if len(batch) > 0 and not self.drop_last:\n                yield batch\n        except TypeError:\n            raise ValueError(\"sampler should be an iterable, \"\n                                       \"but got sampler={}\".format(sampler))\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n\n\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\nloader = DataLoader(dataset_stub, batch_sampler=BatchSampler(sampler, 1, True))\n\nfor elem in loader:\n    print(elem)\n\nM.should-be-W.E.:\nfrom string import ascii_lowercase\nimport torch\nfrom torch.utils.data import sampler, DataLoader\n\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\nloader = DataLoader(dataset_stub, sampler=sampler)\n\nfor elem in loader:\n    print(elem)", "body": "I was trying to use a Dataloader with a sampler of strings to index my dataset. The default [BatchSampler](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L104) threw me an error that essentially str cannot be casted to int. The cast in question happens on line 139 (currently):\r\n```\r\nbatch.append(int(idx))\r\n```\r\nI copied the source for the Batch Loader from master, removed the cast, and everything worked as expected.\r\n\r\n[``torch.utils.data``](https://pytorch.org/docs/stable/data.html) ``DataLoader`` doesn't suggest that samplers should return integers, and the various Samplers just say list of indices. An index set doesn't have to be integers. Now ``Dataset`` *does* say ``__getitem__`` should support **integer** indexing, but why?\r\n\r\nReally, the choice of sampler (including choosing by default the default sampler) drives whether a Dataset must be indexable by integers. On the other hand, sometimes it's more sensible to \"key\" the dataset. For example, the sampler could \"request\" the a filename be loaded. Or, the actual data could be organized in a dict (like I have in my examples below).\r\n\r\nAlong the way, I noticed that BatchLoader is now asserting that ``sampler`` is a ``Sampler``. (This isn't the case in 0.4.0, just master). I don't really understand why. Why not duck type that ``sampler`` is iterable? Just a try-except around the for-loop that catches TypeError and returns whatever type of error. It breaks the example if I'm not mistaken, and excludes a very useful way to unittest.\r\n\r\n## MWE\r\n```\r\nfrom string import ascii_lowercase\r\nimport torch\r\nfrom torch._six import int_classes as _int_classes\r\nfrom torch.utils.data import sampler, DataLoader\r\n\r\n\r\n# CHANGES to BatchSampler in master: Removing type cast and removing sampler\r\n# explicit type-check, adding iterable duck-type\r\nclass BatchSampler(sampler.Sampler):\r\n    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\r\n    Args:\r\n        sampler (Sampler): Base sampler.\r\n        batch_size (int): Size of mini-batch.\r\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\r\n            its size would be less than ``batch_size``\r\n    Example:\r\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\r\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\r\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\r\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\r\n    \"\"\"\r\n    def __init__(self, sampler, batch_size, drop_last):\r\n        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\r\n                batch_size <= 0:\r\n            raise ValueError(\"batch_size should be a positive integeral value, \"\r\n                             \"but got batch_size={}\".format(batch_size))\r\n        if not isinstance(drop_last, bool):\r\n            raise ValueError(\"drop_last should be a boolean value, but got \"\r\n                             \"drop_last={}\".format(drop_last))\r\n        self.sampler = sampler\r\n        self.batch_size = batch_size\r\n        self.drop_last = drop_last\r\n\r\n    def __iter__(self):\r\n        batch = []\r\n        try:\r\n            for idx in self.sampler:\r\n                batch.append(idx)\r\n                if len(batch) == self.batch_size:\r\n                    yield batch\r\n                    batch = []\r\n            if len(batch) > 0 and not self.drop_last:\r\n                yield batch\r\n        except TypeError:\r\n            raise ValueError(\"sampler should be an iterable, \"\r\n                                       \"but got sampler={}\".format(sampler))\r\n\r\n    def __len__(self):\r\n        if self.drop_last:\r\n            return len(self.sampler) // self.batch_size\r\n        else:\r\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\r\n\r\n\r\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\r\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\r\nloader = DataLoader(dataset_stub, batch_sampler=BatchSampler(sampler, 1, True))\r\n\r\nfor elem in loader:\r\n    print(elem)\r\n```\r\n## M.should-be-W.E.:\r\n```\r\nfrom string import ascii_lowercase\r\nimport torch\r\nfrom torch.utils.data import sampler, DataLoader\r\n\r\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\r\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\r\nloader = DataLoader(dataset_stub, sampler=sampler)\r\n\r\nfor elem in loader:\r\n    print(elem)\r\n```"}