{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7248", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7248/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7248/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7248/events", "html_url": "https://github.com/pytorch/pytorch/issues/7248", "id": 320020390, "node_id": "MDU6SXNzdWUzMjAwMjAzOTA=", "number": 7248, "title": "Possible memory leak in nn.ConvTranspose2d", "user": {"login": "emitch", "id": 7739114, "node_id": "MDQ6VXNlcjc3MzkxMTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/7739114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emitch", "html_url": "https://github.com/emitch", "followers_url": "https://api.github.com/users/emitch/followers", "following_url": "https://api.github.com/users/emitch/following{/other_user}", "gists_url": "https://api.github.com/users/emitch/gists{/gist_id}", "starred_url": "https://api.github.com/users/emitch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emitch/subscriptions", "organizations_url": "https://api.github.com/users/emitch/orgs", "repos_url": "https://api.github.com/users/emitch/repos", "events_url": "https://api.github.com/users/emitch/events{/privacy}", "received_events_url": "https://api.github.com/users/emitch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-03T17:56:21Z", "updated_at": "2018-09-07T14:48:28Z", "closed_at": "2018-09-07T14:48:28Z", "author_association": "NONE", "body_html": "<h2>Problem Description</h2>\n<p>When training a 2d UNet, memory usage steadily increases each batch until memory is exhausted. The issue seems to be related to the nn.ConvTranspose2d module, which is used in the upsampling part of the UNet to increase feature map resolution. Simply replacing each nn.ConvTranspose2d with a nn.Upsample eliminates the leak.</p>\n<p>My network is defined as follows:</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Conv(nn.Module):\n    def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True, f_out=None, groups=1):\n        super(Conv, self).__init__()\n        self.f = f\n        self.f_out = f_out\n        self.conv1 = nn.Conv2d(in_ch, out_ch, k, padding=(k-1)/2 if same else 0, groups=groups)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n\n    def forward(self, x):\n        out = self.f(self.conv1(x))\n        out = self.f(self.conv2(out)) if self.f_out is None else self.f_out(self.conv2(out))\n        return out\n\nclass Down(nn.Module):\n    def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True):\n        super(Down, self).__init__()\n        self.f = f\n        self.downsample = nn.MaxPool2d(2)\n        self.conv1 = nn.Conv2d(in_ch, in_ch, k, padding=(k-1)/2 if same else 0)\n        self.conv2 = nn.Conv2d(in_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n\n    def forward(self, x):\n        out = self.f(self.downsample(x))\n        out = self.f(self.conv1(out))\n        return self.f(self.conv2(out))\n\n\nclass Up(nn.Module):\n    def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True, cat=True):\n        super(Up, self).__init__()\n        self.f = f\n        #self.upsample = nn.ConvTranspose2d(in_ch, in_ch, k, stride=2, padding=(k-1)/2 if same else 0, output_padding=1)                                                                                                                                                                                                                                                  \n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n        self.conv1 = nn.Conv2d(in_ch + out_ch if cat else in_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n\n    def forward(self, x, y=None):\n        out = self.f(self.upsample(x))\n        if y is not None:\n            out = torch.cat((out, y),1)\n        out = self.f(self.conv1(out))\n        return self.f(self.conv2(out))\n\nclass UNet(nn.Module):\n    def __init__(self, in_ch=2, out_ch=2, depth=4, fm=24, k=3, same=True):\n        super(UNet, self).__init__()\n        \n        print('--------- Building UNet with depth ' + str(depth) + ' and kernel size ' + str(k) + ' ---------')\n        \n        self.downModules = nn.ModuleList([Down(fm, fm, k=k, same=same) for i in range(depth)])\n        self.upModules = nn.ModuleList([Up(fm, fm, k=k, same=same) for i in range(depth)])\n        \n        self.embed_in = Conv(in_ch, fm, k=7, same=same, groups=2)\n        self.embed_out = Conv(fm, out_ch, same=same, f_out=lambda x: x)\n        self.f = nn.ReLU()\n\n    def forward(self, x):\n        out = self.embed_in(x)\n        downPass = [out]\n        for down in self.downModules:\n            downPass.append(down(downPass[-1]))\n        upPass = [downPass[-1]]\n        for idx, up in enumerate(self.upModules):\n            upPass.append(up(upPass[-1], downPass[-(idx+2)]))\n        out = self.embed_out(upPass[-1])\n        return out.permute(0,2,3,1)\n</code></pre>\n<p>Commenting out the nn.Upsample() module and uncommenting the nn.ConvTranspose2d() module in the Up module triggers the issue.</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.3.0.post4<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 2.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.61<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080 Ti<br>\nGPU 1: GeForce GTX 1080 Ti<br>\nGPU 2: GeForce GTX 1080 Ti<br>\nGPU 3: GeForce GTX 1080 Ti</p>\n<p>Nvidia driver version: 390.25<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.7<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.5<br>\n/usr/local/cuda-8.0/lib64/libcudnn_static.a<br>\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5<br>\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.2)<br>\n[pip] torch (0.3.0.post4)<br>\n[pip] torchfile (0.1.0)<br>\n[pip] torchnet (0.0.1)<br>\n[pip] torchvision (0.2.0)<br>\n[conda] Could not collect</p>", "body_text": "Problem Description\nWhen training a 2d UNet, memory usage steadily increases each batch until memory is exhausted. The issue seems to be related to the nn.ConvTranspose2d module, which is used in the upsampling part of the UNet to increase feature map resolution. Simply replacing each nn.ConvTranspose2d with a nn.Upsample eliminates the leak.\nMy network is defined as follows:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Conv(nn.Module):\n    def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True, f_out=None, groups=1):\n        super(Conv, self).__init__()\n        self.f = f\n        self.f_out = f_out\n        self.conv1 = nn.Conv2d(in_ch, out_ch, k, padding=(k-1)/2 if same else 0, groups=groups)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n\n    def forward(self, x):\n        out = self.f(self.conv1(x))\n        out = self.f(self.conv2(out)) if self.f_out is None else self.f_out(self.conv2(out))\n        return out\n\nclass Down(nn.Module):\n    def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True):\n        super(Down, self).__init__()\n        self.f = f\n        self.downsample = nn.MaxPool2d(2)\n        self.conv1 = nn.Conv2d(in_ch, in_ch, k, padding=(k-1)/2 if same else 0)\n        self.conv2 = nn.Conv2d(in_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n\n    def forward(self, x):\n        out = self.f(self.downsample(x))\n        out = self.f(self.conv1(out))\n        return self.f(self.conv2(out))\n\n\nclass Up(nn.Module):\n    def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True, cat=True):\n        super(Up, self).__init__()\n        self.f = f\n        #self.upsample = nn.ConvTranspose2d(in_ch, in_ch, k, stride=2, padding=(k-1)/2 if same else 0, output_padding=1)                                                                                                                                                                                                                                                  \n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n        self.conv1 = nn.Conv2d(in_ch + out_ch if cat else in_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, k, padding=(k-1)/2 if same else 0)\n\n    def forward(self, x, y=None):\n        out = self.f(self.upsample(x))\n        if y is not None:\n            out = torch.cat((out, y),1)\n        out = self.f(self.conv1(out))\n        return self.f(self.conv2(out))\n\nclass UNet(nn.Module):\n    def __init__(self, in_ch=2, out_ch=2, depth=4, fm=24, k=3, same=True):\n        super(UNet, self).__init__()\n        \n        print('--------- Building UNet with depth ' + str(depth) + ' and kernel size ' + str(k) + ' ---------')\n        \n        self.downModules = nn.ModuleList([Down(fm, fm, k=k, same=same) for i in range(depth)])\n        self.upModules = nn.ModuleList([Up(fm, fm, k=k, same=same) for i in range(depth)])\n        \n        self.embed_in = Conv(in_ch, fm, k=7, same=same, groups=2)\n        self.embed_out = Conv(fm, out_ch, same=same, f_out=lambda x: x)\n        self.f = nn.ReLU()\n\n    def forward(self, x):\n        out = self.embed_in(x)\n        downPass = [out]\n        for down in self.downModules:\n            downPass.append(down(downPass[-1]))\n        upPass = [downPass[-1]]\n        for idx, up in enumerate(self.upModules):\n            upPass.append(up(upPass[-1], downPass[-(idx+2)]))\n        out = self.embed_out(upPass[-1])\n        return out.permute(0,2,3,1)\n\nCommenting out the nn.Upsample() module and uncommenting the nn.ConvTranspose2d() module in the Up module triggers the issue.\nSystem Info\nPyTorch version: 0.3.0.post4\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\nNvidia driver version: 390.25\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/lib64/libcudnn.so\n/usr/local/cuda-8.0/lib64/libcudnn.so.7\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.5\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3\nVersions of relevant libraries:\n[pip] numpy (1.14.2)\n[pip] torch (0.3.0.post4)\n[pip] torchfile (0.1.0)\n[pip] torchnet (0.0.1)\n[pip] torchvision (0.2.0)\n[conda] Could not collect", "body": "## Problem Description\r\n\r\nWhen training a 2d UNet, memory usage steadily increases each batch until memory is exhausted. The issue seems to be related to the nn.ConvTranspose2d module, which is used in the upsampling part of the UNet to increase feature map resolution. Simply replacing each nn.ConvTranspose2d with a nn.Upsample eliminates the leak.\r\n\r\nMy network is defined as follows:\r\n\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    from torch.autograd import Variable\r\n    \r\n    class Conv(nn.Module):\r\n        def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True, f_out=None, groups=1):\r\n            super(Conv, self).__init__()\r\n            self.f = f\r\n            self.f_out = f_out\r\n            self.conv1 = nn.Conv2d(in_ch, out_ch, k, padding=(k-1)/2 if same else 0, groups=groups)\r\n            self.conv2 = nn.Conv2d(out_ch, out_ch, k, padding=(k-1)/2 if same else 0)\r\n    \r\n        def forward(self, x):\r\n            out = self.f(self.conv1(x))\r\n            out = self.f(self.conv2(out)) if self.f_out is None else self.f_out(self.conv2(out))\r\n            return out\r\n    \r\n    class Down(nn.Module):\r\n        def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True):\r\n            super(Down, self).__init__()\r\n            self.f = f\r\n            self.downsample = nn.MaxPool2d(2)\r\n            self.conv1 = nn.Conv2d(in_ch, in_ch, k, padding=(k-1)/2 if same else 0)\r\n            self.conv2 = nn.Conv2d(in_ch, out_ch, k, padding=(k-1)/2 if same else 0)\r\n    \r\n        def forward(self, x):\r\n            out = self.f(self.downsample(x))\r\n            out = self.f(self.conv1(out))\r\n            return self.f(self.conv2(out))\r\n    \r\n    \r\n    class Up(nn.Module):\r\n        def __init__(self, in_ch, out_ch, f=nn.ReLU(inplace=True), k=3, same=True, cat=True):\r\n            super(Up, self).__init__()\r\n            self.f = f\r\n            #self.upsample = nn.ConvTranspose2d(in_ch, in_ch, k, stride=2, padding=(k-1)/2 if same else 0, output_padding=1)                                                                                                                                                                                                                                                  \r\n            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\r\n            self.conv1 = nn.Conv2d(in_ch + out_ch if cat else in_ch, out_ch, k, padding=(k-1)/2 if same else 0)\r\n            self.conv2 = nn.Conv2d(out_ch, out_ch, k, padding=(k-1)/2 if same else 0)\r\n    \r\n        def forward(self, x, y=None):\r\n            out = self.f(self.upsample(x))\r\n            if y is not None:\r\n                out = torch.cat((out, y),1)\r\n            out = self.f(self.conv1(out))\r\n            return self.f(self.conv2(out))\r\n    \r\n    class UNet(nn.Module):\r\n        def __init__(self, in_ch=2, out_ch=2, depth=4, fm=24, k=3, same=True):\r\n            super(UNet, self).__init__()\r\n            \r\n            print('--------- Building UNet with depth ' + str(depth) + ' and kernel size ' + str(k) + ' ---------')\r\n            \r\n            self.downModules = nn.ModuleList([Down(fm, fm, k=k, same=same) for i in range(depth)])\r\n            self.upModules = nn.ModuleList([Up(fm, fm, k=k, same=same) for i in range(depth)])\r\n            \r\n            self.embed_in = Conv(in_ch, fm, k=7, same=same, groups=2)\r\n            self.embed_out = Conv(fm, out_ch, same=same, f_out=lambda x: x)\r\n            self.f = nn.ReLU()\r\n    \r\n        def forward(self, x):\r\n            out = self.embed_in(x)\r\n            downPass = [out]\r\n            for down in self.downModules:\r\n                downPass.append(down(downPass[-1]))\r\n            upPass = [downPass[-1]]\r\n            for idx, up in enumerate(self.upModules):\r\n                upPass.append(up(upPass[-1], downPass[-(idx+2)]))\r\n            out = self.embed_out(upPass[-1])\r\n            return out.permute(0,2,3,1)\r\n\r\nCommenting out the nn.Upsample() module and uncommenting the nn.ConvTranspose2d() module in the Up module triggers the issue.\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.3.0.post4\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.25\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.2)\r\n[pip] torch (0.3.0.post4)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchnet (0.0.1)\r\n[pip] torchvision (0.2.0)\r\n[conda] Could not collect\r\n\r\n"}