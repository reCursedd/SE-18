{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413324554", "html_url": "https://github.com/pytorch/pytorch/issues/10534#issuecomment-413324554", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10534", "id": 413324554, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzMyNDU1NA==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-15T20:26:07Z", "updated_at": "2018-08-15T20:26:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5043628\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/DC-Swind\">@DC-Swind</a> , I believe you need to register the w1 in the Model class as self.w1, in that way it's synced no matter if it's used or not. The way you did in the code example didn't let pytorch know it's a Parameter associated with the model. See the script I used below as a quick demo. I ran it on a two GPU machine with <code>rank=0</code> and <code>rank =1</code>.</p>\n<pre><code>#! /usr/bin/env python\n#################################################################################\n#     File Name           :     re.py\n#     Created By          :     Ailing Zhang\n#     Creation Date       :     [2018-06-10 21:22]\n#     Last Modified       :     [2018-07-11 16:23]\n#     Description         :\n#################################################################################\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\nimport torch.nn as nn\nimport sys\nimport pdb\n\nclass model(nn.Module):\n    def __init__(self):\n        super(model, self).__init__()\n        self.w1 = nn.Parameter(torch.randn(1))\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(3, 2)\n\n    def forward(self, x):\n        # x = self.fc1(x) * self.w1\n        x = self.fc1(x)\n        return x\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    print(\"Started\")\n    net = model().cuda(device=rank)\n    pdb.set_trace()\n    input = torch.randn(1, 10).cuda(device=rank)\n    dist_net = torch.nn.parallel.DistributedDataParallel(net,\n                                                        device_ids=list([rank]),\n                                                     output_device=rank)\n    pdb.set_trace()\n    output = dist_net(input)\n    print(output)\n    pass\n\ndef init_processes(rank, size, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    if len(sys.argv) != 2:\n        print(\"help: python x.py 1, where 1 is rank in DDP\")\n        exit()\n    rank = int(sys.argv[1])\n    init_processes(rank, size)\n    run(rank, size)\n</code></pre>\n<p>Closing this for now, please reopen if you have more questions.</p>", "body_text": "Hi @DC-Swind , I believe you need to register the w1 in the Model class as self.w1, in that way it's synced no matter if it's used or not. The way you did in the code example didn't let pytorch know it's a Parameter associated with the model. See the script I used below as a quick demo. I ran it on a two GPU machine with rank=0 and rank =1.\n#! /usr/bin/env python\n#################################################################################\n#     File Name           :     re.py\n#     Created By          :     Ailing Zhang\n#     Creation Date       :     [2018-06-10 21:22]\n#     Last Modified       :     [2018-07-11 16:23]\n#     Description         :\n#################################################################################\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\nimport torch.nn as nn\nimport sys\nimport pdb\n\nclass model(nn.Module):\n    def __init__(self):\n        super(model, self).__init__()\n        self.w1 = nn.Parameter(torch.randn(1))\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(3, 2)\n\n    def forward(self, x):\n        # x = self.fc1(x) * self.w1\n        x = self.fc1(x)\n        return x\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    print(\"Started\")\n    net = model().cuda(device=rank)\n    pdb.set_trace()\n    input = torch.randn(1, 10).cuda(device=rank)\n    dist_net = torch.nn.parallel.DistributedDataParallel(net,\n                                                        device_ids=list([rank]),\n                                                     output_device=rank)\n    pdb.set_trace()\n    output = dist_net(input)\n    print(output)\n    pass\n\ndef init_processes(rank, size, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    if len(sys.argv) != 2:\n        print(\"help: python x.py 1, where 1 is rank in DDP\")\n        exit()\n    rank = int(sys.argv[1])\n    init_processes(rank, size)\n    run(rank, size)\n\nClosing this for now, please reopen if you have more questions.", "body": "Hi @DC-Swind , I believe you need to register the w1 in the Model class as self.w1, in that way it's synced no matter if it's used or not. The way you did in the code example didn't let pytorch know it's a Parameter associated with the model. See the script I used below as a quick demo. I ran it on a two GPU machine with `rank=0` and `rank =1`. \r\n```\r\n#! /usr/bin/env python\r\n#################################################################################\r\n#     File Name           :     re.py\r\n#     Created By          :     Ailing Zhang\r\n#     Creation Date       :     [2018-06-10 21:22]\r\n#     Last Modified       :     [2018-07-11 16:23]\r\n#     Description         :\r\n#################################################################################\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nfrom torch.multiprocessing import Process\r\nimport torch.nn as nn\r\nimport sys\r\nimport pdb\r\n\r\nclass model(nn.Module):\r\n    def __init__(self):\r\n        super(model, self).__init__()\r\n        self.w1 = nn.Parameter(torch.randn(1))\r\n        self.fc1 = nn.Linear(10, 5)\r\n        self.fc2 = nn.Linear(3, 2)\r\n\r\n    def forward(self, x):\r\n        # x = self.fc1(x) * self.w1\r\n        x = self.fc1(x)\r\n        return x\r\n\r\ndef run(rank, size):\r\n    \"\"\" Distributed function to be implemented later. \"\"\"\r\n    print(\"Started\")\r\n    net = model().cuda(device=rank)\r\n    pdb.set_trace()\r\n    input = torch.randn(1, 10).cuda(device=rank)\r\n    dist_net = torch.nn.parallel.DistributedDataParallel(net,\r\n                                                        device_ids=list([rank]),\r\n                                                     output_device=rank)\r\n    pdb.set_trace()\r\n    output = dist_net(input)\r\n    print(output)\r\n    pass\r\n\r\ndef init_processes(rank, size, backend='gloo'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    if len(sys.argv) != 2:\r\n        print(\"help: python x.py 1, where 1 is rank in DDP\")\r\n        exit()\r\n    rank = int(sys.argv[1])\r\n    init_processes(rank, size)\r\n    run(rank, size)\r\n```\r\nClosing this for now, please reopen if you have more questions. "}