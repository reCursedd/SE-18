{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10268", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10268/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10268/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10268/events", "html_url": "https://github.com/pytorch/pytorch/pull/10268", "id": 348035294, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA2NDgxMjM2", "number": 10268, "title": "Increase TCP listen queue size from 64 to 1024", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-06T18:33:08Z", "updated_at": "2018-08-07T15:27:25Z", "closed_at": "2018-08-07T15:27:25Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10268", "html_url": "https://github.com/pytorch/pytorch/pull/10268", "diff_url": "https://github.com/pytorch/pytorch/pull/10268.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10268.patch"}, "body_html": "<p>Summary:<br>\nRunning torch.distributed.init_process_group fails with more than ~64 processes, with various errors like connection refused or connection reset by peer. After some digging, it looks like the root cause is that all workers have to connect to master via TCP (both in Zeus init and in DataChannelTCP - look for <code>connect()</code>), and the listening socket only has a backlog of 64.</p>\n<p>I increased the backlog to 1024, that seems like enough for reasonable purposes (the hard limit is 65535 in /proc/sys/net/core/somaxconn). There's probably a more correct way to do this that involves retries when connection is refused.</p>\n<p>Differential Revision: D9182216</p>", "body_text": "Summary:\nRunning torch.distributed.init_process_group fails with more than ~64 processes, with various errors like connection refused or connection reset by peer. After some digging, it looks like the root cause is that all workers have to connect to master via TCP (both in Zeus init and in DataChannelTCP - look for connect()), and the listening socket only has a backlog of 64.\nI increased the backlog to 1024, that seems like enough for reasonable purposes (the hard limit is 65535 in /proc/sys/net/core/somaxconn). There's probably a more correct way to do this that involves retries when connection is refused.\nDifferential Revision: D9182216", "body": "Summary:\nRunning torch.distributed.init_process_group fails with more than ~64 processes, with various errors like connection refused or connection reset by peer. After some digging, it looks like the root cause is that all workers have to connect to master via TCP (both in Zeus init and in DataChannelTCP - look for `connect()`), and the listening socket only has a backlog of 64.\n\nI increased the backlog to 1024, that seems like enough for reasonable purposes (the hard limit is 65535 in /proc/sys/net/core/somaxconn). There's probably a more correct way to do this that involves retries when connection is refused.\n\nDifferential Revision: D9182216\n"}