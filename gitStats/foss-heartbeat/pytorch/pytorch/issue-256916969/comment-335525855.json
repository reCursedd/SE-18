{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/335525855", "html_url": "https://github.com/pytorch/pytorch/issues/2702#issuecomment-335525855", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2702", "id": 335525855, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTUyNTg1NQ==", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-10T16:12:22Z", "updated_at": "2017-10-10T16:12:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, thanks for looking into this.</p>\n<p>FYI, I looked at the code a bit, and I think there are several places where the code calls <code>cudaMemcpy()</code>, which always executes on the default stream, even when we might have a non-default stream.  I played around a bit and the following seems to work:</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-c1\">diff --git a/torch/lib/THC/THCReduceAll.cuh b/torch/lib/THC/THCReduceAll.cuh</span>\nindex a6e1e85..5c57c24 100644\n<span class=\"pl-md\">--- a/torch/lib/THC/THCReduceAll.cuh</span>\n<span class=\"pl-mi1\">+++ b/torch/lib/THC/THCReduceAll.cuh</span>\n<span class=\"pl-mdr\">@@ -331,7 +331,10 @@</span> bool THC_reduceAll(THCState* state,\n   // If our destination is not on the device, copy the value back to\n   // the host (synchronous!)\n   if (!outOnDevice) {\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    THCudaCheck(cudaMemcpy(out, devOut, sizeof(AccT), cudaMemcpyDeviceToHost));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    cudaStream_t stream = THCState_getCurrentStream(state);</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    THCudaCheck(cudaMemcpyAsync(out, devOut, sizeof(AccT),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                cudaMemcpyDeviceToHost, stream));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    THCudaCheck(cudaStreamSynchronize(stream));</span>\n   }\n \n   if (freeDevOut) {\n<span class=\"pl-c1\">diff --git a/torch/lib/THC/generic/THCStorage.c b/torch/lib/THC/generic/THCStorage.c</span>\nindex 8a6cede..beb072a 100644\n<span class=\"pl-md\">--- a/torch/lib/THC/generic/THCStorage.c</span>\n<span class=\"pl-mi1\">+++ b/torch/lib/THC/generic/THCStorage.c</span>\n<span class=\"pl-mdr\">@@ -28,8 +28,12 @@</span> real THCStorage_(get)(THCState *state, const THCStorage *self, ptrdiff_t index)\n {\n   THArgCheck((index &gt;= 0) &amp;&amp; (index &lt; self-&gt;size), 2, \"index out of bounds\");\n   real value;\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  THCudaCheck(cudaMemcpy(&amp;value, self-&gt;data + index, sizeof(real),</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                         cudaMemcpyDeviceToHost));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span></span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  cudaStream_t stream = THCState_getCurrentStream(state);</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  THCudaCheck(cudaMemcpyAsync(&amp;value, self-&gt;data + index, sizeof(real),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                              cudaMemcpyDeviceToHost, stream));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  THCudaCheck(cudaStreamSynchronize(stream));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span></span>\n   return value;\n }\n \n<span class=\"pl-c1\">diff --git a/torch/lib/THC/generic/THCTensorCopy.c b/torch/lib/THC/generic/THCTensorCopy.c</span>\nindex b5122b8..b23e013 100644\n<span class=\"pl-md\">--- a/torch/lib/THC/generic/THCTensorCopy.c</span>\n<span class=\"pl-mi1\">+++ b/torch/lib/THC/generic/THCTensorCopy.c</span>\n<span class=\"pl-mdr\">@@ -59,10 +59,13 @@</span> void THTensor_(copyCuda)(THCState *state, THTensor *self, struct THCTensor *src)\n     THTensor *selfc = THTensor_(newContiguous)(self);\n     src = THCTensor_(newContiguous)(state, src);\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    THCudaCheck(cudaMemcpy(THTensor_(data)(selfc),</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                           THCTensor_(data)(state, src),</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                           THCTensor_(nElement)(state, src) * sizeof(real),</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                           cudaMemcpyDeviceToHost));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    cudaStream_t stream = THCState_getCurrentStream(state);</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    THCudaCheck(cudaMemcpyAsync(THTensor_(data)(selfc),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                THCTensor_(data)(state, src),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                THCTensor_(nElement)(state, src) * sizeof(real),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                cudaMemcpyDeviceToHost,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                stream));</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    THCudaCheck(cudaStreamSynchronize(stream));</span>\n \n     THCTensor_(free)(state, src);\n     THTensor_(freeCopyTo)(selfc, self);</pre></div>\n<p>Each fix applies to: reduce operations (such as <code>sum()</code>), picking up an element (<code>A[0, 0]</code>), and <code>copy_()</code>.  However, there are more places in the code that uses <code>cudaMemcpy()</code> and I'm not sure I understand the rest of them.</p>", "body_text": "Hi @zou3519, thanks for looking into this.\nFYI, I looked at the code a bit, and I think there are several places where the code calls cudaMemcpy(), which always executes on the default stream, even when we might have a non-default stream.  I played around a bit and the following seems to work:\ndiff --git a/torch/lib/THC/THCReduceAll.cuh b/torch/lib/THC/THCReduceAll.cuh\nindex a6e1e85..5c57c24 100644\n--- a/torch/lib/THC/THCReduceAll.cuh\n+++ b/torch/lib/THC/THCReduceAll.cuh\n@@ -331,7 +331,10 @@ bool THC_reduceAll(THCState* state,\n   // If our destination is not on the device, copy the value back to\n   // the host (synchronous!)\n   if (!outOnDevice) {\n-    THCudaCheck(cudaMemcpy(out, devOut, sizeof(AccT), cudaMemcpyDeviceToHost));\n+    cudaStream_t stream = THCState_getCurrentStream(state);\n+    THCudaCheck(cudaMemcpyAsync(out, devOut, sizeof(AccT),\n+                                cudaMemcpyDeviceToHost, stream));\n+    THCudaCheck(cudaStreamSynchronize(stream));\n   }\n \n   if (freeDevOut) {\ndiff --git a/torch/lib/THC/generic/THCStorage.c b/torch/lib/THC/generic/THCStorage.c\nindex 8a6cede..beb072a 100644\n--- a/torch/lib/THC/generic/THCStorage.c\n+++ b/torch/lib/THC/generic/THCStorage.c\n@@ -28,8 +28,12 @@ real THCStorage_(get)(THCState *state, const THCStorage *self, ptrdiff_t index)\n {\n   THArgCheck((index >= 0) && (index < self->size), 2, \"index out of bounds\");\n   real value;\n-  THCudaCheck(cudaMemcpy(&value, self->data + index, sizeof(real),\n-                         cudaMemcpyDeviceToHost));\n+\n+  cudaStream_t stream = THCState_getCurrentStream(state);\n+  THCudaCheck(cudaMemcpyAsync(&value, self->data + index, sizeof(real),\n+                              cudaMemcpyDeviceToHost, stream));\n+  THCudaCheck(cudaStreamSynchronize(stream));\n+\n   return value;\n }\n \ndiff --git a/torch/lib/THC/generic/THCTensorCopy.c b/torch/lib/THC/generic/THCTensorCopy.c\nindex b5122b8..b23e013 100644\n--- a/torch/lib/THC/generic/THCTensorCopy.c\n+++ b/torch/lib/THC/generic/THCTensorCopy.c\n@@ -59,10 +59,13 @@ void THTensor_(copyCuda)(THCState *state, THTensor *self, struct THCTensor *src)\n     THTensor *selfc = THTensor_(newContiguous)(self);\n     src = THCTensor_(newContiguous)(state, src);\n \n-    THCudaCheck(cudaMemcpy(THTensor_(data)(selfc),\n-                           THCTensor_(data)(state, src),\n-                           THCTensor_(nElement)(state, src) * sizeof(real),\n-                           cudaMemcpyDeviceToHost));\n+    cudaStream_t stream = THCState_getCurrentStream(state);\n+    THCudaCheck(cudaMemcpyAsync(THTensor_(data)(selfc),\n+                                THCTensor_(data)(state, src),\n+                                THCTensor_(nElement)(state, src) * sizeof(real),\n+                                cudaMemcpyDeviceToHost,\n+                                stream));\n+    THCudaCheck(cudaStreamSynchronize(stream));\n \n     THCTensor_(free)(state, src);\n     THTensor_(freeCopyTo)(selfc, self);\nEach fix applies to: reduce operations (such as sum()), picking up an element (A[0, 0]), and copy_().  However, there are more places in the code that uses cudaMemcpy() and I'm not sure I understand the rest of them.", "body": "Hi @zou3519, thanks for looking into this.\r\n\r\nFYI, I looked at the code a bit, and I think there are several places where the code calls `cudaMemcpy()`, which always executes on the default stream, even when we might have a non-default stream.  I played around a bit and the following seems to work:\r\n\r\n```diff\r\ndiff --git a/torch/lib/THC/THCReduceAll.cuh b/torch/lib/THC/THCReduceAll.cuh\r\nindex a6e1e85..5c57c24 100644\r\n--- a/torch/lib/THC/THCReduceAll.cuh\r\n+++ b/torch/lib/THC/THCReduceAll.cuh\r\n@@ -331,7 +331,10 @@ bool THC_reduceAll(THCState* state,\r\n   // If our destination is not on the device, copy the value back to\r\n   // the host (synchronous!)\r\n   if (!outOnDevice) {\r\n-    THCudaCheck(cudaMemcpy(out, devOut, sizeof(AccT), cudaMemcpyDeviceToHost));\r\n+    cudaStream_t stream = THCState_getCurrentStream(state);\r\n+    THCudaCheck(cudaMemcpyAsync(out, devOut, sizeof(AccT),\r\n+                                cudaMemcpyDeviceToHost, stream));\r\n+    THCudaCheck(cudaStreamSynchronize(stream));\r\n   }\r\n \r\n   if (freeDevOut) {\r\ndiff --git a/torch/lib/THC/generic/THCStorage.c b/torch/lib/THC/generic/THCStorage.c\r\nindex 8a6cede..beb072a 100644\r\n--- a/torch/lib/THC/generic/THCStorage.c\r\n+++ b/torch/lib/THC/generic/THCStorage.c\r\n@@ -28,8 +28,12 @@ real THCStorage_(get)(THCState *state, const THCStorage *self, ptrdiff_t index)\r\n {\r\n   THArgCheck((index >= 0) && (index < self->size), 2, \"index out of bounds\");\r\n   real value;\r\n-  THCudaCheck(cudaMemcpy(&value, self->data + index, sizeof(real),\r\n-                         cudaMemcpyDeviceToHost));\r\n+\r\n+  cudaStream_t stream = THCState_getCurrentStream(state);\r\n+  THCudaCheck(cudaMemcpyAsync(&value, self->data + index, sizeof(real),\r\n+                              cudaMemcpyDeviceToHost, stream));\r\n+  THCudaCheck(cudaStreamSynchronize(stream));\r\n+\r\n   return value;\r\n }\r\n \r\ndiff --git a/torch/lib/THC/generic/THCTensorCopy.c b/torch/lib/THC/generic/THCTensorCopy.c\r\nindex b5122b8..b23e013 100644\r\n--- a/torch/lib/THC/generic/THCTensorCopy.c\r\n+++ b/torch/lib/THC/generic/THCTensorCopy.c\r\n@@ -59,10 +59,13 @@ void THTensor_(copyCuda)(THCState *state, THTensor *self, struct THCTensor *src)\r\n     THTensor *selfc = THTensor_(newContiguous)(self);\r\n     src = THCTensor_(newContiguous)(state, src);\r\n \r\n-    THCudaCheck(cudaMemcpy(THTensor_(data)(selfc),\r\n-                           THCTensor_(data)(state, src),\r\n-                           THCTensor_(nElement)(state, src) * sizeof(real),\r\n-                           cudaMemcpyDeviceToHost));\r\n+    cudaStream_t stream = THCState_getCurrentStream(state);\r\n+    THCudaCheck(cudaMemcpyAsync(THTensor_(data)(selfc),\r\n+                                THCTensor_(data)(state, src),\r\n+                                THCTensor_(nElement)(state, src) * sizeof(real),\r\n+                                cudaMemcpyDeviceToHost,\r\n+                                stream));\r\n+    THCudaCheck(cudaStreamSynchronize(stream));\r\n \r\n     THCTensor_(free)(state, src);\r\n     THTensor_(freeCopyTo)(selfc, self);\r\n```\r\n\r\nEach fix applies to: reduce operations (such as `sum()`), picking up an element (`A[0, 0]`), and `copy_()`.  However, there are more places in the code that uses `cudaMemcpy()` and I'm not sure I understand the rest of them."}