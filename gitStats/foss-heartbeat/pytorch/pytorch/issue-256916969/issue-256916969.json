{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2702", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2702/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2702/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2702/events", "html_url": "https://github.com/pytorch/pytorch/issues/2702", "id": 256916969, "node_id": "MDU6SXNzdWUyNTY5MTY5Njk=", "number": 2702, "title": "The current CUDA stream is not honored by some operations", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-09-12T04:49:56Z", "updated_at": "2018-03-09T02:41:49Z", "closed_at": "2017-10-11T13:49:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, I think something is wrong with handling CUDA streams.  Could someone take a look?</p>\n<p>(I'm using Python 2.7.12, CUDA 8.0, and PyTorch version 0.2.0.post1)</p>\n<pre><code>#!/usr/bin/python\n# PyTorch CUDA stream test.\n\nimport torch\n\nDIM = 2048\nM = torch.cuda.FloatTensor(DIM, DIM)\n\ndef do_test(M, use_stream, fn):\n    M.fill_(0.1)\n    print 'Initial value = %.4f' % fn()\n\n    # Create a new stream if requested.\n    stream = torch.cuda.Stream() if use_stream else None\n    with torch.cuda.stream(stream):\n        for step in range(5):\n            # Do random calculation.\n            K = torch.matmul(M, M) / DIM\n            M -= K\n            print 'After step %d = %.4f' % (step, fn())\n\nfn = M.norm   # or 'M.sum' or 'lambda: M[0, 0]'\n\nprint '=== Without using a new stream ==='\ndo_test(M, False, fn)\nprint '=== With using a new stream ==='\ndo_test(M, True, fn)\n</code></pre>\n<p>Result on my machine (GTX 1080).  The second half keeps changing:</p>\n<pre><code>=== Without using a new stream ===\nInitial value = 204.8000\nAfter step 0 = 184.3197\nAfter step 1 = 167.7312\nAfter step 2 = 153.9943\nAfter step 3 = 142.4149\nAfter step 4 = 132.5118\n=== With using a new stream ===\nInitial value = 204.8000\nAfter step 0 = 3.1469\nAfter step 1 = 3.1469\nAfter step 2 = 3.1469\nAfter step 3 = 3.1469\nAfter step 4 = 3.1469\n</code></pre>\n<p>Basically, when I'm using a different stream, computing any single float value returns random results.  Running nvprof shows that when the second do_test() runs, all the calculations happen in the new stream (13 below), but copying the result back to CPU happens in the default stream (7 below), and at the wrong moments before the correct value is ready.</p>\n<pre><code>$ nvprof --print-gpu-trace ./cuda-stream-test.py\n\n   Start  Duration  Size  Stream  Name\n... (only showing the second half)\n2.28038s     320ns    4B       7  [CUDA memcpy DtoH]\n2.28086s  2.0956ms     -      13  sgemm_128x128x8_NN_vec [796]\n2.28132s     960ns    4B       7  [CUDA memcpy DtoH]\n2.28183s  1.1200us    4B       7  [CUDA memcpy DtoH]\n2.28194s     960ns    4B       7  [CUDA memcpy DtoH]\n2.28206s  1.0560us    4B       7  [CUDA memcpy DtoH]\n2.28218s  1.1200us    4B       7  [CUDA memcpy DtoH]\n2.28296s  147.40us     -      13  void kernelPointwiseApply2&lt;TensorDivConstantOp&lt;float&gt;, float, float, unsigned int, int=-2, int=-2&gt;(TensorInfo&lt;TensorDivConstantOp&lt;float&gt;, float&gt;, TensorInfo&lt;float, float&gt;, float, float) [814]\n2.28311s  212.52us     -      13  void kernelPointwiseApply2&lt;TensorSubOp&lt;float&gt;, float, float, unsigned int, int=-2, int=-2&gt;(TensorInfo&lt;TensorSubOp&lt;float&gt;, float&gt;, TensorInfo&lt;float, float&gt;, float, float) [827]\n2.28332s  93.890us     -      13  void kernelReduceAllPass1&lt;TensorNormOp&lt;float, int=2&gt;, ReduceAdd&lt;float, float&gt;, ReduceAdd&lt;float, float&gt;, float, float, unsigned int, int=-2&gt;(TensorInfo&lt;float, ReduceAdd&lt;float, float&gt;&gt;, ReduceAdd&lt;float, float&gt;, float, float, int=2, TensorNormOp&lt;float, int=2&gt;, float*) [857]\n2.28341s  2.8160us     -      13  void kernelReduceAllPass2&lt;ReduceAdd&lt;float, float&gt;, float, unsigned int&gt;(int, float, float, float*, float*) [866]\n2.28342s  2.0951ms     -      13  sgemm_128x128x8_NN_vec [892]\n... (4 more calls to these kernels)\n</code></pre>\n<p>Please enlighten me if I'm doing something wrong.  Thanks!</p>", "body_text": "Hi, I think something is wrong with handling CUDA streams.  Could someone take a look?\n(I'm using Python 2.7.12, CUDA 8.0, and PyTorch version 0.2.0.post1)\n#!/usr/bin/python\n# PyTorch CUDA stream test.\n\nimport torch\n\nDIM = 2048\nM = torch.cuda.FloatTensor(DIM, DIM)\n\ndef do_test(M, use_stream, fn):\n    M.fill_(0.1)\n    print 'Initial value = %.4f' % fn()\n\n    # Create a new stream if requested.\n    stream = torch.cuda.Stream() if use_stream else None\n    with torch.cuda.stream(stream):\n        for step in range(5):\n            # Do random calculation.\n            K = torch.matmul(M, M) / DIM\n            M -= K\n            print 'After step %d = %.4f' % (step, fn())\n\nfn = M.norm   # or 'M.sum' or 'lambda: M[0, 0]'\n\nprint '=== Without using a new stream ==='\ndo_test(M, False, fn)\nprint '=== With using a new stream ==='\ndo_test(M, True, fn)\n\nResult on my machine (GTX 1080).  The second half keeps changing:\n=== Without using a new stream ===\nInitial value = 204.8000\nAfter step 0 = 184.3197\nAfter step 1 = 167.7312\nAfter step 2 = 153.9943\nAfter step 3 = 142.4149\nAfter step 4 = 132.5118\n=== With using a new stream ===\nInitial value = 204.8000\nAfter step 0 = 3.1469\nAfter step 1 = 3.1469\nAfter step 2 = 3.1469\nAfter step 3 = 3.1469\nAfter step 4 = 3.1469\n\nBasically, when I'm using a different stream, computing any single float value returns random results.  Running nvprof shows that when the second do_test() runs, all the calculations happen in the new stream (13 below), but copying the result back to CPU happens in the default stream (7 below), and at the wrong moments before the correct value is ready.\n$ nvprof --print-gpu-trace ./cuda-stream-test.py\n\n   Start  Duration  Size  Stream  Name\n... (only showing the second half)\n2.28038s     320ns    4B       7  [CUDA memcpy DtoH]\n2.28086s  2.0956ms     -      13  sgemm_128x128x8_NN_vec [796]\n2.28132s     960ns    4B       7  [CUDA memcpy DtoH]\n2.28183s  1.1200us    4B       7  [CUDA memcpy DtoH]\n2.28194s     960ns    4B       7  [CUDA memcpy DtoH]\n2.28206s  1.0560us    4B       7  [CUDA memcpy DtoH]\n2.28218s  1.1200us    4B       7  [CUDA memcpy DtoH]\n2.28296s  147.40us     -      13  void kernelPointwiseApply2<TensorDivConstantOp<float>, float, float, unsigned int, int=-2, int=-2>(TensorInfo<TensorDivConstantOp<float>, float>, TensorInfo<float, float>, float, float) [814]\n2.28311s  212.52us     -      13  void kernelPointwiseApply2<TensorSubOp<float>, float, float, unsigned int, int=-2, int=-2>(TensorInfo<TensorSubOp<float>, float>, TensorInfo<float, float>, float, float) [827]\n2.28332s  93.890us     -      13  void kernelReduceAllPass1<TensorNormOp<float, int=2>, ReduceAdd<float, float>, ReduceAdd<float, float>, float, float, unsigned int, int=-2>(TensorInfo<float, ReduceAdd<float, float>>, ReduceAdd<float, float>, float, float, int=2, TensorNormOp<float, int=2>, float*) [857]\n2.28341s  2.8160us     -      13  void kernelReduceAllPass2<ReduceAdd<float, float>, float, unsigned int>(int, float, float, float*, float*) [866]\n2.28342s  2.0951ms     -      13  sgemm_128x128x8_NN_vec [892]\n... (4 more calls to these kernels)\n\nPlease enlighten me if I'm doing something wrong.  Thanks!", "body": "Hi, I think something is wrong with handling CUDA streams.  Could someone take a look?\r\n\r\n(I'm using Python 2.7.12, CUDA 8.0, and PyTorch version 0.2.0.post1)\r\n\r\n    #!/usr/bin/python\r\n    # PyTorch CUDA stream test.\r\n\r\n    import torch\r\n\r\n    DIM = 2048\r\n    M = torch.cuda.FloatTensor(DIM, DIM)\r\n\r\n    def do_test(M, use_stream, fn):\r\n        M.fill_(0.1)\r\n        print 'Initial value = %.4f' % fn()\r\n\r\n        # Create a new stream if requested.\r\n        stream = torch.cuda.Stream() if use_stream else None\r\n        with torch.cuda.stream(stream):\r\n            for step in range(5):\r\n                # Do random calculation.\r\n                K = torch.matmul(M, M) / DIM\r\n                M -= K\r\n                print 'After step %d = %.4f' % (step, fn())\r\n\r\n    fn = M.norm   # or 'M.sum' or 'lambda: M[0, 0]'\r\n\r\n    print '=== Without using a new stream ==='\r\n    do_test(M, False, fn)\r\n    print '=== With using a new stream ==='\r\n    do_test(M, True, fn)\r\n\r\nResult on my machine (GTX 1080).  The second half keeps changing:\r\n\r\n    === Without using a new stream ===\r\n    Initial value = 204.8000\r\n    After step 0 = 184.3197\r\n    After step 1 = 167.7312\r\n    After step 2 = 153.9943\r\n    After step 3 = 142.4149\r\n    After step 4 = 132.5118\r\n    === With using a new stream ===\r\n    Initial value = 204.8000\r\n    After step 0 = 3.1469\r\n    After step 1 = 3.1469\r\n    After step 2 = 3.1469\r\n    After step 3 = 3.1469\r\n    After step 4 = 3.1469\r\n\r\nBasically, when I'm using a different stream, computing any single float value returns random results.  Running nvprof shows that when the second do_test() runs, all the calculations happen in the new stream (13 below), but copying the result back to CPU happens in the default stream (7 below), and at the wrong moments before the correct value is ready.\r\n\r\n    $ nvprof --print-gpu-trace ./cuda-stream-test.py\r\n\r\n       Start  Duration  Size  Stream  Name\r\n    ... (only showing the second half)\r\n    2.28038s     320ns    4B       7  [CUDA memcpy DtoH]\r\n    2.28086s  2.0956ms     -      13  sgemm_128x128x8_NN_vec [796]\r\n    2.28132s     960ns    4B       7  [CUDA memcpy DtoH]\r\n    2.28183s  1.1200us    4B       7  [CUDA memcpy DtoH]\r\n    2.28194s     960ns    4B       7  [CUDA memcpy DtoH]\r\n    2.28206s  1.0560us    4B       7  [CUDA memcpy DtoH]\r\n    2.28218s  1.1200us    4B       7  [CUDA memcpy DtoH]\r\n    2.28296s  147.40us     -      13  void kernelPointwiseApply2<TensorDivConstantOp<float>, float, float, unsigned int, int=-2, int=-2>(TensorInfo<TensorDivConstantOp<float>, float>, TensorInfo<float, float>, float, float) [814]\r\n    2.28311s  212.52us     -      13  void kernelPointwiseApply2<TensorSubOp<float>, float, float, unsigned int, int=-2, int=-2>(TensorInfo<TensorSubOp<float>, float>, TensorInfo<float, float>, float, float) [827]\r\n    2.28332s  93.890us     -      13  void kernelReduceAllPass1<TensorNormOp<float, int=2>, ReduceAdd<float, float>, ReduceAdd<float, float>, float, float, unsigned int, int=-2>(TensorInfo<float, ReduceAdd<float, float>>, ReduceAdd<float, float>, float, float, int=2, TensorNormOp<float, int=2>, float*) [857]\r\n    2.28341s  2.8160us     -      13  void kernelReduceAllPass2<ReduceAdd<float, float>, float, unsigned int>(int, float, float, float*, float*) [866]\r\n    2.28342s  2.0951ms     -      13  sgemm_128x128x8_NN_vec [892]\r\n    ... (4 more calls to these kernels)\r\n\r\nPlease enlighten me if I'm doing something wrong.  Thanks!"}