{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3233", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3233/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3233/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3233/events", "html_url": "https://github.com/pytorch/pytorch/issues/3233", "id": 267661218, "node_id": "MDU6SXNzdWUyNjc2NjEyMTg=", "number": 3233, "title": "nn.Embedding padding idx doesn't check for negative value", "user": {"login": "cedias", "id": 1705336, "node_id": "MDQ6VXNlcjE3MDUzMzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1705336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cedias", "html_url": "https://github.com/cedias", "followers_url": "https://api.github.com/users/cedias/followers", "following_url": "https://api.github.com/users/cedias/following{/other_user}", "gists_url": "https://api.github.com/users/cedias/gists{/gist_id}", "starred_url": "https://api.github.com/users/cedias/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cedias/subscriptions", "organizations_url": "https://api.github.com/users/cedias/orgs", "repos_url": "https://api.github.com/users/cedias/repos", "events_url": "https://api.github.com/users/cedias/events{/privacy}", "received_events_url": "https://api.github.com/users/cedias/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-10-23T13:08:38Z", "updated_at": "2018-01-09T11:04:12Z", "closed_at": "2018-01-09T11:04:12Z", "author_association": "NONE", "body_html": "<p>nn.Embedding doesn't check for negative padding index value which might be able to lead to subtle bugs if someone wants to use it with inverted indexing (even though it's not specified that you can).</p>\n<pre><code>def forward(self, input):\n        padding_idx = self.padding_idx\n        if padding_idx is None:\n            padding_idx = -1\n        return self._backend.Embedding.apply(\n            input, self.weight,\n            padding_idx, self.max_norm, self.norm_type,\n            self.scale_grad_by_freq, self.sparse\n        )\n\n</code></pre>\n<p>To be clear: defining an embedding layer <code>emb = nn.Embedding(5,10,padding_idx=-1)</code> is possible and raises no error. Moreover the last weights are initialized to 0 as you would expect if negative indexing on embedding was working. This can be confusing.</p>\n<pre><code>In [44]: emb(Variable(torch.LongTensor([4])))\nOut[44]: \nVariable containing:\n    0     0     0     0     0     0     0     0     0     0\n[torch.FloatTensor of size 1x10]\n</code></pre>\n<p>I believe this can be fixed in two ways:</p>\n<ul>\n<li>asserting <code>padding_idx &gt; 0 or None</code></li>\n<li>if <code>padding_idx &lt; 0</code> then setting <code>padding_idx = num_embedding + padding_idx</code></li>\n</ul>", "body_text": "nn.Embedding doesn't check for negative padding index value which might be able to lead to subtle bugs if someone wants to use it with inverted indexing (even though it's not specified that you can).\ndef forward(self, input):\n        padding_idx = self.padding_idx\n        if padding_idx is None:\n            padding_idx = -1\n        return self._backend.Embedding.apply(\n            input, self.weight,\n            padding_idx, self.max_norm, self.norm_type,\n            self.scale_grad_by_freq, self.sparse\n        )\n\n\nTo be clear: defining an embedding layer emb = nn.Embedding(5,10,padding_idx=-1) is possible and raises no error. Moreover the last weights are initialized to 0 as you would expect if negative indexing on embedding was working. This can be confusing.\nIn [44]: emb(Variable(torch.LongTensor([4])))\nOut[44]: \nVariable containing:\n    0     0     0     0     0     0     0     0     0     0\n[torch.FloatTensor of size 1x10]\n\nI believe this can be fixed in two ways:\n\nasserting padding_idx > 0 or None\nif padding_idx < 0 then setting padding_idx = num_embedding + padding_idx", "body": "nn.Embedding doesn't check for negative padding index value which might be able to lead to subtle bugs if someone wants to use it with inverted indexing (even though it's not specified that you can).\r\n\r\n```\r\ndef forward(self, input):\r\n        padding_idx = self.padding_idx\r\n        if padding_idx is None:\r\n            padding_idx = -1\r\n        return self._backend.Embedding.apply(\r\n            input, self.weight,\r\n            padding_idx, self.max_norm, self.norm_type,\r\n            self.scale_grad_by_freq, self.sparse\r\n        )\r\n\r\n```\r\n\r\nTo be clear: defining an embedding layer `emb = nn.Embedding(5,10,padding_idx=-1)` is possible and raises no error. Moreover the last weights are initialized to 0 as you would expect if negative indexing on embedding was working. This can be confusing. \r\n\r\n```\r\nIn [44]: emb(Variable(torch.LongTensor([4])))\r\nOut[44]: \r\nVariable containing:\r\n    0     0     0     0     0     0     0     0     0     0\r\n[torch.FloatTensor of size 1x10]\r\n```\r\n\r\nI believe this can be fixed in two ways:\r\n\r\n- asserting `padding_idx > 0 or None`\r\n- if `padding_idx < 0` then setting `padding_idx = num_embedding + padding_idx`\r\n"}