{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340852141", "html_url": "https://github.com/pytorch/pytorch/pull/2903#issuecomment-340852141", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2903", "id": 340852141, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDg1MjE0MQ==", "user": {"login": "xqding", "id": 20047329, "node_id": "MDQ6VXNlcjIwMDQ3MzI5", "avatar_url": "https://avatars1.githubusercontent.com/u/20047329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xqding", "html_url": "https://github.com/xqding", "followers_url": "https://api.github.com/users/xqding/followers", "following_url": "https://api.github.com/users/xqding/following{/other_user}", "gists_url": "https://api.github.com/users/xqding/gists{/gist_id}", "starred_url": "https://api.github.com/users/xqding/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xqding/subscriptions", "organizations_url": "https://api.github.com/users/xqding/orgs", "repos_url": "https://api.github.com/users/xqding/repos", "events_url": "https://api.github.com/users/xqding/events{/privacy}", "received_events_url": "https://api.github.com/users/xqding/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-31T18:07:43Z", "updated_at": "2017-10-31T18:08:03Z", "author_association": "NONE", "body_html": "<p>Here is a summary of what I have tried:</p>\n<ol>\n<li>Using DistributedDataParrallel gives me the following error:</li>\n</ol>\n<pre><code>Traceback (most recent call last):\nFile \"./script/train_dist_gloo.py\", line 99, in &lt;module&gt;\noutputs = net(inputs)\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 263, in __call__\nresult = self.forward(*input, **kwargs)\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/distributed.py\", line 156, in forward\nself._sync_params()\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/distributed.py\", line 187, in _sync_params\ndist.broadcast(flat_buffers, 0)\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 198, in broadcast\nreturn torch._C._dist_broadcast(tensor, src, group)\n</code></pre>\n<ol start=\"2\">\n<li>In stead of using DistributedDataParrallel, I tried to train independent model on each node and average the gradient using the following code:</li>\n</ol>\n<pre><code>for epoch in range(num_epoches):  # loop over the dataset multiple times\n    running_loss = 0.0\n    print(\"Epoch: {}\".format(epoch))\n    train_sampler.set_epoch(epoch)\n    for i, data in enumerate(train_loader, 0):\n        # gettheinputs\n        inputs = data['image']\n        labels = data['category_id']\n        labels = np.array([category_ids.index(l) for l in labels])\n        print(\"i: {}\".format(i))\n\n        print(\"labels\", labels)\n        # wrap them in Variable\n        inputs = inputs.cuda(async=True)\n        labels = torch.from_numpy(labels).cuda(async=True)\n\tinputs, labels= Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss=criterion(outputs,labels)\n\tloss.backward()\n\n        size = float(dist.get_world_size())\n        for param in net.parameters():\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n            param.grad.data /= size\n\toptimizer.step()\n</code></pre>\n<p>It works fine for the first epoch. It crashes once it starts the second epoch with the follwing error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"./script/train_dist_new.py\", line 126, in &lt;module&gt;\n    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n  File \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 216, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: [/home/yuyou/downloads/mypytorch2/pytorch/torch/lib/gloo/gloo/transport/ibverbs/buffer.cc:108] Read timeout LID: 45 QPN: 29770 PSN: 11590737\n</code></pre>\n<ol start=\"3\">\n<li>My workaround now is to combine multiple epoch of data into one epoch when I define the Dataset. Basically, looping over the dataloader once is same as looping over my train data multiple times. It works pretty well so far.</li>\n</ol>", "body_text": "Here is a summary of what I have tried:\n\nUsing DistributedDataParrallel gives me the following error:\n\nTraceback (most recent call last):\nFile \"./script/train_dist_gloo.py\", line 99, in <module>\noutputs = net(inputs)\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 263, in __call__\nresult = self.forward(*input, **kwargs)\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/distributed.py\", line 156, in forward\nself._sync_params()\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/distributed.py\", line 187, in _sync_params\ndist.broadcast(flat_buffers, 0)\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 198, in broadcast\nreturn torch._C._dist_broadcast(tensor, src, group)\n\n\nIn stead of using DistributedDataParrallel, I tried to train independent model on each node and average the gradient using the following code:\n\nfor epoch in range(num_epoches):  # loop over the dataset multiple times\n    running_loss = 0.0\n    print(\"Epoch: {}\".format(epoch))\n    train_sampler.set_epoch(epoch)\n    for i, data in enumerate(train_loader, 0):\n        # gettheinputs\n        inputs = data['image']\n        labels = data['category_id']\n        labels = np.array([category_ids.index(l) for l in labels])\n        print(\"i: {}\".format(i))\n\n        print(\"labels\", labels)\n        # wrap them in Variable\n        inputs = inputs.cuda(async=True)\n        labels = torch.from_numpy(labels).cuda(async=True)\n\tinputs, labels= Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss=criterion(outputs,labels)\n\tloss.backward()\n\n        size = float(dist.get_world_size())\n        for param in net.parameters():\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n            param.grad.data /= size\n\toptimizer.step()\n\nIt works fine for the first epoch. It crashes once it starts the second epoch with the follwing error:\nTraceback (most recent call last):\n  File \"./script/train_dist_new.py\", line 126, in <module>\n    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n  File \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 216, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: [/home/yuyou/downloads/mypytorch2/pytorch/torch/lib/gloo/gloo/transport/ibverbs/buffer.cc:108] Read timeout LID: 45 QPN: 29770 PSN: 11590737\n\n\nMy workaround now is to combine multiple epoch of data into one epoch when I define the Dataset. Basically, looping over the dataloader once is same as looping over my train data multiple times. It works pretty well so far.", "body": "Here is a summary of what I have tried:\r\n1. Using DistributedDataParrallel gives me the following error:\r\n```\r\nTraceback (most recent call last):\r\nFile \"./script/train_dist_gloo.py\", line 99, in <module>\r\noutputs = net(inputs)\r\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 263, in __call__\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/distributed.py\", line 156, in forward\r\nself._sync_params()\r\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/distributed.py\", line 187, in _sync_params\r\ndist.broadcast(flat_buffers, 0)\r\nFile \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 198, in broadcast\r\nreturn torch._C._dist_broadcast(tensor, src, group)\r\n```\r\n\r\n2. In stead of using DistributedDataParrallel, I tried to train independent model on each node and average the gradient using the following code:\r\n```\r\nfor epoch in range(num_epoches):  # loop over the dataset multiple times\r\n    running_loss = 0.0\r\n    print(\"Epoch: {}\".format(epoch))\r\n    train_sampler.set_epoch(epoch)\r\n    for i, data in enumerate(train_loader, 0):\r\n        # gettheinputs\r\n        inputs = data['image']\r\n        labels = data['category_id']\r\n        labels = np.array([category_ids.index(l) for l in labels])\r\n        print(\"i: {}\".format(i))\r\n\r\n        print(\"labels\", labels)\r\n        # wrap them in Variable\r\n        inputs = inputs.cuda(async=True)\r\n        labels = torch.from_numpy(labels).cuda(async=True)\r\n\tinputs, labels= Variable(inputs), Variable(labels)\r\n\r\n        # zero the parameter gradients\r\n        optimizer.zero_grad()\r\n\r\n        # forward + backward + optimize\r\n        outputs = net(inputs)\r\n        loss=criterion(outputs,labels)\r\n\tloss.backward()\r\n\r\n        size = float(dist.get_world_size())\r\n        for param in net.parameters():\r\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n            param.grad.data /= size\r\n\toptimizer.step()\r\n```\r\nIt works fine for the first epoch. It crashes once it starts the second epoch with the follwing error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./script/train_dist_new.py\", line 126, in <module>\r\n    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n  File \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 216, in all_reduce\r\n    return torch._C._dist_all_reduce(tensor, op, group)\r\nRuntimeError: [/home/yuyou/downloads/mypytorch2/pytorch/torch/lib/gloo/gloo/transport/ibverbs/buffer.cc:108] Read timeout LID: 45 QPN: 29770 PSN: 11590737\r\n```\r\n3. My workaround now is to combine multiple epoch of data into one epoch when I define the Dataset. Basically, looping over the dataloader once is same as looping over my train data multiple times. It works pretty well so far."}