{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340207445", "html_url": "https://github.com/pytorch/pytorch/pull/2903#issuecomment-340207445", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2903", "id": 340207445, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDIwNzQ0NQ==", "user": {"login": "xqding", "id": 20047329, "node_id": "MDQ6VXNlcjIwMDQ3MzI5", "avatar_url": "https://avatars1.githubusercontent.com/u/20047329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xqding", "html_url": "https://github.com/xqding", "followers_url": "https://api.github.com/users/xqding/followers", "following_url": "https://api.github.com/users/xqding/following{/other_user}", "gists_url": "https://api.github.com/users/xqding/gists{/gist_id}", "starred_url": "https://api.github.com/users/xqding/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xqding/subscriptions", "organizations_url": "https://api.github.com/users/xqding/orgs", "repos_url": "https://api.github.com/users/xqding/repos", "events_url": "https://api.github.com/users/xqding/events{/privacy}", "received_events_url": "https://api.github.com/users/xqding/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-28T17:35:53Z", "updated_at": "2017-10-28T17:49:14Z", "author_association": "NONE", "body_html": "<pre><code>train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\ntrain_loader = DataLoader(train_data,\n                          batch_size = 32,\n                          sampler = train_sampler,\n                          num_workers= 2)\n\nfor epoch in range(num_epoches):  # loop over the dataset multiple times\n    running_loss = 0.0\n    print(\"Epoch: {}\".format(epoch))\n    train_sampler.set_epoch(epoch)\n    for i, data in enumerate(train_loader, 0):\n        # gettheinputs\n        inputs = data['image']\n        labels = data['category_id']\n        labels = np.array([category_ids.index(l) for l in labels])\n        print(\"i: {}\".format(i))\n\n        print(\"labels\", labels)\n        # wrap them in Variable\n        inputs = inputs.cuda(async=True)\n        labels = torch.from_numpy(labels).cuda(async=True)\n\tinputs, labels= Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss=criterion(outputs,labels)\n\tloss.backward()\n\n        size = float(dist.get_world_size())\n        for param in net.parameters():\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n            param.grad.data /= size\n\toptimizer.step()\n\n</code></pre>\n<p>When epoch = 0, the code works fine. When it starts the epoch = 1, it crashes when it reaches the dist.all_reduce command with the following error message:</p>\n<pre><code>Traceback (most recent call last):\n  File \"./script/train_dist_new.py\", line 126, in &lt;module&gt;\n    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n  File \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 216, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: [/home/yuyou/downloads/mypytorch2/pytorch/torch/lib/gloo/gloo/transport/ibverbs/buffer.cc:108] Read timeout LID: 45 QPN: 29770 PSN: 11590737\n\n\n\n</code></pre>", "body_text": "train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\ntrain_loader = DataLoader(train_data,\n                          batch_size = 32,\n                          sampler = train_sampler,\n                          num_workers= 2)\n\nfor epoch in range(num_epoches):  # loop over the dataset multiple times\n    running_loss = 0.0\n    print(\"Epoch: {}\".format(epoch))\n    train_sampler.set_epoch(epoch)\n    for i, data in enumerate(train_loader, 0):\n        # gettheinputs\n        inputs = data['image']\n        labels = data['category_id']\n        labels = np.array([category_ids.index(l) for l in labels])\n        print(\"i: {}\".format(i))\n\n        print(\"labels\", labels)\n        # wrap them in Variable\n        inputs = inputs.cuda(async=True)\n        labels = torch.from_numpy(labels).cuda(async=True)\n\tinputs, labels= Variable(inputs), Variable(labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss=criterion(outputs,labels)\n\tloss.backward()\n\n        size = float(dist.get_world_size())\n        for param in net.parameters():\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n            param.grad.data /= size\n\toptimizer.step()\n\n\nWhen epoch = 0, the code works fine. When it starts the epoch = 1, it crashes when it reaches the dist.all_reduce command with the following error message:\nTraceback (most recent call last):\n  File \"./script/train_dist_new.py\", line 126, in <module>\n    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n  File \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 216, in all_reduce\n    return torch._C._dist_all_reduce(tensor, op, group)\nRuntimeError: [/home/yuyou/downloads/mypytorch2/pytorch/torch/lib/gloo/gloo/transport/ibverbs/buffer.cc:108] Read timeout LID: 45 QPN: 29770 PSN: 11590737", "body": "```\r\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\r\ntrain_loader = DataLoader(train_data,\r\n                          batch_size = 32,\r\n                          sampler = train_sampler,\r\n                          num_workers= 2)\r\n\r\nfor epoch in range(num_epoches):  # loop over the dataset multiple times\r\n    running_loss = 0.0\r\n    print(\"Epoch: {}\".format(epoch))\r\n    train_sampler.set_epoch(epoch)\r\n    for i, data in enumerate(train_loader, 0):\r\n        # gettheinputs\r\n        inputs = data['image']\r\n        labels = data['category_id']\r\n        labels = np.array([category_ids.index(l) for l in labels])\r\n        print(\"i: {}\".format(i))\r\n\r\n        print(\"labels\", labels)\r\n        # wrap them in Variable\r\n        inputs = inputs.cuda(async=True)\r\n        labels = torch.from_numpy(labels).cuda(async=True)\r\n\tinputs, labels= Variable(inputs), Variable(labels)\r\n\r\n        # zero the parameter gradients\r\n        optimizer.zero_grad()\r\n\r\n        # forward + backward + optimize\r\n        outputs = net(inputs)\r\n        loss=criterion(outputs,labels)\r\n\tloss.backward()\r\n\r\n        size = float(dist.get_world_size())\r\n        for param in net.parameters():\r\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n            param.grad.data /= size\r\n\toptimizer.step()\r\n\r\n```\r\nWhen epoch = 0, the code works fine. When it starts the epoch = 1, it crashes when it reaches the dist.all_reduce command with the following error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./script/train_dist_new.py\", line 126, in <module>\r\n    dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n  File \"/home/yuyou/apps/anaconda3/lib/python3.5/site-packages/torch/distributed/__init__.py\", line 216, in all_reduce\r\n    return torch._C._dist_all_reduce(tensor, op, group)\r\nRuntimeError: [/home/yuyou/downloads/mypytorch2/pytorch/torch/lib/gloo/gloo/transport/ibverbs/buffer.cc:108] Read timeout LID: 45 QPN: 29770 PSN: 11590737\r\n\r\n\r\n\r\n```"}