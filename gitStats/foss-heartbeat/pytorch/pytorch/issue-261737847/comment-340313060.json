{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340313060", "html_url": "https://github.com/pytorch/pytorch/pull/2903#issuecomment-340313060", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2903", "id": 340313060, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDMxMzA2MA==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-29T23:45:50Z", "updated_at": "2017-10-29T23:46:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20047329\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xqding\">@xqding</a> , I can randomly reproduce the problem on my machine. I wonder if your workaround below solves the problem permanently. If so, could you share that part so that it may help me locate which part of DistributedDataParallel caused the timeout. Thanks!</p>\n<pre><code>If I use the torch.distributed to average the gradient instead of torch.nn.parallel.DistributedDataParallel, the code works fine and has good scaling with the number of GPUs.\n</code></pre>", "body_text": "Hi @xqding , I can randomly reproduce the problem on my machine. I wonder if your workaround below solves the problem permanently. If so, could you share that part so that it may help me locate which part of DistributedDataParallel caused the timeout. Thanks!\nIf I use the torch.distributed to average the gradient instead of torch.nn.parallel.DistributedDataParallel, the code works fine and has good scaling with the number of GPUs.", "body": "Hi @xqding , I can randomly reproduce the problem on my machine. I wonder if your workaround below solves the problem permanently. If so, could you share that part so that it may help me locate which part of DistributedDataParallel caused the timeout. Thanks!\r\n```\r\nIf I use the torch.distributed to average the gradient instead of torch.nn.parallel.DistributedDataParallel, the code works fine and has good scaling with the number of GPUs.\r\n```"}