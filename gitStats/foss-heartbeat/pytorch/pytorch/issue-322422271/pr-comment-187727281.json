{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187727281", "pull_request_review_id": 119573243, "id": 187727281, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzcyNzI4MQ==", "diff_hunk": "@@ -1090,3 +884,133 @@ def symbolic_flattened_wrapper(g, input, *args):\n         symbolic_flattened_wrapper\n     )\n     return tuple(o for o in outputs)\n+\n+\n+def RNN_variant_symbolic_builder(\n+        variant, input_size, hidden_size, num_layers, batch_first, dropout, bidirectional, **kwargs):\n+    def symbolic(g, input, all_weights, initial_states, batch_sizes):\n+        if batch_first:\n+            return _unimplemented(\"RNN/GRU/LSTM\", \"batch_first\")\n+        if dropout and kwargs['train']:\n+            return _unimplemented(\"RNN/GRU/LSTM\", \"dropout in training mode\")\n+\n+        unidirectional = not bidirectional\n+\n+        prev_output = input\n+\n+        if variant == 'RNN' or variant == 'GRU':\n+            h0 = initial_states\n+            h_outs = []\n+        elif variant == 'LSTM':\n+            h0, c0 = initial_states\n+            h_outs = []\n+            c_outs = []\n+\n+        sequence_lens = unused(g) if batch_sizes is None else batch_sizes\n+\n+        for i in range(num_layers):\n+            if unidirectional:\n+                if variant == 'RNN':\n+                    weight_ih, weight_hh, bias_ih, bias_hh = all_weights[i]\n+                elif variant == 'GRU':\n+                    # pytorch is reset, input, hidden\n+                    # onnx is    input, reset, hidden\n+                    weight_ih, weight_hh, bias_ih, bias_hh = \\\n+                        [reform_weights(g, w, hidden_size, [(1, 2), (0, 1), (2, 3)]) for w in all_weights[i]]\n+                elif variant == 'LSTM':\n+                    # pytorch is input, forget, cell, output.\n+                    # onnx is    input, output, forget, cell.\n+                    weight_ih, weight_hh, bias_ih, bias_hh = \\\n+                        [reform_weights(g, w, hidden_size, [(0, 1), (3, 4), (1, 3)]) for w in all_weights[i]]\n+                bias_concat = g.op('Concat', bias_ih, bias_hh, axis_i=0)\n+\n+                # Unidirectional weights in PyTorch omit the\n+                # directionality dimension (dim=0), whereas in ONNX it\n+                # is required even for unidirectional RNN; thus,\n+                # insert a dimension of size at dim=0.\n+                weight_ih = g.op('Unsqueeze', weight_ih, axes_i=[0])\n+                weight_hh = g.op('Unsqueeze', weight_hh, axes_i=[0])\n+                bias_concat = g.op('Unsqueeze', bias_concat, axes_i=[0])\n+\n+                if variant == 'RNN' or variant == 'GRU':\n+                    state_inputs = [\n+                        h0 if num_layers == 1 else g.op('Slice', h0, axes_i=[0], starts_i=[i], ends_i=[i + 1])\n+                    ]\n+                elif variant == 'LSTM':\n+                    state_inputs = [\n+                        h0 if num_layers == 1 else g.op('Slice', h0, axes_i=[0], starts_i=[i], ends_i=[i + 1]),\n+                        c0 if num_layers == 1 else g.op('Slice', c0, axes_i=[0], starts_i=[i], ends_i=[i + 1])\n+                    ]\n+            else:\n+                if variant == 'RNN':\n+                    weight_ih_f, weight_hh_f, bias_ih_f, bias_hh_f = all_weights[2 * i]\n+                    weight_ih_b, weight_hh_b, bias_ih_b, bias_hh_b = all_weights[2 * i + 1]\n+                elif variant == 'GRU':\n+                    # pytorch is reset, input, hidden\n+                    # onnx is    input, reset, hidden\n+                    weight_ih_f, weight_hh_f, bias_ih_f, bias_hh_f = \\\n+                        [reform_weights(g, w, hidden_size, [(1, 2), (0, 1), (2, 3)]) for w in all_weights[2 * i]]\n+                    weight_ih_b, weight_hh_b, bias_ih_b, bias_hh_b = \\\n+                        [reform_weights(g, w, hidden_size, [(1, 2), (0, 1), (2, 3)]) for w in all_weights[2 * i + 1]]\n+                elif variant == 'LSTM':\n+                    # pytorch is input, forget, cell, output.\n+                    # onnx is    input, output, forget, cell.\n+                    weight_ih_f, weight_hh_f, bias_ih_f, bias_hh_f = \\\n+                        [reform_weights(g, w, hidden_size, [(0, 1), (3, 4), (1, 3)]) for w in all_weights[2 * i]]\n+                    weight_ih_b, weight_hh_b, bias_ih_b, bias_hh_b = \\\n+                        [reform_weights(g, w, hidden_size, [(0, 1), (3, 4), (1, 3)]) for w in all_weights[2 * i + 1]]\n+\n+                weight_ih = g.op('Concat',\n+                                 g.op('Unsqueeze', weight_ih_f, axes_i=[0]),\n+                                 g.op('Unsqueeze', weight_ih_b, axes_i=[0]),\n+                                 axis_i=0)\n+                weight_hh = g.op('Concat',\n+                                 g.op('Unsqueeze', weight_hh_f, axes_i=[0]),\n+                                 g.op('Unsqueeze', weight_hh_b, axes_i=[0]),\n+                                 axis_i=0)\n+                bias_f = g.op('Concat', bias_ih_f, bias_hh_f, axis_i=0)\n+                bias_b = g.op('Concat', bias_ih_b, bias_hh_b, axis_i=0)\n+                bias_concat = g.op('Concat',\n+                                   g.op('Unsqueeze', bias_f, axes_i=[0]),\n+                                   g.op('Unsqueeze', bias_b, axes_i=[0]),\n+                                   axis_i=0)\n+\n+                if variant == 'RNN' or variant == 'GRU':\n+                    state_inputs = [", "path": "torch/onnx/symbolic.py", "position": null, "original_position": 324, "commit_id": "1dd664d41ca3e52b60e6aebf7afbc773be650974", "original_commit_id": "2fa42d57d6108c145496609c92949bf3804c415a", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Append treatment here?", "created_at": "2018-05-11T20:40:49Z", "updated_at": "2018-11-23T15:43:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/7506#discussion_r187727281", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7506", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187727281"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7506#discussion_r187727281"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7506"}}, "body_html": "<p>Append treatment here?</p>", "body_text": "Append treatment here?"}