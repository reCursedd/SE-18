{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8823", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8823/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8823/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8823/events", "html_url": "https://github.com/pytorch/pytorch/issues/8823", "id": 335104614, "node_id": "MDU6SXNzdWUzMzUxMDQ2MTQ=", "number": 8823, "title": "CUDA runtime error(2) : Out of memory", "user": {"login": "thFury", "id": 36451726, "node_id": "MDQ6VXNlcjM2NDUxNzI2", "avatar_url": "https://avatars1.githubusercontent.com/u/36451726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thFury", "html_url": "https://github.com/thFury", "followers_url": "https://api.github.com/users/thFury/followers", "following_url": "https://api.github.com/users/thFury/following{/other_user}", "gists_url": "https://api.github.com/users/thFury/gists{/gist_id}", "starred_url": "https://api.github.com/users/thFury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thFury/subscriptions", "organizations_url": "https://api.github.com/users/thFury/orgs", "repos_url": "https://api.github.com/users/thFury/repos", "events_url": "https://api.github.com/users/thFury/events{/privacy}", "received_events_url": "https://api.github.com/users/thFury/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-23T14:25:09Z", "updated_at": "2018-09-26T21:24:40Z", "closed_at": "2018-07-27T05:05:28Z", "author_association": "NONE", "body_html": "<p>Hello. I have been trying to figure out what is happening.</p>\n<h2>Issue description</h2>\n<p>The python script fails here:<br>\n<code>RuntimeError: cuda runtime error (2) : out of memory at ..\\src\\THC\\THCGeneral.cpp:844</code>.<br>\nI've read from other issues that this is because I am out of memory. Weird. I used the debugger to catch exactly when the code throws the error and then I used <code>nvidia-smi</code> to check how much VRAM I had available: 744/4095mb. How could this be?</p>\n<p>This is my code (keep in mind that some parts are written badly and don't work, but they are not relevant to the bug report nonetheless, an example of this is the <code>elif epo == self.num_epochs:</code> doesn't work properly..)</p>\n<pre><code>def get_input_layer(self, word_idx):\n\n    x = torch.zeros(self.vocab_size).float()\n    x[word_idx] = 1.0\n    return x`\n\ndef model(self, mode=1):\n\n    \n    W1 = Variable(torch.randn(self.embedding_dims, self.vocab_size).cuda().float(), requires_grad=True)\n    W2 = Variable(torch.randn(self.vocab_size, self.embedding_dims).cuda().float(), requires_grad=True)\n\n    print('Model Initialized')\n    for epo in range(self.num_epochs):\n        loss_val = None\n        for i_batch, sample_batched in enumerate(self.data_loader):\n            loss_val = 0\n\n            x = self.get_input_layer(sample_batched[0].long()).cuda()\n            y_true = Variable(torch.from_numpy(np.array(sample_batched[1])).cuda().long())\n\n            z1 = torch.matmul(W1, x)\n            z2 = torch.matmul(W2, z1)\n\n            log_softmax = F_nn.log_softmax(z2, dim=0)\n\n            # Loss\n            loss = F_nn.nll_loss(log_softmax.view(1,-1), y_true)\n            loss_val += loss.item()\n            loss.backward()\n\n            # Optimizer\n\n            try:\n                W1.data -= self.learning_rate * W1.grad.data\n            except:\n                print(GPUtil.showUtilization(useOldCode=True))\n            W2.data -= self.learning_rate * W2.grad.data\n\n            W1.grad.data.zero_()\n            W2.grad.data.zero_()\n        if epo % 1 == 0:\n            print(f'Loss at epo {epo}: {loss_val/len(sample_batched)}')\n        elif epo == self.num_epochs:\n            print('Training completed. \\n Final loss:',loss)\n        else:\n            print('Completed epoch', epo)`\n</code></pre>\n<h2>System Info</h2>\n<p>(This is the output of collect_env.py)<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0</p>\n<p>OS: Microsoft Windows 7 Ultimate<br>\nGCC version: Could not collect<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration: GPU 0: GeForce GTX 970<br>\nNvidia driver version: 385.54<br>\ncuDNN version: Probably one of the following:<br>\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.5)</p>\n<p>[pip] torch (0.4.0)<br>\n[conda] cuda90                    1.0                           0    pytorch</p>\n<p>[conda] pytorch                   0.4.0           py36_cuda90_cudnn7he774522_1  [cuda90]  pytorch</p>", "body_text": "Hello. I have been trying to figure out what is happening.\nIssue description\nThe python script fails here:\nRuntimeError: cuda runtime error (2) : out of memory at ..\\src\\THC\\THCGeneral.cpp:844.\nI've read from other issues that this is because I am out of memory. Weird. I used the debugger to catch exactly when the code throws the error and then I used nvidia-smi to check how much VRAM I had available: 744/4095mb. How could this be?\nThis is my code (keep in mind that some parts are written badly and don't work, but they are not relevant to the bug report nonetheless, an example of this is the elif epo == self.num_epochs: doesn't work properly..)\ndef get_input_layer(self, word_idx):\n\n    x = torch.zeros(self.vocab_size).float()\n    x[word_idx] = 1.0\n    return x`\n\ndef model(self, mode=1):\n\n    \n    W1 = Variable(torch.randn(self.embedding_dims, self.vocab_size).cuda().float(), requires_grad=True)\n    W2 = Variable(torch.randn(self.vocab_size, self.embedding_dims).cuda().float(), requires_grad=True)\n\n    print('Model Initialized')\n    for epo in range(self.num_epochs):\n        loss_val = None\n        for i_batch, sample_batched in enumerate(self.data_loader):\n            loss_val = 0\n\n            x = self.get_input_layer(sample_batched[0].long()).cuda()\n            y_true = Variable(torch.from_numpy(np.array(sample_batched[1])).cuda().long())\n\n            z1 = torch.matmul(W1, x)\n            z2 = torch.matmul(W2, z1)\n\n            log_softmax = F_nn.log_softmax(z2, dim=0)\n\n            # Loss\n            loss = F_nn.nll_loss(log_softmax.view(1,-1), y_true)\n            loss_val += loss.item()\n            loss.backward()\n\n            # Optimizer\n\n            try:\n                W1.data -= self.learning_rate * W1.grad.data\n            except:\n                print(GPUtil.showUtilization(useOldCode=True))\n            W2.data -= self.learning_rate * W2.grad.data\n\n            W1.grad.data.zero_()\n            W2.grad.data.zero_()\n        if epo % 1 == 0:\n            print(f'Loss at epo {epo}: {loss_val/len(sample_batched)}')\n        elif epo == self.num_epochs:\n            print('Training completed. \\n Final loss:',loss)\n        else:\n            print('Completed epoch', epo)`\n\nSystem Info\n(This is the output of collect_env.py)\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.0\nOS: Microsoft Windows 7 Ultimate\nGCC version: Could not collect\nCMake version: Could not collect\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: GPU 0: GeForce GTX 970\nNvidia driver version: 385.54\ncuDNN version: Probably one of the following:\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\nVersions of relevant libraries:\n[pip] numpy (1.14.5)\n[pip] torch (0.4.0)\n[conda] cuda90                    1.0                           0    pytorch\n[conda] pytorch                   0.4.0           py36_cuda90_cudnn7he774522_1  [cuda90]  pytorch", "body": "Hello. I have been trying to figure out what is happening.\r\n\r\n## Issue description\r\n\r\nThe python script fails here:\r\n`RuntimeError: cuda runtime error (2) : out of memory at ..\\src\\THC\\THCGeneral.cpp:844`.\r\nI've read from other issues that this is because I am out of memory. Weird. I used the debugger to catch exactly when the code throws the error and then I used `nvidia-smi` to check how much VRAM I had available: 744/4095mb. How could this be?\r\n\r\nThis is my code (keep in mind that some parts are written badly and don't work, but they are not relevant to the bug report nonetheless, an example of this is the `elif epo == self.num_epochs:` doesn't work properly..)\r\n\r\n    def get_input_layer(self, word_idx):\r\n\r\n        x = torch.zeros(self.vocab_size).float()\r\n        x[word_idx] = 1.0\r\n        return x`\r\n\r\n    def model(self, mode=1):\r\n\r\n        \r\n        W1 = Variable(torch.randn(self.embedding_dims, self.vocab_size).cuda().float(), requires_grad=True)\r\n        W2 = Variable(torch.randn(self.vocab_size, self.embedding_dims).cuda().float(), requires_grad=True)\r\n\r\n        print('Model Initialized')\r\n        for epo in range(self.num_epochs):\r\n            loss_val = None\r\n            for i_batch, sample_batched in enumerate(self.data_loader):\r\n                loss_val = 0\r\n\r\n                x = self.get_input_layer(sample_batched[0].long()).cuda()\r\n                y_true = Variable(torch.from_numpy(np.array(sample_batched[1])).cuda().long())\r\n\r\n                z1 = torch.matmul(W1, x)\r\n                z2 = torch.matmul(W2, z1)\r\n\r\n                log_softmax = F_nn.log_softmax(z2, dim=0)\r\n\r\n                # Loss\r\n                loss = F_nn.nll_loss(log_softmax.view(1,-1), y_true)\r\n                loss_val += loss.item()\r\n                loss.backward()\r\n\r\n                # Optimizer\r\n\r\n                try:\r\n                    W1.data -= self.learning_rate * W1.grad.data\r\n                except:\r\n                    print(GPUtil.showUtilization(useOldCode=True))\r\n                W2.data -= self.learning_rate * W2.grad.data\r\n\r\n                W1.grad.data.zero_()\r\n                W2.grad.data.zero_()\r\n            if epo % 1 == 0:\r\n                print(f'Loss at epo {epo}: {loss_val/len(sample_batched)}')\r\n            elif epo == self.num_epochs:\r\n                print('Training completed. \\n Final loss:',loss)\r\n            else:\r\n                print('Completed epoch', epo)`\r\n\r\n## System Info\r\n(This is the output of collect_env.py)\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 7 Ultimate\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 970\r\nNvidia driver version: 385.54\r\ncuDNN version: Probably one of the following:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n\r\n[pip] torch (0.4.0)\r\n[conda] cuda90                    1.0                           0    pytorch\r\n\r\n[conda] pytorch                   0.4.0           py36_cuda90_cudnn7he774522_1  [cuda90]  pytorch\r\n\r\n"}