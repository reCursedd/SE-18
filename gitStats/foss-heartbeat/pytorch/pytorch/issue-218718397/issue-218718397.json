{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1168/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1168/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1168/events", "html_url": "https://github.com/pytorch/pytorch/issues/1168", "id": 218718397, "node_id": "MDU6SXNzdWUyMTg3MTgzOTc=", "number": 1168, "title": "RuntimeError: matrix and matrix expected", "user": {"login": "karlTUM", "id": 15608199, "node_id": "MDQ6VXNlcjE1NjA4MTk5", "avatar_url": "https://avatars2.githubusercontent.com/u/15608199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karlTUM", "html_url": "https://github.com/karlTUM", "followers_url": "https://api.github.com/users/karlTUM/followers", "following_url": "https://api.github.com/users/karlTUM/following{/other_user}", "gists_url": "https://api.github.com/users/karlTUM/gists{/gist_id}", "starred_url": "https://api.github.com/users/karlTUM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karlTUM/subscriptions", "organizations_url": "https://api.github.com/users/karlTUM/orgs", "repos_url": "https://api.github.com/users/karlTUM/repos", "events_url": "https://api.github.com/users/karlTUM/events{/privacy}", "received_events_url": "https://api.github.com/users/karlTUM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-01T19:43:01Z", "updated_at": "2017-04-01T23:32:20Z", "closed_at": "2017-04-01T23:32:19Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I would like to implement a sharing weight 'alexnet' with two input batches of images, and the fully connected layers are concatenated together like this:</p>\n<pre><code>class ALEXNET_two_scale(nn.Module):\n  def __init__(self, num_classes, \n               original_model = models.__dict__['alexnet'](pretrained=True)):\n    super(ALEXNET_two_scale, self).__init__()\n    \n    self.features = original_model.features\n    \n    self.drop_out = nn.Dropout(p=0.75)\n    \n    self.fc6_scl1 = nn.Linear(256 * 6 * 6, 4096)\n    self.fc6_scl2 = nn.Linear(256 * 6 * 6, 4096)\n    \n    self.relu = nn.ReLU(inplace=True)\n    \n    self.fc7_scl1 = nn.Linear(4096, 2048)\n    self.fc7_scl2 = nn.Linear(4096, 2048)\n    \n    self.fc8 = nn.Linear(4096, num_classes)\n    \n    \n  def forward(self, imgs_scl1, imgs_scl2):\n    \n    x_1 = self.features(imgs_scl1)\n    x_2 = self.features(imgs_scl2)\n    \n    x_1 = self.drop_out(x_1)\n    x_2 = self.drop_out(x_2)\n    \n    x_1 = self.fc6_scl1(x_1)\n    x_2 = self.fc6_scl2(x_2)\n    \n    x_1 = self.relu(x_1)\n    x_2 = self.relu(x_2)\n    \n    x_1 = self.drop_out(x_1)\n    x_2 = self.drop_out(x_2)\n    \n    x_1 = self.fc7_scl1(x_1)\n    x_2 = self.fc7_scl2(x_2)\n    \n    x = torch.cat([x_1, x_2], 1)\n    \n    y = self.fc8(x)\n    \n    return y\n</code></pre>\n<p>However, when I run it, I come across the following error:</p>\n<pre><code>/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in &lt;module&gt;()\n    396 \n    397 if __name__ == '__main__':\n--&gt; 398   main()\n    399 \n    400 \n\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in main()\n    369 \n    370     # train for one epoch\n--&gt; 371     train_losses_per_epoch = train(train_img_lab_lists, model, train_transform, criterion, optimizer, epoch)\n    372 \n    373     training_metadata['epoch_{}_train_loss'.format(epoch)] = train_losses_per_epoch\n\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in train(train_img_lab_lists, model, train_transform, criterion, optimizer, epoch)\n    143 \n    144     # compute output\n--&gt; 145     output = model(batch_imgs_scale1_var, batch_imgs_scale2_var)\n    146     loss = criterion(output, batch_labs_var)\n    147 \n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--&gt; 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in forward(self, imgs_scl1, imgs_scl2)\n     70     x_2 = self.drop_out(x_2)\n     71 \n---&gt; 72     x_1 = self.fc6_scl1(x_1)\n     73     x_2 = self.fc6_scl2(x_2)\n     74 \n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--&gt; 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\n     52             return self._backend.Linear()(input, self.weight)\n     53         else:\n---&gt; 54             return self._backend.Linear()(input, self.weight, self.bias)\n     55 \n     56     def __repr__(self):\n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/linear.py in forward(self, input, weight, bias)\n      8         self.save_for_backward(input, weight, bias)\n      9         output = input.new(input.size(0), weight.size(0))\n---&gt; 10         output.addmm_(0, 1, input, weight.t())\n     11         if bias is not None:\n     12             # cuBLAS doesn't support 0 strides in sger, so we can't use expand\n\nRuntimeError: matrix and matrix expected at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.10_1488758793045/work/torch/lib/THC/generic/THCTensorMathBlas.cu:235\n\n</code></pre>\n<p>Does anyone know about it?</p>\n<p>Thank  you very much.</p>", "body_text": "Hi,\nI would like to implement a sharing weight 'alexnet' with two input batches of images, and the fully connected layers are concatenated together like this:\nclass ALEXNET_two_scale(nn.Module):\n  def __init__(self, num_classes, \n               original_model = models.__dict__['alexnet'](pretrained=True)):\n    super(ALEXNET_two_scale, self).__init__()\n    \n    self.features = original_model.features\n    \n    self.drop_out = nn.Dropout(p=0.75)\n    \n    self.fc6_scl1 = nn.Linear(256 * 6 * 6, 4096)\n    self.fc6_scl2 = nn.Linear(256 * 6 * 6, 4096)\n    \n    self.relu = nn.ReLU(inplace=True)\n    \n    self.fc7_scl1 = nn.Linear(4096, 2048)\n    self.fc7_scl2 = nn.Linear(4096, 2048)\n    \n    self.fc8 = nn.Linear(4096, num_classes)\n    \n    \n  def forward(self, imgs_scl1, imgs_scl2):\n    \n    x_1 = self.features(imgs_scl1)\n    x_2 = self.features(imgs_scl2)\n    \n    x_1 = self.drop_out(x_1)\n    x_2 = self.drop_out(x_2)\n    \n    x_1 = self.fc6_scl1(x_1)\n    x_2 = self.fc6_scl2(x_2)\n    \n    x_1 = self.relu(x_1)\n    x_2 = self.relu(x_2)\n    \n    x_1 = self.drop_out(x_1)\n    x_2 = self.drop_out(x_2)\n    \n    x_1 = self.fc7_scl1(x_1)\n    x_2 = self.fc7_scl2(x_2)\n    \n    x = torch.cat([x_1, x_2], 1)\n    \n    y = self.fc8(x)\n    \n    return y\n\nHowever, when I run it, I come across the following error:\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in <module>()\n    396 \n    397 if __name__ == '__main__':\n--> 398   main()\n    399 \n    400 \n\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in main()\n    369 \n    370     # train for one epoch\n--> 371     train_losses_per_epoch = train(train_img_lab_lists, model, train_transform, criterion, optimizer, epoch)\n    372 \n    373     training_metadata['epoch_{}_train_loss'.format(epoch)] = train_losses_per_epoch\n\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in train(train_img_lab_lists, model, train_transform, criterion, optimizer, epoch)\n    143 \n    144     # compute output\n--> 145     output = model(batch_imgs_scale1_var, batch_imgs_scale2_var)\n    146     loss = criterion(output, batch_labs_var)\n    147 \n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--> 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in forward(self, imgs_scl1, imgs_scl2)\n     70     x_2 = self.drop_out(x_2)\n     71 \n---> 72     x_1 = self.fc6_scl1(x_1)\n     73     x_2 = self.fc6_scl2(x_2)\n     74 \n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    200 \n    201     def __call__(self, *input, **kwargs):\n--> 202         result = self.forward(*input, **kwargs)\n    203         for hook in self._forward_hooks.values():\n    204             hook_result = hook(self, input, result)\n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\n     52             return self._backend.Linear()(input, self.weight)\n     53         else:\n---> 54             return self._backend.Linear()(input, self.weight, self.bias)\n     55 \n     56     def __repr__(self):\n\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/linear.py in forward(self, input, weight, bias)\n      8         self.save_for_backward(input, weight, bias)\n      9         output = input.new(input.size(0), weight.size(0))\n---> 10         output.addmm_(0, 1, input, weight.t())\n     11         if bias is not None:\n     12             # cuBLAS doesn't support 0 strides in sger, so we can't use expand\n\nRuntimeError: matrix and matrix expected at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.10_1488758793045/work/torch/lib/THC/generic/THCTensorMathBlas.cu:235\n\n\nDoes anyone know about it?\nThank  you very much.", "body": "Hi,\r\n\r\nI would like to implement a sharing weight 'alexnet' with two input batches of images, and the fully connected layers are concatenated together like this:\r\n```\r\nclass ALEXNET_two_scale(nn.Module):\r\n  def __init__(self, num_classes, \r\n               original_model = models.__dict__['alexnet'](pretrained=True)):\r\n    super(ALEXNET_two_scale, self).__init__()\r\n    \r\n    self.features = original_model.features\r\n    \r\n    self.drop_out = nn.Dropout(p=0.75)\r\n    \r\n    self.fc6_scl1 = nn.Linear(256 * 6 * 6, 4096)\r\n    self.fc6_scl2 = nn.Linear(256 * 6 * 6, 4096)\r\n    \r\n    self.relu = nn.ReLU(inplace=True)\r\n    \r\n    self.fc7_scl1 = nn.Linear(4096, 2048)\r\n    self.fc7_scl2 = nn.Linear(4096, 2048)\r\n    \r\n    self.fc8 = nn.Linear(4096, num_classes)\r\n    \r\n    \r\n  def forward(self, imgs_scl1, imgs_scl2):\r\n    \r\n    x_1 = self.features(imgs_scl1)\r\n    x_2 = self.features(imgs_scl2)\r\n    \r\n    x_1 = self.drop_out(x_1)\r\n    x_2 = self.drop_out(x_2)\r\n    \r\n    x_1 = self.fc6_scl1(x_1)\r\n    x_2 = self.fc6_scl2(x_2)\r\n    \r\n    x_1 = self.relu(x_1)\r\n    x_2 = self.relu(x_2)\r\n    \r\n    x_1 = self.drop_out(x_1)\r\n    x_2 = self.drop_out(x_2)\r\n    \r\n    x_1 = self.fc7_scl1(x_1)\r\n    x_2 = self.fc7_scl2(x_2)\r\n    \r\n    x = torch.cat([x_1, x_2], 1)\r\n    \r\n    y = self.fc8(x)\r\n    \r\n    return y\r\n```\r\n\r\nHowever, when I run it, I come across the following error:\r\n\r\n\r\n```\r\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in <module>()\r\n    396 \r\n    397 if __name__ == '__main__':\r\n--> 398   main()\r\n    399 \r\n    400 \r\n\r\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in main()\r\n    369 \r\n    370     # train for one epoch\r\n--> 371     train_losses_per_epoch = train(train_img_lab_lists, model, train_transform, criterion, optimizer, epoch)\r\n    372 \r\n    373     training_metadata['epoch_{}_train_loss'.format(epoch)] = train_losses_per_epoch\r\n\r\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in train(train_img_lab_lists, model, train_transform, criterion, optimizer, epoch)\r\n    143 \r\n    144     # compute output\r\n--> 145     output = model(batch_imgs_scale1_var, batch_imgs_scale2_var)\r\n    146     loss = criterion(output, batch_labs_var)\r\n    147 \r\n\r\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    200 \r\n    201     def __call__(self, *input, **kwargs):\r\n--> 202         result = self.forward(*input, **kwargs)\r\n    203         for hook in self._forward_hooks.values():\r\n    204             hook_result = hook(self, input, result)\r\n\r\n/home/ga85nej/Documents/pytorch_WH_multiscale/pytorch_multiscale_VGG16 in forward(self, imgs_scl1, imgs_scl2)\r\n     70     x_2 = self.drop_out(x_2)\r\n     71 \r\n---> 72     x_1 = self.fc6_scl1(x_1)\r\n     73     x_2 = self.fc6_scl2(x_2)\r\n     74 \r\n\r\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    200 \r\n    201     def __call__(self, *input, **kwargs):\r\n--> 202         result = self.forward(*input, **kwargs)\r\n    203         for hook in self._forward_hooks.values():\r\n    204             hook_result = hook(self, input, result)\r\n\r\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\r\n     52             return self._backend.Linear()(input, self.weight)\r\n     53         else:\r\n---> 54             return self._backend.Linear()(input, self.weight, self.bias)\r\n     55 \r\n     56     def __repr__(self):\r\n\r\n/home/ga85nej/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/linear.py in forward(self, input, weight, bias)\r\n      8         self.save_for_backward(input, weight, bias)\r\n      9         output = input.new(input.size(0), weight.size(0))\r\n---> 10         output.addmm_(0, 1, input, weight.t())\r\n     11         if bias is not None:\r\n     12             # cuBLAS doesn't support 0 strides in sger, so we can't use expand\r\n\r\nRuntimeError: matrix and matrix expected at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.10_1488758793045/work/torch/lib/THC/generic/THCTensorMathBlas.cu:235\r\n\r\n```\r\n\r\nDoes anyone know about it? \r\n\r\nThank  you very much.\r\n\r\n\r\n\r\n\r\n "}