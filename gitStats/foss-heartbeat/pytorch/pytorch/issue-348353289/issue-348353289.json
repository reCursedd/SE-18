{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10301", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10301/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10301/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10301/events", "html_url": "https://github.com/pytorch/pytorch/pull/10301", "id": 348353289, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA2NzIyOTEw", "number": 10301, "title": "Refactor THCNumerics and add common math functions for at::Half", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 25, "created_at": "2018-08-07T14:43:21Z", "updated_at": "2018-11-23T15:49:04Z", "closed_at": "2018-08-24T23:03:13Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10301", "html_url": "https://github.com/pytorch/pytorch/pull/10301", "diff_url": "https://github.com/pytorch/pytorch/pull/10301.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10301.patch"}, "body_html": "<p><strong>Summary</strong>: This PR is a followup of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a>'s <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"339988863\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9318\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9318/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9318\">#9318</a>. It tries to achieve the following:</p>\n<ul>\n<li>Specializing std common math functions for <code>at::Half</code> type.</li>\n<li>Create <code>CUDANumerics.cuh</code> to contain necessary parts from <code>THCNumerics.cuh</code>.</li>\n<li>Update <code>THCNumerics.cuh</code> with new usage and comments to  demonstrate the best practice for developers and hence, making way for its deprecation.</li>\n<li>Remove legacy/redundant code path.</li>\n<li>Remove unused CUDA HALF macros (see separate PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"346804012\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10147\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10147/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10147\">#10147</a>)</li>\n</ul>\n<p><strong>Comments</strong>: <code>CUDANumerics.cuh</code> contains mathematical functions that are either not in the std namespace or are specialized for compilation with CUDA NVCC or CUDA NVRTC. This header is derived from the legacy <code>THCNumerics.cuh</code>. Following are some rationale behind why some functions were kept while others were removed:</p>\n<ul>\n<li>All arithmetic can now be done in ATen using binary cuda kernel  or CUDA tensor pointwise apply (check <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336004262\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8919\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8919/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8919\">#8919</a> and <code>CUDAApplyUtils</code>). <code>at::Half</code> comparisons rely on implicit conversion to float.</li>\n<li>Functions that are c/c++ standard compliant, have been specialized for user defined types, for instance, the std namespace has been opened up for <code>at::Half</code>, that defines math function definitions for <code>at::Half</code>. Check <code>Half-inl.h</code></li>\n<li>Some standard compliant functions are specialized here for performance reasons. For instance, <code>powi</code> is used for <code>pow</code> calculation on integral types. Moreover, <code>abs</code>, <code>isinf</code>, <code>isnan</code> are specialized to save one API call vs when used with std. Although this is subject to change, depending on if we really care about saving one API call.</li>\n<li>Numeric limits such as <code>max/min</code> is removed since they call standard defines. Moreover, numeric limits for<br>\n<code>at::Half</code> is present in <code>Half-inl.h</code>. I understood that HIP has some issue with <code>std::numeric_limits</code> and this the related github issue I found: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"306039917\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/ROCm-Developer-Tools/HIP/issues/374\" data-hovercard-type=\"issue\" data-hovercard-url=\"/ROCm-Developer-Tools/HIP/issues/374/hovercard\" href=\"https://github.com/ROCm-Developer-Tools/HIP/issues/374\">ROCm-Developer-Tools/HIP#374</a>. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23191400\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AlexVlx\">@AlexVlx</a> mentions that the issue can be avoided by launching <code>std::numeric_limits</code> in <code>__device__</code>. Since, we are launching lambdas with device contexts, I don't see an issue why <code>std::numeric_limits</code> won't compile on HIP if launched with device context within a kernel, unless I am not aware of the real reason why max/min was there in THCNumerics in the first place. (Haven't ever tried a build with HIP).</li>\n</ul>\n<p>Here are some reference PRs that was handy in refactoring TH into ATen:</p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"316077072\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6786\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6786/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6786\">#6786</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"301206395\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5475\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/5475/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/5475\">#5475</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"340829298\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9401\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9401/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9401\">#9401</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"334131086\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8689\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8689/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8689\">#8689</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336004262\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8919\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8919/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8919\">#8919</a></li>\n</ul>", "body_text": "Summary: This PR is a followup of @mruberry's #9318. It tries to achieve the following:\n\nSpecializing std common math functions for at::Half type.\nCreate CUDANumerics.cuh to contain necessary parts from THCNumerics.cuh.\nUpdate THCNumerics.cuh with new usage and comments to  demonstrate the best practice for developers and hence, making way for its deprecation.\nRemove legacy/redundant code path.\nRemove unused CUDA HALF macros (see separate PR #10147)\n\nComments: CUDANumerics.cuh contains mathematical functions that are either not in the std namespace or are specialized for compilation with CUDA NVCC or CUDA NVRTC. This header is derived from the legacy THCNumerics.cuh. Following are some rationale behind why some functions were kept while others were removed:\n\nAll arithmetic can now be done in ATen using binary cuda kernel  or CUDA tensor pointwise apply (check #8919 and CUDAApplyUtils). at::Half comparisons rely on implicit conversion to float.\nFunctions that are c/c++ standard compliant, have been specialized for user defined types, for instance, the std namespace has been opened up for at::Half, that defines math function definitions for at::Half. Check Half-inl.h\nSome standard compliant functions are specialized here for performance reasons. For instance, powi is used for pow calculation on integral types. Moreover, abs, isinf, isnan are specialized to save one API call vs when used with std. Although this is subject to change, depending on if we really care about saving one API call.\nNumeric limits such as max/min is removed since they call standard defines. Moreover, numeric limits for\nat::Half is present in Half-inl.h. I understood that HIP has some issue with std::numeric_limits and this the related github issue I found: ROCm-Developer-Tools/HIP#374. @AlexVlx mentions that the issue can be avoided by launching std::numeric_limits in __device__. Since, we are launching lambdas with device contexts, I don't see an issue why std::numeric_limits won't compile on HIP if launched with device context within a kernel, unless I am not aware of the real reason why max/min was there in THCNumerics in the first place. (Haven't ever tried a build with HIP).\n\nHere are some reference PRs that was handy in refactoring TH into ATen:\n\n#6786\n#5475\n#9401\n#8689\n#8919", "body": "**Summary**: This PR is a followup of @mruberry's https://github.com/pytorch/pytorch/pull/9318/. It tries to achieve the following:\r\n- Specializing std common math functions for `at::Half` type.\r\n- Create `CUDANumerics.cuh` to contain necessary parts from `THCNumerics.cuh`.\r\n- Update `THCNumerics.cuh` with new usage and comments to  demonstrate the best practice for developers and hence, making way for its deprecation.\r\n- Remove legacy/redundant code path.\r\n- Remove unused CUDA HALF macros (see separate PR https://github.com/pytorch/pytorch/pull/10147)\r\n\r\n**Comments**: `CUDANumerics.cuh` contains mathematical functions that are either not in the std namespace or are specialized for compilation with CUDA NVCC or CUDA NVRTC. This header is derived from the legacy `THCNumerics.cuh`. Following are some rationale behind why some functions were kept while others were removed:\r\n- All arithmetic can now be done in ATen using binary cuda kernel  or CUDA tensor pointwise apply (check https://github.com/pytorch/pytorch/pull/8919 and `CUDAApplyUtils`). `at::Half` comparisons rely on implicit conversion to float.\r\n- Functions that are c/c++ standard compliant, have been specialized for user defined types, for instance, the std namespace has been opened up for `at::Half`, that defines math function definitions for `at::Half`. Check `Half-inl.h`\r\n- Some standard compliant functions are specialized here for performance reasons. For instance, `powi` is used for `pow` calculation on integral types. Moreover, `abs`, `isinf`, `isnan` are specialized to save one API call vs when used with std. Although this is subject to change, depending on if we really care about saving one API call.\r\n- Numeric limits such as `max/min` is removed since they call standard defines. Moreover, numeric limits for\r\n`at::Half` is present in `Half-inl.h`. I understood that HIP has some issue with `std::numeric_limits` and this the related github issue I found: https://github.com/ROCm-Developer-Tools/HIP/issues/374. @AlexVlx mentions that the issue can be avoided by launching `std::numeric_limits` in `__device__`. Since, we are launching lambdas with device contexts, I don't see an issue why `std::numeric_limits` won't compile on HIP if launched with device context within a kernel, unless I am not aware of the real reason why max/min was there in THCNumerics in the first place. (Haven't ever tried a build with HIP).\r\n\r\nHere are some reference PRs that was handy in refactoring TH into ATen:\r\n- https://github.com/pytorch/pytorch/pull/6786\r\n- https://github.com/pytorch/pytorch/pull/5475\r\n- https://github.com/pytorch/pytorch/pull/9401\r\n- https://github.com/pytorch/pytorch/pull/8689\r\n- https://github.com/pytorch/pytorch/pull/8919"}