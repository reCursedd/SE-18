{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/416773333", "html_url": "https://github.com/pytorch/pytorch/pull/10301#issuecomment-416773333", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10301", "id": 416773333, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjc3MzMzMw==", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-28T23:29:50Z", "updated_at": "2018-08-28T23:29:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was able to reproduce the error. What's happening is when comparing at::Half with at::Half, the compiler is getting confused with which conversion to do since there are multiple operator overloading (while this is a problem, currently it is avoided by providing half related NVCC flags in <code>Dependencies.cmake</code>). That is, at::Half can implicitly convert to float and the comparison can be float &gt; float for instance. Or at::Half can implicitly convert to __half and the comparison can be __half &gt; __half. There exist operator overloading for comparisons in cuda_fp16.hpp and THCHalfAutonumerics. THCHalfAutonumerics is only used in THCUNN. So in your case, we need to remove half operators and conversions coming from cuda_fp16 header somehow such that the operator overloading from there is not compiled against</p>\n<p>Hence, the fix for your extension is when building using setuptools, add the flags in there as shown below. Currently you'll see, PyTorch top of tree adds these flags in <code>Dependencies.cmake</code> - <code>'-DCUDA_HAS_FP16=1','-D__CUDA_NO_HALF_OPERATORS__','-D__CUDA_NO_HALF_CONVERSIONS__','-D__CUDA_NO_HALF2_OPERATORS__'</code>, which says, don't use any of the operator overloading from cuda_fp16.hpp. This error might not have surfaced before since you might be the first one to provide at::Half support in an extension.</p>\n<pre><code>def get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, 'torchvision', 'csrc')\n\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_cflags = []\n    extra_compile_args = {'cxx':[]}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [('WITH_CUDA', None)]\n        extra_compile_args['nvcc'] = ['-DCUDA_HAS_FP16=1','-D__CUDA_NO_HALF_OPERATORS__','-D__CUDA_NO_HALF_CONVERSIONS__','-D__CUDA_NO_HALF2_OPERATORS__']\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            'torchvision._C',\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n</code></pre>", "body_text": "I was able to reproduce the error. What's happening is when comparing at::Half with at::Half, the compiler is getting confused with which conversion to do since there are multiple operator overloading (while this is a problem, currently it is avoided by providing half related NVCC flags in Dependencies.cmake). That is, at::Half can implicitly convert to float and the comparison can be float > float for instance. Or at::Half can implicitly convert to __half and the comparison can be __half > __half. There exist operator overloading for comparisons in cuda_fp16.hpp and THCHalfAutonumerics. THCHalfAutonumerics is only used in THCUNN. So in your case, we need to remove half operators and conversions coming from cuda_fp16 header somehow such that the operator overloading from there is not compiled against\nHence, the fix for your extension is when building using setuptools, add the flags in there as shown below. Currently you'll see, PyTorch top of tree adds these flags in Dependencies.cmake - '-DCUDA_HAS_FP16=1','-D__CUDA_NO_HALF_OPERATORS__','-D__CUDA_NO_HALF_CONVERSIONS__','-D__CUDA_NO_HALF2_OPERATORS__', which says, don't use any of the operator overloading from cuda_fp16.hpp. This error might not have surfaced before since you might be the first one to provide at::Half support in an extension.\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, 'torchvision', 'csrc')\n\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_cflags = []\n    extra_compile_args = {'cxx':[]}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [('WITH_CUDA', None)]\n        extra_compile_args['nvcc'] = ['-DCUDA_HAS_FP16=1','-D__CUDA_NO_HALF_OPERATORS__','-D__CUDA_NO_HALF_CONVERSIONS__','-D__CUDA_NO_HALF2_OPERATORS__']\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            'torchvision._C',\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules", "body": "I was able to reproduce the error. What's happening is when comparing at::Half with at::Half, the compiler is getting confused with which conversion to do since there are multiple operator overloading (while this is a problem, currently it is avoided by providing half related NVCC flags in `Dependencies.cmake`). That is, at::Half can implicitly convert to float and the comparison can be float > float for instance. Or at::Half can implicitly convert to __half and the comparison can be __half > __half. There exist operator overloading for comparisons in cuda_fp16.hpp and THCHalfAutonumerics. THCHalfAutonumerics is only used in THCUNN. So in your case, we need to remove half operators and conversions coming from cuda_fp16 header somehow such that the operator overloading from there is not compiled against\r\n\r\nHence, the fix for your extension is when building using setuptools, add the flags in there as shown below. Currently you'll see, PyTorch top of tree adds these flags in `Dependencies.cmake` - `'-DCUDA_HAS_FP16=1','-D__CUDA_NO_HALF_OPERATORS__','-D__CUDA_NO_HALF_CONVERSIONS__','-D__CUDA_NO_HALF2_OPERATORS__'`, which says, don't use any of the operator overloading from cuda_fp16.hpp. This error might not have surfaced before since you might be the first one to provide at::Half support in an extension.\r\n\r\n```\r\ndef get_extensions():\r\n    this_dir = os.path.dirname(os.path.abspath(__file__))\r\n    extensions_dir = os.path.join(this_dir, 'torchvision', 'csrc')\r\n\r\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\r\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\r\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\r\n\r\n    sources = main_file + source_cpu\r\n    extension = CppExtension\r\n\r\n    extra_cflags = []\r\n    extra_compile_args = {'cxx':[]}\r\n    define_macros = []\r\n\r\n    if torch.cuda.is_available() and CUDA_HOME is not None:\r\n        extension = CUDAExtension\r\n        sources += source_cuda\r\n        define_macros += [('WITH_CUDA', None)]\r\n        extra_compile_args['nvcc'] = ['-DCUDA_HAS_FP16=1','-D__CUDA_NO_HALF_OPERATORS__','-D__CUDA_NO_HALF_CONVERSIONS__','-D__CUDA_NO_HALF2_OPERATORS__']\r\n\r\n    sources = [os.path.join(extensions_dir, s) for s in sources]\r\n\r\n    include_dirs = [extensions_dir]\r\n\r\n    ext_modules = [\r\n        extension(\r\n            'torchvision._C',\r\n            sources,\r\n            include_dirs=include_dirs,\r\n            define_macros=define_macros,\r\n            extra_compile_args=extra_compile_args,\r\n        )\r\n    ]\r\n\r\n    return ext_modules\r\n```"}