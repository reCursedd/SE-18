{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/374006211", "html_url": "https://github.com/pytorch/pytorch/issues/3749#issuecomment-374006211", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3749", "id": 374006211, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDAwNjIxMQ==", "user": {"login": "tartavull", "id": 4648166, "node_id": "MDQ6VXNlcjQ2NDgxNjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/4648166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tartavull", "html_url": "https://github.com/tartavull", "followers_url": "https://api.github.com/users/tartavull/followers", "following_url": "https://api.github.com/users/tartavull/following{/other_user}", "gists_url": "https://api.github.com/users/tartavull/gists{/gist_id}", "starred_url": "https://api.github.com/users/tartavull/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tartavull/subscriptions", "organizations_url": "https://api.github.com/users/tartavull/orgs", "repos_url": "https://api.github.com/users/tartavull/repos", "events_url": "https://api.github.com/users/tartavull/events{/privacy}", "received_events_url": "https://api.github.com/users/tartavull/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-18T15:03:13Z", "updated_at": "2018-03-18T16:33:22Z", "author_association": "NONE", "body_html": "<p>Here is a quick and dirty prototype that combines both approaches.<br>\nIt lets you decorate a module with <code>@profile</code> and it overwrites the default print function</p>\n<p>Decoration and printing</p>\n<pre><code>@profile\nclass Encoder(nn.Module):\n     def __init__( ...\n\nsensor_shape = (1,28,28)\nimg = np.random.random((64,)+sensor_shape)\nsensor_img = Variable(torch.FloatTensor(img))\nenco = Encoder(sensor_shape, world_shape=world.shape)\nenco_out = enco(sensor_img)\nprint(enco)\n</code></pre>\n<p>printing output</p>\n<pre><code>  CPU Time   GPU Time  Parameters   Input    Architecture  \n===========================================================\n  20.83ms       -       71.79k      2.96m     Encoder(\n  20.83ms       -       71.79k      2.96m       (down_convs): ModuleList(\n  9.64ms        -        2.48k      1.66m         (0): DownConv(\n  2.87ms        -       160.00     50.18k           (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  5.46ms        -        2.32k     802.82k          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  1.31ms        -          -       802.82k          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n                                                  )\n  6.39ms        -       13.89k      1.00m         (1): DownConv(\n  2.28ms        -        4.64k     200.70k          (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  3.71ms        -        9.25k     401.41k          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n 403.13us       -          -       401.41k          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n                                                  )\n  4.80ms        -       55.42k     301.06k        (2): DownConv(\n  1.64ms        -       18.50k     100.35k          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  3.16ms        -       36.93k     200.70k          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n                                                  )\n                                                )\n                                              )\n\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4969797\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhuwenxi\">@zhuwenxi</a> it doesn't yet support backward<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I was wondering if it was possible to extract other metrics that you can usually access with the nvidia profiler, like memory usage.</p>\n<p>Here is the code ( again rather dirty, buggy and limited, but happy to improve it if it is of any use ):</p>\n<pre><code>from __future__ import division\n\nfrom collections import defaultdict\nfrom functools import lru_cache, reduce\nimport math\nimport operator\n\nimport torch\n\n\ndef profile(module, display_cpu=True, display_gpu=True):\n    assert issubclass(module, torch.nn.Module)\n    monkey_patch_init(module)\n    return module\n\ndef monkey_patch_init(_class):\n    old_init = _class.__init__\n    def new_init(self, *args, **kwargs):\n        old_init(self, *args, **kwargs)\n        self.profiler = Profiler(self)\n        _class.__str__ = self.profiler.__str__\n    _class.__init__ = new_init\n\nclass Profiler(object):\n    def __init__(self, module):\n        \"\"\"\n        An operation is a graph node that performs computation on tensors.\n        \"\"\"\n        self._module = module\n        self._events = {\n            'forward': defaultdict(Event),\n            'backward': defaultdict(Event)}\n        self._operations = {}\n        self._enable = True\n\n        #consume generator\n        list(map(self._hook_operation, operations(self._module)))\n    \n    def _hook_operation(self, op):\n        def wrapper_call(op, *input, **kwargs):\n            # Wrapper function to \"__call__\", with time counter in it.\n            if not self._enable:\n                return self._operations[op.__class__](op, *input, **kwargs)\n\n            with torch.autograd.profiler.profile() as prof:\n                result = self._operations[op.__class__](op, *input, **kwargs)\n            \n            self._events['forward'][op] += Event(\n                cpu_time=int(prof.total_average().cpu_time),\n                gpu_time=int(prof.total_average().cuda_time),\n                parameters=count_elements(op.parameters()),\n                input_size=count_elements(input),\n                hits=1)\n            \n            def backward_pre_hook(*args):\n                if not self._enable:\n                    return\n                self._events['backward'][op].append(time.time())\n            #result.grad_fn.register_pre_hook(backward_pre_hook);\n            return result\n\n        # monky patch \"__call__\" with \"wrapper_call\"  for this operation`\n        if op.__class__ not in self._operations:\n            self._operations[op.__class__] = op.__class__.__call__\n            op.__class__.__call__ = wrapper_call\n\n        #def backward_post_hook(*args):\n        #    if not this_profiler.profiling_on:\n        #        return\n        #    # adds ending time\n        #    backward = this_profiler.record['backward']\n        #    backward[-1] = backward[-1] + (time.time(),) \n        #op.register_backward_hook(backward_post_hook)   \n    \n    @lru_cache(maxsize=None)\n    def get_metrics(self, module):\n        if module in self._events['forward']:\n            #it's an operation\n            return self._events['forward'][module]\n        \n        return reduce(operator.add, map(self.get_metrics, module._modules.values()))\n    \n    def __str__(self, module=None, indentation=0, pre_msg=''):\n        tmpstr = ''\n        if module is None:\n            module = self._module\n            tmpstr += Event.header()\n\n        # this is an operation\n        metrics = self.get_metrics(module).tostring()\n    \n        if module.__class__ in self._operations:\n            return  tmpstr + metrics + indent(pre_msg + module.__repr__(), indentation) + '\\n'\n        \n        name = module.__class__.__name__\n        tmpstr += metrics + indent(pre_msg + name  + '(', indentation) + '\\n'\n        for key, sub_module in module._modules.items():\n            tmpstr +=  self.__str__(sub_module, indentation+2, pre_msg='(' + key + '): ')\n        tmpstr +=  indent(')',indentation+len(metrics)) + '\\n'\n        return tmpstr        \n\nclass Event(object):\n    def __init__(self, cpu_time=0, gpu_time=0, parameters=0, input_size=0, hits=0):\n        self.cpu_time = cpu_time\n        self.gpu_time = gpu_time\n        self.parameters = parameters\n        self.input_size = input_size\n        self.hits = hits\n    \n    @classmethod\n    def header(cls):\n        header = format_columns(['CPU Time','GPU Time','Parameters','Input','Architecture'])\n        return '\\n'.join([header,'='*len(header),''])\n\n    def tostring(self):\n        return format_columns([\n                format_time(self.cpu_time),\n                format_time(self.gpu_time),\n                format_count(self.parameters),\n                format_count(self.input_size)])\n    \n    def __add__(self, other):\n        return Event(\n            self.cpu_time + other.cpu_time,\n            self.gpu_time + other.gpu_time,\n            self.parameters + other.parameters,\n            self.input_size + other.input_size,\n            self.hits + other.hits)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\ndef format_columns(cols, width=10):\n    assert isinstance(cols, list)\n    return  ' ' + ' '.join(col.center(width,' ') for col in cols) + '  '\n\ndef format_time(time_in_ns):\n    if not time_in_ns:\n        return '-'\n\n    human_powers = ['n','u','m','']\n    power = int(math.log(time_in_ns, 10) // 3)\n    return '{:.2f}{}s '.format(\n            time_in_ns/1000.**power,\n            human_powers[power])\n\ndef format_count(n):\n    if not n:\n        return '-'\n\n    human_powers = ['','k','m','g']\n    power = int(math.log(n, 10) // 3)\n    return '{:.2f}{} '.format(\n            n/1000.**power,\n            human_powers[power])\n\ndef operations(module):\n    \"\"\"\n    Given a module recursively transverse it\n    to find all atomic operations.\n\n    Atomic operations are the nodes in the graph which\n    perform computations on the tensors.\n    \"\"\"\n    if not len(list(module.children())):\n        # nn.Module who doesn't have sub nn.Module, hook it.\n        yield module\n\n    for name, sub_module in module.named_children():\n        if (isinstance(sub_module, torch.nn.Container)\n            or isinstance(sub_module, torch.nn.Sequential)\n            or isinstance(sub_module, torch.nn.ModuleList)\n            or isinstance(sub_module, torch.nn.Module)):\n            # Recursively visit their decendants.\n            for op in operations(sub_module): #python2 compatibility\n                yield op\n\ndef indent(s, indent):\n    return '\\n'.join((indent* ' ') + line for line in s.split('\\n'))\n\ndef count_elements(tensors):\n    return sum([reduce(operator.mul, t.size()) for t in tensors])\n</code></pre>", "body_text": "Here is a quick and dirty prototype that combines both approaches.\nIt lets you decorate a module with @profile and it overwrites the default print function\nDecoration and printing\n@profile\nclass Encoder(nn.Module):\n     def __init__( ...\n\nsensor_shape = (1,28,28)\nimg = np.random.random((64,)+sensor_shape)\nsensor_img = Variable(torch.FloatTensor(img))\nenco = Encoder(sensor_shape, world_shape=world.shape)\nenco_out = enco(sensor_img)\nprint(enco)\n\nprinting output\n  CPU Time   GPU Time  Parameters   Input    Architecture  \n===========================================================\n  20.83ms       -       71.79k      2.96m     Encoder(\n  20.83ms       -       71.79k      2.96m       (down_convs): ModuleList(\n  9.64ms        -        2.48k      1.66m         (0): DownConv(\n  2.87ms        -       160.00     50.18k           (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  5.46ms        -        2.32k     802.82k          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  1.31ms        -          -       802.82k          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n                                                  )\n  6.39ms        -       13.89k      1.00m         (1): DownConv(\n  2.28ms        -        4.64k     200.70k          (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  3.71ms        -        9.25k     401.41k          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n 403.13us       -          -       401.41k          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n                                                  )\n  4.80ms        -       55.42k     301.06k        (2): DownConv(\n  1.64ms        -       18.50k     100.35k          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  3.16ms        -       36.93k     200.70k          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n                                                  )\n                                                )\n                                              )\n\n\n@zhuwenxi it doesn't yet support backward\n@apaszke I was wondering if it was possible to extract other metrics that you can usually access with the nvidia profiler, like memory usage.\nHere is the code ( again rather dirty, buggy and limited, but happy to improve it if it is of any use ):\nfrom __future__ import division\n\nfrom collections import defaultdict\nfrom functools import lru_cache, reduce\nimport math\nimport operator\n\nimport torch\n\n\ndef profile(module, display_cpu=True, display_gpu=True):\n    assert issubclass(module, torch.nn.Module)\n    monkey_patch_init(module)\n    return module\n\ndef monkey_patch_init(_class):\n    old_init = _class.__init__\n    def new_init(self, *args, **kwargs):\n        old_init(self, *args, **kwargs)\n        self.profiler = Profiler(self)\n        _class.__str__ = self.profiler.__str__\n    _class.__init__ = new_init\n\nclass Profiler(object):\n    def __init__(self, module):\n        \"\"\"\n        An operation is a graph node that performs computation on tensors.\n        \"\"\"\n        self._module = module\n        self._events = {\n            'forward': defaultdict(Event),\n            'backward': defaultdict(Event)}\n        self._operations = {}\n        self._enable = True\n\n        #consume generator\n        list(map(self._hook_operation, operations(self._module)))\n    \n    def _hook_operation(self, op):\n        def wrapper_call(op, *input, **kwargs):\n            # Wrapper function to \"__call__\", with time counter in it.\n            if not self._enable:\n                return self._operations[op.__class__](op, *input, **kwargs)\n\n            with torch.autograd.profiler.profile() as prof:\n                result = self._operations[op.__class__](op, *input, **kwargs)\n            \n            self._events['forward'][op] += Event(\n                cpu_time=int(prof.total_average().cpu_time),\n                gpu_time=int(prof.total_average().cuda_time),\n                parameters=count_elements(op.parameters()),\n                input_size=count_elements(input),\n                hits=1)\n            \n            def backward_pre_hook(*args):\n                if not self._enable:\n                    return\n                self._events['backward'][op].append(time.time())\n            #result.grad_fn.register_pre_hook(backward_pre_hook);\n            return result\n\n        # monky patch \"__call__\" with \"wrapper_call\"  for this operation`\n        if op.__class__ not in self._operations:\n            self._operations[op.__class__] = op.__class__.__call__\n            op.__class__.__call__ = wrapper_call\n\n        #def backward_post_hook(*args):\n        #    if not this_profiler.profiling_on:\n        #        return\n        #    # adds ending time\n        #    backward = this_profiler.record['backward']\n        #    backward[-1] = backward[-1] + (time.time(),) \n        #op.register_backward_hook(backward_post_hook)   \n    \n    @lru_cache(maxsize=None)\n    def get_metrics(self, module):\n        if module in self._events['forward']:\n            #it's an operation\n            return self._events['forward'][module]\n        \n        return reduce(operator.add, map(self.get_metrics, module._modules.values()))\n    \n    def __str__(self, module=None, indentation=0, pre_msg=''):\n        tmpstr = ''\n        if module is None:\n            module = self._module\n            tmpstr += Event.header()\n\n        # this is an operation\n        metrics = self.get_metrics(module).tostring()\n    \n        if module.__class__ in self._operations:\n            return  tmpstr + metrics + indent(pre_msg + module.__repr__(), indentation) + '\\n'\n        \n        name = module.__class__.__name__\n        tmpstr += metrics + indent(pre_msg + name  + '(', indentation) + '\\n'\n        for key, sub_module in module._modules.items():\n            tmpstr +=  self.__str__(sub_module, indentation+2, pre_msg='(' + key + '): ')\n        tmpstr +=  indent(')',indentation+len(metrics)) + '\\n'\n        return tmpstr        \n\nclass Event(object):\n    def __init__(self, cpu_time=0, gpu_time=0, parameters=0, input_size=0, hits=0):\n        self.cpu_time = cpu_time\n        self.gpu_time = gpu_time\n        self.parameters = parameters\n        self.input_size = input_size\n        self.hits = hits\n    \n    @classmethod\n    def header(cls):\n        header = format_columns(['CPU Time','GPU Time','Parameters','Input','Architecture'])\n        return '\\n'.join([header,'='*len(header),''])\n\n    def tostring(self):\n        return format_columns([\n                format_time(self.cpu_time),\n                format_time(self.gpu_time),\n                format_count(self.parameters),\n                format_count(self.input_size)])\n    \n    def __add__(self, other):\n        return Event(\n            self.cpu_time + other.cpu_time,\n            self.gpu_time + other.gpu_time,\n            self.parameters + other.parameters,\n            self.input_size + other.input_size,\n            self.hits + other.hits)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\ndef format_columns(cols, width=10):\n    assert isinstance(cols, list)\n    return  ' ' + ' '.join(col.center(width,' ') for col in cols) + '  '\n\ndef format_time(time_in_ns):\n    if not time_in_ns:\n        return '-'\n\n    human_powers = ['n','u','m','']\n    power = int(math.log(time_in_ns, 10) // 3)\n    return '{:.2f}{}s '.format(\n            time_in_ns/1000.**power,\n            human_powers[power])\n\ndef format_count(n):\n    if not n:\n        return '-'\n\n    human_powers = ['','k','m','g']\n    power = int(math.log(n, 10) // 3)\n    return '{:.2f}{} '.format(\n            n/1000.**power,\n            human_powers[power])\n\ndef operations(module):\n    \"\"\"\n    Given a module recursively transverse it\n    to find all atomic operations.\n\n    Atomic operations are the nodes in the graph which\n    perform computations on the tensors.\n    \"\"\"\n    if not len(list(module.children())):\n        # nn.Module who doesn't have sub nn.Module, hook it.\n        yield module\n\n    for name, sub_module in module.named_children():\n        if (isinstance(sub_module, torch.nn.Container)\n            or isinstance(sub_module, torch.nn.Sequential)\n            or isinstance(sub_module, torch.nn.ModuleList)\n            or isinstance(sub_module, torch.nn.Module)):\n            # Recursively visit their decendants.\n            for op in operations(sub_module): #python2 compatibility\n                yield op\n\ndef indent(s, indent):\n    return '\\n'.join((indent* ' ') + line for line in s.split('\\n'))\n\ndef count_elements(tensors):\n    return sum([reduce(operator.mul, t.size()) for t in tensors])", "body": "Here is a quick and dirty prototype that combines both approaches.\r\nIt lets you decorate a module with `@profile` and it overwrites the default print function\r\n\r\nDecoration and printing\r\n```\r\n@profile\r\nclass Encoder(nn.Module):\r\n     def __init__( ...\r\n\r\nsensor_shape = (1,28,28)\r\nimg = np.random.random((64,)+sensor_shape)\r\nsensor_img = Variable(torch.FloatTensor(img))\r\nenco = Encoder(sensor_shape, world_shape=world.shape)\r\nenco_out = enco(sensor_img)\r\nprint(enco)\r\n```\r\n\r\nprinting output\r\n```\r\n  CPU Time   GPU Time  Parameters   Input    Architecture  \r\n===========================================================\r\n  20.83ms       -       71.79k      2.96m     Encoder(\r\n  20.83ms       -       71.79k      2.96m       (down_convs): ModuleList(\r\n  9.64ms        -        2.48k      1.66m         (0): DownConv(\r\n  2.87ms        -       160.00     50.18k           (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n  5.46ms        -        2.32k     802.82k          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n  1.31ms        -          -       802.82k          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\r\n                                                  )\r\n  6.39ms        -       13.89k      1.00m         (1): DownConv(\r\n  2.28ms        -        4.64k     200.70k          (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n  3.71ms        -        9.25k     401.41k          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n 403.13us       -          -       401.41k          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\r\n                                                  )\r\n  4.80ms        -       55.42k     301.06k        (2): DownConv(\r\n  1.64ms        -       18.50k     100.35k          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n  3.16ms        -       36.93k     200.70k          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n                                                  )\r\n                                                )\r\n                                              )\r\n\r\n```\r\n@zhuwenxi it doesn't yet support backward\r\n@apaszke I was wondering if it was possible to extract other metrics that you can usually access with the nvidia profiler, like memory usage.\r\n\r\nHere is the code ( again rather dirty, buggy and limited, but happy to improve it if it is of any use ):\r\n```\r\nfrom __future__ import division\r\n\r\nfrom collections import defaultdict\r\nfrom functools import lru_cache, reduce\r\nimport math\r\nimport operator\r\n\r\nimport torch\r\n\r\n\r\ndef profile(module, display_cpu=True, display_gpu=True):\r\n    assert issubclass(module, torch.nn.Module)\r\n    monkey_patch_init(module)\r\n    return module\r\n\r\ndef monkey_patch_init(_class):\r\n    old_init = _class.__init__\r\n    def new_init(self, *args, **kwargs):\r\n        old_init(self, *args, **kwargs)\r\n        self.profiler = Profiler(self)\r\n        _class.__str__ = self.profiler.__str__\r\n    _class.__init__ = new_init\r\n\r\nclass Profiler(object):\r\n    def __init__(self, module):\r\n        \"\"\"\r\n        An operation is a graph node that performs computation on tensors.\r\n        \"\"\"\r\n        self._module = module\r\n        self._events = {\r\n            'forward': defaultdict(Event),\r\n            'backward': defaultdict(Event)}\r\n        self._operations = {}\r\n        self._enable = True\r\n\r\n        #consume generator\r\n        list(map(self._hook_operation, operations(self._module)))\r\n    \r\n    def _hook_operation(self, op):\r\n        def wrapper_call(op, *input, **kwargs):\r\n            # Wrapper function to \"__call__\", with time counter in it.\r\n            if not self._enable:\r\n                return self._operations[op.__class__](op, *input, **kwargs)\r\n\r\n            with torch.autograd.profiler.profile() as prof:\r\n                result = self._operations[op.__class__](op, *input, **kwargs)\r\n            \r\n            self._events['forward'][op] += Event(\r\n                cpu_time=int(prof.total_average().cpu_time),\r\n                gpu_time=int(prof.total_average().cuda_time),\r\n                parameters=count_elements(op.parameters()),\r\n                input_size=count_elements(input),\r\n                hits=1)\r\n            \r\n            def backward_pre_hook(*args):\r\n                if not self._enable:\r\n                    return\r\n                self._events['backward'][op].append(time.time())\r\n            #result.grad_fn.register_pre_hook(backward_pre_hook);\r\n            return result\r\n\r\n        # monky patch \"__call__\" with \"wrapper_call\"  for this operation`\r\n        if op.__class__ not in self._operations:\r\n            self._operations[op.__class__] = op.__class__.__call__\r\n            op.__class__.__call__ = wrapper_call\r\n\r\n        #def backward_post_hook(*args):\r\n        #    if not this_profiler.profiling_on:\r\n        #        return\r\n        #    # adds ending time\r\n        #    backward = this_profiler.record['backward']\r\n        #    backward[-1] = backward[-1] + (time.time(),) \r\n        #op.register_backward_hook(backward_post_hook)   \r\n    \r\n    @lru_cache(maxsize=None)\r\n    def get_metrics(self, module):\r\n        if module in self._events['forward']:\r\n            #it's an operation\r\n            return self._events['forward'][module]\r\n        \r\n        return reduce(operator.add, map(self.get_metrics, module._modules.values()))\r\n    \r\n    def __str__(self, module=None, indentation=0, pre_msg=''):\r\n        tmpstr = ''\r\n        if module is None:\r\n            module = self._module\r\n            tmpstr += Event.header()\r\n\r\n        # this is an operation\r\n        metrics = self.get_metrics(module).tostring()\r\n    \r\n        if module.__class__ in self._operations:\r\n            return  tmpstr + metrics + indent(pre_msg + module.__repr__(), indentation) + '\\n'\r\n        \r\n        name = module.__class__.__name__\r\n        tmpstr += metrics + indent(pre_msg + name  + '(', indentation) + '\\n'\r\n        for key, sub_module in module._modules.items():\r\n            tmpstr +=  self.__str__(sub_module, indentation+2, pre_msg='(' + key + '): ')\r\n        tmpstr +=  indent(')',indentation+len(metrics)) + '\\n'\r\n        return tmpstr        \r\n\r\nclass Event(object):\r\n    def __init__(self, cpu_time=0, gpu_time=0, parameters=0, input_size=0, hits=0):\r\n        self.cpu_time = cpu_time\r\n        self.gpu_time = gpu_time\r\n        self.parameters = parameters\r\n        self.input_size = input_size\r\n        self.hits = hits\r\n    \r\n    @classmethod\r\n    def header(cls):\r\n        header = format_columns(['CPU Time','GPU Time','Parameters','Input','Architecture'])\r\n        return '\\n'.join([header,'='*len(header),''])\r\n\r\n    def tostring(self):\r\n        return format_columns([\r\n                format_time(self.cpu_time),\r\n                format_time(self.gpu_time),\r\n                format_count(self.parameters),\r\n                format_count(self.input_size)])\r\n    \r\n    def __add__(self, other):\r\n        return Event(\r\n            self.cpu_time + other.cpu_time,\r\n            self.gpu_time + other.gpu_time,\r\n            self.parameters + other.parameters,\r\n            self.input_size + other.input_size,\r\n            self.hits + other.hits)\r\n\r\n    def __radd__(self, other):\r\n        return self.__add__(other)\r\n\r\ndef format_columns(cols, width=10):\r\n    assert isinstance(cols, list)\r\n    return  ' ' + ' '.join(col.center(width,' ') for col in cols) + '  '\r\n\r\ndef format_time(time_in_ns):\r\n    if not time_in_ns:\r\n        return '-'\r\n\r\n    human_powers = ['n','u','m','']\r\n    power = int(math.log(time_in_ns, 10) // 3)\r\n    return '{:.2f}{}s '.format(\r\n            time_in_ns/1000.**power,\r\n            human_powers[power])\r\n\r\ndef format_count(n):\r\n    if not n:\r\n        return '-'\r\n\r\n    human_powers = ['','k','m','g']\r\n    power = int(math.log(n, 10) // 3)\r\n    return '{:.2f}{} '.format(\r\n            n/1000.**power,\r\n            human_powers[power])\r\n\r\ndef operations(module):\r\n    \"\"\"\r\n    Given a module recursively transverse it\r\n    to find all atomic operations.\r\n\r\n    Atomic operations are the nodes in the graph which\r\n    perform computations on the tensors.\r\n    \"\"\"\r\n    if not len(list(module.children())):\r\n        # nn.Module who doesn't have sub nn.Module, hook it.\r\n        yield module\r\n\r\n    for name, sub_module in module.named_children():\r\n        if (isinstance(sub_module, torch.nn.Container)\r\n            or isinstance(sub_module, torch.nn.Sequential)\r\n            or isinstance(sub_module, torch.nn.ModuleList)\r\n            or isinstance(sub_module, torch.nn.Module)):\r\n            # Recursively visit their decendants.\r\n            for op in operations(sub_module): #python2 compatibility\r\n                yield op\r\n\r\ndef indent(s, indent):\r\n    return '\\n'.join((indent* ' ') + line for line in s.split('\\n'))\r\n\r\ndef count_elements(tensors):\r\n    return sum([reduce(operator.mul, t.size()) for t in tensors])\r\n```"}